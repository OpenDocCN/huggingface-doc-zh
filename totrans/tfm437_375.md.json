["```py\n>>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel\n\n>>> config_encoder = ViTConfig()\n>>> config_decoder = BertConfig()\n\n>>> config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n>>> model = VisionEncoderDecoderModel(config=config)\n```", "```py\n>>> from transformers import VisionEncoderDecoderModel\n\n>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"microsoft/swin-base-patch4-window7-224-in22k\", \"bert-base-uncased\"\n... )\n```", "```py\n>>> import requests\n>>> from PIL import Image\n\n>>> from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n\n>>> # load a fine-tuned image captioning model and corresponding tokenizer and image processor\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n>>> tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n>>> image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n>>> # let's perform inference on an image\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> # autoregressively generate caption (uses greedy decoding by default)\n>>> generated_ids = model.generate(pixel_values)\n>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\na cat laying on a blanket next to a cat laying on a bed\n```", "```py\n>>> from transformers import VisionEncoderDecoderModel, TFVisionEncoderDecoderModel\n\n>>> _model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n>>> _model.encoder.save_pretrained(\"./encoder\")\n>>> _model.decoder.save_pretrained(\"./decoder\")\n\n>>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n... )\n>>> # This is only for copying some specific attributes of this particular model.\n>>> model.config = _model.config\n```", "```py\n>>> from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n\n>>> image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"bert-base-uncased\"\n... )\n\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> labels = tokenizer(\n...     \"an image of two cats chilling on a couch\",\n...     return_tensors=\"pt\",\n... ).input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(pixel_values=pixel_values, labels=labels).loss\n```", "```py\n>>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel\n\n>>> # Initializing a ViT & BERT style configuration\n>>> config_encoder = ViTConfig()\n>>> config_decoder = BertConfig()\n\n>>> config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n\n>>> # Initializing a ViTBert model (with random weights) from a ViT & bert-base-uncased style configurations\n>>> model = VisionEncoderDecoderModel(config=config)\n\n>>> # Accessing the model configuration\n>>> config_encoder = model.config.encoder\n>>> config_decoder = model.config.decoder\n>>> # set decoder config to causal lm\n>>> config_decoder.is_decoder = True\n>>> config_decoder.add_cross_attention = True\n\n>>> # Saving the model, including its configuration\n>>> model.save_pretrained(\"my-model\")\n\n>>> # loading model and config from pretrained folder\n>>> encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(\"my-model\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"my-model\", config=encoder_decoder_config)\n```", "```py\n>>> from transformers import AutoProcessor, VisionEncoderDecoderModel\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n>>> # load image from the IAM dataset\n>>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> # training\n>>> model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n>>> model.config.pad_token_id = processor.tokenizer.pad_token_id\n>>> model.config.vocab_size = model.config.decoder.vocab_size\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> text = \"hello world\"\n>>> labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n>>> outputs = model(pixel_values=pixel_values, labels=labels)\n>>> loss = outputs.loss\n\n>>> # inference (generation)\n>>> generated_ids = model.generate(pixel_values)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```", "```py\n>>> from transformers import VisionEncoderDecoderModel\n\n>>> # initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized\n>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"bert-base-uncased\"\n... )\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./vit-bert\")\n>>> # load fine-tuned model\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"./vit-bert\")\n```", "```py\n>>> from transformers import AutoImageProcessor, AutoTokenizer, TFVisionEncoderDecoderModel\n>>> from PIL import Image\n>>> import requests\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n>>> decoder_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n>>> # initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized\n>>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"gpt2\"\n... )\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> img = Image.open(requests.get(url, stream=True).raw)\n\n>>> # forward\n>>> pixel_values = image_processor(images=img, return_tensors=\"tf\").pixel_values  # Batch size 1\n>>> decoder_input_ids = decoder_tokenizer(\"Linda Davis\", return_tensors=\"tf\").input_ids  # Batch size 1\n>>> outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\n\n>>> # training\n>>> outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)\n>>> loss, logits = outputs.loss, outputs.logits\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-gpt2\")\n>>> model = TFVisionEncoderDecoderModel.from_pretrained(\"vit-gpt2\")\n\n>>> # generation\n>>> generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)\n```", "```py\n>>> from transformers import TFVisionEncoderDecoderModel\n\n>>> # initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized\n>>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"bert-base-uncased\"\n... )\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./vit-bert\")\n>>> # load fine-tuned model\n>>> model = TFVisionEncoderDecoderModel.from_pretrained(\"./vit-bert\")\n```", "```py\n>>> from transformers import FlaxVisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n\n>>> # load output tokenizer\n>>> tokenizer_output = AutoTokenizer.from_pretrained(\"gpt2\")\n\n>>> # initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized\n>>> model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"gpt2\"\n... )\n\n>>> pixel_values = image_processor(images=image, return_tensors=\"np\").pixel_values\n\n>>> # use GPT2's eos_token as the pad as well as eos token\n>>> model.config.eos_token_id = model.config.decoder.eos_token_id\n>>> model.config.pad_token_id = model.config.eos_token_id\n\n>>> # generation\n>>> sequences = model.generate(pixel_values, num_beams=4, max_length=12).sequences\n\n>>> captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)\n```", "```py\n>>> from transformers import FlaxVisionEncoderDecoderModel\n\n>>> # initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized\n>>> model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"gpt2\"\n... )\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./vit-gpt2\")\n>>> # load fine-tuned model\n>>> model = FlaxVisionEncoderDecoderModel.from_pretrained(\"./vit-gpt2\")\n```"]