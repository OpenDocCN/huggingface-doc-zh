- en: GPT-Sw3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The GPT-Sw3 model was first proposed in [Lessons Learned from GPT-SW3: Building
    the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)
    by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine
    Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.'
  prefs: []
  type: TYPE_NORMAL
- en: Since that first paper the authors have extended their work and trained new
    models on their new 1.2TB corpora named The Nordic Pile.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-Sw3 is a collection of large decoder-only pretrained transformer language
    models that were developed by AI Sweden in collaboration with RISE and the WASP
    WARA for Media and Language. GPT-Sw3 has been trained on a dataset containing
    320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming
    code. The model was pretrained using a causal language modeling (CLM) objective
    utilizing the NeMo Megatron GPT implementation.
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [AI Sweden Models](https://huggingface.co/AI-Sweden-Models).
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text classification task guide](../tasks/sequence_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Token classification task guide](../tasks/token_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation uses the `GPT2Model` coupled with our `GPTSw3Tokenizer`.
    Refer to [GPT2Model documentation](gpt2) for API reference and examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note that sentencepiece is required to use our tokenizer and can be installed
    with `pip install transformers[sentencepiece]` or `pip install sentencepiece`
  prefs: []
  type: TYPE_NORMAL
- en: GPTSw3Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTSw3Tokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_lower_case` (`bool`, *optional*, defaults to `False`) — Whether or not
    to lowercase the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_space` (`bool`, *optional*, defaults to `False`) — Whether or not to
    strip the text when tokenizing (removing excess spaces before and after the string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_accents` (`bool`, *optional*, defaults to `False`) — Whether or not to
    keep accents when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*) — The token used for padding, for example when
    batching sequences of different lengths. If not provided, will default to ’<pad>’
    or ’<unk>’ depending on model size.</unk></pad>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*) — The unknown token. A token that is not in
    the vocabulary cannot be converted to an ID and is set to be this token instead.
    If not provided, will default to ’<unk>‘.</unk>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*) — The end of sequence token seen during pretraining.
    If not provided, will default to ’<|endoftext|>’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（'str`，*可选*）--在预训练过程中看到的序列结束标记。如果未提供，则默认为`<|endoftext|>`'
- en: '`bos_token` (`str`, *optional*) — The beginning of sequence token that can
    be used for downstream task, was not seen during pretraining. If not provided,
    will default to ’~~’ or ’<|endoftext|>’, depending on model size.~~'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*）--可用于下游任务的序列标记的开头，在预训练过程中没有看到。如果未提供，将默认为`~~`或`<|endoftext|>`，具体取决于模型大小'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`whitespaces` (`set`) — The whitespaces that are replaced in the whitespace
    normalization in preprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`non_printing_characters_re` (`Pattern`) — The compiled regular expression
    to remove non-printing characters in preprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct an GPTSw3 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L258)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
