- en: Creating an EvaluationSuite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/evaluate/evaluation_suite](https://huggingface.co/docs/evaluate/evaluation_suite)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/evaluation_suite.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/CodeBlock-hf-doc-builder.js">
  prefs: []
  type: TYPE_NORMAL
- en: It can be useful to evaluate models on a variety of different tasks to understand
    their downstream performance. Assessing the model on several types of tasks can
    reveal gaps in performance along some axis. For example, when training a language
    model, it is often useful to measure perplexity on an in-domain corpus, but also
    to concurrently evaluate on tasks which test for general language capabilities
    like natural language entailment or question-answering, or tasks designed to probe
    the model along fairness and bias dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The `EvaluationSuite` provides a way to compose any number of ([evaluator](base_evaluator),
    dataset, metric) tuples as a SubTask to evaluate a model on a collection of several
    evaluation tasks. See the [evaluator documentation](base_evaluator) for a list
    of currently supported tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A new `EvaluationSuite` is made up of a list of `SubTask` classes, each defining
    an evaluation task. The Python file containing the definition can be uploaded
    to a Space on the Hugging Face Hub so it can be shared with the community or saved/loaded
    locally as a Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets require additional preprocessing before passing them to an `Evaluator`.
    You can set a `data_preprocessor` for each `SubTask` which is applied via a `map`
    operation using the `datasets` library. Keyword arguments for the `Evaluator`
    can be passed down through the `args_for_task` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: To create a new `EvaluationSuite`, create a [new Space](https://huggingface.co/new-space)
    with a .py file which matches the name of the Space, add the below template to
    a Python file, and fill in the attributes for a new task.
  prefs: []
  type: TYPE_NORMAL
- en: The mandatory attributes for a new `SubTask` are `task_type` and `data`.
  prefs: []
  type: TYPE_NORMAL
- en: '`task_type` maps to the tasks currently supported by the Evaluator.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`data` can be an instantiated Hugging Face dataset object or the name of a
    dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`subset` and `split` can be used to define which name and split of the dataset
    should be used for evaluation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`args_for_task` should be a dictionary with kwargs to be passed to the Evaluator.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'An `EvaluationSuite` can be loaded by name from the Hugging Face Hub, or locally
    by providing a path, and run with the `run(model_or_pipeline)` method. The evaluation
    results are returned along with their task names and information about the time
    it took to obtain predictions through the pipeline. These can be easily displayed
    with a `pandas.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '| accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds
    | task_name |'
  prefs: []
  type: TYPE_TB
- en: '| --: | --: | --: | --: | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.740811 | 13.4987 | 0.0740811 | glue/sst2 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | 1.67552 | 5.9683 | 0.167552 | glue/rte |'
  prefs: []
  type: TYPE_TB
