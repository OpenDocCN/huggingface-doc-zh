- en: DeepSpeed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and
    scale for distributed training of large models with billions of parameters. At
    its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states
    (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes.
    This drastically reduces memory usage, allowing you to scale your training to
    billion parameter models. To unlock even more memory efficiency, ZeRO-Offload
    reduces GPU compute and memory by leveraging CPU resources during optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these features are supported in ðŸ¤— Accelerate, and you can use them with
    ðŸ¤— PEFT. This guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py).
    Youâ€™ll configure the script to train a large model for conditional generation
    with ZeRO-3 and ZeRO-Offload.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ’¡ To help you get started, check out our example training scripts for [causal
    language modeling](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)
    and [conditional generation](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py).
    You can adapt these scripts for your own applications or even use them out of
    the box if your task is similar to the one in the scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by running the following command to [create a DeepSpeed configuration
    file](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script)
    with ðŸ¤— Accelerate. The `--config_file` flag allows you to save the configuration
    file to a specific location, otherwise it is saved as a `default_config.yaml`
    file in the ðŸ¤— Accelerate cache.
  prefs: []
  type: TYPE_NORMAL
- en: The configuration file is used to set the default options when you launch the
    training script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Youâ€™ll be asked a few questions about your setup, and configure the following
    arguments. In this example, youâ€™ll use ZeRO-3 and ZeRO-Offload so make sure you
    pick those options.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: An example [configuration file](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml)
    might look like the following. The most important thing to notice is that `zero_stage`
    is set to `3`, and `offload_optimizer_device` and `offload_param_device` are set
    to the `cpu`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The important parts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s dive a little deeper into the script so you can see whatâ€™s going on, and
    understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Within the [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103)
    function, the script creates an [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    class to initialize all the necessary requirements for distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ’¡ Feel free to change the model and dataset inside the `main` function. If your
    dataset format is different from the one in the script, you may also need to write
    your own preprocessing function.
  prefs: []
  type: TYPE_NORMAL
- en: The script also creates a configuration for the ðŸ¤— PEFT method youâ€™re using,
    which in this case, is LoRA. The [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    specifies the task type and important parameters such as the dimension of the
    low-rank matrices, the matrices scaling factor, and the dropout probability of
    the LoRA layers. If you want to use a different ðŸ¤— PEFT method, make sure you replace
    `LoraConfig` with the appropriate [class](../package_reference/tuners).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Throughout the script, youâ€™ll see the [main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)
    and [wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)
    functions which help control and synchronize when processes are executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function takes a base model and the `peft_config` you prepared earlier to create
    a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass all the relevant training objects to ðŸ¤— Accelerateâ€™s [prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    which makes sure everything is ready for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next bit of code checks whether the DeepSpeed plugin is used in the `Accelerator`,
    and if the plugin exists, then the `Accelerator` uses ZeRO-3 as specified in the
    configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the training loop, the usual `loss.backward()` is replaced by ðŸ¤— Accelerateâ€™s
    [backward](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)
    which uses the correct `backward()` method based on your configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: That is all! The rest of the script handles the training loop, evaluation, and
    even pushes it to the Hub for you.
  prefs: []
  type: TYPE_NORMAL
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following command to launch the training script. Earlier, you saved
    the configuration file to `ds_zero3_cpu.yaml`, so youâ€™ll need to pass the path
    to the launcher with the `--config_file` argument like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Youâ€™ll see some output logs that track memory usage during training, and once
    itâ€™s completed, the script returns the accuracy and compares the predictions to
    the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
