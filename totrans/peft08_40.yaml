- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/lora](https://huggingface.co/docs/peft/package_reference/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/peft/package_reference/lora](https://huggingface.co/docs/peft/package_reference/lora)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Adaptation ([LoRA](https://huggingface.co/papers/2309.15223)) is a
    PEFT method that decomposes a large matrix into two smaller low-rank matrices
    in the attention layers. This drastically reduces the number of parameters that
    need to be fine-tuned.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（[LoRA](https://huggingface.co/papers/2309.15223)）是一种PEFT方法，它在注意力层中将一个大矩阵分解为两个较小的低秩矩阵。这极大地减少了需要进行微调的参数数量。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We propose a neural language modeling system based on low-rank adaptation
    (LoRA) for speech recognition output rescoring. Although pretrained language models
    (LMs) like BERT have shown superior performance in second-pass rescoring, the
    high computational cost of scaling up the pretraining stage and adapting the pretrained
    models to specific domains limit their practical use in rescoring. Here we present
    a method based on low-rank decomposition to train a rescoring BERT model and adapt
    it to new domains using only a fraction (0.08%) of the pretrained parameters.
    These inserted matrices are optimized through a discriminative training objective
    along with a correlation-based regularization loss. The proposed low-rank adaptation
    Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets
    with decreased training times by factors between 5.4 and 3.6.*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们提出了一种基于低秩适应（LoRA）的神经语言建模系统，用于语音识别输出重评分。尽管像BERT这样的预训练语言模型（LMs）在第二遍重评分中表现出优越性能，但扩大预训练阶段的计算成本以及将预训练模型调整到特定领域的适应性限制了它们在重评分中的实际应用。在这里，我们提出了一种基于低秩分解的方法，用于训练一个重评分BERT模型，并且仅使用预训练参数的一小部分（0.08%）。这些插入的矩阵通过具有基于相关性的正则化损失的判别式训练目标进行优化。所提出的低秩适应Rescore-BERT（LoRB）架构在LibriSpeech和内部数据集上进行了评估，训练时间减少了5.4到3.6倍。*'
- en: LoraConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoraConfig
- en: '### `class peft.LoraConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.LoraConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/config.py#L43)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/config.py#L43)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`r` (`int`) — Lora attention dimension (the “rank”).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` (`int`) — Lora注意力维度（“秩”）。'
- en: '`target_modules` (`Optional[Union[List[str], str]]`) — The names of the modules
    to apply the adapter to. If this is specified, only the modules with the specified
    names will be replaced. When passing a string, a regex match will be performed.
    When passing a list of strings, either an exact match will be performed or it
    is checked if the name of the module ends with any of the passed strings. If this
    is specified as ‘all-linear’, then all linear/Conv1D modules are chosen, excluding
    the output layer. If this is not specified, modules will be chosen according to
    the model architecture. If the architecture is not known, an error will be raised
    — in this case, you should specify the target modules manually.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个，那么只有指定名称的模块将被替换。当传递一个字符串时，将执行正则表达式匹配。当传递一个字符串列表时，要么执行精确匹配，要么检查模块的名称是否以任何传递的字符串结尾。如果指定为''all-linear''，则选择所有线性/Conv1D模块，不包括输出层。如果未指定，则根据模型架构选择模块。如果不知道架构，则会引发错误
    — 在这种情况下，您应该手动指定目标模块。'
- en: '`lora_alpha` (`int`) — The alpha parameter for Lora scaling.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_alpha` (`int`) — Lora缩放的alpha参数。'
- en: '`lora_dropout` (`float`) — The dropout probability for Lora layers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_dropout` (`float`) — Lora层的dropout概率。'
- en: '`fan_in_fan_out` (`bool`) — Set this to True if the layer to replace stores
    weight like (fan_in, fan_out). For example, gpt-2 uses `Conv1D` which stores weights
    like (fan_in, fan_out) and hence this should be set to `True`.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fan_in_fan_out` (`bool`) — 如果要替换的层存储类似于(fan_in, fan_out)的权重，请将此设置为True。例如，gpt-2使用存储类似于(fan_in,
    fan_out)的权重的`Conv1D`，因此应将其设置为`True`。'
- en: '`bias` (`str`) — Bias type for LoRA. Can be ‘none’, ‘all’ or ‘lora_only’. If
    ‘all’ or ‘lora_only’, the corresponding biases will be updated during training.
    Be aware that this means that, even when disabling the adapters, the model will
    not produce the same output as the base model would have without adaptation.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`str`) — LoRA的偏置类型。可以是''none''、''all''或''lora_only''。如果是''all''或''lora_only''，则相应的偏置将在训练过程中更新。请注意，即使禁用适配器，模型也不会产生与没有适应的基础模型相同的输出。'
- en: '`use_rslora` (`bool`) — When set to True, uses [Rank-Stabilized LoRA](https://doi.org/10.48550/arXiv.2312.03732)
    which sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was
    proven to work better. Otherwise, it will use the original default value of `lora_alpha/r`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_rslora` (`bool`) — 当设置为True时，使用[Rank-Stabilized LoRA](https://doi.org/10.48550/arXiv.2312.03732)，它将适配器缩放因子设置为`lora_alpha/math.sqrt(r)`，因为已经证明效果更好。否则，将使用`lora_alpha/r`的原始默认值。'
- en: '`modules_to_save` (`List[str]`) — List of modules apart from adapter layers
    to be set as trainable and saved in the final checkpoint.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_to_save` (`List[str]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。'
- en: '`init_lora_weights` (`bool` | `Literal["gaussian", "loftq"]`) — How to initialize
    the weights of the adapter layers. Passing True (default) results in the default
    initialization from the reference implementation from Microsoft. Passing ‘gaussian’
    results in Gaussian initialization scaled by the LoRA rank for linear and layers.
    Setting the initialization to False leads to completely random initialization
    and is discouraged. Pass `''loftq''` to use LoftQ initialization.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_lora_weights` (`bool` | `Literal["gaussian", "loftq"]`) — 如何初始化适配器层的权重。传递True（默认值）会导致从Microsoft的参考实现中进行默认初始化。传递''gaussian''会导致按线性和层的LoRA秩缩放的高斯初始化。将初始化设置为False会导致完全随机初始化，不建议使用。传递''loftq''以使用LoftQ初始化。'
- en: '`layers_to_transform` (`Union[List[int], int]`) — The layer indices to transform.
    If a list of ints is passed, it will apply the adapter to the layer indices that
    are specified in this list. If a single integer is passed, it will apply the transformations
    on the layer at this index.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递了一个整数列表，它将应用于在此列表中指定的层索引上的适配器。如果传递了一个单个整数，它将在该索引处的层上应用转换。'
- en: '`layers_pattern` (`str`) — The layer pattern name, used only if `layers_to_transform`
    is different from `None`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_pattern` (`str`) — 层模式名称，仅在`layers_to_transform`与`None`不同的情况下使用。'
- en: '`rank_pattern` (`dict`) — The mapping from layer names or regexp expression
    to ranks which are different from the default rank specified by `r`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank_pattern` (`dict`) — 从层名称或正则表达式到与`r`指定的默认rank不同的rank的映射。'
- en: '`alpha_pattern` (`dict`) — The mapping from layer names or regexp expression
    to alphas which are different from the default alpha specified by `lora_alpha`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha_pattern` (`dict`) — 从层名称或正则表达式到与`lora_alpha`指定的默认alpha不同的alpha的映射。'
- en: '`megatron_config` (`Optional[dict]`) — The TransformerConfig arguments for
    Megatron. It is used to create LoRA’s parallel linear layer. You can get it like
    this, `core_transformer_config_from_args(get_args())`, these two functions being
    from Megatron. The arguments will be used to initialize the TransformerConfig
    of Megatron. You need to specify this parameter when you want to apply LoRA to
    the ColumnParallelLinear and RowParallelLinear layers of megatron.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`megatron_config` (`Optional[dict]`) — 用于Megatron的TransformerConfig参数。用于创建LoRA的并行线性层。您可以像这样获取它，`core_transformer_config_from_args(get_args())`，这两个函数来自Megatron。这些参数将用于初始化Megatron的TransformerConfig。当您想要将LoRA应用于megatron的ColumnParallelLinear和RowParallelLinear层时，需要指定此参数。'
- en: '`megatron_core` (`Optional[str]`) — The core module from Megatron to use, defaults
    to `"megatron.core"`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`megatron_core` (`Optional[str]`) — 要使用的Megatron核心模块，默认为`"megatron.core"`。'
- en: '`loftq_config` (`Optional[LoftQConfig]`) — The configuration of LoftQ. If this
    is not None, then LoftQ will be used to quantize the backbone weights and initialize
    Lora layers. Also pass `init_lora_weights=''loftq''`. Note that you should not
    pass a quantized model in this case, as LoftQ will quantize the model itself.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loftq_config` (`Optional[LoftQConfig]`) — LoftQ的配置。如果不为None，则LoftQ将用于量化骨干权重并初始化Lora层。还传递`init_lora_weights=''loftq''`。请注意，在这种情况下，不应传递量化模型，因为LoftQ将量化模型本身。'
- en: This is the configuration class to store the configuration of a [LoraModel](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[LoraModel](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel)的配置。
- en: LoraModel
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoraModel
- en: '### `class peft.LoraModel`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.LoraModel`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L47)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L47)'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — The model to be adapted.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) — 要适配的模型。'
- en: '`config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig))
    — The configuration of the Lora model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig))
    — Lora模型的配置。'
- en: '`adapter_name` (`str`) — The name of the adapter, defaults to `"default"`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`) — 适配器的名称，默认为`"default"`。'
- en: Returns
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.nn.Module`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The Lora model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Lora模型。
- en: Creates Low Rank Adapter (LoRA) model from a pretrained transformers model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的transformers模型创建低秩适配器（LoRA）模型。
- en: The method is described in detail in [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)中有详细描述。
- en: 'Example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Attributes**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**：'
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — The model to be adapted.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — 要适配的模型。'
- en: '`peft_config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)):
    The configuration of the Lora model.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig))：Lora模型的配置。'
- en: '#### `add_weighted_adapter`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_weighted_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L369)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L369)'
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapters` (`list`) — List of adapter names to be merged.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapters` (`list`) — 要合并的适配器名称列表。'
- en: '`weights` (`list`) — List of weights for each adapter.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights` (`list`) — 每个适配器的权重列表。'
- en: '`adapter_name` (`str`) — Name of the new adapter.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`) — 新适配器的名称。'
- en: '`combination_type` (`str`) — The merging type can be one of [`svd`, `linear`,
    `cat`, `ties`, `ties_svd`, `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`].
    When using the `cat` combination_type, the rank of the resulting adapter is equal
    to the sum of all adapters ranks (the mixed adapter may be too big and result
    in OOM errors).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combination_type` (`str`) — 合并类型可以是[`svd`, `linear`, `cat`, `ties`, `ties_svd`,
    `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]之一。当使用`cat`组合类型时，结果适配器的秩等于所有适配器秩的总和（混合适配器可能太大，导致OOM错误）。'
- en: '`svd_rank` (`int`, *optional*) — Rank of output adapter for svd. If None provided,
    will use max rank of merging adapters.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_rank` (`int`, *可选*) — svd的输出适配器的秩。如果未提供，则将使用合并适配器的最大秩。'
- en: '`svd_clamp` (`float`, *optional*) — A quantile threshold for clamping SVD decomposition
    output. If None is provided, do not perform clamping. Defaults to None.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_clamp` (`float`, *可选*) — 用于夹紧SVD分解输出的分位阈值。如果未提供，则不执行夹紧。默认为None。'
- en: '`svd_full_matrices` (`bool`, *optional*) — Controls whether to compute the
    full or reduced SVD, and consequently, the shape of the returned tensors U and
    Vh. Defaults to True.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_full_matrices` (`bool`, *可选*) — 控制是计算完整的还是简化的SVD，从而控制返回的张量U和Vh的形状。默认为True。'
- en: '`svd_driver` (`str`, *optional*) — Name of the cuSOLVER method to be used.
    This keyword argument only works when merging on CUDA. Can be one of [None, `gesvd`,
    `gesvdj`, `gesvda`]. For more info please refer to `torch.linalg.svd` documentation.
    Defaults to None.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_driver`（`str`，*可选*）- 要使用的cuSOLVER方法的名称。此关键字参数仅在CUDA上合并时有效。可以是[None, `gesvd`,
    `gesvdj`, `gesvda`]之一。有关更多信息，请参考`torch.linalg.svd`文档。默认为None。'
- en: '`density` (`float`, *optional*) — Value between 0 and 1\. 0 means all values
    are pruned and 1 means no values are pruned. Should be used with [`ties`, `ties_svd`,
    `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`density`（`float`，*可选*）- 介于0和1之间的值。0表示所有值都被修剪，1表示没有值被修剪。应与[`ties`, `ties_svd`,
    `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]一起使用'
- en: '`majority_sign_method` (`str`) — The method, should be one of [“total”, “frequency”],
    to use to get the magnitude of the sign values. Should be used with [`ties`, `ties_svd`,
    `dare_ties`, `dare_ties_svd`]'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`majority_sign_method`（`str`）- 用于获取符号值大小的方法，应为[“total”, “frequency”]之一。应与[`ties`,
    `ties_svd`, `dare_ties`, `dare_ties_svd`]一起使用'
- en: This method adds a new adapter by merging the given adapters with the given
    weights.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过将给定的适配器与给定的权重合并来添加新的适配器。
- en: When using the `cat` combination_type you should be aware that rank of the resulting
    adapter will be equal to the sum of all adapters ranks. So it’s possible that
    the mixed adapter may become too big and result in OOM errors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`cat`组合类型时，您应该意识到结果适配器的等级将等于所有适配器等级的总和。因此，混合适配器可能会变得过大，导致OOM错误。
- en: '#### `delete_adapter`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L640)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L640)'
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_name` (str) — Name of the adapter to be deleted.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（str）- 要删除的适配器的名称。'
- en: Deletes an existing adapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 删除现有的适配器。
- en: '#### `disable_adapter_layers`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_adapter_layers`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L292)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L292)'
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Disable all adapters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用所有适配器。
- en: When disabling all adapters, the model output corresponds to the output of the
    base model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当禁用所有适配器时，模型输出对应于基础模型的输出。
- en: '#### `enable_adapter_layers`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_adapter_layers`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L285)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L285)'
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable all adapters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 启用所有适配器。
- en: Call this if you have previously disabled all adapters and want to re-enable
    them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前已禁用了所有适配器并希望重新启用它们，请调用此方法。
- en: '#### `merge_and_unload`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `merge_and_unload`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L662)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L662)'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`progressbar` (`bool`) — whether to show a progressbar indicating the unload
    and merge process'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`progressbar`（`bool`）- 是否显示指示卸载和合并过程的进度条'
- en: '`safe_merge` (`bool`) — whether to activate the safe merging check to check
    if there is any potential Nan in the adapter weights'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_merge`（`bool`）- 是否激活安全合并检查以检查适配器权重中是否存在潜在的NaN'
- en: '`adapter_names` (`List[str]`, *optional*) — The list of adapter names that
    should be merged. If None, all active adapters will be merged. Defaults to `None`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`（`List[str]`，*可选*）- 应合并的适配器名称列表。如果为None，则将合并所有活动适配器。默认为`None`。'
- en: This method merges the LoRa layers into the base model. This is needed if someone
    wants to use the base model as a standalone model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将LoRa层合并到基础模型中。如果有人想要将基础模型用作独立模型，则需要这样做。
- en: 'Example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#### `set_adapter`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L307)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L307)'
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_name` (`str` or `list[str]`) — Name of the adapter(s) to be activated.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（`str`或`list[str]`）- 要激活的适配器的名称。'
- en: Set the active adapter(s).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 设置活动适配器。
- en: Additionally, this function will set the specified adapters to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此函数将指定的适配器设置为可训练（即，requires_grad=True）。如果不需要此功能，请使用以下代码。
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `unload`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unload`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L694)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L694)'
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Gets back the base model by removing all the lora modules without merging. This
    gives back the original base model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过删除所有lora模块而获得基础模型。这将还原原始基础模型。
