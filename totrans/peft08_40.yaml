- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/lora](https://huggingface.co/docs/peft/package_reference/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Adaptation ([LoRA](https://huggingface.co/papers/2309.15223)) is a
    PEFT method that decomposes a large matrix into two smaller low-rank matrices
    in the attention layers. This drastically reduces the number of parameters that
    need to be fine-tuned.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '*We propose a neural language modeling system based on low-rank adaptation
    (LoRA) for speech recognition output rescoring. Although pretrained language models
    (LMs) like BERT have shown superior performance in second-pass rescoring, the
    high computational cost of scaling up the pretraining stage and adapting the pretrained
    models to specific domains limit their practical use in rescoring. Here we present
    a method based on low-rank decomposition to train a rescoring BERT model and adapt
    it to new domains using only a fraction (0.08%) of the pretrained parameters.
    These inserted matrices are optimized through a discriminative training objective
    along with a correlation-based regularization loss. The proposed low-rank adaptation
    Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets
    with decreased training times by factors between 5.4 and 3.6.*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: LoraConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.LoraConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/config.py#L43)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '`r` (`int`) — Lora attention dimension (the “rank”).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_modules` (`Optional[Union[List[str], str]]`) — The names of the modules
    to apply the adapter to. If this is specified, only the modules with the specified
    names will be replaced. When passing a string, a regex match will be performed.
    When passing a list of strings, either an exact match will be performed or it
    is checked if the name of the module ends with any of the passed strings. If this
    is specified as ‘all-linear’, then all linear/Conv1D modules are chosen, excluding
    the output layer. If this is not specified, modules will be chosen according to
    the model architecture. If the architecture is not known, an error will be raised
    — in this case, you should specify the target modules manually.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_alpha` (`int`) — The alpha parameter for Lora scaling.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_dropout` (`float`) — The dropout probability for Lora layers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fan_in_fan_out` (`bool`) — Set this to True if the layer to replace stores
    weight like (fan_in, fan_out). For example, gpt-2 uses `Conv1D` which stores weights
    like (fan_in, fan_out) and hence this should be set to `True`.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`str`) — Bias type for LoRA. Can be ‘none’, ‘all’ or ‘lora_only’. If
    ‘all’ or ‘lora_only’, the corresponding biases will be updated during training.
    Be aware that this means that, even when disabling the adapters, the model will
    not produce the same output as the base model would have without adaptation.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_rslora` (`bool`) — When set to True, uses [Rank-Stabilized LoRA](https://doi.org/10.48550/arXiv.2312.03732)
    which sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was
    proven to work better. Otherwise, it will use the original default value of `lora_alpha/r`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_save` (`List[str]`) — List of modules apart from adapter layers
    to be set as trainable and saved in the final checkpoint.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_lora_weights` (`bool` | `Literal["gaussian", "loftq"]`) — How to initialize
    the weights of the adapter layers. Passing True (default) results in the default
    initialization from the reference implementation from Microsoft. Passing ‘gaussian’
    results in Gaussian initialization scaled by the LoRA rank for linear and layers.
    Setting the initialization to False leads to completely random initialization
    and is discouraged. Pass `''loftq''` to use LoftQ initialization.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_to_transform` (`Union[List[int], int]`) — The layer indices to transform.
    If a list of ints is passed, it will apply the adapter to the layer indices that
    are specified in this list. If a single integer is passed, it will apply the transformations
    on the layer at this index.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_pattern` (`str`) — The layer pattern name, used only if `layers_to_transform`
    is different from `None`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_pattern` (`dict`) — The mapping from layer names or regexp expression
    to ranks which are different from the default rank specified by `r`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha_pattern` (`dict`) — The mapping from layer names or regexp expression
    to alphas which are different from the default alpha specified by `lora_alpha`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`megatron_config` (`Optional[dict]`) — The TransformerConfig arguments for
    Megatron. It is used to create LoRA’s parallel linear layer. You can get it like
    this, `core_transformer_config_from_args(get_args())`, these two functions being
    from Megatron. The arguments will be used to initialize the TransformerConfig
    of Megatron. You need to specify this parameter when you want to apply LoRA to
    the ColumnParallelLinear and RowParallelLinear layers of megatron.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`megatron_core` (`Optional[str]`) — The core module from Megatron to use, defaults
    to `"megatron.core"`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loftq_config` (`Optional[LoftQConfig]`) — The configuration of LoftQ. If this
    is not None, then LoftQ will be used to quantize the backbone weights and initialize
    Lora layers. Also pass `init_lora_weights=''loftq''`. Note that you should not
    pass a quantized model in this case, as LoftQ will quantize the model itself.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [LoraModel](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: LoraModel
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.LoraModel`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L47)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to be adapted.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig))
    — The configuration of the Lora model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter, defaults to `"default"`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Module`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The Lora model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Creates Low Rank Adapter (LoRA) model from a pretrained transformers model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The method is described in detail in [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Attributes**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — The model to be adapted.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)):
    The configuration of the Lora model.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `add_weighted_adapter`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L369)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '`adapters` (`list`) — List of adapter names to be merged.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weights` (`list`) — List of weights for each adapter.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — Name of the new adapter.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combination_type` (`str`) — The merging type can be one of [`svd`, `linear`,
    `cat`, `ties`, `ties_svd`, `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`].
    When using the `cat` combination_type, the rank of the resulting adapter is equal
    to the sum of all adapters ranks (the mixed adapter may be too big and result
    in OOM errors).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svd_rank` (`int`, *optional*) — Rank of output adapter for svd. If None provided,
    will use max rank of merging adapters.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svd_clamp` (`float`, *optional*) — A quantile threshold for clamping SVD decomposition
    output. If None is provided, do not perform clamping. Defaults to None.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svd_full_matrices` (`bool`, *optional*) — Controls whether to compute the
    full or reduced SVD, and consequently, the shape of the returned tensors U and
    Vh. Defaults to True.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svd_driver` (`str`, *optional*) — Name of the cuSOLVER method to be used.
    This keyword argument only works when merging on CUDA. Can be one of [None, `gesvd`,
    `gesvdj`, `gesvda`]. For more info please refer to `torch.linalg.svd` documentation.
    Defaults to None.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svd_driver`（`str`，*可选*）- 要使用的cuSOLVER方法的名称。此关键字参数仅在CUDA上合并时有效。可以是[None, `gesvd`,
    `gesvdj`, `gesvda`]之一。有关更多信息，请参考`torch.linalg.svd`文档。默认为None。'
- en: '`density` (`float`, *optional*) — Value between 0 and 1\. 0 means all values
    are pruned and 1 means no values are pruned. Should be used with [`ties`, `ties_svd`,
    `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`density`（`float`，*可选*）- 介于0和1之间的值。0表示所有值都被修剪，1表示没有值被修剪。应与[`ties`, `ties_svd`,
    `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]一起使用'
- en: '`majority_sign_method` (`str`) — The method, should be one of [“total”, “frequency”],
    to use to get the magnitude of the sign values. Should be used with [`ties`, `ties_svd`,
    `dare_ties`, `dare_ties_svd`]'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`majority_sign_method`（`str`）- 用于获取符号值大小的方法，应为[“total”, “frequency”]之一。应与[`ties`,
    `ties_svd`, `dare_ties`, `dare_ties_svd`]一起使用'
- en: This method adds a new adapter by merging the given adapters with the given
    weights.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过将给定的适配器与给定的权重合并来添加新的适配器。
- en: When using the `cat` combination_type you should be aware that rank of the resulting
    adapter will be equal to the sum of all adapters ranks. So it’s possible that
    the mixed adapter may become too big and result in OOM errors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`cat`组合类型时，您应该意识到结果适配器的等级将等于所有适配器等级的总和。因此，混合适配器可能会变得过大，导致OOM错误。
- en: '#### `delete_adapter`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L640)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L640)'
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_name` (str) — Name of the adapter to be deleted.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（str）- 要删除的适配器的名称。'
- en: Deletes an existing adapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 删除现有的适配器。
- en: '#### `disable_adapter_layers`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_adapter_layers`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L292)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L292)'
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Disable all adapters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用所有适配器。
- en: When disabling all adapters, the model output corresponds to the output of the
    base model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当禁用所有适配器时，模型输出对应于基础模型的输出。
- en: '#### `enable_adapter_layers`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_adapter_layers`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L285)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L285)'
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable all adapters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 启用所有适配器。
- en: Call this if you have previously disabled all adapters and want to re-enable
    them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前已禁用了所有适配器并希望重新启用它们，请调用此方法。
- en: '#### `merge_and_unload`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `merge_and_unload`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L662)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L662)'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`progressbar` (`bool`) — whether to show a progressbar indicating the unload
    and merge process'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`progressbar`（`bool`）- 是否显示指示卸载和合并过程的进度条'
- en: '`safe_merge` (`bool`) — whether to activate the safe merging check to check
    if there is any potential Nan in the adapter weights'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_merge`（`bool`）- 是否激活安全合并检查以检查适配器权重中是否存在潜在的NaN'
- en: '`adapter_names` (`List[str]`, *optional*) — The list of adapter names that
    should be merged. If None, all active adapters will be merged. Defaults to `None`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`（`List[str]`，*可选*）- 应合并的适配器名称列表。如果为None，则将合并所有活动适配器。默认为`None`。'
- en: This method merges the LoRa layers into the base model. This is needed if someone
    wants to use the base model as a standalone model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将LoRa层合并到基础模型中。如果有人想要将基础模型用作独立模型，则需要这样做。
- en: 'Example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#### `set_adapter`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L307)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L307)'
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_name` (`str` or `list[str]`) — Name of the adapter(s) to be activated.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（`str`或`list[str]`）- 要激活的适配器的名称。'
- en: Set the active adapter(s).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 设置活动适配器。
- en: Additionally, this function will set the specified adapters to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此函数将指定的适配器设置为可训练（即，requires_grad=True）。如果不需要此功能，请使用以下代码。
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `unload`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unload`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L694)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L694)'
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Gets back the base model by removing all the lora modules without merging. This
    gives back the original base model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过删除所有lora模块而获得基础模型。这将还原原始基础模型。
