# PoolFormer

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/poolformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/poolformer)

## 概述

PoolFormer模型是由Sea AI Labs在[MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418)中提出的。该工作的目标不是设计复杂的令牌混合器来实现SOTA性能，而是展示变压器模型的能力主要源自通用架构MetaFormer。

论文摘要如下：

*变压器在计算机视觉任务中展现出巨大潜力。人们普遍认为它们基于注意力的令牌混合器模块对其能力做出了最大贡献。然而，最近的研究表明，变压器中基于注意力的模块可以被空间MLP替代，结果模型仍然表现出色。基于这一观察，我们假设变压器的通用架构，而不是特定的令牌混合器模块，对模型的性能更为重要。为了验证这一点，我们故意将变压器中的注意力模块替换为一个非常简单的空间池化运算符，仅进行最基本的令牌混合。令人惊讶的是，我们观察到衍生模型PoolFormer在多个计算机视觉任务上取得了竞争性能。例如，在ImageNet-1K上，PoolFormer实现了82.1%的top-1准确率，超过了经过良好调整的视觉变压器/类似MLP基线DeiT-B/ResMLP-B24的0.3%/1.1%准确率，参数减少了35%/52%，MACs减少了48%/60%。PoolFormer的有效性验证了我们的假设，并促使我们提出“MetaFormer”概念，这是从变压器中抽象出来的通用架构，而不指定令牌混合器。基于广泛的实验，我们认为MetaFormer是实现最近变压器和类似MLP模型在视觉任务上取得优越结果的关键因素。这项工作呼吁未来更多的研究致力于改进MetaFormer，而不是专注于令牌混合器模块。此外，我们提出的PoolFormer可以作为未来MetaFormer架构设计的起点基线。*

下图展示了PoolFormer的架构。摘自[原始论文](https://arxiv.org/abs/2111.11418)。

![](../Images/3034170d23213533d28047793aeaaeed.png)

此模型由[heytanay](https://huggingface.co/heytanay)贡献。原始代码可以在[这里](https://github.com/sail-sg/poolformer)找到。

## 使用提示

+   PoolFormer具有分层架构，其中存在一个简单的平均池化层，而不是注意力。模型的所有检查点都可以在[hub](https://huggingface.co/models?other=poolformer)上找到。

+   可以使用[PoolFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerImageProcessor)来为模型准备图像。

+   与大多数模型一样，PoolFormer有不同的大小，详情可以在下表中找到。

| **模型变体** | **深度** | **隐藏大小** | **参数（百万）** | **ImageNet-1k Top 1** |
| :-: | --- | --- | :-: | :-: |
| s12 | [2, 2, 6, 2] | [64, 128, 320, 512] | 12 | 77.2 |
| s24 | [4, 4, 12, 4] | [64, 128, 320, 512] | 21 | 80.3 |
| s36 | [6, 6, 18, 6] | [64, 128, 320, 512] | 31 | 81.4 |
| m36 | [6, 6, 18, 6] | [96, 192, 384, 768] | 56 | 82.1 |
| m48 | [8, 8, 24, 8] | [96, 192, 384, 768] | 73 | 82.5 |

## 资源

以下是官方Hugging Face和社区资源（由🌎表示），可帮助您开始使用PoolFormer。

图像分类

+   [PoolFormerForImageClassification](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。

+   另请参阅：[图像分类任务指南](../tasks/image_classification)

如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将对其进行审查！资源应该展示一些新的东西，而不是重复现有资源。

## PoolFormerConfig

### `class transformers.PoolFormerConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/configuration_poolformer.py#L34)

```py
( num_channels = 3 patch_size = 16 stride = 16 pool_size = 3 mlp_ratio = 4.0 depths = [2, 2, 6, 2] hidden_sizes = [64, 128, 320, 512] patch_sizes = [7, 3, 3, 3] strides = [4, 2, 2, 2] padding = [2, 1, 1, 1] num_encoder_blocks = 4 drop_path_rate = 0.0 hidden_act = 'gelu' use_layer_scale = True layer_scale_init_value = 1e-05 initializer_range = 0.02 **kwargs )
```

参数

+   `num_channels` (`int`, *可选*, 默认为3) — 输入图像中的通道数。

+   `patch_size` (`int`, *可选*, 默认为16) — 输入补丁的大小。

+   `stride` (`int`, *可选*, 默认为16) — 输入补丁的步幅。

+   `pool_size` (`int`, *可选*, 默认为3) — 池化窗口的大小。

+   `mlp_ratio` (`float`, *可选*, 默认为4.0) — MLP输出通道数与输入通道数的比率。

+   `depths` (`list`, *可选*, 默认为`[2, 2, 6, 2]`) — 每个编码器块的深度。

+   `hidden_sizes` (`list`, *可选*, 默认为`[64, 128, 320, 512]`) — 每个编码器块的隐藏大小。

+   `patch_sizes` (`list`, *可选*, 默认为`[7, 3, 3, 3]`) — 每个编码器块的输入补丁的大小。

+   `strides` (`list`, *可选*, 默认为`[4, 2, 2, 2]`) — 每个编码器块的输入补丁的步幅。

+   `padding` (`list`, *可选*, 默认为`[2, 1, 1, 1]`) — 每个编码器块的输入补丁的填充。

+   `num_encoder_blocks` (`int`, *可选*, 默认为4) — 编码器块的数量。

+   `drop_path_rate` (`float`, *可选*, 默认为0.0) — 丢弃层的丢弃率。

+   `hidden_act` (`str`, *可选*, 默认为`"gelu"`) — 隐藏层的激活函数。

+   `use_layer_scale` (`bool`, *可选*, 默认为`True`) — 是否使用层比例。

+   `layer_scale_init_value` (`float`, *可选*, 默认为1e-05) — 层比例的初始值。

+   `initializer_range` (`float`, *可选*, 默认为0.02) — 权重的初始化范围。

这是用于存储[PoolFormerModel](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerModel)配置的类。根据指定的参数实例化一个PoolFormer模型，定义模型架构。使用默认值实例化配置将产生类似于PoolFormer [sail/poolformer_s12](https://huggingface.co/sail/poolformer_s12)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import PoolFormerConfig, PoolFormerModel

>>> # Initializing a PoolFormer sail/poolformer_s12 style configuration
>>> configuration = PoolFormerConfig()

>>> # Initializing a model (with random weights) from the sail/poolformer_s12 style configuration
>>> model = PoolFormerModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## PoolFormerFeatureExtractor

### `class transformers.PoolFormerFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/feature_extraction_poolformer.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理一张图片或一批图片。

## PoolFormerImageProcessor

### `class transformers.PoolFormerImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/image_processing_poolformer.py#L49)

```py
( do_resize: bool = True size: Dict = None crop_pct: int = 0.9 resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None rescale_factor: Union = 0.00392156862745098 do_rescale: bool = True do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

参数

+   `do_resize` (`bool`，*可选*，默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的 `size`。可以被 `preprocess` 方法中的 `do_resize` 覆盖。

+   `size` (`Dict[str, int]` *可选*，默认为 `{"shortest_edge" -- 224}`)：调整大小后的图像大小。可以被 `preprocess` 方法中的 `size` 覆盖。如果未设置 crop_pct：

    +   size 为 `{"height": h, "width": w}`：将图像调整大小为 `(h, w)`。

    +   size 为 `{"shortest_edge": s}`：将图像的最短边调整大小为 s，同时保持纵横比。

    如果设置了 crop_pct：

    +   size 为 `{"height": h, "width": w}`：将图像调整大小为 `(int(floor(h/crop_pct)), int(floor(w/crop_pct)))`

    +   size 为 `{"height": c, "width": c}`：将图像的最短边调整大小为 `int(floor(c/crop_pct)`，同时保持纵横比。

    +   size 为 `{"shortest_edge": c}`：将图像的最短边调整大小为 `int(floor(c/crop_pct)`，同时保持纵横比。

+   `crop_pct` (`float`，*可选*，默认为 0.9) — 从中心裁剪图像的百分比。可以被 `preprocess` 方法中的 `crop_pct` 覆盖。

+   `resample` (`PILImageResampling`，*可选*，默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则要使用的重采样滤波器。可以被 `preprocess` 方法中的 `resample` 覆盖。

+   `do_center_crop` (`bool`，*可选*，默认为 `True`) — 是否对图像进行中心裁剪。如果输入尺寸沿任一边小于 `crop_size`，则图像将填充为 0，然后进行中心裁剪。可以被 `preprocess` 方法中的 `do_center_crop` 覆盖。

+   `crop_size` (`Dict[str, int]`，*可选*，默认为 `{"height" -- 224, "width": 224}`)：应用中心裁剪后的图像大小。仅在 `do_center_crop` 设置为 `True` 时有效。可以被 `preprocess` 方法中的 `crop_size` 参数覆盖。

+   `rescale_factor` (`int` 或 `float`，*可选*，默认为 `1/255`) — 如果重新缩放图像，则使用的比例因子。可以被 `preprocess` 方法中的 `rescale_factor` 参数覆盖。

+   `do_rescale` (`bool`，*可选*，默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以被 `preprocess` 方法中的 `do_rescale` 参数覆盖。

+   `do_normalize` (`bool`，*可选*，默认为 `True`) — 控制是否对图像进行标准化。可以被 `preprocess` 方法中的 `do_normalize` 参数覆盖。

+   `image_mean` (`float` 或 `List[float]`，*可选*，默认为 `IMAGENET_STANDARD_MEAN`) — 如果对图像进行标准化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。

+   `image_std` (`float` 或 `List[float]`，*可选*，默认为 `IMAGENET_STANDARD_STD`) — 如果对图像进行标准化，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

构建一个 PoolFormer 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/image_processing_poolformer.py#L211)

```py
( images: Union do_resize: bool = None size: Dict = None crop_pct: int = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望传入像素值范围为 0 到 255 的单个图像或图像批次。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`，*可选*，默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`，*可选*，默认为 `self.size`) — 调整大小后的图像大小。

+   `crop_pct` (`float`，*可选*，默认为 `self.crop_pct`) — 要裁剪的图像百分比。仅在 `do_resize` 设置为 `True` 时有效。

+   `resample` (`int`，*可选*，默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是枚举 `PILImageResampling` 中的一个。仅当 `do_resize` 设置为 `True` 时才有效。

+   `do_center_crop` (`bool`，*可选*，默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。

+   `crop_size` (`Dict[str, int]`，*可选*，默认为 `self.crop_size`) — 应用中心裁剪后的图像大小。

+   `do_rescale` (`bool`，*可选*，默认为 `self.do_rescale`) — 是否将图像值重新缩放在 [0 - 1] 之间。

+   `rescale_factor` (`float`，*可选*，默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则用于重新缩放图像的重新缩放因子。

+   `do_normalize` (`bool`，*可选*，默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`，*可选*，默认为 `self.image_mean`) — 图像均值。

+   `image_std` (`float` 或 `List[float]`，*可选*，默认为 `self.image_std`) — 图像标准差。

+   `return_tensors` (`str` 或 `TensorType`，*可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回一个类型为 `tf.Tensor` 的批次。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回一个类型为 `torch.Tensor` 的批次。

    +   `TensorType.NUMPY` 或 `'np'`：返回一个类型为 `np.ndarray` 的批次。

    +   `TensorType.JAX` 或 `'jax'`：返回一个类型为 `jax.numpy.ndarray` 的批次。

+   `data_format` (`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`：图像以 (num_channels, height, width) 格式。

    +   `ChannelDimension.LAST`：图像以 (height, width, num_channels) 格式。

+   `input_data_format` (`ChannelDimension` 或 `str`，*可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (num_channels, height, width) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以 (height, width, num_channels) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`：图像以 (height, width) 格式。

预处理图像或一批图像。

## PoolFormerModel

### `class transformers.PoolFormerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/modeling_poolformer.py#L304)

```py
( config )
```

参数

+   `config` ([PoolFormerConfig](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的 PoolFormer 模型变压器，输出原始隐藏状态，没有特定的头部。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/modeling_poolformer.py#L321)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 [PoolFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

返回

`transformers.modeling_outputs.BaseModelOutputWithNoAttention` 或 `tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.BaseModelOutputWithNoAttention`或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时）包括不同的元素，取决于配置（[PoolFormerConfig](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerConfig)）和输入。

+   `last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 模型最后一层的隐藏状态序列输出。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则一个用于嵌入输出，+ 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

[PoolFormerModel](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, PoolFormerModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("sail/poolformer_s12")
>>> model = PoolFormerModel.from_pretrained("sail/poolformer_s12")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 512, 7, 7]
```

## PoolFormerForImageClassification

### `class transformers.PoolFormerForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/modeling_poolformer.py#L369)

```py
( config )
```

参数

+   `config`（[PoolFormerConfig](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

具有图像分类头部的PoolFormer模型变压器

这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/poolformer/modeling_poolformer.py#L391)

```py
( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[PoolFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或者`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时）包括不同的元素，取决于配置（[PoolFormerConfig](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） — 分类（如果config.num_labels==1则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`） — 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。

[PoolFormerForImageClassification](/docs/transformers/v4.37.2/en/model_doc/poolformer#transformers.PoolFormerForImageClassification)的前向方法覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, PoolFormerForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("sail/poolformer_s12")
>>> model = PoolFormerForImageClassification.from_pretrained("sail/poolformer_s12")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```
