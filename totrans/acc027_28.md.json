["```py\npip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n```", "```py\n$ accelerate config\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nIn which compute environment are you running?\nThis machine\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nWhich type of machine are you using?\nNo distributed training\nDo you want to run your training on CPU only (even if a GPU / Apple Silicon device is available)? [yes/NO]:yes\nDo you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:yes\nDo you wish to optimize your script with torch dynamo?[yes/NO]:NO\nDo you want to use DeepSpeed? [yes/NO]: NO\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nDo you wish to use FP16 or BF16 (mixed precision)?\nbf16\n```", "```py\naccelerate launch my_script.py --args_to_my_script\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndistributed_type: 'NO'\ndowncast_bf16: 'no'\nipex_config:\n  ipex: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: true\n```", "```py\naccelerate launch examples/nlp_example.py\n```", "```py\n$ accelerate config\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nIn which compute environment are you running?\nThis machine\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nWhich type of machine are you using?\nmulti-CPU\nHow many different machines will you use (use more than 1 for multi-node training)? [1]: 4\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nWhat is the rank of this machine?\n0\nWhat is the IP address of the machine that will host the main process? 36.112.23.24\nWhat is the port you will use to communicate with the main process? 29500\nAre all the machines on the same local network? Answer `no` if nodes are on the cloud and/or on different network hosts [YES/no]: yes\nDo you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:yes\nDo you wish to optimize your script with torch dynamo?[yes/NO]:NO\nHow many CPU(s) should be used for distributed training? [1]:16\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nDo you wish to use FP16 or BF16 (mixed precision)?\nbf16\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndistributed_type: MULTI_CPU\ndowncast_bf16: 'no'\nipex_config:\n  ipex: true\nmachine_rank: 0\nmain_process_ip: 36.112.23.24\nmain_process_port: 29500\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 4\nnum_processes: 16\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: true\n```", "```py\n$ cat hostfile\nxxx.xxx.xxx.xxx #node0 ip\nxxx.xxx.xxx.xxx #node1 ip\nxxx.xxx.xxx.xxx #node2 ip\nxxx.xxx.xxx.xxx #node3 ip\n```", "```py\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\nexport CCL_ATL_TRANSPORT=ofi\nmpirun -f hostfile -n 16 -ppn 4 accelerate launch examples/nlp_example.py\n```"]