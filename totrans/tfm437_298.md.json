["```py\n( backbone_config: PretrainedConfig = None hidden_size: int = 384 batch_norm_eps: float = 1e-05 initializer_range: float = 0.02 convstream_hidden_sizes: List = [48, 96, 192] fusion_hidden_sizes: List = [256, 128, 64, 32] **kwargs )\n```", "```py\n>>> from transformers import VitMatteConfig, VitMatteForImageMatting\n\n>>> # Initializing a ViTMatte hustvl/vitmatte-small-composition-1k style configuration\n>>> configuration = VitMatteConfig()\n\n>>> # Initializing a model (with random weights) from the hustvl/vitmatte-small-composition-1k style configuration\n>>> model = VitMatteForImageMatting(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( )\n```", "```py\n( do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True size_divisibility: int = 32 **kwargs )\n```", "```py\n( images: Union trimaps: Union do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None size_divisibility: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import VitMatteImageProcessor, VitMatteForImageMatting\n>>> import torch\n>>> from PIL import Image\n>>> from huggingface_hub import hf_hub_download\n\n>>> processor = VitMatteImageProcessor.from_pretrained(\"hustvl/vitmatte-small-composition-1k\")\n>>> model = VitMatteForImageMatting.from_pretrained(\"hustvl/vitmatte-small-composition-1k\")\n\n>>> filepath = hf_hub_download(\n...     repo_id=\"hf-internal-testing/image-matting-fixtures\", filename=\"image.png\", repo_type=\"dataset\"\n... )\n>>> image = Image.open(filepath).convert(\"RGB\")\n>>> filepath = hf_hub_download(\n...     repo_id=\"hf-internal-testing/image-matting-fixtures\", filename=\"trimap.png\", repo_type=\"dataset\"\n... )\n>>> trimap = Image.open(filepath).convert(\"L\")\n\n>>> # prepare image + trimap for the model\n>>> inputs = processor(images=image, trimaps=trimap, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     alphas = model(**inputs).alphas\n>>> print(alphas.shape)\ntorch.Size([1, 1, 640, 960])\n```"]