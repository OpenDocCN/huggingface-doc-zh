# 已添加的令牌

> 原始文本：[https://huggingface.co/docs/tokenizers/api/added-tokens](https://huggingface.co/docs/tokenizers/api/added-tokens)

PythonRustNode

## AddedToken

### `class tokenizers.AddedToken`

```py
( content single_word = False lstrip = False rstrip = False normalized = True )
```

参数

+   `content`（`str`）—令牌的内容

+   `single_word`（`bool`，默认为`False`）—定义此令牌是否应该仅匹配单词。如果为`True`，此令牌将永远不会在单词内部匹配。例如，如果此选项为`False`，则令牌`ing`将在`tokenizing`上匹配，但如果为`True`，则不会。 ”*单词内部*”的概念由正则表达式中的单词边界模式定义（即，令牌应该以单词边界开头和结尾）。

+   `lstrip`（`bool`，默认为`False`）—定义此令牌是否应该剥离其左侧的所有潜在空格。如果为`True`，此令牌将贪婪地匹配其左侧的任何空格。例如，如果我们尝试使用`lstrip=True`匹配令牌`[MASK]`，在文本`"I saw a [MASK]"`中，我们将匹配`" [MASK]"`。（注意左侧的空格）。

+   `rstrip`（`bool`，默认为`False`）—定义此令牌是否应该剥离其右侧的所有潜在空格。如果为`True`，此令牌将贪婪地匹配其右侧的任何空格。它的工作方式与`lstrip`相同，但在右侧。

+   `normalized`（`bool`，默认为`True`与—meth:*~tokenizers.Tokenizer.add_tokens*和`False`与`add_special_tokens()`）：定义此令牌是否应该与输入文本的规范化版本匹配。例如，使用添加的令牌`"yesterday"`，并且一个负责将文本转换为小写的规范化器，该令牌可以从输入`"I saw a lion Yesterday"`中提取出来。

表示一个可以添加到[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)的令牌。它可以具有特殊选项，定义它应该如何行为。

属性 content

获取此`AddedToken`的内容

属性 lstrip

获取`lstrip`选项的值

属性 normalized

获取`normalized`选项的值

属性 rstrip

获取`rstrip`选项的值

属性 single_word

获取`single_word`选项的值
