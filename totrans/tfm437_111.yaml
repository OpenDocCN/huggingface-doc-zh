- en: Callbacks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›è°ƒå‡½æ•°
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks are objects that can customize the behavior of the training loop in
    the PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    (this feature is not yet implemented in TensorFlow) that can inspect the training
    loop state (for progress reporting, logging on TensorBoard or other ML platformsâ€¦)
    and take decisions (like early stopping).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›è°ƒå‡½æ•°æ˜¯å¯ä»¥è‡ªå®šä¹‰PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)è®­ç»ƒå¾ªç¯è¡Œä¸ºçš„å¯¹è±¡ï¼ˆæ­¤åŠŸèƒ½å°šæœªåœ¨TensorFlowä¸­å®ç°ï¼‰ï¼Œå¯ä»¥æ£€æŸ¥è®­ç»ƒå¾ªç¯çŠ¶æ€ï¼ˆç”¨äºè¿›åº¦æŠ¥å‘Šã€åœ¨TensorBoardæˆ–å…¶ä»–MLå¹³å°ä¸Šè®°å½•â€¦ï¼‰å¹¶åšå‡ºå†³ç­–ï¼ˆå¦‚æå‰åœæ­¢ï¼‰ã€‚
- en: Callbacks are â€œread onlyâ€ pieces of code, apart from the [TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)
    object they return, they cannot change anything in the training loop. For customizations
    that require changes in the training loop, you should subclass [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and override the methods you need (see [trainer](trainer) for examples).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å›è°ƒå‡½æ•°æ˜¯â€œåªè¯»â€ä»£ç ç‰‡æ®µï¼Œé™¤äº†å®ƒä»¬è¿”å›çš„[TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)å¯¹è±¡å¤–ï¼Œå®ƒä»¬ä¸èƒ½æ›´æ”¹è®­ç»ƒå¾ªç¯ä¸­çš„ä»»ä½•å†…å®¹ã€‚å¯¹äºéœ€è¦æ›´æ”¹è®­ç»ƒå¾ªç¯çš„è‡ªå®šä¹‰å†…å®¹ï¼Œæ‚¨åº”è¯¥å­ç±»åŒ–[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¹¶è¦†ç›–æ‚¨éœ€è¦çš„æ–¹æ³•ï¼ˆè¯·å‚é˜…[trainer](trainer)ä»¥è·å–ç¤ºä¾‹ï¼‰ã€‚
- en: By default, `TrainingArguments.report_to` is set to `"all"`, so a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will use the following callbacks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œ`TrainingArguments.report_to`è®¾ç½®ä¸º`"all"`ï¼Œå› æ­¤[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†ä½¿ç”¨ä»¥ä¸‹å›è°ƒå‡½æ•°ã€‚
- en: '[DefaultFlowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.DefaultFlowCallback)
    which handles the default behavior for logging, saving and evaluation.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DefaultFlowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.DefaultFlowCallback)å¤„ç†æ—¥å¿—è®°å½•ã€ä¿å­˜å’Œè¯„ä¼°çš„é»˜è®¤è¡Œä¸ºã€‚'
- en: '[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)
    or [ProgressCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.ProgressCallback)
    to display progress and print the logs (the first one is used if you deactivate
    tqdm through the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    otherwise itâ€™s the second one).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)æˆ–[ProgressCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.ProgressCallback)æ˜¾ç¤ºè¿›åº¦å¹¶æ‰“å°æ—¥å¿—ï¼ˆå¦‚æœé€šè¿‡[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)åœç”¨tqdmï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªï¼Œå¦åˆ™ä½¿ç”¨ç¬¬äºŒä¸ªï¼‰ã€‚
- en: '[TensorBoardCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.TensorBoardCallback)
    if tensorboard is accessible (either through PyTorch >= 1.4 or tensorboardX).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœtensorboardå¯è®¿é—®ï¼ˆé€šè¿‡PyTorch >= 1.4æˆ–tensorboardXï¼‰ï¼Œåˆ™ä½¿ç”¨[TensorBoardCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.TensorBoardCallback)ã€‚
- en: '[WandbCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.WandbCallback)
    if [wandb](https://www.wandb.com/) is installed.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[wandb](https://www.wandb.com/)ï¼Œåˆ™ä½¿ç”¨[WandbCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.WandbCallback)ã€‚
- en: '[CometCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CometCallback)
    if [comet_ml](https://www.comet.ml/site/) is installed.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[comet_ml](https://www.comet.ml/site/)ï¼Œåˆ™ä½¿ç”¨[CometCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CometCallback)ã€‚
- en: '[MLflowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.MLflowCallback)
    if [mlflow](https://www.mlflow.org/) is installed.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[mlflow](https://www.mlflow.org/)ï¼Œåˆ™ä½¿ç”¨[MLflowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.MLflowCallback)ã€‚
- en: '[NeptuneCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.NeptuneCallback)
    if [neptune](https://neptune.ai/) is installed.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[neptune](https://neptune.ai/)ï¼Œåˆ™ä½¿ç”¨[NeptuneCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.NeptuneCallback)ã€‚
- en: '[AzureMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.AzureMLCallback)
    if [azureml-sdk](https://pypi.org/project/azureml-sdk/) is installed.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[azureml-sdk](https://pypi.org/project/azureml-sdk/)ï¼Œåˆ™ä½¿ç”¨[AzureMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.AzureMLCallback)ã€‚
- en: '[CodeCarbonCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CodeCarbonCallback)
    if [codecarbon](https://pypi.org/project/codecarbon/) is installed.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[codecarbon](https://pypi.org/project/codecarbon/)ï¼Œåˆ™ä½¿ç”¨[CodeCarbonCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CodeCarbonCallback)ã€‚
- en: '[ClearMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.ClearMLCallback)
    if [clearml](https://github.com/allegroai/clearml) is installed.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[clearml](https://github.com/allegroai/clearml)ï¼Œåˆ™ä½¿ç”¨[ClearMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.ClearMLCallback)ã€‚
- en: '[DagsHubCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DagsHubCallback)
    if [dagshub](https://dagshub.com/) is installed.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[dagshub](https://dagshub.com/)ï¼Œåˆ™ä½¿ç”¨[DagsHubCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DagsHubCallback)ã€‚
- en: '[FlyteCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.FlyteCallback)
    if [flyte](https://flyte.org/) is installed.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[flyte](https://flyte.org/)ï¼Œåˆ™ä½¿ç”¨[FlyteCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.FlyteCallback)ã€‚
- en: '[DVCLiveCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DVCLiveCallback)
    if [dvclive](https://dvc.org/doc/dvclive) is installed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†[dvclive](https://dvc.org/doc/dvclive)ï¼Œåˆ™ä½¿ç”¨[DVCLiveCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DVCLiveCallback)ã€‚
- en: If a package is installed but you donâ€™t wish to use the accompanying integration,
    you can change `TrainingArguments.report_to` to a list of just those integrations
    you want to use (e.g. `["azure_ml", "wandb"]`).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†æŸä¸ªè½¯ä»¶åŒ…ï¼Œä½†æ‚¨ä¸å¸Œæœ›ä½¿ç”¨ç›¸åº”çš„é›†æˆï¼Œå¯ä»¥å°†`TrainingArguments.report_to`æ›´æ”¹ä¸ºæ‚¨æƒ³è¦ä½¿ç”¨çš„é›†æˆåˆ—è¡¨ï¼ˆä¾‹å¦‚`["azure_ml",
    "wandb"]`ï¼‰ã€‚
- en: The main class that implements callbacks is [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    It gets the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    used to instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    can access that Trainerâ€™s internal state via [TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState),
    and can take some actions on the training loop via [TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å›è°ƒçš„ä¸»è¦ç±»æ˜¯[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚å®ƒè·å–ç”¨äºå®ä¾‹åŒ–[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)çš„[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ï¼Œå¯ä»¥é€šè¿‡[TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState)è®¿é—®è¯¥Trainerçš„å†…éƒ¨çŠ¶æ€ï¼Œå¹¶å¯ä»¥é€šè¿‡[TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)å¯¹è®­ç»ƒå¾ªç¯é‡‡å–ä¸€äº›æ“ä½œã€‚
- en: Available Callbacks
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯ç”¨çš„å›è°ƒ
- en: 'Here is the list of the available [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    in the library:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åº“ä¸­å¯ç”¨çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)åˆ—è¡¨ï¼š
- en: '### `class transformers.integrations.CometCallback`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.CometCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L833)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L833)'
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [Comet ML](https://www.comet.ml/site/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ—¥å¿—å‘é€åˆ°[Comet ML](https://www.comet.ml/site/)çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: '#### `setup`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L844)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L844)'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Setup the optional Comet.ml integration.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®å¯é€‰çš„Comet.mlé›†æˆã€‚
- en: 'Environment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼š
- en: '`COMET_MODE` (`str`, *optional*, defaults to `ONLINE`): Whether to create an
    online, offline experiment or disable Comet logging. Can be `OFFLINE`, `ONLINE`,
    or `DISABLED`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_MODE` (`str`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`ONLINE`): æ˜¯å¦åˆ›å»ºåœ¨çº¿ã€ç¦»çº¿å®éªŒæˆ–ç¦ç”¨Cometæ—¥å¿—è®°å½•ã€‚å¯ä»¥æ˜¯`OFFLINE`ã€`ONLINE`æˆ–`DISABLED`ã€‚'
- en: '`COMET_PROJECT_NAME` (`str`, *optional*): Comet project name for experiments.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_PROJECT_NAME` (`str`, *å¯é€‰*): ç”¨äºå®éªŒçš„Cometé¡¹ç›®åç§°ã€‚'
- en: '`COMET_OFFLINE_DIRECTORY` (`str`, *optional*): Folder to use for saving offline
    experiments when `COMET_MODE` is `OFFLINE`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_OFFLINE_DIRECTORY` (`str`, *å¯é€‰*): åœ¨`COMET_MODE`ä¸º`OFFLINE`æ—¶ç”¨äºä¿å­˜ç¦»çº¿å®éªŒçš„æ–‡ä»¶å¤¹ã€‚'
- en: '`COMET_LOG_ASSETS` (`str`, *optional*, defaults to `TRUE`): Whether or not
    to log training assets (tf event logs, checkpoints, etc), to Comet. Can be `TRUE`,
    or `FALSE`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_LOG_ASSETS` (`str`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`TRUE`): æ˜¯å¦å°†è®­ç»ƒèµ„äº§ï¼ˆtfäº‹ä»¶æ—¥å¿—ã€æ£€æŸ¥ç‚¹ç­‰ï¼‰è®°å½•åˆ°Cometã€‚å¯ä»¥æ˜¯`TRUE`æˆ–`FALSE`ã€‚'
- en: For a number of configurable items in the environment, see [here](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ç¯å¢ƒä¸­å¯é…ç½®é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤å¤„](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables)ã€‚
- en: '### `class transformers.DefaultFlowCallback`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DefaultFlowCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L432)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L432)'
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that handles the default flow of the training loop for logs, evaluation and checkpoints.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†è®­ç»ƒå¾ªç¯çš„é»˜è®¤æµç¨‹ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€è¯„ä¼°å’Œæ£€æŸ¥ç‚¹çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: '### `class transformers.PrinterCallback`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PrinterCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L532)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L532)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A bare [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that just prints the logs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼Œåªæ‰“å°æ—¥å¿—ã€‚
- en: '### `class transformers.ProgressCallback`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProgressCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L482)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L482)'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that displays the progress of training or evaluation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ˜¾ç¤ºè®­ç»ƒæˆ–è¯„ä¼°è¿›åº¦çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: '### `class transformers.EarlyStoppingCallback`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.EarlyStoppingCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L543)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L543)'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`early_stopping_patience` (`int`) â€” Use with `metric_for_best_model` to stop
    training when the specified metric worsens for `early_stopping_patience` evaluation
    calls.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_patience` (`int`) â€” ä¸`metric_for_best_model`ä¸€èµ·ä½¿ç”¨ï¼Œå½“æŒ‡å®šçš„æŒ‡æ ‡åœ¨`early_stopping_patience`æ¬¡è¯„ä¼°è°ƒç”¨ä¸­æ¶åŒ–æ—¶åœæ­¢è®­ç»ƒã€‚'
- en: '`early_stopping_threshold(float,` *optional*) â€” Use with TrainingArguments
    `metric_for_best_model` and `early_stopping_patience` to denote how much the specified
    metric must improve to satisfy early stopping conditions. `'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_threshold(float,` *å¯é€‰*) â€” ä¸TrainingArgumentsçš„`metric_for_best_model`å’Œ`early_stopping_patience`ä¸€èµ·ä½¿ç”¨ï¼Œè¡¨ç¤ºæŒ‡å®šæŒ‡æ ‡å¿…é¡»æ”¹å–„å¤šå°‘æ‰èƒ½æ»¡è¶³æå‰åœæ­¢æ¡ä»¶ã€‚'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that handles early stopping.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¤„ç†æå‰åœæ­¢çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: This callback depends on [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    argument *load_best_model_at_end* functionality to set best_metric in [TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState).
    Note that if the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    argument *save_steps* differs from *eval_steps*, the early stopping will not occur
    until the next save step.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å›è°ƒå–å†³äº[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å‚æ•°*load_best_model_at_end*åŠŸèƒ½ï¼Œä»¥åœ¨[TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState)ä¸­è®¾ç½®best_metricã€‚è¯·æ³¨æ„ï¼Œå¦‚æœ[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å‚æ•°*save_steps*ä¸*eval_steps*ä¸åŒï¼Œåˆ™ç›´åˆ°ä¸‹ä¸€ä¸ªä¿å­˜æ­¥éª¤æ‰ä¼šå‘ç”Ÿæ—©åœã€‚
- en: '### `class transformers.integrations.TensorBoardCallback`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.TensorBoardCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L579)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L579)'
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tb_writer` (`SummaryWriter`, *optional*) â€” The writer to use. Will instantiate
    one if not set.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tb_writer`ï¼ˆ`SummaryWriter`ï¼Œ*å¯é€‰*ï¼‰â€”è¦ä½¿ç”¨çš„å†™å…¥å™¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†å®ä¾‹åŒ–ä¸€ä¸ªã€‚'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼Œå°†æ—¥å¿—å‘é€åˆ°[TensorBoard](https://www.tensorflow.org/tensorboard)ã€‚
- en: '### `class transformers.integrations.WandbCallback`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.WandbCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L665)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L665)'
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that logs metrics, media, model checkpoints to [Weight and Biases](https://www.wandb.com/).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼Œå°†æŒ‡æ ‡ã€åª’ä½“ã€æ¨¡å‹æ£€æŸ¥ç‚¹è®°å½•åˆ°[Weights
    and Biases](https://www.wandb.com/)ã€‚
- en: '#### `setup`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L690)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L690)'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Setup the optional Weights & Biases (*wandb*) integration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®å¯é€‰çš„Weights & Biasesï¼ˆ*wandb*ï¼‰é›†æˆã€‚
- en: 'One can subclass and override this method to customize the setup if needed.
    Find more information [here](https://docs.wandb.ai/guides/integrations/huggingface).
    You can also override the following environment variables:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦ï¼Œå¯ä»¥å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ä»¥è‡ªå®šä¹‰è®¾ç½®ã€‚åœ¨[è¿™é‡Œ](https://docs.wandb.ai/guides/integrations/huggingface)æ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚æ‚¨è¿˜å¯ä»¥é‡å†™ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
- en: 'Environment:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼š
- en: '`WANDB_LOG_MODEL` (`str`, *optional*, defaults to `"false"`): Whether to log
    model and checkpoints during training. Can be `"end"`, `"checkpoint"` or `"false"`.
    If set to `"end"`, the model will be uploaded at the end of training. If set to
    `"checkpoint"`, the checkpoint will be uploaded every `args.save_steps` . If set
    to `"false"`, the model will not be uploaded. Use along with `load_best_model_at_end()`
    to upload best model.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_LOG_MODEL`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"false"`ï¼‰ï¼šæ˜¯å¦åœ¨è®­ç»ƒæœŸé—´è®°å½•æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ã€‚å¯ä»¥æ˜¯`"end"`ï¼Œ`"checkpoint"`æˆ–`"false"`ã€‚å¦‚æœè®¾ç½®ä¸º`"end"`ï¼Œæ¨¡å‹å°†åœ¨è®­ç»ƒç»“æŸæ—¶ä¸Šä¼ ã€‚å¦‚æœè®¾ç½®ä¸º`"checkpoint"`ï¼Œå°†åœ¨æ¯æ¬¡`args.save_steps`ä¿å­˜æ—¶ä¸Šä¼ æ£€æŸ¥ç‚¹ã€‚å¦‚æœè®¾ç½®ä¸º`"false"`ï¼Œæ¨¡å‹å°†ä¸ä¼šä¸Šä¼ ã€‚ä¸`load_best_model_at_end()`ä¸€èµ·ä½¿ç”¨ä»¥ä¸Šä¼ æœ€ä½³æ¨¡å‹ã€‚'
- en: Deprecated in 5.0
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨5.0ä¸­å·²å¼ƒç”¨
- en: Setting `WANDB_LOG_MODEL` as `bool` will be deprecated in version 5 of ğŸ¤— Transformers.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Transformersçš„ç¬¬5ç‰ˆä¸­ï¼Œå°†åºŸå¼ƒå°†`WANDB_LOG_MODEL`è®¾ç½®ä¸º`bool`ã€‚
- en: '`WANDB_WATCH` (`str`, *optional* defaults to `"false"`): Can be `"gradients"`,
    `"all"`, `"parameters"`, or `"false"`. Set to `"all"` to log gradients and parameters.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_WATCH`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"false"`ï¼‰ï¼šå¯ä»¥æ˜¯`"gradients"`ï¼Œ`"all"`ï¼Œ`"parameters"`æˆ–`"false"`ã€‚è®¾ç½®ä¸º`"all"`ä»¥è®°å½•æ¢¯åº¦å’Œå‚æ•°ã€‚'
- en: '`WANDB_PROJECT` (`str`, *optional*, defaults to `"huggingface"`): Set this
    to a custom string to store results in a different project.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_PROJECT`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"huggingface"`ï¼‰ï¼šå°†å…¶è®¾ç½®ä¸ºè‡ªå®šä¹‰å­—ç¬¦ä¸²ä»¥å°†ç»“æœå­˜å‚¨åœ¨ä¸åŒçš„é¡¹ç›®ä¸­ã€‚'
- en: '`WANDB_DISABLED` (`bool`, *optional*, defaults to `False`): Whether to disable
    wandb entirely. Set `WANDB_DISABLED=true` to disable.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_DISABLED`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šæ˜¯å¦å®Œå…¨ç¦ç”¨wandbã€‚è®¾ç½®`WANDB_DISABLED=true`ä»¥ç¦ç”¨ã€‚'
- en: '### `class transformers.integrations.MLflowCallback`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.MLflowCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L933)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L933)'
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [MLflow](https://www.mlflow.org/). Can be disabled by setting
    environment variable `DISABLE_MLFLOW_INTEGRATION = TRUE`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼Œå°†æ—¥å¿—å‘é€åˆ°[MLflow](https://www.mlflow.org/)ã€‚å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡`DISABLE_MLFLOW_INTEGRATION
    = TRUE`æ¥ç¦ç”¨ã€‚
- en: '#### `setup`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L952)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L952)'
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Setup the optional MLflow integration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®å¯é€‰çš„MLflowé›†æˆã€‚
- en: 'Environment:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼š
- en: '`HF_MLFLOW_LOG_ARTIFACTS` (`str`, *optional*): Whether to use MLflow `.log_artifact()`
    facility to log artifacts. This only makes sense if logging to a remote server,
    e.g. s3 or GCS. If set to `True` or *1*, will copy each saved checkpoint on each
    save in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)â€™s
    `output_dir` to the local or remote artifact storage. Using it without a remote
    storage will just copy the files to your artifact location.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_MLFLOW_LOG_ARTIFACTS`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰ï¼šæ˜¯å¦ä½¿ç”¨MLflowçš„`.log_artifact()`åŠŸèƒ½æ¥è®°å½•å·¥ä»¶ã€‚åªæœ‰åœ¨å°†æ—¥å¿—è®°å½•åˆ°è¿œç¨‹æœåŠ¡å™¨ï¼ˆä¾‹å¦‚s3æˆ–GCSï¼‰æ—¶æ‰æœ‰æ„ä¹‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`æˆ–*1*ï¼Œå°†åœ¨æ¯æ¬¡åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)çš„`output_dir`ä¸­ä¿å­˜æ—¶å°†æ¯ä¸ªä¿å­˜çš„æ£€æŸ¥ç‚¹å¤åˆ¶åˆ°æœ¬åœ°æˆ–è¿œç¨‹å·¥ä»¶å­˜å‚¨ã€‚åœ¨æ²¡æœ‰è¿œç¨‹å­˜å‚¨çš„æƒ…å†µä¸‹ä½¿ç”¨å®ƒå°†åªæ˜¯å°†æ–‡ä»¶å¤åˆ¶åˆ°æ‚¨çš„å·¥ä»¶ä½ç½®ã€‚'
- en: '`MLFLOW_EXPERIMENT_NAME` (`str`, *optional*, defaults to `None`): Whether to
    use an MLflow experiment_name under which to launch the run. Default to `None`
    which will point to the `Default` experiment in MLflow. Otherwise, it is a case
    sensitive name of the experiment to be activated. If an experiment with this name
    does not exist, a new experiment with this name is created.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_EXPERIMENT_NAME` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`None`)ï¼šæ˜¯å¦ä½¿ç”¨MLflowå®éªŒåç§°æ¥å¯åŠ¨è¿è¡Œã€‚é»˜è®¤ä¸º`None`ï¼Œå°†æŒ‡å‘MLflowä¸­çš„`Default`å®éªŒã€‚å¦åˆ™ï¼Œå®ƒæ˜¯è¦æ¿€æ´»çš„å®éªŒçš„åŒºåˆ†å¤§å°å†™åç§°ã€‚å¦‚æœä¸å­˜åœ¨å…·æœ‰æ­¤åç§°çš„å®éªŒï¼Œåˆ™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰æ­¤åç§°çš„æ–°å®éªŒã€‚'
- en: '`MLFLOW_TAGS` (`str`, *optional*): A string dump of a dictionary of key/value
    pair to be added to the MLflow run as tags. Example: `os.environ[''MLFLOW_TAGS'']=''{"release.candidate":
    "RC1", "release.version": "2.2.0"}''`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_TAGS` (`str`, *å¯é€‰*)ï¼šè¦æ·»åŠ åˆ°MLflowè¿è¡Œä¸­çš„æ ‡ç­¾çš„é”®/å€¼å¯¹çš„å­—ç¬¦ä¸²è½¬å‚¨ã€‚ç¤ºä¾‹ï¼š`os.environ[''MLFLOW_TAGS'']=''{"release.candidate":
    "RC1", "release.version": "2.2.0"}''`ã€‚'
- en: '`MLFLOW_NESTED_RUN` (`str`, *optional*): Whether to use MLflow nested runs.
    If set to `True` or *1*, will create a nested run inside the current run.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_NESTED_RUN` (`str`, *å¯é€‰*)ï¼šæ˜¯å¦ä½¿ç”¨MLflowåµŒå¥—è¿è¡Œã€‚å¦‚æœè®¾ç½®ä¸º`True`æˆ–*1*ï¼Œå°†åœ¨å½“å‰è¿è¡Œå†…åˆ›å»ºä¸€ä¸ªåµŒå¥—è¿è¡Œã€‚'
- en: '`MLFLOW_RUN_ID` (`str`, *optional*): Allow to reattach to an existing run which
    can be usefull when resuming training from a checkpoint. When `MLFLOW_RUN_ID`
    environment variable is set, `start_run` attempts to resume a run with the specified
    run ID and other parameters are ignored.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_RUN_ID` (`str`, *å¯é€‰*)ï¼šå…è®¸é‡æ–°é™„åŠ åˆ°ç°æœ‰è¿è¡Œï¼Œè¿™åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒæ—¶å¯èƒ½å¾ˆæœ‰ç”¨ã€‚å½“è®¾ç½®äº†`MLFLOW_RUN_ID`ç¯å¢ƒå˜é‡æ—¶ï¼Œ`start_run`å°è¯•æ¢å¤å…·æœ‰æŒ‡å®šè¿è¡ŒIDçš„è¿è¡Œï¼Œå…¶ä»–å‚æ•°å°†è¢«å¿½ç•¥ã€‚'
- en: '`MLFLOW_FLATTEN_PARAMS` (`str`, *optional*, defaults to `False`): Whether to
    flatten the parameters dictionary before logging.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_FLATTEN_PARAMS` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`False`)ï¼šæ˜¯å¦åœ¨è®°å½•ä¹‹å‰å±•å¹³å‚æ•°å­—å…¸ã€‚'
- en: '### `class transformers.integrations.AzureMLCallback`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.AzureMLCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L910)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L910)'
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [AzureML](https://pypi.org/project/azureml-sdk/).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼Œå°†æ—¥å¿—å‘é€åˆ°[AzureML](https://pypi.org/project/azureml-sdk/)ã€‚
- en: '### `class transformers.integrations.CodeCarbonCallback`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.CodeCarbonCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1399)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1399)'
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that tracks the CO2 emission of training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼Œç”¨äºè·Ÿè¸ªè®­ç»ƒçš„CO2æ’æ”¾é‡ã€‚
- en: '### `class transformers.integrations.NeptuneCallback`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.NeptuneCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1128)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1128)'
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`api_token` (`str`, *optional*) â€” Neptune API token obtained upon registration.
    You can leave this argument out if you have saved your token to the `NEPTUNE_API_TOKEN`
    environment variable (strongly recommended). See full setup instructions in the
    [docs](https://docs.neptune.ai/setup/installation).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`api_token` (`str`, *å¯é€‰*) â€” åœ¨æ³¨å†Œæ—¶è·å¾—çš„Neptune APIä»¤ç‰Œã€‚å¦‚æœå·²å°†ä»¤ç‰Œä¿å­˜åˆ°`NEPTUNE_API_TOKEN`ç¯å¢ƒå˜é‡ä¸­ï¼Œå¯ä»¥çœç•¥æ­¤å‚æ•°ï¼ˆå¼ºçƒˆå»ºè®®ï¼‰ã€‚åœ¨[æ–‡æ¡£](https://docs.neptune.ai/setup/installation)ä¸­æŸ¥çœ‹å®Œæ•´çš„è®¾ç½®è¯´æ˜ã€‚'
- en: '`project` (`str`, *optional*) â€” Name of an existing Neptune project, in the
    form â€œworkspace-name/project-nameâ€. You can find and copy the name in Neptune
    from the project settings -> Properties. If None (default), the value of the `NEPTUNE_PROJECT`
    environment variable is used.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`project` (`str`, *å¯é€‰*) â€” Neptuneé¡¹ç›®çš„åç§°ï¼Œæ ¼å¼ä¸ºâ€œworkspace-name/project-nameâ€ã€‚æ‚¨å¯ä»¥åœ¨Neptuneä¸­çš„é¡¹ç›®è®¾ç½®
    -> å±æ€§ä¸­æ‰¾åˆ°å¹¶å¤åˆ¶åç§°ã€‚å¦‚æœä¸ºNoneï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™ä½¿ç”¨`NEPTUNE_PROJECT`ç¯å¢ƒå˜é‡çš„å€¼ã€‚'
- en: '`name` (`str`, *optional*) â€” Custom name for the run.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`, *å¯é€‰*) â€” è¿è¡Œçš„è‡ªå®šä¹‰åç§°ã€‚'
- en: '`base_namespace` (`str`, optional, defaults to â€œfinetuningâ€) â€” In the Neptune
    run, the root namespace that will contain all of the metadata logged by the callback.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_namespace` (`str`, å¯é€‰, é»˜è®¤ä¸ºâ€œfinetuningâ€) â€” åœ¨Neptuneè¿è¡Œä¸­ï¼Œå°†åŒ…å«å›è°ƒè®°å½•çš„æ‰€æœ‰å…ƒæ•°æ®çš„æ ¹å‘½åç©ºé—´ã€‚'
- en: '`log_parameters` (`bool`, *optional*, defaults to `True`) â€” If True, logs all
    Trainer arguments and model parameters provided by the Trainer.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_parameters` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” å¦‚æœä¸ºTrueï¼Œåˆ™è®°å½•Traineræä¾›çš„æ‰€æœ‰å‚æ•°å’Œæ¨¡å‹å‚æ•°ã€‚'
- en: '`log_checkpoints` (`str`, *optional*) â€” If â€œsameâ€, uploads checkpoints whenever
    they are saved by the Trainer. If â€œlastâ€, uploads only the most recently saved
    checkpoint. If â€œbestâ€, uploads the best checkpoint (among the ones saved by the
    Trainer). If `None`, does not upload checkpoints.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_checkpoints` (`str`, *å¯é€‰*) â€” å¦‚æœä¸ºâ€œsameâ€ï¼Œåˆ™åœ¨Trainerä¿å­˜æ£€æŸ¥ç‚¹æ—¶ä¸Šä¼ æ£€æŸ¥ç‚¹ã€‚å¦‚æœä¸ºâ€œlastâ€ï¼Œåˆ™ä»…ä¸Šä¼ æœ€è¿‘ä¿å­˜çš„æ£€æŸ¥ç‚¹ã€‚å¦‚æœä¸ºâ€œbestâ€ï¼Œåˆ™ä¸Šä¼ æœ€ä½³æ£€æŸ¥ç‚¹ï¼ˆåœ¨Trainerä¿å­˜çš„æ£€æŸ¥ç‚¹ä¸­é€‰æ‹©ï¼‰ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¸ä¸Šä¼ æ£€æŸ¥ç‚¹ã€‚'
- en: '`run` (`Run`, *optional*) â€” Pass a Neptune run object if you want to continue
    logging to an existing run. Read more about resuming runs in the [docs](https://docs.neptune.ai/logging/to_existing_object).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run` (`Run`, *å¯é€‰*) â€” å¦‚æœè¦ç»§ç»­è®°å½•åˆ°ç°æœ‰è¿è¡Œä¸­ï¼Œè¯·ä¼ é€’ä¸€ä¸ªNeptuneè¿è¡Œå¯¹è±¡ã€‚åœ¨[æ–‡æ¡£](https://docs.neptune.ai/logging/to_existing_object)ä¸­äº†è§£æ›´å¤šå…³äºæ¢å¤è¿è¡Œçš„ä¿¡æ¯ã€‚'
- en: '*`*neptune_run_kwargs` (*optional*) â€” Additional keyword arguments to be passed
    directly to the [`neptune.init_run()`](https://docs.neptune.ai/api/neptune#init_run)
    function when a new run is created.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*neptune_run_kwargs` (*å¯é€‰*) â€” ä¼ é€’ç»™[`neptune.init_run()`](https://docs.neptune.ai/api/neptune#init_run)å‡½æ•°çš„å…¶ä»–å…³é”®å­—å‚æ•°ï¼Œå½“åˆ›å»ºæ–°è¿è¡Œæ—¶ã€‚'
- en: TrainerCallback that sends the logs to [Neptune](https://app.neptune.ai).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ—¥å¿—å‘é€åˆ°[Neptune](https://app.neptune.ai)çš„TrainerCallbackã€‚
- en: For instructions and examples, see the [Transformers integration guide](https://docs.neptune.ai/integrations/transformers)
    in the Neptune documentation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è¯´æ˜å’Œç¤ºä¾‹ï¼Œè¯·å‚é˜…Neptuneæ–‡æ¡£ä¸­çš„[Transformersé›†æˆæŒ‡å—](https://docs.neptune.ai/integrations/transformers)ã€‚
- en: '### `class transformers.integrations.ClearMLCallback`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.ClearMLCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1428)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1428)'
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [ClearML](https://clear.ml/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ—¥å¿—å‘é€åˆ°[ClearML](https://clear.ml/)çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: 'Environment:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼š
- en: '`CLEARML_PROJECT` (`str`, *optional*, defaults to `HuggingFace Transformers`):
    ClearML project name.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLEARML_PROJECT` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `HuggingFace Transformers`): ClearMLé¡¹ç›®åç§°ã€‚'
- en: '`CLEARML_TASK` (`str`, *optional*, defaults to `Trainer`): ClearML task name.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLEARML_TASK` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `Trainer`): ClearMLä»»åŠ¡åç§°ã€‚'
- en: '`CLEARML_LOG_MODEL` (`bool`, *optional*, defaults to `False`): Whether to log
    models as artifacts during training.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLEARML_LOG_MODEL` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`): æ˜¯å¦åœ¨è®­ç»ƒæœŸé—´å°†æ¨¡å‹è®°å½•ä¸ºå·¥ä»¶ã€‚'
- en: '### `class transformers.integrations.DagsHubCallback`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.DagsHubCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1068)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1068)'
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that logs to [DagsHub](https://dagshub.com/). Extends `MLflowCallback`
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ—¥å¿—è®°å½•åˆ°[DagsHub](https://dagshub.com/)çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚æ‰©å±•`MLflowCallback`
- en: '#### `setup`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1082)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1082)'
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Setup the DagsHubâ€™s Logging integration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®DagsHubçš„æ—¥å¿—è®°å½•é›†æˆã€‚
- en: 'Environment:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼š
- en: '`HF_DAGSHUB_LOG_ARTIFACTS` (`str`, *optional*): Whether to save the data and
    model artifacts for the experiment. Default to `False`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_DAGSHUB_LOG_ARTIFACTS` (`str`, *å¯é€‰*): æ˜¯å¦ä¿å­˜å®éªŒçš„æ•°æ®å’Œæ¨¡å‹å·¥ä»¶ã€‚é»˜è®¤ä¸º`False`ã€‚'
- en: '### `class transformers.integrations.FlyteCallback`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.FlyteCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1549)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1549)'
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_log_history` (`bool`, *optional*, defaults to `True`) â€” When set to True,
    the training logs are saved as a Flyte Deck.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_log_history` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” å½“è®¾ç½®ä¸ºTrueæ—¶ï¼Œè®­ç»ƒæ—¥å¿—å°†ä¿å­˜ä¸ºFlyte Deckã€‚'
- en: '`sync_checkpoints` (`bool`, *optional*, defaults to `True`) â€” When set to True,
    checkpoints are synced with Flyte and can be used to resume training in the case
    of an interruption.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sync_checkpoints` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” å½“è®¾ç½®ä¸ºTrueæ—¶ï¼Œæ£€æŸ¥ç‚¹å°†ä¸FlyteåŒæ­¥ï¼Œå¹¶å¯ç”¨äºåœ¨ä¸­æ–­çš„æƒ…å†µä¸‹æ¢å¤è®­ç»ƒã€‚'
- en: 'A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [Flyte](https://flyte.org/). NOTE: This callback only works
    within a Flyte task.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ—¥å¿—å‘é€åˆ°[Flyte](https://flyte.org/)çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚æ³¨æ„ï¼šæ­¤å›è°ƒä»…åœ¨Flyteä»»åŠ¡å†…èµ·ä½œç”¨ã€‚
- en: 'Example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '### `class transformers.integrations.DVCLiveCallback`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.DVCLiveCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1612)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1612)'
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`live` (`dvclive.Live`, *optional*, defaults to `None`) â€” Optional Live instance.
    If None, a new instance will be created using **kwargs.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`live` (`dvclive.Live`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” å¯é€‰çš„Liveå®ä¾‹ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™å°†ä½¿ç”¨**kwargsåˆ›å»ºä¸€ä¸ªæ–°å®ä¾‹ã€‚'
- en: '`log_model` (Union[Literal[â€œallâ€], bool], *optional*, defaults to `None`) â€”
    Whether to use `dvclive.Live.log_artifact()` to log checkpoints created by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If set to `True`, the final checkpoint is logged at the end of training. If set
    to `"all"`, the entire [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)â€™s
    `output_dir` is logged at each checkpoint.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_model` (Union[Literal[â€œallâ€], bool], *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” æ˜¯å¦ä½¿ç”¨`dvclive.Live.log_artifact()`æ¥è®°å½•[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åˆ›å»ºçš„æ£€æŸ¥ç‚¹ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åœ¨è®­ç»ƒç»“æŸæ—¶è®°å½•æœ€ç»ˆæ£€æŸ¥ç‚¹ã€‚å¦‚æœè®¾ç½®ä¸º`"all"`ï¼Œåˆ™åœ¨æ¯ä¸ªæ£€æŸ¥ç‚¹å¤„è®°å½•æ•´ä¸ª[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)çš„`output_dir`ã€‚'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [DVCLive](https://www.dvc.org/doc/dvclive).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ—¥å¿—å‘é€åˆ°[DVCLive](https://www.dvc.org/doc/dvclive)çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: Use the environment variables below in `setup` to configure the integration.
    To customize this callback beyond those environment variables, see [here](https://dvc.org/doc/dvclive/ml-frameworks/huggingface).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`setup`ä¸­ä½¿ç”¨ä¸‹é¢çš„ç¯å¢ƒå˜é‡æ¥é…ç½®é›†æˆã€‚è¦åœ¨è¿™äº›ç¯å¢ƒå˜é‡ä¹‹å¤–è‡ªå®šä¹‰æ­¤å›è°ƒï¼Œè¯·å‚é˜…[æ­¤å¤„](https://dvc.org/doc/dvclive/ml-frameworks/huggingface)ã€‚
- en: '#### `setup`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1648)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1648)'
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Setup the optional DVCLive integration. To customize this callback beyond the
    environment variables below, see [here](https://dvc.org/doc/dvclive/ml-frameworks/huggingface).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®å¯é€‰çš„DVCLiveé›†æˆã€‚è¦åœ¨ç¯å¢ƒå˜é‡ä¹‹å¤–è‡ªå®šä¹‰æ­¤å›è°ƒï¼Œè¯·å‚é˜…[æ­¤å¤„](https://dvc.org/doc/dvclive/ml-frameworks/huggingface)ã€‚
- en: 'Environment:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼š
- en: '`HF_DVCLIVE_LOG_MODEL` (`str`, *optional*): Whether to use `dvclive.Live.log_artifact()`
    to log checkpoints created by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If set to `True` or *1*, the final checkpoint is logged at the end of training.
    If set to `all`, the entire [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)â€™s
    `output_dir` is logged at each checkpoint.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_DVCLIVE_LOG_MODEL` (`str`, *å¯é€‰*): æ˜¯å¦ä½¿ç”¨`dvclive.Live.log_artifact()`æ¥è®°å½•[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åˆ›å»ºçš„æ£€æŸ¥ç‚¹ã€‚å¦‚æœè®¾ç½®ä¸º`True`æˆ–*1*ï¼Œåˆ™åœ¨è®­ç»ƒç»“æŸæ—¶è®°å½•æœ€ç»ˆæ£€æŸ¥ç‚¹ã€‚å¦‚æœè®¾ç½®ä¸º`all`ï¼Œåˆ™åœ¨æ¯ä¸ªæ£€æŸ¥ç‚¹å¤„è®°å½•æ•´ä¸ª[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)çš„`output_dir`ã€‚'
- en: TrainerCallback
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainerCallback
- en: '### `class transformers.TrainerCallback`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainerCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L175)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L175)'
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments))
    â€” The training arguments used to instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments))
    â€” ç”¨äºå®ä¾‹åŒ–[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)çš„è®­ç»ƒå‚æ•°ã€‚'
- en: '`state` ([TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState))
    â€” The current state of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state` ([TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState))
    â€” [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)çš„å½“å‰çŠ¶æ€ã€‚'
- en: '`control` ([TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl))
    â€” The object that is returned to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and can be used to make some decisions.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control` ([TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl))
    â€” è¿”å›ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¹¶å¯ç”¨äºåšå‡ºä¸€äº›å†³ç­–çš„å¯¹è±¡ã€‚'
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or `torch.nn.Module`) â€” The model being trained.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)æˆ–`torch.nn.Module`)
    â€” æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹ã€‚'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” The tokenizer used for encoding the data.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” ç”¨äºç¼–ç æ•°æ®çš„åˆ†è¯å™¨ã€‚'
- en: '`optimizer` (`torch.optim.Optimizer`) â€” The optimizer used for the training
    steps.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`torch.optim.Optimizer`) â€” ç”¨äºè®­ç»ƒæ­¥éª¤çš„ä¼˜åŒ–å™¨ã€‚'
- en: '`lr_scheduler` (`torch.optim.lr_scheduler.LambdaLR`) â€” The scheduler used for
    setting the learning rate.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler` (`torch.optim.lr_scheduler.LambdaLR`) â€” ç”¨äºè®¾ç½®å­¦ä¹ ç‡çš„è°ƒåº¦å™¨ã€‚'
- en: '`train_dataloader` (`torch.utils.data.DataLoader`, *optional*) â€” The current
    dataloader used for training.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataloader` (`torch.utils.data.DataLoader`, *optional*) â€” ç”¨äºè®­ç»ƒçš„å½“å‰æ•°æ®åŠ è½½å™¨ã€‚'
- en: '`eval_dataloader` (`torch.utils.data.DataLoader`, *optional*) â€” The current
    dataloader used for training.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataloader` (`torch.utils.data.DataLoader`, *optional*) â€” ç”¨äºè®­ç»ƒçš„å½“å‰æ•°æ®åŠ è½½å™¨ã€‚'
- en: '`metrics` (`Dict[str, float]`) â€” The metrics computed by the last evaluation
    phase.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` (`Dict[str, float]`) â€” ä¸Šæ¬¡è¯„ä¼°é˜¶æ®µè®¡ç®—çš„æŒ‡æ ‡ã€‚'
- en: Those are only accessible in the event `on_evaluate`.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™äº›åªèƒ½åœ¨äº‹ä»¶`on_evaluate`ä¸­è®¿é—®ã€‚
- en: '`logs` (`Dict[str, float]`) â€” The values to log.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs` (`Dict[str, float]`) â€” è¦è®°å½•çš„å€¼ã€‚'
- en: Those are only accessible in the event `on_log`.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™äº›åªèƒ½åœ¨äº‹ä»¶`on_log`ä¸­è®¿é—®ã€‚
- en: 'A class for objects that will inspect the state of the training loop at some
    events and take some decisions. At each of those events the following arguments
    are available:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç”¨äºåœ¨æŸäº›äº‹ä»¶ä¸­æ£€æŸ¥è®­ç»ƒå¾ªç¯çŠ¶æ€å¹¶åšå‡ºä¸€äº›å†³ç­–çš„å¯¹è±¡ç±»ã€‚åœ¨æ¯ä¸ªäº‹ä»¶ä¸­ï¼Œä»¥ä¸‹å‚æ•°éƒ½æ˜¯å¯ç”¨çš„ï¼š
- en: The `control` object is the only one that can be changed by the callback, in
    which case the event that changes it should return the modified version.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`control`å¯¹è±¡æ˜¯å”¯ä¸€å¯ä»¥è¢«å›è°ƒå‡½æ•°æ›´æ”¹çš„å¯¹è±¡ï¼Œå¦‚æœæ›´æ”¹äº†`control`çš„äº‹ä»¶åº”è¯¥è¿”å›ä¿®æ”¹åçš„ç‰ˆæœ¬ã€‚'
- en: The argument `args`, `state` and `control` are positionals for all events, all
    the others are grouped in `kwargs`. You can unpack the ones you need in the signature
    of the event using them. As an example, see the code of the simple [PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°`args`ã€`state`å’Œ`control`å¯¹äºæ‰€æœ‰äº‹ä»¶éƒ½æ˜¯ä½ç½®å‚æ•°ï¼Œå…¶ä»–å‚æ•°éƒ½è¢«åˆ†ç»„åœ¨`kwargs`ä¸­ã€‚æ‚¨å¯ä»¥åœ¨äº‹ä»¶çš„ç­¾åä¸­è§£åŒ…æ‚¨éœ€è¦çš„å‚æ•°ã€‚ä¾‹å¦‚ï¼ŒæŸ¥çœ‹ç®€å•çš„[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)çš„ä»£ç ã€‚
- en: 'Example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#### `on_epoch_begin`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_epoch_begin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L244)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L244)'
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Event called at the beginning of an epoch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªepochå¼€å§‹æ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_epoch_end`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_epoch_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L250)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L250)'
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Event called at the end of an epoch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªepochç»“æŸæ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_evaluate`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L276)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L276)'
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Event called after an evaluation phase.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯„ä¼°é˜¶æ®µåè°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_init_end`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_init_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L226)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L226)'
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Event called at the end of the initialization of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åˆå§‹åŒ–ç»“æŸæ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_log`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_log`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L294)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L294)'
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Event called after logging the last logs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®°å½•æœ€åæ—¥å¿—åè°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_predict`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L282)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L282)'
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Event called after a successful prediction.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆåŠŸé¢„æµ‹åè°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_prediction_step`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_prediction_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L300)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L300)'
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Event called after a prediction step.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¢„æµ‹æ­¥éª¤åè°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_save`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_save`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L288)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L288)'
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Event called after a checkpoint save.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¿å­˜æ£€æŸ¥ç‚¹åè°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_step_begin`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_step_begin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L256)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L256)'
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Event called at the beginning of a training step. If using gradient accumulation,
    one training step might take several inputs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ­¥éª¤å¼€å§‹æ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚å¦‚æœä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤å¯èƒ½éœ€è¦å¤šä¸ªè¾“å…¥ã€‚
- en: '#### `on_step_end`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_step_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L269)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L269)'
- en: '[PRE32]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Event called at the end of a training step. If using gradient accumulation,
    one training step might take several inputs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚å¦‚æœä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤å¯èƒ½éœ€è¦å¤šä¸ªè¾“å…¥ã€‚
- en: '#### `on_substep_end`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_substep_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L263)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L263)'
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Event called at the end of an substep during gradient accumulation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¢¯åº¦ç´¯ç§¯æœŸé—´å­æ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_train_begin`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_train_begin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L232)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L232)'
- en: '[PRE34]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Event called at the beginning of training.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚
- en: '#### `on_train_end`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`on_train_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L238)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L238)'
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Event called at the end of training.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒç»“æŸæ—¶è°ƒç”¨çš„äº‹ä»¶ã€‚
- en: 'Here is an example of how to register a custom callback with the PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¦‚ä½•åœ¨PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä¸­æ³¨å†Œè‡ªå®šä¹‰å›è°ƒçš„ç¤ºä¾‹ï¼š
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Another way to register a callback is to call `trainer.add_callback()` as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ³¨å†Œå›è°ƒçš„æ–¹æ³•æ˜¯è°ƒç”¨`trainer.add_callback()`å¦‚ä¸‹ï¼š
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: TrainerState
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainerState
- en: '### `class transformers.TrainerState`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainerState`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L34)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L34)'
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`epoch` (`float`, *optional*) â€” Only set during training, will represent the
    epoch the training is at (the decimal part being the percentage of the current
    epoch completed).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epoch` (`float`, *å¯é€‰*) â€” ä»…åœ¨è®­ç»ƒæœŸé—´è®¾ç½®ï¼Œè¡¨ç¤ºè®­ç»ƒæ‰€å¤„çš„æ—¶ä»£ï¼ˆå°æ•°éƒ¨åˆ†è¡¨ç¤ºå½“å‰æ—¶ä»£å®Œæˆçš„ç™¾åˆ†æ¯”ï¼‰ã€‚'
- en: '`global_step` (`int`, *optional*, defaults to 0) â€” During training, represents
    the number of update steps completed.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_step` (`int`, *å¯é€‰*, é»˜è®¤ä¸º0) â€” åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¡¨ç¤ºå·²å®Œæˆçš„æ›´æ–°æ­¥éª¤æ•°é‡ã€‚'
- en: '`max_steps` (`int`, *optional*, defaults to 0) â€” The number of update steps
    to do during the current training.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º0) â€” å½“å‰è®­ç»ƒè¦æ‰§è¡Œçš„æ›´æ–°æ­¥éª¤æ•°é‡ã€‚'
- en: '`logging_steps` (`int`, *optional*, defaults to 500) â€” Log every X updates
    steps'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º500) â€” æ¯Xä¸ªæ›´æ–°æ­¥éª¤è®°å½•ä¸€æ¬¡æ—¥å¿—'
- en: '`eval_steps` (`int`, *optional*) â€” Run an evaluation every X steps.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps` (`int`, *å¯é€‰*) â€” æ¯Xæ­¥è¿è¡Œä¸€æ¬¡è¯„ä¼°ã€‚'
- en: '`save_steps` (`int`, *optional*, defaults to 500) â€” Save checkpoint every X
    updates steps.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º500) â€” æ¯Xä¸ªæ›´æ–°æ­¥éª¤ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ã€‚'
- en: '`train_batch_size` (`int`, *optional*) â€” The batch size for the training dataloader.
    Only needed when `auto_find_batch_size` has been used.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_batch_size` (`int`, *å¯é€‰*) â€” è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ‰¹é‡å¤§å°ã€‚ä»…åœ¨ä½¿ç”¨`auto_find_batch_size`æ—¶éœ€è¦ã€‚'
- en: '`num_input_tokens_seen` (`int`, *optional*, defaults to 0) â€” The number of
    tokens seen during training (number of input tokens, not the number of prediction
    tokens).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_input_tokens_seen` (`int`, *å¯é€‰*, é»˜è®¤ä¸º0) â€” è®­ç»ƒæœŸé—´çœ‹åˆ°çš„æ ‡è®°æ•°é‡ï¼ˆè¾“å…¥æ ‡è®°æ•°é‡ï¼Œè€Œä¸æ˜¯é¢„æµ‹æ ‡è®°æ•°é‡ï¼‰ã€‚'
- en: '`total_flos` (`float`, *optional*, defaults to 0) â€” The total number of floating
    operations done by the model since the beginning of training (stored as floats
    to avoid overflow).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_flos` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0) â€” è‡ªè®­ç»ƒå¼€å§‹ä»¥æ¥æ¨¡å‹æ‰§è¡Œçš„æµ®ç‚¹æ“ä½œæ€»æ•°ï¼ˆå­˜å‚¨ä¸ºæµ®ç‚¹æ•°ä»¥é¿å…æº¢å‡ºï¼‰ã€‚'
- en: '`log_history` (`List[Dict[str, float]]`, *optional*) â€” The list of logs done
    since the beginning of training.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_history` (`List[Dict[str, float]]`, *å¯é€‰*) â€” è‡ªè®­ç»ƒå¼€å§‹ä»¥æ¥å®Œæˆçš„æ—¥å¿—åˆ—è¡¨ã€‚'
- en: '`best_metric` (`float`, *optional*) â€” When tracking the best model, the value
    of the best metric encountered so far.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_metric` (`float`, *å¯é€‰*) â€” è·Ÿè¸ªæœ€ä½³æ¨¡å‹æ—¶ï¼Œè¿„ä»Šä¸ºæ­¢é‡åˆ°çš„æœ€ä½³æŒ‡æ ‡çš„å€¼ã€‚'
- en: '`best_model_checkpoint` (`str`, *optional*) â€” When tracking the best model,
    the value of the name of the checkpoint for the best model encountered so far.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_model_checkpoint` (`str`, *å¯é€‰*) â€” è·Ÿè¸ªæœ€ä½³æ¨¡å‹æ—¶ï¼Œè¿„ä»Šä¸ºæ­¢é‡åˆ°çš„æœ€ä½³æ¨¡å‹çš„æ£€æŸ¥ç‚¹åç§°çš„å€¼ã€‚'
- en: '`is_local_process_zero` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not this process is the local (e.g., on one machine if training in a distributed
    fashion on several machines) main process.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_local_process_zero` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿™ä¸ªè¿›ç¨‹æ˜¯æœ¬åœ°ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¤šå°æœºå™¨ä¸Šä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒæ—¶ï¼Œæ˜¯ä¸€å°æœºå™¨ä¸Šçš„ä¸»è¿›ç¨‹ï¼‰ã€‚'
- en: '`is_world_process_zero` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not this process is the global main process (when training in a distributed
    fashion on several machines, this is only going to be `True` for one process).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_world_process_zero` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿™ä¸ªè¿›ç¨‹æ˜¯å…¨å±€ä¸»è¿›ç¨‹ï¼ˆåœ¨å¤šå°æœºå™¨ä¸Šä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒæ—¶ï¼Œåªæœ‰ä¸€ä¸ªè¿›ç¨‹ä¼šæ˜¯`True`ï¼‰ã€‚'
- en: '`is_hyper_param_search` (`bool`, *optional*, defaults to `False`) â€” Whether
    we are in the process of a hyper parameter search using Trainer.hyperparameter_search.
    This will impact the way data will be logged in TensorBoard.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_hyper_param_search` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ­£åœ¨ä½¿ç”¨Trainer.hyperparameter_searchè¿›è¡Œè¶…å‚æ•°æœç´¢ã€‚è¿™å°†å½±å“æ•°æ®åœ¨TensorBoardä¸­è®°å½•çš„æ–¹å¼ã€‚'
- en: A class containing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    inner state that will be saved along the model and optimizer when checkpointing
    and passed to the [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«å°†åœ¨æ£€æŸ¥ç‚¹æ—¶ä¿å­˜çš„[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å†…éƒ¨çŠ¶æ€çš„ç±»ï¼Œå¹¶ä¼ é€’ç»™[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ã€‚
- en: 'In all this class, one step is to be understood as one update step. When using
    gradient accumulation, one update step may require several forward and backward
    passes: if you use `gradient_accumulation_steps=n`, then one update step requires
    going through *n* batches.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç±»ä¸­ï¼Œä¸€ä¸ªæ­¥éª¤è¢«ç†è§£ä¸ºä¸€ä¸ªæ›´æ–°æ­¥éª¤ã€‚å½“ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œä¸€ä¸ªæ›´æ–°æ­¥éª¤å¯èƒ½éœ€è¦å¤šæ¬¡å‰å‘å’Œåå‘ä¼ é€’ï¼šå¦‚æœä½¿ç”¨`gradient_accumulation_steps=n`ï¼Œåˆ™ä¸€ä¸ªæ›´æ–°æ­¥éª¤éœ€è¦é€šè¿‡*n*æ‰¹æ¬¡ã€‚
- en: '#### `load_from_json`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_from_json`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L117)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L117)'
- en: '[PRE39]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Create an instance from the content of `json_path`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ä»`json_path`çš„å†…å®¹åˆ›å»ºä¸€ä¸ªå®ä¾‹ã€‚
- en: '#### `save_to_json`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_to_json`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L111)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L111)'
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Save the content of this instance in JSON format inside `json_path`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å®ä¾‹çš„å†…å®¹ä»¥JSONæ ¼å¼ä¿å­˜åœ¨`json_path`ä¸­ã€‚
- en: TrainerControl
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainerControl
- en: '### `class transformers.TrainerControl`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainerControl`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L125)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L125)'
- en: '[PRE41]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`should_training_stop` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the training should be interrupted.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_training_stop`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ˜¯å¦åº”ä¸­æ–­è®­ç»ƒã€‚'
- en: If `True`, this variable will not be set back to `False`. The training will
    just stop.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä¸º`True`ï¼Œåˆ™æ­¤å˜é‡ä¸ä¼šè¢«è®¾ç½®å›`False`ã€‚è®­ç»ƒå°†ç›´æ¥åœæ­¢ã€‚
- en: '`should_epoch_stop` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the current epoch should be interrupted.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_epoch_stop`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”å½“å‰è½®æ˜¯å¦åº”ä¸­æ–­ã€‚'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next epoch.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä¸º`True`ï¼Œåˆ™æ­¤å˜é‡å°†åœ¨ä¸‹ä¸€è½®å¼€å§‹æ—¶è®¾ç½®å›`False`ã€‚
- en: '`should_save` (`bool`, *optional*, defaults to `False`) â€” Whether or not the
    model should be saved at this step.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_save`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ¨¡å‹æ˜¯å¦åº”åœ¨æ­¤æ­¥éª¤ä¿å­˜ã€‚'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä¸º`True`ï¼Œåˆ™æ­¤å˜é‡å°†åœ¨ä¸‹ä¸€æ­¥å¼€å§‹æ—¶è®¾ç½®å›`False`ã€‚
- en: '`should_evaluate` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    the model should be evaluated at this step.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_evaluate`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ¨¡å‹æ˜¯å¦åº”åœ¨æ­¤æ­¥éª¤è¿›è¡Œè¯„ä¼°ã€‚'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä¸º`True`ï¼Œåˆ™æ­¤å˜é‡å°†åœ¨ä¸‹ä¸€æ­¥å¼€å§‹æ—¶è®¾ç½®å›`False`ã€‚
- en: '`should_log` (`bool`, *optional*, defaults to `False`) â€” Whether or not the
    logs should be reported at this step.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_log`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ˜¯å¦åº”åœ¨æ­¤æ­¥éª¤æŠ¥å‘Šæ—¥å¿—ã€‚'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä¸º`True`ï¼Œåˆ™æ­¤å˜é‡å°†åœ¨ä¸‹ä¸€æ­¥å¼€å§‹æ—¶è®¾ç½®å›`False`ã€‚
- en: A class that handles the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    control flow. This class is used by the [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    to activate some switches in the training loop.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ§åˆ¶æµçš„ç±»ã€‚æ­¤ç±»ç”±[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ä½¿ç”¨ï¼Œä»¥åœ¨è®­ç»ƒå¾ªç¯ä¸­æ¿€æ´»ä¸€äº›å¼€å…³ã€‚
