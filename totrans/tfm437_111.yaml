- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/38.08374997.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks are objects that can customize the behavior of the training loop in
    the PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    (this feature is not yet implemented in TensorFlow) that can inspect the training
    loop state (for progress reporting, logging on TensorBoard or other ML platforms…)
    and take decisions (like early stopping).
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks are “read only” pieces of code, apart from the [TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)
    object they return, they cannot change anything in the training loop. For customizations
    that require changes in the training loop, you should subclass [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and override the methods you need (see [trainer](trainer) for examples).
  prefs: []
  type: TYPE_NORMAL
- en: By default, `TrainingArguments.report_to` is set to `"all"`, so a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will use the following callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: '[DefaultFlowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.DefaultFlowCallback)
    which handles the default behavior for logging, saving and evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)
    or [ProgressCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.ProgressCallback)
    to display progress and print the logs (the first one is used if you deactivate
    tqdm through the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    otherwise it’s the second one).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorBoardCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.TensorBoardCallback)
    if tensorboard is accessible (either through PyTorch >= 1.4 or tensorboardX).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WandbCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.WandbCallback)
    if [wandb](https://www.wandb.com/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CometCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CometCallback)
    if [comet_ml](https://www.comet.ml/site/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLflowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.MLflowCallback)
    if [mlflow](https://www.mlflow.org/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[NeptuneCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.NeptuneCallback)
    if [neptune](https://neptune.ai/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AzureMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.AzureMLCallback)
    if [azureml-sdk](https://pypi.org/project/azureml-sdk/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CodeCarbonCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CodeCarbonCallback)
    if [codecarbon](https://pypi.org/project/codecarbon/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ClearMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.ClearMLCallback)
    if [clearml](https://github.com/allegroai/clearml) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DagsHubCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DagsHubCallback)
    if [dagshub](https://dagshub.com/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlyteCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.FlyteCallback)
    if [flyte](https://flyte.org/) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DVCLiveCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DVCLiveCallback)
    if [dvclive](https://dvc.org/doc/dvclive) is installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a package is installed but you don’t wish to use the accompanying integration,
    you can change `TrainingArguments.report_to` to a list of just those integrations
    you want to use (e.g. `["azure_ml", "wandb"]`).
  prefs: []
  type: TYPE_NORMAL
- en: The main class that implements callbacks is [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    It gets the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    used to instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    can access that Trainer’s internal state via [TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState),
    and can take some actions on the training loop via [TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl).
  prefs: []
  type: TYPE_NORMAL
- en: Available Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the list of the available [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    in the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.CometCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L833)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [Comet ML](https://www.comet.ml/site/).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `setup`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L844)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Setup the optional Comet.ml integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`COMET_MODE` (`str`, *optional*, defaults to `ONLINE`): Whether to create an
    online, offline experiment or disable Comet logging. Can be `OFFLINE`, `ONLINE`,
    or `DISABLED`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COMET_PROJECT_NAME` (`str`, *optional*): Comet project name for experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COMET_OFFLINE_DIRECTORY` (`str`, *optional*): Folder to use for saving offline
    experiments when `COMET_MODE` is `OFFLINE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COMET_LOG_ASSETS` (`str`, *optional*, defaults to `TRUE`): Whether or not
    to log training assets (tf event logs, checkpoints, etc), to Comet. Can be `TRUE`,
    or `FALSE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a number of configurable items in the environment, see [here](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables).
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.DefaultFlowCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L432)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that handles the default flow of the training loop for logs, evaluation and checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.PrinterCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L532)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A bare [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that just prints the logs.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ProgressCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L482)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that displays the progress of training or evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.EarlyStoppingCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L543)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`early_stopping_patience` (`int`) — Use with `metric_for_best_model` to stop
    training when the specified metric worsens for `early_stopping_patience` evaluation
    calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping_threshold(float,` *optional*) — Use with TrainingArguments
    `metric_for_best_model` and `early_stopping_patience` to denote how much the specified
    metric must improve to satisfy early stopping conditions. `'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that handles early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: This callback depends on [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    argument *load_best_model_at_end* functionality to set best_metric in [TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState).
    Note that if the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    argument *save_steps* differs from *eval_steps*, the early stopping will not occur
    until the next save step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.TensorBoardCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L579)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tb_writer` (`SummaryWriter`, *optional*) — The writer to use. Will instantiate
    one if not set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.WandbCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L665)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that logs metrics, media, model checkpoints to [Weight and Biases](https://www.wandb.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `setup`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L690)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Setup the optional Weights & Biases (*wandb*) integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can subclass and override this method to customize the setup if needed.
    Find more information [here](https://docs.wandb.ai/guides/integrations/huggingface).
    You can also override the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WANDB_LOG_MODEL` (`str`, *optional*, defaults to `"false"`): Whether to log
    model and checkpoints during training. Can be `"end"`, `"checkpoint"` or `"false"`.
    If set to `"end"`, the model will be uploaded at the end of training. If set to
    `"checkpoint"`, the checkpoint will be uploaded every `args.save_steps` . If set
    to `"false"`, the model will not be uploaded. Use along with `load_best_model_at_end()`
    to upload best model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 5.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Setting `WANDB_LOG_MODEL` as `bool` will be deprecated in version 5 of 🤗 Transformers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`WANDB_WATCH` (`str`, *optional* defaults to `"false"`): Can be `"gradients"`,
    `"all"`, `"parameters"`, or `"false"`. Set to `"all"` to log gradients and parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WANDB_PROJECT` (`str`, *optional*, defaults to `"huggingface"`): Set this
    to a custom string to store results in a different project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WANDB_DISABLED` (`bool`, *optional*, defaults to `False`): Whether to disable
    wandb entirely. Set `WANDB_DISABLED=true` to disable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.MLflowCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L933)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [MLflow](https://www.mlflow.org/). Can be disabled by setting
    environment variable `DISABLE_MLFLOW_INTEGRATION = TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `setup`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L952)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Setup the optional MLflow integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_MLFLOW_LOG_ARTIFACTS` (`str`, *optional*): Whether to use MLflow `.log_artifact()`
    facility to log artifacts. This only makes sense if logging to a remote server,
    e.g. s3 or GCS. If set to `True` or *1*, will copy each saved checkpoint on each
    save in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)’s
    `output_dir` to the local or remote artifact storage. Using it without a remote
    storage will just copy the files to your artifact location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLFLOW_EXPERIMENT_NAME` (`str`, *optional*, defaults to `None`): Whether to
    use an MLflow experiment_name under which to launch the run. Default to `None`
    which will point to the `Default` experiment in MLflow. Otherwise, it is a case
    sensitive name of the experiment to be activated. If an experiment with this name
    does not exist, a new experiment with this name is created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLFLOW_TAGS` (`str`, *optional*): A string dump of a dictionary of key/value
    pair to be added to the MLflow run as tags. Example: `os.environ[''MLFLOW_TAGS'']=''{"release.candidate":
    "RC1", "release.version": "2.2.0"}''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLFLOW_NESTED_RUN` (`str`, *optional*): Whether to use MLflow nested runs.
    If set to `True` or *1*, will create a nested run inside the current run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLFLOW_RUN_ID` (`str`, *optional*): Allow to reattach to an existing run which
    can be usefull when resuming training from a checkpoint. When `MLFLOW_RUN_ID`
    environment variable is set, `start_run` attempts to resume a run with the specified
    run ID and other parameters are ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLFLOW_FLATTEN_PARAMS` (`str`, *optional*, defaults to `False`): Whether to
    flatten the parameters dictionary before logging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.AzureMLCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L910)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [AzureML](https://pypi.org/project/azureml-sdk/).
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.CodeCarbonCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1399)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that tracks the CO2 emission of training.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.NeptuneCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`api_token` (`str`, *optional*) — Neptune API token obtained upon registration.
    You can leave this argument out if you have saved your token to the `NEPTUNE_API_TOKEN`
    environment variable (strongly recommended). See full setup instructions in the
    [docs](https://docs.neptune.ai/setup/installation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`project` (`str`, *optional*) — Name of an existing Neptune project, in the
    form “workspace-name/project-name”. You can find and copy the name in Neptune
    from the project settings -> Properties. If None (default), the value of the `NEPTUNE_PROJECT`
    environment variable is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` (`str`, *optional*) — Custom name for the run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`base_namespace` (`str`, optional, defaults to “finetuning”) — In the Neptune
    run, the root namespace that will contain all of the metadata logged by the callback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_parameters` (`bool`, *optional*, defaults to `True`) — If True, logs all
    Trainer arguments and model parameters provided by the Trainer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_checkpoints` (`str`, *optional*) — If “same”, uploads checkpoints whenever
    they are saved by the Trainer. If “last”, uploads only the most recently saved
    checkpoint. If “best”, uploads the best checkpoint (among the ones saved by the
    Trainer). If `None`, does not upload checkpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run` (`Run`, *optional*) — Pass a Neptune run object if you want to continue
    logging to an existing run. Read more about resuming runs in the [docs](https://docs.neptune.ai/logging/to_existing_object).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*neptune_run_kwargs` (*optional*) — Additional keyword arguments to be passed
    directly to the [`neptune.init_run()`](https://docs.neptune.ai/api/neptune#init_run)
    function when a new run is created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TrainerCallback that sends the logs to [Neptune](https://app.neptune.ai).
  prefs: []
  type: TYPE_NORMAL
- en: For instructions and examples, see the [Transformers integration guide](https://docs.neptune.ai/integrations/transformers)
    in the Neptune documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.ClearMLCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1428)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [ClearML](https://clear.ml/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CLEARML_PROJECT` (`str`, *optional*, defaults to `HuggingFace Transformers`):
    ClearML project name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CLEARML_TASK` (`str`, *optional*, defaults to `Trainer`): ClearML task name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CLEARML_LOG_MODEL` (`bool`, *optional*, defaults to `False`): Whether to log
    models as artifacts during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.DagsHubCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1068)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that logs to [DagsHub](https://dagshub.com/). Extends `MLflowCallback`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `setup`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1082)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Setup the DagsHub’s Logging integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_DAGSHUB_LOG_ARTIFACTS` (`str`, *optional*): Whether to save the data and
    model artifacts for the experiment. Default to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.integrations.FlyteCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1549)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_log_history` (`bool`, *optional*, defaults to `True`) — When set to True,
    the training logs are saved as a Flyte Deck.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync_checkpoints` (`bool`, *optional*, defaults to `True`) — When set to True,
    checkpoints are synced with Flyte and can be used to resume training in the case
    of an interruption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [Flyte](https://flyte.org/). NOTE: This callback only works
    within a Flyte task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.integrations.DVCLiveCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1612)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`live` (`dvclive.Live`, *optional*, defaults to `None`) — Optional Live instance.
    If None, a new instance will be created using **kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_model` (Union[Literal[“all”], bool], *optional*, defaults to `None`) —
    Whether to use `dvclive.Live.log_artifact()` to log checkpoints created by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If set to `True`, the final checkpoint is logged at the end of training. If set
    to `"all"`, the entire [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)’s
    `output_dir` is logged at each checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [DVCLive](https://www.dvc.org/doc/dvclive).
  prefs: []
  type: TYPE_NORMAL
- en: Use the environment variables below in `setup` to configure the integration.
    To customize this callback beyond those environment variables, see [here](https://dvc.org/doc/dvclive/ml-frameworks/huggingface).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `setup`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1648)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Setup the optional DVCLive integration. To customize this callback beyond the
    environment variables below, see [here](https://dvc.org/doc/dvclive/ml-frameworks/huggingface).
  prefs: []
  type: TYPE_NORMAL
- en: 'Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_DVCLIVE_LOG_MODEL` (`str`, *optional*): Whether to use `dvclive.Live.log_artifact()`
    to log checkpoints created by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If set to `True` or *1*, the final checkpoint is logged at the end of training.
    If set to `all`, the entire [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)’s
    `output_dir` is logged at each checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TrainerCallback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TrainerCallback`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L175)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments))
    — The training arguments used to instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state` ([TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState))
    — The current state of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control` ([TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl))
    — The object that is returned to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and can be used to make some decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or `torch.nn.Module`) — The model being trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer used for encoding the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer` (`torch.optim.Optimizer`) — The optimizer used for the training
    steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_scheduler` (`torch.optim.lr_scheduler.LambdaLR`) — The scheduler used for
    setting the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_dataloader` (`torch.utils.data.DataLoader`, *optional*) — The current
    dataloader used for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_dataloader` (`torch.utils.data.DataLoader`, *optional*) — The current
    dataloader used for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics` (`Dict[str, float]`) — The metrics computed by the last evaluation
    phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those are only accessible in the event `on_evaluate`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`logs` (`Dict[str, float]`) — The values to log.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those are only accessible in the event `on_log`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A class for objects that will inspect the state of the training loop at some
    events and take some decisions. At each of those events the following arguments
    are available:'
  prefs: []
  type: TYPE_NORMAL
- en: The `control` object is the only one that can be changed by the callback, in
    which case the event that changes it should return the modified version.
  prefs: []
  type: TYPE_NORMAL
- en: The argument `args`, `state` and `control` are positionals for all events, all
    the others are grouped in `kwargs`. You can unpack the ones you need in the signature
    of the event using them. As an example, see the code of the simple [PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#### `on_epoch_begin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L244)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the beginning of an epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_epoch_end`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L250)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the end of an epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_evaluate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L276)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Event called after an evaluation phase.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_init_end`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L226)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the end of the initialization of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_log`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L294)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Event called after logging the last logs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_predict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L282)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Event called after a successful prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_prediction_step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L300)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Event called after a prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_save`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L288)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Event called after a checkpoint save.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_step_begin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L256)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the beginning of a training step. If using gradient accumulation,
    one training step might take several inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_step_end`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L269)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the end of a training step. If using gradient accumulation,
    one training step might take several inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_substep_end`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L263)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the end of an substep during gradient accumulation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_train_begin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L232)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the beginning of training.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_train_end`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L238)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Event called at the end of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how to register a custom callback with the PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to register a callback is to call `trainer.add_callback()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: TrainerState
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TrainerState`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`epoch` (`float`, *optional*) — Only set during training, will represent the
    epoch the training is at (the decimal part being the percentage of the current
    epoch completed).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_step` (`int`, *optional*, defaults to 0) — During training, represents
    the number of update steps completed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_steps` (`int`, *optional*, defaults to 0) — The number of update steps
    to do during the current training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logging_steps` (`int`, *optional*, defaults to 500) — Log every X updates
    steps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_steps` (`int`, *optional*) — Run an evaluation every X steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_steps` (`int`, *optional*, defaults to 500) — Save checkpoint every X
    updates steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_batch_size` (`int`, *optional*) — The batch size for the training dataloader.
    Only needed when `auto_find_batch_size` has been used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_input_tokens_seen` (`int`, *optional*, defaults to 0) — The number of
    tokens seen during training (number of input tokens, not the number of prediction
    tokens).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_flos` (`float`, *optional*, defaults to 0) — The total number of floating
    operations done by the model since the beginning of training (stored as floats
    to avoid overflow).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_history` (`List[Dict[str, float]]`, *optional*) — The list of logs done
    since the beginning of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`best_metric` (`float`, *optional*) — When tracking the best model, the value
    of the best metric encountered so far.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`best_model_checkpoint` (`str`, *optional*) — When tracking the best model,
    the value of the name of the checkpoint for the best model encountered so far.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_local_process_zero` (`bool`, *optional*, defaults to `True`) — Whether
    or not this process is the local (e.g., on one machine if training in a distributed
    fashion on several machines) main process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_world_process_zero` (`bool`, *optional*, defaults to `True`) — Whether
    or not this process is the global main process (when training in a distributed
    fashion on several machines, this is only going to be `True` for one process).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_hyper_param_search` (`bool`, *optional*, defaults to `False`) — Whether
    we are in the process of a hyper parameter search using Trainer.hyperparameter_search.
    This will impact the way data will be logged in TensorBoard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A class containing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    inner state that will be saved along the model and optimizer when checkpointing
    and passed to the [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  prefs: []
  type: TYPE_NORMAL
- en: 'In all this class, one step is to be understood as one update step. When using
    gradient accumulation, one update step may require several forward and backward
    passes: if you use `gradient_accumulation_steps=n`, then one update step requires
    going through *n* batches.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_from_json`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L117)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Create an instance from the content of `json_path`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_to_json`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Save the content of this instance in JSON format inside `json_path`.
  prefs: []
  type: TYPE_NORMAL
- en: TrainerControl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TrainerControl`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L125)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`should_training_stop` (`bool`, *optional*, defaults to `False`) — Whether
    or not the training should be interrupted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `True`, this variable will not be set back to `False`. The training will
    just stop.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`should_epoch_stop` (`bool`, *optional*, defaults to `False`) — Whether or
    not the current epoch should be interrupted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next epoch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`should_save` (`bool`, *optional*, defaults to `False`) — Whether or not the
    model should be saved at this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`should_evaluate` (`bool`, *optional*, defaults to `False`) — Whether or not
    the model should be evaluated at this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`should_log` (`bool`, *optional*, defaults to `False`) — Whether or not the
    logs should be reported at this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A class that handles the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    control flow. This class is used by the [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    to activate some switches in the training loop.
  prefs: []
  type: TYPE_NORMAL
