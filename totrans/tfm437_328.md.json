["```py\npython src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path \"\" --pytorch_dump_folder_path \"Arthur/whisper-3\" --convert_preprocessor True\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n\n>>> # Select an audio file and read it:\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> audio_sample = ds[0][\"audio\"]\n>>> waveform = audio_sample[\"array\"]\n>>> sampling_rate = audio_sample[\"sampling_rate\"]\n\n>>> # Load the Whisper model in Hugging Face format:\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> # Use the model and processor to transcribe the audio:\n>>> input_features = processor(\n...     waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\n... ).input_features\n\n>>> # Generate token ids\n>>> predicted_ids = model.generate(input_features)\n\n>>> # Decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\n>>> transcription[0]\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\npip install -U openai-whisper\npython convert_hf_to_openai.py \\\n    --checkpoint openai/whisper-tiny \\\n    --whisper_dump_path whisper-tiny-openai.pt\n```", "```py\n>>> from transformers import WhisperConfig, WhisperModel\n\n>>> # Initializing a Whisper tiny style configuration\n>>> configuration = WhisperConfig()\n\n>>> # Initializing a model (with random weights) from the tiny style configuration\n>>> model = WhisperModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> # instantiate the tokenizer and set the prefix token to Spanish\n>>> tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"spanish\")\n>>> # now switch the prefix token from Spanish to French\n>>> tokenizer.set_prefix_tokens(language=\"french\")\n```", "```py\n>>> # instantiate the tokenizer and set the prefix token to Spanish\n>>> tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-tiny\", language=\"spanish\")\n>>> # now switch the prefix token from Spanish to French\n>>> tokenizer.set_prefix_tokens(language=\"french\")\n```", "```py\n>>> import torch\n>>> from transformers import AutoFeatureExtractor, WhisperModel\n>>> from datasets import load_dataset\n\n>>> model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n>>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n>>> list(last_hidden_state.shape)\n[1, 2, 512]\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n\n>>> generated_ids = model.generate(inputs=input_features)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset, Audio\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model.cuda()\n\n>>> # load audios > 30 seconds\n>>> ds = load_dataset(\"distil-whisper/meanwhile\", \"default\")[\"test\"]\n>>> # resample to 16kHz\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> # take first 8 audios and retrieve array\n>>> audio = ds[:8][\"audio\"]\n>>> audio = [x[\"array\"] for x in audio]\n\n>>> # make sure to NOT truncate the input audio, to return the `attention_mask` and to pad to the longest audio\n>>> inputs = processor(audio, return_tensors=\"pt\", truncation=False, padding=\"longest\", return_attention_mask=True, sampling_rate=16_000)\n>>> inputs = inputs.to(\"cuda\", torch.float32)\n\n>>> # transcribe audio to ids\n>>> generated_ids = model.generate(**inputs)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> transcription[0]\n' Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile!'\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n\n>>> generated_ids = model.generate(inputs=input_features)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n>>> from transformers import WhisperForCausalLM, WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n\n>>> assistant_model = WhisperForCausalLM.from_pretrained(\"distil-whisper/distil-large-v2\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(\n...     sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\"\n... ).input_features\n\n>>> predicted_ids = model.generate(input_features, assistant_model=assistant_model)\n\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'\n```", "```py\n>>> import torch\n>>> from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n>>> from datasets import load_dataset\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n>>> model = WhisperForAudioClassification.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n\n>>> ds = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True)\n>>> sample = next(iter(ds))\n\n>>> inputs = feature_extractor(\n...     sample[\"audio\"][\"array\"], sampling_rate=sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n... )\n>>> input_features = inputs.input_features\n\n>>> with torch.no_grad():\n...     logits = model(input_features).logits\n\n>>> predicted_class_ids = torch.argmax(logits).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'Afrikaans'\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import TFWhisperModel, AutoFeatureExtractor\n>>> from datasets import load_dataset\n\n>>> model = TFWhisperModel.from_pretrained(\"openai/whisper-base\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"tf\")\n>>> input_features = inputs.input_features\n>>> decoder_input_ids = tf.convert_to_tensor([[1, 1]]) * model.config.decoder_start_token_id\n>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n>>> list(last_hidden_state.shape)\n[1, 2, 512]\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoProcessor, TFWhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"tf\")\n>>> input_features = inputs.input_features\n\n>>> generated_ids = model.generate(input_features=input_features)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxWhisperModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-tiny\")\n>>> model = FlaxWhisperModel.from_pretrained(\"openai/whisper-tiny\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import WhisperProcessor, FlaxWhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = FlaxWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\", from_pt=True)\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"np\")\n>>> input_features = inputs.input_features\n>>> generated_ids = model.generate(input_ids=input_features)\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoFeatureExtractor, FlaxWhisperForAudioClassification\n>>> from datasets import load_dataset\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n>>> model = FlaxWhisperForAudioClassification.from_pretrained(\n...     \"sanchit-gandhi/whisper-medium-fleurs-lang-id\", from_pt=True\n... )\n>>> ds = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True)\n\n>>> sample = next(iter(ds))\n\n>>> inputs = feature_extractor(\n...     sample[\"audio\"][\"array\"], sampling_rate=sample[\"audio\"][\"sampling_rate\"], return_tensors=\"np\"\n... )\n>>> input_features = inputs.input_features\n\n>>> logits = model(input_features).logits\n\n>>> predicted_class_ids = jnp.argmax(logits).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'af_za'\n```"]