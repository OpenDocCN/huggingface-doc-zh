["```py\npython src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path \"\" --pytorch_dump_folder_path \"Arthur/whisper-3\" --convert_preprocessor True\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n\n>>> # Select an audio file and read it:\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> audio_sample = ds[0][\"audio\"]\n>>> waveform = audio_sample[\"array\"]\n>>> sampling_rate = audio_sample[\"sampling_rate\"]\n\n>>> # Load the Whisper model in Hugging Face format:\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> # Use the model and processor to transcribe the audio:\n>>> input_features = processor(\n...     waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\n... ).input_features\n\n>>> # Generate token ids\n>>> predicted_ids = model.generate(input_features)\n\n>>> # Decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\n>>> transcription[0]\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\npip install -U openai-whisper\npython convert_hf_to_openai.py \\\n    --checkpoint openai/whisper-tiny \\\n    --whisper_dump_path whisper-tiny-openai.pt\n```", "```py\n( vocab_size = 51865 num_mel_bins = 80 encoder_layers = 4 encoder_attention_heads = 6 decoder_layers = 4 decoder_attention_heads = 6 decoder_ffn_dim = 1536 encoder_ffn_dim = 1536 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 decoder_start_token_id = 50257 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 384 dropout = 0.0 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 scale_embedding = False max_source_positions = 1500 max_target_positions = 448 pad_token_id = 50256 bos_token_id = 50256 eos_token_id = 50256 suppress_tokens = None begin_suppress_tokens = [220, 50256] use_weighted_layer_sum = False classifier_proj_size = 256 apply_spec_augment = False mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 median_filter_width = 7 **kwargs )\n```", "```py\n>>> from transformers import WhisperConfig, WhisperModel\n\n>>> # Initializing a Whisper tiny style configuration\n>>> configuration = WhisperConfig()\n\n>>> # Initializing a model (with random weights) from the tiny style configuration\n>>> model = WhisperModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file normalizer_file = None errors = 'replace' unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' pad_token = None add_prefix_space = False language = None task = None predict_timestamps = False **kwargs )\n```", "```py\n( language: str = None task: str = None predict_timestamps: bool = None )\n```", "```py\n>>> # instantiate the tokenizer and set the prefix token to Spanish\n>>> tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"spanish\")\n>>> # now switch the prefix token from Spanish to French\n>>> tokenizer.set_prefix_tokens(language=\"french\")\n```", "```py\n( token_ids_0 token_ids_1 = None )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( token_ids skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_offsets: bool = False time_precision: float = 0.02 decode_with_timestamps: bool = False normalize: bool = False basic_normalize: bool = False remove_diacritics: bool = False **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( vocab_file = None merges_file = None normalizer_file = None tokenizer_file = None unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' add_prefix_space = False language = None task = None predict_timestamps = False **kwargs )\n```", "```py\n( language: str = None task: str = None predict_timestamps: bool = None )\n```", "```py\n>>> # instantiate the tokenizer and set the prefix token to Spanish\n>>> tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-tiny\", language=\"spanish\")\n>>> # now switch the prefix token from Spanish to French\n>>> tokenizer.set_prefix_tokens(language=\"french\")\n```", "```py\n( token_ids_0 token_ids_1 = None )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( token_ids skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_offsets: bool = False time_precision: float = 0.02 decode_with_timestamps: bool = False normalize: bool = False basic_normalize: bool = False remove_diacritics: bool = False **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( feature_size = 80 sampling_rate = 16000 hop_length = 160 chunk_length = 30 n_fft = 400 padding_value = 0.0 return_attention_mask = False **kwargs )\n```", "```py\n( raw_speech: Union truncation: bool = True pad_to_multiple_of: Optional = None return_tensors: Union = None return_attention_mask: Optional = None padding: Optional = 'max_length' max_length: Optional = None sampling_rate: Optional = None do_normalize: Optional = None **kwargs )\n```", "```py\n( feature_extractor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n( save_directory push_to_hub: bool = False **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: WhisperConfig )\n```", "```py\n( input_features: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None decoder_position_ids: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoFeatureExtractor, WhisperModel\n>>> from datasets import load_dataset\n\n>>> model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n>>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n>>> list(last_hidden_state.shape)\n[1, 2, 512]\n```", "```py\n( input_features: FloatTensor attention_mask: Optional = None )\n```", "```py\n( config: WhisperConfig )\n```", "```py\n( input_features: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None decoder_position_ids: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n\n>>> generated_ids = model.generate(inputs=input_features)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n( input_features: Optional = None generation_config: Optional = None logits_processor: Optional = None stopping_criteria: Optional = None prefix_allowed_tokens_fn: Optional = None synced_gpus: bool = False return_timestamps: Optional = None task: Optional = None language: Optional = None is_multilingual: Optional = None prompt_ids: Optional = None condition_on_prev_tokens: Optional = None temperature: Union = None compression_ratio_threshold: Optional = None logprob_threshold: Optional = None no_speech_threshold: Optional = None num_segment_frames: Optional = None attention_mask: Optional = None time_precision: float = 0.02 return_token_timestamps: Optional = None return_segments: bool = False return_dict_in_generate: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';ModelOutput or torch.LongTensor or Dict[str, Any]\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset, Audio\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model.cuda()\n\n>>> # load audios > 30 seconds\n>>> ds = load_dataset(\"distil-whisper/meanwhile\", \"default\")[\"test\"]\n>>> # resample to 16kHz\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> # take first 8 audios and retrieve array\n>>> audio = ds[:8][\"audio\"]\n>>> audio = [x[\"array\"] for x in audio]\n\n>>> # make sure to NOT truncate the input audio, to return the `attention_mask` and to pad to the longest audio\n>>> inputs = processor(audio, return_tensors=\"pt\", truncation=False, padding=\"longest\", return_attention_mask=True, sampling_rate=16_000)\n>>> inputs = inputs.to(\"cuda\", torch.float32)\n\n>>> # transcribe audio to ids\n>>> generated_ids = model.generate(**inputs)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> transcription[0]\n' Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile!'\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n\n>>> generated_ids = model.generate(inputs=input_features)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_outputs: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import WhisperForCausalLM, WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n\n>>> assistant_model = WhisperForCausalLM.from_pretrained(\"distil-whisper/distil-large-v2\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(\n...     sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\"\n... ).input_features\n\n>>> predicted_ids = model.generate(input_features, assistant_model=assistant_model)\n\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'\n```", "```py\n( config )\n```", "```py\n( input_features: Optional = None head_mask: Optional = None encoder_outputs: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n>>> from datasets import load_dataset\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n>>> model = WhisperForAudioClassification.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n\n>>> ds = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True)\n>>> sample = next(iter(ds))\n\n>>> inputs = feature_extractor(\n...     sample[\"audio\"][\"array\"], sampling_rate=sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n... )\n>>> input_features = inputs.input_features\n\n>>> with torch.no_grad():\n...     logits = model(input_features).logits\n\n>>> predicted_class_ids = torch.argmax(logits).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'Afrikaans'\n```", "```py\n( config: WhisperConfig **kwargs )\n```", "```py\n( input_features: TFModelInputType | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None decoder_position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None cross_attn_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None decoder_inputs_embeds: Optional[Tuple[Union[np.ndarray, tf.Tensor]]] = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import TFWhisperModel, AutoFeatureExtractor\n>>> from datasets import load_dataset\n\n>>> model = TFWhisperModel.from_pretrained(\"openai/whisper-base\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"tf\")\n>>> input_features = inputs.input_features\n>>> decoder_input_ids = tf.convert_to_tensor([[1, 1]]) * model.config.decoder_start_token_id\n>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n>>> list(last_hidden_state.shape)\n[1, 2, 512]\n```", "```py\n( config: WhisperConfig **kwargs )\n```", "```py\n( input_features: TFModelInputType | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None decoder_position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None cross_attn_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None decoder_inputs_embeds: Optional[Tuple[Union[np.ndarray, tf.Tensor]]] = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoProcessor, TFWhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"tf\")\n>>> input_features = inputs.input_features\n\n>>> generated_ids = model.generate(input_features=input_features)\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n( config: WhisperConfig input_shape: Tuple = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_features: Array decoder_input_ids: Array attention_mask: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxWhisperModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-tiny\")\n>>> model = FlaxWhisperModel.from_pretrained(\"openai/whisper-tiny\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: WhisperConfig input_shape: Tuple = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_features: Array decoder_input_ids: Array attention_mask: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import WhisperProcessor, FlaxWhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = FlaxWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\", from_pt=True)\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"np\")\n>>> input_features = inputs.input_features\n>>> generated_ids = model.generate(input_ids=input_features)\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> transcription\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```", "```py\n( config: WhisperConfig input_shape: Tuple = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_features: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoFeatureExtractor, FlaxWhisperForAudioClassification\n>>> from datasets import load_dataset\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n>>> model = FlaxWhisperForAudioClassification.from_pretrained(\n...     \"sanchit-gandhi/whisper-medium-fleurs-lang-id\", from_pt=True\n... )\n>>> ds = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True)\n\n>>> sample = next(iter(ds))\n\n>>> inputs = feature_extractor(\n...     sample[\"audio\"][\"array\"], sampling_rate=sample[\"audio\"][\"sampling_rate\"], return_tensors=\"np\"\n... )\n>>> input_features = inputs.input_features\n\n>>> logits = model(input_features).logits\n\n>>> predicted_class_ids = jnp.argmax(logits).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'af_za'\n```"]