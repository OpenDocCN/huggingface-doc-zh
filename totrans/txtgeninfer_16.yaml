- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/quantization](https://huggingface.co/docs/text-generation-inference/conceptual/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TGI offers GPTQ and bits-and-bytes quantization to quantize large language models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with GPTQ
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPTQ is a post-training quantization method to make the model smaller. It quantizes
    the layers by finding a compressed version of that weight, that will yield a minimum
    mean squared error like below 👇
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a layer<math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math>l
    with weight matrix<math><semantics><mrow><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">W_{l}</annotation></semantics></math>Wl​ and layer
    input<math><semantics><mrow><msub><mi>X</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">X_{l}</annotation></semantics></math>Xl​, find quantized
    weight<math><semantics><mrow><mi>h</mi><mi>a</mi><mi>t</mi><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">\\hat{W}_{l}</annotation></semantics></math>hatWl​:
    <math display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mo lspace="0em" rspace="0em">∗</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mover
    accent="true"><msub><mi>W</mi><mi>l</mi></msub><mo>^</mo></mover></msub><mi mathvariant="normal">∣</mi><mi
    mathvariant="normal">∣</mi><msub><mi>W</mi><mi>l</mi></msub><mi>X</mi><mo>−</mo><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mi>X</mi><mi mathvariant="normal">∣</mi><msubsup><mi
    mathvariant="normal">∣</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} ||W_{l}X-\hat{W}_{l}X||^{2}_{2})</annotation></semantics></math>(W^l​∗=argminWl​^​​∣∣Wl​X−W^l​X∣∣22​)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: TGI allows you to both run an already GPTQ quantized model (see available models
    [here](https://huggingface.co/models?search=gptq)) or quantize a model of your
    choice using quantization script. You can run a quantized model by simply passing
    —quantize like below 👇
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that TGI’s GPTQ implementation doesn’t use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    under the hood. However, models quantized using AutoGPTQ or Optimum can still
    be served by TGI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: To quantize a given model using GPTQ with a calibration dataset, simply run
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will create a new directory with the quantized files which you can use
    with,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can learn more about the quantization options by running `text-generation-server
    quantize --help`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to do more with GPTQ models (e.g. train an adapter on top), you
    can read about transformers GPTQ integration [here](https://huggingface.co/blog/gptq-integration).
    You can learn more about GPTQ from the [paper](https://arxiv.org/pdf/2210.17323.pdf).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with bitsandbytes
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: bitsandbytes is a library used to apply 8-bit and 4-bit quantization to models.
    Unlike GPTQ quantization, bitsandbytes doesn’t require a calibration dataset or
    any post-processing – weights are automatically quantized on load. However, inference
    with bitsandbytes is slower than GPTQ or FP16 precision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 8-bit quantization enables multi-billion parameter scale models to fit in smaller
    hardware without degrading performance too much. In TGI, you can use 8-bit quantization
    by adding `--quantize bitsandbytes` like below 👇
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '4-bit quantization is also possible with bitsandbytes. You can choose one of
    the following 4-bit data types: 4-bit float (`fp4`), or 4-bit `NormalFloat` (`nf4`).
    These data types were introduced in the context of parameter-efficient fine-tuning,
    but you can apply them for inference by automatically converting the model weights
    on load.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In TGI, you can use 4-bit quantization by adding `--quantize bitsandbytes-nf4`
    or `--quantize bitsandbytes-fp4` like below 👇
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TGI 中，您可以通过添加 `--quantize bitsandbytes-nf4` 或 `--quantize bitsandbytes-fp4`
    来使用 4 位量化，就像下面这样👇
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can get more information about 8-bit quantization by reading this [blog
    post](https://huggingface.co/blog/hf-bitsandbytes-integration), and 4-bit quantization
    by reading [this blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过阅读这篇[博客文章](https://huggingface.co/blog/hf-bitsandbytes-integration)了解更多关于
    8 位量化的信息，以及通过阅读[这篇博客文章](https://huggingface.co/blog/4bit-transformers-bitsandbytes)了解
    4 位量化。
