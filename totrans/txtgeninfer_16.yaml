- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/quantization](https://huggingface.co/docs/text-generation-inference/conceptual/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/text-generation-inference/conceptual/quantization](https://huggingface.co/docs/text-generation-inference/conceptual/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TGI offers GPTQ and bits-and-bytes quantization to quantize large language models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TGIæä¾›äº†GPTQå’Œbits-and-bytesé‡åŒ–ï¼Œä»¥é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- en: Quantization with GPTQ
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨GPTQè¿›è¡Œé‡åŒ–
- en: GPTQ is a post-training quantization method to make the model smaller. It quantizes
    the layers by finding a compressed version of that weight, that will yield a minimum
    mean squared error like below ğŸ‘‡
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQæ˜¯ä¸€ç§åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å°ã€‚å®ƒé€šè¿‡æ‰¾åˆ°è¯¥æƒé‡çš„å‹ç¼©ç‰ˆæœ¬æ¥é‡åŒ–å±‚ï¼Œè¿™å°†äº§ç”Ÿä¸€ä¸ªæœ€å°å‡æ–¹è¯¯å·®ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡
- en: 'Given a layer<math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math>l
    with weight matrix<math><semantics><mrow><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">W_{l}</annotation></semantics></math>Wlâ€‹ and layer
    input<math><semantics><mrow><msub><mi>X</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">X_{l}</annotation></semantics></math>Xlâ€‹, find quantized
    weight<math><semantics><mrow><mi>h</mi><mi>a</mi><mi>t</mi><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">\\hat{W}_{l}</annotation></semantics></math>hatWlâ€‹:
    <math display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mo lspace="0em" rspace="0em">âˆ—</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mover
    accent="true"><msub><mi>W</mi><mi>l</mi></msub><mo>^</mo></mover></msub><mi mathvariant="normal">âˆ£</mi><mi
    mathvariant="normal">âˆ£</mi><msub><mi>W</mi><mi>l</mi></msub><mi>X</mi><mo>âˆ’</mo><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mi>X</mi><mi mathvariant="normal">âˆ£</mi><msubsup><mi
    mathvariant="normal">âˆ£</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} ||W_{l}X-\hat{W}_{l}X||^{2}_{2})</annotation></semantics></math>(W^lâ€‹âˆ—=argminWlâ€‹^â€‹â€‹âˆ£âˆ£Wlâ€‹Xâˆ’W^lâ€‹Xâˆ£âˆ£22â€‹)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªå¸¦æœ‰æƒé‡çŸ©é˜µ<math><semantics><mrow><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">W_{l}</annotation></semantics></math>Wlå’Œå±‚è¾“å…¥<math><semantics><mrow><msub><mi>X</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">X_{l}</annotation></semantics></math>Xlçš„å±‚<math><semantics><mrow><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l</annotation></semantics></math>lï¼Œæ‰¾åˆ°é‡åŒ–æƒé‡<math><semantics><mrow><mi>h</mi><mi>a</mi><mi>t</mi><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">\\hat{W}_{l}</annotation></semantics></math>hatWlï¼š
    <math display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mo lspace="0em" rspace="0em">âˆ—</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mover
    accent="true"><msub><mi>W</mi><mi>l</mi></msub><mo>^</mo></mover></msub><mi mathvariant="normal">âˆ£</mi><mi
    mathvariant="normal">âˆ£</mi><msub><mi>W</mi><mi>l</mi></msub><mi>X</mi><mo>âˆ’</mo><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mi>X</mi><mi mathvariant="normal">âˆ£</mi><msubsup><mi
    mathvariant="normal">âˆ£</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} ||W_{l}X-\hat{W}_{l}X||^{2}_{2})</annotation></semantics></math>(W^lâ€‹âˆ—=argminWlâ€‹^â€‹â€‹âˆ£âˆ£Wlâ€‹Xâˆ’W^lâ€‹Xâˆ£âˆ£22â€‹)
- en: TGI allows you to both run an already GPTQ quantized model (see available models
    [here](https://huggingface.co/models?search=gptq)) or quantize a model of your
    choice using quantization script. You can run a quantized model by simply passing
    â€”quantize like below ğŸ‘‡
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TGIå…è®¸æ‚¨è¿è¡Œå·²ç»GPTQé‡åŒ–çš„æ¨¡å‹ï¼ˆæŸ¥çœ‹å¯ç”¨æ¨¡å‹[æ­¤å¤„](https://huggingface.co/models?search=gptq)ï¼‰æˆ–ä½¿ç”¨é‡åŒ–è„šæœ¬é‡åŒ–æ‚¨é€‰æ‹©çš„æ¨¡å‹ã€‚æ‚¨å¯ä»¥é€šè¿‡ç®€å•åœ°ä¼ é€’â€”quantizeæ¥è¿è¡Œä¸€ä¸ªé‡åŒ–æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that TGIâ€™s GPTQ implementation doesnâ€™t use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    under the hood. However, models quantized using AutoGPTQ or Optimum can still
    be served by TGI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒTGIçš„GPTQå®ç°ä¸ä½¿ç”¨[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ä½œä¸ºåº•å±‚ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨AutoGPTQæˆ–Optimumé‡åŒ–çš„æ¨¡å‹ä»ç„¶å¯ä»¥è¢«TGIæä¾›æœåŠ¡ã€‚
- en: To quantize a given model using GPTQ with a calibration dataset, simply run
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨GPTQå¯¹ç»™å®šæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œåªéœ€è¿è¡Œ
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will create a new directory with the quantized files which you can use
    with,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«é‡åŒ–æ–‡ä»¶çš„æ–°ç›®å½•ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ“ä½œï¼Œ
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can learn more about the quantization options by running `text-generation-server
    quantize --help`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ`text-generation-server quantize --help`æ¥äº†è§£æ›´å¤šæœ‰å…³é‡åŒ–é€‰é¡¹çš„ä¿¡æ¯ã€‚
- en: If you wish to do more with GPTQ models (e.g. train an adapter on top), you
    can read about transformers GPTQ integration [here](https://huggingface.co/blog/gptq-integration).
    You can learn more about GPTQ from the [paper](https://arxiv.org/pdf/2210.17323.pdf).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¸Œæœ›å¯¹GPTQæ¨¡å‹è¿›è¡Œæ›´å¤šæ“ä½œï¼ˆä¾‹å¦‚åœ¨é¡¶éƒ¨è®­ç»ƒä¸€ä¸ªé€‚é…å™¨ï¼‰ï¼Œæ‚¨å¯ä»¥é˜…è¯»æœ‰å…³transformers GPTQé›†æˆçš„ä¿¡æ¯[æ­¤å¤„](https://huggingface.co/blog/gptq-integration)ã€‚æ‚¨å¯ä»¥ä»[è®ºæ–‡](https://arxiv.org/pdf/2210.17323.pdf)ä¸­äº†è§£æ›´å¤šå…³äºGPTQçš„ä¿¡æ¯ã€‚
- en: Quantization with bitsandbytes
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨bitsandbytesè¿›è¡Œé‡åŒ–
- en: bitsandbytes is a library used to apply 8-bit and 4-bit quantization to models.
    Unlike GPTQ quantization, bitsandbytes doesnâ€™t require a calibration dataset or
    any post-processing â€“ weights are automatically quantized on load. However, inference
    with bitsandbytes is slower than GPTQ or FP16 precision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytesæ˜¯ä¸€ä¸ªç”¨äºå°†8ä½å’Œ4ä½é‡åŒ–åº”ç”¨äºæ¨¡å‹çš„åº“ã€‚ä¸GPTQé‡åŒ–ä¸åŒï¼Œbitsandbytesä¸éœ€è¦æ ¡å‡†æ•°æ®é›†æˆ–ä»»ä½•åå¤„ç†-æƒé‡ä¼šåœ¨åŠ è½½æ—¶è‡ªåŠ¨é‡åŒ–ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨bitsandbytesè¿›è¡Œæ¨æ–­æ¯”GPTQæˆ–FP16ç²¾åº¦è¦æ…¢ã€‚
- en: 8-bit quantization enables multi-billion parameter scale models to fit in smaller
    hardware without degrading performance too much. In TGI, you can use 8-bit quantization
    by adding `--quantize bitsandbytes` like below ğŸ‘‡
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 8ä½é‡åŒ–ä½¿å¾—å¤šåäº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹å¯ä»¥é€‚åº”æ›´å°çš„ç¡¬ä»¶è€Œä¸ä¼šå¤ªå¤§ç¨‹åº¦åœ°é™ä½æ€§èƒ½ã€‚åœ¨TGIä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ `--quantize bitsandbytes`æ¥ä½¿ç”¨8ä½é‡åŒ–ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '4-bit quantization is also possible with bitsandbytes. You can choose one of
    the following 4-bit data types: 4-bit float (`fp4`), or 4-bit `NormalFloat` (`nf4`).
    These data types were introduced in the context of parameter-efficient fine-tuning,
    but you can apply them for inference by automatically converting the model weights
    on load.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 4ä½é‡åŒ–ä¹Ÿå¯ä»¥ä½¿ç”¨bitsandbytesã€‚æ‚¨å¯ä»¥é€‰æ‹©ä»¥ä¸‹4ä½æ•°æ®ç±»å‹ä¹‹ä¸€ï¼š4ä½æµ®ç‚¹æ•°ï¼ˆ`fp4`ï¼‰æˆ–4ä½`NormalFloat`ï¼ˆ`nf4`ï¼‰ã€‚è¿™äº›æ•°æ®ç±»å‹æ˜¯åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒçš„èƒŒæ™¯ä¸‹å¼•å…¥çš„ï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡åœ¨åŠ è½½æ—¶è‡ªåŠ¨è½¬æ¢æ¨¡å‹æƒé‡æ¥å°†å…¶åº”ç”¨äºæ¨æ–­ã€‚
- en: In TGI, you can use 4-bit quantization by adding `--quantize bitsandbytes-nf4`
    or `--quantize bitsandbytes-fp4` like below ğŸ‘‡
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ TGI ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ  `--quantize bitsandbytes-nf4` æˆ– `--quantize bitsandbytes-fp4`
    æ¥ä½¿ç”¨ 4 ä½é‡åŒ–ï¼Œå°±åƒä¸‹é¢è¿™æ ·ğŸ‘‡
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can get more information about 8-bit quantization by reading this [blog
    post](https://huggingface.co/blog/hf-bitsandbytes-integration), and 4-bit quantization
    by reading [this blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡é˜…è¯»è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)äº†è§£æ›´å¤šå…³äº
    8 ä½é‡åŒ–çš„ä¿¡æ¯ï¼Œä»¥åŠé€šè¿‡é˜…è¯»[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)äº†è§£
    4 ä½é‡åŒ–ã€‚
