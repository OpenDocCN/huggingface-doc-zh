- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/quantization](https://huggingface.co/docs/text-generation-inference/conceptual/quantization)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: TGI offers GPTQ and bits-and-bytes quantization to quantize large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with GPTQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPTQ is a post-training quantization method to make the model smaller. It quantizes
    the layers by finding a compressed version of that weight, that will yield a minimum
    mean squared error like below ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a layer<math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math>l
    with weight matrix<math><semantics><mrow><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">W_{l}</annotation></semantics></math>Wlâ€‹ and layer
    input<math><semantics><mrow><msub><mi>X</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">X_{l}</annotation></semantics></math>Xlâ€‹, find quantized
    weight<math><semantics><mrow><mi>h</mi><mi>a</mi><mi>t</mi><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">\\hat{W}_{l}</annotation></semantics></math>hatWlâ€‹:
    <math display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mo lspace="0em" rspace="0em">âˆ—</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mover
    accent="true"><msub><mi>W</mi><mi>l</mi></msub><mo>^</mo></mover></msub><mi mathvariant="normal">âˆ£</mi><mi
    mathvariant="normal">âˆ£</mi><msub><mi>W</mi><mi>l</mi></msub><mi>X</mi><mo>âˆ’</mo><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mi>X</mi><mi mathvariant="normal">âˆ£</mi><msubsup><mi
    mathvariant="normal">âˆ£</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} ||W_{l}X-\hat{W}_{l}X||^{2}_{2})</annotation></semantics></math>(W^lâ€‹âˆ—=argminWlâ€‹^â€‹â€‹âˆ£âˆ£Wlâ€‹Xâˆ’W^lâ€‹Xâˆ£âˆ£22â€‹)'
  prefs: []
  type: TYPE_NORMAL
- en: TGI allows you to both run an already GPTQ quantized model (see available models
    [here](https://huggingface.co/models?search=gptq)) or quantize a model of your
    choice using quantization script. You can run a quantized model by simply passing
    â€”quantize like below ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that TGIâ€™s GPTQ implementation doesnâ€™t use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    under the hood. However, models quantized using AutoGPTQ or Optimum can still
    be served by TGI.
  prefs: []
  type: TYPE_NORMAL
- en: To quantize a given model using GPTQ with a calibration dataset, simply run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will create a new directory with the quantized files which you can use
    with,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can learn more about the quantization options by running `text-generation-server
    quantize --help`.
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to do more with GPTQ models (e.g. train an adapter on top), you
    can read about transformers GPTQ integration [here](https://huggingface.co/blog/gptq-integration).
    You can learn more about GPTQ from the [paper](https://arxiv.org/pdf/2210.17323.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with bitsandbytes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: bitsandbytes is a library used to apply 8-bit and 4-bit quantization to models.
    Unlike GPTQ quantization, bitsandbytes doesnâ€™t require a calibration dataset or
    any post-processing â€“ weights are automatically quantized on load. However, inference
    with bitsandbytes is slower than GPTQ or FP16 precision.
  prefs: []
  type: TYPE_NORMAL
- en: 8-bit quantization enables multi-billion parameter scale models to fit in smaller
    hardware without degrading performance too much. In TGI, you can use 8-bit quantization
    by adding `--quantize bitsandbytes` like below ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '4-bit quantization is also possible with bitsandbytes. You can choose one of
    the following 4-bit data types: 4-bit float (`fp4`), or 4-bit `NormalFloat` (`nf4`).
    These data types were introduced in the context of parameter-efficient fine-tuning,
    but you can apply them for inference by automatically converting the model weights
    on load.'
  prefs: []
  type: TYPE_NORMAL
- en: In TGI, you can use 4-bit quantization by adding `--quantize bitsandbytes-nf4`
    or `--quantize bitsandbytes-fp4` like below ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can get more information about 8-bit quantization by reading this [blog
    post](https://huggingface.co/blog/hf-bitsandbytes-integration), and 4-bit quantization
    by reading [this blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  prefs: []
  type: TYPE_NORMAL
