- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/quantization](https://huggingface.co/docs/text-generation-inference/conceptual/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TGI offers GPTQ and bits-and-bytes quantization to quantize large language models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with GPTQ
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPTQ is a post-training quantization method to make the model smaller. It quantizes
    the layers by finding a compressed version of that weight, that will yield a minimum
    mean squared error like below ğŸ‘‡
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a layer<math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math>l
    with weight matrix<math><semantics><mrow><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">W_{l}</annotation></semantics></math>Wlâ€‹ and layer
    input<math><semantics><mrow><msub><mi>X</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">X_{l}</annotation></semantics></math>Xlâ€‹, find quantized
    weight<math><semantics><mrow><mi>h</mi><mi>a</mi><mi>t</mi><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation
    encoding="application/x-tex">\\hat{W}_{l}</annotation></semantics></math>hatWlâ€‹:
    <math display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mo lspace="0em" rspace="0em">âˆ—</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mover
    accent="true"><msub><mi>W</mi><mi>l</mi></msub><mo>^</mo></mover></msub><mi mathvariant="normal">âˆ£</mi><mi
    mathvariant="normal">âˆ£</mi><msub><mi>W</mi><mi>l</mi></msub><mi>X</mi><mo>âˆ’</mo><msub><mover
    accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mi>X</mi><mi mathvariant="normal">âˆ£</mi><msubsup><mi
    mathvariant="normal">âˆ£</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} ||W_{l}X-\hat{W}_{l}X||^{2}_{2})</annotation></semantics></math>(W^lâ€‹âˆ—=argminWlâ€‹^â€‹â€‹âˆ£âˆ£Wlâ€‹Xâˆ’W^lâ€‹Xâˆ£âˆ£22â€‹)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: TGI allows you to both run an already GPTQ quantized model (see available models
    [here](https://huggingface.co/models?search=gptq)) or quantize a model of your
    choice using quantization script. You can run a quantized model by simply passing
    â€”quantize like below ğŸ‘‡
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that TGIâ€™s GPTQ implementation doesnâ€™t use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    under the hood. However, models quantized using AutoGPTQ or Optimum can still
    be served by TGI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: To quantize a given model using GPTQ with a calibration dataset, simply run
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will create a new directory with the quantized files which you can use
    with,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can learn more about the quantization options by running `text-generation-server
    quantize --help`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to do more with GPTQ models (e.g. train an adapter on top), you
    can read about transformers GPTQ integration [here](https://huggingface.co/blog/gptq-integration).
    You can learn more about GPTQ from the [paper](https://arxiv.org/pdf/2210.17323.pdf).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with bitsandbytes
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: bitsandbytes is a library used to apply 8-bit and 4-bit quantization to models.
    Unlike GPTQ quantization, bitsandbytes doesnâ€™t require a calibration dataset or
    any post-processing â€“ weights are automatically quantized on load. However, inference
    with bitsandbytes is slower than GPTQ or FP16 precision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 8-bit quantization enables multi-billion parameter scale models to fit in smaller
    hardware without degrading performance too much. In TGI, you can use 8-bit quantization
    by adding `--quantize bitsandbytes` like below ğŸ‘‡
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '4-bit quantization is also possible with bitsandbytes. You can choose one of
    the following 4-bit data types: 4-bit float (`fp4`), or 4-bit `NormalFloat` (`nf4`).
    These data types were introduced in the context of parameter-efficient fine-tuning,
    but you can apply them for inference by automatically converting the model weights
    on load.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In TGI, you can use 4-bit quantization by adding `--quantize bitsandbytes-nf4`
    or `--quantize bitsandbytes-fp4` like below ğŸ‘‡
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ TGI ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ  `--quantize bitsandbytes-nf4` æˆ– `--quantize bitsandbytes-fp4`
    æ¥ä½¿ç”¨ 4 ä½é‡åŒ–ï¼Œå°±åƒä¸‹é¢è¿™æ ·ğŸ‘‡
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can get more information about 8-bit quantization by reading this [blog
    post](https://huggingface.co/blog/hf-bitsandbytes-integration), and 4-bit quantization
    by reading [this blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡é˜…è¯»è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)äº†è§£æ›´å¤šå…³äº
    8 ä½é‡åŒ–çš„ä¿¡æ¯ï¼Œä»¥åŠé€šè¿‡é˜…è¯»[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)äº†è§£
    4 ä½é‡åŒ–ã€‚
