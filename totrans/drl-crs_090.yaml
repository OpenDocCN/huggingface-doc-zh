- en: 'Self-Play: a classic technique to train competitive agents in adversarial games'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit7/self-play](https://huggingface.co/learn/deep-rl-course/unit7/self-play)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/78.c6eb4218.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve studied the basics of multi-agents, we’re ready to go deeper.
    As mentioned in the introduction, we’re going **to train agents in an adversarial
    game with SoccerTwos, a 2vs2 game**.
  prefs: []
  type: TYPE_NORMAL
- en: '![SoccerTwos](../Images/b8d7d800c316a50a5f64472742088b73.png)'
  prefs: []
  type: TYPE_IMG
- en: This environment was made by the [Unity MLAgents Team](https://github.com/Unity-Technologies/ml-agents)
  prefs: []
  type: TYPE_NORMAL
- en: What is Self-Play?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training agents correctly in an adversarial game can be **quite complex**.
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, we need to find how to get a well-trained opponent to play
    against your training agent. And on the other hand, if you find a very good trained
    opponent, how will your agent improve its policy when the opponent is too strong?
  prefs: []
  type: TYPE_NORMAL
- en: Think of a child that just started to learn soccer. Playing against a very good
    soccer player will be useless since it will be too hard to win or at least get
    the ball from time to time. So the child will continuously lose without having
    time to learn a good policy.
  prefs: []
  type: TYPE_NORMAL
- en: The best solution would be **to have an opponent that is on the same level as
    the agent and will upgrade its level as the agent upgrades its own**. Because
    if the opponent is too strong, we’ll learn nothing; if it is too weak, we’ll overlearn
    useless behavior against a stronger opponent then.
  prefs: []
  type: TYPE_NORMAL
- en: This solution is called *self-play*. In self-play, **the agent uses former copies
    of itself (of its policy) as an opponent**. This way, the agent will play against
    an agent of the same level (challenging but not too much), have opportunities
    to gradually improve its policy, and then update its opponent as it becomes better.
    It’s a way to bootstrap an opponent and progressively increase the opponent’s
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s the same way humans learn in competition:'
  prefs: []
  type: TYPE_NORMAL
- en: We start to train against an opponent of similar level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we learn from it, and when we acquire some skills, we can move further
    with stronger opponents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We do the same with self-play:'
  prefs: []
  type: TYPE_NORMAL
- en: We **start with a copy of our agent as an opponent** this way, this opponent
    is on a similar level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We **learn from it** and, when we acquire some skills, we **update our opponent
    with a more recent copy of our training policy**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theory behind self-play is not something new. It was already used by Arthur
    Samuel’s checker player system in the fifties and by Gerald Tesauro’s TD-Gammon
    in 1995\. If you want to learn more about the history of self-play [check out
    this very good blogpost by Andrew Cohen](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents)
  prefs: []
  type: TYPE_NORMAL
- en: Self-Play in MLAgents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-Play is integrated into the MLAgents library and is managed by multiple
    hyperparameters that we’re going to study. But the main focus, as explained in
    the documentation, is the **tradeoff between the skill level and generality of
    the final policy and the stability of learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Training against a set of slowly changing or unchanging adversaries with low
    diversity **results in more stable training. But a risk to overfit if the change
    is too slow.**
  prefs: []
  type: TYPE_NORMAL
- en: 'So we need to control:'
  prefs: []
  type: TYPE_NORMAL
- en: How **often we change opponents** with the `swap_steps` and `team_change` parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **number of opponents saved** with the `window` parameter. A larger value
    of `window`  means that an agent’s pool of opponents will contain a larger diversity
    of behaviors since it will contain policies from earlier in the training run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **probability of playing against the current self vs opponent** sampled
    from the pool with `play_against_latest_model_ratio`. A larger value of `play_against_latest_model_ratio`
     indicates that an agent will be playing against the current opponent more often.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **number of training steps before saving a new opponent** with `save_steps`
    parameters. A larger value of `save_steps`  will yield a set of opponents that
    cover a wider range of skill levels and possibly play styles since the policy
    receives more training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get more details about these hyperparameters, you definitely need [to check
    out this part of the documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#self-play)
  prefs: []
  type: TYPE_NORMAL
- en: The ELO Score to evaluate our agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is ELO Score?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In adversarial games, tracking the **cumulative reward is not always a meaningful
    metric to track the learning progress:** because this metric is **dependent only
    on the skill of the opponent.**
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we’re using an ***ELO rating system*** (named after Arpad Elo) that
    calculates the **relative skill level** between 2 players from a given population
    in a zero-sum game.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a zero-sum game: one agent wins, and the other agent loses. It’s a mathematical
    representation of a situation in which each participant’s gain or loss of utility
    **is exactly balanced by the gain or loss of the utility of the other participants.**
    We talk about zero-sum games because the sum of utility is equal to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This ELO (starting at a specific score: frequently 1200) can decrease initially
    but should increase progressively during the training.'
  prefs: []
  type: TYPE_NORMAL
- en: The Elo system is **inferred from the losses and draws against other players.**
    It means that player ratings depend **on the ratings of their opponents and the
    results scored against them.**
  prefs: []
  type: TYPE_NORMAL
- en: Elo defines an Elo score that is the relative skills of a player in a zero-sum
    game. **We say relative because it depends on the performance of opponents.**
  prefs: []
  type: TYPE_NORMAL
- en: The central idea is to think of the performance of a player **as a random variable
    that is normally distributed.**
  prefs: []
  type: TYPE_NORMAL
- en: The difference in rating between 2 players serves as **the predictor of the
    outcomes of a match.** If the player wins, but the probability of winning is high,
    it will only win a few points from its opponent since it means that it is much
    stronger than it.
  prefs: []
  type: TYPE_NORMAL
- en: 'After every game:'
  prefs: []
  type: TYPE_NORMAL
- en: The winning player takes **points from the losing one.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of points is determined **by the difference in the 2 players ratings
    (hence relative).**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the higher-rated player wins → few points will be taken from the lower-rated
    player.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the lower-rated player wins → a lot of points will be taken from the high-rated
    player.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If it’s a draw → the lower-rated player gains a few points from the higher.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So if A and B have rating Ra, and Rb, then the **expected scores are** given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ELO Score](../Images/170b63d61ac8a30759838921d82860c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, at the end of the game, we need to update the player’s actual Elo score.
    We use a linear adjustment **proportional to the amount by which the player over-performed
    or under-performed.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We also define a maximum adjustment rating per game: K-factor.'
  prefs: []
  type: TYPE_NORMAL
- en: K=16 for master.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K=32 for weaker players.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If Player A has Ea points but scored Sa points, then the player’s rating is
    updated using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ELO Score](../Images/1e0296c75fde41fbfd850f8d5b2c8cdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: Player A has a rating of 2600
  prefs: []
  type: TYPE_NORMAL
- en: Player B has a rating of 2300
  prefs: []
  type: TYPE_NORMAL
- en: 'We first calculate the expected score: <math><semantics><mrow><msub><mi>E</mi><mi>A</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>1</mn><msup><mn>0</mn><mrow><mo
    stretchy="false">(</mo><mn>2300</mn><mo>−</mo><mn>2600</mn><mo stretchy="false">)</mo><mi
    mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac><mo>=</mo><mn>0.849</mn></mrow><annotation
    encoding="application/x-tex">E_{A} = \frac{1}{1+10^{(2300-2600)/400}} = 0.849</annotation></semantics></math>
    EA​=1+10(2300−2600)/4001​=0.849 <math><semantics><mrow><msub><mi>E</mi><mi>B</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>1</mn><msup><mn>0</mn><mrow><mo
    stretchy="false">(</mo><mn>2600</mn><mo>−</mo><mn>2300</mn><mo stretchy="false">)</mo><mi
    mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac><mo>=</mo><mn>0.151</mn></mrow><annotation
    encoding="application/x-tex">E_{B} = \frac{1}{1+10^{(2600-2300)/400}} = 0.151</annotation></semantics></math>
    EB​=1+10(2600−2300)/4001​=0.151'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the organizers determined that K=16 and A wins, the new rating would be:
    <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>A</mi></msub><mo>=</mo><mn>2600</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>0.849</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2602</mn></mrow><annotation
    encoding="application/x-tex">ELO_A = 2600 + 16*(1-0.849) = 2602</annotation></semantics></math>
    ELOA​=2600+16∗(1−0.849)=2602 <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>B</mi></msub><mo>=</mo><mn>2300</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo>−</mo><mn>0.151</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2298</mn></mrow><annotation
    encoding="application/x-tex">ELO_B = 2300 + 16*(0-0.151) = 2298</annotation></semantics></math>
    ELOB​=2300+16∗(0−0.151)=2298'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the organizers determined that K=16 and B wins, the new rating would be:
    <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>A</mi></msub><mo>=</mo><mn>2600</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo>−</mo><mn>0.849</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2586</mn></mrow><annotation
    encoding="application/x-tex">ELO_A = 2600 + 16*(0-0.849) = 2586</annotation></semantics></math>
    ELOA​=2600+16∗(0−0.849)=2586 <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>B</mi></msub><mo>=</mo><mn>2300</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>0.151</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2314</mn></mrow><annotation
    encoding="application/x-tex">ELO_B = 2300 + 16 *(1-0.151) = 2314</annotation></semantics></math>
    ELOB​=2300+16∗(1−0.151)=2314'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Advantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the ELO score has multiple advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Points are **always balanced** (more points are exchanged when there is an unexpected
    outcome, but the sum is always the same).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a **self-corrected system** since if a player wins against a weak player,
    they will only win a few points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It **works with team games**: we calculate the average for each team and use
    it in Elo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Disadvantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ELO **does not take into account the individual contribution** of each people
    in the team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rating deflation: **a good rating requires skill over time to keep the same
    rating**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Can’t compare rating in history**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
