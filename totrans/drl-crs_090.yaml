- en: 'Self-Play: a classic technique to train competitive agents in adversarial games'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自我对弈：在对抗性游戏中训练竞争性代理的经典技术
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit7/self-play](https://huggingface.co/learn/deep-rl-course/unit7/self-play)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit7/self-play](https://huggingface.co/learn/deep-rl-course/unit7/self-play)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve studied the basics of multi-agents, we’re ready to go deeper.
    As mentioned in the introduction, we’re going **to train agents in an adversarial
    game with SoccerTwos, a 2vs2 game**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了多智能体的基础知识，我们准备深入研究。正如在介绍中提到的，我们将**在SoccerTwos中进行2对2的对抗游戏训练代理**。
- en: '![SoccerTwos](../Images/b8d7d800c316a50a5f64472742088b73.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![SoccerTwos](../Images/b8d7d800c316a50a5f64472742088b73.png)'
- en: This environment was made by the [Unity MLAgents Team](https://github.com/Unity-Technologies/ml-agents)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境是由[Unity MLAgents团队](https://github.com/Unity-Technologies/ml-agents)制作的
- en: What is Self-Play?
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是自我对弈？
- en: Training agents correctly in an adversarial game can be **quite complex**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗性游戏中正确训练代理可能会**非常复杂**。
- en: On the one hand, we need to find how to get a well-trained opponent to play
    against your training agent. And on the other hand, if you find a very good trained
    opponent, how will your agent improve its policy when the opponent is too strong?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们需要找到一个训练有素的对手来对抗你的训练代理。另一方面，如果你找到一个训练有素的对手，当对手太强时，你的代理如何改进其策略？
- en: Think of a child that just started to learn soccer. Playing against a very good
    soccer player will be useless since it will be too hard to win or at least get
    the ball from time to time. So the child will continuously lose without having
    time to learn a good policy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个刚开始学习踢足球的孩子。与一位非常优秀的足球运动员对战将是没有意义的，因为很难赢得比赛或者至少偶尔抢到球。所以孩子将不断失败，没有时间学习一个好的策略。
- en: The best solution would be **to have an opponent that is on the same level as
    the agent and will upgrade its level as the agent upgrades its own**. Because
    if the opponent is too strong, we’ll learn nothing; if it is too weak, we’ll overlearn
    useless behavior against a stronger opponent then.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的解决方案是**有一个与代理水平相同的对手，并且随着代理提升自身水平而提升对手水平**。因为如果对手太强，我们将学不到东西；如果对手太弱，我们将在与更强对手对战时学到无用的行为。
- en: This solution is called *self-play*. In self-play, **the agent uses former copies
    of itself (of its policy) as an opponent**. This way, the agent will play against
    an agent of the same level (challenging but not too much), have opportunities
    to gradually improve its policy, and then update its opponent as it becomes better.
    It’s a way to bootstrap an opponent and progressively increase the opponent’s
    complexity.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案被称为*自我对弈*。在自我对弈中，**代理使用自身以前的副本（其策略）作为对手**。这样，代理将与同一水平的代理对战（具有挑战性但不会太难），有机会逐渐改进其策略，然后在变得更好时更新其对手。这是一种启动对手并逐渐增加对手复杂性的方法。
- en: 'It’s the same way humans learn in competition:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是人类在竞争中学习的方式：
- en: We start to train against an opponent of similar level
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们开始训练与相似水平的对手对战
- en: Then we learn from it, and when we acquire some skills, we can move further
    with stronger opponents.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们从中学习，当我们获得一些技能时，我们可以继续与更强的对手对战。
- en: 'We do the same with self-play:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对自我对弈也是一样的：
- en: We **start with a copy of our agent as an opponent** this way, this opponent
    is on a similar level.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们**从我们的代理副本开始作为对手**，这样，这个对手就处于相似水平。
- en: We **learn from it** and, when we acquire some skills, we **update our opponent
    with a more recent copy of our training policy**.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从中**学习**，当我们获得一些技能时，我们会**用我们训练策略的最新副本更新我们的对手**。
- en: The theory behind self-play is not something new. It was already used by Arthur
    Samuel’s checker player system in the fifties and by Gerald Tesauro’s TD-Gammon
    in 1995\. If you want to learn more about the history of self-play [check out
    this very good blogpost by Andrew Cohen](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈背后的理论并不是什么新鲜事。它早在五十年代由Arthur Samuel的跳棋玩家系统和1995年由Gerald Tesauro的TD-Gammon中就已经使用过。如果你想了解更多关于自我对弈历史的内容，请查看Andrew
    Cohen的这篇非常好的博文：[点击这里](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents)
- en: Self-Play in MLAgents
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLAgents中的自我对弈
- en: Self-Play is integrated into the MLAgents library and is managed by multiple
    hyperparameters that we’re going to study. But the main focus, as explained in
    the documentation, is the **tradeoff between the skill level and generality of
    the final policy and the stability of learning**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈已经集成到MLAgents库中，并由多个超参数管理，我们将对其进行研究。但正如文档中所解释的那样，主要关注点是**技能水平和最终策略的普遍性与学习稳定性之间的权衡**。
- en: Training against a set of slowly changing or unchanging adversaries with low
    diversity **results in more stable training. But a risk to overfit if the change
    is too slow.**
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练对抗一组缓慢变化或不变的对手，具有低多样性**会导致更稳定的训练。但如果变化太慢，就会有过拟合的风险。**
- en: 'So we need to control:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们需要控制：
- en: How **often we change opponents** with the `swap_steps` and `team_change` parameters.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过`swap_steps`和`team_change`参数来确定**多久改变对手**。
- en: The **number of opponents saved** with the `window` parameter. A larger value
    of `window`  means that an agent’s pool of opponents will contain a larger diversity
    of behaviors since it will contain policies from earlier in the training run.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`window`参数保存的**对手数量**。较大的`window`值意味着代理的对手池将包含更多行为的多样性，因为它将包含训练运行中较早的策略。
- en: The **probability of playing against the current self vs opponent** sampled
    from the pool with `play_against_latest_model_ratio`. A larger value of `play_against_latest_model_ratio`
     indicates that an agent will be playing against the current opponent more often.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`play_against_latest_model_ratio`中的池中抽样，确定**与当前自身对手对战的概率**。较大的`play_against_latest_model_ratio`值表示代理将更频繁地与当前对手对战。
- en: The **number of training steps before saving a new opponent** with `save_steps`
    parameters. A larger value of `save_steps`  will yield a set of opponents that
    cover a wider range of skill levels and possibly play styles since the policy
    receives more training.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get more details about these hyperparameters, you definitely need [to check
    out this part of the documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#self-play)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The ELO Score to evaluate our agent
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is ELO Score?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In adversarial games, tracking the **cumulative reward is not always a meaningful
    metric to track the learning progress:** because this metric is **dependent only
    on the skill of the opponent.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we’re using an ***ELO rating system*** (named after Arpad Elo) that
    calculates the **relative skill level** between 2 players from a given population
    in a zero-sum game.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'In a zero-sum game: one agent wins, and the other agent loses. It’s a mathematical
    representation of a situation in which each participant’s gain or loss of utility
    **is exactly balanced by the gain or loss of the utility of the other participants.**
    We talk about zero-sum games because the sum of utility is equal to zero.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'This ELO (starting at a specific score: frequently 1200) can decrease initially
    but should increase progressively during the training.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The Elo system is **inferred from the losses and draws against other players.**
    It means that player ratings depend **on the ratings of their opponents and the
    results scored against them.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Elo defines an Elo score that is the relative skills of a player in a zero-sum
    game. **We say relative because it depends on the performance of opponents.**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The central idea is to think of the performance of a player **as a random variable
    that is normally distributed.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: The difference in rating between 2 players serves as **the predictor of the
    outcomes of a match.** If the player wins, but the probability of winning is high,
    it will only win a few points from its opponent since it means that it is much
    stronger than it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'After every game:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The winning player takes **points from the losing one.**
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of points is determined **by the difference in the 2 players ratings
    (hence relative).**
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the higher-rated player wins → few points will be taken from the lower-rated
    player.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the lower-rated player wins → a lot of points will be taken from the high-rated
    player.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If it’s a draw → the lower-rated player gains a few points from the higher.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So if A and B have rating Ra, and Rb, then the **expected scores are** given
    by:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![ELO Score](../Images/170b63d61ac8a30759838921d82860c5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Then, at the end of the game, we need to update the player’s actual Elo score.
    We use a linear adjustment **proportional to the amount by which the player over-performed
    or under-performed.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'We also define a maximum adjustment rating per game: K-factor.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: K=16 for master.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K=32 for weaker players.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If Player A has Ea points but scored Sa points, then the player’s rating is
    updated using the formula:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![ELO Score](../Images/1e0296c75fde41fbfd850f8d5b2c8cdd.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Example
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we take an example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Player A has a rating of 2600
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Player B has a rating of 2300
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'We first calculate the expected score: <math><semantics><mrow><msub><mi>E</mi><mi>A</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>1</mn><msup><mn>0</mn><mrow><mo
    stretchy="false">(</mo><mn>2300</mn><mo>−</mo><mn>2600</mn><mo stretchy="false">)</mo><mi
    mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac><mo>=</mo><mn>0.849</mn></mrow><annotation
    encoding="application/x-tex">E_{A} = \frac{1}{1+10^{(2300-2600)/400}} = 0.849</annotation></semantics></math>
    EA​=1+10(2300−2600)/4001​=0.849 <math><semantics><mrow><msub><mi>E</mi><mi>B</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>1</mn><msup><mn>0</mn><mrow><mo
    stretchy="false">(</mo><mn>2600</mn><mo>−</mo><mn>2300</mn><mo stretchy="false">)</mo><mi
    mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac><mo>=</mo><mn>0.151</mn></mrow><annotation
    encoding="application/x-tex">E_{B} = \frac{1}{1+10^{(2600-2300)/400}} = 0.151</annotation></semantics></math>
    EB​=1+10(2600−2300)/4001​=0.151'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先计算预期得分：<math><semantics><mrow><msub><mi>E</mi><mi>A</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>1</mn><msup><mn>0</mn><mrow><mo
    stretchy="false">(</mo><mn>2300</mn><mo>−</mo><mn>2600</mn><mo stretchy="false">)</mo><mi
    mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac><mo>=</mo><mn>0.849</mn></mrow><annotation
    encoding="application/x-tex">E_{A} = \frac{1}{1+10^{(2300-2600)/400}} = 0.849</annotation></semantics></math>
    EA​=1+10(2300−2600)/4001​=0.849 <math><semantics><mrow><msub><mi>E</mi><mi>B</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>1</mn><msup><mn>0</mn><mrow><mo
    stretchy="false">(</mo><mn>2600</mn><mo>−</mo><mn>2300</mn><mo stretchy="false">)</mo><mi
    mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac><mo>=</mo><mn>0.151</mn></mrow><annotation
    encoding="application/x-tex">E_{B} = \frac{1}{1+10^{(2600-2300)/400}} = 0.151</annotation></semantics></math>
    EB​=1+10(2600−2300)/4001​=0.151
- en: 'If the organizers determined that K=16 and A wins, the new rating would be:
    <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>A</mi></msub><mo>=</mo><mn>2600</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>0.849</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2602</mn></mrow><annotation
    encoding="application/x-tex">ELO_A = 2600 + 16*(1-0.849) = 2602</annotation></semantics></math>
    ELOA​=2600+16∗(1−0.849)=2602 <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>B</mi></msub><mo>=</mo><mn>2300</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo>−</mo><mn>0.151</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2298</mn></mrow><annotation
    encoding="application/x-tex">ELO_B = 2300 + 16*(0-0.151) = 2298</annotation></semantics></math>
    ELOB​=2300+16∗(0−0.151)=2298'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果组织者确定K=16并且A获胜，新的评分将是：<math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>A</mi></msub><mo>=</mo><mn>2600</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>0.849</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2602</mn></mrow><annotation
    encoding="application/x-tex">ELO_A = 2600 + 16*(1-0.849) = 2602</annotation></semantics></math>
    ELOA​=2600+16∗(1−0.849)=2602 <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>B</mi></msub><mo>=</mo><mn>2300</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo>−</mo><mn>0.151</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2298</mn></mrow><annotation
    encoding="application/x-tex">ELO_B = 2300 + 16*(0-0.151) = 2298</annotation></semantics></math>
    ELOB​=2300+16∗(0−0.151)=2298
- en: 'If the organizers determined that K=16 and B wins, the new rating would be:
    <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>A</mi></msub><mo>=</mo><mn>2600</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo>−</mo><mn>0.849</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2586</mn></mrow><annotation
    encoding="application/x-tex">ELO_A = 2600 + 16*(0-0.849) = 2586</annotation></semantics></math>
    ELOA​=2600+16∗(0−0.849)=2586 <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>B</mi></msub><mo>=</mo><mn>2300</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>0.151</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2314</mn></mrow><annotation
    encoding="application/x-tex">ELO_B = 2300 + 16 *(1-0.151) = 2314</annotation></semantics></math>
    ELOB​=2300+16∗(1−0.151)=2314'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果组织者确定K=16并且B获胜，新的评分将是：<math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>A</mi></msub><mo>=</mo><mn>2600</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo>−</mo><mn>0.849</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2586</mn></mrow><annotation
    encoding="application/x-tex">ELO_A = 2600 + 16*(0-0.849) = 2586</annotation></semantics></math>
    ELOA​=2600+16∗(0−0.849)=2586 <math><semantics><mrow><mi>E</mi><mi>L</mi><msub><mi>O</mi><mi>B</mi></msub><mo>=</mo><mn>2300</mn><mo>+</mo><mn>16</mn><mo>∗</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>0.151</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2314</mn></mrow><annotation
    encoding="application/x-tex">ELO_B = 2300 + 16 *(1-0.151) = 2314</annotation></semantics></math>
    ELOB​=2300+16∗(1−0.151)=2314
- en: The Advantages
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'Using the ELO score has multiple advantages:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ELO分数有多个优点：
- en: Points are **always balanced** (more points are exchanged when there is an unexpected
    outcome, but the sum is always the same).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 积分**始终平衡**（当有意外结果时，会交换更多积分，但总和始终相同）。
- en: It is a **self-corrected system** since if a player wins against a weak player,
    they will only win a few points.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个**自我纠正的系统**，因为如果一个玩家赢了一个弱玩家，他们只会赢得一点。
- en: 'It **works with team games**: we calculate the average for each team and use
    it in Elo.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它**适用于团队比赛**：我们为每个团队计算平均值，并在Elo中使用它。
- en: The Disadvantages
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: ELO **does not take into account the individual contribution** of each people
    in the team.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELO**不考虑**团队中每个人的个人贡献。
- en: 'Rating deflation: **a good rating requires skill over time to keep the same
    rating**.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分通缩：**一个好的评分需要随着时间的推移保持相同的评分**。
- en: '**Can’t compare rating in history**.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法比较历史评分**。'
