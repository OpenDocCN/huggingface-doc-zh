# ConvNeXt V2

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnextv2`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnextv2)

## 概述

ConvNeXt V2 模型是由 Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie 在[ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808)中提出的。ConvNeXt V2 是一个纯卷积模型（ConvNet），受到 Vision Transformers 设计的启发，是 ConvNeXT 的后继者。

论文的摘要如下：

*受到改进的架构和更好的表示学习框架的驱动，视觉识别领域在 2020 年代初迅速现代化和性能提升。例如，现代 ConvNets，如 ConvNeXt，已经在各种场景中展示出强大的性能。虽然这些模型最初是为带有 ImageNet 标签的监督学习而设计的，但它们也有可能从像遮罩自动编码器（MAE）这样的自监督学习技术中受益。然而，我们发现简单地将这两种方法结合起来会导致性能不佳。在本文中，我们提出了一个完全卷积的遮罩自动编码器框架和一个新的全局响应归一化（GRN）层，可以添加到 ConvNeXt 架构中以增强通道间特征竞争。这种自监督学习技术和架构改进的共同设计导致了一个名为 ConvNeXt V2 的新模型系列，显著提高了纯 ConvNets 在各种识别基准上的性能，包括 ImageNet 分类、COCO 检测和 ADE20K 分割。我们还提供了各种规模的预训练 ConvNeXt V2 模型，从一个高效的 3.7M 参数 Atto 模型，在 ImageNet 上达到 76.7% 的 top-1 准确率，到一个 650M 的 Huge 模型，仅使用公共训练数据就实现了最先进的 88.9% 准确率。*

![drawing](img/25b30b91eaa46dd2f91e4a036533fc5f.png) ConvNeXt V2 架构。摘自[原始论文](https://arxiv.org/abs/2301.00808)。

这个模型是由[adirik](https://huggingface.co/adirik)贡献的。原始代码可以在[这里](https://github.com/facebookresearch/ConvNeXt-V2)找到。

## 资源

一个官方的 Hugging Face 和社区（由 🌎 表示）资源列表，帮助您开始使用 ConvNeXt V2。

图像分类

+   ConvNextV2ForImageClassification 受到这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)的支持。

如果您有兴趣提交资源以包含在这里，请随时打开一个 Pull Request，我们将进行审查！资源应该展示出一些新东西，而不是重复现有资源。

## ConvNextV2Config

### `class transformers.ConvNextV2Config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/configuration_convnextv2.py#L30)

```py
( num_channels = 3 patch_size = 4 num_stages = 4 hidden_sizes = None depths = None hidden_act = 'gelu' initializer_range = 0.02 layer_norm_eps = 1e-12 drop_path_rate = 0.0 image_size = 224 out_features = None out_indices = None **kwargs )
```

参数

+   `num_channels` (`int`，*可选*，默认为 3) — 输入通道数。

+   `patch_size` (`int`，可选，默认为 4) — 在补丁嵌入层中使用的补丁大小。

+   `num_stages` (`int`，可选，默认为 4) — 模型中的阶段数。

+   `hidden_sizes` (`List[int]`，*可选*，默认为 `[96, 192, 384, 768]`) — 每个阶段的维度（隐藏大小）。

+   `depths` (`List[int]`，*可选*，默认为 `[3, 3, 9, 3]`) — 每个阶段的深度（块数）。

+   `hidden_act` (`str` 或 `function`，*可选*，默认为 `"gelu"`) — 每个块中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"selu"` 和 `"gelu_new"`。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-12) — 层归一化层使用的 epsilon。

+   `drop_path_rate` (`float`, *optional*, 默认为 0.0) — 随机深度的丢弃率。

+   `out_features` (`List[str]`, *optional*) — 如果用作骨干，要输出的特征列表。可以是`"stem"`、`"stage1"`、`"stage2"`等（取决于模型有多少阶段）。如果未设置且设置了`out_indices`，将默认为相应的阶段。如果未设置且未设置`out_indices`，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。

+   `out_indices` (`List[int]`, *optional*) — 如果用作骨干，要输出的特征索引列表。可以是 0、1、2 等（取决于模型有多少阶段）。如果未设置且设置了`out_features`，将默认为相应的阶段。如果未设置且未设置`out_features`，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。

这是用于存储 ConvNextV2Model 配置的配置类。它用于根据指定的参数实例化 ConvNeXTV2 模型，定义模型架构。使用默认值实例化配置将产生类似于 ConvNeXTV2 [facebook/convnextv2-tiny-1k-224](https://huggingface.co/facebook/convnextv2-tiny-1k-224)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import ConvNeXTV2Config, ConvNextV2Model

>>> # Initializing a ConvNeXTV2 convnextv2-tiny-1k-224 style configuration
>>> configuration = ConvNeXTV2Config()

>>> # Initializing a model (with random weights) from the convnextv2-tiny-1k-224 style configuration
>>> model = ConvNextV2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ConvNextV2Model

### `class transformers.ConvNextV2Model`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L344)

```py
( config )
```

参数

+   `config` (ConvNextV2Config) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 ConvNextV2 模型输出原始特征，没有任何特定的头部。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L363)

```py
( pixel_values: FloatTensor = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 ConvNextImageProcessor 获取。有关详细信息，请参阅 ConvNextImageProcessor.`call`()。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` 或 `tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（ConvNextV2Config）和输入的各种元素。

+   `last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 模型最后一层的隐藏状态的序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 空间维度上池化操作后的最后一层隐藏状态。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。

    模型在每一层输出处的隐藏状态加上可选的初始嵌入输出。

ConvNextV2Model 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ConvNextV2Model
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/convnextv2-tiny-1k-224")
>>> model = ConvNextV2Model.from_pretrained("facebook/convnextv2-tiny-1k-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 768, 7, 7]
```

## ConvNextV2ForImageClassification

### `class transformers.ConvNextV2ForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L408)

```py
( config )
```

参数

+   `config`（ConvNextV2Config）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

ConvNextV2 模型在顶部具有图像分类头部（在池化特征之上的线性层），例如用于 ImageNet。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L431)

```py
( pixel_values: FloatTensor = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用 ConvNextImageProcessor 获取。有关详细信息，请参阅 ConvNextImageProcessor.`call`()。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

transformers.modeling_outputs.ImageClassifierOutputWithNoAttention 或`tuple(torch.FloatTensor)`

transformers.modeling_outputs.ImageClassifierOutputWithNoAttention 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（ConvNextV2Config）和输入。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。

ConvNextV2ForImageClassification 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ConvNextV2ForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/convnextv2-tiny-1k-224")
>>> model = ConvNextV2ForImageClassification.from_pretrained("facebook/convnextv2-tiny-1k-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## TFConvNextV2Model

### `class transformers.TFConvNextV2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L544)

```py
( config: ConvNextV2Config *inputs **kwargs )
```

参数

+   `config`（ConvNextV2Config） — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 ConvNextV2 模型输出原始特征，没有特定的头部。此模型继承自 TFPreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以了解与一般使用和行为相关的所有事项。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），或

+   将所有输入作为列表、元组或字典在第一个位置参数中。

支持第二种格式的原因是 Keras 方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需传递您支持的任何格式的输入和标签给`model.fit()`！但是，如果您想在 Keras 方法之外使用第二种格式，比如在使用 Keras`Functional`API 创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：

+   只有`pixel_values`的单个张量，没有其他内容：`model(pixel_values)`

+   一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([pixel_values, attention_mask])`或`model([pixel_values, attention_mask, token_type_ids])`

+   一个包含与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些问题，因为您可以像对待任何其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L553)

```py
( pixel_values: TFModelInputType | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例必须具有形状`(batch_size, num_channels, height, width)`） — 像素值。像素值可以使用 AutoImageProcessor 获取。有关详细信息，请参阅 ConvNextImageProcessor.`call`()。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 ModelOutput 而不是一个普通的元组。这个参数可以在急切模式下使用，在图模式下，该值将始终设置为`True`。

返回

`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`或`tuple(tf.Tensor)`

一个`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，这取决于配置（ConvNextV2Config）和输入。

+   `last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`） — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`tf.Tensor`，形状为`(batch_size, hidden_size)`) — 在空间维度上进行池化操作后的最后一层隐藏状态。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

TFConvNextV2Model 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFConvNextV2Model
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/convnextv2-tiny-1k-224")
>>> model = TFConvNextV2Model.from_pretrained("facebook/convnextv2-tiny-1k-224")

>>> inputs = image_processor(image, return_tensors="tf")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 768, 7, 7]
```

## TFConvNextV2ForImageClassification

### `class transformers.TFConvNextV2ForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L602)

```py
( config: ConvNextV2Config *inputs **kwargs )
```

参数

+   `config`（ConvNextV2Config）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

ConvNextV2 模型，顶部带有图像分类头（在池化特征的顶部是一个线性层），例如用于 ImageNet。

此模型继承自 TFPreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取有关一般用法和行为的所有相关信息。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，当将输入传递给模型和层时，Keras 方法更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以正常工作 - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果要在 Keras 方法之外使用第二种格式，例如在使用 Keras`Functional`API 创建自己的层或模型时，有三种可能性可用于在第一个位置参数中收集所有输入张量：

+   只有一个张量，其中仅包含`pixel_values`，没有其他内容：`model(pixel_values)`

+   一个长度不定的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([pixel_values, attention_mask])`或`model([pixel_values, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L624)

```py
( pixel_values: TFModelInputType | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参见 ConvNextImageProcessor.`call`()。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为`True`。

+   `labels` (`tf.Tensor`或`np.ndarray`，形状为`(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` 或 `tuple(tf.Tensor)`

一个`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（ConvNextV2Config）和输入。

+   `loss` (`tf.Tensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`tf.Tensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。

TFConvNextV2ForImageClassification 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFConvNextV2ForImageClassification
>>> import tensorflow as tf
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/convnextv2-tiny-1k-224")
>>> model = TFConvNextV2ForImageClassification.from_pretrained("facebook/convnextv2-tiny-1k-224")

>>> inputs = image_processor(image, return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = int(tf.math.argmax(logits, axis=-1))
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```
