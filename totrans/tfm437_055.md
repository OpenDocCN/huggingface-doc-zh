# Trainer

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/trainer)

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)是在Transformers库中实现的PyTorch模型的完整训练和评估循环。您只需要传递训练所需的必要部分（模型、分词器、数据集、评估函数、训练超参数等），[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类会处理其余部分。这使得更容易开始训练，而无需手动编写自己的训练循环。但同时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)非常可定制，并提供大量的训练选项，因此您可以根据自己的训练需求进行定制。

除了[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类外，Transformers还提供了一个[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)类，用于序列到序列任务，如翻译或摘要。还有来自[TRL](https://hf.co/docs/trl)库的[SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)类，它包装了[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，并针对使用自回归技术的Llama-2和Mistral等语言模型进行了优化。[SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)还支持序列打包、LoRA、量化和DeepSpeed等功能，以有效地扩展到任何模型大小。

随时查看[API参考](./main_classes/trainer)以了解其他[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类型类的更多信息，以便了解何时使用哪种。一般来说，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)是最通用的选择，适用于广泛的任务。[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)专为序列到序列任务设计，而[SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)专为训练语言模型设计。

在开始之前，请确保已安装[Accelerate](https://hf.co/docs/accelerate) - 一个用于在分布式环境中启用和运行PyTorch训练的库。

```py
pip install accelerate

# upgrade
pip install accelerate --upgrade
```

这个指南提供了[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类的概述。

## 基本用法

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)包含在基本训练循环中找到的所有代码：

1.  执行训练步骤来计算损失

1.  使用[backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)方法计算梯度

1.  根据梯度更新权重

1.  重复这个过程，直到达到预定的epoch数量

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类将所有这些代码抽象化，因此您无需每次手动编写训练循环，或者如果您刚开始使用PyTorch和训练时担心。您只需要提供训练所需的基本组件，如模型和数据集，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类会处理其他一切。

如果要指定任何训练选项或超参数，您可以在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类中找到它们。例如，让我们定义在`output_dir`中保存模型的位置，并在训练后使用`push_to_hub=True`将模型推送到Hub。

```py
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)
```

将`training_args`传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，以及一个模型、数据集、用于预处理数据集的内容（根据数据类型可能是令牌化器、特征提取器或图像处理器）、数据整理器和一个函数来计算您想要在训练过程中跟踪的指标。

最后，调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)开始训练！

```py
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

### 检查点

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类将您的模型检查点保存到[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中指定的目录中的`output_dir`参数。您将在`checkpoint-000`子文件夹中找到保存的检查点，其中末尾的数字对应训练步骤。保存检查点对于稍后恢复训练很有用。

```py
# resume from latest checkpoint
trainer.train(resume_from_checkpoint=True)

# resume from specific checkpoint saved in output directory
trainer.train(resume_from_checkpoint="your-model/checkpoint-1000")
```

您可以通过在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中设置`push_to_hub=True`将检查点保存到Hub以提交和推送它们（默认情况下不保存优化器状态）。设置如何保存检查点的其他选项在[`hub_strategy`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.hub_strategy)参数中设置：

+   `hub_strategy="checkpoint"` 将最新的检查点推送到名为“last-checkpoint”的子文件夹，您可以从中恢复训练

+   `hug_strategy="all_checkpoints"` 将所有检查点推送到`output_dir`中定义的目录（您将在模型存储库中看到每个文件夹中的一个检查点）

当您从检查点恢复训练时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)会尝试保持Python、NumPy和PyTorch RNG状态与保存检查点时相同。但由于PyTorch具有各种非确定性的默认设置，RNG状态不能保证相同。如果要启用完全确定性，请查看[控制随机性源](https://pytorch.org/docs/stable/notes/randomness#controlling-sources-of-randomness)指南，了解您可以启用哪些内容以使您的训练完全确定性。请记住，通过使某些设置确定性，训练可能会变慢。

## 自定义Trainer

虽然[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类旨在易于访问和使用，但也为更有冒险精神的用户提供了许多可定制性。许多[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的方法可以被子类化和重写，以支持您想要的功能，而无需重写整个训练循环以适应它。这些方法包括：

+   [get_train_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_train_dataloader) 创建一个训练DataLoader

+   [get_eval_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_eval_dataloader) 创建一个评估DataLoader

+   [get_test_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_test_dataloader) 创建一个测试DataLoader

+   [log()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log) 记录监视训练的各种对象的信息

+   [create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler) 在`__init__`中没有传入优化器和学习率调度器时创建它们；也可以使用[create_optimizer()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer)和[create_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_scheduler)进行单独定制

+   [compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss) 计算一批训练输入的损失

+   [training_step()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.training_step) 执行训练步骤

+   [prediction_step()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.prediction_step) 执行预测和测试步骤

+   [evaluate()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.evaluate) 评估模型并返回评估指标

+   [predict()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.predict) 在测试集上进行预测（如果有标签，则带有指标）

例如，如果您想要自定义[compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)方法以使用加权损失。

```py
from torch import nn
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss for 3 labels with different weights
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

### 回调

自定义[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的另一个选项是使用[callbacks](callbacks)。回调*不会改变*训练循环中的任何内容。它们检查训练循环状态，然后根据状态执行某些操作（提前停止、记录结果等）。换句话说，回调不能用于实现类似自定义损失函数的内容，您需要子类化并重写[compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)方法。

例如，如果您想在训练循环中的10步后添加一个提前停止回调。

```py
from transformers import TrainerCallback

class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step >= self.num_steps:
            return {"should_training_stop": True}
        else:
            return {}
```

然后将其传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的`callback`参数。

```py
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callback=[EarlyStoppingCallback()],
)
```

## 日志

查看[logging](./main_classes/logging) API参考以获取有关不同日志级别的更多信息。

默认情况下，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)设置为`logging.INFO`，报告错误、警告和其他基本信息。在分布式环境中，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的副本设置为`logging.WARNING`，仅报告错误和警告。您可以使用[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中的`log_level`和`log_level_replica`参数更改日志级别。

要为每个节点配置日志级别设置，请使用[`log_on_each_node`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.log_on_each_node)参数来确定是在每个节点上使用日志级别还是仅在主节点上使用。

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 在`Trainer.__init__()`方法中为每个节点单独设置日志级别，因此如果您在创建[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对象之前使用其他Transformers功能，可能需要考虑更早设置这个。

例如，要根据每个节点设置主代码和模块使用相同的日志级别：

```py
logger = logging.getLogger(__name__)

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)

log_level = training_args.get_process_log_level()
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)

trainer = Trainer(...)
```

使用不同组合的`log_level`和`log_level_replica`来配置每个节点上记录什么。

单节点多节点

```py
my_app.py ... --log_level warning --log_level_replica error
```

## NEFTune

[NEFTune](https://hf.co/papers/2310.05914)是一种通过在训练过程中向嵌入向量添加噪音来提高性能的技术。要在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中启用它，设置[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中的`neftune_noise_alpha`参数来控制添加多少噪音。

```py
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(..., neftune_noise_alpha=0.1)
trainer = Trainer(..., args=training_args)
```

训练后禁用NEFTune，以恢复原始嵌入层，避免任何意外行为。

## 加速和Trainer

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类由[Accelerate](https://hf.co/docs/accelerate)支持，这是一个用于在分布式环境中轻松训练PyTorch模型的库，支持集成如[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)和[DeepSpeed](https://www.deepspeed.ai/)。

在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的[Fully Sharded Data Parallel](fsdp)指南中了解更多关于FSDP分片策略、CPU卸载等内容。

使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)与Accelerate，运行[`accelerate.config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)命令来设置您的训练环境。这个命令会创建一个`config_file.yaml`，在您启动训练脚本时会用到。例如，您可以设置一些示例配置：

DistributedDataParallelFSDPDeepSpeedDeepSpeed与Accelerate插件

```py
compute_environment: LOCAL_MACHINE                                                                                             
distributed_type: MULTI_GPU                                                                                                    
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0 #change rank as per the node
main_process_ip: 192.168.20.1
main_process_port: 9898
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

[`accelerate_launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)命令是在分布式系统上启动您的训练脚本的推荐方式，使用Accelerate和[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中在`config_file.yaml`中指定的参数。这个文件保存在Accelerate缓存文件夹中，并在运行`accelerate_launch`时自动加载。

例如，使用FSDP配置运行[run_glue.py](https://github.com/huggingface/transformers/blob/f4db565b695582891e43a5e042e5d318e28f20b8/examples/pytorch/text-classification/run_glue.py#L4)训练脚本：

```py
accelerate launch \
    ./examples/pytorch/text-classification/run_glue.py \
    --model_name_or_path bert-base-cased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 16 \
    --learning_rate 5e-5 \
    --num_train_epochs 3 \
    --output_dir /tmp/$TASK_NAME/ \
    --overwrite_output_dir
```

您也可以直接在命令行中指定来自`config_file.yaml`文件的参数：

```py
accelerate launch --num_processes=2 \
    --use_fsdp \
    --mixed_precision=bf16 \
    --fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \
    --fsdp_transformer_layer_cls_to_wrap="BertLayer" \
    --fsdp_sharding_strategy=1 \
    --fsdp_state_dict_type=FULL_STATE_DICT \
    ./examples/pytorch/text-classification/run_glue.py
    --model_name_or_path bert-base-cased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 16 \
    --learning_rate 5e-5 \
    --num_train_epochs 3 \
    --output_dir /tmp/$TASK_NAME/ \
    --overwrite_output_dir
```

查看[启动您的Accelerate脚本](https://huggingface.co/docs/accelerate/basic_tutorials/launch)教程，了解更多关于`accelerate_launch`和自定义配置的内容。
