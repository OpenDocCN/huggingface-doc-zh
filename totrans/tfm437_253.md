# BEiT

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit)

## æ¦‚è¿°

BEiTæ¨¡å‹æ˜¯ç”±é²èˆªæ³¢ã€è‘£ç«‹å’Œé­ç”«èŒ¹åœ¨ã€ŠBEiT: BERT Pre-Training of Image Transformersã€‹ä¸­æå‡ºçš„ã€‚å—BERTå¯å‘ï¼ŒBEiTæ˜¯ç¬¬ä¸€ç¯‡ä½¿è‡ªç›‘ç£é¢„è®­ç»ƒçš„Vision Transformersï¼ˆViTsï¼‰ä¼˜äºç›‘ç£é¢„è®­ç»ƒçš„è®ºæ–‡ã€‚BEiTæ¨¡å‹çš„é¢„è®­ç»ƒä¸æ˜¯é¢„æµ‹å›¾åƒçš„ç±»åˆ«ï¼ˆå¦‚åŸå§‹ViTè®ºæ–‡ä¸­æ‰€åšçš„é‚£æ ·ï¼‰ï¼Œè€Œæ˜¯é¢„è®­ç»ƒæ¨¡å‹ä»¥é¢„æµ‹æ¥è‡ªOpenAIçš„DALL-Eæ¨¡å‹çš„ä»£ç æœ¬çš„è§†è§‰æ ‡è®°ï¼Œç»™å®šé®ç½©è¡¥ä¸ã€‚

è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è‡ªç›‘ç£è§†è§‰è¡¨ç¤ºæ¨¡å‹BEiTï¼Œä»£è¡¨ä»å›¾åƒå˜æ¢å™¨ä¸­çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¼€å‘çš„BERTä¹‹åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé®ç½©å›¾åƒå»ºæ¨¡ä»»åŠ¡æ¥é¢„è®­ç»ƒè§†è§‰å˜æ¢å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒä¸­ï¼Œæ¯ä¸ªå›¾åƒæœ‰ä¸¤ä¸ªè§†å›¾ï¼Œå³å›¾åƒè¡¥ä¸ï¼ˆä¾‹å¦‚16x16åƒç´ ï¼‰å’Œè§†è§‰æ ‡è®°ï¼ˆå³ç¦»æ•£æ ‡è®°ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆå°†åŸå§‹å›¾åƒâ€œæ ‡è®°åŒ–â€ä¸ºè§†è§‰æ ‡è®°ã€‚ç„¶åæˆ‘ä»¬éšæœºé®ç½©ä¸€äº›å›¾åƒè¡¥ä¸å¹¶å°†å®ƒä»¬é¦ˆé€åˆ°ä¸»å¹²Transformerä¸­ã€‚é¢„è®­ç»ƒç›®æ ‡æ˜¯åŸºäºæŸåçš„å›¾åƒè¡¥ä¸æ¢å¤åŸå§‹çš„è§†è§‰æ ‡è®°ã€‚åœ¨å¯¹BEiTè¿›è¡Œé¢„è®­ç»ƒåï¼Œæˆ‘ä»¬é€šè¿‡åœ¨é¢„è®­ç»ƒçš„ç¼–ç å™¨ä¸Šé™„åŠ ä»»åŠ¡å±‚æ¥ç›´æ¥å¾®è°ƒæ¨¡å‹å‚æ•°ä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä»¥å‰çš„é¢„è®­ç»ƒæ–¹æ³•ä¸­å–å¾—äº†ç«äº‰æ€§çš„ç»“æœã€‚ä¾‹å¦‚ï¼ŒåŸºç¡€å°ºå¯¸çš„BEiTåœ¨ImageNet-1Kä¸Šå®ç°äº†83.2%çš„top-1å‡†ç¡®ç‡ï¼Œæ˜æ˜¾ä¼˜äºä½¿ç”¨ç›¸åŒè®¾ç½®çš„DeiTä»å¤´å¼€å§‹è®­ç»ƒï¼ˆ81.8%ï¼‰ã€‚æ­¤å¤–ï¼Œå¤§å°ºå¯¸çš„BEiTä»…ä½¿ç”¨ImageNet-1Kå°±è¾¾åˆ°äº†86.3%ï¼Œç”šè‡³ä¼˜äºåœ¨ImageNet-22Kä¸Šè¿›è¡Œç›‘ç£é¢„è®­ç»ƒçš„ViT-Lï¼ˆ85.2%ï¼‰ã€‚*

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®çš„ã€‚è¿™ä¸ªæ¨¡å‹çš„JAX/FLAXç‰ˆæœ¬æ˜¯ç”±[kamalkraj](https://huggingface.co/kamalkraj)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/microsoft/unilm/tree/master/beit)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   BEiTæ¨¡å‹æ˜¯å¸¸è§„çš„Vision Transformersï¼Œä½†æ˜¯æ˜¯ä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œä¸æ˜¯ç›‘ç£è®­ç»ƒã€‚å½“åœ¨ImageNet-1Kå’ŒCIFAR-100ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼˜äºåŸå§‹æ¨¡å‹ï¼ˆViTï¼‰ä»¥åŠæ•°æ®é«˜æ•ˆå›¾åƒå˜æ¢å™¨ï¼ˆDeiTï¼‰ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹å…³äºæ¨ç†ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¼”ç¤ºç¬”è®°æœ¬[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)ï¼ˆæ‚¨åªéœ€å°†[ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)æ›¿æ¢ä¸º[BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)ï¼Œå°†[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)æ›¿æ¢ä¸º[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)ï¼‰ã€‚

+   è¿˜æœ‰ä¸€ä¸ªæ¼”ç¤ºç¬”è®°æœ¬å¯ç”¨ï¼Œå±•ç¤ºäº†å¦‚ä½•å°†DALL-Eçš„å›¾åƒæ ‡è®°å™¨ä¸BEiTç»“åˆèµ·æ¥æ‰§è¡Œé®ç½©å›¾åƒå»ºæ¨¡ã€‚æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT)æ‰¾åˆ°ã€‚

+   ç”±äºBEiTæ¨¡å‹æœŸæœ›æ¯ä¸ªå›¾åƒå…·æœ‰ç›¸åŒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨[BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)æ¥è°ƒæ•´ï¼ˆæˆ–é‡æ–°ç¼©æ”¾ï¼‰å’Œè§„èŒƒåŒ–å›¾åƒä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚

+   åœ¨é¢„è®­ç»ƒæˆ–å¾®è°ƒæœŸé—´ä½¿ç”¨çš„è¡¥ä¸åˆ†è¾¨ç‡å’Œå›¾åƒåˆ†è¾¨ç‡åæ˜ åœ¨æ¯ä¸ªæ£€æŸ¥ç‚¹çš„åç§°ä¸­ã€‚ä¾‹å¦‚ï¼Œ`microsoft/beit-base-patch16-224`æŒ‡çš„æ˜¯ä¸€ä¸ªåŸºæœ¬å¤§å°çš„æ¶æ„ï¼Œè¡¥ä¸åˆ†è¾¨ç‡ä¸º16x16ï¼Œå¾®è°ƒåˆ†è¾¨ç‡ä¸º224x224ã€‚æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨[hub](https://huggingface.co/models?search=microsoft/beit)ä¸Šæ‰¾åˆ°ã€‚

+   å¯ç”¨çš„æ£€æŸ¥ç‚¹è¦ä¹ˆï¼ˆ1ï¼‰ä»…åœ¨[ImageNet-22k](http://www.image-net.org/)ï¼ˆåŒ…å«1400ä¸‡å›¾åƒå’Œ22kç±»åˆ«ï¼‰ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¦ä¹ˆï¼ˆ2ï¼‰è¿˜åœ¨ImageNet-22kä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¦ä¹ˆï¼ˆ3ï¼‰è¿˜åœ¨[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)ï¼ˆä¹Ÿç§°ä¸ºILSVRC 2012ï¼ŒåŒ…å«130ä¸‡å›¾åƒå’Œ1000ç±»åˆ«ï¼‰ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚

+   BEiTä½¿ç”¨ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå—T5æ¨¡å‹å¯å‘ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œä½œè€…åœ¨å‡ ä¸ªè‡ªæ³¨æ„åŠ›å±‚ä¹‹é—´å…±äº«äº†ç›¸å¯¹ä½ç½®åå·®ã€‚åœ¨å¾®è°ƒæœŸé—´ï¼Œæ¯ä¸ªå±‚çš„ç›¸å¯¹ä½ç½®åå·®éƒ½æ˜¯ç”¨é¢„è®­ç»ƒåè·å¾—çš„å…±äº«ç›¸å¯¹ä½ç½®åå·®åˆå§‹åŒ–çš„ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœè¦ä»å¤´å¼€å§‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦å°†[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)çš„`use_relative_position_bias`æˆ–`use_relative_position_bias`å±æ€§è®¾ç½®ä¸º`True`ï¼Œä»¥æ·»åŠ ä½ç½®åµŒå…¥ã€‚

![å›¾ç¤º](../Images/6740a7101e0ab9d00b7aec6fa2bd9f0c.png) BEiTé¢„è®­ç»ƒã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2106.08254)

## èµ„æº

ä¸€ç³»åˆ—å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨BEiTã€‚

å›¾åƒåˆ†ç±»

+   [BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚

+   å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)

**è¯­ä¹‰åˆ†å‰²**

+   [è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—](../tasks/semantic_segmentation)

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## BEiTç‰¹å®šè¾“å‡º

### `class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L69)

```py
( last_hidden_state: FloatTensor = None pooler_output: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )
```

å‚æ•°

+   `last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” å¦‚æœ*config.use_mean_pooling*è®¾ç½®ä¸ºTrueï¼Œåˆ™æ˜¯è¡¥ä¸æ ‡è®°çš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ï¼ˆä¸åŒ…æ‹¬*[CLS]*æ ‡è®°ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™å°†è¿”å›*[CLS]*æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

ç”¨äº[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)è¾“å‡ºçš„ç±»ã€‚

### `class transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L44)

```py
( last_hidden_state: Array = None pooler_output: Array = None hidden_states: Optional = None attentions: Optional = None )
```

å‚æ•°

+   `last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼‰ â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, hidden_size)`ï¼‰ â€” å¦‚æœ *config.use_mean_pooling* è®¾ç½®ä¸º Trueï¼Œåˆ™ä¸ºè¡¥ä¸æ ‡è®°çš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ï¼ˆä¸åŒ…æ‹¬ *[CLS]* æ ‡è®°ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º Falseï¼Œåˆ™å°†è¿”å› *[CLS]* æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

ç”¨äº [FlaxBeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.FlaxBeitModel) è¾“å‡ºçš„ç±»ã€‚

## BeitConfig

### `class transformers.BeitConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/configuration_beit.py#L37)

```py
( vocab_size = 8192 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 use_mask_token = False use_absolute_position_embeddings = False use_relative_position_bias = False use_shared_relative_position_bias = False layer_scale_init_value = 0.1 drop_path_rate = 0.1 use_mean_pooling = True pool_scales = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input = False semantic_loss_ignore_index = 255 out_features = None out_indices = None add_fpn = False reshape_hidden_states = True **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 8192) â€” BEiT æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨é¢„è®­ç»ƒæœŸé—´å¯ä»¥ä½¿ç”¨çš„ä¸åŒå›¾åƒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `intermediate_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` æˆ– `function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`ã€`"relu"`ã€`"selu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `image_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `num_channels` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 3) â€” è¾“å…¥é€šé“æ•°ã€‚

+   `use_mask_token` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä¸ºé®ç½©å›¾åƒå»ºæ¨¡ä½¿ç”¨é®ç½©æ ‡è®°ã€‚

+   `use_absolute_position_embeddings` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ç±»ä¼¼BERTçš„ç»å¯¹ä½ç½®åµŒå…¥ã€‚

+   `use_relative_position_bias` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ T5 é£æ ¼çš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚

+   `use_shared_relative_position_bias` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ Transformer çš„æ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ç›¸åŒçš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚

+   `layer_scale_init_value` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨çš„æ¯”ä¾‹ã€‚åŸºç¡€ä¸º 0.1ï¼Œå¤§å‹ä¸º 1e-5ã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨å±‚æ¯”ä¾‹ã€‚

+   `drop_path_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” æ¯ä¸ªæ ·æœ¬çš„éšæœºæ·±åº¦ç‡ï¼ˆåº”ç”¨äºæ®‹å·®å±‚çš„ä¸»è·¯å¾„ï¼‰ã€‚

+   `use_mean_pooling` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹è¡¥ä¸çš„æœ€ç»ˆéšè—çŠ¶æ€è¿›è¡Œå‡å€¼æ± åŒ–ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ CLS æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ååº”ç”¨åˆ†ç±»å¤´ã€‚

+   `pool_scales` (`Tuple[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[1, 2, 3, 6]`) â€” åœ¨æœ€åä¸€ä¸ªç‰¹å¾å›¾ä¸Šåº”ç”¨çš„æ± åŒ–é‡‘å­—å¡”æ¨¡å—ä¸­ä½¿ç”¨çš„æ± åŒ–æ¯”ä¾‹ã€‚

+   `use_auxiliary_head` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è¾…åŠ©å¤´ã€‚

+   `auxiliary_loss_weight` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±çš„æƒé‡ã€‚

+   `auxiliary_channels` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„é€šé“æ•°ã€‚

+   `auxiliary_num_convs` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„å·ç§¯å±‚æ•°ã€‚

+   `auxiliary_concat_input` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨åˆ†ç±»å±‚ä¹‹å‰å°†è¾…åŠ©å¤´çš„è¾“å‡ºä¸è¾“å…¥è¿›è¡Œè¿æ¥ã€‚

+   `semantic_loss_ignore_index` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚

+   `out_features` (`List[str]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œéª¨å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾åˆ—è¡¨ã€‚å¯ä»¥æ˜¯ `"stem"`ã€`"stage1"`ã€`"stage2"` ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº† `out_indices`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”æœªè®¾ç½® `out_indices`ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§ `stage_names` å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚

+   `out_indices` (`List[int]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œéª¨å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾çš„ç´¢å¼•åˆ—è¡¨ã€‚å¯ä»¥æ˜¯ 0ã€1ã€2 ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº† `out_features`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”æœªè®¾ç½® `out_features`ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§ `stage_names` å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚

+   `add_fpn` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°† FPN æ·»åŠ ä¸ºéª¨å¹²çš„ä¸€éƒ¨åˆ†ã€‚ä»…é€‚ç”¨äº `BeitBackbone`ã€‚

+   `reshape_hidden_states` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨å°†æ¨¡å‹ç”¨ä½œéª¨å¹²æ—¶å°†ç‰¹å¾å›¾é‡å¡‘ä¸ºå½¢çŠ¶ä¸º `(batch_size, hidden_size, height, width)` çš„4Då¼ é‡ã€‚å¦‚æœä¸º `False`ï¼Œç‰¹å¾å›¾å°†æ˜¯å½¢çŠ¶ä¸º `(batch_size, seq_len, hidden_size)` çš„3Då¼ é‡ã€‚ä»…é€‚ç”¨äº `BeitBackbone`ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel) çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª BEiT æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº BEiT [microsoft/beit-base-patch16-224-pt22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k) æ¶æ„çš„é…ç½®ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import BeitConfig, BeitModel

>>> # Initializing a BEiT beit-base-patch16-224-pt22k style configuration
>>> configuration = BeitConfig()

>>> # Initializing a model (with random weights) from the beit-base-patch16-224-pt22k style configuration
>>> model = BeitModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## BeitFeatureExtractor

### `class transformers.BeitFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/feature_extraction_beit.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L307)

```py
( images segmentation_maps = None **kwargs )
```

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)

```py
( outputs target_sizes: List = None ) â†’ export const metadata = 'undefined';semantic_segmentation
```

å‚æ•°

+   `outputs`ï¼ˆ[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)ï¼‰ â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes`ï¼ˆé•¿åº¦ä¸º `batch_size` çš„ `List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå°ºå¯¸ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

è¯­ä¹‰åˆ†å‰²

é•¿åº¦ä¸º `batch_size` çš„ `List[torch.Tensor]`ï¼Œæ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸º (height, width) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº `target_sizes` æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºè¯­ä¹‰ç±»åˆ« idã€‚

å°† [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation) çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

## BeitImageProcessor

### `class transformers.BeitImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L49)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None rescale_factor: Union = 0.00392156862745098 do_rescale: bool = True do_normalize: bool = True image_mean: Union = None image_std: Union = None do_reduce_labels: bool = False **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸è°ƒæ•´ä¸ºæŒ‡å®šçš„ `size`ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_resize` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `size` (`Dict[str, int]` *å¯é€‰*ï¼Œé»˜è®¤ä¸º `{"height" -- 256, "width": 256}`)ï¼šè°ƒæ•´å¤§å°åçš„è¾“å‡ºå›¾åƒå°ºå¯¸ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `size` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `resample` (`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `Resampling.BICUBIC`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `do_center_crop` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¦‚æœè¾“å…¥å°ºå¯¸æ²¿ä»»ä½•è¾¹ç¼˜å°äº `crop_size`ï¼Œåˆ™å›¾åƒå°†å¡«å……ä¸º 0ï¼Œç„¶åè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_center_crop` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `crop_size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `{"height" -- 224, "width": 224}`)ï¼šåº”ç”¨ä¸­å¿ƒè£å‰ªæ—¶çš„æœŸæœ›è¾“å‡ºå°ºå¯¸ã€‚ä»…åœ¨ `do_center_crop` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `crop_size` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `rescale_factor` (`int` æˆ– `float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `do_rescale` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹ `rescale_factor` é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_normalize` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `IMAGENET_STANDARD_MEAN`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `image_mean` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `image_std` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `IMAGENET_STANDARD_STD`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

+   `do_reduce_labels` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦å‡å°‘æ‰€æœ‰åˆ†å‰²åœ°å›¾çš„æ ‡ç­¾å€¼ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ä¸­å°† 0 ç”¨äºèƒŒæ™¯ï¼Œä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»ä¸­ï¼ˆä¾‹å¦‚ ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º 255ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_reduce_labels` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚

æ„å»º BEiT å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L312)

```py
( images: Union segmentation_maps: Union = None do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º0åˆ°255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º`self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå¤§å°ã€‚

+   `resample` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.resample`) â€” è°ƒæ•´å›¾åƒå¤§å°æ—¶è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚è¿™å¯ä»¥æ˜¯æšä¸¾`PILImageResampling`ä¹‹ä¸€ï¼Œä»…åœ¨è®¾ç½®`do_resize=True`æ—¶æœ‰æ•ˆã€‚

+   `do_center_crop` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_center_crop`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚

+   `crop_size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º`self.crop_size`) â€” ä¸­å¿ƒè£å‰ªåçš„å›¾åƒå¤§å°ã€‚å¦‚æœå›¾åƒçš„ä¸€æ¡è¾¹å°äº`crop_size`ï¼Œåˆ™å°†ç”¨é›¶å¡«å……ï¼Œç„¶åè£å‰ªã€‚

+   `do_rescale` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾ä¸º[0 - 1]ã€‚

+   `rescale_factor` (`float`, *å¯é€‰*, é»˜è®¤ä¸º`self.rescale_factor`) â€” å¦‚æœè®¾ç½®`do_rescale=True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚

+   `do_normalize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º`self.image_mean`) â€” å›¾åƒå‡å€¼ã€‚

+   `image_std` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º`self.image_std`) â€” å›¾åƒæ ‡å‡†å·®ã€‚

+   `do_reduce_labels` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_reduce_labels`) â€” æ˜¯å¦å‡å°‘æ‰€æœ‰åˆ†å‰²åœ°å›¾çš„æ ‡ç­¾å€¼ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ä¸­ä½¿ç”¨0è¡¨ç¤ºèƒŒæ™¯ï¼Œå¹¶ä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»ä¸­ï¼ˆä¾‹å¦‚ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º255ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`, *å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª`np.ndarray`åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`tf.Tensor`çš„æ‰¹é‡ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`torch.Tensor`çš„æ‰¹é‡ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`np.ndarray`çš„æ‰¹é‡ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`jax.numpy.ndarray`çš„æ‰¹é‡ã€‚

+   `data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º`ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   `"none"` æˆ– `ChannelDimension.NONE`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)

```py
( outputs target_sizes: List = None ) â†’ export const metadata = 'undefined';semantic_segmentation
```

å‚æ•°

+   `outputs`ï¼ˆ[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)ï¼‰ â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes` (`List[Tuple]`ï¼Œé•¿åº¦ä¸º`batch_size`ï¼Œ*å¯é€‰*) â€” ä¸æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

è¯­ä¹‰åˆ†å‰²

`List[torch.Tensor]` é•¿åº¦ä¸º `batch_size`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸º (height, width) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äºç›®æ ‡å¤§å°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚

å°† [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation) çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒPyTorchã€‚

PytorchHide Pytorch content

## BeitModel

### `class transformers.BeitModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L620)

```py
( config: BeitConfig add_pooling_layer: bool = True )
```

å‚æ•°

+   `config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„Beitæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L651)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.beit.modeling_beit.BeitModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)ã€‚

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*) â€” å¸ƒå°”æ©ç ä½ç½®ã€‚æŒ‡ç¤ºå“ªäº›è¡¥ä¸è¢«æ©ç›–ï¼ˆ1ï¼‰å“ªäº›æ²¡æœ‰ï¼ˆ0ï¼‰ã€‚

è¿”å›

[transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling) æˆ– `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰- å¦‚æœ*config.use_mean_pooling*è®¾ç½®ä¸ºTrueï¼Œåˆ™æ˜¯è¡¥ä¸æ ‡è®°çš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ï¼ˆä¸åŒ…æ‹¬*[CLS]*æ ‡è®°ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™å°†è¿”å›*[CLS]*æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, BeitModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
>>> model = BeitModel.from_pretrained("microsoft/beit-base-patch16-224-pt22k")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 197, 768]
```

## BeitForMaskedImageModeling

### `class transformers.BeitForMaskedImageModeling`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L732)

```py
( config: BeitConfig )
```

å‚æ•°

+   `config`ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰- æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Beitæ¨¡å‹å˜å‹å™¨é¡¶éƒ¨å¸¦æœ‰â€œè¯­è¨€â€å»ºæ¨¡å¤´ã€‚BEiTé€šè¿‡é¢„æµ‹çŸ¢é‡é‡åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQ-VAEï¼‰çš„è§†è§‰æ ‡è®°æ¥è¿›è¡Œé®è”½å›¾åƒå»ºæ¨¡ï¼Œè€Œå…¶ä»–è§†è§‰æ¨¡å‹å¦‚ViTå’ŒDeiTåˆ™é¢„æµ‹RGBåƒç´ å€¼ã€‚å› æ­¤ï¼Œæ­¤ç±»ä¸[AutoModelForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMaskedImageModeling)ä¸å…¼å®¹ï¼Œå› æ­¤å¦‚æœè¦ä½¿ç”¨BEiTè¿›è¡Œé®è”½å›¾åƒå»ºæ¨¡ï¼Œåˆ™éœ€è¦ç›´æ¥ä½¿ç”¨[BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L753)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰- åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)ã€‚

+   `head_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æŸ¥çœ‹è¿”å›å¼ é‡ä¸­çš„ `hidden_states` ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `bool_masked_pos` (`torch.BoolTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_patches)`) â€” å¸ƒå°”æ©ç ä½ç½®ã€‚æŒ‡ç¤ºå“ªäº›è¡¥ä¸è¢«æ©ç›–ï¼ˆ1ï¼‰å“ªäº›æ²¡æœ‰ï¼ˆ0ï¼‰ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›å€¼

[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput) æˆ– `tuple(torch.FloatTensor)`

[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›) â€” æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡º + æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚

    è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling
>>> import torch
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
>>> model = BeitForMaskedImageModeling.from_pretrained("microsoft/beit-base-patch16-224-pt22k")

>>> num_patches = (model.config.image_size // model.config.patch_size) ** 2
>>> pixel_values = image_processor(images=image, return_tensors="pt").pixel_values
>>> # create random boolean mask of shape (batch_size, num_patches)
>>> bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()

>>> outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
>>> loss, logits = outputs.loss, outputs.logits
>>> list(logits.shape)
[1, 196, 8192]
```

## BeitForImageClassification

### `class transformers.BeitForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L832)

```py
( config: BeitConfig )
```

å‚æ•°

+   `config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Beit æ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ä½äºè¡¥ä¸æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€çš„å¹³å‡å€¼ä¹‹ä¸Šï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚

æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L852)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)ã€‚

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›å€¼

[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, BeitForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224")
>>> model = BeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## BeitForSemanticSegmentation

### `class transformers.BeitForSemanticSegmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1156)

```py
( config: BeitConfig )
```

å‚æ•°

+   `config`ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰- æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Beit æ¨¡å‹å˜æ¢å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­ä¹‰åˆ†å‰²å¤´ï¼Œä¾‹å¦‚ç”¨äº ADE20kã€CityScapesã€‚

æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1214)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰- åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚åœ¨`[0, 1]`ä¸­é€‰æ‹©çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰- åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ`config.num_labels==1`ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height, logits_width)`çš„`torch.FloatTensor`ï¼‰- æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚

    <tip warning="{true}">è¿”å›çš„logitsä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°†logitsè°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”è¯¥å§‹ç»ˆæ£€æŸ¥æ‚¨çš„logitså½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, patch_size, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, BeitForSemanticSegmentation
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-finetuned-ade-640-640")
>>> model = BeitForSemanticSegmentation.from_pretrained("microsoft/beit-base-finetuned-ade-640-640")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> # logits are of shape (batch_size, num_labels, height, width)
>>> logits = outputs.logits
```

JAXHide JAXå†…å®¹

## FlaxBeitModel

### `class transformers.FlaxBeitModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L741)

```py
( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰- åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype`ï¼ˆ`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`jax.numpy.float32`ï¼‰- è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚

    è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨ç†ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`

    å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„dtypeï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

è£¸Beitæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä»PyTorchæ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„Flax linenæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

æœ€åï¼Œè¿™ä¸ªæ¨¡å‹æ”¯æŒJAXçš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)

```py
( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling or tuple(torch.FloatTensor)
```

è¿”å›

[transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” å¦‚æœ*config.use_mean_pooling*è®¾ç½®ä¸ºTrueï¼Œåˆ™æ˜¯è¡¥ä¸æ ‡è®°çš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ï¼ˆä¸åŒ…æ‹¬*[CLS]*æ ‡è®°ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™å°†è¿”å›*[CLS]*æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªå±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxBeitPreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, FlaxBeitModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k-ft22k")
>>> model = FlaxBeitModel.from_pretrained("microsoft/beit-base-patch16-224-pt22k-ft22k")

>>> inputs = image_processor(images=image, return_tensors="np")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## FlaxBeitForMaskedImageModeling

### `class transformers.FlaxBeitForMaskedImageModeling`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L825)

```py
( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰ â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *å¯é€‰*, é»˜è®¤ä¸º`jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚

    è¿™å¯ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`

    å¦‚æœå¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

åœ¨é¡¶éƒ¨æ·»åŠ ä¸€ä¸ªâ€œè¯­è¨€â€å»ºæ¨¡å¤´çš„Beitæ¨¡å‹å˜æ¢å™¨ï¼ˆç”¨äºé¢„æµ‹è§†è§‰æ ‡è®°ï¼‰ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä»PyTorchæ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„Flax linenæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥äº†è§£ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒJAXçš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)

```py
( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

è¿”å›

[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`jnp.ndarray`ï¼‰ â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxBeitPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

bool_masked_posï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_patches)`çš„`numpy.ndarray`ï¼‰ï¼šå¸ƒå°”æ©ç ä½ç½®ã€‚æŒ‡ç¤ºå“ªäº›è¡¥ä¸è¢«å±è”½ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ²¡æœ‰ï¼ˆ0ï¼‰ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
>>> model = BeitForMaskedImageModeling.from_pretrained("microsoft/beit-base-patch16-224-pt22k")

>>> inputs = image_processor(images=image, return_tensors="np")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
```

## FlaxBeitForImageClassification

### `class transformers.FlaxBeitForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L909)

```py
( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *optional*, é»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯ `jax.numpy.float32`, `jax.numpy.float16` (åœ¨GPUä¸Š) å’Œ `jax.numpy.bfloat16` (åœ¨TPUä¸Š) ä¸­çš„ä¸€ä¸ªã€‚

    è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†ï¼Œæ‰€æœ‰çš„è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„ `dtype` è¿›è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™åªæŒ‡å®šäº†è®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`

    å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) å’Œ [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

Beit æ¨¡å‹å˜æ¢å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªå›¾åƒåˆ†ç±»å¤´ï¼ˆä¸€ä¸ªçº¿æ€§å±‚ä½äºè¡¥ä¸æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€çš„å¹³å‡å€¼ä¹‹ä¸Šï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½ã€ä¿å­˜å’Œä» PyTorch æ¨¡å‹è½¬æ¢æƒé‡ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œè¿™ä¸ªæ¨¡å‹æ”¯æŒ JAX çš„å†…åœ¨ç‰¹æ€§ï¼Œæ¯”å¦‚ï¼š

+   [å³æ—¶ç¼–è¯‘ (JIT)](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [å‘é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)

```py
( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)
```

è¿”å›

[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½® (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`) å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰ã€‚

    è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxBeitPreTrainedModel` çš„å‰å‘æ–¹æ³•è¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡Œè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, FlaxBeitForImageClassification
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224")
>>> model = FlaxBeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224")

>>> inputs = image_processor(images=image, return_tensors="np")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
```
