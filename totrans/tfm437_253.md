# BEiT

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit)

## 概述

BEiT模型是由鲍航波、董立和魏甫茹在《BEiT: BERT Pre-Training of Image Transformers》中提出的。受BERT启发，BEiT是第一篇使自监督预训练的Vision Transformers（ViTs）优于监督预训练的论文。BEiT模型的预训练不是预测图像的类别（如原始ViT论文中所做的那样），而是预训练模型以预测来自OpenAI的DALL-E模型的代码本的视觉标记，给定遮罩补丁。

该论文的摘要如下：

*我们介绍了一种自监督视觉表示模型BEiT，代表从图像变换器中的双向编码器表示。在自然语言处理领域开发的BERT之后，我们提出了一个遮罩图像建模任务来预训练视觉变换器。具体来说，我们的预训练中，每个图像有两个视图，即图像补丁（例如16x16像素）和视觉标记（即离散标记）。我们首先将原始图像“标记化”为视觉标记。然后我们随机遮罩一些图像补丁并将它们馈送到主干Transformer中。预训练目标是基于损坏的图像补丁恢复原始的视觉标记。在对BEiT进行预训练后，我们通过在预训练的编码器上附加任务层来直接微调模型参数以进行下游任务。图像分类和语义分割的实验结果表明，我们的模型在以前的预训练方法中取得了竞争性的结果。例如，基础尺寸的BEiT在ImageNet-1K上实现了83.2%的top-1准确率，明显优于使用相同设置的DeiT从头开始训练（81.8%）。此外，大尺寸的BEiT仅使用ImageNet-1K就达到了86.3%，甚至优于在ImageNet-22K上进行监督预训练的ViT-L（85.2%）。*

这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。这个模型的JAX/FLAX版本是由[kamalkraj](https://huggingface.co/kamalkraj)贡献的。原始代码可以在[这里](https://github.com/microsoft/unilm/tree/master/beit)找到。

## 使用提示

+   BEiT模型是常规的Vision Transformers，但是是以自监督的方式进行预训练，而不是监督训练。当在ImageNet-1K和CIFAR-100上进行微调时，它们的性能优于原始模型（ViT）以及数据高效图像变换器（DeiT）。您可以查看关于推理以及在自定义数据上进行微调的演示笔记本[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)（您只需将[ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)替换为[BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)，将[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)替换为[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)）。

+   还有一个演示笔记本可用，展示了如何将DALL-E的图像标记器与BEiT结合起来执行遮罩图像建模。您可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT)找到。

+   由于BEiT模型期望每个图像具有相同的大小（分辨率），可以使用[BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)来调整（或重新缩放）和规范化图像以供模型使用。

+   在预训练或微调期间使用的补丁分辨率和图像分辨率反映在每个检查点的名称中。例如，`microsoft/beit-base-patch16-224`指的是一个基本大小的架构，补丁分辨率为16x16，微调分辨率为224x224。所有检查点都可以在[hub](https://huggingface.co/models?search=microsoft/beit)上找到。

+   可用的检查点要么（1）仅在[ImageNet-22k](http://www.image-net.org/)（包含1400万图像和22k类别）上进行了预训练，要么（2）还在ImageNet-22k上进行了微调，要么（3）还在[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)（也称为ILSVRC 2012，包含130万图像和1000类别）上进行了微调。

+   BEiT使用相对位置嵌入，受T5模型启发。在预训练期间，作者在几个自注意力层之间共享了相对位置偏差。在微调期间，每个层的相对位置偏差都是用预训练后获得的共享相对位置偏差初始化的。请注意，如果要从头开始预训练模型，需要将[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)的`use_relative_position_bias`或`use_relative_position_bias`属性设置为`True`，以添加位置嵌入。

![图示](../Images/6740a7101e0ab9d00b7aec6fa2bd9f0c.png) BEiT预训练。摘自[原始论文。](https://arxiv.org/abs/2106.08254)

## 资源

一系列官方Hugging Face和社区（由🌎表示）资源的列表，可帮助您开始使用BEiT。

图像分类

+   [BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。

+   另请参阅：[图像分类任务指南](../tasks/image_classification)

**语义分割**

+   [语义分割任务指南](../tasks/semantic_segmentation)

如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新东西，而不是重复现有资源。

## BEiT特定输出

### `class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L69)

```py
( last_hidden_state: FloatTensor = None pooler_output: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )
```

参数

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 如果*config.use_mean_pooling*设置为True，则是补丁标记的最后一层隐藏状态的平均值（不包括*[CLS]*标记）。如果设置为False，则将返回*[CLS]*标记的最终隐藏状态。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

用于[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)输出的类。

### `class transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L44)

```py
( last_hidden_state: Array = None pooler_output: Array = None hidden_states: Optional = None attentions: Optional = None )
```

参数

+   `last_hidden_state` (`jnp.ndarray`，形状为 `(batch_size, sequence_length, hidden_size)`） — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`jnp.ndarray`，形状为 `(batch_size, hidden_size)`） — 如果 *config.use_mean_pooling* 设置为 True，则为补丁标记的最后一层隐藏状态的平均值（不包括 *[CLS]* 标记）。如果设置为 False，则将返回 *[CLS]* 标记的最终隐藏状态。

+   `hidden_states` (`tuple(jnp.ndarray)`，*可选*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入的输出 + 一个用于每层的输出）。模型在每层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

用于 [FlaxBeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.FlaxBeitModel) 输出的类。

## BeitConfig

### `class transformers.BeitConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/configuration_beit.py#L37)

```py
( vocab_size = 8192 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 use_mask_token = False use_absolute_position_embeddings = False use_relative_position_bias = False use_shared_relative_position_bias = False layer_scale_init_value = 0.1 drop_path_rate = 0.1 use_mean_pooling = True pool_scales = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input = False semantic_loss_ignore_index = 255 out_features = None out_indices = None add_fpn = False reshape_hidden_states = True **kwargs )
```

参数

+   `vocab_size` (`int`，*可选*，默认为 8192) — BEiT 模型的词汇表大小。定义了在预训练期间可以使用的不同图像标记数量。

+   `hidden_size` (`int`，*可选*，默认为 768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`，*可选*，默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`，*可选*，默认为 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`，*可选*，默认为 3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` 或 `function`，*可选*，默认为 `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"selu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`，*可选*，默认为 0.0) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `attention_probs_dropout_prob` (`float`，*可选*，默认为 0.0) — 注意力概率的丢弃比率。

+   `initializer_range` (`float`，*可选*，默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`，*可选*，默认为 1e-12) — 层归一化层使用的 epsilon。

+   `image_size` (`int`，*可选*，默认为 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`，*可选*，默认为 16) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`，*可选*，默认为 3) — 输入通道数。

+   `use_mask_token` (`bool`，*可选*，默认为 `False`) — 是否为遮罩图像建模使用遮罩标记。

+   `use_absolute_position_embeddings` (`bool`，*可选*，默认为 `False`) — 是否使用类似BERT的绝对位置嵌入。

+   `use_relative_position_bias` (`bool`，*可选*，默认为 `False`) — 是否在自注意力层中使用 T5 风格的相对位置嵌入。

+   `use_shared_relative_position_bias` (`bool`, *可选*, 默认为 `False`) — 是否在 Transformer 的所有自注意力层中使用相同的相对位置嵌入。

+   `layer_scale_init_value` (`float`, *可选*, 默认为 0.1) — 在自注意力层中使用的比例。基础为 0.1，大型为 1e-5。设置为 0 以禁用层比例。

+   `drop_path_rate` (`float`, *可选*, 默认为 0.1) — 每个样本的随机深度率（应用于残差层的主路径）。

+   `use_mean_pooling` (`bool`, *可选*, 默认为 `True`) — 是否对补丁的最终隐藏状态进行均值池化，而不是使用 CLS 标记的最终隐藏状态后应用分类头。

+   `pool_scales` (`Tuple[int]`, *可选*, 默认为 `[1, 2, 3, 6]`) — 在最后一个特征图上应用的池化金字塔模块中使用的池化比例。

+   `use_auxiliary_head` (`bool`, *可选*, 默认为 `True`) — 是否在训练过程中使用辅助头。

+   `auxiliary_loss_weight` (`float`, *可选*, 默认为 0.4) — 辅助头的交叉熵损失的权重。

+   `auxiliary_channels` (`int`, *可选*, 默认为 256) — 辅助头中要使用的通道数。

+   `auxiliary_num_convs` (`int`, *可选*, 默认为 1) — 辅助头中要使用的卷积层数。

+   `auxiliary_concat_input` (`bool`, *可选*, 默认为 `False`) — 是否在分类层之前将辅助头的输出与输入进行连接。

+   `semantic_loss_ignore_index` (`int`, *可选*, 默认为 255) — 语义分割模型的损失函数中被忽略的索引。

+   `out_features` (`List[str]`, *可选*) — 如果用作骨干，要输出的特征列表。可以是 `"stem"`、`"stage1"`、`"stage2"` 等（取决于模型有多少阶段）。如果未设置且设置了 `out_indices`，将默认为相应的阶段。如果未设置且未设置 `out_indices`，将默认为最后一个阶段。必须按照 `stage_names` 属性中定义的顺序。

+   `out_indices` (`List[int]`, *可选*) — 如果用作骨干，要输出的特征的索引列表。可以是 0、1、2 等（取决于模型有多少阶段）。如果未设置且设置了 `out_features`，将默认为相应的阶段。如果未设置且未设置 `out_features`，将默认为最后一个阶段。必须按照 `stage_names` 属性中定义的顺序。

+   `add_fpn` (`bool`, *可选*, 默认为 `False`) — 是否将 FPN 添加为骨干的一部分。仅适用于 `BeitBackbone`。

+   `reshape_hidden_states` (`bool`, *可选*, 默认为 `True`) — 是否在将模型用作骨干时将特征图重塑为形状为 `(batch_size, hidden_size, height, width)` 的4D张量。如果为 `False`，特征图将是形状为 `(batch_size, seq_len, hidden_size)` 的3D张量。仅适用于 `BeitBackbone`。

这是一个配置类，用于存储 [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel) 的配置。根据指定的参数实例化一个 BEiT 模型，定义模型架构。使用默认值实例化配置将产生类似于 BEiT [microsoft/beit-base-patch16-224-pt22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k) 架构的配置。

示例：

```py
>>> from transformers import BeitConfig, BeitModel

>>> # Initializing a BEiT beit-base-patch16-224-pt22k style configuration
>>> configuration = BeitConfig()

>>> # Initializing a model (with random weights) from the beit-base-patch16-224-pt22k style configuration
>>> model = BeitModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## BeitFeatureExtractor

### `class transformers.BeitFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/feature_extraction_beit.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L307)

```py
( images segmentation_maps = None **kwargs )
```

#### `post_process_semantic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)

```py
( outputs target_sizes: List = None ) → export const metadata = 'undefined';semantic_segmentation
```

参数

+   `outputs`（[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)） — 模型的原始输出。

+   `target_sizes`（长度为 `batch_size` 的 `List[Tuple]`，*可选*） — 每个预测的请求最终尺寸（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。

返回

语义分割

长度为 `batch_size` 的 `List[torch.Tensor]`，每个项目是形状为 (height, width) 的语义分割地图，对应于 `target_sizes` 条目（如果指定）。每个 `torch.Tensor` 的每个条目对应于语义类别 id。

将 [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation) 的输出转换为语义分割地图。仅支持 PyTorch。

## BeitImageProcessor

### `class transformers.BeitImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L49)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None rescale_factor: Union = 0.00392156862745098 do_rescale: bool = True do_normalize: bool = True image_mean: Union = None image_std: Union = None do_reduce_labels: bool = False **kwargs )
```

参数

+   `do_resize` (`bool`，*可选*，默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的 `size`。可以通过 `preprocess` 方法中的 `do_resize` 参数进行覆盖。

+   `size` (`Dict[str, int]` *可选*，默认为 `{"height" -- 256, "width": 256}`)：调整大小后的输出图像尺寸。可以通过 `preprocess` 方法中的 `size` 参数进行覆盖。

+   `resample` (`PILImageResampling`，*可选*，默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则要使用的重采样滤波器。可以通过 `preprocess` 方法中的 `resample` 参数进行覆盖。

+   `do_center_crop` (`bool`，*可选*，默认为 `True`) — 是否对图像进行中心裁剪。如果输入尺寸沿任何边缘小于 `crop_size`，则图像将填充为 0，然后进行中心裁剪。可以通过 `preprocess` 方法中的 `do_center_crop` 参数进行覆盖。

+   `crop_size` (`Dict[str, int]`，*可选*，默认为 `{"height" -- 224, "width": 224}`)：应用中心裁剪时的期望输出尺寸。仅在 `do_center_crop` 设置为 `True` 时有效。可以通过 `preprocess` 方法中的 `crop_size` 参数进行覆盖。

+   `rescale_factor` (`int` 或 `float`，*可选*，默认为 `1/255`) — 如果重新缩放图像，则使用的缩放因子。可以通过 `preprocess` 方法中的 `rescale_factor` 参数进行覆盖。

+   `do_rescale` (`bool`，*可选*，默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以通过 `preprocess` 方法中的 `do_rescale` 参数进行覆盖。

+   `do_normalize` (`bool`，*可选*，默认为 `True`) — 是否对图像进行归一化。可以通过 `preprocess` 方法中的 `do_normalize` 参数进行覆盖。

+   `image_mean` (`float` 或 `List[float]`，*可选*，默认为 `IMAGENET_STANDARD_MEAN`) — 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以通过 `preprocess` 方法中的 `image_mean` 参数进行覆盖。

+   `image_std` (`float` 或 `List[float]`，*可选*，默认为 `IMAGENET_STANDARD_STD`) — 如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以通过 `preprocess` 方法中的 `image_std` 参数进行覆盖。

+   `do_reduce_labels` (`bool`，*可选*，默认为 `False`) — 是否减少所有分割地图的标签值。通常用于数据集中将 0 用于背景，且背景本身不包含在数据集的所有类中（例如 ADE20k）。背景标签将被替换为 255。可以通过 `preprocess` 方法中的 `do_reduce_labels` 参数进行覆盖。

构建 BEiT 图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L312)

```py
( images: Union segmentation_maps: Union = None do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 预处理的图像。期望单个或批量图像，像素值范围为0到255。如果传入像素值在0到1之间的图像，请设置`do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为`self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为`self.size`) — 调整大小后的图像大小。

+   `resample` (`int`, *可选*, 默认为`self.resample`) — 调整图像大小时要使用的重采样滤波器。这可以是枚举`PILImageResampling`之一，仅在设置`do_resize=True`时有效。

+   `do_center_crop` (`bool`, *可选*, 默认为`self.do_center_crop`) — 是否对图像进行中心裁剪。

+   `crop_size` (`Dict[str, int]`, *可选*, 默认为`self.crop_size`) — 中心裁剪后的图像大小。如果图像的一条边小于`crop_size`，则将用零填充，然后裁剪。

+   `do_rescale` (`bool`, *可选*, 默认为`self.do_rescale`) — 是否将图像值重新缩放为[0 - 1]。

+   `rescale_factor` (`float`, *可选*, 默认为`self.rescale_factor`) — 如果设置`do_rescale=True`，则用于重新缩放图像的重新缩放因子。

+   `do_normalize` (`bool`, *可选*, 默认为`self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为`self.image_mean`) — 图像均值。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为`self.image_std`) — 图像标准差。

+   `do_reduce_labels` (`bool`, *可选*, 默认为`self.do_reduce_labels`) — 是否减少所有分割地图的标签值。通常用于数据集中使用0表示背景，并且背景本身不包含在数据集的所有类中（例如ADE20k）。背景标签将被替换为255。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个`np.ndarray`列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回一个类型为`tf.Tensor`的批量。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回一个类型为`torch.Tensor`的批量。

    +   `TensorType.NUMPY` 或 `'np'`：返回一个类型为`np.ndarray`的批量。

    +   `TensorType.JAX` 或 `'jax'`：返回一个类型为`jax.numpy.ndarray`的批量。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为`ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   `"none"` 或 `ChannelDimension.NONE`：图像以（高度，宽度）格式。

预处理图像或一批图像。

#### `post_process_semantic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)

```py
( outputs target_sizes: List = None ) → export const metadata = 'undefined';semantic_segmentation
```

参数

+   `outputs`（[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)） — 模型的原始输出。

+   `target_sizes` (`List[Tuple]`，长度为`batch_size`，*可选*) — 与每个预测的请求最终大小（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。

返回

语义分割

`List[torch.Tensor]` 长度为 `batch_size`，其中每个项目是形状为 (height, width) 的语义分割地图，对应于目标大小条目（如果指定了 `target_sizes`）。每个 `torch.Tensor` 的每个条目对应于一个语义类别 id。

将 [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation) 的输出转换为语义分割地图。仅支持PyTorch。

PytorchHide Pytorch content

## BeitModel

### `class transformers.BeitModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L620)

```py
( config: BeitConfig add_pooling_layer: bool = True )
```

参数

+   `config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的Beit模型变压器输出原始隐藏状态，没有特定的头部。这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L651)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.beit.modeling_beit.BeitModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取。有关详细信息，请参阅 [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)。

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被掩盖，

    +   0 表示头部被掩盖。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通的元组。

+   `bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*) — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。

返回

[transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling) 或 `tuple(torch.FloatTensor)`

一个 [transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling) 或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）和输入的各种元素。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 如果*config.use_mean_pooling*设置为True，则是补丁标记的最后一层隐藏状态的平均值（不包括*[CLS]*标记）。如果设置为False，则将返回*[CLS]*标记的最终隐藏状态。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层的输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, BeitModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
>>> model = BeitModel.from_pretrained("microsoft/beit-base-patch16-224-pt22k")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 197, 768]
```

## BeitForMaskedImageModeling

### `class transformers.BeitForMaskedImageModeling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L732)

```py
( config: BeitConfig )
```

参数

+   `config`（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Beit模型变压器顶部带有“语言”建模头。BEiT通过预测矢量量化变分自动编码器（VQ-VAE）的视觉标记来进行遮蔽图像建模，而其他视觉模型如ViT和DeiT则预测RGB像素值。因此，此类与[AutoModelForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMaskedImageModeling)不兼容，因此如果要使用BEiT进行遮蔽图像建模，则需要直接使用[BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L753)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)。

+   `head_mask`（`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*）- 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被遮蔽，

    +   0表示头部被遮蔽。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。查看返回张量中的 `hidden_states` 以获取更多细节。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `bool_masked_pos` (`torch.BoolTensor`，形状为 `(batch_size, num_patches)`) — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回值

[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput) 或 `tuple(torch.FloatTensor)`

[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 掩码语言建模（MLM）损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层的输出，则为嵌入层的输出 + 每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组。

    自注意力头中的注意力权重，用于计算自注意力头中的加权平均值。

[BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling
>>> import torch
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
>>> model = BeitForMaskedImageModeling.from_pretrained("microsoft/beit-base-patch16-224-pt22k")

>>> num_patches = (model.config.image_size // model.config.patch_size) ** 2
>>> pixel_values = image_processor(images=image, return_tensors="pt").pixel_values
>>> # create random boolean mask of shape (batch_size, num_patches)
>>> bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()

>>> outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
>>> loss, logits = outputs.loss, outputs.logits
>>> list(logits.shape)
[1, 196, 8192]
```

## BeitForImageClassification

### `class transformers.BeitForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L832)

```py
( config: BeitConfig )
```

参数

+   `config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

Beit 模型变压器，顶部带有图像分类头（线性层位于补丁标记的最终隐藏状态的平均值之上），例如用于 ImageNet。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L852)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)。

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被遮蔽，

    +   0表示头部被遮蔽。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回值

[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, BeitForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224")
>>> model = BeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## BeitForSemanticSegmentation

### `class transformers.BeitForSemanticSegmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1156)

```py
( config: BeitConfig )
```

参数

+   `config`（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Beit 模型变换器，顶部带有语义分割头，例如用于 ADE20k、CityScapes。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1214)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1 表示头部`未被掩盖`，

    +   0 表示头部`被掩盖`。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）- 用于计算损失的地面真实语义分割地图。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）和输入的各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 分类（或回归，如果`config.num_labels==1`）损失。

+   `logits`（形状为`(batch_size, config.num_labels, logits_height, logits_width)`的`torch.FloatTensor`）- 每个像素的分类分数。

    <tip warning="{true}">返回的logits不一定与作为输入传递的`pixel_values`具有相同的大小。这是为了避免进行两次插值并在用户需要将logits调整为原始图像大小时丢失一些质量。您应该始终检查您的logits形状并根据需要调整大小。</tip>

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, patch_size, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    模型在每个层的输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, BeitForSemanticSegmentation
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-finetuned-ade-640-640")
>>> model = BeitForSemanticSegmentation.from_pretrained("microsoft/beit-base-finetuned-ade-640-640")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> # logits are of shape (batch_size, num_labels, height, width)
>>> logits = outputs.logits
```

JAXHide JAX内容

## FlaxBeitModel

### `class transformers.FlaxBeitModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L741)

```py
( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）- 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可以用于在GPU或TPU上启用混合精度训练或半精度推理。如果指定，所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的数据类型，不影响模型参数的数据类型。`

    如果您希望更改模型参数的dtype，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

裸Beit模型变压器输出原始隐藏状态，没有特定的头部。

这个模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库实现的所有模型的通用方法（例如从PyTorch模型下载、保存和转换权重）。

这个模型也是一个[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)子类。将其用作常规的Flax linen模块，并参考Flax文档以获取与一般用法和行为相关的所有内容。

最后，这个模型支持JAX的固有特性，例如：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)

```py
( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling or tuple(torch.FloatTensor)
```

返回

[transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling) 或 `tuple(torch.FloatTensor)`

一个[transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`jnp.ndarray`，形状为`(batch_size, hidden_size)`) — 如果*config.use_mean_pooling*设置为True，则是补丁标记的最后一层隐藏状态的平均值（不包括*[CLS]*标记）。如果设置为False，则将返回*[CLS]*标记的最终隐藏状态。

+   `hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每个层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxBeitPreTrainedModel`的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, FlaxBeitModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k-ft22k")
>>> model = FlaxBeitModel.from_pretrained("microsoft/beit-base-patch16-224-pt22k-ft22k")

>>> inputs = image_processor(images=image, return_tensors="np")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## FlaxBeitForMaskedImageModeling

### `class transformers.FlaxBeitForMaskedImageModeling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L825)

```py
( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)） — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *可选*, 默认为`jax.numpy.float32`) — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的数据类型，不影响模型参数的数据类型。`

    如果希望更改模型参数的数据类型，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

在顶部添加一个“语言”建模头的Beit模型变换器（用于预测视觉标记）。

此模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如从PyTorch模型下载、保存和转换权重）。

此模型还是一个[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)子类。将其用作常规的Flax linen模块，并参考Flax文档以了解与一般用法和行为相关的所有事项。

最后，此模型支持JAX的固有特性，例如：

+   [即时编译（JIT）](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)

```py
( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

返回

[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`）和输入的不同元素。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`jnp.ndarray`） — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxBeitPreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

bool_masked_pos（形状为`(batch_size, num_patches)`的`numpy.ndarray`）：布尔掩码位置。指示哪些补丁被屏蔽（1），哪些没有（0）。

示例：

```py
>>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
>>> model = BeitForMaskedImageModeling.from_pretrained("microsoft/beit-base-patch16-224-pt22k")

>>> inputs = image_processor(images=image, return_tensors="np")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
```

## FlaxBeitForImageClassification

### `class transformers.FlaxBeitForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L909)

```py
( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *optional*, 默认为 `jax.numpy.float32`) — 计算的数据类型。可以是 `jax.numpy.float32`, `jax.numpy.float16` (在GPU上) 和 `jax.numpy.bfloat16` (在TPU上) 中的一个。

    这可以用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了，所有的计算将使用给定的 `dtype` 进行。

    `请注意，这只指定了计算的数据类型，不影响模型参数的数据类型。`

    如果您希望更改模型参数的数据类型，请参阅 [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) 和 [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

Beit 模型变换器，顶部带有一个图像分类头（一个线性层位于补丁标记的最终隐藏状态的平均值之上），例如用于 ImageNet。

这个模型继承自 [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载、保存和从 PyTorch 模型转换权重）。

这个模型也是一个 [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) 的子类。将其用作常规的 Flax linen 模块，并参考 Flax 文档以获取有关一般用法和行为的所有相关信息。

最后，这个模型支持 JAX 的内在特性，比如：

+   [即时编译 (JIT)](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)

```py
( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)
```

返回

[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置 (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`) 和输入的不同元素。

+   `logits` (`jnp.ndarray`，形状为 `(batch_size, config.num_labels)`) — 分类（如果 `config.num_labels==1` 则为回归）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每一层一个）。

    自注意力头中的注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxBeitPreTrainedModel` 的前向方法覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用 `Module` 实例，而不是在这里调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, FlaxBeitForImageClassification
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224")
>>> model = FlaxBeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224")

>>> inputs = image_processor(images=image, return_tensors="np")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
```
