# GroupViT

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/groupvit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/groupvit)

## 概述

GroupViT模型是由Jiarui Xu、Shalini De Mello、Sifei Liu、Wonmin Byeon、Thomas Breuel、Jan Kautz、Xiaolong Wang在[GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094)中提出的。受[CLIP](clip)启发，GroupViT是一种可以对任何给定词汇类别执行零调校语义分割的视觉语言模型。

论文摘要如下：

*分组和识别是视觉场景理解的重要组成部分，例如目标检测和语义分割。在端到端深度学习系统中，图像区域的分组通常是通过来自像素级别识别标签的自上而下监督隐式发生的。相反，在本文中，我们提出将分组机制重新引入深度网络中，这允许语义段仅通过文本监督自动出现。我们提出了一种分层分组视觉Transformer（GroupViT），它超越了常规的网格结构表示，并学会将图像区域分组成逐渐变大的任意形状的段。我们通过对比损失在大规模图像文本数据集上联合训练GroupViT和文本编码器。仅通过文本监督且没有任何像素级注释，GroupViT学会将语义区域组合在一起，并成功地以零调校的方式转移到语义分割任务，即无需进一步微调。在PASCAL VOC 2012数据集上实现了52.3%的零调校mIoU准确率，在PASCAL Context数据集上实现了22.4%的mIoU，并且与需要更高级别监督的最先进的迁移学习方法竞争力相当。*

该模型由[xvjiarui](https://huggingface.co/xvjiarui)贡献。TensorFlow版本由[ariG23498](https://huggingface.co/ariG23498)与[Yih-Dar SHIEH](https://huggingface.co/ydshieh)、[Amy Roberts](https://huggingface.co/amyeroberts)和[Joao Gante](https://huggingface.co/joaogante)的帮助下贡献。原始代码可以在[这里](https://github.com/NVlabs/GroupViT)找到。

## 使用提示

+   您可以在`GroupViTModel`的前向传递中指定`output_segmentation=True`以获取输入文本的分割logits。

## 资源

一系列官方Hugging Face和社区（由🌎表示）资源，可帮助您开始使用GroupViT。

+   开始使用GroupViT的最快方法是查看[示例笔记本](https://github.com/xvjiarui/GroupViT/blob/main/demo/GroupViT_hf_inference_notebook.ipynb)（展示了零调校分割推断）。

+   您也可以查看[HuggingFace Spaces演示](https://huggingface.co/spaces/xvjiarui/GroupViT)来体验GroupViT。

## GroupViTConfig

### `class transformers.GroupViTConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/configuration_groupvit.py#L272)

```py
( text_config = None vision_config = None projection_dim = 256 projection_intermediate_dim = 4096 logit_scale_init_value = 2.6592 **kwargs )
```

参数

+   `text_config` (`dict`, *可选*) — 用于初始化[GroupViTTextConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTTextConfig)的配置选项字典。

+   `vision_config` (`dict`, *可选*) — 用于初始化[GroupViTVisionConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTVisionConfig)的配置选项字典。

+   `projection_dim` (`int`, *可选*, 默认为256) — 文本和视觉投影层的维度。

+   `projection_intermediate_dim` (`int`, *可选*, 默认为4096) — 文本和视觉投影层的中间层的维度。

+   `logit_scale_init_value` (`float`, *可选*, 默认为2.6592) — *logit_scale*参数的初始值。默认值根据原始GroupViT实现使用。

+   `kwargs` (*可选*) — 关键字参数字典。

[GroupViTConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTConfig)是用于存储[GroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTModel)配置的配置类。它用于根据指定的参数实例化一个GroupViT模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生类似于GroupViT [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

#### `from_text_vision_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/configuration_groupvit.py#L397)

```py
( text_config: GroupViTTextConfig vision_config: GroupViTVisionConfig **kwargs ) → export const metadata = 'undefined';GroupViTConfig
```

返回

[GroupViTConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTConfig)

配置对象的一个实例

从groupvit文本模型配置和groupvit视觉模型配置实例化一个[GroupViTConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTConfig)（或派生类）。

## GroupViTTextConfig

### `class transformers.GroupViTTextConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/configuration_groupvit.py#L38)

```py
( vocab_size = 49408 hidden_size = 256 intermediate_size = 1024 num_hidden_layers = 12 num_attention_heads = 4 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 dropout = 0.0 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 49408) — GroupViT文本模型的词汇量。定义了在调用[GroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTModel)时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 256) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 1024) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 4) — Transformer编码器中每个注意力层的注意力头数。

+   `max_position_embeddings` (`int`, *optional*, defaults to 77) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。

+   `hidden_act` (`str`或`function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `dropout` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

这是用于存储[GroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTTextModel)配置的配置类。根据指定的参数实例化一个GroupViT模型，定义模型架构。使用默认值实例化配置将产生类似于GroupViT [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import GroupViTTextConfig, GroupViTTextModel

>>> # Initializing a GroupViTTextModel with nvidia/groupvit-gcc-yfcc style configuration
>>> configuration = GroupViTTextConfig()

>>> model = GroupViTTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## GroupViTVisionConfig

### `class transformers.GroupViTVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/configuration_groupvit.py#L147)

```py
( hidden_size = 384 intermediate_size = 1536 depths = [6, 3, 3] num_hidden_layers = 12 num_group_tokens = [64, 8, 0] num_output_groups = [64, 8, 8] num_attention_heads = 6 image_size = 224 patch_size = 16 num_channels = 3 hidden_act = 'gelu' layer_norm_eps = 1e-05 dropout = 0.0 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 assign_eps = 1.0 assign_mlp_ratio = [0.5, 4] **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 384) — 编码器层和池化器层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 1536) — Transformer编码器中“中间”（即前馈）层的维度。

+   `depths` (`List[int]`, *optional*, defaults to [6, 3, 3]) — 每个编码器块中的层数。

+   `num_group_tokens` (`List[int]`, *optional*, defaults to [64, 8, 0]) — 每个阶段的组令牌数。

+   `num_output_groups` (`List[int]`, *optional*, defaults to [64, 8, 8]) — 每个阶段的输出组数，0表示没有组。

+   `num_attention_heads` (`int`, *optional*, defaults to 6) — Transformer编码器中每个注意力层的注意力头数。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 16) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的epsilon。

+   `dropout` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

这是用于存储[GroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTVisionModel)配置的配置类。根据指定的参数实例化一个GroupViT模型，定义模型架构。使用默认值实例化配置将产生类似于GroupViT [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import GroupViTVisionConfig, GroupViTVisionModel

>>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration
>>> configuration = GroupViTVisionConfig()

>>> model = GroupViTVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

PytorchHide Pytorch content

## GroupViTModel

### `class transformers.GroupViTModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1302)

```py
( config: GroupViTConfig )
```

参数

+   `config` ([GroupViTConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法加载模型权重。

该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1445)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_segmentation: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.groupvit.modeling_groupvit.GroupViTModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1 代表未被掩盖的标记，

    +   0 代表被掩盖的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参见[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`，*optional*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.groupvit.modeling_groupvit.GroupViTModelOutput` 或 `tuple(torch.FloatTensor)`

一个`transformers.models.groupvit.modeling_groupvit.GroupViTModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置(`<class 'transformers.models.groupvit.configuration_groupvit.GroupViTConfig'>`)和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当`return_loss`为`True`时返回） — 图像-文本相似性的对比损失。

+   `logits_per_image` (`torch.FloatTensor`，形状为`(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text` (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`) — `text_embeds` 和 `image_embeds` 之间的缩放点积分数。这代表文本-图像相似性分数。

+   `segmentation_logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`) — 每个像素的分类分数。

    <tip warning="{true}">返回的对数不一定与传入的 `pixel_values` 具有相同的大小。这是为了避免进行两次插值并在用户需要将对数调整为原始图像大小时丢失一些质量。您应该始终检查您的对数形状并根据需要调整大小。</tip>

+   `text_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将[GroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTTextModel)的池化输出应用到投影层获得的文本嵌入。

+   `image_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将[GroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTVisionModel)的池化输出应用到投影层获得的图像嵌入。

+   `text_model_output` (`BaseModelOutputWithPooling`) — [GroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTTextModel)的输出。

+   `vision_model_output` (`BaseModelOutputWithPooling`) — [GroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTVisionModel)的输出。

[GroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, GroupViTModel

>>> model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1349)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。 

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]` 之间：

    +   1 代表`未被掩盖`的标记，

    +   0 代表`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

text_features (`torch.FloatTensor`，形状为`(batch_size, output_dim)`)

通过将投影层应用于[GroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTTextModel)的池化输出获得的文本嵌入。

[GroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import CLIPTokenizer, GroupViTModel

>>> model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> tokenizer = CLIPTokenizer.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1396)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下会忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

image_features (`torch.FloatTensor`，形状为`(batch_size, output_dim)`)

通过将投影层应用于[GroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTVisionModel)的池化输出获得的图像嵌入。

[GroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, GroupViTModel

>>> model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> image_features = model.get_image_features(**inputs)
```

## GroupViTTextModel

### `class transformers.GroupViTTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1139)

```py
( config: GroupViTTextConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1154)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下会忽略填充。

    可以使用[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记，值为1。

    +   对于被`masked`的标记，值为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（`<class 'transformers.models.groupvit.configuration_groupvit.GroupViTTextConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的输出处的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后，序列中第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力softmax后的注意力权重。

[GroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTTextModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import CLIPTokenizer, GroupViTTextModel

>>> tokenizer = CLIPTokenizer.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> model = GroupViTTextModel.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## GroupViTVisionModel

`class transformers.GroupViTVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1250)

```py
( config: GroupViTVisionConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_groupvit.py#L1263)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。默认情况下会忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.groupvit.configuration_groupvit.GroupViTVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 序列的第一个标记（分类标记）的最后一层隐藏状态，在通过用于辅助预训练任务的层进一步处理后。例如，对于BERT系列模型，这将返回通过线性层和tanh激活函数处理后的分类标记。线性层的权重是从下一个句子预测（分类）目标在预训练期间训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每一层输出的一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[GroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTVisionModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例而不是此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, GroupViTVisionModel

>>> processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> model = GroupViTVisionModel.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```

TensorFlowHide TensorFlow内容

## TFGroupViTModel

### `class transformers.TFGroupViTModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L1976)

```py
( config: GroupViTConfig *inputs **kwargs )
```

参数

+   `config`（[GroupViTConfig](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.GroupViTConfig））-模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

TF 2.0模型接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

这第二个选项在使用`tf.keras.Model.fit`方法时很有用，该方法目前要求在模型调用函数的第一个参数中具有所有张量：`model(inputs)`。

如果选择第二个选项，则有三种可能性可用于收集第一个位置参数中的所有输入张量：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L2069)

```py
( input_ids: TFModelInputType | None = None pixel_values: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None return_loss: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None output_segmentation: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.models.groupvit.modeling_tf_groupvit.TFGroupViTModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]` ``Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`）-词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入ID？](../glossary#input-ids)

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]` `Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为）-像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）-避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被屏蔽的标记为1，

    +   对于被屏蔽的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）-每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `return_loss`（`bool`，*可选*）-是否返回对比损失。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`，*可选*，默认为 `False“) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

`transformers.models.groupvit.modeling_tf_groupvit.TFGroupViTModelOutput` 或者 `tuple(tf.Tensor)`

`transformers.models.groupvit.modeling_tf_groupvit.TFGroupViTModelOutput` 或者一个 `tf.Tensor` 元组（如果传入 `return_dict=False` 或者 `config.return_dict=False`）包含不同元素，取决于配置（`<class 'transformers.models.groupvit.configuration_groupvit.GroupViTConfig'>`）和输入。

+   `loss` (`tf.Tensor`，形状为 `(1,)`，*可选*，当 `return_loss` 为 `True` 时返回） — 图像-文本相似性的对比损失。

+   `logits_per_image` (`tf.Tensor`，形状为 `(image_batch_size, text_batch_size)`) — `image_embeds` 和 `text_embeds` 之间的缩放点积分数。这代表了图像-文本相似性分数。

+   `logits_per_text` (`tf.Tensor`，形状为 `(text_batch_size, image_batch_size)`) — `text_embeds` 和 `image_embeds` 之间的缩放点积分数。这代表了文本-图像相似性分数。

+   `segmentation_logits` (`tf.Tensor`，形状为 `(batch_size, config.num_labels, logits_height, logits_width)`) — 每个像素的分类分数。

    <tip warning="{true}">返回的对数不一定与作为输入传递的 `pixel_values` 大小相同。这是为了避免进行两次插值并在用户需要将对数调整为原始图像大小时丢失一些质量。您应该始终检查您的对数形状并根据需要调整大小。</tip>

+   `text_embeds` (`tf.Tensor`，形状为 `(batch_size, output_dim`) — 通过将投影层应用于 [TFGroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTTextModel) 的汇聚输出获得的文本嵌入。

+   `image_embeds` (`tf.Tensor`，形状为 `(batch_size, output_dim`) — 通过将投影层应用于 [TFGroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTVisionModel) 的汇聚输出获得的图像嵌入。

+   `text_model_output` (`TFBaseModelOutputWithPooling`) — [TFGroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTTextModel) 的输出。

+   `vision_model_output` (`TFBaseModelOutputWithPooling`) — [TFGroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTVisionModel) 的输出。

[TFGroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在之后调用 `Module` 实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFGroupViTModel
>>> import tensorflow as tf

>>> model = TFGroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="tf", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L1985)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';text_features (tf.Tensor of shape (batch_size, output_dim)
```

参数

+   `input_ids`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    输入ID是什么？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。

    注意力掩码是什么？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    位置ID是什么？

+   `output_attentions`（*可选*，`bool`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（*可选*，`bool`）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict`（*可选*，`bool`）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为True。

+   `training`（*可选*，默认为`False`）— 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

返回

text_features（形状为`(batch_size, output_dim)`的`tf.Tensor`）

通过将投影层应用于[TFGroupViTTextModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTTextModel)的汇聚输出获得的文本嵌入。

[TFGroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTModel)的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import CLIPTokenizer, TFGroupViTModel

>>> model = TFGroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> tokenizer = CLIPTokenizer.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L2026)

```py
( pixel_values: TFModelInputType | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';image_features (tf.Tensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（*可选*，`bool`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *可选*，默认为 `False“) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回值

image_features (`tf.Tensor`，形状为 `(batch_size, output_dim`)

通过将投影层应用于 [TFGroupViTVisionModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTVisionModel) 的汇聚输出获得的图像嵌入。

[TFGroupViTModel](/docs/transformers/v4.37.2/en/model_doc/groupvit#transformers.TFGroupViTModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例:

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFGroupViTModel

>>> model = TFGroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="tf")

>>> image_features = model.get_image_features(**inputs)
```

## TFGroupViTTextModel

### `class transformers.TFGroupViTTextModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L1853)

```py
( config: GroupViTTextConfig *inputs **kwargs )
```

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L1862)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

参数

+   `input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` 或 `Dict[str, np.ndarray]`，每个示例的形状必须为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 和 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`np.ndarray` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]`：

    +   对于未被掩码的标记为 1，

    +   对于被掩码的标记为 0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`np.ndarray` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.groupvit.configuration_groupvit.GroupViTTextConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`tf.Tensor`）— 序列的第一个令牌（分类令牌）的最后一层隐藏状态，进一步由线性层和Tanh激活函数处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

    这个输出通常*不是*输入的语义内容的好摘要，通常最好对整个输入序列的隐藏状态进行平均或池化。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

TFGroupViTTextModel的forward方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数中定义，但应该在之后调用`Module`实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import CLIPTokenizer, TFGroupViTTextModel

>>> tokenizer = CLIPTokenizer.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> model = TFGroupViTTextModel.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## TFGroupViTVisionModel

### `class transformers.TFGroupViTVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L1914)

```py
( config: GroupViTVisionConfig *inputs **kwargs )
```

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/groupvit/modeling_tf_groupvit.py#L1923)

```py
( pixel_values: TFModelInputType | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例必须具有形状`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参见[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数只能在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式中将使用配置中的值。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可以在急切模式下使用，在图模式中该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`）- 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或`tuple(tf.Tensor)`

[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.groupvit.configuration_groupvit.GroupViTVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`tf.Tensor`）- 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和Tanh激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

    这个输出通常*不是*输入语义内容的好摘要，通常最好对整个输入序列的隐藏状态序列进行平均或池化。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出+一个用于每一层的输出）。

    模型在每一层输出处的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

TFGroupViTVisionModel的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFGroupViTVisionModel

>>> processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")
>>> model = TFGroupViTVisionModel.from_pretrained("nvidia/groupvit-gcc-yfcc")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="tf")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```
