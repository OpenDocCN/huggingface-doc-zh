["```py\nfrom datasets import load_dataset\n\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\")  # downloads the full dataset\nprint(imagenet[0])\n```", "```py\nfrom datasets import load_dataset\n\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\", streaming=True)  # will start loading the data when iterated over\nfor example in imagenet:\n    print(example)\n    break\n```", "```py\nmy_dataset = Dataset.from_dict({\"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})\nprint(my_dataset[0])\n```", "```py\ndef my_generator(n):\n    for i in range(n):\n        yield {\"col_1\": i}\n\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"n\": 10})\nfor example in my_iterable_dataset:\n    print(example)\n    break\n```", "```py\ndata_files = {\"train\": [\"path/to/data.csv\"]}\nmy_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\")\nprint(my_dataset[0])\n```", "```py\ndata_files = {\"train\": [\"path/to/data.csv\"]}\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\nfor example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset\n    print(example)\n    break\n```", "```py\nmy_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset\nprint(my_dataset[0])\n```", "```py\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_1)\nmy_iterable_dataset = my_iterable_dataset.filter(filter_fn)\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_2)\n\n# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\nfor example in my_iterable_dataset:  \n    print(example)\n    break\n```", "```py\nmy_dataset = my_dataset.shuffle(seed=42)\nprint(my_dataset[0])\n```", "```py\nmy_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\nfor example in my_iterable_dataset:\n    print(example)\n    break\n```", "```py\n# Stream from the internet\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True)\nmy_iterable_dataset.n_shards  # 39\n\n# Stream from local files\ndata_files = {\"train\": [f\"path/to/data_{i}.csv\" for i in range(1024)]}\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\nmy_iterable_dataset.n_shards  # 1024\n\n# From a generator function\ndef my_generator(n, sources):\n    for source in sources:\n        for example_id_for_current_source in range(n):\n            yield {\"example_id\": f\"{source}_{example_id_for_current_source}\"}\n\ngen_kwargs = {\"n\": 10, \"sources\": [f\"path/to/data_{i}\" for i in range(1024)]}\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)\nmy_iterable_dataset.n_shards  # 1024\n```", "```py\nmy_dataset[0]  # fast\nmy_dataset = my_dataset.shuffle(seed=42)\nmy_dataset[0]  # up to 10x slower\nmy_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\nmy_dataset[0]  # fast again\n```", "```py\nfor example in enumerate(my_iterable_dataset):  # fast\n    pass\n\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n\nfor example in enumerate(shuffled_iterable_dataset):  # as fast as before\n    pass\n\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # reshuffling using another seed is instantaneous\n\nfor example in enumerate(shuffled_iterable_dataset):  # still as fast as before\n    pass\n```", "```py\nfor epoch in range(n_epochs):\n    my_iterable_dataset.set_epoch(epoch)\n    for example in my_iterable_dataset:  # fast + reshuffled at each epoch using `effective_seed = seed + epoch`\n        pass\n```", "```py\nmy_iterable_dataset = my_dataset.to_iterable_dataset()\n```", "```py\nmy_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)\nmy_iterable_dataset.n_shards  # 1024\n```"]