- en: Automatic speech recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/asr](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/asr)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/TksaY_FDgnk](https://www.youtube-nocookie.com/embed/TksaY_FDgnk)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Automatic speech recognition (ASR) converts a speech signal to text, mapping
    a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa
    use ASR models to help users everyday, and there are many other useful user-facing
    applications like live captioning and note-taking during meetings.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will show you how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Finetune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset to transcribe audio to text.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your finetuned model for inference.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[Data2VecAudio](../model_doc/data2vec-audio), [Hubert](../model_doc/hubert),
    [M-CTC-T](../model_doc/mctct), [SEW](../model_doc/sew), [SEW-D](../model_doc/sew-d),
    [UniSpeech](../model_doc/unispeech), [UniSpeechSat](../model_doc/unispeech-sat),
    [Wav2Vec2](../model_doc/wav2vec2), [Wav2Vec2-BERT](../model_doc/wav2vec2-bert),
    [Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer), [WavLM](../model_doc/wavlm)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load MInDS-14 dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading a smaller subset of the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset from the ğŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and
    make sure everything works before spending more time training on the full dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Split the datasetâ€™s `train` split into a train and test set with the `~Dataset.train_test_split`
    method:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then take a look at the dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'While the dataset contains a lot of useful information, like `lang_id` and
    `english_transcription`, youâ€™ll focus on the `audio` and `transcription` in this
    guide. Remove the other columns with the [remove_columns](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.remove_columns)
    method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Take a look at the example again:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There are two fields:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '`audio`: a 1-dimensional `array` of the speech signal that must be called to
    load and resample the audio file.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transcription`: the target text.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to load a Wav2Vec2 processor to process the audio signal:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information
    in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which
    means youâ€™ll need to resample the dataset to 16000kHz to use the pretrained Wav2Vec2
    model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you can see in the `transcription` above, the text contains a mix of upper
    and lowercase characters. The Wav2Vec2 tokenizer is only trained on uppercase
    characters so youâ€™ll need to make sure the text matches the tokenizerâ€™s vocabulary:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now create a preprocessing function that:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Calls the `audio` column to load and resample the audio file.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracts the `input_values` from the audio file and tokenize the `transcription`
    column with the processor.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    function. You can speed up `map` by increasing the number of processes with the
    `num_proc` parameter. Remove the columns you donâ€™t need with the [remove_columns](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.remove_columns)
    method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ğŸ¤— Transformers doesnâ€™t have a data collator for ASR, so youâ€™ll need to adapt
    the [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)
    to create a batch of examples. Itâ€™ll also dynamically pad your text and labels
    to the length of the longest element in its batch (instead of the entire dataset)
    so they are a uniform length. While it is possible to pad your text in the `tokenizer`
    function by setting `padding=True`, dynamic padding is more efficient.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersæ²¡æœ‰ç”¨äºASRçš„æ•°æ®æ•´ç†å™¨ï¼Œå› æ­¤æ‚¨éœ€è¦è°ƒæ•´[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)ä»¥åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚å®ƒè¿˜ä¼šåŠ¨æ€å¡«å……æ‚¨çš„æ–‡æœ¬å’Œæ ‡ç­¾åˆ°å…¶æ‰¹æ¬¡ä¸­æœ€é•¿å…ƒç´ çš„é•¿åº¦ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼‰ï¼Œä»¥ä½¿å®ƒä»¬å…·æœ‰ç»Ÿä¸€çš„é•¿åº¦ã€‚è™½ç„¶å¯ä»¥é€šè¿‡åœ¨`tokenizer`å‡½æ•°ä¸­è®¾ç½®`padding=True`æ¥å¡«å……æ–‡æœ¬ï¼Œä½†åŠ¨æ€å¡«å……æ›´æœ‰æ•ˆã€‚
- en: 'Unlike other data collators, this specific data collator needs to apply a different
    padding method to `input_values` and `labels`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œè¿™ä¸ªç‰¹å®šçš„æ•°æ®æ•´ç†å™¨éœ€è¦å¯¹`input_values`å’Œ`labels`åº”ç”¨ä¸åŒçš„å¡«å……æ–¹æ³•ï¼š
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now instantiate your `DataCollatorForCTCWithPadding`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ä¾‹åŒ–æ‚¨çš„`DataCollatorForCTCWithPadding`ï¼š
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Evaluate
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: 'Including a metric during training is often helpful for evaluating your modelâ€™s
    performance. You can quickly load a evaluation method with the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [word error rate](https://huggingface.co/spaces/evaluate-metric/wer)
    (WER) metric (see the ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªæŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½[word
    error rate](https://huggingface.co/spaces/evaluate-metric/wer) (WER)æŒ‡æ ‡ï¼ˆæŸ¥çœ‹ğŸ¤— Evaluate
    [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡ï¼‰ï¼š
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the WER:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™`compute`ä»¥è®¡ç®—WERï¼š
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Your `compute_metrics` function is ready to go now, and youâ€™ll return to it
    when you setup your training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„`compute_metrics`å‡½æ•°å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ä¼šè¿”å›åˆ°å®ƒã€‚
- en: Train
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: PytorchHide Pytorch content
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[training#train-with-pytorch-trainer]ï¼
- en: 'Youâ€™re ready to start training your model now! Load Wav2Vec2 with [AutoModelForCTC](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCTC).
    Specify the reduction to apply with the `ctc_loss_reduction` parameter. It is
    often better to use the average instead of the default summation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[AutoModelForCTC](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCTC)åŠ è½½Wav2Vec2ã€‚æŒ‡å®šè¦åº”ç”¨çš„å‡å°‘é‡ï¼Œä½¿ç”¨`ctc_loss_reduction`å‚æ•°ã€‚é€šå¸¸æœ€å¥½ä½¿ç”¨å¹³å‡å€¼è€Œä¸æ˜¯é»˜è®¤çš„æ±‚å’Œï¼š
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'At this point, only three steps remain:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model). At the end of each epoch,
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the WER and save the training checkpoint.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•Hugging
    Faceæ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è¯„ä¼°WERå¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¼ é€’æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For a more in-depth example of how to finetune a model for automatic speech
    recognition, take a look at this blog [post](https://huggingface.co/blog/fine-tune-wav2vec2-english)
    for English ASR and this [post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)
    for multilingual ASR.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¦‚ä½•ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¾®è°ƒæ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡åšå®¢[post](https://huggingface.co/blog/fine-tune-wav2vec2-english)ä»¥è·å–è‹±è¯­ASRï¼Œä»¥åŠè¿™ç¯‡[post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)ä»¥è·å–å¤šè¯­è¨€ASRã€‚
- en: Inference
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼
- en: Load an audio file youâ€™d like to run inference on. Remember to resample the
    sampling rate of the audio file to match the sampling rate of the model if you
    need to!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½è¦è¿è¡Œæ¨ç†çš„éŸ³é¢‘æ–‡ä»¶ã€‚è®°å¾—é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡ä»¥åŒ¹é…æ¨¡å‹çš„é‡‡æ ·ç‡ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ï¼
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for automatic speech recognition with your model, and
    pass your audio file to it:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ¥è¿›è¡Œæ¨ç†æ˜¯å°è¯•æ‚¨å¾®è°ƒæ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•ã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„`pipeline`ï¼Œå¹¶å°†éŸ³é¢‘æ–‡ä»¶ä¼ é€’ç»™å®ƒï¼š
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The transcription is decent, but it could be better! Try finetuning your model
    on more examples to get even better results!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬å½•ç»“æœè¿˜ä¸é”™ï¼Œä½†å¯ä»¥æ›´å¥½ï¼å°è¯•åœ¨æ›´å¤šç¤ºä¾‹ä¸Šå¾®è°ƒæ‚¨çš„æ¨¡å‹ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š
- en: PytorchHide Pytorch content
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorchå†…å®¹
- en: 'Load a processor to preprocess the audio file and transcription and return
    the `input` as PyTorch tensors:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å¤„ç†å™¨ä»¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶å’Œè½¬å½•ï¼Œå¹¶å°†`input`è¿”å›ä¸ºPyTorchå¼ é‡ï¼š
- en: '[PRE21]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Pass your inputs to the model and return the logits:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›logitsï¼š
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Get the predicted `input_ids` with the highest probability, and use the processor
    to decode the predicted `input_ids` back into text:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„é¢„æµ‹`input_ids`ï¼Œå¹¶ä½¿ç”¨å¤„ç†å™¨å°†é¢„æµ‹çš„`input_ids`è§£ç å›æ–‡æœ¬ï¼š
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
