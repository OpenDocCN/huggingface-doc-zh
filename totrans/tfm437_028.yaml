- en: Automatic speech recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/asr](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/asr)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/TksaY_FDgnk](https://www.youtube-nocookie.com/embed/TksaY_FDgnk)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Automatic speech recognition (ASR) converts a speech signal to text, mapping
    a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa
    use ASR models to help users everyday, and there are many other useful user-facing
    applications like live captioning and note-taking during meetings.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will show you how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Finetune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset to transcribe audio to text.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your finetuned model for inference.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[Data2VecAudio](../model_doc/data2vec-audio), [Hubert](../model_doc/hubert),
    [M-CTC-T](../model_doc/mctct), [SEW](../model_doc/sew), [SEW-D](../model_doc/sew-d),
    [UniSpeech](../model_doc/unispeech), [UniSpeechSat](../model_doc/unispeech-sat),
    [Wav2Vec2](../model_doc/wav2vec2), [Wav2Vec2-BERT](../model_doc/wav2vec2-bert),
    [Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer), [WavLM](../model_doc/wavlm)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load MInDS-14 dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading a smaller subset of the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset from the 🤗 Datasets library. This’ll give you a chance to experiment and
    make sure everything works before spending more time training on the full dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Split the dataset’s `train` split into a train and test set with the `~Dataset.train_test_split`
    method:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then take a look at the dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'While the dataset contains a lot of useful information, like `lang_id` and
    `english_transcription`, you’ll focus on the `audio` and `transcription` in this
    guide. Remove the other columns with the [remove_columns](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.remove_columns)
    method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Take a look at the example again:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There are two fields:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '`audio`: a 1-dimensional `array` of the speech signal that must be called to
    load and resample the audio file.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transcription`: the target text.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to load a Wav2Vec2 processor to process the audio signal:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information
    in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which
    means you’ll need to resample the dataset to 16000kHz to use the pretrained Wav2Vec2
    model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you can see in the `transcription` above, the text contains a mix of upper
    and lowercase characters. The Wav2Vec2 tokenizer is only trained on uppercase
    characters so you’ll need to make sure the text matches the tokenizer’s vocabulary:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now create a preprocessing function that:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Calls the `audio` column to load and resample the audio file.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracts the `input_values` from the audio file and tokenize the `transcription`
    column with the processor.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To apply the preprocessing function over the entire dataset, use 🤗 Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    function. You can speed up `map` by increasing the number of processes with the
    `num_proc` parameter. Remove the columns you don’t need with the [remove_columns](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.remove_columns)
    method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 🤗 Transformers doesn’t have a data collator for ASR, so you’ll need to adapt
    the [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)
    to create a batch of examples. It’ll also dynamically pad your text and labels
    to the length of the longest element in its batch (instead of the entire dataset)
    so they are a uniform length. While it is possible to pad your text in the `tokenizer`
    function by setting `padding=True`, dynamic padding is more efficient.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers没有用于ASR的数据整理器，因此您需要调整[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)以创建一批示例。它还会动态填充您的文本和标签到其批次中最长元素的长度（而不是整个数据集），以使它们具有统一的长度。虽然可以通过在`tokenizer`函数中设置`padding=True`来填充文本，但动态填充更有效。
- en: 'Unlike other data collators, this specific data collator needs to apply a different
    padding method to `input_values` and `labels`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他数据整理器不同，这个特定的数据整理器需要对`input_values`和`labels`应用不同的填充方法：
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now instantiate your `DataCollatorForCTCWithPadding`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在实例化您的`DataCollatorForCTCWithPadding`：
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Evaluate
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: 'Including a metric during training is often helpful for evaluating your model’s
    performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [word error rate](https://huggingface.co/spaces/evaluate-metric/wer)
    (WER) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中包含一个指标通常有助于评估模型的性能。您可以使用🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载一个评估方法。对于这个任务，加载[word
    error rate](https://huggingface.co/spaces/evaluate-metric/wer) (WER)指标（查看🤗 Evaluate
    [快速入门](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算指标）：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the WER:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个函数，将您的预测和标签传递给`compute`以计算WER：
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Your `compute_metrics` function is ready to go now, and you’ll return to it
    when you setup your training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`compute_metrics`函数已经准备就绪，当您设置训练时会返回到它。
- en: Train
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: PytorchHide Pytorch content
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: If you aren’t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)微调模型，请查看这里的基本教程[training#train-with-pytorch-trainer]！
- en: 'You’re ready to start training your model now! Load Wav2Vec2 with [AutoModelForCTC](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCTC).
    Specify the reduction to apply with the `ctc_loss_reduction` parameter. It is
    often better to use the average instead of the default summation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好开始训练您的模型了！使用[AutoModelForCTC](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCTC)加载Wav2Vec2。指定要应用的减少量，使用`ctc_loss_reduction`参数。通常最好使用平均值而不是默认的求和：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'At this point, only three steps remain:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，只剩下三个步骤：
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. You’ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model). At the end of each epoch,
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the WER and save the training checkpoint.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中定义您的训练超参数。唯一必需的参数是`output_dir`，指定保存模型的位置。通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging
    Face才能上传模型）。在每个时代结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估WER并保存训练检查点。
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，同时还需要传递模型、数据集、分词器、数据整理器和`compute_metrics`函数。
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，以便每个人都可以使用您的模型：
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For a more in-depth example of how to finetune a model for automatic speech
    recognition, take a look at this blog [post](https://huggingface.co/blog/fine-tune-wav2vec2-english)
    for English ASR and this [post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)
    for multilingual ASR.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何为自动语音识别微调模型的更深入示例，请查看这篇博客[post](https://huggingface.co/blog/fine-tune-wav2vec2-english)以获取英语ASR，以及这篇[post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)以获取多语言ASR。
- en: Inference
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: Great, now that you’ve finetuned a model, you can use it for inference!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在您已经微调了一个模型，可以用它进行推理！
- en: Load an audio file you’d like to run inference on. Remember to resample the
    sampling rate of the audio file to match the sampling rate of the model if you
    need to!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 加载要运行推理的音频文件。记得重新采样音频文件的采样率以匹配模型的采样率（如果需要的话）！
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for automatic speech recognition with your model, and
    pass your audio file to it:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)来进行推理是尝试您微调模型的最简单方法。使用您的模型实例化一个用于自动语音识别的`pipeline`，并将音频文件传递给它：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The transcription is decent, but it could be better! Try finetuning your model
    on more examples to get even better results!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 转录结果还不错，但可以更好！尝试在更多示例上微调您的模型，以获得更好的结果！
- en: 'You can also manually replicate the results of the `pipeline` if you’d like:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，也可以手动复制`pipeline`的结果：
- en: PytorchHide Pytorch content
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch内容
- en: 'Load a processor to preprocess the audio file and transcription and return
    the `input` as PyTorch tensors:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 加载处理器以预处理音频文件和转录，并将`input`返回为PyTorch张量：
- en: '[PRE21]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Pass your inputs to the model and return the logits:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的输入传递给模型并返回logits：
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Get the predicted `input_ids` with the highest probability, and use the processor
    to decode the predicted `input_ids` back into text:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 获取具有最高概率的预测`input_ids`，并使用处理器将预测的`input_ids`解码回文本：
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
