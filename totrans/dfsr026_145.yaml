- en: InstructPix2Pix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/diffusers/api/pipelines/pix2pix](https://huggingface.co/docs/diffusers/api/pipelines/pix2pix)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/59.95b7d60a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/ExampleCodeBlock.658f5cd6.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructPix2Pix: Learning to Follow Image Editing Instructions](https://huggingface.co/papers/2211.09800)
    is by Tim Brooks, Aleksander Holynski and Alexei A. Efros.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We propose a method for editing images from human instructions: given an input
    image and a written instruction that tells the model what to do, our model follows
    these instructions to edit the image. To obtain training data for this problem,
    we combine the knowledge of two large pretrained models ‚Äî a language model (GPT-3)
    and a text-to-image model (Stable Diffusion) ‚Äî to generate a large dataset of
    image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained
    on our generated data, and generalizes to real images and user-written instructions
    at inference time. Since it performs edits in the forward pass and does not require
    per example fine-tuning or inversion, our model edits images quickly, in a matter
    of seconds. We show compelling editing results for a diverse collection of input
    images and written instructions.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can find additional information about InstructPix2Pix on the [project page](https://www.timothybrooks.com/instruct-pix2pix),
    [original codebase](https://github.com/timothybrooks/instruct-pix2pix), and try
    it out in a [demo](https://huggingface.co/spaces/timbrooks/instruct-pix2pix).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionInstructPix2PixPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.StableDiffusionInstructPix2PixPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L75)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet:
    UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker
    feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker:
    bool = True )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vae** ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet** ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    ‚Äî A `UNet2DConditionModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler** ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**safety_checker** (`StableDiffusionSafetyChecker`) ‚Äî Classification module
    that estimates whether generated images could be considered offensive or harmful.
    Please refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for more details about a model‚Äôs potential harms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**feature_extractor** ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    ‚Äî A `CLIPImageProcessor` to extract features from generated images; used as inputs
    to the `safety_checker`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for pixel-level image editing by following text instructions (based
    on Stable Diffusion).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L159)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: Union = None image: Union = None num_inference_steps: int = 100 guidance_scale:
    float = 7.5 image_guidance_scale: float = 1.5 negative_prompt: Union = None num_images_per_prompt:
    Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None
    prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image:
    Union = None output_type: Optional = ''pil'' return_dict: bool = True callback_on_step_end:
    Optional = None callback_on_step_end_tensor_inputs: List = [''latents''] **kwargs
    ) ‚Üí [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image** (`torch.FloatTensor` `np.ndarray`, `PIL.Image.Image`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, or `List[np.ndarray]`) ‚Äî `Image` or tensor representing
    an image batch to be repainted according to `prompt`. Can also accept image latents
    as `image`, but if passing latents directly it is not encoded again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 100) ‚Äî The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 7.5) ‚Äî A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_guidance_scale** (`float`, *optional*, defaults to 1.5) ‚Äî Push the
    generated image towards the inital `image`. Image guidance scale is enabled by
    setting `image_guidance_scale > 1`. Higher image guidance scale encourages generated
    images that are closely linked to the source `image`, usually at the expense of
    lower image quality. This pipeline requires a value of at least `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) ‚Äî The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eta** (`float`, *optional*, defaults to 0.0) ‚Äî Corresponds to parameter eta
    (Œ∑) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator`, *optional*) ‚Äî A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image ‚Äî (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) ‚Äî The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_on_step_end** (`Callable`, *optional*) ‚Äî A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_on_step_end_tensor_inputs** (`List`, *optional*) ‚Äî The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw)
    content.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### load_textual_inversion'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional
    = None text_encoder: Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pretrained_model_name_or_path** (`str` or `os.PathLike` or `List[str or os.PathLike]`
    or `Dict` or `List[Dict]`) ‚Äî Can be either one of the following or a list of them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`)
    of a pretrained model hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_text_inversion_directory/`) containing
    the textual inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *file* (for example `./my_text_inversions.pt`) containing textual
    inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str` or `List[str]`, *optional*) ‚Äî Override the token to use for
    the textual inversion weights. If `pretrained_model_name_or_path` is a list, then
    `token` must also be a list of equal length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel),
    *optional*) ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).
    If not specified, function will take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer),
    *optional*) ‚Äî A `CLIPTokenizer` to tokenize text. If not specified, function will
    take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weight_name** (`str`, *optional*) ‚Äî Name of a custom weight file. This should
    be used when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in ü§ó Diffusers format, but was saved under
    a specific weight name such as `text_inv.bin`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in the Automatic1111 format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cache_dir** (`Union[str, os.PathLike]`, *optional*) ‚Äî Path to a directory
    where a downloaded pretrained model configuration is cached if the standard cache
    is not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**force_download** (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resume_download** (`bool`, *optional*, defaults to `False`) ‚Äî Whether or
    not to resume downloading the model weights and configuration files. If set to
    `False`, any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**proxies** (`Dict[str, str]`, *optional*) ‚Äî A dictionary of proxy servers
    to use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_files_only** (`bool`, *optional*, defaults to `False`) ‚Äî Whether to
    only load local model weights and configuration files or not. If set to `True`,
    the model won‚Äôt be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str` or *bool*, *optional*) ‚Äî The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**revision** (`str`, *optional*, defaults to `"main"`) ‚Äî The specific model
    version to use. It can be a branch name, a tag name, a commit id, or any identifier
    allowed by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**subfolder** (`str`, *optional*, defaults to `""`) ‚Äî The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mirror** (`str`, *optional*) ‚Äî Mirror source to resolve accessibility issues
    if you‚Äôre downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load Textual Inversion embeddings into the text encoder of [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    (both ü§ó Diffusers and Automatic1111 formats are supported).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a Textual Inversion embedding vector in ü§ó Diffusers format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To load a Textual Inversion embedding vector in Automatic1111 format, make sure
    to download the vector first (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857))
    and then load the vector
  prefs: []
  type: TYPE_NORMAL
- en: 'locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### load_lora_weights'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pretrained_model_name_or_path_or_dict** (`str` or `os.PathLike` or `dict`)
    ‚Äî See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`dict`, *optional*) ‚Äî See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**adapter_name** (`str`, *optional*) ‚Äî Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: All kwargs are forwarded to `self.lora_state_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  prefs: []
  type: TYPE_NORMAL
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_lora_weights'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers:
    Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True
    weight_name: str = None save_function: Callable = None safe_serialization: bool
    = True )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**save_directory** (`str` or `os.PathLike`) ‚Äî Directory to save LoRA parameters
    to. Will be created if it doesn‚Äôt exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet_lora_layers** (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    ‚Äî State dict of the LoRA layers corresponding to the `unet`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder_lora_layers** (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    ‚Äî State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly
    pass the text encoder LoRA state dict because it comes from ü§ó Transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_main_process** (`bool`, *optional*, defaults to `True`) ‚Äî Whether the
    process calling this is the main process or not. Useful during distributed training
    and you need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**save_function** (`Callable`) ‚Äî The function to use to save the state dictionary.
    Useful during distributed training when you need to replace `torch.save` with
    another method. Can be configured with the environment variable `DIFFUSERS_SAVE_MODE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**safe_serialization** (`bool`, *optional*, defaults to `True`) ‚Äî Whether to
    save the model using `safetensors` or the traditional PyTorch way with `pickle`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the LoRA parameters corresponding to the UNet and text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_freeu'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L832)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_freeu'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L809)'
  prefs: []
  type: TYPE_NORMAL
- en: '( s1: float s2: float b1: float b2: float )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**s1** (`float`) ‚Äî Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**s2** (`float`) ‚Äî Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b1** (`float`) ‚Äî Scaling factor for stage 1 to amplify the contributions
    of backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b2** (`float`) ‚Äî Scaling factor for stage 2 to amplify the contributions
    of backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionXLInstructPix2PixPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.StableDiffusionXLInstructPix2PixPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L120)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection
    tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel
    scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True
    add_watermarker: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vae** ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** (`CLIPTextModel`) ‚Äî Frozen text-encoder. Stable Diffusion
    XL uses the text portion of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel),
    specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
    variant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder_2** ( `CLIPTextModelWithProjection`) ‚Äî Second frozen text-encoder.
    Stable Diffusion XL uses the text and pool portion of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
    specifically the [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
    variant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** (`CLIPTokenizer`) ‚Äî Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer_2** (`CLIPTokenizer`) ‚Äî Second Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet** ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    ‚Äî Conditional U-Net architecture to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler** ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**requires_aesthetics_score** (`bool`, *optional*, defaults to `"False"`) ‚Äî
    Whether the `unet` requires a aesthetic_score condition to be passed during inference.
    Also see the config of `stabilityai/stable-diffusion-xl-refiner-1-0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**force_zeros_for_empty_prompt** (`bool`, *optional*, defaults to `"True"`)
    ‚Äî Whether the negative prompt embeddings shall be forced to always be set to 0\.
    Also see the config of `stabilityai/stable-diffusion-xl-base-1-0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_watermarker** (`bool`, *optional*) ‚Äî Whether to use the [invisible_watermark
    library](https://github.com/ShieldMnt/invisible-watermark/) to watermark output
    images. If not defined, it will default to True if the package is installed, otherwise
    no watermarker will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for pixel-level image editing by following text instructions. Based
    on Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_lora_weights()` for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L652)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional
    = None width: Optional = None num_inference_steps: int = 100 denoising_end: Optional
    = None guidance_scale: float = 5.0 image_guidance_scale: float = 1.5 negative_prompt:
    Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional =
    1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds:
    Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds:
    Optional = None negative_pooled_prompt_embeds: Optional = None output_type: Optional
    = ''pil'' return_dict: bool = True callback: Optional = None callback_steps: int
    = 1 cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size:
    Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None )
    ‚Üí `~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to
    be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
    used in both text-encoders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image** (`torch.FloatTensor` or `PIL.Image.Image` or `np.ndarray` or `List[torch.FloatTensor]`
    or `List[PIL.Image.Image]` or `List[np.ndarray]`) ‚Äî The image(s) to modify with
    the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    ‚Äî The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    ‚Äî The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 50) ‚Äî The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**denoising_end** (`float`, *optional*) ‚Äî When specified, determines the fraction
    (between 0.0 and 1.0) of the total denoising process to be completed before it
    is intentionally prematurely terminated. As a result, the returned sample will
    still retain a substantial amount of noise as determined by the discrete timesteps
    selected by the scheduler. The denoising_end parameter should ideally be utilized
    when this pipeline forms a part of a ‚ÄúMixture of Denoisers‚Äù multi-pipeline setup,
    as elaborated in [**Refining the Image Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 5.0) ‚Äî Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_guidance_scale** (`float`, *optional*, defaults to 1.5) ‚Äî Image guidance
    scale is to push the generated image towards the inital image `image`. Image guidance
    scale is enabled by setting `image_guidance_scale > 1`. Higher image guidance
    scale encourages to generate images that are closely linked to the source image
    `image`, usually at the expense of lower image quality. This pipeline requires
    a value of at least `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) ‚Äî The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eta** (`float`, *optional*, defaults to 0.0) ‚Äî Corresponds to parameter eta
    (Œ∑) in the DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    will be ignored for others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) ‚Äî
    One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, pooled text embeddings will be generated from `prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) ‚Äî The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a `~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` instead
    of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback** (`Callable`, *optional*) ‚Äî A function that will be called every
    `callback_steps` steps during inference. The function will be called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_steps** (`int`, *optional*, defaults to 1) ‚Äî The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_kwargs** (`dict`, *optional*) ‚Äî A kwargs dictionary that
    if specified is passed along to the `AttentionProcessor` as defined under `self.processor`
    in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_rescale** (`float`, *optional*, defaults to 0.0) ‚Äî Guidance rescale
    factor proposed by [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)
    `guidance_scale` is defined as `œÜ` in equation 16\. of [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
    Guidance rescale factor should fix overexposure when using zero terminal SNR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**original_size** (`Tuple[int]`, *optional*, defaults to (1024, 1024)) ‚Äî If
    `original_size` is not the same as `target_size` the image will appear to be down-
    or upsampled. `original_size` defaults to `(height, width)` if not specified.
    Part of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**crops_coords_top_left** (`Tuple[int]`, *optional*, defaults to (0, 0)) ‚Äî
    `crops_coords_top_left` can be used to generate an image that appears to be ‚Äúcropped‚Äù
    from the position `crops_coords_top_left` downwards. Favorable, well-centered
    images are usually achieved by setting `crops_coords_top_left` to (0, 0). Part
    of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**target_size** (`Tuple[int]`, *optional*, defaults to (1024, 1024)) ‚Äî For
    most cases, `target_size` should be set to the desired height and width of the
    generated image. If not specified it will default to `(height, width)`. Part of
    SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**aesthetic_score** (`float`, *optional*, defaults to 6.0) ‚Äî Used to simulate
    an aesthetic score of the generated image by influencing the positive text condition.
    Part of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_aesthetic_score** (`float`, *optional*, defaults to 2.5) ‚Äî Part
    of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    Can be used to simulate an aesthetic score of the generated image by influencing
    the negative text condition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: '`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` if `return_dict`
    is True, otherwise a `tuple`. When returning a tuple, the first element is a list
    with the generated images.'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#### disable_freeu'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L648)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_vae_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L217)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_vae_tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L233)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_freeu'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L625)'
  prefs: []
  type: TYPE_NORMAL
- en: '( s1: float s2: float b1: float b2: float )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**s1** (`float`) ‚Äî Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**s2** (`float`) ‚Äî Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b1** (`float`) ‚Äî Scaling factor for stage 1 to amplify the contributions
    of backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b2** (`float`) ‚Äî Scaling factor for stage 2 to amplify the contributions
    of backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_vae_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L208)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Enable sliced VAE decoding.
  prefs: []
  type: TYPE_NORMAL
- en: When this option is enabled, the VAE will split the input tensor in slices to
    compute decoding in several steps. This is useful to save some memory and allow
    larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_vae_tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L224)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Enable tiled VAE decoding.
  prefs: []
  type: TYPE_NORMAL
- en: When this option is enabled, the VAE will split the input tensor into tiles
    to compute decoding and encoding in several steps. This is useful to save a large
    amount of memory and to allow the processing of larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### encode_prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L240)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt:
    int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None
    negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds:
    Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds:
    Optional = None lora_scale: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) ‚Äî prompt to be encoded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to
    be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
    used in both text-encoders device ‚Äî (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`) ‚Äî number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_classifier_free_guidance** (`bool`) ‚Äî whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, pooled text embeddings will be generated from `prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lora_scale** (`float`, *optional*) ‚Äî A lora scale that will be applied to
    all LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
