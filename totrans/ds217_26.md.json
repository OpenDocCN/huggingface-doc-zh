["```py\n>>> import datasets\n>>> metric = datasets.load_metric('my_metric')\n>>> for model_input, gold_references in evaluation_dataset:\n...     model_predictions = model(model_inputs)\n...     metric.add_batch(predictions=model_predictions, references=gold_references)\n>>> final_score = metric.compute()\n```", "```py\n>>> import datasets\n>>> metric = datasets.load_metric('sacrebleu')\n```", "```py\n>>> print(metric.inputs_description)\nProduces BLEU scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: The system stream (a sequence of segments).\n    references: A list of one or more reference streams (each a sequence of segments).\n    smooth_method: The smoothing method to use. (Default: 'exp').\n    smooth_value: The smoothing value. Only valid for 'floor' and 'add-k'. (Defaults: floor: 0.1, add-k: 1).\n    tokenize: Tokenization method to use for BLEU. If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for Japanese and '13a' (mteval) otherwise.\n    lowercase: Lowercase the data. If True, enables case-insensitivity. (Default: False).\n    force: Insist that your tokenized input is actually detokenized.\n...\n```", "```py\n>>> score = metric.compute(smooth_method=\"floor\", smooth_value=0.2)\n```", "```py\nclass Squad(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=datasets.Features(\n                {\n                    \"predictions\": {\"id\": datasets.Value(\"string\"), \"prediction_text\": datasets.Value(\"string\")},\n                    \"references\": {\n                        \"id\": datasets.Value(\"string\"),\n                        \"answers\": datasets.features.Sequence(\n                            {\n                                \"text\": datasets.Value(\"string\"),\n                                \"answer_start\": datasets.Value(\"int32\"),\n                            }\n                        ),\n                    },\n                }\n            ),\n            codebase_urls=[\"https://rajpurkar.github.io/SQuAD-explorer/\"],\n            reference_urls=[\"https://rajpurkar.github.io/SQuAD-explorer/\"],\n        )\n```", "```py\nCHECKPOINT_URLS = {\n    \"bleurt-tiny-128\": \"https://storage.googleapis.com/bleurt-oss/bleurt-tiny-128.zip\",\n    \"bleurt-tiny-512\": \"https://storage.googleapis.com/bleurt-oss/bleurt-tiny-512.zip\",\n    \"bleurt-base-128\": \"https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\",\n    \"bleurt-base-512\": \"https://storage.googleapis.com/bleurt-oss/bleurt-base-512.zip\",\n    \"bleurt-large-128\": \"https://storage.googleapis.com/bleurt-oss/bleurt-large-128.zip\",\n    \"bleurt-large-512\": \"https://storage.googleapis.com/bleurt-oss/bleurt-large-512.zip\",\n}\n```", "```py\ndef _download_and_prepare(self, dl_manager):\n\n    # check that config name specifies a valid BLEURT model\n    if self.config_name == \"default\":\n        logger.warning(\n            \"Using default BLEURT-Base checkpoint for sequence maximum length 128\\. \"\n            \"You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').\"\n        )\n        self.config_name = \"bleurt-base-128\"\n    if self.config_name not in CHECKPOINT_URLS.keys():\n        raise KeyError(\n            f\"{self.config_name} model not found. You should supply the name of a model checkpoint for bleurt in {CHECKPOINT_URLS.keys()}\"\n        )\n\n    # download the model checkpoint specified by self.config_name and set up the scorer\n    model_path = dl_manager.download_and_extract(CHECKPOINT_URLS[self.config_name])\n    self.scorer = score.BleurtScorer(os.path.join(model_path, self.config_name))\n```", "```py\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean().item()\n\ndef acc_and_f1(preds, labels):\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds).item()\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n    }\n\ndef pearson_and_spearman(preds, labels):\n    pearson_corr = pearsonr(preds, labels)[0].item()\n    spearman_corr = spearmanr(preds, labels)[0].item()\n    return {\n        \"pearson\": pearson_corr,\n        \"spearmanr\": spearman_corr,\n    }\n```", "```py\ndef _compute(self, predictions, references):\n    if self.config_name == \"cola\":\n        return {\"matthews_correlation\": matthews_corrcoef(references, predictions)}\n    elif self.config_name == \"stsb\":\n        return pearson_and_spearman(predictions, references)\n    elif self.config_name in [\"mrpc\", \"qqp\"]:\n        return acc_and_f1(predictions, references)\n    elif self.config_name in [\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]:\n        return {\"accuracy\": simple_accuracy(predictions, references)}\n    else:\n        raise KeyError(\n            \"You should supply a configuration name selected in \"\n            '[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '\n            '\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'\n        )\n```", "```py\n>>> from datasets import load_metric\n>>> metric = load_metric('PATH/TO/MY/SCRIPT.py')\n```"]