- en: Consuming Text Generation Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-generation-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/start.96d64f85.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/scheduler.9680c161.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/singletons.5632daf5.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.9d57cde4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/paths.5eca520f.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/app.48a2a24c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.38d74ee1.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/0.c01ff294.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/2.39ea5d7f.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/CodeBlock.1371964c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Heading.74c51a96.js">
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways you can consume Text Generation Inference server in your
    applications. After launching, you can use the `/generate` route and make a `POST`
    request to get results from the server. You can also use the `/generate_stream`
    route if you want TGI to return a stream of tokens. You can make the requests
    using the tool of your preference, such as curl, Python or TypeScrpt. For a final
    end-to-end experience, we also open-sourced ChatUI, a chat interface for open-source
    models.
  prefs: []
  type: TYPE_NORMAL
- en: curl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the launch, you can query the model using either the `/generate` or `/generate_stream`
    routes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Inference Client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)
    is a Python library to interact with the Hugging Face Hub, including its endpoints.
    It provides a nice high-level class, [`~huggingface_hub.InferenceClient`], which
    makes it easy to make calls to a TGI endpoint. `InferenceClient` also takes care
    of parameter validation and provides a simple to-use interface. You can simply
    install `huggingface-hub` package with pip.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once you start the TGI server, instantiate `InferenceClient()` with the URL
    to the endpoint serving the model. You can then call `text_generation()` to hit
    the endpoint through Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can do streaming with `InferenceClient` by passing `stream=True`. Streaming
    will return tokens as they are being generated in the server. To use streaming,
    you can do as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Another parameter you can use with TGI backend is `details`. You can get more
    details on generation (tokens, probabilities, etc.) by setting `details` to `True`.
    When itâ€™s specified, TGI will return a `TextGenerationResponse` or `TextGenerationStreamResponse`
    rather than a string or stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can see how to stream below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can check out the details of the function [here](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).
    There is also an async version of the client, `AsyncInferenceClient`, based on
    `asyncio` and `aiohttp`. You can find docs for it [here](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
  prefs: []
  type: TYPE_NORMAL
- en: ChatUI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatUI is an open-source interface built for LLM serving. It offers many customization
    options, such as web search with SERP API and more. ChatUI can automatically consume
    the TGI server and even provides an option to switch between different TGI endpoints.
    You can try it out at [Hugging Chat](https://huggingface.co/chat/), or use the
    [ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)
    to deploy your own Hugging Chat to Spaces.
  prefs: []
  type: TYPE_NORMAL
- en: To serve both ChatUI and TGI in same environment, simply add your own endpoints
    to the `MODELS` variable in `.env.local` file inside the `chat-ui` repository.
    Provide the endpoints pointing to where TGI is served.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradio is a Python library that helps you build web applications for your machine
    learning models with a few lines of code. It has a `ChatInterface` wrapper that
    helps create neat UIs for chatbots. Letâ€™s take a look at how to create a chatbot
    with streaming mode using TGI and Gradio. Letâ€™s install Gradio and Hub Python
    library first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Assume you are serving your model on port 8080, we will query through [InferenceClient](consuming_tgi#inference-client).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The UI looks like this ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)'
  prefs: []
  type: TYPE_IMG
- en: You can try the demo directly here ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
  prefs: []
  type: TYPE_NORMAL
- en: You can disable streaming mode using `return` instead of `yield` in your inference
    function, like below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can read more about how to customize a `ChatInterface` [here](https://www.gradio.app/guides/creating-a-chatbot-fast).
  prefs: []
  type: TYPE_NORMAL
- en: API documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can consult the OpenAPI documentation of the `text-generation-inference`
    REST API using the `/docs` route. The Swagger UI is also available [here](https://huggingface.github.io/text-generation-inference).
  prefs: []
  type: TYPE_NORMAL
