- en: Consuming Text Generation Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways you can consume Text Generation Inference server in your
    applications. After launching, you can use the `/generate` route and make a `POST`
    request to get results from the server. You can also use the `/generate_stream`
    route if you want TGI to return a stream of tokens. You can make the requests
    using the tool of your preference, such as curl, Python or TypeScrpt. For a final
    end-to-end experience, we also open-sourced ChatUI, a chat interface for open-source
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: curl
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the launch, you can query the model using either the `/generate` or `/generate_stream`
    routes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inference Client
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)
    is a Python library to interact with the Hugging Face Hub, including its endpoints.
    It provides a nice high-level class, [`~huggingface_hub.InferenceClient`], which
    makes it easy to make calls to a TGI endpoint. `InferenceClient` also takes care
    of parameter validation and provides a simple to-use interface. You can simply
    install `huggingface-hub` package with pip.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you start the TGI server, instantiate `InferenceClient()` with the URL
    to the endpoint serving the model. You can then call `text_generation()` to hit
    the endpoint through Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can do streaming with `InferenceClient` by passing `stream=True`. Streaming
    will return tokens as they are being generated in the server. To use streaming,
    you can do as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Another parameter you can use with TGI backend is `details`. You can get more
    details on generation (tokens, probabilities, etc.) by setting `details` to `True`.
    When it‚Äôs specified, TGI will return a `TextGenerationResponse` or `TextGenerationStreamResponse`
    rather than a string or stream.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see how to stream below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can check out the details of the function [here](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).
    There is also an async version of the client, `AsyncInferenceClient`, based on
    `asyncio` and `aiohttp`. You can find docs for it [here](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: ChatUI
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatUI is an open-source interface built for LLM serving. It offers many customization
    options, such as web search with SERP API and more. ChatUI can automatically consume
    the TGI server and even provides an option to switch between different TGI endpoints.
    You can try it out at [Hugging Chat](https://huggingface.co/chat/), or use the
    [ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)
    to deploy your own Hugging Chat to Spaces.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: To serve both ChatUI and TGI in same environment, simply add your own endpoints
    to the `MODELS` variable in `.env.local` file inside the `chat-ui` repository.
    Provide the endpoints pointing to where TGI is served.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Gradio
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradio is a Python library that helps you build web applications for your machine
    learning models with a few lines of code. It has a `ChatInterface` wrapper that
    helps create neat UIs for chatbots. Let‚Äôs take a look at how to create a chatbot
    with streaming mode using TGI and Gradio. Let‚Äôs install Gradio and Hub Python
    library first.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Assume you are serving your model on port 8080, we will query through [InferenceClient](consuming_tgi#inference-client).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The UI looks like this üëá
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: You can try the demo directly here üëá
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
- en: You can disable streaming mode using `return` instead of `yield` in your inference
    function, like below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•Âú®Êé®Êñ≠ÂáΩÊï∞‰∏≠‰ΩøÁî®`return`ËÄå‰∏çÊòØ`yield`Êù•Á¶ÅÁî®ÊµÅÂºèÊ®°ÂºèÔºåÂ∞±ÂÉè‰∏ãÈù¢ËøôÊ†∑„ÄÇ
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can read more about how to customize a `ChatInterface` [here](https://www.gradio.app/guides/creating-a-chatbot-fast).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•ÈòÖËØªÊúâÂÖ≥Â¶Ç‰ΩïËá™ÂÆö‰πâ`ChatInterface`ÁöÑÊõ¥Â§ö‰ø°ÊÅØ[ËøôÈáå](https://www.gradio.app/guides/creating-a-chatbot-fast)„ÄÇ
- en: API documentation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: APIÊñáÊ°£
- en: You can consult the OpenAPI documentation of the `text-generation-inference`
    REST API using the `/docs` route. The Swagger UI is also available [here](https://huggingface.github.io/text-generation-inference).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•‰ΩøÁî®`/docs`Ë∑ØÁî±Êü•ÈòÖ`text-generation-inference` REST APIÁöÑOpenAPIÊñáÊ°£„ÄÇSwagger UI‰πüÂèØÂú®[ËøôÈáå](https://huggingface.github.io/text-generation-inference)ÊâæÂà∞„ÄÇ
