# 优化LLMs的速度和内存

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial_optimization](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial_optimization)

诸如GPT3/4、[Falcon](https://huggingface.co/tiiuae/falcon-40b)和[Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf)等大型语言模型（LLMs）正在快速发展，能够处理以人类为中心的任务，成为现代知识型产业中不可或缺的工具。然而，在实际任务中部署这些模型仍然具有挑战性：

+   为了展示接近人类文本理解和生成能力，目前LLMs需要由数十亿参数组成（参见[Kaplan等人](https://arxiv.org/abs/2001.08361)，[Wei等人](https://arxiv.org/abs/2206.07682)）。这进一步增加了推断的内存需求。

+   在许多现实世界的任务中，LLMs需要提供广泛的上下文信息。这要求模型在推断过程中能够处理非常长的输入序列。

这些挑战的关键在于增强LLMs的计算和内存能力，特别是在处理庞大的输入序列时。

在本指南中，我们将介绍高效LLM部署的有效技术：

1.  **低精度：**研究表明，以降低的数值精度，即[8位和4位](./main_classes/quantization.md)可以在不显著降低模型性能的情况下实现计算优势。

1.  **快闪注意力：**快闪注意力是注意力算法的一种变体，不仅提供了更节省内存的方法，还通过优化GPU内存利用率实现了增加的效率。

1.  **架构创新：**考虑到LLMs在推断过程中始终以相同方式部署，即具有长输入上下文的自回归文本生成，已经提出了专门的模型架构，允许更高效的推断。在模型架构方面最重要的进展是[Alibi](https://arxiv.org/abs/2108.12409)、[Rotary embeddings](https://arxiv.org/abs/2104.09864)、[多查询注意力（MQA）](https://arxiv.org/abs/1911.02150)和[分组查询注意力（GQA）](https://arxiv.org/abs/2305.13245)。

在本指南中，我们将从张量的角度对自回归生成进行分析。我们深入探讨采用低精度的利弊，全面探索最新的注意力算法，并讨论改进的LLM架构。在此过程中，我们运行实际示例展示每个功能改进。

## 1. 低精度

通过将LLM视为一组权重矩阵和向量，将文本输入视为一系列向量，可以更好地理解LLMs的内存需求。在接下来的内容中，定义*权重*将用于表示所有模型权重矩阵和向量。

在撰写本指南时，LLMs至少包含数十亿参数。因此，每个参数由一个十进制数组成，例如`4.5689`，通常以[float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)、[bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)或[float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)格式存储。这使我们能够轻松计算加载LLM到内存所需的内存量：

> *加载具有X十亿参数的模型的权重大约需要4*X GB的VRAM，精度为float32*

如今，模型很少以完整的float32精度进行训练，而通常以bfloat16精度或更少的float16精度进行训练。因此，经验法则变为：

> *加载具有X十亿参数的模型的权重大约需要2*X GB的VRAM，精度为bfloat16/float16*

对于较短的文本输入（少于1024个标记），推理的内存需求主要受加载权重的内存需求支配。因此，现在让我们假设推理的内存需求等于将模型加载到GPU VRAM中的内存需求。

举例说明加载 bfloat16 模型大致需要多少 VRAM：

+   GPT3 需要 2 * 175 GB = 350 GB VRAM

+   [Bloom](https://huggingface.co/bigscience/bloom) 需要 2 * 176 GB = 352 GB VRAM

+   [Llama-2-70b](https://huggingface.co/meta-llama/Llama-2-70b-hf) 需要 2 * 70 GB = 140 GB VRAM

+   [Falcon-40b](https://huggingface.co/tiiuae/falcon-40b) 需要 2 * 40 GB = 80 GB VRAM

+   [MPT-30b](https://huggingface.co/mosaicml/mpt-30b) 需要 2 * 30 GB = 60 GB VRAM

+   [bigcode/starcoder](https://huggingface.co/bigcode/starcoder) 需要 2 * 15.5 = 31 GB VRAM

截至撰写本文时，市场上最大的GPU芯片是 A100 & H100，提供 80GB 的 VRAM。之前列出的大多数模型需要超过 80GB 的内存才能加载，因此必然需要张量并行处理和/或管道并行处理。

🤗 Transformers不支持张量并行处理，因为它要求模型架构以特定方式编写。如果您有兴趣以张量并行友好的方式编写模型，请随时查看[文本生成推理库](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling)。

天真的管道并行处理是开箱即用的。为此，只需使用 `device="auto"` 加载模型，它将自动将不同的层放置在可用的GPU上，如此处所述。请注意，尽管非常有效，但这种天真的管道并行处理并未解决GPU空闲的问题。为此，需要更高级的管道并行处理，如此处所述。

如果您可以访问一个 8 x 80GB A100 节点，您可以按照以下方式加载 BLOOM

```py
!pip install transformers accelerate bitsandbytes optimum
```

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom", device_map="auto", pad_token_id=0)
```

通过使用 `device_map="auto"`，注意力层将均匀分布在所有可用的GPU上。

在本指南中，我们将使用[bigcode/octocoder](https://huggingface.co/bigcode/octocoder)，因为它可以在单个 40 GB A100 GPU 设备芯片上运行。请注意，我们将要应用的所有内存和速度优化都同样适用于需要模型或张量并行处理的模型。

由于模型以 bfloat16 精度加载，根据我们上面的经验法则，我们预计使用 `bigcode/octocoder` 运行推理的内存需求约为 31 GB VRAM。让我们试一试。

我们首先加载模型和分词器，然后将两者传递给Transformers的[管道](https://huggingface.co/docs/transformers/main_classes/pipelines)对象。

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto", pad_token_id=0)
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
```

```py
prompt = "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer:"

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

**输出**：

```py
Here is a Python function that transforms bytes to Giga bytes:\n\n```python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n```py\n\nThis function takes a single
```

很好，我们现在可以直接使用结果将字节转换为千兆字节。

```py
def bytes_to_giga_bytes(bytes):
  return bytes / 1024 / 1024 / 1024
```

让我们调用[`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html)来测量GPU内存分配的峰值。

```py
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

**输出**：

```py
29.0260648727417
```

接近我们粗略计算的结果！我们可以看到数字并不完全正确，因为从字节到千字节需要乘以1024而不是1000。因此，粗略计算公式也可以理解为“最多XGB”的计算。请注意，如果我们尝试以完整的float32精度运行模型，将需要64GB的VRAM。

> 几乎所有模型现在都是在bfloat16中训练的，如果[您的GPU支持bfloat16](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5)，就没有理由以完整的float32精度运行模型。Float32不会比用于训练模型的精度提供更好的推断结果。

如果您不确定模型权重以哪种格式存储在Hub上，您可以随时查看检查点的配置，在`"torch_dtype"`下，例如[这里](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21)。建议在使用`from_pretrained(..., torch_dtype=...)`加载模型时，将模型设置为与配置中写入的相同精度类型，除非原始类型为float32，此时可以在推断中使用`float16`或`bfloat16`。

让我们定义一个`flush(...)`函数来释放所有分配的内存，以便我们可以准确地测量分配的GPU内存峰值。

```py
del pipe
del model

import gc
import torch

def flush():
  gc.collect()
  torch.cuda.empty_cache()
  torch.cuda.reset_peak_memory_stats()
```

现在让我们为下一个实验调用它。

```py
flush()
```

在最近的accelerate库版本中，您还可以使用一个名为`release_memory()`的实用方法

```py
from accelerate.utils import release_memory
# ...

release_memory(model)
```

那么如果您的GPU没有32GB的VRAM怎么办？已经发现模型权重可以量化为8位或4位而不会显著降低性能（参见[Dettmers等人](https://arxiv.org/abs/2208.07339)）。正如最近的[GPTQ论文](https://arxiv.org/abs/2210.17323)所示，模型可以量化为3位或2位，性能损失是可以接受的🤯。

不深入细节，量化方案旨在降低权重的精度，同时尽可能保持模型推断结果的准确性（即尽可能接近bfloat16）。请注意，量化在文本生成方面特别有效，因为我们只关心选择*最可能的下一个标记集*，并不真正关心下一个标记*logit*分布的确切值。重要的是下一个标记*logit*分布保持大致相同，以便`argmax`或`topk`操作给出相同的结果。

有各种量化技术，我们这里不会详细讨论，但总的来说，所有量化技术的工作方式如下：

+   1.  将所有权重量化为目标精度

+   1.  加载量化的权重，并以bfloat16精度传递输入序列的向量

+   1.  动态将权重去量化为bfloat16，以bfloat16精度执行计算

简而言之，这意味着*输入-权重矩阵*乘法，其中<math><semantics><mrow><mi>X</mi></mrow> <annotation encoding="application/x-tex">X</annotation></semantics></math> X是*输入*，<math><semantics><mrow><mi>W</mi></mrow> <annotation encoding="application/x-tex">W</annotation></semantics></math> W是权重矩阵，<math><semantics><mrow><mi>Y</mi></mrow> <annotation encoding="application/x-tex">Y</annotation></semantics></math> Y是输出：<math display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>∗</mo><mi>W</mi></mrow> <annotation encoding="application/x-tex">Y = X * W</annotation></semantics></math> Y=X∗W

被改变为<math display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>∗</mo><mtext>dequantize</mtext><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">Y = X * \text{dequantize}(W)</annotation></semantics></math> Y=X∗dequantize(W)

对于每个矩阵乘法。当输入通过网络图时，权重矩阵的反量化和重新量化是按顺序执行的。

因此，当使用量化权重时，推理时间通常**不会**减少，而是增加。足够的理论，让我们试一试！要使用Transformers量化权重，您需要确保已安装[`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes)库。

```py
!pip install bitsandbytes
```

然后，我们可以通过简单地在`from_pretrained`中添加`load_in_8bit=True`标志来加载8位量化的模型。

```py
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", load_in_8bit=True, pad_token_id=0)
```

现在，让我们再次运行我们的示例并测量内存使用情况。

```py
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

**输出**：

```py
Here is a Python function that transforms bytes to Giga bytes:\n\n```python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n```py\n\nThis function takes a single
```

很好，我们得到了与之前相同的结果，因此在准确性上没有损失！让我们看看这次使用了多少内存。

```py
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

**输出**：

```py
15.219234466552734
```

显著减少！我们只剩下略高于15GB，因此可以在像4090这样的消费级GPU上运行此模型。我们在内存效率上获得了非常好的收益，几乎没有对模型输出的降级。但是，在推理过程中我们也可以注意到略微减速。

我们删除模型并再次清空内存。

```py
del model
del pipe
```

```py
flush()
```

让我们看看4位量化对GPU内存消耗的峰值。将模型量化为4位可以通过与之前相同的API完成 - 这次是通过传递`load_in_4bit=True`而不是`load_in_8bit=True`来完成。

```py
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

**输出**：

```py
Here is a Python function that transforms bytes to Giga bytes:\n\n```\ndef bytes_to_gigabytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n```py\n\nThis function takes a single argument
```

我们几乎看到与之前相同的输出文本 - 只是在代码片段之前缺少了`python`。让我们看看需要多少内存。

```py
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

**输出**：

```py
9.543574333190918
```

只有9.5GB！对于一个超过15亿参数的模型来说，这真的不多。

虽然我们在这里看到模型准确性几乎没有下降，但实际上，4位量化通常会导致与8位量化或完整的`bfloat16`推理相比产生不同的结果。这取决于用户是否尝试。

还要注意，与8位量化相比，这里的推理速度再次稍慢一些，这是因为4位量化使用了更激进的量化方法，导致在推理过程中<math><semantics><mrow><mtext>量化</mtext></mrow></semantics></math>和<math><semantics><mrow><mtext>反量化</mtext></mrow></semantics></math>过程需要更长的时间。

```py
del model
del pipe
```

```py
flush()
```

总的来说，我们发现在8位精度下运行OctoCoder将所需的GPU VRAM从32G GPU VRAM减少到仅15GB，并且在4位精度下运行模型进一步将所需的GPU VRAM减少到略高于9GB。

4位量化使模型可以在RTX3090、V100和T4等GPU上运行，这对大多数人来说非常容易获得。

有关量化的更多信息以及如何将模型量化以便比4位更少地使用GPU VRAM内存，我们建议查看[`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60)实现。

> 最后，重要的是要记住，模型量化在内存效率和准确性之间进行了权衡，并且在某些情况下会增加推理时间。

如果GPU内存对您的用例不是限制，通常不需要考虑量化。但是许多GPU无法在没有量化方法的情况下运行LLMs，在这种情况下，4位和8位量化方案是非常有用的工具。

有关更详细的使用信息，我们强烈建议查看[Transformers量化文档](https://huggingface.co/docs/transformers/main_classes/quantization#general-usage)。接下来，让我们看看如何通过使用更好的算法和改进的模型架构来提高计算和内存效率。

## 2\. 闪光关注

今天表现最佳的LLMs基本上共享相同的基本架构，包括前馈层、激活层、层归一化层，以及最关键的自注意力层。

自注意力层对于大型语言模型（LLMs）至关重要，因为它们使模型能够理解输入标记之间的上下文关系。然而，自注意力层的峰值GPU内存消耗随着输入标记数量（也称为*序列长度*）的增加呈二次增长，我们在下文中用<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math> N 表示。虽然对于较短的输入序列（最多1000个输入标记）这并不明显，但对于较长的输入序列（大约16000个输入标记）则成为一个严重问题。

让我们仔细看看。计算自注意力层对于长度为<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math> N 的输入<math><semantics><mrow><mi mathvariant="bold">X</mi></mrow> <annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> X 的输出<math><semantics><mrow><mi mathvariant="bold">O</mi></mrow> <annotation encoding="application/x-tex">\mathbf{O}</annotation></semantics></math> O 的公式是：<math display="block"><semantics><mrow><mtext mathvariant="bold">O</mtext><mo>=</mo><mtext>Attn</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold">V</mi><mo>×</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup><mo stretchy="false">)</mo><mtext> with </mtext><mi mathvariant="bold">Q</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>q</mi></msub><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>v</mi></msub><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>k</mi></msub><mi mathvariant="bold">X</mi></mrow> <annotation encoding="application/x-tex">\textbf{O} = \text{Attn}(\mathbf{X}) = \mathbf{V} \times \text{Softmax}(\mathbf{QK}^T) \text{ with } \mathbf{Q} = \mathbf{W}_q \mathbf{X}, \mathbf{V} = \mathbf{W}_v \mathbf{X}, \mathbf{K} = \mathbf{W}_k \mathbf{X}</annotation></semantics></math> O=Attn(X)=V×Softmax(QKT) with Q=Wq​X,V=Wv​X,K=Wk​X <math><semantics><mrow><mi mathvariant="bold">X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\mathbf{X} = (\mathbf{x}_1, ... \mathbf{x}_{N})</annotation></semantics></math> X=(x1​,...xN​) 是注意力层的输入序列。投影<math><semantics><mrow><mi mathvariant="bold">Q</mi></mrow> <annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics></math> Q 和<math><semantics><mrow><mi mathvariant="bold">K</mi></mrow> <annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics></math> K 将分别包含<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math> N 个向量，导致<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT 的大小为<math><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow> <annotation encoding="application/x-tex">N^2</annotation></semantics></math> N2 。

LLMs通常具有多个注意力头，因此可以并行进行多个自注意力计算。假设LLM有40个注意力头并且以bfloat16精度运行，我们可以计算存储<math><semantics><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="bold">T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK^T}</annotation></semantics></math> QKT 矩阵所需的内存为<math><semantics><mrow><mn>40</mn><mo>∗</mo><mn>2</mn><mo>∗</mo><msup><mi>N</mi><mn>2</mn></msup></mrow> <annotation encoding="application/x-tex">40 * 2 * N^2</annotation></semantics></math> 40∗2∗N2 字节。对于<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow> <annotation encoding="application/x-tex">N=1000</annotation></semantics></math> N=1000，只需要大约50MB的VRAM，然而，对于<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>16000</mn></mrow> <annotation encoding="application/x-tex">N=16000</annotation></semantics></math> N=16000，我们将需要19GB的VRAM，而对于<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn><mo separator="true">,</mo><mn>000</mn></mrow> <annotation encoding="application/x-tex">N=100,000</annotation></semantics></math> N=100,000，我们将需要近1TB的VRAM来存储<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT 矩阵。

长话短说，对于大型输入上下文来说，默认的自注意力算法很快变得内存消耗过高。

随着LLMs在文本理解和生成方面的改进，它们被应用于越来越复杂的任务。虽然模型曾经处理几句话的翻译或总结，现在它们可以处理整页的内容，需要处理广泛的输入长度。

我们如何摆脱大型输入长度的过高内存需求？我们需要一种新的方式来计算自注意力机制，摆脱<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">QK^T</annotation></semantics></math> QKT 矩阵。[Tri Dao等人](https://arxiv.org/abs/2205.14135)开发了一种全新的算法，称之为**Flash Attention**。

简而言之，Flash Attention将<math><semantics><mrow><mi mathvariant="bold">V</mi><mo>×</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{V} \times \text{Softmax}(\mathbf{QK}^T</annotation></semantics></math>V×Softmax(QKT)计算分开，而是通过迭代多个softmax计算步骤来计算输出的较小块：<math display="block"><semantics><mrow><msub><mtext mathvariant="bold">O</mtext><mi>i</mi></msub><mo>←</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup><mo>∗</mo><msub><mtext mathvariant="bold">O</mtext><mi>i</mi></msub><mo>+</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup><mo>∗</mo><msub><mi mathvariant="bold">V</mi><mi>j</mi></msub><mo>×</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msubsup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>T</mi></msubsup><mo stretchy="false">)</mo><mtext> for multiple </mtext><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mtext> iterations</mtext></mrow> <annotation encoding="application/x-tex">\textbf{O}_i \leftarrow s^a_{ij} * \textbf{O}_i + s^b_{ij} * \mathbf{V}_{j} \times \text{Softmax}(\mathbf{QK}^T_{i,j}) \text{ for multiple } i, j \text{ iterations}</annotation></semantics></math> Oi​←sija​∗Oi​+sijb​∗Vj​×Softmax(QKi,jT​) for multiple i,j iterations

其中<math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup></mrow> <annotation encoding="application/x-tex">s^a_{ij}</annotation></semantics></math>和<math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup></mrow> <annotation encoding="application/x-tex">s^b_{ij}</annotation></semantics></math>是一些需要为每个<math><semantics><mrow><mi>i</mi></mrow> <annotation encoding="application/x-tex">i</annotation></semantics></math>和<math><semantics><mrow><mi>j</mi></mrow> <annotation encoding="application/x-tex">j</annotation></semantics></math>重新计算的softmax归一化统计量。

请注意，整个Flash Attention有点复杂，在这里大大简化了，因为深入讨论超出了本指南的范围。读者可以查看写得很好的[Flash Attention论文](https://arxiv.org/abs/2205.14135)以获取更多详细信息。

这里的主要要点是：

> 通过跟踪softmax归一化统计量，并使用一些智能数学，Flash Attention给出了与默认自注意力层**数值相同**的输出，而内存成本仅随<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>线性增加。

从公式来看，人们直觉地会说Flash Attention必须比默认的自注意力公式慢得多，因为需要进行更多的计算。事实上，与普通注意力相比，Flash Attention需要更多的FLOPs，因为softmax归一化统计量必须不断重新计算（如果感兴趣，请参阅[论文](https://arxiv.org/abs/2205.14135)获取更多详细信息）

> 然而，与默认注意力相比，Flash Attention在推理速度上要快得多，这是因为它能够显著减少对GPU（VRAM）较慢、高带宽内存的需求，而是专注于更快的片上内存（SRAM）。

基本上，Flash Attention确保所有中间写入和读取操作都可以使用快速的*片上*SRAM内存来完成，而无需访问较慢的VRAM内存来计算输出向量<math><semantics><mrow><mi mathvariant="bold">O</mi></mrow> <annotation encoding="application/x-tex">\mathbf{O}</annotation></semantics></math>。

实际上，如果可用，目前绝对没有理由**不**使用Flash Attention。该算法在数学上给出相同的输出，而且速度更快，内存效率更高。

让我们看一个实际的例子。

我们的OctoCoder模型现在得到了一个明显更长的输入提示，其中包括所谓的*系统提示*。系统提示用于引导LLM成为一个更好的助手，专门为用户的任务定制。接下来，我们使用一个系统提示，将使OctoCoder成为一个更好的编码助手。

```py
system_prompt = """Below are a series of dialogues between various people and an AI technical assistant.
The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.
The assistant is happy to help with code questions and will do their best to understand exactly what is needed.
It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.
That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.

The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).
The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.

-----

Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.

Answer: Sure. Here is a function that does that.

def alternating(list1, list2):
   results = []
   for i in range(len(list1)):
       results.append(list1[i])
       results.append(list2[i])
   return results

Question: Can you write some test cases for this function?

Answer: Sure, here are some tests.

assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]
assert alternating([True, False], [4, 5]) == [True, 4, False, 5]
assert alternating([], []) == []

Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.

Answer: Here is the modified function.

def alternating(list1, list2):
   results = []
   for i in range(min(len(list1), len(list2))):
       results.append(list1[i])
       results.append(list2[i])
   if len(list1) > len(list2):
       results.extend(list1[i+1:])
   else:
       results.extend(list2[i+1:])
   return results

-----
"""
```

为了演示目的，我们将系统提示复制十次，以便输入长度足够长，以观察Flash Attention的内存节省。我们附加原始文本提示`"问题：请用Python编写一个将字节转换为千兆字节的函数。\n\n答案：在这里"`

```py
long_prompt = 10 * system_prompt + prompt
```

我们再次以bfloat16精度实例化我们的模型。

```py
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
```

现在让我们像之前一样运行模型*不使用Flash Attention*，并测量GPU内存需求的峰值和推理时间。

```py
import time

start_time = time.time()
result = pipe(long_prompt, max_new_tokens=60)[0]["generated_text"][len(long_prompt):]

print(f"Generated in {time.time() - start_time} seconds.")
result
```

**输出**：

```py
Generated in 10.96854019165039 seconds.
Sure. Here is a function that does that.\n\ndef bytes_to_giga(bytes):\n   return bytes / 1024 / 1024 / 1024\n\nAnswer: Sure. Here is a function that does that.\n\ndef
```

我们得到了与之前相同的输出，但是这一次，模型会重复答案多次，直到达到60个标记的截止。这并不奇怪，因为我们为演示目的重复了系统提示十次，从而提示模型重复自己。

**请注意**，在实际应用中，系统提示不应重复十次-一次就足够了！

让我们测量GPU内存需求的峰值。

```py
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

**输出**：

```py
37.668193340301514
```

正如我们所看到的，峰值GPU内存需求现在比一开始显着更高，这在很大程度上是由于更长的输入序列。此外，生成现在需要一分钟多一点。

我们调用`flush()`来释放GPU内存，以便进行下一个实验。

```py
flush()
```

为了进行比较，让我们运行相同的函数，但启用Flash Attention。为此，我们将模型转换为[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)，从而启用PyTorch的[SDPA自注意力](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)，进而能够使用Flash Attention。

```py
model.to_bettertransformer()
```

现在我们运行与之前完全相同的代码片段，在底层Transformers将利用Flash Attention。

```py
start_time = time.time()
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    result = pipe(long_prompt, max_new_tokens=60)[0]["generated_text"][len(long_prompt):]

print(f"Generated in {time.time() - start_time} seconds.")
result
```

**输出**：

```py
Generated in 3.0211617946624756 seconds.
 Sure. Here is a function that does that.\n\ndef bytes_to_giga(bytes):\n   return bytes / 1024 / 1024 / 1024\n\nAnswer: Sure. Here is a function that does that.\n\ndef
```

我们得到了与之前完全相同的结果，但由于Flash Attention，我们可以观察到非常显著的加速。

让我们最后一次测量内存消耗。

```py
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

**输出**：

```py
32.617331981658936
```

我们几乎回到了最初的29GB GPU内存峰值。

我们可以观察到，与一开始传递短输入序列相比，使用Flash Attention传递非常长的输入序列时，我们只使用了大约多100MB的GPU内存。

```py
flush()
```

有关如何使用Flash Attention的更多信息，请查看[此文档页面](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2)。

## 3. 架构创新

到目前为止，我们已经研究了通过以下方式提高计算和内存效率：

+   将权重转换为较低精度格式

+   用更节省内存和计算资源的版本替换自注意力算法

现在让我们看看如何改变LLM的架构，使其对需要长文本输入的任务最有效和高效，例如：

+   检索增强问答，

+   总结，

+   聊天

请注意，*chat*不仅要求LLM处理长文本输入，还要求LLM能够有效地处理用户和助手之间的来回对话（例如ChatGPT）。

一旦训练完成，基本的LLM架构很难改变，因此在事先考虑LLM的任务并相应地优化模型架构非常重要。模型架构的两个重要组件很快成为大型输入序列的内存和/或性能瓶颈。

+   位置嵌入

+   键-值缓存

让我们更详细地讨论每个组件

### 3.1 改进LLM的位置嵌入

自注意力将每个标记与其他标记相关联。例如，文本输入序列的Softmax(QKT)矩阵*“Hello”, “I”, “love”, “you”*可能如下所示：

![](../Images/a42237d68d8acd5442beddab2228c8d5.png)

每个单词标记都被赋予一个概率质量，用于关注所有其他单词标记，因此与所有其他单词标记相关联。例如，单词*“love”*关注单词*“Hello”*的概率为5%，关注*“I”*的概率为30%，自身的概率为65%。

基于自注意力的LLM，但没有位置嵌入，将在理解文本输入之间的位置方面遇到很大困难。这是因为由<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT 计算的概率分数将每个单词标记与其他单词标记在<math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">O(1)</annotation></semantics></math> O(1) 计算中相关联，而不考虑它们之间的相对位置距离。因此，对于没有位置嵌入的LLM，每个标记似乎与所有其他标记具有相同的距离，例如，区分“你好 我爱你”和“你爱我 你好”将会非常具有挑战性。

为了让LLM理解句子顺序，需要额外的*提示*，通常以*位置编码*（也称为*位置嵌入*）的形式应用。位置编码将每个标记的位置编码为LLM可以利用的数值表示，以更好地理解句子顺序。

[*注意力机制就是你所需要的*](https://arxiv.org/abs/1706.03762)论文的作者们引入了正弦位置嵌入<math><semantics><mrow><mi mathvariant="bold">P</mi><mo>=</mo><msub><mi mathvariant="bold">p</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">p</mi><mi>N</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{P} = \mathbf{p}_1, \ldots, \mathbf{p}_N</annotation></semantics></math> P=p1​,…,pN​ 。其中每个向量<math><semantics><mrow><msub><mi mathvariant="bold">p</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{p}_i</annotation></semantics></math> pi​ 是根据其位置<math><semantics><mrow><mi>i</mi></mrow> <annotation encoding="application/x-tex">i</annotation></semantics></math> i 计算的正弦函数。然后将位置编码简单地添加到输入序列向量中<math><semantics><mrow><mover accent="true"><mi mathvariant="bold">X</mi><mo>^</mo></mover><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>N</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{\hat{X}} = \mathbf{\hat{x}}_1, \ldots, \mathbf{\hat{x}}_N</annotation></semantics></math> X^=x^1​,…,x^N​ =<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo>+</mo><msub><mi mathvariant="bold">p</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo>+</mo><msub><mi mathvariant="bold">p</mi><mi>N</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{x}_1 + \mathbf{p}_1, \ldots, \mathbf{x}_N + \mathbf{p}_N</annotation></semantics></math> x1​+p1​,…,xN​+pN​ 从而提示模型更好地学习句子顺序。

其他人（例如[Devlin等人](https://arxiv.org/abs/1810.04805)）使用了学习的位置编码，而不是固定的位置嵌入，这些位置嵌入在训练期间进行学习。

正弦和学习位置嵌入曾经是将句子顺序编码到LLM中的主要方法，但发现了与这些位置编码相关的一些问题：

1.  正弦和学习位置嵌入都是绝对位置嵌入，即为每个位置id编码一个唯一的嵌入：<math><semantics><mrow><mn>0</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>N</mi></mrow> <annotation encoding="application/x-tex">0, \ldots, N</annotation></semantics></math> 0,…,N。正如[Huang等人](https://arxiv.org/abs/2009.13658)和[苏等人](https://arxiv.org/abs/2104.09864)所示，绝对位置嵌入导致长文本输入的LLM性能较差。对于长文本输入，如果模型学习输入标记之间的相对位置距离而不是它们的绝对位置，将是有利的。

1.  当使用学习位置嵌入时，LLM必须在固定的输入长度<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math> N上进行训练，这使得难以推广到比其训练长度更长的输入。

最近，能够解决上述问题的相对位置嵌入变得更加流行，其中最著名的是：

+   [旋转位置嵌入（RoPE）](https://arxiv.org/abs/2104.09864)

+   [ALiBi](https://arxiv.org/abs/2108.12409)

*RoPE*和*ALiBi*都认为最好直接在自注意力算法中提示LLM关于句子顺序，因为在那里单词标记彼此关联。更具体地说，句子顺序应该通过修改<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT计算来提示。

不详细讨论，*RoPE*指出位置信息可以被编码到查询-键对中，例如：<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math> 和<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{x}_j</annotation></semantics></math> ，通过将每个向量旋转一个角度<math><semantics><mrow><mi>θ</mi><mo>∗</mo><mi>i</mi></mrow> <annotation encoding="application/x-tex">\theta * i</annotation></semantics></math> 和<math><semantics><mrow><mi>θ</mi><mo>∗</mo><mi>j</mi></mrow> <annotation encoding="application/x-tex">\theta * j</annotation></semantics></math> ，其中<math><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow> <annotation encoding="application/x-tex">i, j</annotation></semantics></math> 描述每个向量的句子位置：<math display="block"><semantics><mrow><msubsup><mover accent="true"><mi mathvariant="bold">q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi mathvariant="bold">R</mi><mrow><mi>θ</mi><mo separator="true">,</mo><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow> <annotation encoding="application/x-tex">\mathbf{\hat{q}}_i^T \mathbf{\hat{x}}_j = \mathbf{{q}}_i^T \mathbf{R}_{\theta, i -j} \mathbf{{x}}_j.</annotation></semantics></math> q^​iT​x^j​=qiT​Rθ,i−j​xj​。 <math><semantics><mrow><msub><mi mathvariant="bold">R</mi><mrow><mi>θ</mi><mo separator="true">,</mo><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow> <annotation encoding="application/x-tex">\mathbf{R}_{\theta, i - j}</annotation></semantics></math> Rθ,i−j​代表一个旋转矩阵。<math><semantics><mrow><mi>θ</mi></mrow> <annotation encoding="application/x-tex">\theta</annotation></semantics></math> θ在训练过程中*不会*被学习，而是设置为一个预定义的值，该值取决于训练过程中的最大输入序列长度。

> 通过这样做，<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math> 和<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>j</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_j</annotation></semantics></math> 之间的概率分数只有在<math><semantics><mrow><mi>i</mi><mo mathvariant="normal">≠</mo><mi>j</mi></mrow> <annotation encoding="application/x-tex">i \ne j</annotation></semantics></math> 时才会受到影响，并且仅取决于相对距离<math><semantics><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow> <annotation encoding="application/x-tex">i - j</annotation></semantics></math> ，而不考虑每个向量的具体位置<math><semantics><mrow><mi>i</mi></mrow> <annotation encoding="application/x-tex">i</annotation></semantics></math> 和<math><semantics><mrow><mi>j</mi></mrow> <annotation encoding="application/x-tex">j</annotation></semantics></math> 。

*RoPE*被用在当今一些最重要的LLM中，例如：

+   [**猎鹰**](https://huggingface.co/tiiuae/falcon-40b)

+   [**大羊驼**](https://arxiv.org/abs/2302.13971)

+   [**PaLM**](https://arxiv.org/abs/2204.02311)

作为一种替代方案，*ALiBi* 提出了一种更简单的相对位置编码方案。输入令牌之间的相对距离被添加为负整数，乘以预定义值 `m`，并添加到 softmax 计算之前的<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT 矩阵的每个查询-键条目中。

![](../Images/b4adb6fd6b6aaa790ed5e4200191e405.png)

正如[ALiBi](https://arxiv.org/abs/2108.12409) 论文所示，这种简单的相对位置编码使模型能够在非常长的文本输入序列中保持高性能。

*ALiBi* 在当今一些最重要的 LLM 中使用，例如：

+   [**MPT**](https://huggingface.co/mosaicml/mpt-30b)

+   [**BLOOM**](https://huggingface.co/bigscience/bloom)

*RoPE* 和 *ALiBi* 位置编码都可以外推到训练中未见过的输入长度，然而已经证明相对于 *RoPE*，外推对于 *ALiBi* 来说更容易。对于 ALiBi，只需增加下三角位置矩阵的值以匹配输入序列的长度。对于 *RoPE*，保持训练期间使用的相同<math><semantics><mrow><mi>θ</mi></mrow> <annotation encoding="application/x-tex">\theta</annotation></semantics></math> θ，在传递比训练期间看到的文本输入长得多的文本输入时会导致结果不佳，*参见* [Press 等人](https://arxiv.org/abs/2108.12409)。然而，社区已经发现了一些有效的技巧，可以调整<math><semantics><mrow><mi>θ</mi></mrow> <annotation encoding="application/x-tex">\theta</annotation></semantics></math> θ，从而使 *RoPE* 位置嵌入适用于外推的文本输入序列（参见[这里](https://github.com/huggingface/transformers/pull/24653)）。

> RoPE 和 ALiBi 都是相对位置嵌入，它们在训练期间 *不* 被学习，而是基于以下直觉：
> 
> +   关于文本输入的位置提示应直接提供给自注意力层的<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">QK^T</annotation></semantics></math> QKT 矩阵
> +   
> +   LLM 应该被激励学习常数 *相对* 距离位置编码之间的关系
> +   
> +   文本输入令牌之间的距离越远，它们的查询-值概率就越低。RoPE 和 ALiBi 都降低了远离彼此的令牌的查询-键概率。RoPE 通过增加查询-键向量之间的角度来减少它们的向量积。ALiBi 通过向向量积添加大的负数

总之，用于处理大文本输入的任务的 LLM 最好使用相对位置嵌入进行训练，例如 RoPE 和 ALiBi。还要注意，即使一个带有 RoPE 和 ALiBi 的 LLM 只在固定长度的数据上进行了训练，比如<math><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>=</mo><mn>2048</mn></mrow> <annotation encoding="application/x-tex">N_1 = 2048</annotation></semantics></math> N1​=2048，它仍然可以在实践中用于比<math><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub></mrow> <annotation encoding="application/x-tex">N_1</annotation></semantics></math> N1​更大的文本输入，比如<math><semantics><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>=</mo><mn>8192</mn><mo>></mo><msub><mi>N</mi><mn>1</mn></msub></mrow> <annotation encoding="application/x-tex">N_2 = 8192 > N_1</annotation></semantics></math> N2​=8192>N1​，通过外推位置嵌入。

### 3.2 关键-值缓存

LLMs的自回归文本生成通过迭代地输入一个序列，抽样下一个标记，将下一个标记附加到输入序列中，并继续这样做，直到LLM生成一个表示生成结束的标记。

请查看[Transformer生成文本教程](https://huggingface.co/docs/transformers/llm_tutorial#generate-text)，以获得更直观的自回归生成工作原理解释。

让我们运行一个快速的代码片段，展示自回归在实践中是如何工作的。我们将简单地通过`torch.argmax`获取最有可能的下一个标记。

```py
input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

for _ in range(5):
  next_logits = model(input_ids)["logits"][:, -1:]
  next_token_id = torch.argmax(next_logits,dim=-1)

  input_ids = torch.cat([input_ids, next_token_id], dim=-1)
  print("shape of input_ids", input_ids.shape)

generated_text = tokenizer.batch_decode(input_ids[:, -5:])
generated_text
```

**输出**：

```py
shape of input_ids torch.Size([1, 21])
shape of input_ids torch.Size([1, 22])
shape of input_ids torch.Size([1, 23])
shape of input_ids torch.Size([1, 24])
shape of input_ids torch.Size([1, 25])
[' Here is a Python function']
```

正如我们所看到的，每次我们通过刚刚抽样的标记增加文本输入标记。

除了极少数例外，LLMs是使用[因果语言建模目标](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling)进行训练的，因此会屏蔽注意力分数的上三角矩阵 - 这就是为什么在上述两个图表中，注意力分数留空（*也就是*概率为0）。关于因果语言建模的快速回顾，您可以参考[*Illustrated Self Attention blog*](https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention)。

因此，标记*永远*不依赖于先前的标记，更具体地说，<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math> qi​向量永远不会与任何键、值向量<math><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{k}_j, \mathbf{v}_j</annotation></semantics></math> kj​,vj​相关联，如果<math><semantics><mrow><mi>j</mi><mo>></mo><mi>i</mi></mrow> <annotation encoding="application/x-tex">j > i</annotation></semantics></math> j>i。相反，<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math> qi​只关注先前的键-值向量<math><semantics><mrow><msub><mi mathvariant="bold">k</mi><mrow><mi>m</mi><mo><</mo><mi>i</mi></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mrow><mi>m</mi><mo><</mo><mi>i</mi></mrow></msub><mtext> , for </mtext><mi>m</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mo>…</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false">}</mo></mrow> <annotation encoding="application/x-tex">\mathbf{k}_{m < i}, \mathbf{v}_{m < i} \text{ , for } m \in \{0, \ldots i - 1\}</annotation></semantics></math> km<i​,vm<i​ , for m∈{0,…i−1}。为了减少不必要的计算，可以为每一层缓存所有先前时间步的键-值向量。

接下来，我们将告诉LLM利用键-值缓存，通过在每次前向传递中检索并转发它。在Transformers中，我们可以通过向`forward`调用传递`use_cache`标志来检索键-值缓存，然后可以将其与当前标记一起传递。

```py
past_key_values = None # past_key_values is the key-value cache
generated_tokens = []
next_token_id = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

for _ in range(5):
  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()
  next_logits = next_logits[:, -1:]
  next_token_id = torch.argmax(next_logits, dim=-1)

  print("shape of input_ids", next_token_id.shape)
  print("length of key-value cache", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]
  generated_tokens.append(next_token_id.item())

generated_text = tokenizer.batch_decode(generated_tokens)
generated_text
```

**输出**：

```py
shape of input_ids torch.Size([1, 1])
length of key-value cache 20
shape of input_ids torch.Size([1, 1])
length of key-value cache 21
shape of input_ids torch.Size([1, 1])
length of key-value cache 22
shape of input_ids torch.Size([1, 1])
length of key-value cache 23
shape of input_ids torch.Size([1, 1])
length of key-value cache 24 [' Here', ' is', ' a', ' Python', ' function']
```

正如大家所看到的，当使用键-值缓存时，文本输入标记的长度*不会*增加，而是保持为单个输入向量。另一方面，键-值缓存的长度在每个解码步骤都会增加一个。

> 利用键值缓存意味着<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT基本上被简化为<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi mathvariant="bold">K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{q}_c\mathbf{K}^T</annotation></semantics></math> qc​KT，其中<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_c</annotation></semantics></math> qc​是当前传递的输入令牌的查询投影，它*始终*只是一个单一向量。

使用键值缓存有两个优点：

+   与计算完整的QKT矩阵相比，计算效率显著提高，因为进行的计算较少。这导致推理速度增加。

+   所需的最大内存并不是随着生成的令牌数量的平方增加，而是线性增加。

> 应该*始终*利用键值缓存，因为它会产生相同的结果，并且对于较长的输入序列会显著加快速度。当使用文本管道或[`generate`方法](https://huggingface.co/docs/transformers/main_classes/text_generation)时，Transformers默认启用键值缓存。

请注意，尽管我们建议使用键值缓存，但当您使用它们时，您的LLM输出可能会略有不同。这是矩阵乘法核心本身的属性 — 您可以在[这里](https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535)了解更多信息。

#### 3.2.1 多轮对话

键值缓存在需要多次自回归解码的应用程序中特别有用，让我们看一个例子。

```py
User: How many people live in France?
Assistant: Roughly 75 million people live in France
User: And how many are in Germany?
Assistant: Germany has ca. 81 million inhabitants
```

在这个对话中，LLM会自回归解码两次：

1.  第一次，键值缓存为空，输入提示是`"用户：法国有多少人口？"`，模型会自回归生成文本`"法国大约有7500万人口"，同时在每个解码步骤中增加键值缓存。

1.  第二次输入提示是`"用户：法国有多少人口？\n助手：法国大约有7500万人口\n用户：德国有多少人口？"`。由于缓存的存在，前两个句子的所有键值向量已经计算完毕。因此，输入提示只包括`"用户：德国有多少人口？"`。在处理缩短的输入提示时，它的计算键值向量会与第一次解码的键值缓存连接起来。然后第二个助手的回答`"德国大约有8100万居民"`会根据编码的键值向量`"用户：法国有多少人口？\n助手：法国大约有7500万人口\n用户：德国有多少人口？"`进行自回归生成。

这里有两点需要注意：

1.  对于部署在聊天中的LLM来说，保留所有上下文对于LLM理解对话的先前上下文至关重要。例如，对于上面的例子，LLM需要理解用户在询问`"德国有多少人口？"`时指的是人口。

1.  键值缓存对于聊天非常有用，因为它允许我们持续增加编码的聊天历史，而不必重新从头开始重新编码聊天历史（例如，当使用编码器-解码器架构时会发生这种情况）。

在`transformers`中，当传递`return_dict_in_generate=True`时，`generate`调用将返回`past_key_values`，除了默认的`use_cache=True`。请注意，这还不适用于`pipeline`接口。

```py
# Generation as usual
prompt = system_prompt + "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer: Here"
model_inputs = tokenizer(prompt, return_tensors='pt')
generation_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)
decoded_output = tokenizer.batch_decode(generation_output.sequences)[0]

# Piping the returned `past_key_values` to speed up the next conversation round
prompt = decoded_output + "\nQuestion: How can I modify the function above to return Mega bytes instead?\n\nAnswer: Here"
model_inputs = tokenizer(prompt, return_tensors='pt')
generation_output = model.generate(
  **model_inputs,
  past_key_values=generation_output.past_key_values,
  max_new_tokens=60,
  return_dict_in_generate=True
)
tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]
```

**输出**：

```py
 is a modified version of the function that returns Mega bytes instead.

def bytes_to_megabytes(bytes):
   return bytes / 1024 / 1024

Answer: The function takes a number of bytes as input and returns the number of
```

太好了，不需要额外的时间来重新计算注意力层的相同键和值！然而，有一个问题。虽然<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT 矩阵所需的峰值内存显著减少，但在内存中保持键值缓存可能会对长输入序列或多轮对话非常昂贵。请记住，键值缓存需要存储所有先前输入向量的键值向量<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mtext>, for </mtext><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>c</mi><mo>−</mo><mn>1</mn><mo stretchy="false">}</mo></mrow> <annotation encoding="application/x-tex">\mathbf{x}_i \text{, for } i \in \{1, \ldots, c - 1\}</annotation></semantics></math> xi​, for i∈{1,…,c−1} 对于所有自注意力层和所有注意力头部。

让我们计算之前使用的LLM `bigcode/octocoder` 需要存储在键值缓存中的浮点值的数量。浮点值的数量等于序列长度乘以注意力头数乘以注意力头维度乘以层数的两倍。对于我们的LLM，在假设输入序列长度为16000时计算如下：

```py
config = model.config
2 * 16_000 * config.n_layer * config.n_head * config.n_embd // config.n_head
```

**输出**：

```py
7864320000
```

大约80亿个浮点值！以`float16`精度存储80亿个浮点值需要大约15 GB的内存，这大约是模型权重本身的一半！研究人员提出了两种方法，可以显著减少存储键值缓存的内存成本，这将在接下来的小节中探讨。

#### 3.2.2 多查询注意力（MQA）

[Multi-Query-Attention](https://arxiv.org/abs/1911.02150) 是Noam Shazeer在*Fast Transformer Decoding: One Write-Head is All You Need*论文中提出的。正如标题所说，Noam 发现，可以使用一个单一的头值投影权重对，而不是使用`n_head`个键值投影权重，这个对在所有注意力头部之间共享，而不会显著降低模型的性能。

> 通过使用单个头值投影权重对，键值向量<math><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{k}_i, \mathbf{v}_i</annotation></semantics></math> ki​,vi​ 在所有注意力头部之间必须是相同的，这意味着我们只需要在缓存中存储1个键值投影对，而不是`n_head`个。

由于大多数LLM使用20到100个注意力头部，MQA显著减少了键值缓存的内存消耗。对于本笔记本中使用的LLM，因此我们可以将所需的内存消耗从15 GB减少到输入序列长度为16000时的不到400 MB。

除了节省内存外，MQA还提高了计算效率，如下所述。在自回归解码中，需要重新加载大的键值向量，将其与当前的键值向量对连接，然后将其馈送到每一步的<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi mathvariant="bold">K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{q}_c\mathbf{K}^T</annotation></semantics></math> qc​KT 计算中。对于自回归解码，常量重新加载所需的内存带宽可能成为严重的时间瓶颈。通过减小键值向量的大小，可以减少访问的内存量，从而减少内存带宽瓶颈。更详细的信息，请参阅[Noam的论文](https://arxiv.org/abs/1911.02150)。

这里需要理解的重要部分是，将关键值注意力头的数量减少到1只有在使用关键值缓存时才有意义。模型在没有关键值缓存的单次前向传递中的峰值内存消耗保持不变，因为每个注意力头仍然具有唯一的查询向量，因此每个注意力头仍然具有不同的<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math> QKT 矩阵。

MQA已经被社区广泛采用，现在许多最受欢迎的LLM都在使用：

+   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)

+   [**PaLM**](https://arxiv.org/abs/2204.02311)

+   [**MPT**](https://huggingface.co/mosaicml/mpt-30b)

+   [**BLOOM**](https://huggingface.co/bigscience/bloom)

此外，本笔记中使用的检查点`bigcode/octocoder`使用了MQA。

#### 3.2.3 分组查询注意力（GQA）

[分组查询注意力](https://arxiv.org/abs/2305.13245)，由谷歌的Ainslie等人提出，发现使用MQA与使用普通的多键值头投影相比，通常会导致质量下降。该论文认为，通过减少查询头投影权重的数量，可以保留更多的模型性能。不要仅使用单个键值投影权重，应使用`n < n_head`个键值投影权重。通过选择`n`为远小于`n_head`的值，例如2、4或8，几乎可以保留来自MQA的所有内存和速度增益，同时牺牲较少的模型容量，因此可以说是性能更好。

此外，GQA的作者发现，现有的模型检查点可以通过仅使用原始预训练计算量的5%进行*更新训练*，以实现GQA架构。虽然原始预训练计算量的5%仍然是一个巨大的数量，但GQA的*更新训练*使现有的检查点可以用于更长的输入序列。 

GQA是最近提出的，因此在撰写本笔记时采用的情况较少。GQA最显著的应用是[Llama-v2](https://huggingface.co/meta-llama/Llama-2-70b-hf)。

> 总之，强烈建议在LLM部署自回归解码并需要处理大型输入序列的情况下使用GQA或MQA。

## 结论

研究界不断提出新的巧妙方法来加快越来越大的LLM的推理时间。例如，一个有前途的研究方向是[推测解码](https://arxiv.org/abs/2211.17192)，其中较小、更快的语言模型生成“简单标记”，只有LLM本身生成“困难标记”。更详细的内容超出了本笔记的范围，但可以在这篇[不错的博客文章](https://huggingface.co/blog/assisted-generation)中阅读。

大型LLM（如GPT3/4、Llama-2-70b、Claude、PaLM）能够在[Hugging Face Chat](https://huggingface.co/chat/)或ChatGPT等聊天界面中运行如此迅速，这在很大程度上要归功于上述精度、算法和架构的改进。未来，像GPU、TPU等加速器将会变得更快，允许更多的内存，但仍然应始终确保使用最佳的可用算法和架构，以获得最大的性价比🤗
