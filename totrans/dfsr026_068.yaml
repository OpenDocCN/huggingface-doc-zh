- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This is experimental and the API may change in the future.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å®éªŒæ€§çš„ï¼ŒAPIå¯èƒ½ä¼šåœ¨å°†æ¥æ›´æ”¹ã€‚
- en: '[LoRA (Low-Rank Adaptation of Large Language Models)](https://hf.co/papers/2106.09685)
    is a popular and lightweight training technique that significantly reduces the
    number of trainable parameters. It works by inserting a smaller number of new
    weights into the model and only these are trained. This makes training with LoRA
    much faster, memory-efficient, and produces smaller model weights (a few hundred
    MBs), which are easier to store and share. LoRA can also be combined with other
    training techniques like DreamBooth to speedup training.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[LoRAï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”ï¼‰](https://hf.co/papers/2106.09685)æ˜¯ä¸€ç§æµè¡Œä¸”è½»é‡çº§çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚å®ƒé€šè¿‡å‘æ¨¡å‹æ’å…¥è¾ƒå°‘æ•°é‡çš„æ–°æƒé‡ï¼Œå¹¶ä»…å¯¹è¿™äº›æƒé‡è¿›è¡Œè®­ç»ƒæ¥å·¥ä½œã€‚è¿™ä½¿å¾—ä½¿ç”¨LoRAè¿›è¡Œè®­ç»ƒæ›´å¿«é€Ÿã€å†…å­˜æ•ˆç‡æ›´é«˜ï¼Œå¹¶ä¸”äº§ç”Ÿæ›´å°çš„æ¨¡å‹æƒé‡ï¼ˆå‡ ç™¾MBï¼‰ï¼Œæ›´å®¹æ˜“å­˜å‚¨å’Œå…±äº«ã€‚LoRAè¿˜å¯ä»¥ä¸å…¶ä»–è®­ç»ƒæŠ€æœ¯ï¼ˆå¦‚DreamBoothï¼‰ç»“åˆä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚'
- en: LoRA is very versatile and supported for [DreamBooth](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py),
    [Kandinsky 2.2](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_lora_decoder.py),
    [Stable Diffusion XL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py),
    [text-to-image](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py),
    and [Wuerstchen](https://github.com/huggingface/diffusers/blob/main/examples/wuerstchen/text_to_image/train_text_to_image_lora_prior.py).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: LoRAéå¸¸çµæ´»ï¼Œå¹¶æ”¯æŒ[DreamBooth](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py)ã€[Kandinsky
    2.2](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_lora_decoder.py)ã€[Stable
    Diffusion XL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py)ã€[text-to-image](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py)å’Œ[Wuerstchen](https://github.com/huggingface/diffusers/blob/main/examples/wuerstchen/text_to_image/train_text_to_image_lora_prior.py)ã€‚
- en: This guide will explore the [train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py)
    script to help you become more familiar with it, and how you can adapt it for
    your own use-case.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŒ‡å—å°†æ¢è®¨[train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py)è„šæœ¬ï¼Œå¸®åŠ©æ‚¨æ›´ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•ä¸ºè‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨ä»æºä»£ç å®‰è£…åº“ï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Navigate to the example folder with the training script and install the required
    dependencies for the script youâ€™re using:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼èˆªåˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š
- en: PyTorchFlax
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ğŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. Itâ€™ll automatically configure your training setup based on your
    hardware and environment. Take a look at the ğŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateæ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šä¸ªGPU/TPUä¸Šè¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨æ··åˆç²¾åº¦çš„åº“ã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤— Accelerate
    [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šã€‚
- en: 'Initialize an ğŸ¤— Accelerate environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ğŸ¤— Accelerateç¯å¢ƒï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default ğŸ¤— Accelerate environment without choosing any configurations:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¾ç½®é»˜è®¤çš„ğŸ¤— Accelerateç¯å¢ƒè€Œä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesnâ€™t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…å¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shellï¼Œæ¯”å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºä¸è®­ç»ƒè„šæœ¬å…¼å®¹çš„æ•°æ®é›†ã€‚
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesnâ€™t cover every aspect of the script
    in detail. If youâ€™re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py)
    and let us know if you have any questions or concerns.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹éƒ¨åˆ†çªå‡ºæ˜¾ç¤ºäº†è®­ç»ƒè„šæœ¬çš„ä¸€äº›é‡è¦éƒ¨åˆ†ï¼Œä»¥å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¿®æ”¹å®ƒï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯»[è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py)ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ã€‚
- en: Script parameters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è„šæœ¬å‚æ•°
- en: The training script has many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L85)
    function. Default values are provided for most parameters that work pretty well,
    but you can also set your own values in the training command if youâ€™d like.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬æœ‰è®¸å¤šå‚æ•°å¯å¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°éƒ½å¯ä»¥åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L85)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚å¤§å¤šæ•°å‚æ•°éƒ½æä¾›äº†é»˜è®¤å€¼ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚
- en: 'For example, to increase the number of epochs to train:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦å¢åŠ è®­ç»ƒçš„æ—¶ä»£æ•°ï¼š
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Many of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters)
    training guide, so this guide just focuses on the LoRA relevant parameters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šåŸºæœ¬å’Œé‡è¦çš„å‚æ•°åœ¨[Text-to-image](text2image#script-parameters)è®­ç»ƒæŒ‡å—ä¸­æœ‰æè¿°ï¼Œå› æ­¤æœ¬æŒ‡å—åªå…³æ³¨ä¸LoRAç›¸å…³çš„å‚æ•°ï¼š
- en: '`--rank`: the inner dimension of the low-rank matrices to train; a higher rank
    means more trainable parameters'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--rank`ï¼šè¦è®­ç»ƒçš„ä½ç§©çŸ©é˜µçš„å†…éƒ¨ç»´åº¦ï¼›æ›´é«˜çš„ç§©æ„å‘³ç€æ›´å¤šçš„å¯è®­ç»ƒå‚æ•°'
- en: '`--learning_rate`: the default learning rate is 1e-4, but with LoRA, you can
    use a higher learning rate'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--learning_rate`ï¼šé»˜è®¤å­¦ä¹ ç‡ä¸º1e-4ï¼Œä½†ä½¿ç”¨LoRAæ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡'
- en: Training script
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬
- en: The dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L371)
    function, and if you need to adapt the training script, this is where youâ€™ll make
    your changes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯å¯ä»¥åœ¨ [`main()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L371)
    å‡½æ•°ä¸­æ‰¾åˆ°ï¼Œå¦‚æœæ‚¨éœ€è¦è°ƒæ•´è®­ç»ƒè„šæœ¬ï¼Œè¿™å°±æ˜¯æ‚¨éœ€è¦è¿›è¡Œæ›´æ”¹çš„åœ°æ–¹ã€‚
- en: As with the script parameters, a walkthrough of the training script is provided
    in the [Text-to-image](text2image#training-script) training guide. Instead, this
    guide takes a look at the LoRA relevant parts of the script.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è„šæœ¬å‚æ•°ä¸€æ ·ï¼Œ[æ–‡æœ¬åˆ°å›¾åƒ](text2image#training-script)è®­ç»ƒæŒ‡å—ä¸­æä¾›äº†è®­ç»ƒè„šæœ¬çš„è¯¦ç»†æ­¥éª¤ã€‚è€Œè¿™ä»½æŒ‡å—åˆ™ç€çœ¼äºè„šæœ¬ä¸­ä¸ LoRA
    ç›¸å…³çš„éƒ¨åˆ†ã€‚
- en: 'The script begins by adding the [new LoRA weights](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L447)
    to the attention layers. This involves correctly configuring the weight size for
    each block in the UNet. Youâ€™ll see the `rank` parameter is used to create the
    [LoRAAttnProcessor](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.LoRAAttnProcessor):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬é¦–å…ˆé€šè¿‡å°† [æ–°çš„ LoRA æƒé‡](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L447)
    æ·»åŠ åˆ°æ³¨æ„åŠ›å±‚æ¥å¼€å§‹ã€‚è¿™æ¶‰åŠæ­£ç¡®é…ç½® UNet ä¸­æ¯ä¸ªå—çš„æƒé‡å¤§å°ã€‚æ‚¨ä¼šçœ‹åˆ° `rank` å‚æ•°è¢«ç”¨æ¥åˆ›å»º [LoRAAttnProcessor](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.LoRAAttnProcessor)ï¼š
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The [optimizer](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L519)
    is initialized with the `lora_layers` because these are the only weights thatâ€™ll
    be optimized:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¼˜åŒ–å™¨](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L519)
    ä½¿ç”¨ `lora_layers` è¿›è¡Œåˆå§‹åŒ–ï¼Œå› ä¸ºè¿™äº›æ˜¯å”¯ä¸€ä¼šè¢«ä¼˜åŒ–çš„æƒé‡ï¼š'
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Aside from setting up the LoRA layers, the training script is more or less the
    same as train_text_to_image.py!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†è®¾ç½® LoRA å±‚ä¹‹å¤–ï¼Œè®­ç»ƒè„šæœ¬åŸºæœ¬ä¸Šä¸ train_text_to_image.py ç›¸åŒï¼
- en: Launch the script
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯åŠ¨è„šæœ¬
- en: Once youâ€™ve made all your changes or youâ€™re okay with the default configuration,
    youâ€™re ready to launch the training script! ğŸš€
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å®Œæˆäº†æ‰€æœ‰æ›´æ”¹æˆ–è€…æ‚¨å¯¹é»˜è®¤é…ç½®æ»¡æ„ï¼Œæ‚¨å°±å¯ä»¥å¯åŠ¨è®­ç»ƒè„šæœ¬ï¼ğŸš€
- en: 'Letâ€™s train on the [PokÃ©mon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    dataset to generate our yown PokÃ©mon. Set the environment variables `MODEL_NAME`
    and `DATASET_NAME` to the model and dataset respectively. You should also specify
    where to save the model in `OUTPUT_DIR`, and the name of the model to save to
    on the Hub with `HUB_MODEL_ID`. The script creates and saves the following files
    to your repository:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ [PokÃ©mon BLIP æ ‡é¢˜](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆæˆ‘ä»¬è‡ªå·±çš„ PokÃ©monã€‚å°†ç¯å¢ƒå˜é‡ `MODEL_NAME` å’Œ `DATASET_NAME` åˆ†åˆ«è®¾ç½®ä¸ºæ¨¡å‹å’Œæ•°æ®é›†ã€‚æ‚¨è¿˜åº”è¯¥æŒ‡å®šåœ¨
    `OUTPUT_DIR` ä¸­ä¿å­˜æ¨¡å‹çš„ä½ç½®ï¼Œå¹¶ä½¿ç”¨ `HUB_MODEL_ID` æŒ‡å®šè¦ä¿å­˜åˆ° Hub ä¸Šçš„æ¨¡å‹çš„åç§°ã€‚è„šæœ¬å°†åˆ›å»ºå¹¶ä¿å­˜ä»¥ä¸‹æ–‡ä»¶åˆ°æ‚¨çš„å­˜å‚¨åº“ä¸­ï¼š
- en: saved model checkpoints
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿å­˜çš„æ¨¡å‹æ£€æŸ¥ç‚¹
- en: '`pytorch_lora_weights.safetensors` (the trained LoRA weights)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch_lora_weights.safetensors`ï¼ˆè®­ç»ƒçš„ LoRA æƒé‡ï¼‰'
- en: If youâ€™re training on more than one GPU, add the `--multi_gpu` parameter to
    the `accelerate launch` command.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·åœ¨ `accelerate launch` å‘½ä»¤ä¸­æ·»åŠ  `--multi_gpu` å‚æ•°ã€‚
- en: A full training run takes ~5 hours on a 2080 Ti GPU with 11GB of VRAM.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‹¥æœ‰ 11GB VRAM çš„ 2080 Ti GPU ä¸Šè¿›è¡Œå®Œæ•´è®­ç»ƒå¤§çº¦éœ€è¦ 5 å°æ—¶ã€‚
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once training has been completed, you can use your model for inference:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next steps
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Congratulations on training a new model with LoRA! To learn more about how
    to use your new model, the following guides may be helpful:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œæ‚¨ä½¿ç”¨ LoRA è®­ç»ƒäº†ä¸€ä¸ªæ–°æ¨¡å‹ï¼è¦äº†è§£å¦‚ä½•ä½¿ç”¨æ‚¨çš„æ–°æ¨¡å‹ï¼Œä»¥ä¸‹æŒ‡å—å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š
- en: Learn how to [load different LoRA formats](../using-diffusers/loading_adapters#LoRA)
    trained using community trainers like Kohya and TheLastBen.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½• [åŠ è½½ä¸åŒçš„ LoRA æ ¼å¼](../using-diffusers/loading_adapters#LoRA) ï¼Œè¿™äº›æ ¼å¼æ˜¯ä½¿ç”¨ Kohya
    å’Œ TheLastBen ç­‰ç¤¾åŒºè®­ç»ƒè€…è®­ç»ƒçš„ã€‚
- en: Learn how to use and [combine multiple LoRAâ€™s](../tutorials/using_peft_for_inference)
    with PEFT for inference.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•ä½¿ç”¨ [ç»“åˆå¤šä¸ª LoRA](../tutorials/using_peft_for_inference) ä¸ PEFT è¿›è¡Œæ¨ç†ã€‚
