["```py\nfrom transformers import LiltModel\n\nmodel = LiltModel.from_pretrained(\"path_to_your_files\")\nmodel.push_to_hub(\"name_of_repo_on_the_hub\")\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' classifier_dropout = None channel_shrink_ratio = 4 max_2d_position_embeddings = 1024 **kwargs )\n```", "```py\n>>> from transformers import LiltConfig, LiltModel\n\n>>> # Initializing a LiLT SCUT-DLVCLab/lilt-roberta-en-base style configuration\n>>> configuration = LiltConfig()\n>>> # Randomly initializing a model from the SCUT-DLVCLab/lilt-roberta-en-base style configuration\n>>> model = LiltModel(configuration)\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModel\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModel.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> predicted_class_idx = outputs.logits.argmax(-1).item()\n>>> predicted_class = model.config.id2label[predicted_class_idx]\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> predicted_class_indices = outputs.logits.argmax(-1)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = encoding.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> predicted_answer = tokenizer.decode(predict_answer_tokens)\n```"]