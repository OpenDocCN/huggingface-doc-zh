["```py\nfrom transformers import LiltModel\n\nmodel = LiltModel.from_pretrained(\"path_to_your_files\")\nmodel.push_to_hub(\"name_of_repo_on_the_hub\")\n```", "```py\n>>> from transformers import LiltConfig, LiltModel\n\n>>> # Initializing a LiLT SCUT-DLVCLab/lilt-roberta-en-base style configuration\n>>> configuration = LiltConfig()\n>>> # Randomly initializing a model from the SCUT-DLVCLab/lilt-roberta-en-base style configuration\n>>> model = LiltModel(configuration)\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModel\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModel.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> predicted_class_idx = outputs.logits.argmax(-1).item()\n>>> predicted_class = model.config.id2label[predicted_class_idx]\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> predicted_class_indices = outputs.logits.argmax(-1)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n\n>>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n>>> example = dataset[0]\n>>> words = example[\"tokens\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = encoding.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> predicted_answer = tokenizer.decode(predict_answer_tokens)\n```"]