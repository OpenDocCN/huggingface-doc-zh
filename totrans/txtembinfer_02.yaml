- en: Text Embeddings Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-embeddings-inference/index](https://huggingface.co/docs/text-embeddings-inference/index)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-embeddings-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/entry/start.f5781b4e.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/scheduler.b108d059.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/singletons.26f524d0.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/paths.e8cea87f.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/entry/app.ca5804ae.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/index.008de539.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/nodes/0.a44871a2.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/nodes/4.1c12cf3b.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/Heading.88bfeb84.js">
  prefs: []
  type: TYPE_NORMAL
- en: Text Embeddings Inference (TEI) is a comprehensive toolkit designed for efficient
    deployment and serving of open source text embeddings models. It enables high-performance
    extraction for the most popular models, including FlagEmbedding, Ember, GTE, and
    E5.
  prefs: []
  type: TYPE_NORMAL
- en: TEI offers multiple features tailored to optimize the deployment process and
    enhance overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Features:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streamlined Deployment:** TEI eliminates the need for a model graph compilation
    step for a more efficient deployment process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient Resource Utilization:** Benefit from small Docker images and rapid
    boot times, allowing for true serverless capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Batching:** TEI incorporates token-based dynamic batching thus optimizing
    resource utilization during inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized Inference:** TEI leverages [Flash Attention](https://github.com/HazyResearch/flash-attention),
    [Candle](https://github.com/huggingface/candle), and [cuBLASLt](https://docs.nvidia.com/cuda/cublas/#using-the-cublaslt-api)
    by using optimized transformers code for inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safetensors weight loading:** TEI loads [Safetensors](https://github.com/huggingface/safetensors)
    weights to enable tensor parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production-Ready:** TEI supports distributed tracing through Open Telemetry
    and Prometheus metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benchmarks**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark for [BAAI/bge-base-en-v1.5](https://hf.co/BAAI/bge-large-en-v1.5)
    on an NVIDIA A10 with a sequence length of 512 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latency comparison for batch size of 1](../Images/e6750e035fb2bc7b7101ae271c72e9b5.png)
    ![Throughput comparison for batch size of 1](../Images/415b7acba9a0a287166646493d086977.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Latency comparison for batch size of 32](../Images/2e474d86a47b081240bf1445d61ecead.png)
    ![Throughput comparison for batch size of 32](../Images/077e11f720e151d587efe4e9967a3f68.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Getting Started:**'
  prefs: []
  type: TYPE_NORMAL
- en: To start using TEI, check the [Quick Tour](quick_tour) guide.
  prefs: []
  type: TYPE_NORMAL
