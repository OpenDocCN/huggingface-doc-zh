# PEFT集成

> 原文链接：[https://huggingface.co/docs/peft/tutorial/peft_integrations](https://huggingface.co/docs/peft/tutorial/peft_integrations)

PEFT的实际好处延伸到其他Hugging Face库，如[Diffusers](https://hf.co/docs/diffusers)和[Transformers](https://hf.co/docs/transformers)。PEFT的主要好处之一是，由PEFT方法生成的适配器文件比原始模型小得多，这使得管理和使用多个适配器变得非常容易。您可以通过简单加载针对您正在解决的任务进行微调的新适配器，使用一个预训练基础模型来执行多个任务。或者，您可以将多个适配器与文本到图像扩散模型结合起来，创建新的效果。

这个教程将向您展示PEFT如何帮助您管理Diffusers和Transformers中的适配器。

## Diffusers

Diffusers是一个生成式AI库，用于从文本或图像生成图像和视频，使用扩散模型。LoRA是扩散模型的一种特别流行的训练方法，因为您可以非常快速地训练和共享扩散模型，以生成新风格的图像。为了更容易使用和尝试多个LoRA模型，Diffusers使用PEFT库来帮助管理不同的适配器进行推断。

例如，加载一个基础模型，然后使用[`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)方法加载[artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1)适配器进行推断。加载方法中的`adapter_name`参数由PEFT启用，允许您为适配器设置一个名称，以便更容易引用。

```py
import torch
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
).to("cuda")
pipeline.load_lora_weights(
    "peft-internal-testing/artificialguybr__3DRedmond-V1", 
    weight_name="3DRedmond-3DRenderStyle-3DRenderAF.safetensors", 
    adapter_name="3d"
)
image = pipeline("sushi rolls shaped like kawaii cat faces").images[0]
image
```

![](../Images/44d7d3f44fb0552b1f55c92fcbce7784.png)

现在让我们尝试另一个很酷的LoRA模型，[ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora)。您只需要加载并命名这个新的适配器，使用`adapter_name`，并使用[`set_adapters`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)方法将其设置为当前活动的适配器。

```py
pipeline.load_lora_weights(
    "ostris/super-cereal-sdxl-lora", 
    weight_name="cereal_box_sdxl_v1.safetensors", 
    adapter_name="cereal"
)
pipeline.set_adapters("cereal")
image = pipeline("sushi rolls shaped like kawaii cat faces").images[0]
image
```

![](../Images/93e1d1297cecfeb1cad00cf824e024ed.png)

最后，您可以调用[`disable_lora`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora)方法来恢复基础模型。

```py
pipeline.disable_lora()
```

在[使用PEFT进行推断](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference)教程中了解更多关于PEFT如何支持Diffusers的信息。

## Transformers

Transformers是各种任务和所有形式的预训练模型的集合。您可以加载这些模型进行训练或推断。许多模型都是大型语言模型（LLMs），因此将PEFT与Transformers集成以管理和训练适配器是有意义的。

加载一个基础预训练模型进行训练。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
```

接下来，添加一个适配器配置来指定如何调整模型参数。调用[add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)方法将配置添加到基础模型中。

```py
from peft import LoraConfig

config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM"
)
model.add_adapter(peft_config)
```

现在，您可以使用Transformer的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类或您喜欢的任何训练框架来训练模型。

要将新训练的模型用于推断，[AutoModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)类在后端使用PEFT加载适配器权重和配置文件到基础预训练模型中。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
```

如果您有兴趣比较或使用多个适配器，您也可以调用[add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)方法将适配器配置添加到基础模型中。唯一的要求是适配器类型必须相同（不能混合LoRA和LoHa适配器）。

```py
from transformers import AutoModelForCausalLM
from peft import LoraConfig

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
model.add_adapter(lora_config_1, adapter_name="adapter_1")
```

再次调用[add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)以将新的适配器附加到基础模型。

```py
model.add_adapter(lora_config_2, adapter_name="adapter_2")
```

然后，您可以使用[set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.set_adapter)来设置当前活动的适配器。

```py
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))
```

要禁用适配器，请调用[disable_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.disable_adapter)方法。

```py
model.disable_adapter()
```

如果你感兴趣，可以查看[使用PEFT加载和训练适配器](https://huggingface.co/docs/transformers/main/peft)教程以了解更多。
