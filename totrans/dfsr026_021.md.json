["```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"sd-dreambooth-library/herge-style\", torch_dtype=torch.float16).to(\"cuda\")\nprompt = \"A cute herge_style brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\nimage = pipeline(prompt).images[0]\nimage\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```", "```py\npipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, <gta5-artwork> style\"\nimage = pipeline(prompt).images[0]\nimage\n```", "```py\npipeline.load_textual_inversion(\n    \"sayakpaul/EasyNegative-test\", weight_name=\"EasyNegative.safetensors\", token=\"EasyNegative\"\n)\n```", "```py\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative\"\nnegative_prompt = \"EasyNegative\"\n\nimage = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=50).images[0]\nimage\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n```", "```py\npipeline.load_lora_weights(\"ostris/super-cereal-sdxl-lora\", weight_name=\"cereal_box_sdxl_v1.safetensors\")\nprompt = \"bears, pizza bites\"\nimage = pipeline(prompt).images[0]\nimage\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.unet.load_attn_procs(\"jbilcke-hf/sdxl-cinematic-1\", weight_name=\"pytorch_lora_weights.safetensors\")\n\n# use cnmt in the prompt to trigger the LoRA\nprompt = \"A cute cnmt eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\nimage = pipeline(prompt).images[0]\nimage\n```", "```py\npipeline.unload_lora_weights()\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline, AutoencoderKL\nimport torch\n\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    vae=vae,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n```", "```py\npipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\")\npipeline.fuse_lora(lora_scale=0.7)\n\n# to unfuse the LoRA weights\npipeline.unfuse_lora()\n```", "```py\npipeline.load_lora_weights(\"ostris/super-cereal-sdxl-lora\")\npipeline.fuse_lora(lora_scale=0.7)\n```", "```py\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\nimage = pipeline(prompt).images[0]\nimage\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\npipeline.load_lora_weights(\"ostris/super-cereal-sdxl-lora\", weight_name=\"cereal_box_sdxl_v1.safetensors\", adapter_name=\"cereal\")\n```", "```py\npipeline.set_adapters([\"ikea\", \"cereal\"], adapter_weights=[0.7, 0.5])\n```", "```py\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\nimage = pipeline(prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": 1.0}).images[0]\nimage\n```", "```py\n!wget https://civitai.com/api/download/models/168776 -O blueprintify-sd-xl-10.safetensors\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"path/to/weights\", weight_name=\"blueprintify-sd-xl-10.safetensors\")\n```", "```py\n# use bl3uprint in the prompt to trigger the LoRA\nprompt = \"bl3uprint, a highly detailed blueprint of the eiffel tower, explaining how to build all parts, many txt, blueprint grid backdrop\"\nimage = pipeline(prompt).images[0]\nimage\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"TheLastBen/William_Eggleston_Style_SDXL\", weight_name=\"wegg.safetensors\")\n\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \"a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\"\nimage = pipeline(prompt=prompt).images[0]\nimage\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\nfrom diffusers.utils import load_image\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```", "```py\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nfrom transformers import CLIPVisionModelWithProjection\nimport torch\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"h94/IP-Adapter\", \n    subfolder=\"models/image_encoder\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n```", "```py\npipeline.set_ip_adapter_scale(0.6)\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimages = pipeline(\n\u00a0 \u00a0 prompt='best quality, high quality, wearing sunglasses', \n\u00a0 \u00a0 ip_adapter_image=image,\n\u00a0 \u00a0 negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\", \n\u00a0 \u00a0 num_inference_steps=50,\n\u00a0 \u00a0 generator=generator,\n).images\nimages[0]\n```", "```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nfrom diffusers.utils import load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n\nimage = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/vermeer.jpg\")\nip_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/river.png\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimages = pipeline(\n\u00a0 \u00a0 prompt='best quality, high quality', \n\u00a0 \u00a0 image = image,\n\u00a0 \u00a0 ip_adapter_image=ip_image,\n\u00a0 \u00a0 num_inference_steps=50,\n\u00a0 \u00a0 generator=generator,\n\u00a0 \u00a0 strength=0.6,\n).images\nimages[0]\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/watercolor_painting.jpeg\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimage = pipeline(\n    prompt=\"best quality, high quality\", \n    ip_adapter_image=image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\", \n    num_inference_steps=25,\n    generator=generator,\n).images[0]\nimage.save(\"sdxl_t2i.png\")\n```", "```py\n# Load ip-adapter-full-face_sd15.bin\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter-full-face_sd15.bin\")\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\nfrom diffusers.utils import load_image\n\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter-full-face_sd15.bin\")\n\npipeline.set_ip_adapter_scale(0.7)\n\nimage = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ai_face2.png\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\n\nimage = pipeline(\n    prompt=\"A photo of a girl wearing a black dress, holding red roses in hand, upper body, behind is the Eiffel Tower\",\n    ip_adapter_image=image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\", \n    num_inference_steps=50, num_images_per_prompt=1, width=512, height=704,\n    generator=generator,\n).images[0]\n```", "```py\nimport torch\nfrom diffusers import AutoPipelineForText2Image, DDIMScheduler\nfrom transformers import CLIPVisionModelWithProjection\nfrom diffusers.utils import load_image\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"h94/IP-Adapter\", \n    subfolder=\"models/image_encoder\",\n    torch_dtype=torch.float16,\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    image_encoder=image_encoder,\n)\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.load_ip_adapter(\n  \"h94/IP-Adapter\", \n  subfolder=\"sdxl_models\", \n  weight_name=[\"ip-adapter-plus_sdxl_vit-h.safetensors\", \"ip-adapter-plus-face_sdxl_vit-h.safetensors\"]\n)\npipeline.set_ip_adapter_scale([0.7, 0.3])\npipeline.enable_model_cpu_offload()\n\nface_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png\")\nstyle_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\nstyle_images =  [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\n\nimage = pipeline(\n    prompt=\"wonderwoman\",\n    ip_adapter_image=[style_images, face_image],\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\", \n    num_inference_steps=50, num_images_per_prompt=1,\n    generator=generator,\n).images[0]\n```", "```py\nfrom diffusers import DiffusionPipeline, LCMScheduler\nimport torch\nfrom diffusers.utils import load_image\n\nmodel_id =  \"sd-dreambooth-library/herge-style\"\nlcm_lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n\npipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n\npipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\npipe.load_lora_weights(lcm_lora_id)\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"best quality, high quality\"\nimage = load_image(\"https://user-images.githubusercontent.com/24734142/266492875-2d50d223-8475-44f0-a7c6-08b51cb53572.png\")\nimages = pipe(\n    prompt=prompt,\n    ip_adapter_image=image,\n    num_inference_steps=4,\n    guidance_scale=1,\n).images[0]\n```", "```py\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nimport torch\nfrom diffusers.utils import load_image\n\ncontrolnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\ncontrolnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16)\npipeline.to(\"cuda\")\n\nimage = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/statue.png\")\ndepth_map = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/depth.png\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimages = pipeline(\n    prompt='best quality, high quality', \n    image=depth_map,\n    ip_adapter_image=image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\", \n    num_inference_steps=50,\n    generator=generator,\n).images\nimages[0]\n```"]