# 文本反演

> 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/textual_inversion_inference](https://huggingface.co/docs/diffusers/using-diffusers/textual_inversion_inference)

[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)支持文本反演，这是一种使稳定扩散等模型能够仅从少量示例图像中学习新概念的技术。这使您对生成的图像有更多控制，并允许您将模型定制为特定概念。您可以通过[Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer)中的社区创建的概念集快速入门。

本指南将向您展示如何使用稳定扩散概念化器中的预先学习概念来运行文本反演推断。如果您有兴趣使用文本反演教授模型新概念，请查看[文本反演](../training/text_inversion)培训指南。

导入必要的库：

```py
import torch
from diffusers import StableDiffusionPipeline
from diffusers.utils import make_image_grid
```

## 稳定扩散 1 和 2

选择一个稳定扩散的检查点和一个来自[稳定扩散概念化器](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer)的预先学习的概念：

```py
pretrained_model_name_or_path = "runwayml/stable-diffusion-v1-5"
repo_id_embeds = "sd-concepts-library/cat-toy"
```

现在您可以加载一个流水线，并将预先学习的概念传递给它：

```py
pipeline = StableDiffusionPipeline.from_pretrained(
    pretrained_model_name_or_path, torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipeline.load_textual_inversion(repo_id_embeds)
```

通过使用特殊的占位符标记`<cat-toy>`创建一个包含预先学习概念的提示，并选择要生成的样本数量和图像行数：

```py
prompt = "a grafitti in a favela wall with a <cat-toy> on it"

num_samples_per_row = 2
num_rows = 2
```

然后运行流水线（可以随意调整参数，如`num_inference_steps`和`guidance_scale`，以查看它们如何影响图像质量），保存生成的图像，并使用您在开始时创建的辅助函数进行可视化：

```py
all_images = []
for _ in range(num_rows):
    images = pipeline(prompt, num_images_per_prompt=num_samples_per_row, num_inference_steps=50, guidance_scale=7.5).images
    all_images.extend(images)

grid = make_image_grid(all_images, num_rows, num_samples_per_row)
grid
```

![](../Images/71bc10f1a2656599ea557d150439d022.png)

## 稳定扩散 XL

稳定扩散 XL（SDXL）也可以使用文本反演向量进行推断。与稳定扩散 1 和 2 不同，SDXL 有两个文本编码器，因此您将需要两个文本反演嵌入 - 每个文本编码器模型一个。

让我们下载 SDXL 文本反演嵌入并更仔细地查看它的结构：

```py
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file

file = hf_hub_download("dn118/unaestheticXL", filename="unaestheticXLv31.safetensors")
state_dict = load_file(file)
state_dict
```

```py
{'clip_g': tensor([[ 0.0077, -0.0112,  0.0065,  ...,  0.0195,  0.0159,  0.0275],
         ...,
         [-0.0170,  0.0213,  0.0143,  ..., -0.0302, -0.0240, -0.0362]],
 'clip_l': tensor([[ 0.0023,  0.0192,  0.0213,  ..., -0.0385,  0.0048, -0.0011],
         ...,
         [ 0.0475, -0.0508, -0.0145,  ...,  0.0070, -0.0089, -0.0163]],
```

有两个张量，`"clip_g"`和`"clip_l"`。`"clip_g"`对应于 SDXL 中更大的文本编码器，并指的是`pipe.text_encoder_2`，`"clip_l"`指的是`pipe.text_encoder`。

现在您可以通过将它们与正确的文本编码器和标记器一起传递给[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)来单独加载每个张量：

```py
from diffusers import AutoPipelineForText2Image
import torch

pipe = AutoPipelineForText2Image.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", variant="fp16", torch_dtype=torch.float16)
pipe.to("cuda")

pipe.load_textual_inversion(state_dict["clip_g"], token="unaestheticXLv31", text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)
pipe.load_textual_inversion(state_dict["clip_l"], token="unaestheticXLv31", text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)

# the embedding should be used as a negative embedding, so we pass it as a negative prompt
generator = torch.Generator().manual_seed(33)
image = pipe("a woman standing in front of a mountain", negative_prompt="unaestheticXLv31", generator=generator).images[0]
image
```
