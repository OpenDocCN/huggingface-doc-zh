["```py\nfrom transformers import TFMobileViTForImageClassification\nimport tensorflow as tf\n\nmodel_ckpt = \"apple/mobilevit-xx-small\"\nmodel = TFMobileViTForImageClassification.from_pretrained(model_ckpt)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.SELECT_TF_OPS,\n]\ntflite_model = converter.convert()\ntflite_filename = model_ckpt.split(\"/\")[-1] + \".tflite\"\nwith open(tflite_filename, \"wb\") as f:\n    f.write(tflite_model)\n```", "```py\n( num_channels = 3 image_size = 256 patch_size = 2 hidden_sizes = [144, 192, 240] neck_hidden_sizes = [16, 32, 64, 96, 128, 160, 640] num_attention_heads = 4 mlp_ratio = 2.0 expand_ratio = 4.0 hidden_act = 'silu' conv_kernel_size = 3 output_stride = 32 hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.0 classifier_dropout_prob = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 qkv_bias = True aspp_out_channels = 256 atrous_rates = [6, 12, 18] aspp_dropout_prob = 0.1 semantic_loss_ignore_index = 255 **kwargs )\n```", "```py\n>>> from transformers import MobileViTConfig, MobileViTModel\n\n>>> # Initializing a mobilevit-small style configuration\n>>> configuration = MobileViTConfig()\n\n>>> # Initializing a model from the mobilevit-small style configuration\n>>> model = MobileViTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( *args **kwargs )\n```", "```py\n( images segmentation_maps = None **kwargs )\n```", "```py\n( outputs target_sizes: List = None ) \u2192 export const metadata = 'undefined';semantic_segmentation\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_center_crop: bool = True crop_size: Dict = None do_flip_channel_order: bool = True **kwargs )\n```", "```py\n( images: Union segmentation_maps: Union = None do_resize: bool = None size: Dict = None resample: Resampling = None do_rescale: bool = None rescale_factor: float = None do_center_crop: bool = None crop_size: Dict = None do_flip_channel_order: bool = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( outputs target_sizes: List = None ) \u2192 export const metadata = 'undefined';semantic_segmentation\n```", "```py\n( config: MobileViTConfig expand_output: bool = True )\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, MobileViTModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevit-small\")\n>>> model = MobileViTModel.from_pretrained(\"apple/mobilevit-small\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 640, 8, 8]\n```", "```py\n( config: MobileViTConfig )\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, MobileViTForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevit-small\")\n>>> model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntabby, tabby cat\n```", "```py\n( config: MobileViTConfig )\n```", "```py\n( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import requests\n>>> import torch\n>>> from PIL import Image\n>>> from transformers import AutoImageProcessor, MobileViTForSemanticSegmentation\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n>>> model = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # logits are of shape (batch_size, num_labels, height, width)\n>>> logits = outputs.logits\n```", "```py\n( config: MobileViTConfig expand_output: bool = True *inputs **kwargs )\n```", "```py\n( pixel_values: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFMobileViTModel\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevit-small\")\n>>> model = TFMobileViTModel.from_pretrained(\"apple/mobilevit-small\")\n\n>>> inputs = image_processor(image, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 640, 8, 8]\n```", "```py\n( config: MobileViTConfig *inputs **kwargs )\n```", "```py\n( pixel_values: tf.Tensor | None = None output_hidden_states: Optional[bool] = None labels: tf.Tensor | None = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFMobileViTForImageClassification\n>>> import tensorflow as tf\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevit-small\")\n>>> model = TFMobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n\n>>> inputs = image_processor(image, return_tensors=\"tf\")\n>>> logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = int(tf.math.argmax(logits, axis=-1))\n>>> print(model.config.id2label[predicted_label])\ntabby, tabby cat\n```", "```py\n( config: MobileViTConfig **kwargs )\n```", "```py\n( pixel_values: tf.Tensor | None = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFMobileViTForSemanticSegmentation\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n>>> model = TFMobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n\n>>> # logits are of shape (batch_size, num_labels, height, width)\n>>> logits = outputs.logits\n```"]