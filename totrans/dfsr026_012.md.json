["```py\npip install -U diffusers\n```", "```py\npip install -U transformers accelerate peft\n```", "```py\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline\n\n# Load the pipeline in full-precision and place its model components on CUDA.\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\"\n).to(\"cuda\")\n\n# Run the attention ops without SDPA.\npipe.unet.set_default_attn_processor()\npipe.vae.set_default_attn_processor()\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\n# Run the attention ops without SDPA.\npipe.unet.set_default_attn_processor()\npipe.vae.set_default_attn_processor()\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\ntorch._inductor.config.conv_1x1_as_mm = True\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.epilogue_fusion = False\ntorch._inductor.config.coordinate_descent_check_all_directions = True\n```", "```py\npipe.unet.to(memory_format=torch.channels_last)\npipe.vae.to(memory_format=torch.channels_last)\n```", "```py\n# Compile the UNet and VAE.\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)\npipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# First call to `pipe` is slow, subsequent ones are faster.\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```", "```py\n- latents = unet(\n-   latents, timestep=timestep, encoder_hidden_states=prompt_embeds\n-).sample\n\n+ latents = unet(\n+   latents, timestep=timestep, encoder_hidden_states=prompt_embeds, return_dict=False\n+)[0]\n```", "```py\npipe.fuse_qkv_projections()\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch \n\n# Notice the two new flags at the end.\ntorch._inductor.config.conv_1x1_as_mm = True\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.epilogue_fusion = False\ntorch._inductor.config.coordinate_descent_check_all_directions = True\ntorch._inductor.config.force_fuse_int_mm_with_mul = True\ntorch._inductor.config.use_mixed_mm = True\n```", "```py\ndef dynamic_quant_filter_fn(mod, *args):\n    return (\n        isinstance(mod, torch.nn.Linear)\n        and mod.in_features > 16\n        and (mod.in_features, mod.out_features)\n        not in [\n            (1280, 640),\n            (1920, 1280),\n            (1920, 640),\n            (2048, 1280),\n            (2048, 2560),\n            (2560, 1280),\n            (256, 128),\n            (2816, 1280),\n            (320, 640),\n            (512, 1536),\n            (512, 256),\n            (512, 512),\n            (640, 1280),\n            (640, 1920),\n            (640, 320),\n            (640, 5120),\n            (640, 640),\n            (960, 320),\n            (960, 640),\n        ]\n    )\n\ndef conv_filter_fn(mod, *args):\n    return (\n        isinstance(mod, torch.nn.Conv2d) and mod.kernel_size == (1, 1) and 128 in [mod.in_channels, mod.out_channels]\n    )\n```", "```py\n# SDPA + bfloat16.\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\n# Combine attention projection matrices.\npipe.fuse_qkv_projections()\n\n# Change the memory layout.\npipe.unet.to(memory_format=torch.channels_last)\npipe.vae.to(memory_format=torch.channels_last)\n```", "```py\nfrom torchao import swap_conv2d_1x1_to_linear\n\nswap_conv2d_1x1_to_linear(pipe.unet, conv_filter_fn)\nswap_conv2d_1x1_to_linear(pipe.vae, conv_filter_fn)\n```", "```py\nfrom torchao import apply_dynamic_quant\n\napply_dynamic_quant(pipe.unet, dynamic_quant_filter_fn)\napply_dynamic_quant(pipe.vae, dynamic_quant_filter_fn)\n```", "```py\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)\npipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```"]