- en: Model Based Reinforcement Learning (MBRL)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/model-based](https://huggingface.co/learn/deep-rl-course/unitbonus3/model-based)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Model-based reinforcement learning only differs from its model-free counterpart
    in learning a *dynamics model*, but that has substantial downstream effects on
    how the decisions are made.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The dynamics model usually models the environment transition dynamics,<math><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">s_{t+1}
    = f_\theta (s_t, a_t)</annotation></semantics></math> st+1​=fθ​(st​,at​), but
    things like inverse dynamics models (mapping from states to actions) or reward
    models (predicting rewards) can be used in this framework.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Simple definition
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an agent that repeatedly tries to solve a problem, **accumulating state
    and action data**.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that data, the agent creates a structured learning tool, *a dynamics model*,
    to reason about the world.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the dynamics model, the agent **decides how to act by predicting the future**.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With those actions, **the agent collects more data, improves said model, and
    hopefully improves future actions**.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Academic definition
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model-based reinforcement learning (MBRL) follows the framework of an agent
    interacting in an environment, **learning a model of said environment**, and then
    **leveraging the model for control (making decisions).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the agent acts in a Markov Decision Process (MDP) governed by
    a transition function<math><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>f</mi><mo
    stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">s_{t+1}
    = f (s_t , a_t)</annotation></semantics></math> st+1​=f(st​,at​) and returns a
    reward at each step<math><semantics><mrow><mi>r</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">r(s_t, a_t)</annotation></semantics></math>
    r(st​,at​). With a collected dataset<math><semantics><mrow><mi>D</mi><mo>:</mo><mo>=</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi>a</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    separator="true">,</mo><msub><mi>r</mi><mi>i</mi></msub></mrow></mrow> <annotation
    encoding="application/x-tex">D :={ s_i, a_i, s_{i+1}, r_i}</annotation></semantics></math>
    D:=si​,ai​,si+1​,ri​, the agent learns a model,<math><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">s_{t+1}
    = f_\theta (s_t , a_t)</annotation></semantics></math> st+1​=fθ​(st​,at​) **to
    minimize the negative log-likelihood of the transitions**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: We employ sample-based model-predictive control (MPC) using the learned dynamics
    model, which optimizes the expected reward over a finite, recursively predicted
    horizon,<math><semantics><mrow><mi>τ</mi></mrow> <annotation encoding="application/x-tex">\tau</annotation></semantics></math>
    τ, from a set of actions sampled from a uniform distribution<math><semantics><mrow><mi>U</mi><mo
    stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">U(a)</annotation></semantics></math> U(a), (see [paper](https://arxiv.org/pdf/2002.04523)
    or [paper](https://arxiv.org/pdf/2012.09156.pdf) or [paper](https://arxiv.org/pdf/2009.01221.pdf)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基于样本的模型预测控制（MPC），利用学习的动力学模型优化在有限的、递归预测的时间范围内的预期奖励，从一个从均匀分布中采样的动作集合U(a)中进行优化（参见[论文](https://arxiv.org/pdf/2002.04523)或[论文](https://arxiv.org/pdf/2012.09156.pdf)或[论文](https://arxiv.org/pdf/2009.01221.pdf)）。
- en: Further reading
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on MBRL, we recommend you check out the following resources:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有关MBRL的更多信息，我们建议您查看以下资源：
- en: A [blog post on debugging MBRL](https://www.natolambert.com/writing/debugging-mbrl).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于调试MBRL的[博客文章](https://www.natolambert.com/writing/debugging-mbrl)。
- en: A [recent review paper on MBRL](https://arxiv.org/abs/2006.16712),
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于MBRL的[最新综述论文](https://arxiv.org/abs/2006.16712)，
- en: Author
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者
- en: This section was written by [Nathan Lambert](https://twitter.com/natolambert)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分由[Nathan Lambert](https://twitter.com/natolambert)撰写
