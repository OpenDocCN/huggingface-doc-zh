# Vision Transformerï¼ˆViTï¼‰

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit)

## æ¦‚è¿°

Vision Transformerï¼ˆViTï¼‰æ¨¡å‹æ˜¯ç”±Alexey Dosovitskiyã€Lucas Beyerã€Alexander Kolesnikovã€Dirk Weissenbornã€Xiaohua Zhaiã€Thomas Unterthinerã€Mostafa Dehghaniã€Matthias Mindererã€Georg Heigoldã€Sylvain Gellyã€Jakob Uszkoreitã€Neil Houlsbyåœ¨[ä¸€å¼ å›¾å€¼16x16ä¸ªè¯ï¼šç”¨äºå¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„Transformer](https://arxiv.org/abs/2010.11929)ä¸­æå‡ºçš„ã€‚è¿™æ˜¯ç¬¬ä¸€ç¯‡æˆåŠŸåœ¨ImageNetä¸Šè®­ç»ƒTransformerç¼–ç å™¨çš„è®ºæ–‡ï¼Œä¸ç†Ÿæ‚‰çš„å·ç§¯æ¶æ„ç›¸æ¯”å–å¾—äº†éå¸¸å¥½çš„ç»“æœã€‚

è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*å°½ç®¡Transformeræ¶æ„å·²ç»æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„äº‹å®æ ‡å‡†ï¼Œä½†å®ƒåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚åœ¨è§†è§‰ä¸­ï¼Œæ³¨æ„åŠ›è¦ä¹ˆä¸å·ç§¯ç½‘ç»œä¸€èµ·åº”ç”¨ï¼Œè¦ä¹ˆç”¨æ¥æ›¿æ¢å·ç§¯ç½‘ç»œçš„æŸäº›ç»„ä»¶ï¼ŒåŒæ—¶ä¿æŒå®ƒä»¬çš„æ•´ä½“ç»“æ„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¿™ç§å¯¹CNNçš„ä¾èµ–æ˜¯ä¸å¿…è¦çš„ï¼Œç›´æ¥åº”ç”¨äºå›¾åƒå—åºåˆ—çš„çº¯Transformerå¯ä»¥åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶è½¬ç§»åˆ°å¤šä¸ªä¸­ç­‰æˆ–å°å‹å›¾åƒè¯†åˆ«åŸºå‡†ï¼ˆImageNetã€CIFAR-100ã€VTABç­‰ï¼‰æ—¶ï¼ŒVision Transformerï¼ˆViTï¼‰å–å¾—äº†ä¸æœ€å…ˆè¿›çš„å·ç§¯ç½‘ç»œç›¸æ¯”çš„ä¼˜å¼‚ç»“æœï¼ŒåŒæ—¶éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒã€‚*

![drawing](../Images/603d60786d4fe02e975af3e3e9553816.png) ViTæ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2010.11929)ã€‚

åœ¨åŸå§‹Vision Transformerä¹‹åï¼Œå·²ç»è¿›è¡Œäº†ä¸€äº›åç»­å·¥ä½œï¼š

+   [DeiT](deit)ï¼ˆé«˜æ•ˆæ•°æ®å›¾åƒTransformerï¼‰ç”±Facebook AIæå‡ºã€‚DeiTæ¨¡å‹æ˜¯ç»è¿‡è’¸é¦çš„è§†è§‰Transformerã€‚DeiTçš„ä½œè€…è¿˜å‘å¸ƒäº†æ›´é«˜æ•ˆè®­ç»ƒçš„ViTæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç›´æ¥å°†å…¶æ’å…¥[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)æˆ–[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)ã€‚æœ‰4ä¸ªå˜ä½“å¯ç”¨ï¼ˆ3ç§ä¸åŒå¤§å°ï¼‰ï¼š*facebook/deit-tiny-patch16-224*ã€*facebook/deit-small-patch16-224*ã€*facebook/deit-base-patch16-224*å’Œ*facebook/deit-base-patch16-384*ã€‚è¯·æ³¨æ„ï¼Œåº”ä½¿ç”¨[DeiTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/deit#transformers.DeiTImageProcessor)æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚

+   [BEiT](beit)ï¼ˆå›¾åƒTransformerçš„BERTé¢„è®­ç»ƒï¼‰ç”±å¾®è½¯ç ”ç©¶é™¢ã€‚BEiTæ¨¡å‹é€šè¿‡å—BERTå¯å‘çš„è‡ªç›‘ç£æ–¹æ³•ï¼ˆé®è”½å›¾åƒå»ºæ¨¡ï¼‰å’ŒåŸºäºVQ-VAEçš„æ–¹æ³•ï¼Œä¼˜äºä½¿ç”¨ç›‘ç£é¢„è®­ç»ƒçš„è§†è§‰Transformerã€‚

+   DINOï¼ˆä¸€ç§ç”¨äºè‡ªç›‘ç£è®­ç»ƒçš„Vision Transformeræ–¹æ³•ï¼‰ç”±Facebook AIæå‡ºã€‚ä½¿ç”¨DINOæ–¹æ³•è®­ç»ƒçš„Vision Transformerå±•ç°å‡ºä¸å·ç§¯æ¨¡å‹ä¸åŒçš„éå¸¸æœ‰è¶£çš„ç‰¹æ€§ã€‚å®ƒä»¬èƒ½å¤Ÿåˆ†å‰²å¯¹è±¡ï¼Œè€Œæ— éœ€ç»è¿‡è®­ç»ƒã€‚å¯ä»¥åœ¨[hub](https://huggingface.co/models?other=dino)ä¸Šæ‰¾åˆ°DINOçš„æ£€æŸ¥ç‚¹ã€‚

+   [MAE](vit_mae)ï¼ˆé®è”½è‡ªåŠ¨ç¼–ç å™¨ï¼‰ç”±Facebook AIæå‡ºã€‚é€šè¿‡é¢„è®­ç»ƒVision Transformeræ¥é‡å»º75%çš„é®è”½è¡¥ä¸çš„åƒç´ å€¼ï¼ˆä½¿ç”¨ä¸å¯¹ç§°çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼‰ï¼Œä½œè€…è¡¨æ˜ï¼Œè¿™ç§ç®€å•æ–¹æ³•åœ¨å¾®è°ƒåä¼˜äºç›‘ç£é¢„è®­ç»ƒã€‚

æ­¤æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç ï¼ˆä½¿ç”¨JAXç¼–å†™ï¼‰å¯åœ¨[æ­¤å¤„](https://github.com/google-research/vision_transformer)æ‰¾åˆ°ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä»Ross Wightmançš„[timmåº“](https://github.com/rwightman/pytorch-image-models)ä¸­è½¬æ¢äº†æƒé‡ï¼Œä»–å·²ç»å°†æƒé‡ä»JAXè½¬æ¢ä¸ºPyTorchã€‚æ„Ÿè°¢ä»–ï¼

## ä½¿ç”¨æç¤º

+   ä¸ºäº†å°†å›¾åƒé¦ˆé€åˆ°Transformerç¼–ç å™¨ä¸­ï¼Œæ¯ä¸ªå›¾åƒè¢«åˆ†å‰²æˆä¸€ç³»åˆ—å›ºå®šå¤§å°ä¸”ä¸é‡å çš„è¡¥ä¸ï¼Œç„¶åè¿›è¡Œçº¿æ€§åµŒå…¥ã€‚æ·»åŠ äº†ä¸€ä¸ª[CLS]æ ‡è®°ï¼Œç”¨ä½œæ•´ä¸ªå›¾åƒçš„è¡¨ç¤ºï¼Œå¯ç”¨äºåˆ†ç±»ã€‚ä½œè€…è¿˜æ·»åŠ äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå¹¶å°†ç»“æœå‘é‡åºåˆ—é¦ˆé€åˆ°æ ‡å‡†Transformerç¼–ç å™¨ã€‚

+   ç”±äºVision TransformeræœŸæœ›æ¯ä¸ªå›¾åƒå…·æœ‰ç›¸åŒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨[ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)æ¥è°ƒæ•´ï¼ˆæˆ–é‡æ–°ç¼©æ”¾ï¼‰å’Œè§„èŒƒåŒ–å›¾åƒä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚

+   åœ¨é¢„è®­ç»ƒæˆ–å¾®è°ƒæœŸé—´ä½¿ç”¨çš„è¡¥ä¸åˆ†è¾¨ç‡å’Œå›¾åƒåˆ†è¾¨ç‡åæ˜ åœ¨æ¯ä¸ªæ£€æŸ¥ç‚¹çš„åç§°ä¸­ã€‚ä¾‹å¦‚ï¼Œ`google/vit-base-patch16-224`æŒ‡çš„æ˜¯ä¸€ä¸ªåŸºæœ¬å¤§å°çš„æ¶æ„ï¼Œè¡¥ä¸åˆ†è¾¨ç‡ä¸º16x16ï¼Œå¾®è°ƒåˆ†è¾¨ç‡ä¸º224x224ã€‚æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨[hub](https://huggingface.co/models?search=vit)ä¸Šæ‰¾åˆ°ã€‚

+   å¯ç”¨çš„æ£€æŸ¥ç‚¹è¦ä¹ˆï¼ˆ1ï¼‰ä»…åœ¨[ImageNet-21k](http://www.image-net.org/)ï¼ˆä¸€ä¸ªåŒ…å«1400ä¸‡å›¾åƒå’Œ21kç±»åˆ«çš„é›†åˆï¼‰ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¦ä¹ˆï¼ˆ2ï¼‰è¿˜åœ¨[ImageNet](http://www.image-net.org/challenges/LSVRC/2012/)ï¼ˆä¹Ÿç§°ä¸ºILSVRC 2012ï¼Œä¸€ä¸ªåŒ…å«130ä¸‡å›¾åƒå’Œ1000ç±»åˆ«çš„é›†åˆï¼‰ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚

+   Vision Transformeræ˜¯ä½¿ç”¨åˆ†è¾¨ç‡ä¸º224x224è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚åœ¨å¾®è°ƒæœŸé—´ï¼Œé€šå¸¸æ¯”é¢„è®­ç»ƒä½¿ç”¨æ›´é«˜çš„åˆ†è¾¨ç‡æœ‰ç›Š[(Touvronç­‰äººï¼Œ2019)](https://arxiv.org/abs/1906.06423)ï¼Œ[(Kolesnikovç­‰äººï¼Œ2020)](https://arxiv.org/abs/1912.11370)ã€‚ä¸ºäº†åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹å¾®è°ƒï¼Œä½œè€…å¯¹é¢„è®­ç»ƒçš„ä½ç½®åµŒå…¥è¿›è¡Œäº†2Dæ’å€¼ï¼Œæ ¹æ®å®ƒä»¬åœ¨åŸå§‹å›¾åƒä¸­çš„ä½ç½®ã€‚

+   æœ€ä½³ç»“æœæ˜¯é€šè¿‡ç›‘ç£é¢„è®­ç»ƒè·å¾—çš„ï¼Œè¿™åœ¨NLPä¸­å¹¶éå¦‚æ­¤ã€‚ä½œè€…è¿˜è¿›è¡Œäº†ä¸€ä¸ªå®éªŒï¼Œä½¿ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒç›®æ ‡ï¼Œå³æ©ç è¡¥ä¸é¢„æµ‹ï¼ˆå—åˆ°æ©ç è¯­è¨€å»ºæ¨¡çš„å¯å‘ï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè¾ƒå°çš„ViT-B/16æ¨¡å‹åœ¨ImageNetä¸Šå®ç°äº†79.9%çš„å‡†ç¡®ç‡ï¼Œæ¯”ä»å¤´å¼€å§‹è®­ç»ƒæé«˜äº†2%ï¼Œä½†ä»ç„¶è½åäºç›‘ç£é¢„è®­ç»ƒ4%ã€‚

## èµ„æº

å…³äºæ¨ç†ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šå¾®è°ƒViTçš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)æ‰¾åˆ°ã€‚è¿™é‡Œåˆ—å‡ºäº†å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ViTã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

`ViTForImageClassification`ç”±ä»¥ä¸‹æ”¯æŒï¼š

å›¾åƒåˆ†ç±»

+   å…³äºå¦‚ä½•ä½¿ç”¨Hugging Face Transformerså¯¹å›¾åƒåˆ†ç±»è¿›è¡Œå¾®è°ƒçš„åšå®¢æ–‡ç« 

+   å…³äºä½¿ç”¨Hugging Face Transformerså’Œ`Keras`è¿›è¡Œå›¾åƒåˆ†ç±»çš„åšå®¢æ–‡ç« 

+   å…³äºä½¿ç”¨Hugging Face Transformersè¿›è¡Œå›¾åƒåˆ†ç±»çš„å¾®è°ƒçš„ç¬”è®°

+   å…³äºå¦‚ä½•ä½¿ç”¨Hugging Face Traineråœ¨CIFAR-10ä¸Šå¾®è°ƒVision Transformerçš„ç¬”è®°

+   å…³äºå¦‚ä½•åœ¨CIFAR-10ä¸Šä½¿ç”¨PyTorch Lightningå¯¹Vision Transformerè¿›è¡Œå¾®è°ƒçš„ç¬”è®°

âš—ï¸ ä¼˜åŒ–

+   å…³äºå¦‚ä½•ä½¿ç”¨Optimumå¯¹Vision Transformerï¼ˆViTï¼‰è¿›è¡Œé‡åŒ–åŠ é€Ÿçš„åšå®¢æ–‡ç« 

âš¡ï¸ æ¨ç†

+   ä¸€ä¸ªå…³äº[å¿«é€Ÿæ¼”ç¤ºï¼šGoogle Brain çš„ Vision Transformerï¼ˆViTï¼‰](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb)çš„ç¬”è®°æœ¬

ğŸš€ éƒ¨ç½²

+   ä¸€ç¯‡å…³äº[åœ¨ Hugging Face ä¸­ä½¿ç”¨ TF Serving éƒ¨ç½² Tensorflow è§†è§‰æ¨¡å‹](https://huggingface.co/blog/tf-serving-vision)çš„åšå®¢æ–‡ç« 

+   ä¸€ç¯‡å…³äº[åœ¨ Vertex AI ä¸Šéƒ¨ç½² Hugging Face ViT](https://huggingface.co/blog/deploy-vertex-ai)çš„åšå®¢æ–‡ç« 

+   ä¸€ç¯‡å…³äº[åœ¨ Kubernetes ä¸Šä½¿ç”¨ TF Serving éƒ¨ç½² Hugging Face ViT](https://huggingface.co/blog/deploy-tfserving-kubernetes)çš„åšå®¢æ–‡ç« 

## ViTConfig

### `class transformers.ViTConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/configuration_vit.py#L35)

```py
( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 qkv_bias = True encoder_stride = 16 **kwargs )
```

å‚æ•°

+   `hidden_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` æˆ– `function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `image_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `num_channels` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 3) â€” è¾“å…¥é€šé“æ•°ã€‚

+   `qkv_bias` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åç½®ã€‚

+   `encoder_stride` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 16) â€” ç”¨äºåœ¨è§£ç å™¨å¤´éƒ¨å¢åŠ ç©ºé—´åˆ†è¾¨ç‡çš„å› å­ï¼Œç”¨äºé®è”½å›¾åƒå»ºæ¨¡ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– ViT æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº ViT [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import ViTConfig, ViTModel

>>> # Initializing a ViT vit-base-patch16-224 style configuration
>>> configuration = ViTConfig()

>>> # Initializing a model (with random weights) from the vit-base-patch16-224 style configuration
>>> model = ViTModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ViTFeatureExtractor

### `class transformers.ViTFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/feature_extraction_vit.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## ViTImageProcessor

### `class transformers.ViTImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/image_processing_vit.py#L41)

```py
( do_resize: bool = True size: Optional = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ï¼ˆé«˜åº¦ã€å®½åº¦ï¼‰å°ºå¯¸è°ƒæ•´ä¸ºæŒ‡å®šçš„ `(size["height"], size["width"])`ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_resize` å‚æ•°è¦†ç›–ã€‚

+   `size` (`dict`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `{"height" -- 224, "width": 224}`): è°ƒæ•´å¤§å°åè¾“å‡ºå›¾åƒçš„å°ºå¯¸ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `size` å‚æ•°è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `Resampling.BILINEAR`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¦†ç›–ã€‚

+   `do_rescale` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹ `rescale_factor` é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¦†ç›–ã€‚

+   `rescale_factor` (`int` æˆ– `float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ¯”ä¾‹å› å­ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¦†ç›–ã€‚

+   `do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_normalize` å‚æ•°è¦†ç›–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `IMAGENET_STANDARD_MEAN`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œè¦ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_mean` å‚æ•°è¦†ç›–ã€‚

+   `image_std` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `IMAGENET_STANDARD_STD`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œè¦ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ª ViT å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/image_processing_vit.py#L146)

```py
( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_rescale=False`ã€‚

+   `do_resize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.size`) â€” æ ¼å¼ä¸º `{"height": h, "width": w}` çš„å­—å…¸ï¼ŒæŒ‡å®šè°ƒæ•´å¤§å°åè¾“å‡ºå›¾åƒçš„å°ºå¯¸ã€‚

+   `resample` (`PILImageResampling` æ»¤æ³¢å™¨ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„ `PILImageResampling` æ»¤æ³¢å™¨ï¼Œä¾‹å¦‚ `PILImageResampling.BILINEAR`ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_rescale` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾åœ¨ [0 - 1] ä¹‹é—´ã€‚

+   `rescale_factor` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.rescale_factor`) â€” å¦‚æœ `do_rescale` è®¾ç½®ä¸º `True`ï¼Œåˆ™é‡æ–°ç¼©æ”¾å›¾åƒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚

+   `do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.image_mean`) â€” å¦‚æœ `do_normalize` è®¾ç½®ä¸º `True`ï¼Œè¦ä½¿ç”¨çš„å›¾åƒå‡å€¼ã€‚

+   `image_std` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.image_std`) â€” å¦‚æœ `do_normalize` è®¾ç½®ä¸º `True`ï¼Œè¦ä½¿ç”¨çš„å›¾åƒæ ‡å‡†å·®ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`: è¿”å›ä¸€ä¸ªç±»å‹ä¸º `tf.Tensor` çš„æ‰¹é‡ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`: è¿”å›ä¸€ä¸ªç±»å‹ä¸º `torch.Tensor` çš„æ‰¹é‡ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`: è¿”å›ä¸€ä¸ªç±»å‹ä¸º `np.ndarray` çš„æ‰¹é‡ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`: è¿”å›ä¸€ä¸ªç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹é‡ã€‚

+   `data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚

+   `input_data_format` (`ChannelDimension`æˆ–`str`, *å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   `"none"`æˆ–`ChannelDimension.NONE`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

PytorchHide Pytorchå†…å®¹

## ViTModel

### `class transformers.ViTModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_vit.py#L501)

```py
( config: ViTConfig add_pooling_layer: bool = True use_mask_token: bool = False )
```

å‚æ•°

+   `config`ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ViTæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_vit.py#L530)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `interpolate_pos_encoding` (`bool`, *å¯é€‰*) â€” æ˜¯å¦æ’å€¼é¢„è®­ç»ƒä½ç½®ç¼–ç ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `bool_masked_pos` (`torch.BoolTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_patches)`ï¼Œ*å¯é€‰*) â€” å¸ƒå°”æ©ç ä½ç½®ã€‚æŒ‡ç¤ºå“ªäº›è¡¥ä¸è¢«å±è”½ï¼ˆ1ï¼‰å“ªäº›æ²¡æœ‰ï¼ˆ0ï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œåœ¨é€šè¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åã€‚ä¾‹å¦‚ï¼Œå¯¹äºBERTç³»åˆ—æ¨¡å‹ï¼Œè¿™å°†è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œtanhæ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ª+æ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    æ¨¡å‹çš„æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ä½¿ç”¨çš„æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚

[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, ViTModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> model = ViTModel.from_pretrained("google/vit-base-patch16-224-in21k")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 197, 768]
```

## ViTForMaskedImageModeling

### `class transformers.ViTForMaskedImageModeling`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_vit.py#L615)

```py
( config: ViTConfig )
```

å‚æ•°

+   `config`ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ViTæ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰è§£ç å™¨ï¼Œç”¨äºé®ç½©å›¾åƒå»ºæ¨¡ï¼Œå¦‚[SimMIM](https://arxiv.org/abs/2111.09886)ä¸­æå‡ºçš„ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æˆ‘ä»¬çš„[ç¤ºä¾‹ç›®å½•](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining)ä¸­æä¾›äº†ä¸€ä¸ªè„šæœ¬ï¼Œç”¨äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šé¢„è®­ç»ƒæ­¤æ¨¡å‹ã€‚

æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_vit.py#L645)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.MaskedImageModelingOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `interpolate_pos_encoding` (`bool`, *å¯é€‰*) â€” æ˜¯å¦æ’å€¼é¢„è®­ç»ƒä½ç½®ç¼–ç ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `bool_masked_pos` (`torch.BoolTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_patches)`) â€” å¸ƒå°”æ©ç ä½ç½®ã€‚æŒ‡ç¤ºå“ªäº›è¡¥ä¸è¢«æ©ç›–ï¼ˆ1ï¼‰å“ªäº›ä¸è¢«æ©ç›–ï¼ˆ0ï¼‰ã€‚

è¿”å›

`transformers.modeling_outputs.MaskedImageModelingOutput` æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª `transformers.modeling_outputs.MaskedImageModelingOutput` æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰ï¼ŒåŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `bool_masked_pos` æ—¶è¿”å›) â€” é‡æ„æŸå¤±ã€‚

+   `reconstruction` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`) â€” é‡æ„/å®Œæˆçš„å›¾åƒã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æ—¶è¿”å›æˆ–

+   `when` `config.output_hidden_states=True`) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æ—¶è¿”å›æˆ–

+   `config.output_attentions=True):` å½¢çŠ¶ä¸º `(batch_size, num_heads, patch_size, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[ViTForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForMaskedImageModeling) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, ViTForMaskedImageModeling
>>> import torch
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> model = ViTForMaskedImageModeling.from_pretrained("google/vit-base-patch16-224-in21k")

>>> num_patches = (model.config.image_size // model.config.patch_size) ** 2
>>> pixel_values = image_processor(images=image, return_tensors="pt").pixel_values
>>> # create random boolean mask of shape (batch_size, num_patches)
>>> bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()

>>> outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
>>> loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction
>>> list(reconstructed_pixel_values.shape)
[1, 3, 224, 224]
```

## ViTForImageClassification

### `class transformers.ViTForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_vit.py#L741)

```py
( config: ViTConfig )
```

å‚æ•°

+   `config` ([ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ViT æ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨ [CLS] æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚

è¯·æ³¨æ„ï¼Œå¯ä»¥é€šè¿‡åœ¨æ¨¡å‹çš„å‰å‘ä¼ é€’ä¸­å°† `interpolate_pos_encoding` è®¾ç½®ä¸º `True`ï¼Œåœ¨æ¯”æ¨¡å‹è®­ç»ƒæ—¶æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒä¸Šå¯¹ ViT è¿›è¡Œå¾®è°ƒã€‚è¿™å°†å¯¹é¢„è®­ç»ƒçš„ä½ç½®åµŒå…¥è¿›è¡Œæ’å€¼ä»¥é€‚åº”æ›´é«˜åˆ†è¾¨ç‡ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_vit.py#L769)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)` çš„ `torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)` çš„ `torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `interpolate_pos_encoding`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ’å€¼é¢„è®­ç»ƒä½ç½®ç¼–ç ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º `(batch_size,)` çš„ `torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` ä¸­ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º `(1,)` çš„ `torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, config.num_labels)` çš„ `torch.FloatTensor`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰çš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, patch_size, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ã€‚

[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, ViTForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
>>> model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
Egyptian cat
```

TensorFlowéšè—TensorFlowå†…å®¹

## TFViTModel

### `class transformers.TFViTModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_tf_vit.py#L740)

```py
( config: ViTConfig *inputs add_pooling_layer = True **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰- æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ViTæ¨¡å‹å˜æ¢å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨åœ¨é¡¶éƒ¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF 2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–è€…

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å‘æ¨¡å‹å’Œå±‚ä¼ é€’è¾“å…¥æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥â€œåªéœ€å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras`Functional` APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   ä»…ä½¿ç”¨`pixel_values`ä½œä¸ºå•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(pixel_values)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([pixel_values, attention_mask])`æˆ–`model([pixel_values, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_tf_vit.py#L750)

```py
( pixel_values: TFModelInputType | None = None head_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None interpolate_pos_encoding: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]` ``Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`) - åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ - ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šä¸º`[0, 1]`ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨â€œæœªå±è”½â€,

    +   0è¡¨ç¤ºå¤´éƒ¨â€œå·²å±è”½â€ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `interpolate_pos_encoding` (`bool`, *å¯é€‰*) â€” æ˜¯å¦æ’å€¼é¢„è®­ç»ƒä½ç½®ç¼–ç ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚

+   `training` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`Falseâ€œ) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

è¿”å›

[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)æˆ–`tf.Tensor`å…ƒç»„

ä¸€ä¸ª[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œç»è¿‡çº¿æ€§å±‚å’ŒTanhæ¿€æ´»å‡½æ•°è¿›ä¸€æ­¥å¤„ç†ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

    è¯¥è¾“å‡ºé€šå¸¸*ä¸æ˜¯*è¾“å…¥è¯­ä¹‰å†…å®¹çš„è‰¯å¥½æ‘˜è¦ï¼Œé€šå¸¸æœ€å¥½å¯¹æ•´ä¸ªè¾“å…¥åºåˆ—çš„éšè—çŠ¶æ€è¿›è¡Œå¹³å‡æˆ–æ± åŒ–ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[TFViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.TFViTModel)å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, TFViTModel
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> model = TFViTModel.from_pretrained("google/vit-base-patch16-224-in21k")

>>> inputs = image_processor(image, return_tensors="tf")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 197, 768]
```

## TFViTForImageClassification

### `class transformers.TFViTForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_tf_vit.py#L819)

```py
( config: ViTConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ViTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªå›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚

è¯·æ³¨æ„ï¼Œå¯ä»¥é€šè¿‡åœ¨æ¨¡å‹çš„å‰å‘ä¼ é€’ä¸­å°†`interpolate_pos_encoding`è®¾ç½®ä¸º`True`æ¥åœ¨æ¯”å…¶è®­ç»ƒæ—¶æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒä¸Šå¾®è°ƒViTã€‚è¿™å°†å¯¹é¢„è®­ç»ƒçš„ä½ç½®åµŒå…¥è¿›è¡Œæ’å€¼åˆ°æ›´é«˜åˆ†è¾¨ç‡ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF 2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKerasæ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œä½ åº”è¯¥å¯ä»¥â€œè½»æ¾ä½¿ç”¨â€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ç„¶è€Œï¼Œå¦‚æœä½ æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨Keras`Functional`APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   ä¸€ä¸ªåªæœ‰`pixel_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(pixel_values)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([pixel_values, attention_mask])`æˆ–`model([pixel_values, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œä½ ä¸éœ€è¦æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºä½ å¯ä»¥åƒå¯¹å¾…å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_tf_vit.py#L849)

```py
( pixel_values: TFModelInputType | None = None head_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None interpolate_pos_encoding: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`np.ndarray`ã€`tf.Tensor`ã€`List[tf.Tensor]`ã€`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `interpolate_pos_encoding` (`bool`, *å¯é€‰*) â€” æ˜¯å¦æ’å€¼é¢„è®­ç»ƒçš„ä½ç½®ç¼–ç ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `Falseâ€œ) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels` (`tf.Tensor` æˆ–å½¢çŠ¶ä¸º `(batch_size,)` çš„ `np.ndarray`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` ä¸­ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput) æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput) æˆ–ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, )`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[TFViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.TFViTForImageClassification) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, TFViTForImageClassification
>>> import tensorflow as tf
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
>>> model = TFViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

>>> inputs = image_processor(image, return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = int(tf.math.argmax(logits, axis=-1))
>>> print(model.config.id2label[predicted_label])
Egyptian cat
```

JAXHide JAX å†…å®¹

## FlaxVitModel

### `class transformers.FlaxViTModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit/modeling_flax_vit.py#L552)

```py
( config: ViTConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°
