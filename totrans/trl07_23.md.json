["```py\npip install trl[peft]\npip install bitsandbytes loralib\npip install git+https://github.com/huggingface/transformers.git@main\n#optional: wandb\npip install wandb\n```", "```py\nfrom peft import LoraConfig\nfrom trl import AutoModelForCausalLMWithValueHead\n\nmodel_id = \"edbeeching/gpt-neo-125M-imdb\"\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    model_id, \n    peft_config=lora_config,\n)\n```", "```py\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    load_in_8bit=True,\n    peft_config=lora_config,\n)\n```", "```py\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    peft_config=lora_config,\n    load_in_4bit=True,\n)\n```", "```py\naccelerate config # will prompt you to define the training configuration\naccelerate launch scripts/gpt2-sentiment_peft.py # launches training\n```", "```py\nfrom peft import LoraConfig\n...\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    peft_config=lora_config,\n)\n```", "```py\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    peft_config=lora_config,\n    load_in_8bit=True,\n)\n```", "```py\npretrained_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name, \n    peft_config=lora_config,\n    load_in_4bit=True,\n)\n```", "```py\npython PATH_TO_SCRIPT\n```", "```py\npython examples/scripts/sft.py --model_name meta-llama/Llama-2-7b-hf --dataset_name timdettmers/openassistant-guanaco --load_in_4bit --use_peft --batch_size 4 --gradient_accumulation_steps 2\n```"]