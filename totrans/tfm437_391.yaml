- en: Custom Layers and Utilities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义层和实用工具
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/internal/modeling_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/modeling_utils)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/modeling_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/modeling_utils)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This page lists all the custom layers used by the library, as well as the utility
    functions it provides for modeling.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此页面列出了库中使用的所有自定义层，以及为建模提供的实用函数。
- en: Most of those are only useful if you are studying the code of the models in
    the library.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，这些只有在研究库中模型的代码时才有用。
- en: Pytorch custom modules
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pytorch自定义模块
- en: '### `class transformers.Conv1D`'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Conv1D`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L82)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L82)'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`nf` (`int`) — The number of output features.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nf`（`int`）- 输出特征的数量。'
- en: '`nx` (`int`) — The number of input features.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nx`（`int`）- 输入特征的数量。'
- en: 1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also
    used in GPT-2).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由Radford等人为OpenAI GPT（也用于GPT-2）定义的一维卷积层。
- en: Basically works like a linear layer but the weights are transposed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上像一个线性层，但权重是转置的。
- en: '### `class transformers.modeling_utils.PoolerStartLogits`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.PoolerStartLogits`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4546)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4546)'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)）-
    模型使用的配置，将用于获取模型的`hidden_size`。'
- en: Compute SQuAD start logits from sequence hidden states.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '从序列隐藏状态计算SQuAD开始logits。 '
- en: '#### `forward`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4559)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4559)'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — The final hidden states of the model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（形状为`(batch_size, seq_len, hidden_size)`的`torch.FloatTensor`）-
    模型的最终隐藏状态。'
- en: '`p_mask` (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — Mask for tokens at invalid position, such as query and special symbols (PAD,
    SEP, CLS). 1.0 means token should be masked.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p_mask`（形状为`(batch_size, seq_len)`的`torch.FloatTensor`，*可选*）- 用于无效位置的标记的掩码，例如查询和特殊符号（PAD，SEP，CLS）。1.0表示应该屏蔽标记。'
- en: Returns
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`'
- en: The start logits for SQuAD.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD的开始logits。
- en: '### `class transformers.modeling_utils.PoolerEndLogits`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.PoolerEndLogits`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4584)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4584)'
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model and the `layer_norm_eps` to use.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)）-
    模型使用的配置，将用于获取模型的`hidden_size`和要使用的`layer_norm_eps`。'
- en: Compute SQuAD end logits from sequence hidden states.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从序列隐藏状态计算SQuAD结束logits。
- en: '#### `forward`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4601)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4601)'
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — The final hidden states of the model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（形状为`(batch_size, seq_len, hidden_size)`的`torch.FloatTensor`）-
    模型的最终隐藏状态。'
- en: '`start_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`,
    *optional*) — The hidden states of the first tokens for the labeled span.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_states`（形状为`(batch_size, seq_len, hidden_size)`的`torch.FloatTensor`，*可选*）-
    标记跨度的第一个标记的隐藏状态。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — The position of the first token for the labeled span.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）- 标记跨度的第一个标记的位置。'
- en: '`p_mask` (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — Mask for tokens at invalid position, such as query and special symbols (PAD,
    SEP, CLS). 1.0 means token should be masked.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p_mask`（形状为`(batch_size, seq_len)`的`torch.FloatTensor`，*可选*）- 用于无效位置的标记的掩码，例如查询和特殊符号（PAD，SEP，CLS）。1.0表示应该屏蔽标记。'
- en: Returns
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`'
- en: The end logits for SQuAD.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD的结束logits。
- en: One of `start_states` or `start_positions` should be not `None`. If both are
    set, `start_positions` overrides `start_states`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`start_states`或`start_positions`中的一个应该不是`None`。如果两者都设置了，`start_positions`会覆盖`start_states`。'
- en: '### `class transformers.modeling_utils.PoolerAnswerClass`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.PoolerAnswerClass`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4653)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4653)'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)）-
    模型使用的配置，将用于获取模型的`hidden_size`。'
- en: Compute SQuAD 2.0 answer class from classification and start tokens hidden states.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从分类和开始标记的隐藏状态计算SQuAD 2.0答案类。
- en: '#### `forward`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4668)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4668)'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — The final hidden states of the model.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（形状为`(batch_size, seq_len, hidden_size)`的`torch.FloatTensor`）-
    模型的最终隐藏状态。'
- en: '`start_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`,
    *optional*) — The hidden states of the first tokens for the labeled span.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`,
    *optional*) — 标记范围内第一个标记的隐藏状态。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — The position of the first token for the labeled span.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — 标记范围内第一个标记的位置。'
- en: '`cls_index` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Position
    of the CLS token for each sentence in the batch. If `None`, takes the last token.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_index` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 每个句子中CLS标记的位置。如果为`None`，则取最后一个标记。'
- en: Returns
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`'
- en: The SQuAD 2.0 answer class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD 2.0 答案类。
- en: One of `start_states` or `start_positions` should be not `None`. If both are
    set, `start_positions` overrides `start_states`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`start_states`或`start_positions`中的一个应该不是`None`。如果两者都设置了，`start_positions`会覆盖`start_states`。'
- en: '### `class transformers.modeling_utils.SquadHeadOutput`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.SquadHeadOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4718)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4718)'
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions`
    and `end_positions` are provided) — Classification loss as the sum of start token,
    end token (and is_impossible if provided) classification losses.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 如果提供了`start_positions`和`end_positions`则返回)
    — 分类损失，作为起始标记、结束标记（如果提供了is_impossible则包括）分类损失的总和。'
- en: '`start_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, 如果未提供`start_positions`或`end_positions`则返回) — 前 config.start_n_top
    个起始标记可能性的对数概率（beam-search）。'
- en: '`start_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, 如果未提供`start_positions`或`end_positions`则返回) — 前 config.start_n_top
    个起始标记可能性的索引（beam-search）。'
- en: '`end_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, 如果未提供`start_positions`或`end_positions`则返回) —
    前`config.start_n_top * config.end_n_top`个结束标记可能性的对数概率（beam-search）。'
- en: '`end_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, 如果未提供`start_positions`或`end_positions`则返回) —
    前`config.start_n_top * config.end_n_top`个结束标记可能性的索引（beam-search）。'
- en: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, 如果未提供`start_positions`或`end_positions`则返回)
    — 答案的`is_impossible`标签的对数概率。'
- en: Base class for outputs of question answering models using a [SQuADHead](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SQuADHead).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用[SQuADHead](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SQuADHead)的问答模型输出的基类。
- en: '### `class transformers.modeling_utils.SQuADHead`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.SQuADHead`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4749)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4749)'
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model and the `layer_norm_eps` to use.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — 模型使用的配置，将用于获取模型的`hidden_size`和要使用的`layer_norm_eps`。'
- en: A SQuAD head inspired by XLNet.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 受XLNet启发的SQuAD头部。
- en: '#### `forward`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4768)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4768)'
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — Final hidden states of the model on the sequence tokens.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — 模型在序列标记上的最终隐藏状态。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Positions of the first token for the labeled span.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — 标记范围内第一个标记的位置。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Positions of the last token for the labeled span.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    标记范围内最后一个标记的位置。'
- en: '`cls_index` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Position
    of the CLS token for each sentence in the batch. If `None`, takes the last token.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_index` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 每个句子中CLS标记的位置。如果为`None`，则取最后一个标记。'
- en: '`is_impossible` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Whether the question has a possible answer in the paragraph or not.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_impossible` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    问题在段落中是否有可能回答。'
- en: '`p_mask` (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — Mask for tokens at invalid position, such as query and special symbols (PAD,
    SEP, CLS). 1.0 means token should be masked.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p_mask` (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — 用于无效位置的标记的掩码，例如查询和特殊符号（PAD、SEP、CLS）。1.0 表示应该屏蔽标记。'
- en: '`return_dict` (`bool`, *optional*, defaults to `False`) — Whether or not to
    return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为`False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.configuration_utils.PretrainedConfig'>`)
    and inputs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class
    'transformers.configuration_utils.PretrainedConfig'>`）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions`
    and `end_positions` are provided) — Classification loss as the sum of start token,
    end token (and is_impossible if provided) classification losses.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions`
    and `end_positions` are provided) — 分类损失，作为开始令牌、结束令牌（如果提供）的分类损失之和。'
- en: '`start_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — 前端令牌可能性的前`config.start_n_top`个的对数概率（波束搜索）。'
- en: '`start_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — 前`config.start_n_top`个开始令牌可能性的索引（波束搜索）。'
- en: '`end_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — 顶部`config.start_n_top * config.end_n_top`结束令牌可能性的对数概率（波束搜索）。'
- en: '`end_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — 顶部`config.start_n_top * config.end_n_top`结束令牌可能性的索引（波束搜索）。'
- en: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — 答案的`is_impossible`标签的对数概率。'
- en: '### `class transformers.modeling_utils.SequenceSummary`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.SequenceSummary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4866)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4866)'
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model. Relevant arguments in the config class of the
    model are (refer to the actual config class of your model for the default values
    it uses):'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — 模型使用的配置。模型的配置类中的相关参数为（请参考您的模型的实际配置类以查看其使用的默认值）：'
- en: '`summary_type` (`str`) — The method to use to make this summary. Accepted values
    are:'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type` (`str`) — 用于制作摘要的方法。接受的值为：'
- en: '`"last"` — Take the last token hidden state (like XLNet)'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"last"` — 获取最后一个令牌的隐藏状态（类似于XLNet）'
- en: '`"first"` — Take the first token hidden state (like Bert)'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"first"` — 获取第一个令牌的隐藏状态（类似于Bert）'
- en: '`"mean"` — Take the mean of all tokens hidden states'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mean"` — 取所有令牌隐藏状态的平均值'
- en: '`"cls_index"` — Supply a Tensor of classification token position (GPT/GPT-2)'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cls_index"` — 提供分类令牌位置的张量（GPT/GPT-2）'
- en: '`"attn"` — Not implemented now, use multi-head attention'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"attn"` — 目前未实现，使用多头注意力'
- en: '`summary_use_proj` (`bool`) — Add a projection after the vector extraction.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_use_proj` (`bool`) — 在向量提取后添加一个投影。'
- en: '`summary_proj_to_labels` (`bool`) — If `True`, the projection outputs to `config.num_labels`
    classes (otherwise to `config.hidden_size`).'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_proj_to_labels` (`bool`) — 如果为`True`，则投影输出到`config.num_labels`类（否则为`config.hidden_size`）。'
- en: '`summary_activation` (`Optional[str]`) — Set to `"tanh"` to add a tanh activation
    to the output, another string or `None` will add no activation.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_activation` (`Optional[str]`) — 设置为`"tanh"`以在输出中添加tanh激活，另一个字符串或`None`将不添加激活。'
- en: '`summary_first_dropout` (`float`) — Optional dropout probability before the
    projection and activation.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_first_dropout` (`float`) — 投影和激活之前的可选丢弃概率。'
- en: '`summary_last_dropout` (`float`)— Optional dropout probability after the projection
    and activation.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_last_dropout` (`float`)— 投影和激活后的可选丢弃概率。'
- en: Compute a single vector summary of a sequence hidden states.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 计算序列隐藏状态的单个向量摘要。
- en: '#### `forward`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4921)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4921)'
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`)
    — The hidden states of the last layer.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（形状为`[batch_size, seq_len, hidden_size]`的`torch.FloatTensor`）-
    最后一层的隐藏状态。'
- en: '`cls_index` (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]`
    where … are optional leading dimensions of `hidden_states`, *optional*) — Used
    if `summary_type == "cls_index"` and takes the last token of the sequence as classification
    token.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_index`（形状为`[batch_size]`或`[batch_size, ...]`的`torch.LongTensor`，其中…是`hidden_states`的可选前导维度，*可选*）-
    如果`summary_type == "cls_index"`，则用于将序列的最后一个标记作为分类标记。'
- en: Returns
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`'
- en: The summary of the sequence hidden states.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 序列隐藏状态的摘要。
- en: Compute a single vector summary of a sequence hidden states.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 计算序列隐藏状态的单个向量摘要。
- en: PyTorch Helper Functions
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch辅助函数
- en: '#### `transformers.apply_chunking_to_forward`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.apply_chunking_to_forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L164)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L164)'
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`forward_fn` (`Callable[..., torch.Tensor]`) — The forward function of the
    model.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward_fn`（`Callable[..., torch.Tensor]`）- 模型的前向函数。'
- en: '`chunk_size` (`int`) — The chunk size of a chunked tensor: `num_chunks = len(input_tensors[0])
    / chunk_size`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_size`（`int`）- 分块张量的块大小：`num_chunks = len(input_tensors[0]) / chunk_size`。'
- en: '`chunk_dim` (`int`) — The dimension over which the `input_tensors` should be
    chunked.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_dim`（`int`）- 应该对`input_tensors`进行分块的维度。'
- en: '`input_tensors` (`Tuple[torch.Tensor]`) — The input tensors of `forward_fn`
    which will be chunked'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_tensors`（`Tuple[torch.Tensor]`）- `forward_fn`的输入张量，将被分块'
- en: Returns
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: A tensor with the same shape as the `forward_fn` would have given if applied`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个与应用`forward_fn`后给出的`forward_fn`相同形状的张量。
- en: This function chunks the `input_tensors` into smaller input tensor parts of
    size `chunk_size` over the dimension `chunk_dim`. It then applies a layer `forward_fn`
    to each chunk independently to save memory.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将`input_tensors`分块为大小为`chunk_size`的较小输入张量部分，沿着维度`chunk_dim`。然后独立地对每个块应用层`forward_fn`以节省内存。
- en: If the `forward_fn` is independent across the `chunk_dim` this function will
    yield the same result as directly applying `forward_fn` to `input_tensors`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`forward_fn`在`chunk_dim`上独立，这个函数将产生与直接将`forward_fn`应用于`input_tensors`相同的结果。
- en: 'Examples:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#### `transformers.pytorch_utils.find_pruneable_heads_and_indices`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.pytorch_utils.find_pruneable_heads_and_indices`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L239)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L239)'
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`heads` (`List[int]`) — List of the indices of heads to prune.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heads`（`List[int]`）- 要修剪的头的索引列表。'
- en: '`n_heads` (`int`) — The number of heads in the model.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_heads`（`int`）- 模型中头的数量。'
- en: '`head_size` (`int`) — The size of each head.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_size`（`int`）- 每个头的大小。'
- en: '`already_pruned_heads` (`Set[int]`) — A set of already pruned heads.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_pruned_heads`（`Set[int]`）- 一个已经修剪的头的集合。'
- en: Returns
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Tuple[Set[int], torch.LongTensor]`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple[Set[int], torch.LongTensor]`'
- en: A tuple with the indices of heads to prune taking `already_pruned_heads` into
    account and the indices of rows/columns to keep in the layer weight.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个元组，其中包括要修剪的头的索引（考虑`already_pruned_heads`）和要在层权重中保留的行/列的索引。
- en: Finds the heads and their indices taking `already_pruned_heads` into account.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 找到头和它们的索引，考虑`already_pruned_heads`。
- en: '#### `transformers.prune_layer`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.prune_layer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L140)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L140)'
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`layer` (`Union[torch.nn.Linear, Conv1D]`) — The layer to prune.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer`（`Union[torch.nn.Linear, Conv1D]`）- 要修剪的层。'
- en: '`index` (`torch.LongTensor`) — The indices to keep in the layer.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`（`torch.LongTensor`）- 要在层中保留的索引。'
- en: '`dim` (`int`, *optional*) — The dimension on which to keep the indices.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`（`int`，*可选*）- 保留索引的维度。'
- en: Returns
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.nn.Linear` or [Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Linear`或[Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
- en: The pruned layer as a new layer with `requires_grad=True`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将修剪后的层作为一个新的层，`requires_grad=True`。
- en: Prune a Conv1D or linear layer to keep only entries in index.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪一个Conv1D或线性层，只保留索引中的条目。
- en: Used to remove heads.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 用于移除头。
- en: '#### `transformers.pytorch_utils.prune_conv1d_layer`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.pytorch_utils.prune_conv1d_layer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L107)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L107)'
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`layer` ([Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D))
    — The layer to prune.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer`（[Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)）-
    要修剪的层。'
- en: '`index` (`torch.LongTensor`) — The indices to keep in the layer.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`（`torch.LongTensor`）- 要在层中保留的索引。'
- en: '`dim` (`int`, *optional*, defaults to 1) — The dimension on which to keep the
    indices.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`（`int`，*可选*，默认为1）- 保留索引的维度。'
- en: Returns
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
- en: The pruned layer as a new layer with `requires_grad=True`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 将修剪后的层作为一个新的层，`requires_grad=True`。
- en: Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear
    layer (see e.g. BERT) but the weights are transposed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪一个Conv1D层，只保留索引中的条目。Conv1D作为一个线性层工作（参见例如BERT），但权重是转置的。
- en: Used to remove heads.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 用于移除头。
- en: '#### `transformers.pytorch_utils.prune_linear_layer`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.pytorch_utils.prune_linear_layer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L48)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L48)'
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`layer` (`torch.nn.Linear`) — The layer to prune.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer`（`torch.nn.Linear`）- 要修剪的层。'
- en: '`index` (`torch.LongTensor`) — The indices to keep in the layer.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`（`torch.LongTensor`）- 要在层中保留的索引。'
- en: '`dim` (`int`, *optional*, defaults to 0) — The dimension on which to keep the
    indices.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`（`int`，*可选*，默认为0）- 保留索引的维度。'
- en: Returns
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.nn.Linear`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Linear`'
- en: The pruned layer as a new layer with `requires_grad=True`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪的层作为一个新层，`requires_grad=True`。
- en: Prune a linear layer to keep only entries in index.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪线性层以仅保留索引中的条目。
- en: Used to remove heads.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 用于移除头部。
- en: TensorFlow custom layers
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow自定义层
- en: '### `class transformers.modeling_tf_utils.TFConv1D`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.modeling_tf_utils.TFConv1D`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3203)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3203)'
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`nf` (`int`) — The number of output features.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nf`（`int`）— 输出特征的数量。'
- en: '`nx` (`int`) — The number of input features.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nx`（`int`）— 输入特征的数量。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation to use to initialize the weights.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range`（`float`，*可选*，默认为0.02）— 用于初始化权重的标准差。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the `__init__` of `tf.keras.layers.Layer`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）— 传递给`tf.keras.layers.Layer`的`__init__`的额外关键字参数。'
- en: 1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also
    used in GPT-2).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由Radford等人为OpenAI GPT定义的一维卷积层（也用于GPT-2）。
- en: Basically works like a linear layer but the weights are transposed.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上像一个线性层，但权重是转置的。
- en: '### `class transformers.TFSequenceSummary`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.TFSequenceSummary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3350)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3350)'
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model. Relevant arguments in the config class of the
    model are (refer to the actual config class of your model for the default values
    it uses):'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)）—
    模型使用的配置。模型的配置类中的相关参数为（请参考您的模型的实际配置类以获取其使用的默认值）：'
- en: '`summary_type` (`str`) — The method to use to make this summary. Accepted values
    are:'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type`（`str`）— 用于生成此摘要的方法。接受的值为：'
- en: '`"last"` — Take the last token hidden state (like XLNet)'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"last"` — 获取最后一个标记的隐藏状态（类似于XLNet）'
- en: '`"first"` — Take the first token hidden state (like Bert)'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"first"` — 获取第一个标记的隐藏状态（类似于Bert）'
- en: '`"mean"` — Take the mean of all tokens hidden states'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mean"` — 获取所有标记的隐藏状态的平均值'
- en: '`"cls_index"` — Supply a Tensor of classification token position (GPT/GPT-2)'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cls_index"` — 提供一个分类标记位置的张量（GPT/GPT-2）'
- en: '`"attn"` — Not implemented now, use multi-head attention'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “attn” — 目前未实现，使用多头注意力
- en: '`summary_use_proj` (`bool`) — Add a projection after the vector extraction.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_use_proj`（`bool`）— 在向量提取后添加投影。'
- en: '`summary_proj_to_labels` (`bool`) — If `True`, the projection outputs to `config.num_labels`
    classes (otherwise to `config.hidden_size`).'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_proj_to_labels`（`bool`）— 如果为`True`，则投影输出到`config.num_labels`类（否则到`config.hidden_size`）。'
- en: '`summary_activation` (`Optional[str]`) — Set to `"tanh"` to add a tanh activation
    to the output, another string or `None` will add no activation.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_activation`（`Optional[str]`）— 设置为`"tanh"`以在输出中添加tanh激活，另一个字符串或`None`将不添加激活。'
- en: '`summary_first_dropout` (`float`) — Optional dropout probability before the
    projection and activation.'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_first_dropout`（`float`）— 投影和激活之前的可选丢弃概率。'
- en: '`summary_last_dropout` (`float`)— Optional dropout probability after the projection
    and activation.'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_last_dropout`（`float`）— 投影和激活之后的可选丢弃概率。'
- en: '`initializer_range` (`float`, defaults to 0.02) — The standard deviation to
    use to initialize the weights.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range`（`float`，默认为0.02）— 用于初始化权重的标准差。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the `__init__` of `tf.keras.layers.Layer`.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）— 传递给`tf.keras.layers.Layer`的`__init__`的额外关键字参数。'
- en: Compute a single vector summary of a sequence hidden states.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 计算序列隐藏状态的单个向量摘要。
- en: TensorFlow loss functions
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow损失函数
- en: '### `class transformers.modeling_tf_utils.TFCausalLanguageModelingLoss`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.modeling_tf_utils.TFCausalLanguageModelingLoss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L191)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L191)'
- en: '[PRE20]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Loss function suitable for causal language modeling (CLM), that is, the task
    of guessing the next token.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于因果语言建模（CLM）的损失函数，即猜测下一个标记的任务。
- en: Any label of -100 will be ignored (along with the corresponding logits) in the
    loss computation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 任何标签为-100的将在损失计算中被忽略（以及相应的对数）。
- en: '### `class transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L310)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L310)'
- en: '[PRE21]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Loss function suitable for masked language modeling (MLM), that is, the task
    of guessing the masked tokens.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于掩码语言建模（MLM）的损失函数，即猜测掩码标记的任务。
- en: Any label of -100 will be ignored (along with the corresponding logits) in the
    loss computation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 任何标签为-100的将在损失计算中被忽略（以及相应的对数）。
- en: '### `class transformers.modeling_tf_utils.TFMultipleChoiceLoss`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.modeling_tf_utils.TFMultipleChoiceLoss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L300)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L300)'
- en: '[PRE22]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Loss function suitable for multiple choice tasks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于多项选择任务的损失函数。
- en: '### `class transformers.modeling_tf_utils.TFQuestionAnsweringLoss`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.modeling_tf_utils.TFQuestionAnsweringLoss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L222)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L222)'
- en: '[PRE23]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Loss function suitable for question answering.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于问答的损失函数。
- en: '### `class transformers.modeling_tf_utils.TFSequenceClassificationLoss`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.modeling_tf_utils.TFSequenceClassificationLoss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L281)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L281)'
- en: '[PRE24]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Loss function suitable for sequence classification.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于序列分类的损失函数。
- en: '### `class transformers.modeling_tf_utils.TFTokenClassificationLoss`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_tf_utils.TFTokenClassificationLoss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L237)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L237)'
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Loss function suitable for token classification.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于标记分类的损失函数。
- en: Any label of -100 will be ignored (along with the corresponding logits) in the
    loss computation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 任何标签为-100的将在损失计算中被忽略（以及相应的logits）。
- en: TensorFlow Helper Functions
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow辅助函数
- en: '#### `transformers.modeling_tf_utils.get_initializer`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.modeling_tf_utils.get_initializer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3475)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3475)'
- en: '[PRE26]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`initializer_range` (*float*, defaults to 0.02) — Standard deviation of the
    initializer range.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range`（*float*，默认为0.02）- 初始化器范围的标准差。'
- en: Returns
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.keras.initializers.TruncatedNormal`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.initializers.TruncatedNormal`'
- en: The truncated normal initializer.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 截断正态初始化器。
- en: Creates a `tf.keras.initializers.TruncatedNormal` with the given range.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定范围创建一个`tf.keras.initializers.TruncatedNormal`。
- en: '#### `transformers.modeling_tf_utils.keras_serializable`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.modeling_tf_utils.keras_serializable`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L126)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L126)'
- en: '[PRE27]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`cls` (a `tf.keras.layers.Layers subclass`) — Typically a `TF.MainLayer` class
    in this project, in general must accept a `config` argument to its initializer.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls`（一个`tf.keras.layers.Layers子类`）- 通常在这个项目中是一个`TF.MainLayer`类，一般必须接受一个`config`参数作为其初始化器。'
- en: Decorate a Keras Layer class to support Keras serialization.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 装饰一个Keras Layer类以支持Keras序列化。
- en: 'This is done by:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过以下方式完成的：
- en: Adding a `transformers_config` dict to the Keras config dictionary in `get_config`
    (called by Keras at serialization time.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`get_config`中的Keras配置字典中添加一个`transformers_config`字典（由Keras在序列化时调用）。
- en: Wrapping `__init__` to accept that `transformers_config` dict (passed by Keras
    at deserialization time) and convert it to a config object for the actual layer
    initializer.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包装`__init__`以接受`transformers_config`字典（由Keras在反序列化时传递）并将其转换为实际层初始化器的配置对象。
- en: Registering the class as a custom object in Keras (if the Tensorflow version
    supports this), so that it does not need to be supplied in `custom_objects` in
    the call to `tf.keras.models.load_model`.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中将类注册为自定义对象（如果Tensorflow版本支持），这样在调用`tf.keras.models.load_model`时就不需要在`custom_objects`中提供它。
- en: '#### `transformers.shape_list`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.shape_list`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tf_utils.py#L26)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tf_utils.py#L26)'
- en: '[PRE28]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tensor` (`tf.Tensor` or `np.ndarray`) — The tensor we want the shape of.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`（`tf.Tensor`或`np.ndarray`）- 我们想要形状的张量。'
- en: Returns
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The shape of the tensor as a list.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 作为列表的张量形状。
- en: Deal with dynamic shape in tensorflow cleanly.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在tensorflow中处理动态形状。
