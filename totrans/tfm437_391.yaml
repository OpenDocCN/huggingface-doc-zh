- en: Custom Layers and Utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/modeling_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/modeling_utils)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/29.86236db8.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: This page lists all the custom layers used by the library, as well as the utility
    functions it provides for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Most of those are only useful if you are studying the code of the models in
    the library.
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch custom modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Conv1D'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: ( nf nx )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**nf** (`int`) — The number of output features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nx** (`int`) — The number of input features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also
    used in GPT-2).
  prefs: []
  type: TYPE_NORMAL
- en: Basically works like a linear layer but the weights are transposed.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_utils.PoolerStartLogits'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4546)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: PretrainedConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute SQuAD start logits from sequence hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4559)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_states: FloatTensor p_mask: Optional = None ) → `torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_states** (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — The final hidden states of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p_mask** (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — Mask for tokens at invalid position, such as query and special symbols (PAD,
    SEP, CLS). 1.0 means token should be masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: The start logits for SQuAD.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_utils.PoolerEndLogits'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4584)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: PretrainedConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model and the `layer_norm_eps` to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute SQuAD end logits from sequence hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4601)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_states: FloatTensor start_states: Optional = None start_positions:
    Optional = None p_mask: Optional = None ) → `torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_states** (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — The final hidden states of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_states** (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`,
    *optional*) — The hidden states of the first tokens for the labeled span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — The position of the first token for the labeled span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p_mask** (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — Mask for tokens at invalid position, such as query and special symbols (PAD,
    SEP, CLS). 1.0 means token should be masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: The end logits for SQuAD.
  prefs: []
  type: TYPE_NORMAL
- en: One of `start_states` or `start_positions` should be not `None`. If both are
    set, `start_positions` overrides `start_states`.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_utils.PoolerAnswerClass'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4653)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute SQuAD 2.0 answer class from classification and start tokens hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4668)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_states: FloatTensor start_states: Optional = None start_positions:
    Optional = None cls_index: Optional = None ) → `torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_states** (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — The final hidden states of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_states** (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`,
    *optional*) — The hidden states of the first tokens for the labeled span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — The position of the first token for the labeled span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_index** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Position
    of the CLS token for each sentence in the batch. If `None`, takes the last token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: The SQuAD 2.0 answer class.
  prefs: []
  type: TYPE_NORMAL
- en: One of `start_states` or `start_positions` should be not `None`. If both are
    set, `start_positions` overrides `start_states`.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_utils.SquadHeadOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4718)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None start_top_log_probs: Optional = None start_top_index:
    Optional = None end_top_log_probs: Optional = None end_top_index: Optional = None
    cls_logits: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both
    `start_positions` and `end_positions` are provided) — Classification loss as the
    sum of start token, end token (and is_impossible if provided) classification losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_logits** (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of question answering models using a [SQuADHead](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SQuADHead).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_utils.SQuADHead'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4749)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model, will be used to grab the `hidden_size` of the
    model and the `layer_norm_eps` to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A SQuAD head inspired by XLNet.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4768)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_states: FloatTensor start_positions: Optional = None end_positions:
    Optional = None cls_index: Optional = None is_impossible: Optional = None p_mask:
    Optional = None return_dict: bool = False ) → [transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_states** (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`)
    — Final hidden states of the model on the sequence tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Positions of the first token for the labeled span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Positions of the last token for the labeled span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_index** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Position
    of the CLS token for each sentence in the batch. If `None`, takes the last token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_impossible** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Whether the question has a possible answer in the paragraph or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p_mask** (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*)
    — Mask for tokens at invalid position, such as query and special symbols (PAD,
    SEP, CLS). 1.0 means token should be masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `False`) — Whether or not
    to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_utils.SquadHeadOutput](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.configuration_utils.PretrainedConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both
    `start_positions` and `end_positions` are provided) — Classification loss as the
    sum of start token, end token (and is_impossible if provided) classification losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_logits** (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### class transformers.modeling_utils.SequenceSummary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4866)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: PretrainedConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model. Relevant arguments in the config class of the
    model are (refer to the actual config class of your model for the default values
    it uses):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_type** (`str`) — The method to use to make this summary. Accepted
    values are:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"last"` — Take the last token hidden state (like XLNet)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"first"` — Take the first token hidden state (like Bert)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"mean"` — Take the mean of all tokens hidden states'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"cls_index"` — Supply a Tensor of classification token position (GPT/GPT-2)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"attn"` — Not implemented now, use multi-head attention'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_use_proj** (`bool`) — Add a projection after the vector extraction.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_proj_to_labels** (`bool`) — If `True`, the projection outputs to
    `config.num_labels` classes (otherwise to `config.hidden_size`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_activation** (`Optional[str]`) — Set to `"tanh"` to add a tanh activation
    to the output, another string or `None` will add no activation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_first_dropout** (`float`) — Optional dropout probability before the
    projection and activation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_last_dropout** (`float`)— Optional dropout probability after the
    projection and activation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute a single vector summary of a sequence hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4921)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_states: FloatTensor cls_index: Optional = None ) → `torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_states** (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`)
    — The hidden states of the last layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_index** (`torch.LongTensor` of shape `[batch_size]` or `[batch_size,
    ...]` where … are optional leading dimensions of `hidden_states`, *optional*)
    — Used if `summary_type == "cls_index"` and takes the last token of the sequence
    as classification token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: The summary of the sequence hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Compute a single vector summary of a sequence hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Helper Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### transformers.apply_chunking_to_forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: '( forward_fn: Callable chunk_size: int chunk_dim: int *input_tensors ) → `torch.Tensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**forward_fn** (`Callable[..., torch.Tensor]`) — The forward function of the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chunk_size** (`int`) — The chunk size of a chunked tensor: `num_chunks =
    len(input_tensors[0]) / chunk_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chunk_dim** (`int`) — The dimension over which the `input_tensors` should
    be chunked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_tensors** (`Tuple[torch.Tensor]`) — The input tensors of `forward_fn`
    which will be chunked'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.Tensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A tensor with the same shape as the `forward_fn` would have given if applied`.
  prefs: []
  type: TYPE_NORMAL
- en: This function chunks the `input_tensors` into smaller input tensor parts of
    size `chunk_size` over the dimension `chunk_dim`. It then applies a layer `forward_fn`
    to each chunk independently to save memory.
  prefs: []
  type: TYPE_NORMAL
- en: If the `forward_fn` is independent across the `chunk_dim` this function will
    yield the same result as directly applying `forward_fn` to `input_tensors`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### transformers.pytorch_utils.find_pruneable_heads_and_indices'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L239)'
  prefs: []
  type: TYPE_NORMAL
- en: '( heads: List n_heads: int head_size: int already_pruned_heads: Set ) → `Tuple[Set[int],
    torch.LongTensor]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**heads** (`List[int]`) — List of the indices of heads to prune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_heads** (`int`) — The number of heads in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_size** (`int`) — The size of each head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_pruned_heads** (`Set[int]`) — A set of already pruned heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Tuple[Set[int], torch.LongTensor]`'
  prefs: []
  type: TYPE_NORMAL
- en: A tuple with the indices of heads to prune taking `already_pruned_heads` into
    account and the indices of rows/columns to keep in the layer weight.
  prefs: []
  type: TYPE_NORMAL
- en: Finds the heads and their indices taking `already_pruned_heads` into account.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.prune_layer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L140)'
  prefs: []
  type: TYPE_NORMAL
- en: '( layer: Union index: LongTensor dim: Optional = None ) → `torch.nn.Linear`
    or [Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**layer** (`Union[torch.nn.Linear, Conv1D]`) — The layer to prune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**index** (`torch.LongTensor`) — The indices to keep in the layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim** (`int`, *optional*) — The dimension on which to keep the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Linear` or [Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
  prefs: []
  type: TYPE_NORMAL
- en: The pruned layer as a new layer with `requires_grad=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Prune a Conv1D or linear layer to keep only entries in index.
  prefs: []
  type: TYPE_NORMAL
- en: Used to remove heads.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.pytorch_utils.prune_conv1d_layer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L107)'
  prefs: []
  type: TYPE_NORMAL
- en: '( layer: Conv1D index: LongTensor dim: int = 1 ) → [Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**layer** ([Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D))
    — The layer to prune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**index** (`torch.LongTensor`) — The indices to keep in the layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim** (`int`, *optional*, defaults to 1) — The dimension on which to keep
    the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Conv1D](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.Conv1D)'
  prefs: []
  type: TYPE_NORMAL
- en: The pruned layer as a new layer with `requires_grad=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear
    layer (see e.g. BERT) but the weights are transposed.
  prefs: []
  type: TYPE_NORMAL
- en: Used to remove heads.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.pytorch_utils.prune_linear_layer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pytorch_utils.py#L48)'
  prefs: []
  type: TYPE_NORMAL
- en: '( layer: Linear index: LongTensor dim: int = 0 ) → `torch.nn.Linear`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**layer** (`torch.nn.Linear`) — The layer to prune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**index** (`torch.LongTensor`) — The indices to keep in the layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim** (`int`, *optional*, defaults to 0) — The dimension on which to keep
    the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Linear`'
  prefs: []
  type: TYPE_NORMAL
- en: The pruned layer as a new layer with `requires_grad=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Prune a linear layer to keep only entries in index.
  prefs: []
  type: TYPE_NORMAL
- en: Used to remove heads.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow custom layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFConv1D'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3203)'
  prefs: []
  type: TYPE_NORMAL
- en: ( nf nx initializer_range = 0.02 **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**nf** (`int`) — The number of output features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nx** (`int`) — The number of input features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) — The standard
    deviation to use to initialize the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the `__init__` of `tf.keras.layers.Layer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also
    used in GPT-2).
  prefs: []
  type: TYPE_NORMAL
- en: Basically works like a linear layer but the weights are transposed.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.TFSequenceSummary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3350)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: PretrainedConfig initializer_range: float = 0.02 **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The config used by the model. Relevant arguments in the config class of the
    model are (refer to the actual config class of your model for the default values
    it uses):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_type** (`str`) — The method to use to make this summary. Accepted
    values are:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"last"` — Take the last token hidden state (like XLNet)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"first"` — Take the first token hidden state (like Bert)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"mean"` — Take the mean of all tokens hidden states'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"cls_index"` — Supply a Tensor of classification token position (GPT/GPT-2)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"attn"` — Not implemented now, use multi-head attention'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_use_proj** (`bool`) — Add a projection after the vector extraction.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_proj_to_labels** (`bool`) — If `True`, the projection outputs to
    `config.num_labels` classes (otherwise to `config.hidden_size`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_activation** (`Optional[str]`) — Set to `"tanh"` to add a tanh activation
    to the output, another string or `None` will add no activation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_first_dropout** (`float`) — Optional dropout probability before the
    projection and activation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_last_dropout** (`float`)— Optional dropout probability after the
    projection and activation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, defaults to 0.02) — The standard deviation
    to use to initialize the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the `__init__` of `tf.keras.layers.Layer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute a single vector summary of a sequence hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow loss functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFCausalLanguageModelingLoss'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L191)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Loss function suitable for causal language modeling (CLM), that is, the task
    of guessing the next token.
  prefs: []
  type: TYPE_NORMAL
- en: Any label of -100 will be ignored (along with the corresponding logits) in the
    loss computation.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L310)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Loss function suitable for masked language modeling (MLM), that is, the task
    of guessing the masked tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Any label of -100 will be ignored (along with the corresponding logits) in the
    loss computation.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFMultipleChoiceLoss'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L300)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Loss function suitable for multiple choice tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFQuestionAnsweringLoss'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L222)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Loss function suitable for question answering.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFSequenceClassificationLoss'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L281)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Loss function suitable for sequence classification.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.modeling_tf_utils.TFTokenClassificationLoss'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L237)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Loss function suitable for token classification.
  prefs: []
  type: TYPE_NORMAL
- en: Any label of -100 will be ignored (along with the corresponding logits) in the
    loss computation.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Helper Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### transformers.modeling_tf_utils.get_initializer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3475)'
  prefs: []
  type: TYPE_NORMAL
- en: '( initializer_range: float = 0.02 ) → `tf.keras.initializers.TruncatedNormal`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**initializer_range** (*float*, defaults to 0.02) — Standard deviation of the
    initializer range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras.initializers.TruncatedNormal`'
  prefs: []
  type: TYPE_NORMAL
- en: The truncated normal initializer.
  prefs: []
  type: TYPE_NORMAL
- en: Creates a `tf.keras.initializers.TruncatedNormal` with the given range.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.modeling_tf_utils.keras_serializable'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L126)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**cls** (a `tf.keras.layers.Layers subclass`) — Typically a `TF.MainLayer`
    class in this project, in general must accept a `config` argument to its initializer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decorate a Keras Layer class to support Keras serialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a `transformers_config` dict to the Keras config dictionary in `get_config`
    (called by Keras at serialization time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrapping `__init__` to accept that `transformers_config` dict (passed by Keras
    at deserialization time) and convert it to a config object for the actual layer
    initializer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Registering the class as a custom object in Keras (if the Tensorflow version
    supports this), so that it does not need to be supplied in `custom_objects` in
    the call to `tf.keras.models.load_model`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### transformers.shape_list'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tf_utils.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tensor: Union ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** (`tf.Tensor` or `np.ndarray`) — The tensor we want the shape of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: The shape of the tensor as a list.
  prefs: []
  type: TYPE_NORMAL
- en: Deal with dynamic shape in tensorflow cleanly.
  prefs: []
  type: TYPE_NORMAL
