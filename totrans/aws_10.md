# åœ¨AWS Inferentiaä¸Šä½¿ç”¨llama-2-13Båˆ›å»ºæ‚¨è‡ªå·±çš„èŠå¤©æœºå™¨äºº

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/optimum-neuron/tutorials/llama2-13b-chatbot](https://huggingface.co/docs/optimum-neuron/tutorials/llama2-13b-chatbot)

*è¿™é‡Œæœ‰ä¸€ä¸ªç¬”è®°æœ¬ç‰ˆæœ¬çš„æ•™ç¨‹[é“¾æ¥](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-generation/llama2-13b-chatbot.ipynb)*ã€‚

æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨AWS inferentiaä¸Šå¯¼å‡ºã€éƒ¨ç½²å’Œè¿è¡Œ**LLama-2 13B**èŠå¤©æ¨¡å‹ã€‚

æ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š

+   å°†Llama-2æ¨¡å‹å¯¼å‡ºä¸ºNeuronæ ¼å¼ï¼Œ

+   å°†å¯¼å‡ºçš„æ¨¡å‹æ¨é€åˆ°Hugging Face Hubï¼Œ

+   éƒ¨ç½²æ¨¡å‹å¹¶åœ¨èŠå¤©åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å®ƒã€‚

æ³¨æ„ï¼šæœ¬æ•™ç¨‹æ˜¯åœ¨ä¸€ä¸ªinf2.48xlargeçš„AWS EC2å®ä¾‹ä¸Šåˆ›å»ºçš„ã€‚

## 1\. å°†Llama 2æ¨¡å‹å¯¼å‡ºåˆ°Neuron

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨éé—¨æ§[NousResearch/Llama-2-13b-chat-hf](https://huggingface.co/NousResearch/Llama-2-13b-chat-hf)æ¨¡å‹ï¼Œå®ƒåœ¨åŠŸèƒ½ä¸Šç­‰åŒäºåŸå§‹çš„[meta-llama/Llama-2-13b-chat-hf](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)æ¨¡å‹ã€‚

è¯¥æ¨¡å‹æ˜¯**Llama 2**ç³»åˆ—æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå·²ç»è°ƒæ•´ä¸ºè¯†åˆ«*ç”¨æˆ·*å’Œ*åŠ©æ‰‹*ä¹‹é—´çš„èŠå¤©äº’åŠ¨ï¼ˆç¨åä¼šè¯¦ç»†ä»‹ç»ï¼‰ã€‚

å¦‚[æœ€ä½³ç¥ç»å…ƒæ–‡æ¡£](https://huggingface.co/docs/optimum-neuron/guides/export_model#why-compile-to-neuron-model)ä¸­æ‰€è§£é‡Šçš„ï¼Œæ¨¡å‹éœ€è¦åœ¨Neuronè®¾å¤‡ä¸Šè¿è¡Œä¹‹å‰è¢«ç¼–è¯‘å’Œå¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼ã€‚

å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— **optimum-neuron** æä¾›äº†ä¸€ä¸ªéå¸¸ç®€å•çš„ [API](https://huggingface.co/docs/optimum-neuron/guides/models#configuring-the-export-of-a-generative-model) æ¥å°†æ ‡å‡† ğŸ¤— [transformers æ¨¡å‹](https://huggingface.co/docs/transformers/index) å¯¼å‡ºä¸ºNeuronæ ¼å¼ã€‚

åœ¨å¯¼å‡ºæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†æŒ‡å®šä¸¤ç»„å‚æ•°ï¼š

+   ä½¿ç”¨*compiler_args*ï¼Œæˆ‘ä»¬æŒ‡å®šè¦éƒ¨ç½²æ¨¡å‹çš„æ ¸å¿ƒæ•°ï¼ˆæ¯ä¸ªç¥ç»å…ƒè®¾å¤‡æœ‰ä¸¤ä¸ªæ ¸å¿ƒï¼‰ï¼Œä»¥åŠç²¾åº¦ï¼ˆè¿™é‡Œæ˜¯*float16*ï¼‰ï¼Œ

+   ä½¿ç”¨*input_shapes*ï¼Œæˆ‘ä»¬è®¾ç½®æ¨¡å‹çš„é™æ€è¾“å…¥å’Œè¾“å‡ºç»´åº¦ã€‚æ‰€æœ‰æ¨¡å‹ç¼–è¯‘å™¨éƒ½éœ€è¦é™æ€å½¢çŠ¶ï¼Œç¥ç»å…ƒä¹Ÿä¸ä¾‹å¤–ã€‚è¯·æ³¨æ„ï¼Œ*sequence_length*ä¸ä»…é™åˆ¶äº†è¾“å…¥ä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œè¿˜é™åˆ¶äº†Key/Valueç¼“å­˜çš„é•¿åº¦ï¼Œå› æ­¤ä¹Ÿé™åˆ¶äº†è¾“å‡ºé•¿åº¦ã€‚

æ ¹æ®æ‚¨é€‰æ‹©çš„å‚æ•°å’Œinferentiaä¸»æœºï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°ä¸€ä¸ªå¤šå°æ—¶çš„æ—¶é—´ã€‚

ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åœ¨Hugging Face hubä¸Šæ‰˜ç®¡äº†è¯¥æ¨¡å‹çš„é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼Œå› æ­¤æ‚¨å¯ä»¥è·³è¿‡å¯¼å‡ºå¹¶ç«‹å³åœ¨ç¬¬2èŠ‚ä¸­å¼€å§‹ä½¿ç”¨è¯¥æ¨¡å‹ã€‚

```py
from optimum.neuron import NeuronModelForCausalLM

compiler_args = {"num_cores": 24, "auto_cast_type": 'fp16'}
input_shapes = {"batch_size": 1, "sequence_length": 2048}
model = NeuronModelForCausalLM.from_pretrained(
        "NousResearch/Llama-2-13b-chat-hf",
        export=True,
        **compiler_args,
        **input_shapes)
```

è¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚

å¹¸è¿çš„æ˜¯ï¼Œæ‚¨åªéœ€è¦åšä¸€æ¬¡è¿™ä¸ªæ“ä½œï¼Œå› ä¸ºæ‚¨å¯ä»¥ä¿å­˜æ‚¨çš„æ¨¡å‹å¹¶ç¨åé‡æ–°åŠ è½½å®ƒã€‚

```py
model.save_pretrained("llama-2-13b-chat-neuron")
```

æ›´å¥½çš„æ˜¯ï¼Œæ‚¨å¯ä»¥å°†å…¶æ¨é€åˆ°[Hugging Face hub](https://huggingface.co/models)ã€‚

ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦ç™»å½•åˆ°[HuggingFaceå¸æˆ·](https://huggingface.co/join)ã€‚

åœ¨ç»ˆç«¯ä¸­ï¼Œåªéœ€è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼Œå¹¶åœ¨è¯·æ±‚æ—¶ç²˜è´´æ‚¨çš„Hugging Faceä»¤ç‰Œï¼š

```py
huggingface-cli login
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†ä¸Šä¼ åˆ°æ‚¨çš„å¸æˆ·ï¼ˆç»„ç»‡ç­‰äºæ‚¨çš„ç”¨æˆ·åï¼‰ã€‚

å¦‚æœæ‚¨æƒ³å°†æ¨¡å‹ä¸Šä¼ åˆ°ç‰¹å®šçš„[Hugging Faceç»„ç»‡](https://huggingface.co/docs/hub/organizations)ï¼Œè¯·éšæ„ç¼–è¾‘ä¸‹é¢çš„ä»£ç ã€‚

```py
from huggingface_hub import whoami

org = whoami()['name']

repo_id = f"{org}/llama-2-13b-chat-neuron"

model.push_to_hub("llama-2-13b-chat-neuron", repository_id=repo_id)
```

### å…³äºå¯¼å‡ºå‚æ•°çš„æ›´å¤šè¯´æ˜ã€‚

åŠ è½½æ¨¡å‹æ‰€éœ€çš„æœ€å°å†…å­˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š

```py
   memory = bytes per parameter * number of parameters
```

**Llama 2 13B**æ¨¡å‹ä½¿ç”¨*float16*æƒé‡ï¼ˆå­˜å‚¨åœ¨2å­—èŠ‚ä¸­ï¼‰ï¼Œæœ‰130äº¿å‚æ•°ï¼Œè¿™æ„å‘³ç€è‡³å°‘éœ€è¦2 * 13Bæˆ–~26GBçš„å†…å­˜æ¥å­˜å‚¨å…¶æƒé‡ã€‚

æ¯ä¸ªNeuronCoreæœ‰16GBçš„å†…å­˜ï¼Œè¿™æ„å‘³ç€26GBçš„æ¨¡å‹æ— æ³•æ”¾å…¥å•ä¸ªNeuronCoreä¸­ã€‚

å®é™…ä¸Šï¼Œç”±äºç¼“å­˜æ³¨æ„åŠ›å±‚æŠ•å½±ï¼ˆKVç¼“å­˜ï¼‰ï¼Œæ‰€éœ€çš„æ€»ç©ºé—´è¿œè¿œå¤§äºå‚æ•°æ•°é‡ã€‚è¿™ç§ç¼“å­˜æœºåˆ¶ä¼šéšç€åºåˆ—é•¿åº¦å’Œæ‰¹é‡å¤§å°çº¿æ€§å¢é•¿å†…å­˜åˆ†é…ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†*batch_size*è®¾ç½®ä¸º1ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªèƒ½å¹¶è¡Œå¤„ç†ä¸€ä¸ªè¾“å…¥æç¤ºã€‚æˆ‘ä»¬å°†*sequence_length*è®¾ç½®ä¸º2048ï¼Œè¿™å¯¹åº”äºæ¨¡å‹æœ€å¤§å®¹é‡çš„ä¸€åŠï¼ˆ4096ï¼‰ã€‚

è¯„ä¼°KVç¼“å­˜å¤§å°çš„å…¬å¼æ›´å¤æ‚ï¼Œå› ä¸ºå®ƒè¿˜å–å†³äºä¸æ¨¡å‹æ¶æ„ç›¸å…³çš„å‚æ•°ï¼Œæ¯”å¦‚åµŒå…¥çš„å®½åº¦å’Œè§£ç å™¨å—çš„æ•°é‡ã€‚

æ€»çš„æ¥è¯´ï¼Œä¸ºäº†ä½¿éå¸¸å¤§çš„è¯­è¨€æ¨¡å‹é€‚åº”ï¼Œå¼ é‡å¹¶è¡Œæ€§è¢«ç”¨æ¥åœ¨å¤šä¸ªNeuronCoresä¹‹é—´åˆ†å‰²æƒé‡ã€æ•°æ®å’Œè®¡ç®—ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯æ¯ä¸ªæ ¸å¿ƒä¸Šçš„å†…å­˜ä¸èƒ½è¶…è¿‡16GBã€‚

è¯·æ³¨æ„ï¼Œå°†æ ¸å¿ƒæ•°é‡å¢åŠ åˆ°æœ€ä½è¦æ±‚ä¹‹ä¸Šå‡ ä¹æ€»æ˜¯ä¼šå¯¼è‡´æ¨¡å‹è¿è¡Œæ›´å¿«ã€‚å¢åŠ å¼ é‡å¹¶è¡Œåº¦ä¼šæé«˜å†…å­˜å¸¦å®½ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚

ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨å®ä¾‹ä¸Šæ‰€æœ‰å¯ç”¨çš„æ ¸å¿ƒã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨*inf2.48xlarge*çš„æ‰€æœ‰24ä¸ªæ ¸å¿ƒï¼Œä½†å¦‚æœæ‚¨ä½¿ç”¨*inf2.24xlarge*å®ä¾‹ï¼Œåˆ™åº”å°†å…¶æ›´æ”¹ä¸º12ä¸ªæ ¸å¿ƒã€‚

## 2. åœ¨AWS Inferentia2ä¸Šä½¿ç”¨Llama 2ç”Ÿæˆæ–‡æœ¬

ä¸€æ—¦æ‚¨çš„æ¨¡å‹è¢«å¯¼å‡ºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨transformersåº“ç”Ÿæˆæ–‡æœ¬ï¼Œå¦‚åœ¨[è¿™ç¯‡æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ä¸­è¯¦ç»†æè¿°çš„ã€‚

å¦‚æœæ‚¨æŒ‰ç…§å»ºè®®è·³è¿‡äº†ç¬¬ä¸€éƒ¨åˆ†ï¼Œä¸ç”¨æ‹…å¿ƒï¼šæˆ‘ä»¬å°†ä½¿ç”¨hubä¸Šå·²ç»å­˜åœ¨çš„é¢„ç¼–è¯‘æ¨¡å‹ã€‚

```py
from optimum.neuron import NeuronModelForCausalLM

try:
    model
except NameError:
    # Edit this to use another base model
    model = NeuronModelForCausalLM.from_pretrained('aws-neuron/Llama-2-13b-chat-hf-neuron-latency')
```

æˆ‘ä»¬å°†éœ€è¦ä¸€ä¸ª*Llama 2*åˆ†è¯å™¨å°†æç¤ºå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ–‡æœ¬æ ‡è®°ã€‚

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-13b-chat-hf")
```

æ”¯æŒä»¥ä¸‹ç”Ÿæˆç­–ç•¥ï¼š

+   è´ªå©ªæœç´¢ï¼Œ

+   ä½¿ç”¨top-kå’Œtop-pï¼ˆå¸¦æœ‰æ¸©åº¦ï¼‰çš„å¤šé¡¹å¼é‡‡æ ·ã€‚

å¤§å¤šæ•°å¯¹æ•°é¢„å¤„ç†/è¿‡æ»¤ï¼ˆå¦‚é‡å¤æƒ©ç½šï¼‰éƒ½å—æ”¯æŒã€‚

```py
inputs = tokenizer("What is deep-learning ?", return_tensors="pt")
outputs = model.generate(**inputs,
                         max_new_tokens=128,
                         do_sample=True,
                         temperature=0.9,
                         top_k=50,
                         top_p=0.9)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

## 3. åœ¨AWS Inferentia2ä¸Šä½¿ç”¨llamaåˆ›å»ºèŠå¤©åº”ç”¨

æˆ‘ä»¬ç‰¹æ„é€‰æ‹©äº†**Llama 2**èŠå¤©å˜ä½“ï¼Œä»¥å±•ç¤ºå¯¼å‡ºæ¨¡å‹åœ¨ç¼–ç ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ æ—¶çš„å‡ºè‰²è¡Œä¸ºã€‚

æ¨¡å‹æœŸæœ›æç¤ºæŒ‰ç…§ç‰¹å®šæ¨¡æ¿æ ¼å¼åŒ–ï¼Œè¯¥æ¨¡æ¿å¯¹åº”äº*ç”¨æˆ·*è§’è‰²å’Œ*åŠ©æ‰‹*è§’è‰²ä¹‹é—´çš„äº’åŠ¨ã€‚

æ¯ä¸ªèŠå¤©æ¨¡å‹éƒ½æœ‰è‡ªå·±çš„çº¦å®šæ¥ç¼–ç è¿™äº›å†…å®¹ï¼Œæˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­ä¸ä¼šè¯¦ç»†ä»‹ç»ï¼Œå› ä¸ºæˆ‘ä»¬å°†ç›´æ¥ä½¿ç”¨ä¸æˆ‘ä»¬çš„æ¨¡å‹å¯¹åº”çš„[Hugging FaceèŠå¤©æ¨¡æ¿](https://huggingface.co/blog/chat-templates)ã€‚

ä¸‹é¢çš„å®ç”¨å‡½æ•°å°†æŠŠç”¨æˆ·å’Œæ¨¡å‹ä¹‹é—´çš„äº¤æµåˆ—è¡¨è½¬æ¢ä¸ºæ ¼å¼è‰¯å¥½çš„èŠå¤©æç¤ºã€‚

```py
def format_chat_prompt(message, history, max_tokens):
    """ Convert a history of messages to a chat prompt

    Args:
        message(str): the new user message.
        history (List[str]): the list of user messages and assistant responses.
        max_tokens (int): the maximum number of input tokens accepted by the model.

    Returns:
        a `str` prompt.
    """
    chat = []
    # Convert all messages in history to chat interactions
    for interaction in history:
        chat.append({"role": "user", "content" : interaction[0]})
        chat.append({"role": "assistant", "content" : interaction[1]})
    # Add the new message
    chat.append({"role": "user", "content" : message})
    # Generate the prompt, verifying that we don't go beyond the maximum number of tokens
    for i in range(0, len(chat), 2):
        # Generate candidate prompt with the last n-i entries
        prompt = tokenizer.apply_chat_template(chat[i:], tokenize=False)
        # Tokenize to check if we're over the limit
        tokens = tokenizer(prompt)
        if len(tokens.input_ids) <= max_tokens:
            # We're good, stop here
            return prompt
    # We shall never reach this line
    raise SystemError
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ„å»ºä¸€ä¸ªç®€å•çš„èŠå¤©åº”ç”¨ç¨‹åºã€‚

æˆ‘ä»¬ç®€å•åœ°å°†ç”¨æˆ·å’ŒåŠ©æ‰‹ä¹‹é—´çš„äº’åŠ¨å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæˆ‘ä»¬ç”¨è¿™ä¸ªåˆ—è¡¨æ¥ç”Ÿæˆè¾“å…¥æç¤ºã€‚

```py
history = []
max_tokens = 1024

def chat(message, history, max_tokens):
    prompt = format_chat_prompt(message, history, max_tokens)
    # Uncomment the line below to see what the formatted prompt looks like
    #print(prompt)
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs,
                             max_length=2048,
                             do_sample=True,
                             temperature=0.9,
                             top_k=50,
                             repetition_penalty=1.2)
    # Do not include the input tokens
    outputs = outputs[0, inputs.input_ids.size(-1):]
    response = tokenizer.decode(outputs, skip_special_tokens=True)
    history.append([message, response])
    return response
```

è¦æµ‹è¯•èŠå¤©åº”ç”¨ç¨‹åºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æç¤ºåºåˆ—ï¼š

```py
print(chat("My favorite color is blue. My favorite fruit is strawberry.", history, max_tokens))
print(chat("Name a fruit that is on my favorite colour.", history, max_tokens))
print(chat("What is the colour of my favorite fruit ?", history, max_tokens))
```

<è­¦å‘Š>

å°½ç®¡éå¸¸å¼ºå¤§ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ—¶å¯èƒ½ä¼šâ€œå¹»è§‰â€ã€‚æˆ‘ä»¬ç§°ç”Ÿæˆçš„å†…å®¹ä¸ºâ€œå¹»è§‰â€ï¼Œå¦‚æœå†…å®¹ä¸ä¸»é¢˜æ— å…³æˆ–æ˜¯è™šæ„çš„ï¼Œä½†æ¨¡å‹å‘ˆç°å‡ºæ¥çš„å¥½åƒæ˜¯å‡†ç¡®çš„ã€‚è¿™æ˜¯LLMçš„ä¸€ä¸ªç¼ºé™·ï¼Œä¸æ˜¯åœ¨Trainium / Inferentiaä¸Šä½¿ç”¨å®ƒä»¬çš„å‰¯ä½œç”¨ã€‚

</è­¦å‘Š>
