["```py\nfrom torch import nn\n\nclass MLP(nn.Module):\n    def __init__(self, num_units_hidden=2000):\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(20, num_units_hidden),\n            nn.ReLU(),\n            nn.Linear(num_units_hidden, num_units_hidden),\n            nn.ReLU(),\n            nn.Linear(num_units_hidden, 2),\n            nn.LogSoftmax(dim=-1),\n        )\n\n    def forward(self, X):\n        return self.seq(X)\n```", "```py\nprint([(n, type(m)) for n, m in MLP().named_modules()])\n```", "```py\n[('', __main__.MLP),\n ('seq', torch.nn.modules.container.Sequential),\n ('seq.0', torch.nn.modules.linear.Linear),\n ('seq.1', torch.nn.modules.activation.ReLU),\n ('seq.2', torch.nn.modules.linear.Linear),\n ('seq.3', torch.nn.modules.activation.ReLU),\n ('seq.4', torch.nn.modules.linear.Linear),\n ('seq.5', torch.nn.modules.activation.LogSoftmax)]\n```", "```py\nfrom peft import LoraConfig\n\nconfig = LoraConfig(\n    target_modules=[\"seq.0\", \"seq.2\"],\n    modules_to_save=[\"seq.4\"],\n)\n```", "```py\nfrom peft import get_peft_model\n\nmodel = MLP()\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922\n```", "```py\npython -m pip install -U timm\n```", "```py\nimport timm\n\nnum_classes = ...\nmodel_id = \"timm/poolformer_m36.sail_in1k\"\nmodel = timm.create_model(model_id, pretrained=True, num_classes=num_classes)\n```", "```py\nprint([(n, type(m)) for n, m in model.named_modules()])\n```", "```py\n[('', timm.models.metaformer.MetaFormer),\n ('stem', timm.models.metaformer.Stem),\n ('stem.conv', torch.nn.modules.conv.Conv2d),\n ('stem.norm', torch.nn.modules.linear.Identity),\n ('stages', torch.nn.modules.container.Sequential),\n ('stages.0', timm.models.metaformer.MetaFormerStage),\n ('stages.0.downsample', torch.nn.modules.linear.Identity),\n ('stages.0.blocks', torch.nn.modules.container.Sequential),\n ('stages.0.blocks.0', timm.models.metaformer.MetaFormerBlock),\n ('stages.0.blocks.0.norm1', timm.layers.norm.GroupNorm1),\n ('stages.0.blocks.0.token_mixer', timm.models.metaformer.Pooling),\n ('stages.0.blocks.0.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),\n ('stages.0.blocks.0.drop_path1', torch.nn.modules.linear.Identity),\n ('stages.0.blocks.0.layer_scale1', timm.models.metaformer.Scale),\n ('stages.0.blocks.0.res_scale1', torch.nn.modules.linear.Identity),\n ('stages.0.blocks.0.norm2', timm.layers.norm.GroupNorm1),\n ('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),\n ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv.Conv2d),\n ('stages.0.blocks.0.mlp.act', torch.nn.modules.activation.GELU),\n ('stages.0.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),\n ('stages.0.blocks.0.mlp.norm', torch.nn.modules.linear.Identity),\n ('stages.0.blocks.0.mlp.fc2', torch.nn.modules.conv.Conv2d),\n ('stages.0.blocks.0.mlp.drop2', torch.nn.modules.dropout.Dropout),\n ('stages.0.blocks.0.drop_path2', torch.nn.modules.linear.Identity),\n ('stages.0.blocks.0.layer_scale2', timm.models.metaformer.Scale),\n ('stages.0.blocks.0.res_scale2', torch.nn.modules.linear.Identity),\n ('stages.0.blocks.1', timm.models.metaformer.MetaFormerBlock),\n ('stages.0.blocks.1.norm1', timm.layers.norm.GroupNorm1),\n ('stages.0.blocks.1.token_mixer', timm.models.metaformer.Pooling),\n ('stages.0.blocks.1.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),\n ...\n ('head.global_pool.flatten', torch.nn.modules.linear.Identity),\n ('head.norm', timm.layers.norm.LayerNorm2d),\n ('head.flatten', torch.nn.modules.flatten.Flatten),\n ('head.drop', torch.nn.modules.linear.Identity),\n ('head.fc', torch.nn.modules.linear.Linear)]\n ]\n```", "```py\nconfig = LoraConfig(target_modules=r\".*\\.mlp\\.fc\\d\", modules_to_save=[\"head.fc\"])\n```", "```py\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n# prints trainable params: 1,064,454 || all params: 56,467,974 || trainable%: 1.88505789139876\n```", "```py\nfrom peft import LoraConfig, get_peft_model\n\nmy_mistral_model = ...\nconfig = LoraConfig(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    ...,  # other LoRA arguments\n)\npeft_model = get_peft_model(my_mistral_model, config)\n```", "```py\npeft_model.print_trainable_parameters()\n```", "```py\nprint(peft_model.targeted_module_names)\n```"]