- en: CodeGen
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CodeGen
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/codegen](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/codegen)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/codegen](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/codegen)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474)
    by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio
    Savarese, and Caiming Xiong.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CodeGen模型在[Erik Nijkamp、Bo Pang、Hiroaki Hayashi、Lifu Tu、Huan Wang、Yingbo Zhou、Silvio
    Savarese和Caiming Xiong的《程序合成的对话范式》](https://arxiv.org/abs/2203.13474)中提出。
- en: CodeGen is an autoregressive language model for program synthesis trained sequentially
    on [The Pile](https://pile.eleuther.ai/), BigQuery, and BigPython.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: CodeGen是一个自回归语言模型，用于在[The Pile](https://pile.eleuther.ai/)、BigQuery和BigPython上顺序训练程序合成。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Program synthesis strives to generate a computer program as a solution to
    a given problem specification. We propose a conversational program synthesis approach
    via large language models, which addresses the challenges of searching over a
    vast program space and user intent specification faced in prior approaches. Our
    new approach casts the process of writing a specification and program as a multi-turn
    conversation between a user and a system. It treats program synthesis as a sequence
    prediction problem, in which the specification is expressed in natural language
    and the desired program is conditionally sampled. We train a family of large language
    models, called CodeGen, on natural language and programming language data. With
    weak supervision in the data and the scaling up of data size and model size, conversational
    capacities emerge from the simple autoregressive language modeling. To study the
    model behavior on conversational program synthesis, we develop a multi-turn programming
    benchmark (MTPB), where solving each problem requires multi-step synthesis via
    multi-turn conversation between the user and the model. Our findings show the
    emergence of conversational capabilities and the effectiveness of the proposed
    conversational program synthesis paradigm. In addition, our model CodeGen (with
    up to 16B parameters trained on TPU-v4) outperforms OpenAI’s Codex on the HumanEval
    benchmark. We make the training library JaxFormer including checkpoints available
    as open source contribution: [this https URL](https://github.com/salesforce/codegen).*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*程序合成旨在生成计算机程序作为给定问题规范的解决方案。我们提出了一种通过大型语言模型进行对话式程序合成的方法，该方法解决了在先前方法中面临的在广泛的程序空间和用户意图规范中搜索的挑战。我们的新方法将编写规范和程序的过程视为用户和系统之间的多轮对话。它将程序合成视为一个序列预测问题，其中规范用自然语言表达，所需程序是有条件地抽样的。我们在自然语言和编程语言数据上训练了一系列大型语言模型，称为CodeGen。通过数据中的弱监督和数据规模和模型规模的扩大，对话能力从简单的自回归语言建模中出现。为了研究对话式程序合成的模型行为，我们开发了一个多轮编程基准（MTPB），在这个基准中，解决每个问题都需要通过用户和模型之间的多轮对话进行多步合成。我们的研究结果显示了对话能力的出现以及所提出的对话式程序合成范式的有效性。此外，我们的CodeGen模型（在TPU-v4上训练的参数高达16B）在HumanEval基准测试中优于OpenAI的Codex。我们将包括检查点在内的训练库JaxFormer作为开源贡献提供：[此链接](https://github.com/salesforce/codegen)。*'
- en: This model was contributed by [Hiroaki Hayashi](https://huggingface.co/rooa).
    The original code can be found [here](https://github.com/salesforce/codegen).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[Hiroaki Hayashi](https://huggingface.co/rooa)贡献。原始代码可以在[这里](https://github.com/salesforce/codegen)找到。
- en: Checkpoint Naming
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查点命名
- en: CodeGen model [checkpoints](https://huggingface.co/models?other=codegen) are
    available on different pre-training data with variable sizes.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CodeGen模型的[检查点](https://huggingface.co/models?other=codegen)可在不同的预训练数据上以不同大小的变量大小获得。
- en: 'The format is: `Salesforce/codegen-{size}-{data}`, where'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式为：`Salesforce/codegen-{size}-{data}`，其中
- en: '`size`: `350M`, `2B`, `6B`, `16B`'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size`: `350M`、`2B`、`6B`、`16B`'
- en: '`data`:'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`:'
- en: '`nl`: Pre-trained on the Pile'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nl`: 在The Pile上预训练'
- en: '`multi`: Initialized with `nl`, then further pre-trained on multiple programming
    languages data'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multi`: 初始化为`nl`，然后在多种编程语言数据上进一步预训练'
- en: '`mono`: Initialized with `multi`, then further pre-trained on Python data'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mono`: 初始化为`multi`，然后在Python数据上进一步预训练'
- en: For example, `Salesforce/codegen-350M-mono` offers a 350 million-parameter checkpoint
    pre-trained sequentially on the Pile, multiple programming languages, and Python.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，`Salesforce/codegen-350M-mono`提供了一个在The Pile、多种编程语言和Python上顺序预训练的3.5亿参数检查点。
- en: Usage example
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: CodeGenConfig
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CodeGenConfig
- en: '### `class transformers.CodeGenConfig`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CodeGenConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/configuration_codegen.py#L44)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/configuration_codegen.py#L44)'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 50400) — Vocabulary size of the
    CodeGen model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [CodeGenModel](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenModel).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为50400）—CodeGen模型的词汇量。定义了在调用[CodeGenModel](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenModel)时可以表示的不同标记的数量。'
- en: '`n_positions` (`int`, *optional*, defaults to 2048) — The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_positions`（`int`，*可选*，默认为2048）—此模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`n_ctx` (`int`, *optional*, defaults to 2048) — This attribute is used in `CodeGenModel.__init__`
    without any real effect.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_ctx`（`int`，*可选*，默认为2048）—此属性在`CodeGenModel.__init__`中使用，没有实际效果。'
- en: '`n_embd` (`int`, *optional*, defaults to 4096) — Dimensionality of the embeddings
    and hidden states.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_embd`（`int`，*可选*，默认为4096）—嵌入和隐藏状态的维度。'
- en: '`n_layer` (`int`, *optional*, defaults to 28) — Number of hidden layers in
    the Transformer encoder.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_head` (`int`, *optional*, defaults to 16) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rotary_dim` (`int`, *optional*, defaults to 64) — Number of dimensions in
    the embedding that Rotary Position Embedding is applied to.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_inner` (`int`, *optional*) — Dimensionality of the inner feed-forward layers.
    `None` will set it to 4 times n_embd'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu_new"`) — Activation
    function, to be selected in the list `["relu", "silu", "gelu", "tanh", "gelu_new"]`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.0) — The dropout ratio for the
    embeddings.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.0) — The dropout ratio for
    the attention.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — The epsilon
    to use in the layer normalization layers.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 50256) — Beginning of stream
    token id.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 50256) — End of stream token
    id.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether the
    model’s input and output word embeddings should be tied. Note that this is only
    relevant if the model has a output word embedding layer.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [CodeGenModel](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenModel).
    It is used to instantiate a CodeGen model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the CodeGen [Salesforce/codegen-2B-mono](https://huggingface.co/Salesforce/codegen-2B-mono)
    architecture. Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: CodeGenTokenizer
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CodeGenTokenizer`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/tokenization_codegen.py#L98)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The beginning
    of sequence token.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*，默认为`"<|endoftext|>"`）--序列标记的开头。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*) — The token used for padding, for example when
    batching sequences of different lengths.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (CodeGen tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_bos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    a beginning of sequence token at the start of sequences.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a CodeGen tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: When used with `is_split_into_words=True`, this tokenizer will add a space before
    each word (even the first one).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/tokenization_codegen.py#L288)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: CodeGenTokenizerFast
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CodeGenTokenizerFast`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/tokenization_codegen_fast.py#L63)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`, *optional*) — Path to the vocabulary file.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merges_file` (`str`, *optional*) — Path to the merges file.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer_file` (`str`, *optional*) — Path to [tokenizers](https://github.com/huggingface/tokenizers)
    file (generally has a .json extension) that contains everything needed to load
    the tokenizer.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`"<|endoftext|>"`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The beginning
    of sequence token.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (CodeGen tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a “fast” CodeGen tokenizer (backed by HuggingFace’s *tokenizers* library).
    Based on byte-level Byte-Pair-Encoding.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer, but since the model was not pretrained this way, it might yield
    a decrease in performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: When used with `is_split_into_words=True`, this tokenizer needs to be instantiated
    with `add_prefix_space=True`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/tokenization_codegen_fast.py#L184)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`
    (available in the `tokenizer_config`).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncate_before_pattern` (`List[str]`, *optional*, defaults to `None`) — A
    list of regular expression strings that will be used to truncate the returned
    string. This can be used to remove extra pieces of code (e.g. truncate if observing
    a comment symbol ”#” at the beginning of a new line). An example pattern could
    be `[”^#”, re.escape(”<|endoftext|>”), ”^''''''”, ”'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The decoded sentence.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '”]`. kwargs (additional keyword arguments, *optional*): Will be passed to the
    underlying model specific decode method.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: CodeGenModel
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CodeGenModel`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/modeling_codegen.py#L404)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare CodeGen Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/modeling_codegen.py#L431)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using `AutoProcenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_attention_heads,)` or `(n_layer,
    num_attention_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_dim)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig))
    and inputs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）—
    模型最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则仅输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回）—
    长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中如果`config.is_encoder_decoder=True`的情况下可选地使用）可用（请参见`past_key_values`输入）以加快顺序解码的速度。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为嵌入的输出一个+每一层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [CodeGenModel](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenModel)
    forward method, overrides the `__call__` special method.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[CodeGenModel](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenModel)前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: CodeGenForCausalLM
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CodeGenForCausalLM
- en: '### `class transformers.CodeGenForCausalLM`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CodeGenForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/modeling_codegen.py#L587)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/modeling_codegen.py#L587)'
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The CodeGen Model transformer with a language modeling head on top.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 带有语言建模头部的CodeGen模型变压器。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/modeling_codegen.py#L646)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/codegen/modeling_codegen.py#L646)'
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using `AutoProcenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用`AutoProcenizer`获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '什么是注意力掩码？ '
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*的标记。
- en: 1 corresponds to a *sentence B* token.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_attention_heads,)` or `(n_layer,
    num_attention_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_attention_heads,)`或`(n_layer, num_attention_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_dim)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_dim)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将*input_ids*索引转换为相关向量，而不是使用模型内部的嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于语言建模的标签。请注意，标签在模型内部**被移位**，即您可以设置`labels
    = input_ids`。在`[-100, 0, ..., config.vocab_size]`中选择索引。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0,
    ..., config.vocab_size]`中的标签。'
- en: Returns
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或者`tuple(torch.FloatTensor)`。'
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig))
    and inputs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者`config.return_dict=False`时）包含不同的元素，取决于配置（[CodeGenConfig](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供了`labels`时返回） — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递了`use_cache=True`或者`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值），可以用来加速顺序解码（参见`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_attentions=True`或者`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [CodeGenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[CodeGenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenForCausalLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
