["```py\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\ntorchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py\n```", "```py\n# load model in 8bit\nmodel = AutoModelForCausalLM.from_pretrained(\n        args.model_path,\n        load_in_8bit=True,\n        device_map={\"\": Accelerator().local_process_index}\n    )\nmodel = prepare_model_for_kbit_training(model)\n\n# add LoRA to model\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\n```", "```py\nclass RewardTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        rewards_j = model(input_ids=inputs[\"input_ids_j\"],  attention_mask=inputs[\"attention_mask_j\"])[0]\n        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n        if return_outputs:\n            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n        return loss\n```", "```py\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n```", "```py\nQuestion: <Query>\n\nAnswer: <Response>\n```", "```py\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    question_tensors = batch[\"input_ids\"]\n\n\t# sample from the policy and to generate responses\n    response_tensors = ppo_trainer.generate(\n        question_tensors,\n        return_prompt=False,\n        length_sampler=output_length_sampler,\n        **generation_kwargs,\n    )\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n\n    # Compute sentiment score\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n\n    # Run PPO step\n    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n\t# Log stats to Wandb\n    ppo_trainer.log_stats(stats, batch, rewards)\n```"]