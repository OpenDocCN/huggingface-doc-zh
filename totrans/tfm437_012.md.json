["```py\npip install peft\n```", "```py\npip install git+https://github.com/huggingface/peft.git\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = \"ybelkada/opt-350m-lora\"\nmodel = AutoModelForCausalLM.from_pretrained(peft_model_id)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"facebook/opt-350m\"\npeft_model_id = \"ybelkada/opt-350m-lora\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\nmodel.load_adapter(peft_model_id)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = \"ybelkada/opt-350m-lora\"\nmodel = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", load_in_8bit=True)\n```", "```py\nfrom transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\nfrom peft import LoraConfig\n\nmodel_id = \"facebook/opt-350m\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\nlora_config = LoraConfig(\n    target_modules=[\"q_proj\", \"k_proj\"],\n    init_lora_weights=False\n)\n\nmodel.add_adapter(lora_config, adapter_name=\"adapter_1\")\n```", "```py\n# attach new adapter with same config\nmodel.add_adapter(lora_config, adapter_name=\"adapter_2\")\n```", "```py\n# use adapter_1\nmodel.set_adapter(\"adapter_1\")\noutput = model.generate(**inputs)\nprint(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n\n# use adapter_2\nmodel.set_adapter(\"adapter_2\")\noutput_enabled = model.generate(**inputs)\nprint(tokenizer.decode(output_enabled[0], skip_special_tokens=True))\n```", "```py\nfrom transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\nfrom peft import PeftConfig\n\nmodel_id = \"facebook/opt-350m\"\nadapter_model_id = \"ybelkada/opt-350m-lora\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntext = \"Hello\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\npeft_config = PeftConfig.from_pretrained(adapter_model_id)\n\n# to initiate with random weights\npeft_config.init_lora_weights = False\n\nmodel.add_adapter(peft_config)\nmodel.enable_adapters()\noutput = model.generate(**inputs)\n```", "```py\nmodel.disable_adapters()\noutput = model.generate(**inputs)\n```", "```py\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n```", "```py\nmodel.add_adapter(peft_config)\n```", "```py\ntrainer = Trainer(model=model, ...)\ntrainer.train()\n```", "```py\nmodel.save_pretrained(save_dir)\nmodel = AutoModelForCausalLM.from_pretrained(save_dir)\n```", "```py\nfrom transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\nfrom peft import LoraConfig\n\nmodel_id = \"facebook/opt-350m\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\nlora_config = LoraConfig(\n    target_modules=[\"q_proj\", \"k_proj\"],\n    modules_to_save=[\"lm_head\"],\n)\n\nmodel.add_adapter(lora_config)\n```"]