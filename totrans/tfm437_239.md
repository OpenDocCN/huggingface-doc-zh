# TAPEX

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex)

此模型仅处于维护模式，我们不接受任何更改其代码的新PR。

如果在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。

## 概述

TAPEX模型是由Qian Liu、Bei Chen、Jiaqi Guo、Morteza Ziyadi、Zeqi Lin、Weizhu Chen、Jian-Guang Lou在[TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653)中提出的。TAPEX对BART模型进行预训练，以解决合成SQL查询，然后可以微调以回答与表格数据相关的自然语言问题，以及执行表格事实检查。

TAPEX已在几个数据集上进行了微调：

+   [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Microsoft的顺序问答)

+   [WTQ](https://github.com/ppasupat/WikiTableQuestions)（由斯坦福大学提供的维基表问题）

+   [WikiSQL](https://github.com/salesforce/WikiSQL)（由Salesforce提供）

+   [TabFact](https://tabfact.github.io/)（由USCB NLP实验室提供）。

论文摘要如下：

*最近在语言模型预训练方面取得了巨大成功，通过利用大规模的非结构化文本数据。然而，由于缺乏大规模高质量的表格数据，将预训练应用于结构化表格数据仍然是一个挑战。在本文中，我们提出了TAPEX，以表明通过学习一个神经SQL执行器可以实现表格预训练，这是通过自动合成可执行的SQL查询及其执行输出获得的合成语料库。TAPEX通过引导语言模型在多样化、大规模和高质量的合成语料库上模仿SQL执行器来解决数据稀缺性挑战。我们在四个基准数据集上评估了TAPEX。实验结果表明，TAPEX在所有这些数据集上均大幅优于以前的表格预训练方法，并在所有数据集上取得了新的最先进结果。这包括将弱监督的WikiSQL指称准确率提高到89.5%（+2.3%），将WikiTableQuestions指称准确率提高到57.5%（+4.8%），将SQA指称准确率提高到74.5%（+3.5%），以及将TabFact准确率提高到84.2%（+3.2%）。据我们所知，这是第一项利用合成可执行程序进行表格预训练并在各种下游任务上取得新的最先进结果的工作。*

## 使用提示

+   TAPEX是一个生成（seq2seq）模型。可以直接将TAPEX的权重插入到BART模型中。

+   TAPEX在中心点上有检查点，这些检查点要么只是预训练的，要么在WTQ、SQA、WikiSQL和TabFact上进行了微调。

+   句子+表格以`句子 + " " + 线性化表格`的形式呈现给模型。线性化表格的格式如下：`col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ...`。

+   TAPEX有自己的分词器，可以轻松为模型准备所有数据。可以将Pandas DataFrames和字符串传递给分词器，它将自动创建`input_ids`和`attention_mask`（如下面的使用示例所示）。

### 用途：推理

下面，我们说明如何使用TAPEX进行表格问答。正如大家所看到的，可以直接将TAPEX的权重插入到BART模型中。我们使用[Auto API](auto)，它将根据中心点上的配置文件自动实例化适当的分词器（[TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer)）和模型（[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)）。

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
>>> import pandas as pd

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/tapex-large-finetuned-wtq")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("microsoft/tapex-large-finetuned-wtq")

>>> # prepare table + question
>>> data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
>>> table = pd.DataFrame.from_dict(data)
>>> question = "how many movies does Leonardo Di Caprio have?"

>>> encoding = tokenizer(table, question, return_tensors="pt")

>>> # let the model generate an answer autoregressively
>>> outputs = model.generate(**encoding)

>>> # decode back to text
>>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
>>> print(predicted_answer)
53
```

请注意，[TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer)还支持批量推断。因此，可以提供不同表/问题的批处理，或单个表和多个问题的批处理，或单个查询和多个表的批处理。让我们举个例子：

```py
>>> # prepare table + question
>>> data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
>>> table = pd.DataFrame.from_dict(data)
>>> questions = [
...     "how many movies does Leonardo Di Caprio have?",
...     "which actor has 69 movies?",
...     "what's the first name of the actor who has 87 movies?",
... ]
>>> encoding = tokenizer(table, questions, padding=True, return_tensors="pt")

>>> # let the model generate an answer autoregressively
>>> outputs = model.generate(**encoding)

>>> # decode back to text
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
[' 53', ' george clooney', ' brad pitt']
```

如果要进行表验证（即确定给定句子是否由表的内容支持或反驳的任务），可以实例化一个[BartForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForSequenceClassification)模型。 TAPEX在hub上有针对TabFact进行微调的检查点，这是表事实检查的重要基准（它达到84%的准确率）。下面的代码示例再次利用了[Auto API](auto)。

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/tapex-large-finetuned-tabfact")
>>> model = AutoModelForSequenceClassification.from_pretrained("microsoft/tapex-large-finetuned-tabfact")

>>> # prepare table + sentence
>>> data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
>>> table = pd.DataFrame.from_dict(data)
>>> sentence = "George Clooney has 30 movies"

>>> encoding = tokenizer(table, sentence, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**encoding)

>>> # print prediction
>>> predicted_class_idx = outputs.logits[0].argmax(dim=0).item()
>>> print(model.config.id2label[predicted_class_idx])
Refused
```

TAPEX架构与BART相同，除了标记化。有关配置类及其参数的信息，请参阅[BART文档](bart)。下面记录了TAPEX特定的标记器。

## TapexTokenizer

### `class transformers.TapexTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L194)

```py
( vocab_file merges_file do_lower_case = True errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False max_cell_length = 15 **kwargs )
```

参数

+   `vocab_file`（`str`）—词汇表文件的路径。

+   `merges_file`（`str`）—合并文件的路径。

+   `do_lower_case`（`bool`，*可选*，默认为`True`）—在标记化时是否将输入转换为小写。

+   `errors`（`str`，*可选*，默认为`"replace"`）—解码字节为UTF-8时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。

+   `bos_token`（`str`，*可选*，默认为`"<s>"`）—在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。

+   `eos_token`（`str`，*可选*，默认为`"</s>"`）—序列结束标记。

    构建序列时使用特殊标记时，这不是用于序列结尾的标记。使用的标记是`sep_token`。

+   `sep_token`（`str`，*可选*，默认为`"</s>"`）—分隔符标记，在从多个序列构建序列时使用，例如，用于序列分类的两个序列或用于问题回答的文本和问题。它也用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token`（`str`，*可选*，默认为`"<s>"`）—分类器标记，用于进行序列分类（对整个序列进行分类，而不是对每个标记进行分类）。在使用特殊标记构建时，它是序列的第一个标记。

+   `unk_token`（`str`，*可选*，默认为`"<unk>"`）—未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `pad_token`（`str`，*可选*，默认为`"<pad>"`）—用于填充的标记，例如在批处理不同长度的序列时。

+   `mask_token`（`str`，*可选*，默认为`"<mask>"`）—用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `add_prefix_space`（`bool`，*可选*，默认为`False`）—是否在输入中添加初始空格。这允许将前导单词视为任何其他单词。（BART标记器通过前导空格检测单词的开头）。

+   `max_cell_length`（`int`，*可选*，默认为15）—线性化表时每个单元格的最大字符数。如果超过此数字，将进行截断。

构建一个TAPEX标记器。基于字节级字节对编码（BPE）。

此分词器可用于展平一个或多个表格，并将它们与一个或多个相关句子连接起来，以供TAPEX模型使用。TAPEX分词器创建的格式如下：

句子列：col1 | col2 | col 3 行1：val1 | val2 | val3 行2：…

分词器支持单个表格+单个查询，单个表格和多个查询（在这种情况下，每个查询将被复制以匹配每个表格），单个查询和多个表格（在这种情况下，每个表格将被复制以匹配每个查询），以及多个表格和查询。换句话说，您可以为分词器提供一批表格+问题，以便为模型准备它们。

分词本身基于BPE算法。它与BART、RoBERTa和GPT-2使用的算法相同。

此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L515)

```py
( table: Union = None query: Union = None answer: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs )
```

参数

+   `table`（`pd.DataFrame`，`List[pd.DataFrame]`）— 包含表格数据的表格。

+   `query`（`str`或`List[str]`，*可选*）— 与一个或多个表格相关的句子或句子批次。请注意，句子的数量必须与表格的数量相匹配。

+   `answer`（`str`或`List[str]`，*可选*）— 可选地，与问题相关的答案作为监督。

+   `add_special_tokens`（`bool`，*可选*，默认为`True`）— 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）— 激活和控制填充。接受以下值：

    +   `True`或`'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False`或`'do_not_pad'`（默认）：不填充（即可以输出具有不同长度的序列的批次）。

+   `truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`）— 激活和控制截断。接受以下值：

    +   `True`或`'longest_first'`：使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐个标记截断，从最长序列中删除一个标记。

    +   `'only_first'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第二个序列。

    +   `False`或`'do_not_truncate'`（默认）：不截断（即可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。

+   `is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预分词（例如，已分词）。如果设置为 `True`，分词器会假定输入已经分词（例如，通过在空格上分割），然后对其进行标记化。这对于 NER 或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活 `padding`。这对于启用 NVIDIA 硬件上的 Tensor Cores（计算能力 `>= 7.5`，Volta）特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 是否对序列进行编码，使用相对于其模型的特殊标记。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 填充到由参数 `max_length` 指定的最大长度，或者填充到模型可接受的最大输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）: 无填充（即，可以输出长度不同的序列批次）。

+   `truncation` (`bool`, `str`, `TapexTruncationStrategy` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), — *可选*, 默认为 `False`):

    激活和控制截断。接受以下值：

    +   `'drop_rows_to_fit'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。这将逐行截断，从表中删除行。

    +   `True` 或 `'longest_first'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则将逐个标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第一个序列。

    +   `'only_second'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第二个序列。

    +   `False`或`'do_not_truncate'`（默认）：不截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。

+   `max_length` (`int`, *optional*) — 控制截断/填充参数之一使用的最大长度。如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数之一需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则截断/填充到最大长度将被停用。

+   `stride` (`int`, *optional*, 默认为0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。

+   `return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回Numpy `np.ndarray`对象。

对一个或多个表序列对进行标记化和为模型准备的主要方法。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L486)

```py
( save_directory: str filename_prefix: Optional = None )
```
