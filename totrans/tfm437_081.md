# 实例化一个大模型

> [https://huggingface.co/docs/transformers/v4.37.2/en/big_models](https://huggingface.co/docs/transformers/v4.37.2/en/big_models)

当您想要使用非常大的预训练模型时，一个挑战是尽量减少RAM的使用。来自PyTorch的通常工作流程是：

1.  用随机权重创建您的模型。

1.  加载您的预训练权重。

1.  将这些预训练权重放入您的随机模型中。

步骤1和2都需要内存中的完整模型版本，在大多数情况下这不是问题，但是如果您的模型开始占用数千兆字节，这两个副本可能会使您的RAM不足。更糟糕的是，如果您使用`torch.distributed`启动分布式训练，每个进程都会加载预训练模型并将这两个副本存储在RAM中。

请注意，随机创建的模型是用“空”张量初始化的，这些张量占用内存空间而不填充它（因此随机值是在给定时间内内存块中的内容）。适合模型/参数实例化的适当分布（例如正态分布）的随机初始化仅在第3步对未初始化的权重执行，以尽可能快地完成！

在本指南中，我们探讨了Transformers提供的解决此问题的解决方案。请注意，这是一个正在积极发展的领域，因此这里解释的API可能在未来略有变化。

## 分片检查点

自版本4.18.0以来，占用超过10GB空间的模型检查点会自动分片成较小的部分。在执行`model.save_pretrained(save_dir)`时，您将得到几个部分检查点（每个大小均小于10GB）和一个将参数名称映射到存储文件的索引。

您可以使用`max_shard_size`参数控制分片之前的最大大小，因此为了举例，我们将使用一个具有小分片大小的正常大小模型：让我们使用传统的BERT模型。

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")
```

如果您使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存它，您将获得一个新文件夹，其中包含模型的配置和权重：

```py
>>> import os
>>> import tempfile

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir)
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model.bin']
```

现在让我们使用最大分片大小为200MB：

```py
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']
```

除了模型的配置之外，我们看到三个不同的权重文件，以及一个`index.json`文件，这是我们的索引。可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法完全重新加载这样的检查点：

```py
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     new_model = AutoModel.from_pretrained(tmp_dir)
```

这样做大模型的主要优势在于，在上述工作流程的第2步中，检查点的每个分片在前一个分片之后加载，将RAM中的内存使用限制在模型大小加上最大分片大小的大小。

在幕后，索引文件用于确定检查点中的键以及相应权重存储的位置。我们可以像加载任何json一样加载该索引并获得一个字典：

```py
>>> import json

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     with open(os.path.join(tmp_dir, "pytorch_model.bin.index.json"), "r") as f:
...         index = json.load(f)

>>> print(index.keys())
dict_keys(['metadata', 'weight_map'])
```

元数据目前只包含模型的总大小。我们计划在未来添加其他信息：

```py
>>> index["metadata"]
{'total_size': 433245184}
```

权重映射是此索引的主要部分，它将每个参数名称（通常在PyTorch模型`state_dict`中找到）映射到其存储的文件：

```py
>>> index["weight_map"]
{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',
 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',
 ...
```

如果您想在不使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)（就像您为完整检查点执行`model.load_state_dict()`一样）的情况下直接加载这样的分片检查点，您应该使用[load_sharded_checkpoint()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint)：

```py
>>> from transformers.modeling_utils import load_sharded_checkpoint

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     load_sharded_checkpoint(model, tmp_dir)
```

## 低内存加载

分片检查点减少了上述工作流程第2步中的内存使用，但为了在低内存环境中使用该模型，我们建议利用基于Accelerate库的工具。

请阅读以下指南以获取更多信息：[使用 Accelerate 进行大型模型加载](./main_classes/model#large-model-loading)
