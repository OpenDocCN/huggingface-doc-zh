- en: LLaVa
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVa
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated
    multimodal instruction-following data. It is an auto-regressive language model,
    based on the transformer architecture. In other words, it is an multi-modal version
    of LLMs fine-tuned for chat / instructions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVaæ˜¯é€šè¿‡åœ¨GPTç”Ÿæˆçš„å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªæ•°æ®ä¸Šè¿›è¡ŒLlamA/Vicunaå¾®è°ƒè€Œè®­ç»ƒçš„å¼€æºèŠå¤©æœºå™¨äººã€‚å®ƒæ˜¯ä¸€ç§åŸºäºå˜å‹å™¨æ¶æ„çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯ä¸ºèŠå¤©/æŒ‡ä»¤å¾®è°ƒçš„LLMsçš„å¤šæ¨¡æ€ç‰ˆæœ¬ã€‚
- en: The LLaVa model was proposed in [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
    and improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744)
    by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVaæ¨¡å‹æœ€åˆåœ¨[è§†è§‰æŒ‡å¯¼è°ƒæ•´](https://arxiv.org/abs/2304.08485)ä¸­æå‡ºï¼Œå¹¶åœ¨[é€šè¿‡è§†è§‰æŒ‡å¯¼è°ƒæ•´æ”¹è¿›åŸºçº¿](https://arxiv.org/pdf/2310.03744)ä¸­ç”±Haotian
    Liuã€Chunyuan Liã€Yuheng Liå’ŒYong Jae Leeæ”¹è¿›ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Large multimodal models (LMM) have recently shown encouraging progress with
    visual instruction tuning. In this note, we show that the fully-connected vision-language
    cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With
    simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection
    and adding academic-task-oriented VQA data with simple response formatting prompts,
    we establish stronger baselines that achieve state-of-the-art across 11 benchmarks.
    Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes
    full training in âˆ¼1 day on a single 8-A100 node. We hope this can make state-of-the-art
    LMM research more accessible. Code and model will be publicly available*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€è¿‘ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨è§†è§‰æŒ‡å¯¼è°ƒæ•´æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LLaVAä¸­çš„å…¨è¿æ¥è§†è§‰-è¯­è¨€è·¨æ¨¡æ€è¿æ¥å™¨å‡ºäººæ„æ–™åœ°å¼ºå¤§ä¸”é«˜æ•ˆã€‚é€šè¿‡å¯¹LLaVAè¿›è¡Œç®€å•ä¿®æ”¹ï¼Œå³ä½¿ç”¨CLIP-ViT-L-336pxä¸MLPæŠ•å½±ï¼Œå¹¶æ·»åŠ å­¦æœ¯ä»»åŠ¡å¯¼å‘çš„VQAæ•°æ®ä»¥åŠç®€å•çš„å“åº”æ ¼å¼æç¤ºï¼Œæˆ‘ä»¬å»ºç«‹äº†æ›´å¼ºçš„åŸºçº¿ï¼Œå®ç°äº†11ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æœ€ç»ˆ13Bæ£€æŸ¥ç‚¹ä»…ä½¿ç”¨äº†120ä¸‡ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®ï¼Œå¹¶åœ¨å•ä¸ª8-A100èŠ‚ç‚¹ä¸Šçš„çº¦1å¤©å†…å®Œæˆäº†å®Œæ•´è®­ç»ƒã€‚æˆ‘ä»¬å¸Œæœ›è¿™å¯ä»¥ä½¿æœ€å…ˆè¿›çš„LMMç ”ç©¶æ›´æ˜“äºè®¿é—®ã€‚ä»£ç å’Œæ¨¡å‹å°†ä¼šå…¬å¼€å‘å¸ƒ*'
- en: '![drawing](../Images/616d246acc70828a994631a4667a609e.png) LLaVa architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2304.08485)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![drawing](../Images/616d246acc70828a994631a4667a609e.png) LLaVaæ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2304.08485)'
- en: This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ) and
    [ybelkada](https://huggingface.co/ybelkada). The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main/llava).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç”±[ArthurZ](https://huggingface.co/ArthurZ)å’Œ[ybelkada](https://huggingface.co/ybelkada)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/haotian-liu/LLaVA/tree/main/llava)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: We advise users to use `padding_side="left"` when computing batched generation
    as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side
    = "left"` before generating.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®ç”¨æˆ·åœ¨è®¡ç®—æ‰¹é‡ç”Ÿæˆæ—¶ä½¿ç”¨`padding_side="left"`ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´æ›´å‡†ç¡®çš„ç»“æœã€‚åªéœ€ç¡®ä¿åœ¨ç”Ÿæˆä¹‹å‰è°ƒç”¨`processor.tokenizer.padding_side
    = "left"`ã€‚
- en: Note the model has not been explicitly trained to process multiple images in
    the same prompt, although this is technically possible, you may experience inaccurate
    results.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹å°šæœªæ˜ç¡®è®­ç»ƒä»¥å¤„ç†åŒä¸€æç¤ºä¸­çš„å¤šä¸ªå›¾åƒï¼Œå°½ç®¡ä»æŠ€æœ¯ä¸Šè®²è¿™æ˜¯å¯èƒ½çš„ï¼Œä½†æ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸å‡†ç¡®çš„ç»“æœã€‚
- en: 'For better results, we recommend users to prompt the model with the correct
    prompt format:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨æ­£ç¡®çš„æç¤ºæ ¼å¼æç¤ºæ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For multiple turns conversation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤šè½®å¯¹è¯ï¼š
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using Flash Attention 2
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flash Attention 2
- en: Flash Attention 2 is an even faster, optimized version of the previous optimization,
    please refer to the [Flash Attention 2 section of performance docs](https://huggingface.co/docs/transformers/perf_infer_gpu_one).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Flash Attention 2æ˜¯å…ˆå‰ä¼˜åŒ–çš„æ›´å¿«ã€æ›´ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼Œè¯·å‚é˜…[æ€§èƒ½æ–‡æ¡£ä¸­çš„Flash Attention 2éƒ¨åˆ†](https://huggingface.co/docs/transformers/perf_infer_gpu_one)ã€‚
- en: Resources
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with BEiT.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨BEiTã€‚
- en: â€‹ Image-to-Text
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°æ–‡æœ¬
- en: A [Google Colab demo](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing)
    on how to run Llava on a free-tier Google colab instance leveraging 4-bit inference.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•åœ¨å…è´¹çš„Google Colabå®ä¾‹ä¸Šè¿è¡ŒLlavaçš„[Google Colabæ¼”ç¤º](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing)ï¼Œåˆ©ç”¨4ä½æ¨ç†ã€‚
- en: A [similar notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb)
    showcasing batched inference. ğŸŒ
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±•ç¤ºæ‰¹é‡æ¨ç†çš„[ç±»ä¼¼ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb)ã€‚ğŸŒ
- en: LlavaConfig
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlavaConfig
- en: '### `class transformers.LlavaConfig`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlavaConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/configuration_llava.py#L28)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/configuration_llava.py#L28)'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vision_config` (`LlavaVisionConfig`, *optional*) â€” Custom vision config or
    dict'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_config`ï¼ˆ`LlavaVisionConfig`ï¼Œ*å¯é€‰*ï¼‰â€” è‡ªå®šä¹‰è§†è§‰é…ç½®æˆ–å­—å…¸'
- en: '`text_config` (`Union[AutoConfig, dict]`, *optional*) â€” The config object of
    the text backbone. Can be any of `LlamaConfig` or `MistralConfig`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config`ï¼ˆ`Union[AutoConfig, dict]`ï¼Œ*å¯é€‰*ï¼‰â€” æ–‡æœ¬ä¸»å¹²çš„é…ç½®å¯¹è±¡ã€‚å¯ä»¥æ˜¯`LlamaConfig`æˆ–`MistralConfig`ä¹‹ä¸€ã€‚'
- en: '`ignore_index` (`int`, *optional*, defaults to -100) â€” The ignore index for
    the loss function.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º-100ï¼‰â€” æŸå¤±å‡½æ•°çš„å¿½ç•¥ç´¢å¼•ã€‚'
- en: '`image_token_index` (`int`, *optional*, defaults to 32000) â€” The image token
    index to encode the image prompt.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º32000ï¼‰â€” ç”¨äºç¼–ç å›¾åƒæç¤ºçš„å›¾åƒæ ‡è®°ç´¢å¼•ã€‚'
- en: '`projector_hidden_act` (`str`, *optional*, defaults to `"gelu"`) â€” The activation
    function used by the multimodal projector.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projector_hidden_act`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€” å¤šæ¨¡æ€æŠ•å½±å™¨ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚'
- en: '`vision_feature_select_strategy` (`str`, *optional*, defaults to `"default"`)
    â€” The feature selection strategy used to select the vision feature from the CLIP
    backbone.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_feature_select_strategy` (`str`, *optional*, é»˜è®¤ä¸º`"default"`) â€” ç”¨äºä»CLIPéª¨å¹²ä¸­é€‰æ‹©è§†è§‰ç‰¹å¾çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚'
- en: '`vision_feature_layer` (`int`, *optional*, defaults to -2) â€” The index of the
    layer to select the vision feature.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_feature_layer` (`int`, *optional*, é»˜è®¤ä¸º-2) â€” é€‰æ‹©è§†è§‰ç‰¹å¾çš„å±‚çš„ç´¢å¼•ã€‚'
- en: '`vocab_size` (`int`, *optional*, defaults to 32000) â€” Vocabulary size of the
    Llava model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [~LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, é»˜è®¤ä¸º32000) â€” Llavaæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[~LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)æ—¶å¯ä»¥ç”±`inputs_ids`è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: This is the configuration class to store the configuration of a [LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration).
    It is used to instantiate an Llava model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Llava-9B.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªLlavaæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLlava-9Bçš„é…ç½®ã€‚
- en: e.g. [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: LlavaProcessor
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlavaProcessor
- en: '### `class transformers.LlavaProcessor`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlavaProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L29)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L29)'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_processor` ([CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor),
    *optional*) â€” The image processor is a required input.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` ([CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor),
    *optional*) â€” å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` ([LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast),
    *optional*) â€” The tokenizer is a required input.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast),
    *optional*) â€” Tokenizeræ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a Llava processor which wraps a Llava image processor and a Llava
    tokenizer into a single processor.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªLlavaå¤„ç†å™¨ï¼Œå°†Llavaå›¾åƒå¤„ç†å™¨å’ŒLlavaåˆ†è¯å™¨å°è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚
- en: '[LlavaProcessor](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaProcessor)
    offers all the functionalities of [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    and [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast).
    See the `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaProcessor.decode)
    for more information.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlavaProcessor](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaProcessor)æä¾›äº†[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)å’Œ[LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast)çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹`__call__()`å’Œ[decode()](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaProcessor.decode)ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '#### `batch_decode`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L116)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L116)'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This method forwards all its arguments to LlamaTokenizerFastâ€™s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™LlamaTokenizerFastçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `decode`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L124)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L124)'
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This method forwards all its arguments to LlamaTokenizerFastâ€™s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™LlamaTokenizerFastçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: LlavaForConditionalGeneration
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlavaForConditionalGeneration
- en: '### `class transformers.LlavaForConditionalGeneration`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlavaForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L233)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L233)'
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LlavaConfig](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaConfig)
    or `LlavaVisionConfig`) â€” Model configuration class with all the parameters of
    the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LlavaConfig](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaConfig)æˆ–`LlavaVisionConfig`ï¼‰â€”æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The LLAVA model which consists of a vision backbone and a language model. This
    model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LLAVAæ¨¡å‹ç”±è§†è§‰ä¸»å¹²å’Œè¯­è¨€æ¨¡å‹ç»„æˆã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L348)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L348)'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€”è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç´¢å¼•å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size,
    image_size)) -- The tensors corresponding to the input images. Pixel values can
    be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.__call__()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details ([]`LlavaProcessor`] uses [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    for processing images).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size,
    image_size)) -- è¾“å…¥å›¾åƒå¯¹åº”çš„å¼ é‡ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[CLIPImageProcessor.__call__()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ï¼ˆ[]`LlavaProcessor`]ä½¿ç”¨[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)æ¥å¤„ç†å›¾åƒï¼‰ã€‚'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0,
    1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç´¢å¼•å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: 1 indicates the head is `not masked`,
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç `ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç `ã€‚
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`. [What are position
    IDs?](../glossary#position-ids)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0,
    config.n_positions - 1]`ã€‚[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œä»¥åŠ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰
    â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: 'Args â€” labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‚æ•° â€” labels (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰ï¼šç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆè¯·å‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚
- en: Returns
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` or `tuple(torch.FloatTensor)`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` æˆ– `tuple(torch.FloatTensor)`'
- en: A `transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LlavaConfig](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaConfig))
    and inputs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LlavaConfig](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º
    + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–è€…`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`image_hidden_states` (`tuple(torch.FloatTensor)`, *optional*) â€” Tuple of `torch.FloatTensor`
    (one for the output of the image embeddings, `(batch_size, num_images, sequence_length,
    hidden_size)`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*) â€” å›¾åƒåµŒå…¥è¾“å‡ºçš„å…ƒç»„`torch.FloatTensor`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,
    num_images, sequence_length, hidden_size)`ï¼‰ã€‚'
- en: image_hidden_states of the model produced by the vision encoder, and optionally
    by the perceiver
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç”±è§†è§‰ç¼–ç å™¨äº§ç”Ÿçš„æ¨¡å‹çš„å›¾åƒéšè—çŠ¶æ€ï¼Œä»¥åŠå¯é€‰çš„ç”±æ„ŸçŸ¥å™¨äº§ç”Ÿçš„éšè—çŠ¶æ€ã€‚
- en: The [LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
