- en: LLaVa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/167.36d6a9c7.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/PipelineTag.44585822.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated
    multimodal instruction-following data. It is an auto-regressive language model,
    based on the transformer architecture. In other words, it is an multi-modal version
    of LLMs fine-tuned for chat / instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The LLaVa model was proposed in [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
    and improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744)
    by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large multimodal models (LMM) have recently shown encouraging progress with
    visual instruction tuning. In this note, we show that the fully-connected vision-language
    cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With
    simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection
    and adding academic-task-oriented VQA data with simple response formatting prompts,
    we establish stronger baselines that achieve state-of-the-art across 11 benchmarks.
    Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes
    full training in âˆ¼1 day on a single 8-A100 node. We hope this can make state-of-the-art
    LMM research more accessible. Code and model will be publicly available*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/616d246acc70828a994631a4667a609e.png) LLaVa architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2304.08485)'
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ) and
    [ybelkada](https://huggingface.co/ybelkada). The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main/llava).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We advise users to use `padding_side="left"` when computing batched generation
    as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side
    = "left"` before generating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the model has not been explicitly trained to process multiple images in
    the same prompt, although this is technically possible, you may experience inaccurate
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For better results, we recommend users to prompt the model with the correct
    prompt format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For multiple turns conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using Flash Attention 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flash Attention 2 is an even faster, optimized version of the previous optimization,
    please refer to the [Flash Attention 2 section of performance docs](https://huggingface.co/docs/transformers/perf_infer_gpu_one).
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to
    help you get started with BEiT.
  prefs: []
  type: TYPE_NORMAL
- en: â€‹ Image-to-Text
  prefs: []
  type: TYPE_NORMAL
- en: A [Google Colab demo](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing)
    on how to run Llava on a free-tier Google colab instance leveraging 4-bit inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [similar notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb)
    showcasing batched inference. ðŸŒŽ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlavaConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlavaConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/configuration_llava.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vision_config = None text_config = None ignore_index = -100 image_token_index
    = 32000 projector_hidden_act = 'gelu' vision_feature_select_strategy = 'default'
    vision_feature_layer = -2 vocab_size = 32000 **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vision_config** (`LlavaVisionConfig`, *optional*) â€” Custom vision config
    or dict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_config** (`Union[AutoConfig, dict]`, *optional*) â€” The config object
    of the text backbone. Can be any of `LlamaConfig` or `MistralConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ignore_index** (`int`, *optional*, defaults to -100) â€” The ignore index for
    the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_token_index** (`int`, *optional*, defaults to 32000) â€” The image token
    index to encode the image prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**projector_hidden_act** (`str`, *optional*, defaults to `"gelu"`) â€” The activation
    function used by the multimodal projector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vision_feature_select_strategy** (`str`, *optional*, defaults to `"default"`)
    â€” The feature selection strategy used to select the vision feature from the CLIP
    backbone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vision_feature_layer** (`int`, *optional*, defaults to -2) â€” The index of
    the layer to select the vision feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 32000) â€” Vocabulary size of
    the Llava model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [~LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration).
    It is used to instantiate an Llava model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Llava-9B.
  prefs: []
  type: TYPE_NORMAL
- en: e.g. [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: LlavaProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlavaProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image_processor = None tokenizer = None )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image_processor** ([CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor),
    *optional*) â€” The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** ([LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast),
    *optional*) â€” The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Llava processor which wraps a Llava image processor and a Llava
    tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[LlavaProcessor](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaProcessor)
    offers all the functionalities of [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    and [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast).
    See the `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaProcessor.decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### batch_decode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L116)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: This method forwards all its arguments to LlamaTokenizerFastâ€™s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### decode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L124)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: This method forwards all its arguments to LlamaTokenizerFastâ€™s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: LlavaForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlavaForConditionalGeneration'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L233)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: LlavaConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([LlavaConfig](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaConfig)
    or `LlavaVisionConfig`) â€” Model configuration class with all the parameters of
    the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLAVA model which consists of a vision backbone and a language model. This
    model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L348)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: LongTensor = None pixel_values: FloatTensor = None attention_mask:
    Optional = None position_ids: Optional = None past_key_values: Optional = None
    inputs_embeds: Optional = None vision_feature_layer: Optional = None vision_feature_select_strategy:
    Optional = None labels: Optional = None use_cache: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None ) â†’ `transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    image_size, image_size)) -- The tensors corresponding to the input images. Pixel
    values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.__call__()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details ([]`LlavaProcessor`] uses [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    for processing images).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`. [What are position
    IDs?](../glossary#position-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Args â€” labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LlavaConfig](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**image_hidden_states** (`tuple(torch.FloatTensor)`, *optional*) â€” Tuple of
    `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size,
    num_images, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: image_hidden_states of the model produced by the vision encoder, and optionally
    by the perceiver
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LlavaForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
