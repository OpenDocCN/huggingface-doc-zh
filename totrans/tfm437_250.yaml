- en: XLNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLNet
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlnet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlnet)
- en: '[![Models](../Images/09fb4ec5e9dc49c97e92796ae9d5a2d4.png)](https://huggingface.co/models?filter=xlnet)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/xlnet-base-cased)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![模型](../Images/09fb4ec5e9dc49c97e92796ae9d5a2d4.png)](https://huggingface.co/models?filter=xlnet)
    [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/xlnet-base-cased)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining
    for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang,
    Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. XLnet
    is an extension of the Transformer-XL model pre-trained using an autoregressive
    method to learn bidirectional contexts by maximizing the expected likelihood over
    all permutations of the input sequence factorization order.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'XLNet模型是由Zhilin Yang、Zihang Dai、Yiming Yang、Jaime Carbonell、Ruslan Salakhutdinov、Quoc
    V. Le提出的，其论文名为《XLNet: Generalized Autoregressive Pretraining for Language Understanding》(https://arxiv.org/abs/1906.08237)。XLNet是Transformer-XL模型的扩展，使用自回归方法进行预训练，通过最大化输入序列分解顺序的所有排列的期望似然来学习双向上下文。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*With the capability of modeling bidirectional contexts, denoising autoencoding
    based pretraining like BERT achieves better performance than pretraining approaches
    based on autoregressive language modeling. However, relying on corrupting the
    input with masks, BERT neglects dependency between the masked positions and suffers
    from a pretrain-finetune discrepancy. In light of these pros and cons, we propose
    XLNet, a generalized autoregressive pretraining method that (1) enables learning
    bidirectional contexts by maximizing the expected likelihood over all permutations
    of the factorization order and (2) overcomes the limitations of BERT thanks to
    its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL,
    the state-of-the-art autoregressive model, into pretraining. Empirically, under
    comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a
    large margin, including question answering, natural language inference, sentiment
    analysis, and document ranking.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*具有建模双向上下文的能力，基于去噪自编码的BERT比基于自回归语言建模的预训练方法表现更好。然而，BERT依赖于用掩码损坏输入，忽略了掩码位置之间的依赖关系，并且存在预训练和微调之间的差异。鉴于这些优缺点，我们提出了XLNet，一种广义的自回归预训练方法，它(1)通过最大化分解顺序的所有排列的期望似然来实现学习双向上下文，(2)通过其自回归公式克服了BERT的局限性。此外，XLNet将Transformer-XL的思想整合到预训练中。在可比的实验设置下，XLNet在20个任务中表现优于BERT，通常差距很大，包括问答、自然语言推理、情感分析和文档排名。*'
- en: This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The
    original code can be found [here](https://github.com/zihangdai/xlnet/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[thomwolf](https://huggingface.co/thomwolf)贡献。原始代码可以在[这里](https://github.com/zihangdai/xlnet/)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: The specific attention pattern can be controlled at training and test time using
    the `perm_mask` input.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定的注意力模式可以通过`perm_mask`输入在训练和测试时进行控制。
- en: Due to the difficulty of training a fully auto-regressive model over various
    factorization order, XLNet is pretrained using only a sub-set of the output tokens
    as target which are selected with the `target_mapping` input.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于在各种分解顺序上训练完全自回归模型的困难，XLNet仅使用一部分输出令牌作为目标进行预训练，这些令牌是使用`target_mapping`输入选择的。
- en: To use XLNet for sequential decoding (i.e. not in fully bi-directional setting),
    use the `perm_mask` and `target_mapping` inputs to control the attention span
    and outputs (see examples in *examples/pytorch/text-generation/run_generation.py*)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将XLNet用于顺序解码(即不在完全双向设置中)，请使用`perm_mask`和`target_mapping`输入来控制注意力范围和输出(请参见*examples/pytorch/text-generation/run_generation.py*中的示例)
- en: XLNet is one of the few models that has no sequence length limit.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet是少数没有序列长度限制的模型之一。
- en: XLNet is not a traditional autoregressive model but uses a training strategy
    that builds on that. It permutes the tokens in the sentence, then allows the model
    to use the last n tokens to predict the token n+1\. Since this is all done with
    a mask, the sentence is actually fed in the model in the right order, but instead
    of masking the first n tokens for n+1, XLNet uses a mask that hides the previous
    tokens in some given permutation of 1,…,sequence length.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet不是传统的自回归模型，而是使用建立在其基础上的训练策略。它对句子中的令牌进行排列，然后允许模型使用最后n个令牌来预测第n+1个令牌。由于这一切都是通过掩码完成的，因此实际上是以正确顺序将句子输入模型，但是XLNet使用一个掩码，隐藏了给定排列中1,…，序列长度之间的先前令牌，而不是为n+1掩码前n个令牌。
- en: XLNet also uses the same recurrence mechanism as Transformer-XL to build long-term
    dependencies.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet还使用与Transformer-XL相同的循环机制来构建长期依赖关系。
- en: Resources
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标记分类任务指南](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多项选择任务指南](../tasks/multiple_choice)'
- en: XLNetConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetConfig
- en: '### `class transformers.XLNetConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/configuration_xlnet.py#L32)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/configuration_xlnet.py#L32)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32000) — Vocabulary size of the
    XLNet model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    or [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 32000) — XLNet模型的词汇量。定义了在调用[XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)或[TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel)时可以表示的不同标记数量。'
- en: '`d_model` (`int`, *optional*, defaults to 1024) — Dimensionality of the encoder
    layers and the pooler layer.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, defaults to 1024) — 编码器层和池化层的维度。'
- en: '`n_layer` (`int`, *optional*, defaults to 24) — Number of hidden layers in
    the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer` (`int`, *optional*, defaults to 24) — Transformer编码器中的隐藏层数。'
- en: '`n_head` (`int`, *optional*, defaults to 16) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`d_inner` (`int`, *optional*, defaults to 4096) — Dimensionality of the “intermediate”
    (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_inner` (`int`, *optional*, defaults to 4096) — Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`ff_activation` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the If string, `"gelu"`,
    `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ff_activation` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 在Transformer编码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`untie_r` (`bool`, *optional*, defaults to `True`) — Whether or not to untie
    relative position biases'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`untie_r` (`bool`, *optional*, defaults to `True`) — 是否解开相对位置偏差'
- en: '`attn_type` (`str`, *optional*, defaults to `"bi"`) — The attention type used
    by the model. Set `"bi"` for XLNet, `"uni"` for Transformer-XL.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_type` (`str`, *optional*, defaults to `"bi"`) — 模型使用的注意力类型。为XLNet设置`"bi"`，为Transformer-XL设置`"uni"`。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`mem_len` (`int` or `None`, *optional*) — The number of tokens to cache. The
    key/value pairs that have already been pre-computed in a previous forward pass
    won’t be re-computed. See the [quickstart](https://huggingface.co/transformers/quickstart.html#using-the-past)
    for more information.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mem_len` (`int` or `None`, *optional*) — 要缓存的标记数。已经在先前的前向传递中预先计算的键/值对不会重新计算。有关更多信息，请参阅[快速入门](https://huggingface.co/transformers/quickstart.html#using-the-past)。'
- en: '`reuse_len` (`int`, *optional*) — The number of tokens in the current batch
    to be cached and reused in the future.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reuse_len` (`int`, *optional*) — 当前批次中要缓存和将来重复使用的标记数。'
- en: '`bi_data` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    bidirectional input pipeline. Usually set to `True` during pretraining and `False`
    during finetuning.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bi_data` (`bool`, *optional*, defaults to `False`) — 是否使用双向输入管道。通常在预训练期间设置为`True`，在微调期间设置为`False`。'
- en: '`clamp_len` (`int`, *optional*, defaults to -1) — Clamp all relative distances
    larger than clamp_len. Setting this attribute to -1 means no clamping.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clamp_len` (`int`, *optional*, defaults to -1) — 将大于clamp_len的所有相对距离限制。将此属性设置为-1表示不限制。'
- en: '`same_length` (`bool`, *optional*, defaults to `False`) — Whether or not to
    use the same attention length for each token.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`same_length` (`bool`, *optional*, defaults to `False`) — 是否对每个标记使用相同的注意力长度。'
- en: '`summary_type` (`str`, *optional*, defaults to “last”) — Argument used when
    doing sequence summary. Used in the sequence classification and multiple choice
    models.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type` (`str`, *optional*, defaults to “last”) — 在进行序列摘要时使用的参数。用于序列分类和多选模型。'
- en: 'Has to be one of the following options:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 必须是以下选项之一：
- en: '`"last"`: Take the last token hidden state (like XLNet).'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"last"`: 取最后一个标记的隐藏状态（类似于XLNet）。'
- en: '`"first"`: Take the first token hidden state (like BERT).'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"first"`: 取第一个标记的隐藏状态（类似于BERT）。'
- en: '`"mean"`: Take the mean of all tokens hidden states.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mean"`: 取所有标记隐藏状态的平均值。'
- en: '`"cls_index"`: Supply a Tensor of classification token position (like GPT/GPT-2).'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cls_index"`: 提供分类标记位置的张量（类似于GPT/GPT-2）。'
- en: '`"attn"`: Not implemented now, use multi-head attention.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"attn"`: 目前未实现，使用多头注意力。'
- en: '`summary_use_proj` (`bool`, *optional*, defaults to `True`) — Argument used
    when doing sequence summary. Used in the sequence classification and multiple
    choice models.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_use_proj` (`bool`, *optional*, defaults to `True`) — 在进行序列摘要时使用的参数。用于序列分类和多选模型。'
- en: Whether or not to add a projection after the vector extraction.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否在向量提取后添加投影。
- en: '`summary_activation` (`str`, *optional*) — Argument used when doing sequence
    summary. Used in the sequence classification and multiple choice models.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_activation` (`str`, *optional*) — 在进行序列摘要时使用的参数。用于序列分类和多选模型。'
- en: Pass `"tanh"` for a tanh activation to the output, any other value will result
    in no activation.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将输出传递给tanh激活以获得tanh激活，其他任何值都将导致无激活。
- en: '`summary_proj_to_labels` (`boo`, *optional*, defaults to `True`) — Used in
    the sequence classification and multiple choice models.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_proj_to_labels` (`boo`, *optional*, defaults to `True`) — 用于序列分类和多选模型。'
- en: Whether the projection outputs should have `config.num_labels` or `config.hidden_size`
    classes.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 投影输出应具有`config.num_labels`或`config.hidden_size`类。
- en: '`summary_last_dropout` (`float`, *optional*, defaults to 0.1) — Used in the
    sequence classification and multiple choice models.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_last_dropout` (`float`, *optional*, defaults to 0.1) — 用于序列分类和多选模型。'
- en: The dropout ratio to be used after the projection and activation.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在投影和激活之后要使用的丢失比率。
- en: '`start_n_top` (`int`, *optional*, defaults to 5) — Used in the SQuAD evaluation
    script.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_n_top` (`int`, *optional*, defaults to 5) — 在 SQuAD 评估脚本中使用。'
- en: '`end_n_top` (`int`, *optional*, defaults to 5) — Used in the SQuAD evaluation
    script.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_n_top` (`int`, *optional*, defaults to 5) — 在 SQuAD 评估脚本中使用。'
- en: '`use_mems_eval` (`bool`, *optional*, defaults to `True`) — Whether or not the
    model should make use of the recurrent memory mechanism in evaluation mode.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mems_eval` (`bool`, *optional*, defaults to `True`) — 模型在评估模式下是否应使用循环记忆机制。'
- en: '`use_mems_train` (`bool`, *optional*, defaults to `False`) — Whether or not
    the model should make use of the recurrent memory mechanism in train mode.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mems_train` (`bool`, *optional*, defaults to `False`) — 模型在训练模式下是否应使用循环记忆机制。'
- en: For pretraining, it is recommended to set `use_mems_train` to `True`. For fine-tuning,
    it is recommended to set `use_mems_train` to `False` as discussed [here](https://github.com/zihangdai/xlnet/issues/41#issuecomment-505102587).
    If `use_mems_train` is set to `True`, one has to make sure that the train batches
    are correctly pre-processed, *e.g.* `batch_1 = [[This line is], [This is the]]`
    and `batch_2 = [[ the first line], [ second line]]` and that all batches are of
    equal size.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于预训练，建议将 `use_mems_train` 设置为 `True`。对于微调，建议将 `use_mems_train` 设置为 `False`，如[此处](https://github.com/zihangdai/xlnet/issues/41#issuecomment-505102587)所述。如果将
    `use_mems_train` 设置为 `True`，则必须确保训练批次已正确预处理，例如 `batch_1 = [[This line is], [This
    is the]]` 和 `batch_2 = [[ the first line], [ second line]]`，并且所有批次大小相等。
- en: This is the configuration class to store the configuration of a [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    or a [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel).
    It is used to instantiate a XLNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the [xlnet-large-cased](https://huggingface.co/xlnet-large-cased)
    architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    或 [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel)
    配置的类。根据指定的参数实例化 XLNet 模型，定义模型架构。使用默认值实例化配置将产生类似于 [xlnet-large-cased](https://huggingface.co/xlnet-large-cased)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: XLNetTokenizer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetTokenizer
- en: '### `class transformers.XLNetTokenizer`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L53)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L53)'
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .spm extension) that contains the vocabulary necessary to
    instantiate a tokenizer.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含实例化标记器所需词汇的 [SentencePiece](https://github.com/google/sentencepiece)
    文件（通常具有 .spm 扩展名）。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `False`) — Whether to lowercase
    the input when tokenizing.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *optional*, defaults to `False`) — 是否在标记化时将输入转换为小写。'
- en: '`remove_space` (`bool`, *optional*, defaults to `True`) — Whether to strip
    the text when tokenizing (removing excess spaces before and after the string).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_space` (`bool`, *optional*, defaults to `True`) — 在标记化时是否去除文本中的空格（删除字符串前后的多余空格）。'
- en: '`keep_accents` (`bool`, *optional*, defaults to `False`) — Whether to keep
    accents when tokenizing.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_accents` (`bool`, *optional*, defaults to `False`) — 在标记化时是否保留重音。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开始标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建序列时，这不是用于序列开头的标记。使用的标记是 `cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建序列时，这不是用于序列结尾的标记。使用的标记是 `sep_token`。
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为
    ID，而是设置为此标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"<sep>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"<sep>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<cls>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *可选*, 默认为 `"<cls>"`) — 在进行序列分类（对整个序列进行分类而不是每个标记的分类）时使用的分类器标记。在构建带有特殊标记的序列时，它是序列的第一个标记。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *可选*, 默认为 `"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `[''<eop>'',
    ''<eod>'']`) — Additional special tokens used by the tokenizer.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *可选*, 默认为 `[''<eop>'', ''<eod>'']`)
    — 分词器使用的额外特殊标记。'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs` (`dict`, *可选*) — 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`: 启用子词正则化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`: 单字采样参数。对于BPE-Dropout无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`: 不执行采样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`: 从nbest_size结果中进行采样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`: 假设nbest_size为无限，并使用前向过滤和后向采样算法从所有假设（格）中进行采样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: 用于单字采样的平滑参数，以及用于BPE-dropout合并操作的丢弃概率。'
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和ID）的*SentencePiece*处理器。'
- en: Construct an XLNet tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个XLNet分词器。基于[SentencePiece](https://github.com/google/sentencepiece)。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L298)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L298)'
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的第二个ID列表（可选）。'
- en: Returns
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLNet sequence has the following
    format:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。一个XLNet序列的格式如下：
- en: 'single sequence: `X <sep> <cls>`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '单个序列: `X <sep> <cls>`'
- en: 'pair of sequences: `A <sep> B <sep> <cls>`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`A <sep> B <sep> <cls>`
- en: '#### `get_special_tokens_mask`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L323)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L323)'
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的第二个ID列表（可选）。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经使用特殊标记格式化为模型。'
- en: Returns
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器的`prepare_for_model`方法添加特殊标记时调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L351)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L351)'
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的第二个ID列表（可选）。'
- en: Returns
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLNet
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。一个XLNet
- en: 'sequence pair mask has the following format:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对掩码的格式如下：
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0s）。
- en: '#### `save_vocabulary`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L381)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L381)'
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: XLNetTokenizerFast
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetTokenizerFast
- en: '### `class transformers.XLNetTokenizerFast`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L63)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L63)'
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .spm extension) that contains the vocabulary necessary to
    instantiate a tokenizer.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`）— [SentencePiece](https://github.com/google/sentencepiece)
    文件（通常具有.spm扩展名），其中包含实例化标记器所需的词汇。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether to lowercase
    the input when tokenizing.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case`（`bool`，*可选*，默认为`True`）— 在标记化时是否将输入转换为小写。'
- en: '`remove_space` (`bool`, *optional*, defaults to `True`) — Whether to strip
    the text when tokenizing (removing excess spaces before and after the string).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_space`（`bool`，*可选*，默认为`True`）— 在标记化时是否去除文本（删除字符串前后的多余空格）。'
- en: '`keep_accents` (`bool`, *optional*, defaults to `False`) — Whether to keep
    accents when tokenizing.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_accents`（`bool`，*可选*，默认为`False`）— 在标记化时是否保留重音。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"<sep>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`，*可选*，默认为`"<sep>"`）— 分隔符标记，在构建来自多个序列的序列时使用，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"<pad>"`）— 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<cls>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token`（`str`，*可选*，默认为`"<cls>"`）— 在进行序列分类（整个序列的分类而不是每个标记的分类）时使用的分类器标记。在使用特殊标记构建时，它是序列的第一个标记。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`，*可选*，默认为`"<mask>"`）— 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `["<eop>",
    "<eod>"]`) — Additional special tokens used by the tokenizer.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`（`List[str]`，*可选*，默认为`["<eop>", "<eod>"]`）— 标记器使用的其他特殊标记。'
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model`（`SentencePieceProcessor`）— 用于每次转换（字符串、标记和ID）的*SentencePiece*处理器。'
- en: Construct a “fast” XLNet tokenizer (backed by HuggingFace’s *tokenizers* library).
    Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 构建“快速”XLNet标记器（由HuggingFace的*tokenizers*库支持）。基于[Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models)。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此标记器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L177)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L177)'
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLNet sequence has the following
    format:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，为序列分类任务构建来自序列或序列对的模型输入。XLNet序列的格式如下：
- en: 'single sequence: `X <sep> <cls>`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`X <sep> <cls>`
- en: 'pair of sequences: `A <sep> B <sep> <cls>`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`A <sep> B <sep> <cls>`
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L202)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L202)'
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）- ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）- 序列对的第二个ID列表（可选）。'
- en: Returns
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定的序列，列出[令牌类型ID](../glossary#token-type-ids)。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLNet
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。一个XLNet
- en: 'sequence pair mask has the following format:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对掩码的格式如下：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。
- en: XLNet specific outputs
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNet特定的输出
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetModelOutput`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L578)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L578)'
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_predict, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict`对应于`target_mapping.shape[1]`。如果`target_mapping`为`None`，则`num_predict`对应于`sequence_length`。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）- 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去传递给此模型的令牌ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L612)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L612)'
- en: '[PRE13]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为*(1,)*的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 语言建模损失（用于下一个令牌预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_predict, config.vocab_size)`的`torch.FloatTensor`）-
    语言建模头的预测分数（SoftMax之前每个词汇令牌的分数）。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict`对应于`target_mapping.shape[1]`。如果`target_mapping`为`None`，则`num_predict`对应于`sequence_length`。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）- 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去传递给此模型的令牌ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: Output type of [XLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L649)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L649)'
- en: '[PRE14]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*, 当提供`label`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态。可以用于加速顺序解码。将过去给定给该模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: Output type of [XLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L717)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L717)'
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为*(1,)*，*可选*, 当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, num_choices)`) — *num_choices*是输入张量的第二维度。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类得分（SoftMax之前）。
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态。可以用于加速顺序解码。将过去给定给该模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: Output type of [XLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L683)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L683)'
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.num_labels)`)
    — 分类分数（SoftMax之前）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去传递给此模型的标记ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of `XLNetForTokenClassificationOutput`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`XLNetForTokenClassificationOutput`的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L753)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L753)'
- en: '[PRE17]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 总跨度提取损失是开始和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-start scores (before SoftMax).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,)`) —
    跨度开始分数（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-end scores (before SoftMax).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,)`) — 跨度结束分数（SoftMax之前）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去传递给此模型的标记ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [XLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L790)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L790)'
- en: '[PRE18]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions`
    and `end_positions` are provided) — Classification loss as the sum of start token,
    end token (and is_impossible if provided) classification losses.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，如果提供了`start_positions`和`end_positions`则返回)
    — 分类损失，作为开始标记、结束标记（如果提供）的分类损失之和。'
- en: '`start_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_top_log_probs`（形状为`(batch_size, config.start_n_top)`的`torch.FloatTensor`，*可选*，如果未提供`start_positions`或`end_positions`则返回）-
    顶部`config.start_n_top`开始标记可能性（波束搜索）的对数概率。'
- en: '`start_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_top_index`（形状为`(batch_size, config.start_n_top)`的`torch.LongTensor`，*可选*，如果未提供`start_positions`或`end_positions`则返回）-
    顶部`config.start_n_top`开始标记可能性（波束搜索）的索引。'
- en: '`end_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_top_log_probs`（形状为`(batch_size, config.start_n_top * config.end_n_top)`的`torch.FloatTensor`，*可选*，如果未提供`start_positions`或`end_positions`则返回）-
    顶部`config.start_n_top * config.end_n_top`结束标记可能性（波束搜索）的对数概率。'
- en: '`end_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_top_index`（形状为`(batch_size, config.start_n_top * config.end_n_top)`的`torch.LongTensor`，*可选*，如果未提供`start_positions`或`end_positions`则返回）-
    顶部`config.start_n_top * config.end_n_top`结束标记可能性（波束搜索）的索引。'
- en: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_logits`（形状为`(batch_size,)`的`torch.FloatTensor`，*可选*，如果未提供`start_positions`或`end_positions`则返回）-
    答案的`is_impossible`标签的对数概率。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）- 包含预先计算的隐藏状态。可以用于加速顺序解码（查看`mems`输入）。将其过去给予该模型的标记id不应作为`input_ids`传递，因为它们已经被计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [XLNetForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnswering).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnswering)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L844)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L844)'
- en: '[PRE19]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, num_predict, hidden_size)`)
    — Sequence of hidden-states at the last layer of the model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_predict, hidden_size)`的`tf.Tensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict`对应于`target_mapping.shape[1]`。如果`target_mapping`为`None`，则`num_predict`对应于`sequence_length`。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[tf.Tensor]`）- 包含预先计算的隐藏状态。可以用于加速顺序解码（查看`mems`输入）。将其过去给予该模型的标记id不应作为`input_ids`传递，因为它们已经被计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L878)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L878)'
- en: '[PRE20]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Language modeling loss (for next-token prediction).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`形状为*(1,)*的tf.Tensor`，*可选*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`形状为`(batch_size, num_predict, config.vocab_size)`的tf.Tensor`) —
    语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict`对应于`target_mapping.shape[1]`。如果`target_mapping`为`None`，则`num_predict`对应于`sequence_length`。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`长度为`config.n_layers`的List[tf.Tensor]`) — 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去给定给该模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [TFXLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFXLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L915)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L915)'
- en: '[PRE21]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `label` is provided)
    — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`，形状为`(1,)`，*可选*，当提供`label`时返回) — 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`形状为`(batch_size, config.num_labels)`的tf.Tensor`) — 分类（如果config.num_labels==1则为回归）分数（SoftMax之前）。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`长度为`config.n_layers`的List[tf.Tensor]`) — 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去给定给该模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [TFXLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFXLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L983)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L983)'
- en: '[PRE22]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`形状为`(1,)`的`tf.Tensor`, *optional*, 当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`形状为`(batch_size, num_choices)`的`tf.Tensor`) — *num_choices*是输入张量的第二维度。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类分数（SoftMax之前）。
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`长度为`config.n_layers`的`List[tf.Tensor]`) — 包含预先计算的隐藏状态。可用于加速顺序解码。将其过去传递给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [TFXLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFXLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice)的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput`'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L949)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L949)'
- en: '[PRE23]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`形状为`(1,)`的`tf.Tensor`, *optional*, 当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`形状为`(batch_size, sequence_length, config.num_labels)`的`tf.Tensor`)
    — 分类分数（SoftMax之前）。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`长度为`config.n_layers`的`List[tf.Tensor]`) — 包含预先计算的隐藏状态。可用于加速顺序解码。将其过去传递给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of `TFXLNetForTokenClassificationOutput`.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`TFXLNetForTokenClassificationOutput`的输出类型。'
- en: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput`'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1019)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1019)'
- en: '[PRE24]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Total span extraction loss is the sum of a Cross-Entropy for the start
    and end positions.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`形状为`(1,)`的`tf.Tensor`, *optional*, 当提供`labels`时返回) — 总跨度提取损失是起始和结束位置的交叉熵之和。'
- en: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length,)`) — Span-start
    scores (before SoftMax).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`形状为`(batch_size, sequence_length,)`的`tf.Tensor`) — 跨度起始分数（SoftMax之前）。'
- en: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length,)`) — Span-end
    scores (before SoftMax).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`形状为`(batch_size, sequence_length,)`的`tf.Tensor`) — 跨度结束分数（SoftMax之前）。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`长度为`config.n_layers`的`List[tf.Tensor]`) — 包含预先计算的隐藏状态。可用于加速顺序解码。将其过去传递给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True`
    时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True`
    时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor`
    元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of [TFXLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFXLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple)
    的输出类型。'
- en: PytorchHide Pytorch content
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch 内容
- en: XLNetModel
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetModel
- en: '### `class transformers.XLNetModel`'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L927)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L927)'
- en: '[PRE25]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare XLNet Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的 XLNet 模型变压器输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1059)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1059)'
- en: '[PRE26]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`） — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。查看 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    以获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.FloatTensor`，*可选*）
    — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被 `masked` 的标记为 1，
- en: 0 for tokens that are `masked`.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被 `masked` 的标记为 0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为 `config.n_layers` 的 `List[torch.FloatTensor]`） — 包含预先计算的隐藏状态（参见下面的
    `mems` 输出）。可用于加速顺序解码。将其过去传递给此模型的标记 id 不应作为 `input_ids` 传递，因为它们已经计算过了。'
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`use_mems` 必须设置为 `True` 才能使用 `mems`。'
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perm_mask`（形状为 `(batch_size, sequence_length, sequence_length)` 的 `torch.FloatTensor`，*可选*）
    — 用于指示每个输入标记的注意力模式的掩码，值选在 `[0, 1]`：'
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `perm_mask[k, i, j] = 0`，则在批次 k 中我关注 j；
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `perm_mask[k, i, j] = 1`，则在批次 k 中 i 不关注 j。
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置，每个标记都会关注其他所有标记（完全双向注意力）。仅在预训练期间（用于定义分解顺序）或用于顺序解码（生成）时使用。
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_mapping`（形状为`(batch_size, num_predict, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于指示要使用的输出标记的掩码。如果`target_mapping[k, i, j] = 1`，则第k批次中的第i个预测位于第j个标记上。仅在预训练期间用于部分预测或用于顺序解码（生成）。'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应一个*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应一个*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_mask`（形状为`batch_size, sequence_length`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。负的`attention_mask`，即对于真实标记为0，对于填充为1，这保留了与原始代码库的兼容性。'
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择的掩码值在`[0, 1]`中：
- en: 1 for tokens that are `masked`,
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于被`masked`的标记，
- en: 0 for tokens that are `not masked`.
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于未被`masked`的标记。
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您只能使用`input_mask`和`attention_mask`中的一个。
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_predict, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict` 对应于`target_mapping.shape[1]`。如果`target_mapping`为`None`，则`num_predict`对应于`sequence_length`。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）- 包含预先计算的隐藏状态。可以用于加速顺序解码（请参见`mems`输入）。将其过去传递给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    forward method, overrides the `__call__` special method.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默忽略它们。
- en: 'Example:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE27]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: XLNetLMHeadModel
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetLMHeadModel
- en: '### `class transformers.XLNetLMHeadModel`'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetLMHeadModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1288)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1288)'
- en: '[PRE28]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: XLNet Model with a language modeling head on top (linear layer with weights
    tied to the input embeddings).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有语言建模头的XLNet模型（线性层，权重与输入嵌入绑定）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1356)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1356)'
- en: '[PRE29]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)以获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）
    — 避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`not masked`的标记为1。
- en: 0 for tokens that are `masked`.
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`） — 包含预先计算的隐藏状态（参见下面的`mems`输出）。可用于加速顺序解码。将其过去传递给此模型的标记ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`use_mems`必须设置为`True`才能使用`mems`。'
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perm_mask`（形状为`(batch_size, sequence_length, sequence_length)`的`torch.FloatTensor`，*可选*）
    — 用于指示每个输入标记的注意力模式的掩码，值在`[0, 1]`中选择：'
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`perm_mask[k, i, j] = 0`，则在批次k中，i参与j；
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`perm_mask[k, i, j] = 1`，则在批次k中，i不参与j。
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, num_predict)`, *optional*)
    — Labels for masked language modeling. `num_predict` corresponds to `target_mapping.shape[1]`.
    If `target_mapping` is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The labels should correspond to the masked input words that should be predicted
    and depends on `target_mapping`. Note in order to perform standard auto-regressive
    language modeling a *<mask></mask>*token has to be added to the `input_ids` (see
    the `prepare_inputs_for_generation` function and examples below)
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to
    `-100` are ignored, the loss is only computed for labels in `[0, ..., config.vocab_size]`
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) Language modeling loss (for next-token prediction).'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, num_predict, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict` 对应于 `target_mapping.shape[1]`。如果 `target_mapping` 是 `None`，那么
    `num_predict` 对应于 `sequence_length`。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为 `config.n_layers`) — 包含预先计算的隐藏状态。可以用于加速顺序解码。将其过去传递给此模型的标记id不应作为
    `input_ids` 传递，因为它们已经计算过了。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '模型在每个层的输出以及初始嵌入输出的隐藏状态。 '
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [XLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE30]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: XLNetForSequenceClassification
  id: totrans-467
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetForSequenceClassification
- en: '### `class transformers.XLNetForSequenceClassification`'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1493)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1493)'
- en: '[PRE31]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: XLNet Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部具有序列分类/回归头的XLNet模型（在汇总输出的顶部有一个线性层），例如用于GLUE任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1513)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1513)'
- en: '[PRE32]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被 `masked` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-485
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `masked` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为 `config.n_layers` 的 `List[torch.FloatTensor]`）— 包含预先计算的隐藏状态（参见下面的
    `mems` 输出）。可用于加速顺序解码。将其过去传递给此模型的标记 id 不应作为 `input_ids` 传递，因为它们已经计算过。'
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`use_mems` 必须设置为 `True` 才能使用 `mems`。'
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perm_mask`（形状为 `(batch_size, sequence_length, sequence_length)` 的 `torch.FloatTensor`，*可选*）—
    用于指示每个输入标记的注意力模式的掩码，值在 `[0, 1]` 中选择：'
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `perm_mask[k, i, j] = 0`，则在批次 k 中，i 关注 j；
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `perm_mask[k, i, j] = 1`，则在批次 k 中，i 不关注 j。
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置，则每个标记都关注所有其他标记（完全双向注意力）。仅在预训练期间（用于定义分解顺序）或用于顺序解码（生成）时使用。
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_mapping`（形状为 `(batch_size, num_predict, sequence_length)` 的 `torch.FloatTensor`，*可选*）—
    用于指示要使用的输出标记的掩码。如果 `target_mapping[k, i, j] = 1`，则批次 k 中的第 i 个预测位于第 j 个标记上。仅在预训练期间用于部分预测或用于顺序解码（生成）。'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *sentence A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *sentence B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_mask`（形状为 `batch_size, sequence_length` 的 `torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。`attention_mask` 的负值，即对于真实标记为 0，对于填充为 1，这是为了与原始代码库保持兼容性。'
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 掩码值在 `[0, 1]` 中选择：
- en: 1 for tokens that are `masked`,
  id: totrans-500
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示被 `masked` 的标记，
- en: 0 for tokens that are `not masked`.
  id: totrans-501
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示未被 `masked` 的标记。
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您只能使用 `input_mask` 和 `attention_mask` 中的一个。
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为 `(num_heads,)` 或 `(num_layers, num_heads)` 的 `torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-504
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-505
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为 `(batch_size,)` 的 `torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 中。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果
    `config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`label`时返回）- 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（如果config.num_labels==1则为回归）分数（SoftMax之前）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（`List[torch.FloatTensor]`，长度为`config.n_layers`）- 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去传递给该模型的令牌id不应作为`input_ids`传递，因为它们已经被计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出处的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [XLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification)前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类示例：
- en: '[PRE33]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Example of multi-label classification:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类示例：
- en: '[PRE34]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: XLNetForMultipleChoice
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetForMultipleChoice
- en: '### `class transformers.XLNetForMultipleChoice`'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetForMultipleChoice`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1689)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1689)'
- en: '[PRE35]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: XLNet Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RACE/SWAG tasks.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部具有多选分类头的XLNet模型（池化输出上的线性层和softmax），例如用于RACE/SWAG任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1707)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1707)'
- en: '[PRE36]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`）-
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, num_choices, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-544
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“masked”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-545
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）- 包含预先计算的隐藏状态（参见下面的`mems`输出）。可用于加速顺序解码。将其过去提供给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`use_mems`必须设置为`True`才能使用`mems`。'
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perm_mask`（形状为`(batch_size, sequence_length, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于指示每个输入标记的注意力模式的掩码，选择的值在`[0, 1]`中：'
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-550
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`perm_mask[k, i, j] = 0`，则在批次k中，i参与j；
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-551
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`perm_mask[k, i, j] = 1`，则在批次k中，i不参与j。
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置，则每个标记都会关注其他所有标记（完全双向注意力）。仅在预训练期间（用于定义分解顺序）或用于顺序解码（生成）时使用。
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_mapping`（形状为`(batch_size, num_predict, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于指示要使用的输出标记的掩码。如果`target_mapping[k, i, j] = 1`，则批次k中的第i个预测在第j个标记上。仅在预训练期间用于部分预测或用于顺序解码（生成）。'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-555
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-556
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-557
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, num_choices, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_mask`（形状为`batch_size, num_choices, sequence_length`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。`attention_mask`的负值，即对于真实标记为0，对于填充为1，这是为了与原始代码库保持兼容性。'
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`[0, 1]`中选择的掩码值：
- en: 1 for tokens that are `masked`,
  id: totrans-560
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”的标记为1，
- en: 0 for tokens that are `not masked`.
  id: totrans-561
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“masked”的标记为0。
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您只能使用`input_mask`和`attention_mask`中的一个。
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 indicates the head is `not masked`,
  id: totrans-564
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“masked”，
- en: 0 indicates the head is `masked`.
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“masked”。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, num_choices, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices-1]` where `num_choices` is the size of the second dimension of
    the input tensors. (See `input_ids` above)'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算多项选择分类损失的标签。索引应在`[0,
    ..., num_choices-1]`范围内，其中`num_choices`是输入张量的第二维的大小。（见上面的`input_ids`）'
- en: Returns
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为*(1,)*的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`）— *num_choices*是输入张量的第二维。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SoftMax之前的分类分数。
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）— 包含预计算的隐藏状态。可以用于加速顺序解码。将过去给定给该模型的令牌id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出处的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [XLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: XLNetForTokenClassification
  id: totrans-586
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetForTokenClassification
- en: '### `class transformers.XLNetForTokenClassification`'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1602)'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1602)'
- en: '[PRE38]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: XLNet Model with a token classification head on top (a linear layer on top of
    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有令牌分类头的XLNet模型（隐藏状态输出的顶部线性层），例如用于命名实体识别（NER）任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头部等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '`前进`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1620)'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1620)'
- en: '[PRE39]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）— 包含预先计算的隐藏状态（参见下面的`mems`输出）。可用于加速顺序解码。将其过去传递给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 必须将`use_mems`设置为`True`才能使用`mems`。
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perm_mask`（形状为`(batch_size, sequence_length, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于指示每个输入标记的注意力模式的掩码，值在`[0, 1]`中选择：'
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-609
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`perm_mask[k, i, j] = 0`，则第k批次中的i关注j；
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-610
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`perm_mask[k, i, j] = 1`，则第k批次中的i不会关注j。
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置，则每个标记都关注所有其他标记（完全双向注意力）。仅在预训练期间（用于定义分解顺序）或用于顺序解码（生成）时使用。
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_mapping`（形状为`(batch_size, num_predict, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于指示要使用的输出标记的掩码。如果`target_mapping[k, i, j] = 1`，则第k批次中第i个预测位于第j个标记上。仅在预训练期间用于部分预测或用于顺序解码（生成）。'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-614
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*句子A*的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-615
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*句子B*的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_mask`（形状为`batch_size, sequence_length`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。`attention_mask`的负值，即对于真实标记为0，对于填充标记为1，这是为了与原始代码库保持兼容性。'
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 掩码值在`[0, 1]`中选择：
- en: 1 for tokens that are `masked`,
  id: totrans-619
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示被`masked`的标记，
- en: 0 for tokens that are `not masked`.
  id: totrans-620
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示未被`masked`的标记。
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您只能使用`input_mask`和`attention_mask`中的一个。
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-623
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-624
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices]` where *num_choices* is the size of the second dimension of
    the input tensors. (see *input_ids* above)'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算多项选择分类损失的标签。索引应在`[0,
    ..., num_choices]`范围内，其中*num_choices*是输入张量第二维的大小（参见上面的*input_ids*）。'
- en: Returns
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）—
    分类分数（SoftMax之前）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[torch.FloatTensor]`）— 包含预先计算的隐藏状态。可以用于加速顺序解码（参见`mems`输入）。将其过去给予该模型的标记id不应作为`input_ids`传递，因为它们已经被计算。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [XLNetForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForTokenClassification)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE40]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: XLNetForQuestionAnsweringSimple
  id: totrans-644
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNetForQuestionAnsweringSimple
- en: '### `class transformers.XLNetForQuestionAnsweringSimple`'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.XLNetForQuestionAnsweringSimple`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1792)'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1792)'
- en: '[PRE41]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1810)'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-659
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-661
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-662
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-667
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-668
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-672
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-673
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-676
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-677
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-678
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-679
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-681
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-682
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-start scores (before SoftMax).'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-end scores (before SoftMax).'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-697
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-699
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple)
    forward method, overrides the `__call__` special method.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: XLNetForQuestionAnswering
  id: totrans-704
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.XLNetForQuestionAnswering`'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1902)'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1923)'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-719
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-721
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-722
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-723
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-727
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-728
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-732
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-733
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-734
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-736
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-737
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-738
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-739
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-741
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-742
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_impossible` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels whether a question has an answer or no answer (SQuAD 2.0)'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_index` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the classification token to use as input for computing
    plausibility of the answer.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Optional mask of tokens which can’t be in answers (e.g. [CLS], [PAD], …). 1.0
    means token should be masked. 0.0 mean token is not masked.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions`
    and `end_positions` are provided) — Classification loss as the sum of start token,
    end token (and is_impossible if provided) classification losses.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_top_log_probs` (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_top_index` (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-763
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-765
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: TensorFlowHide TensorFlow content
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: TFXLNetModel
  id: totrans-771
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFXLNetModel`'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1171)'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-774
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare XLNet Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1180)'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-790
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-793
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-794
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-796
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-797
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-798
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-800
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-802
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-803
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-804
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-807
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-808
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-809
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-811
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-812
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-813
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-816
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-817
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, num_predict, hidden_size)`)
    — Sequence of hidden-states at the last layer of the model.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel)
    forward method, overrides the `__call__` special method.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: TFXLNetLMHeadModel
  id: totrans-836
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFXLNetLMHeadModel`'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1232)'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a language modeling head on top (linear layer with weights
    tied to the input embeddings).
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1292)'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-861
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-862
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-865
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-867
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-868
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-872
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-873
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-874
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-876
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-877
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-878
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-881
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-882
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the cross entropy classification loss. Indices should be
    in `[0, ..., config.vocab_size - 1]`.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) Language modeling loss (for next-token prediction).'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-893
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-896
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-898
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: TFXLNetForSequenceClassification
  id: totrans-903
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFXLNetForSequenceClassification`'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1402)'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1423)'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-922
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-925
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-926
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-928
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-929
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-930
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-932
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-934
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-935
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-936
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-939
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-940
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-941
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-943
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-944
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-945
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-946
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-948
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-949
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for computing
    the sequence classification/regression loss. Indices should be in `[0, ..., config.num_labels
    - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square
    loss), If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `label` is provided)
    — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-962
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-964
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-968
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: TFLNetForMultipleChoice
  id: totrans-970
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFXLNetForMultipleChoice`'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1504)'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Parameters
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNET Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1524)'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-992
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-993
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-995
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-996
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-997
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-999
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-1001
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-1002
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-1003
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-1006
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-1007
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-1008
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, num_choices, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-1010
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-1011
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-1012
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-1013
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-1015
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-1016
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for computing
    the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`
    where `num_choices` is the size of the second dimension of the input tensors.
    (See `input_ids` above)'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax).
  id: totrans-1027
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-1030
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-1032
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: TFXLNetForTokenClassification
  id: totrans-1037
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFXLNetForTokenClassification`'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1620)'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-1040
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a token classification head on top (a linear layer on top of
    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1638)'
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-1056
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Parameters
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1059
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1060
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-1062
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-1063
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-1064
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-1066
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-1068
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-1069
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-1070
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-1073
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-1074
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-1075
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-1077
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-1078
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-1079
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-1080
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-1082
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-1083
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-1096
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-1098
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-1102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-1103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: TFXLNetForQuestionAnsweringSimple
  id: totrans-1104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFXLNetForQuestionAnsweringSimple`'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1712)'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-1107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Parameters
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1728)'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-1123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-1129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-1130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-1131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (see `mems` output below) . Can be used to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  id: totrans-1133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`perm_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length)`,
    *optional*) — Mask to indicate the attention pattern for each input token with
    values selected in `[0, 1]`:'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  id: totrans-1135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  id: totrans-1136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  id: totrans-1137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`target_mapping` (`torch.FloatTensor` of shape `(batch_size, num_predict, sequence_length)`,
    *optional*) — Mask to indicate the output tokens to use. If `target_mapping[k,
    i, j] = 1`, the i-th predict in batch k is on the j-th token. Only used during
    pretraining for partial prediction or for sequential decoding (generation).'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-1140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-1141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-1142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*)
    — Mask to avoid performing attention on padding token indices. Negative of `attention_mask`,
    i.e. with 0 for real tokens and 1 for padding which is kept for compatibility
    with the original code base.'
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  id: totrans-1144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are `masked`,
  id: totrans-1145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not masked`.
  id: totrans-1146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  id: totrans-1147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-1149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-1150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the start of the labelled span for computing the token
    classification loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the end of the labelled span for computing the token classification
    loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Total span extraction loss is the sum of a Cross-Entropy for the start
    and end positions.'
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length,)`) — Span-start
    scores (before SoftMax).'
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length,)`) — Span-end
    scores (before SoftMax).'
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  id: totrans-1163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-1164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-1165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-1166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-1167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple)
    forward method, overrides the `__call__` special method.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-1172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
