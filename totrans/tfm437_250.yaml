- en: XLNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlnet)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/304.90df6459.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">[![Models](../Images/09fb4ec5e9dc49c97e92796ae9d5a2d4.png)](https://huggingface.co/models?filter=xlnet)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/xlnet-base-cased)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining
    for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang,
    Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. XLnet
    is an extension of the Transformer-XL model pre-trained using an autoregressive
    method to learn bidirectional contexts by maximizing the expected likelihood over
    all permutations of the input sequence factorization order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*With the capability of modeling bidirectional contexts, denoising autoencoding
    based pretraining like BERT achieves better performance than pretraining approaches
    based on autoregressive language modeling. However, relying on corrupting the
    input with masks, BERT neglects dependency between the masked positions and suffers
    from a pretrain-finetune discrepancy. In light of these pros and cons, we propose
    XLNet, a generalized autoregressive pretraining method that (1) enables learning
    bidirectional contexts by maximizing the expected likelihood over all permutations
    of the factorization order and (2) overcomes the limitations of BERT thanks to
    its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL,
    the state-of-the-art autoregressive model, into pretraining. Empirically, under
    comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a
    large margin, including question answering, natural language inference, sentiment
    analysis, and document ranking.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The
    original code can be found [here](https://github.com/zihangdai/xlnet/).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The specific attention pattern can be controlled at training and test time using
    the `perm_mask` input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the difficulty of training a fully auto-regressive model over various
    factorization order, XLNet is pretrained using only a sub-set of the output tokens
    as target which are selected with the `target_mapping` input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use XLNet for sequential decoding (i.e. not in fully bi-directional setting),
    use the `perm_mask` and `target_mapping` inputs to control the attention span
    and outputs (see examples in *examples/pytorch/text-generation/run_generation.py*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet is one of the few models that has no sequence length limit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet is not a traditional autoregressive model but uses a training strategy
    that builds on that. It permutes the tokens in the sentence, then allows the model
    to use the last n tokens to predict the token n+1\. Since this is all done with
    a mask, the sentence is actually fed in the model in the right order, but instead
    of masking the first n tokens for n+1, XLNet uses a mask that hides the previous
    tokens in some given permutation of 1,…,sequence length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet also uses the same recurrence mechanism as Transformer-XL to build long-term
    dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text classification task guide](../tasks/sequence_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Token classification task guide](../tasks/token_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Question answering task guide](../tasks/question_answering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNetConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/configuration_xlnet.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_size = 32000 d_model = 1024 n_layer = 24 n_head = 16 d_inner = 4096
    ff_activation = 'gelu' untie_r = True attn_type = 'bi' initializer_range = 0.02
    layer_norm_eps = 1e-12 dropout = 0.1 mem_len = 512 reuse_len = None use_mems_eval
    = True use_mems_train = False bi_data = False clamp_len = -1 same_length = False
    summary_type = 'last' summary_use_proj = True summary_activation = 'tanh' summary_last_dropout
    = 0.1 start_n_top = 5 end_n_top = 5 pad_token_id = 5 bos_token_id = 1 eos_token_id
    = 2 **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 32000) — Vocabulary size of
    the XLNet model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    or [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d_model** (`int`, *optional*, defaults to 1024) — Dimensionality of the encoder
    layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_layer** (`int`, *optional*, defaults to 24) — Number of hidden layers in
    the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_head** (`int`, *optional*, defaults to 16) — Number of attention heads
    for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d_inner** (`int`, *optional*, defaults to 4096) — Dimensionality of the “intermediate”
    (often named feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ff_activation** (`str` or `Callable`, *optional*, defaults to `"gelu"`) —
    The non-linear activation function (function or string) in the If string, `"gelu"`,
    `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**untie_r** (`bool`, *optional*, defaults to `True`) — Whether or not to untie
    relative position biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attn_type** (`str`, *optional*, defaults to `"bi"`) — The attention type
    used by the model. Set `"bi"` for XLNet, `"uni"` for Transformer-XL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout** (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mem_len** (`int` or `None`, *optional*) — The number of tokens to cache.
    The key/value pairs that have already been pre-computed in a previous forward
    pass won’t be re-computed. See the [quickstart](https://huggingface.co/transformers/quickstart.html#using-the-past)
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reuse_len** (`int`, *optional*) — The number of tokens in the current batch
    to be cached and reused in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bi_data** (`bool`, *optional*, defaults to `False`) — Whether or not to use
    bidirectional input pipeline. Usually set to `True` during pretraining and `False`
    during finetuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clamp_len** (`int`, *optional*, defaults to -1) — Clamp all relative distances
    larger than clamp_len. Setting this attribute to -1 means no clamping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**same_length** (`bool`, *optional*, defaults to `False`) — Whether or not
    to use the same attention length for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_type** (`str`, *optional*, defaults to “last”) — Argument used when
    doing sequence summary. Used in the sequence classification and multiple choice
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Has to be one of the following options:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"last"`: Take the last token hidden state (like XLNet).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"first"`: Take the first token hidden state (like BERT).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"mean"`: Take the mean of all tokens hidden states.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"cls_index"`: Supply a Tensor of classification token position (like GPT/GPT-2).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"attn"`: Not implemented now, use multi-head attention.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**summary_use_proj** (`bool`, *optional*, defaults to `True`) — Argument used
    when doing sequence summary. Used in the sequence classification and multiple
    choice models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not to add a projection after the vector extraction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**summary_activation** (`str`, *optional*) — Argument used when doing sequence
    summary. Used in the sequence classification and multiple choice models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass `"tanh"` for a tanh activation to the output, any other value will result
    in no activation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**summary_proj_to_labels** (`boo`, *optional*, defaults to `True`) — Used in
    the sequence classification and multiple choice models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the projection outputs should have `config.num_labels` or `config.hidden_size`
    classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**summary_last_dropout** (`float`, *optional*, defaults to 0.1) — Used in the
    sequence classification and multiple choice models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dropout ratio to be used after the projection and activation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**start_n_top** (`int`, *optional*, defaults to 5) — Used in the SQuAD evaluation
    script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_n_top** (`int`, *optional*, defaults to 5) — Used in the SQuAD evaluation
    script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_mems_eval** (`bool`, *optional*, defaults to `True`) — Whether or not
    the model should make use of the recurrent memory mechanism in evaluation mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_mems_train** (`bool`, *optional*, defaults to `False`) — Whether or not
    the model should make use of the recurrent memory mechanism in train mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For pretraining, it is recommended to set `use_mems_train` to `True`. For fine-tuning,
    it is recommended to set `use_mems_train` to `False` as discussed [here](https://github.com/zihangdai/xlnet/issues/41#issuecomment-505102587).
    If `use_mems_train` is set to `True`, one has to make sure that the train batches
    are correctly pre-processed, *e.g.* `batch_1 = [[This line is], [This is the]]`
    and `batch_2 = [[ the first line], [ second line]]` and that all batches are of
    equal size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    or a [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel).
    It is used to instantiate a XLNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the [xlnet-large-cased](https://huggingface.co/xlnet-large-cased)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: XLNetTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L53)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vocab_file do_lower_case = False remove_space = True keep_accents = False
    bos_token = ''<s>'' eos_token = ''</s>'' unk_token = ''<unk>'' sep_token = ''<sep>''
    pad_token = ''<pad>'' cls_token = ''<cls>'' mask_token = ''<mask>'' additional_special_tokens
    = [''<eop>'', ''<eod>''] sp_model_kwargs: Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .spm extension) that contains the vocabulary necessary to
    instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_lower_case** (`bool`, *optional*, defaults to `False`) — Whether to lowercase
    the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_space** (`bool`, *optional*, defaults to `True`) — Whether to strip
    the text when tokenizing (removing excess spaces before and after the string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_accents** (`bool`, *optional*, defaults to `False`) — Whether to keep
    accents when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token** (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sep_token** (`str`, *optional*, defaults to `"<sep>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token** (`str`, *optional*, defaults to `"<cls>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask_token** (`str`, *optional*, defaults to `"<mask>"`) — The token used
    for masking values. This is the token used when training this model with masked
    language modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**additional_special_tokens** (`List[str]`, *optional*, defaults to `[''<eop>'',
    ''<eod>'']`) — Additional special tokens used by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model_kwargs** (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model** (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct an XLNet tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L298)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLNet sequence has the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `X <sep> <cls>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair of sequences: `A <sep> B <sep> <cls>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L323)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    bool = False ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) —
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L351)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLNet
  prefs: []
  type: TYPE_NORMAL
- en: 'sequence pair mask has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet.py#L381)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: str filename_prefix: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: XLNetTokenizerFast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetTokenizerFast'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L63)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_file = None tokenizer_file = None do_lower_case = False remove_space
    = True keep_accents = False bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>'
    sep_token = '<sep>' pad_token = '<pad>' cls_token = '<cls>' mask_token = '<mask>'
    additional_special_tokens = ['<eop>', '<eod>'] **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .spm extension) that contains the vocabulary necessary to
    instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_lower_case** (`bool`, *optional*, defaults to `True`) — Whether to lowercase
    the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_space** (`bool`, *optional*, defaults to `True`) — Whether to strip
    the text when tokenizing (removing excess spaces before and after the string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_accents** (`bool`, *optional*, defaults to `False`) — Whether to keep
    accents when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token** (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sep_token** (`str`, *optional*, defaults to `"<sep>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token** (`str`, *optional*, defaults to `"<cls>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask_token** (`str`, *optional*, defaults to `"<mask>"`) — The token used
    for masking values. This is the token used when training this model with masked
    language modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**additional_special_tokens** (`List[str]`, *optional*, defaults to `["<eop>",
    "<eod>"]`) — Additional special tokens used by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model** (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a “fast” XLNet tokenizer (backed by HuggingFace’s *tokenizers* library).
    Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L177)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLNet sequence has the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `X <sep> <cls>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair of sequences: `A <sep> B <sep> <cls>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/tokenization_xlnet_fast.py#L202)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLNet
  prefs: []
  type: TYPE_NORMAL
- en: 'sequence pair mask has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  prefs: []
  type: TYPE_NORMAL
- en: XLNet specific outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L578)'
  prefs: []
  type: TYPE_NORMAL
- en: '( last_hidden_state: FloatTensor mems: Optional = None hidden_states: Optional
    = None attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L612)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states:
    Optional = None attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [XLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L649)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states:
    Optional = None attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [XLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L717)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states:
    Optional = None attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [XLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L683)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states:
    Optional = None attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of `XLNetForTokenClassificationOutput`.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L753)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None start_logits: FloatTensor = None end_logits: FloatTensor
    = None mems: Optional = None hidden_states: Optional = None attentions: Optional
    = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-start scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-end scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [XLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L790)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None start_top_log_probs: Optional = None start_top_index:
    Optional = None end_top_log_probs: Optional = None end_top_index: Optional = None
    cls_logits: Optional = None mems: Optional = None hidden_states: Optional = None
    attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both
    `start_positions` and `end_positions` are provided) — Classification loss as the
    sum of start token, end token (and is_impossible if provided) classification losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_logits** (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [XLNetForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnswering).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L844)'
  prefs: []
  type: TYPE_NORMAL
- en: '( last_hidden_state: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states:
    Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`tf.Tensor` of shape `(batch_size, num_predict, hidden_size)`)
    — Sequence of hidden-states at the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L878)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor]
    | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor]
    | None = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [TFXLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L915)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor]
    | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor]
    | None = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `label` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [TFXLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L983)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor]
    | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor]
    | None = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [TFXLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L949)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor]
    | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor]
    | None = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of `TFXLNetForTokenClassificationOutput`.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1019)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: tf.Tensor | None = None start_logits: tf.Tensor = None end_logits:
    tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor]
    | None = None attentions: Tuple[tf.Tensor] | None = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Total span extraction loss is the sum of a Cross-Entropy for the start
    and end positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_logits** (`tf.Tensor` of shape `(batch_size, sequence_length,)`) —
    Span-start scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_logits** (`tf.Tensor` of shape `(batch_size, sequence_length,)`) — Span-end
    scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output type of [TFXLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple).
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: XLNetModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L927)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare XLNet Model transformer outputting raw hidden-states without any specific
    head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1059)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None mems: Optional
    = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids:
    Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: XLNetLMHeadModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetLMHeadModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1288)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a language modeling head on top (linear layer with weights
    tied to the input embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1356)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None mems: Optional
    = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids:
    Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None labels: Optional = None use_mems: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, num_predict)`, *optional*)
    — Labels for masked language modeling. `num_predict` corresponds to `target_mapping.shape[1]`.
    If `target_mapping` is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The labels should correspond to the masked input words that should be predicted
    and depends on `target_mapping`. Note in order to perform standard auto-regressive
    language modeling a *<mask></mask>*token has to be added to the `input_ids` (see
    the `prepare_inputs_for_generation` function and examples below)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to
    `-100` are ignored, the loss is only computed for labels in `[0, ..., config.vocab_size]`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: XLNetForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetForSequenceClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1493)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1513)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None mems: Optional
    = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids:
    Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None labels: Optional = None use_mems: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example of single-label classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of multi-label classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: XLNetForMultipleChoice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetForMultipleChoice'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1689)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RACE/SWAG tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1707)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None token_type_ids: Optional = None input_mask: Optional
    = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional
    = None target_mapping: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None labels: Optional = None use_mems: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Segment token indices to indicate first and second
    portions of the inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, num_choices, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, num_choices,
    sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices-1]` where `num_choices` is the size of the second dimension of
    the input tensors. (See `input_ids` above)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: XLNetForTokenClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetForTokenClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1602)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a token classification head on top (a linear layer on top of
    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1620)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None mems: Optional
    = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids:
    Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None labels: Optional = None use_mems: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices]` where *num_choices* is the size of the second dimension of
    the input tensors. (see *input_ids* above)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForTokenClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: XLNetForQuestionAnsweringSimple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetForQuestionAnsweringSimple'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1792)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1810)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None mems: Optional
    = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids:
    Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None start_positions: Optional = None end_positions: Optional = None
    use_mems: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-start scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length,)`)
    — Span-end scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: XLNetForQuestionAnswering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.XLNetForQuestionAnswering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1902)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_xlnet.py#L1923)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None mems: Optional
    = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids:
    Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds:
    Optional = None start_positions: Optional = None end_positions: Optional = None
    is_impossible: Optional = None cls_index: Optional = None p_mask: Optional = None
    use_mems: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None **kwargs ) → [transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_impossible** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels whether a question has an answer or no answer (SQuAD 2.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_index** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the classification token to use as input for computing
    plausibility of the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Optional mask of tokens which can’t be in answers (e.g. [CLS], [PAD], …). 1.0
    means token should be masked. 0.0 mean token is not masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both
    `start_positions` and `end_positions` are provided) — Classification loss as the
    sum of start token, end token (and is_impossible if provided) classification losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Log probabilities for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`,
    *optional*, returned if `start_positions` or `end_positions` is not provided)
    — Indices for the top config.start_n_top start token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_log_probs** (`torch.FloatTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Log probabilities for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_top_index** (`torch.LongTensor` of shape `(batch_size, config.start_n_top
    * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions`
    is not provided) — Indices for the top `config.start_n_top * config.end_n_top`
    end token possibilities (beam-search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_logits** (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned
    if `start_positions` or `end_positions` is not provided) — Log probabilities for
    the `is_impossible` label of the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states. Can be used (see `mems` input) to speed up sequential
    decoding. The token ids which have their past given to this model should not be
    passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [XLNetForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: TFXLNetModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFXLNetModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1171)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config *inputs **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare XLNet Model transformer outputting raw hidden-states without any specific
    head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1180)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor
    | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray
    | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None
    token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray |
    tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds:
    np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions:
    Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict:
    Optional[bool] = None training: bool = False ) → [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`tf.Tensor` of shape `(batch_size, num_predict, hidden_size)`)
    — Sequence of hidden-states at the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: TFXLNetLMHeadModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFXLNetLMHeadModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1232)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config *inputs **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a language modeling head on top (linear layer with weights
    tied to the input embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1292)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor
    | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray
    | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None
    token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray |
    tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds:
    np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions:
    Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict:
    Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool
    = False ) → [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the cross entropy classification loss. Indices should be
    in `[0, ..., config.vocab_size - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, num_predict, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: TFXLNetForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFXLNetForSequenceClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1402)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config *inputs **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1423)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor
    | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray
    | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None
    token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray |
    tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds:
    np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions:
    Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict:
    Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool
    = False ) → [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for
    computing the sequence classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `label` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: TFLNetForMultipleChoice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFXLNetForMultipleChoice'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1504)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config *inputs **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNET Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1524)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: TFModelInputType | None = None token_type_ids: np.ndarray | tf.Tensor
    | None = None input_mask: np.ndarray | tf.Tensor | None = None attention_mask:
    np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None
    perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor
    | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray
    | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool]
    = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool]
    = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) →
    [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Segment token indices to indicate first and second
    portions of the inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, num_choices, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, num_choices,
    sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for
    computing the multiple choice classification loss. Indices should be in `[0, ...,
    num_choices]` where `num_choices` is the size of the second dimension of the input
    tensors. (See `input_ids` above)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: TFXLNetForTokenClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFXLNetForTokenClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1620)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config *inputs **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a token classification head on top (a linear layer on top of
    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1638)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor
    | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray
    | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None
    token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray |
    tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds:
    np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions:
    Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict:
    Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool
    = False ) → [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: TFXLNetForQuestionAnsweringSimple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFXLNetForQuestionAnsweringSimple'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1712)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config *inputs **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLNet Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlnet/modeling_tf_xlnet.py#L1728)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor
    | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray
    | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None
    token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray |
    tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds:
    np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions:
    Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict:
    Optional[bool] = None start_positions: np.ndarray | tf.Tensor | None = None end_positions:
    np.ndarray | tf.Tensor | None = None training: bool = False ) → [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**mems** (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains
    pre-computed hidden-states (see `mems` output below) . Can be used to speed up
    sequential decoding. The token ids which have their past given to this model should
    not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mems` has to be set to `True` to make use of `mems`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**perm_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length)`, *optional*) — Mask to indicate the attention pattern for each
    input token with values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 0`, i attend to j in batch k;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if `perm_mask[k, i, j] = 1`, i does not attend to j in batch k.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not set, each token attends to all the others (full bidirectional attention).
    Only used during pretraining (to define factorization order) or for sequential
    decoding (generation).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**target_mapping** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    sequence_length)`, *optional*) — Mask to indicate the output tokens to use. If
    `target_mapping[k, i, j] = 1`, the i-th predict in batch k is on the j-th token.
    Only used during pretraining for partial prediction or for sequential decoding
    (generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**input_mask** (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Negative
    of `attention_mask`, i.e. with 0 for real tokens and 1 for padding which is kept
    for compatibility with the original code base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for tokens that are **masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **not masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only uses one of `input_mask` and `attention_mask`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the start of the labelled span for computing the token
    classification loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_positions** (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the end of the labelled span for computing the token classification
    loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([XLNetConfig](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Total span extraction loss is the sum of a Cross-Entropy for the start
    and end positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_logits** (`tf.Tensor` of shape `(batch_size, sequence_length,)`) —
    Span-start scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_logits** (`tf.Tensor` of shape `(batch_size, sequence_length,)`) — Span-end
    scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mems** (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states. Can be used (see `mems` input) to speed up sequential decoding.
    The token ids which have their past given to this model should not be passed as
    `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFXLNetForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
