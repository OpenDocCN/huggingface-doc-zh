["```py\n>>> from datasets import load_dataset, Audio\n>>> from transformers import EncodecModel, AutoProcessor\n>>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n>>> processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n>>> librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n>>> audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n>>> inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n\n>>> encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n>>> audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n>>> # or the equivalent with a forward pass\n>>> audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n```", "```py\n( target_bandwidths = [1.5, 3.0, 6.0, 12.0, 24.0] sampling_rate = 24000 audio_channels = 1 normalize = False chunk_length_s = None overlap = None hidden_size = 128 num_filters = 32 num_residual_layers = 1 upsampling_ratios = [8, 5, 4, 2] norm_type = 'weight_norm' kernel_size = 7 last_kernel_size = 7 residual_kernel_size = 3 dilation_growth_rate = 2 use_causal_conv = True pad_mode = 'reflect' compress = 2 num_lstm_layers = 2 trim_right_ratio = 1.0 codebook_size = 1024 codebook_dim = None use_conv_shortcut = True **kwargs )\n```", "```py\n>>> from transformers import EncodecModel, EncodecConfig\n\n>>> # Initializing a \"facebook/encodec_24khz\" style configuration\n>>> configuration = EncodecConfig()\n\n>>> # Initializing a model (with random weights) from the \"facebook/encodec_24khz\" style configuration\n>>> model = EncodecModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( feature_size: int = 1 sampling_rate: int = 24000 padding_value: float = 0.0 chunk_length_s: float = None overlap: float = None **kwargs )\n```", "```py\n( raw_audio: Union padding: Union = None truncation: Optional = False max_length: Optional = None return_tensors: Union = None sampling_rate: Optional = None )\n```", "```py\n( config: EncodecConfig )\n```", "```py\n( audio_codes: Tensor audio_scales: Tensor padding_mask: Optional = None return_dict: Optional = None )\n```", "```py\n( input_values: Tensor padding_mask: Tensor = None bandwidth: Optional = None return_dict: Optional = None )\n```", "```py\n( input_values: Tensor padding_mask: Optional = None bandwidth: Optional = None audio_codes: Optional = None audio_scales: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.encodec.modeling_encodec.EncodecOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoProcessor, EncodecModel\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> model_id = \"facebook/encodec_24khz\"\n>>> model = EncodecModel.from_pretrained(model_id)\n>>> processor = AutoProcessor.from_pretrained(model_id)\n\n>>> inputs = processor(raw_audio=audio_sample, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> audio_codes = outputs.audio_codes\n>>> audio_values = outputs.audio_values\n```"]