["```py\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n```", "```py\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```", "```py\n!pip install stable-baselines3[extra]\n!pip install gymnasium\n!pip install huggingface_sb3\n!pip install huggingface_hub\n!pip install panda_gym\n```", "```py\nimport os\n\nimport gymnasium as gym\nimport panda_gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_hub import notebook_login\n```", "```py\nenv_id = \"PandaReachDense-v3\"\n\n# Create the env\nenv = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape\na_size = env.action_space\n```", "```py\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n```", "```py\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```", "```py\nenv = make_vec_env(env_id, n_envs=4)\n\n# Adding this wrapper to normalize the observation and the reward\nenv = # TODO: Add the wrapper\n```", "```py\nenv = make_vec_env(env_id, n_envs=4)\n\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n```", "```py\nmodel = # Create the A2C model and try to find the best parameters\n```", "```py\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n```", "```py\nmodel.learn(1_000_000)\n```", "```py\n# Save the model and  VecNormalize statistics when saving the agent\nmodel.save(\"a2c-PandaReachDense-v3\")\nenv.save(\"vec_normalize.pkl\")\n```", "```py\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n# We need to override the render_mode\neval_env.render_mode = \"rgb_array\"\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(\"a2c-PandaReachDense-v3\")\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n```", "```py\nnotebook_login()\n!git config --global credential.helper store\n```", "```py\nfrom huggingface_sb3 import package_to_hub\n\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # Change the username\n    commit_message=\"Initial commit\",\n)\n```", "```py\n# 1 - 2\nenv_id = \"PandaPickAndPlace-v3\"\nenv = make_vec_env(env_id, n_envs=4)\n\n# 3\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n\n# 4\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n# 5\nmodel.learn(1_000_000)\n```", "```py\n# 6\nmodel_name = \"a2c-PandaPickAndPlace-v3\";\nmodel.save(model_name)\nenv.save(\"vec_normalize.pkl\")\n\n# 7\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaPickAndPlace-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(model_name)\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 8\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # TODO: Change the username\n    commit_message=\"Initial commit\",\n)\n```"]