- en: ProphetNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ProphetNet
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/prophetnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/prophetnet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/prophetnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/prophetnet)
- en: '[![Models](../Images/29825340e5ec090ff93b526bc34bf2f3.png)](https://huggingface.co/models?filter=prophetnet)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/prophetnet-large-uncased)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![模型](../Images/29825340e5ec090ff93b526bc34bf2f3.png)](https://huggingface.co/models?filter=prophetnet)
    [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/prophetnet-large-uncased)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram
    for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu
    Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang,
    Ming Zhou on 13 Jan, 2020.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'ProphetNet模型是由Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng
    Chen, Ruofei Zhang, Ming Zhou于2020年1月13日提出的[ProphetNet: Predicting Future N-gram
    for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063)。'
- en: ProphetNet is an encoder-decoder model and can predict n-future tokens for “ngram”
    language modeling instead of just the next token.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ProphetNet是一个编码器-解码器模型，可以预测“ngram”语言建模的n个未来标记，而不仅仅是下一个标记。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*In this paper, we present a new sequence-to-sequence pretraining model called
    ProphetNet, which introduces a novel self-supervised objective named future n-gram
    prediction and the proposed n-stream self-attention mechanism. Instead of the
    optimization of one-step ahead prediction in traditional sequence-to-sequence
    model, the ProphetNet is optimized by n-step ahead prediction which predicts the
    next n tokens simultaneously based on previous context tokens at each time step.
    The future n-gram prediction explicitly encourages the model to plan for the future
    tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet
    using a base scale dataset (16GB) and a large scale dataset (160GB) respectively.
    Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks
    for abstractive summarization and question generation tasks. Experimental results
    show that ProphetNet achieves new state-of-the-art results on all these datasets
    compared to the models using the same scale pretraining corpus.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我们提出了一种名为ProphetNet的新的序列到序列预训练模型，引入了一种名为未来n-gram预测的新型自监督目标和提出的n流自注意力机制。与传统序列到序列模型中一步预测的优化不同，ProphetNet通过n步预测进行优化，即在每个时间步基于先前上下文标记同时预测接下来的n个标记。未来n-gram预测明确鼓励模型规划未来标记，并防止在强烈的局部相关性上过拟合。我们分别使用基本规模数据集（16GB）和大规模数据集（160GB）对ProphetNet进行预训练。然后我们在CNN/DailyMail、Gigaword和SQuAD
    1.1基准上进行抽象摘要和问题生成任务的实验。实验结果表明，与使用相同规模预训练语料库的模型相比，ProphetNet在所有这些数据集上都取得了新的最先进结果。*'
- en: The Authors’ code can be found [here](https://github.com/microsoft/ProphetNet).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的代码可以在[这里](https://github.com/microsoft/ProphetNet)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: ProphetNet is a model with absolute position embeddings so it’s usually advised
    to pad the inputs on the right rather than the left.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ProphetNet是一个带有绝对位置嵌入的模型，因此通常建议在右侧而不是左侧填充输入。
- en: The model architecture is based on the original Transformer, but replaces the
    “standard” self-attention mechanism in the decoder by a a main self-attention
    mechanism and a self and n-stream (predict) self-attention mechanism.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型架构基于原始Transformer，但是在解码器中用一个主自注意力机制和一个自注意力机制和n流（预测）自注意力机制替换了“标准”自注意力机制。
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: '[Translation task guide](../tasks/translation)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[翻译任务指南](../tasks/translation)'
- en: '[Summarization task guide](../tasks/summarization)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[摘要任务指南](../tasks/summarization)'
- en: ProphetNetConfig
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNetConfig
- en: '### `class transformers.ProphetNetConfig`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/configuration_prophetnet.py#L32)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/configuration_prophetnet.py#L32)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout`（`float`，*可选*，默认为0.1）— 全连接层内激活的丢失比率。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function`（`str`或`function`，*可选*，默认为`"gelu"`）— 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    ProphetNET model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为30522）— ProphetNET模型的词汇大小。定义了在调用[ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel)时可以表示的不同标记的数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) — Dimensionality of the
    layers and the pooler layer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为1024）— 层和池化器层的维度。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 4096) — Dimensionality of
    the “intermediate” (often named feed-forward) layer in decoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim`（`int`，*可选*，默认为4096）— 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`num_encoder_layers` (`int`, *optional*, defaults to 12) — Number of encoder
    layers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_encoder_layers`（`int`，*可选*，默认为12）— 编码器层数。'
- en: '`num_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer encoder.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_encoder_attention_heads`（`int`，*可选*，默认为16）— Transformer编码器中每个注意力层的注意力头数。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 4096) — Dimensionality of
    the `intermediate` (often named feed-forward) layer in decoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *可选*, 默认为4096) — 解码器中`intermediate`（通常称为前馈）层的维度。'
- en: '`num_decoder_layers` (`int`, *optional*, defaults to 12) — Number of decoder
    layers.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_decoder_layers` (`int`, *可选*, 默认为12) — 解码器层数。'
- en: '`num_decoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer decoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_decoder_attention_heads` (`int`, *可选*, 默认为16) — Transformer解码器中每个注意力层的注意力头数。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *可选*, 默认为0.1) — 注意力概率的丢失比率。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *可选*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *可选*, 默认为512) — 此模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512或1024或2048）。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`add_cross_attention` (`bool`, *optional*, defaults to `True`) — Whether cross-attention
    layers should be added to the model.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_cross_attention` (`bool`, *可选*, 默认为`True`) — 是否应向模型添加交叉注意力层。'
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) — Whether this
    is an encoder/decoder model.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder_decoder` (`bool`, *可选*, 默认为`True`) — 是否这是一个编码器/解码器模型。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 1) — Padding token id.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *可选*, 默认为1) — 填充标记id。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 0) — Beginning of stream token
    id.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *可选*, 默认为0) — 流的起始标记id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — End of stream token id.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *可选*, 默认为2) — 流的结束标记id。'
- en: '`ngram` (`int`, *optional*, defaults to 2) — Number of future tokens to predict.
    Set to 1 to be same as traditional Language model to predict next first token.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram` (`int`, *可选*, 默认为2) — 预测未来标记的数量。设置为1以与传统语言模型相同，以预测下一个第一个标记。'
- en: '`num_buckets` (`int`, *optional*, defaults to 32) — The number of buckets to
    use for each attention layer. This is for relative position calculation. See the
    [T5 paper](see [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683))
    for more details.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_buckets` (`int`, *可选*, 默认为32) — 每个注意力层使用的桶数。这是用于相对位置计算的。更多细节请参阅[T5论文](参见[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683))。'
- en: '`relative_max_distance` (`int`, *optional*, defaults to 128) — Relative distances
    greater than this number will be put into the last same bucket. This is for relative
    position calculation. See the [T5 paper](see [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683))
    for more details.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_max_distance` (`int`, *可选*, 默认为128) — 大于此数字的相对距离将放入最后一个相同的桶中。这是用于相对位置计算的。更多细节请参阅[T5论文](参见[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683))。'
- en: '`disable_ngram_loss` (`bool`, *optional*, defaults to `False`) — Whether be
    trained predicting only the next first token.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_ngram_loss` (`bool`, *可选*, 默认为`False`) — 是否训练仅预测下一个第一个标记。'
- en: '`eps` (`float`, *optional*, defaults to 0.0) — Controls the `epsilon` parameter
    value for label smoothing in the loss calculation. If set to 0, no label smoothing
    is performed.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` (`float`, *可选*, 默认为0.0) — 控制损失计算中标签平滑的`epsilon`参数值。如果设置为0，则不执行标签平滑。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: This is the configuration class to store the configuration of a [ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel).
    It is used to instantiate a ProphetNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ProphetNet [microsoft/prophetnet-large-uncased](https://huggingface.co/microsoft/prophetnet-large-uncased)
    architecture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel)的配置。根据指定的参数实例化ProphetNet模型，定义模型架构。使用默认值实例化配置将产生类似于ProphetNet
    [microsoft/prophetnet-large-uncased](https://huggingface.co/microsoft/prophetnet-large-uncased)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: ProphetNetTokenizer
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNetTokenizer
- en: '### `class transformers.ProphetNetTokenizer`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L287)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L287)'
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含词汇表的文件。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *可选*, 默认为`True`) — 在标记化时是否将输入转换为小写。'
- en: '`do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — Whether or not
    to do basic tokenization before WordPiece.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_basic_tokenize` (`bool`, *可选*, 默认为`True`) — 在WordPiece之前是否进行基本标记化。'
- en: '`never_split` (`Iterable`, *optional*) — Collection of tokens which will never
    be split during tokenization. Only has an effect when `do_basic_tokenize=True`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`never_split`（`Iterable`，*可选*）— 在标记化期间永远不会拆分的标记集合。仅在`do_basic_tokenize=True`时才有效果'
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"[UNK]"`）— 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`，*可选*，默认为`"[SEP]"`）— 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作由特殊标记构建的序列的最后一个标记。'
- en: '`x_sep_token` (`str`, *optional*, defaults to `"[X_SEP]"`) — Special second
    separator token, which can be generated by [ProphetNetForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration).
    It is used to separate bullet-point like sentences in summarization, *e.g.*.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_sep_token`（`str`，*可选*，默认为`"[X_SEP]"`）— 特殊的第二个分隔符标记，可以由[ProphetNetForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration)生成。用于在摘要中分隔类似项目符号的句子，*例如*。'
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"[PAD]"`）— 用于填充的标记，例如在批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`，*可选*，默认为`"[MASK]"`）— 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — Whether
    or not to tokenize Chinese characters.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize_chinese_chars`（`bool`，*可选*，默认为`True`）— 是否对中文字符进行标记化。'
- en: This should likely be deactivated for Japanese (see this [issue](https://github.com/huggingface/transformers/issues/328)).
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可能应该在日语中停用（请参阅此[问题](https://github.com/huggingface/transformers/issues/328)）。
- en: '`strip_accents` (`bool`, *optional*) — Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original BERT).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip_accents`（`bool`，*可选*）— 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始BERT相同）。'
- en: Construct a ProphetNetTokenizer. Based on WordPiece.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个ProphetNetTokenizer。基于WordPiece。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L496)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L496)'
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A BERT sequence has the following
    format:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。一个BERT序列具有以下格式：
- en: 'single sequence: `[CLS] X [SEP]`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`[CLS] X [SEP]`
- en: 'pair of sequences: `[CLS] A [SEP] B [SEP]`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`[CLS] A [SEP] B [SEP]`
- en: '#### `convert_tokens_to_string`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L413)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L413)'
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Converts a sequence of tokens (string) in a single string.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L448)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L448)'
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. A ProphetNet
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。一个ProphetNet
- en: 'sequence pair mask has the following format:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对掩码具有以下格式：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`是`None`，这个方法只返回掩码的第一部分（0s）。
- en: '#### `get_special_tokens_mask`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L418)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L418)'
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经使用特殊标记格式化为模型。'
- en: Returns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。
- en: ProphetNet specific outputs
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNet特定输出
- en: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L252)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L252)'
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, decoder_sequence_length, config.vocab_size)`的`torch.FloatTensor`）—
    主流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`logits_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_ngram`（形状为`(batch_size, ngram * decoder_sequence_length, config.vocab_size)`的`torch.FloatTensor`）—
    预测流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（注意力块中的键和值）的解码器，可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流的隐藏状态，在每层的输出以及初始嵌入输出。
- en: '`decoder_ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, ngram * decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的隐藏状态，在每层的输出以及初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the self-attention heads.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）
    — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的隐藏状态，在每一层的输出以及初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`. Attentions weights of the
    encoder, after the attention softmax, used to compute the weighted average in
    the self-attention heads.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: Base class for sequence-to-sequence language models outputs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 用于序列到序列语言模型输出的基类。
- en: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L337)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L337)'
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`）
    — 模型解码器最后一层的主流隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`last_hidden_state_ngram` (`torch.FloatTensor` of shape `(batch_size,ngram
    * decoder_sequence_length, config.vocab_size)`, *optional*) — Sequence of predict
    stream hidden-states at the output of the last layer of the decoder of the model.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state_ngram`（形状为`(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`的`torch.FloatTensor`，*可选*） — 模型解码器最后一层的预测流隐藏状态序列。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预先计算隐藏状态（注意力块中的键和值），可以用于加速顺序解码（查看`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流的隐藏状态，在每一层的输出以及初始嵌入输出。
- en: '`decoder_ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, ngram * decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的预测流的隐藏状态，在每一层的输出以及初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的注意力权重，在注意力softmax之后，用于计算加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, encoder_sequence_length,
    hidden_size)`，*可选的*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每层的输出以及初始嵌入输出的隐藏状态。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: 'Base class for model encoder’s outputs that also contains : pre-computed hidden
    states that can speed up sequential decoding.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 模型编码器输出的基类，还包含：可加速顺序解码的预计算隐藏状态。
- en: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L423)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L423)'
- en: '[PRE9]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, decoder_sequence_length,
    hidden_size)`) — 模型解码器最后一层的主流隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`last_hidden_state_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram
    * decoder_sequence_length, config.vocab_size)`) — Sequence of predict stream hidden-states
    at the output of the last layer of the decoder of the model.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state_ngram` (`torch.FloatTensor`，形状为`(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — 模型解码器最后一层输出的预测流隐藏状态序列。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *可选的*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流的隐藏状态，在每层的输出以及初始嵌入输出。
- en: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（每层一个，包括嵌入层和每一层的输出），形状为`(batch_size, ngram * decoder_sequence_length,
    hidden_size)`。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个），形状为`(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个），形状为`(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的注意力权重，在注意力softmax之后，用于计算加权平均值
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个），形状为`(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出的基类，可能还包含过去的键/值（用于加速顺序解码）。
- en: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L483)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L483)'
- en: '[PRE10]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*, 当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, decoder_sequence_length, config.vocab_size)`)
    — 主流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`logits_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_ngram` (`torch.FloatTensor`，形状为`(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — 预测流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（每层一个，包括嵌入层和每一层的输出），形状为`(batch_size, decoder_sequence_length,
    hidden_size)`。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流每一层输出的隐藏状态加上初始嵌入输出。
- en: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（每层一个，包括嵌入层和每一层的输出），形状为`(batch_size, ngram * decoder_sequence_length,
    hidden_size)`。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的解码器预测流的注意力权重，用于计算加权平均值
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出的基类，可能还包含过去的键/值（以加速顺序解码）。
- en: ProphetNetModel
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNetModel
- en: '### `class transformers.ProphetNetModel`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1726)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1726)'
- en: '[PRE11]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare ProphetNet Model outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 裸ProphetNet模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 原始ProphetNet代码可以在[这里](https://github.com/microsoft/ProphetNet)找到。检查点是从原始Fairseq检查点转换而来的。有关检查点转换的更多信息，请查看文件`convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1769)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1769)'
- en: '[PRE12]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`） — 词汇表中输入序列标记的索引。默认情况下会忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 索引可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获得。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩盖的标记，为1，
- en: 0 for tokens that are `masked`.
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被遮蔽的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力蒙版？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: ProphetNet uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ProphetNet使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，可以选择只输入最后一个`decoder_input_ids`（参见`past_key_values`）。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果蒙版也将默认使用。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使编码器中注意力模块中选择的头部失效的蒙版。蒙版值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使解码器中注意力模块中选择的头部失效的蒙版。蒙版值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使交叉注意力模块中选择的头部失效的蒙版。蒙版值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有4个形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的张量）— 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择只输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或 `config.return_dict=False`），包括根据配置（`ProphenetConfig`）和输入而异的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — 模型解码器最后一层的主流隐藏状态序列输出。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用 `past_key_values`，则只输出形状为 `(batch_size, 1, hidden_size)` 的序列的最后一个隐藏状态。
- en: '`last_hidden_state_ngram` (`torch.FloatTensor` of shape `(batch_size,ngram
    * decoder_sequence_length, config.vocab_size)`, *optional*) — Sequence of predict
    stream hidden-states at the output of the last layer of the decoder of the model.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state_ngram` (`torch.FloatTensor` of shape `(batch_size,ngram
    * decoder_sequence_length, config.vocab_size)`, *optional*) — 模型解码器最后一层的预测流隐藏状态序列输出。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — 长度为 `config.n_layers` 的 `torch.FloatTensor`
    列表，每个张量的形状为 `(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — 每一层的 `torch.FloatTensor` 元组，形状为 `(batch_size, decoder_sequence_length, hidden_size)`。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流的每一层的隐藏状态，加上初始嵌入输出。
- en: '`decoder_ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — 每一层的 `torch.FloatTensor` 元组，形状为 `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的每一层的隐藏状态，加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    每一层的 `torch.FloatTensor` 元组，形状为 `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — 每一层的 `torch.FloatTensor` 元组，形状为 `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的注意力权重，在注意力softmax之后，用于计算`last_hidden_state`中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — 每一层的 `torch.FloatTensor`
    元组，形状为 `(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)`。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列输出。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个层的编码器的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel)
    forward method, overrides the `__call__` special method.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ProphetNetEncoder
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNet编码器
- en: '### `class transformers.ProphetNetEncoder`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetEncoder`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1224)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1224)'
- en: '[PRE14]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The standalone encoder part of the ProphetNetModel. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ProphetNetModel的独立编码器部分。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）的超类文档。
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 原始ProphetNet代码可以在[这里](https://github.com/microsoft/ProphetNet)找到。检查点是从原始Fairseq检查点转换而来的。有关检查点转换的更多信息，请查看文件`convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: 'word_embeddings (`torch.nn.Embeddings` of shape `(config.vocab_size, config.hidden_size)`,
    *optional*): The word embedding parameters. This can be used to initialize [ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    with pre-defined word embeddings instead of randomly initialized word embeddings.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: word_embeddings（形状为`(config.vocab_size, config.hidden_size)`的`torch.nn.Embeddings`，*可选*）：词嵌入参数。这可以用于使用预定义的词嵌入而不是随机初始化的词嵌入初始化[ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)。
- en: '#### `forward`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 前进
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1258)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1258)'
- en: '[PRE15]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示被 `未掩码` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `掩码` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[注意力掩码是什么？](../glossary#attention-mask)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`，形状为 `(encoder_layers, encoder_attention_heads)`，*可选*)
    — 用于使编码器中注意力模块的选定头部失效的掩码。掩码值选在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部是 `未掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是 `掩码`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: Returns
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含各种元素，取决于配置 (`ProphenetConfig`) 和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层，则为嵌入输出的一个 + 每层输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    forward method, overrides the `__call__` special method.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ProphetNetDecoder
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNetDecoder
- en: '### `class transformers.ProphetNetDecoder`'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetDecoder`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1358)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1358)'
- en: '[PRE17]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The standalone decoder part of the ProphetNetModel. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ProphetNetModel的独立解码器部分。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查库实现的所有模型的通用方法的超类文档（例如下载或保存，调整输入嵌入，修剪头等）。
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 原始ProphetNet代码可以在[这里](https://github.com/microsoft/ProphetNet)找到。检查点是从原始Fairseq检查点转换而来的。有关检查点转换的更多信息，请查看文件`convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有事项。
- en: 'word_embeddings (`torch.nn.Embeddings` of shape `(config.vocab_size, config.hidden_size)`,
    *optional*): The word embedding parameters. This can be used to initialize [ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    with pre-defined word embeddings instead of randomly initialized word embeddings.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 'word_embeddings (`torch.nn.Embeddings` of shape `(config.vocab_size, config.hidden_size)`,
    *可选*): 词嵌入参数。这可以用于使用预定义的词嵌入初始化[ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)，而不是随机初始化的词嵌入。'
- en: '#### `forward`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1399)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1399)'
- en: '[PRE18]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。默认情况下会忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示标记是`not masked`，
- en: 0 for tokens that are `masked`.
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示标记是`masked`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *可选*) — 用于使编码器中注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部是`not masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *可选*) — 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。'
- en: '`encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on the padding token indices
    of the encoder input. This mask is used in the cross-attention if the model is
    configured as a decoder. Mask values selected in `[0, 1]`:'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。掩码值选择在`[0, 1]`中：'
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *可选*) — 用于使交叉注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部是`not masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是`masked`。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组包含形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的4个张量) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（即没有将过去的键值状态提供给该模型的那些）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: 1 for tokens that are `not masked`,
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的标记为0。
- en: Returns
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)或者`tuple(torch.FloatTensor)`'
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)或者一个`torch.FloatTensor`元组（如果传入`return_dict=False`或者`config.return_dict=False`）包含根据配置(`ProphenetConfig`)和输入不同的元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`)
    — 模型解码器最后一层输出的主流隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`last_hidden_state_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram
    * decoder_sequence_length, config.vocab_size)`) — Sequence of predict stream hidden-states
    at the output of the last layer of the decoder of the model.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state_ngram` (`形状为`(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`的`torch.FloatTensor`) — 模型解码器最后一层输出的预测流隐藏状态序列。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *可选*, 当传入`use_cache=True`或者`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传入`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每层输出）。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流每层的隐藏状态加上初始嵌入输出。
- en: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传入`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, ngram * decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每层输出）。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器每层预测流的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传入`output_attentions=True`或者`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的预测流的注意力权重，在注意力 softmax 之后，用于计算加权平均值
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算加权平均值
- en: The [ProphetNetDecoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetDecoder)
    forward method, overrides the `__call__` special method.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[ProphetNetDecoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetDecoder)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ProphetNetForConditionalGeneration
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNetForConditionalGeneration
- en: '### `class transformers.ProphetNetForConditionalGeneration`'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1860)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1860)'
- en: '[PRE20]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The ProphetNet Model with a language modeling head. Can be used for sequence
    generation tasks. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 带有语言建模头的 ProphetNet 模型。可用于序列生成任务。此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 ProphetNet 代码可以在[这里](https://github.com/microsoft/ProphetNet)找到。检查点是从原始的
    Fairseq 检查点转换而来的。有关检查点转换的更多信息，请查看文件 `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1891)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1891)'
- en: '[PRE21]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被 `masked` 的标记为 1，
- en: 0 for tokens that are `masked`.
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被 `masked` 的标记为 0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    解码器输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: ProphetNet uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ProphetNet使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，可以选择性地只输入最后一个`decoder_input_ids`（参见`past_key_values`）。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于将编码器中注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于将解码器中注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于将交叉注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组包含形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的4个张量）— 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择性地只输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将它们的过去键值状态提供给该模型的）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多细节，请参见返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多细节，请参见返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[-100,
    0, ..., config.vocab_size - 1]`范围内。所有设置为`-100`的标签将被忽略（遮蔽），损失仅计算在`[0, ..., config.vocab_size]`范围内的标签。'
- en: Returns
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)或一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或`config.return_dict=False`），包括根据配置（`ProphenetConfig`）和输入而变化的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — 主流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`logits_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — 预测流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2,
    batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — `torch.FloatTensor`的元组（一个用于嵌入的输出 + 一个用于每一层的输出）的形状为`(batch_size, decoder_sequence_length,
    hidden_size)`。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流的每一层输出加上初始嵌入输出的隐藏状态。
- en: '`decoder_ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — `torch.FloatTensor`的元组（一个用于嵌入的输出 + 一个用于每一层的输出）的形状为`(batch_size, ngram * decoder_sequence_length,
    hidden_size)`。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的每一层输出加上初始嵌入输出的隐藏状态。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    `torch.FloatTensor`的元组（每一层一个）的形状为`(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — `torch.FloatTensor`的元组（每一层一个）的形状为`(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the self-attention heads.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — `torch.FloatTensor`的元组（每一层一个）的形状为`(batch_size,
    num_attn_heads, encoder_sequence_length, decoder_sequence_length)`。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, encoder_sequence_length,
    hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层的隐藏状态以及初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`. Attentions weights of the
    encoder, after the attention softmax, used to compute the weighted average in
    the self-attention heads.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)`的`torch.FloatTensor`元组（每层一个）。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: The [ProphetNetForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[ProphetNetForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration)
    的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ProphetNetForCausalLM
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ProphetNetForCausalLM
- en: '### `class transformers.ProphetNetForCausalLM`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProphetNetForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L2076)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L2076)'
- en: '[PRE23]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The standalone decoder part of the ProphetNetModel with a lm head on top. The
    model can be used for causal language modeling. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ProphetNetModel的独立解码器部分，顶部带有lm头。该模型可用于因果语言建模。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 原始ProphetNet代码可以在[这里](https://github.com/microsoft/ProphetNet)找到。检查点是从原始Fairseq检查点转换而来的。有关检查点转换的更多信息，请查看文件`convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L2126)'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L2126)'
- en: '[PRE24]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。默认情况下会忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被屏蔽`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使编码器中注意力模块中选择的头部失效的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记为1，
- en: 0 indicates the head is `masked`.
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`屏蔽`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。'
- en: '`encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on the padding token indices
    of the encoder input. This mask is used in the cross-attention if the model is
    configured as a decoder. Mask values selected in `[0, 1]`:'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用此掩码。选择的掩码值在`[0, 1]`中：'
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使交叉注意力模块中选择的头部失效的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记为1，
- en: 0 indicates the head is `masked`.
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被屏蔽`的标记为0。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有4个形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的张量）— 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（这些没有将其过去的键值状态提供给此模型）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态并可用于加速解码（参见`past_key_values`）。'
- en: 1 for tokens that are `not masked`,
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被屏蔽`的标记为0。
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the left-to-right language modeling loss (next word prediction).
    Indices should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring)
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels n `[0, ..., config.vocab_size]`'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算从左到右的语言建模损失（下一个单词预测）的标签。索引应在`[-100,
    0, ..., config.vocab_size]`中（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（屏蔽），损失仅计算具有标签n
    `[0, ..., config.vocab_size]`的标记'
- en: Returns
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`ProphenetConfig`）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, decoder_sequence_length, config.vocab_size)`)
    — 主流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`logits_ngram` (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_ngram` (`torch.FloatTensor`，形状为`(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — 预测流语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（查看`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器主流的每一层的隐藏状态加上初始嵌入输出。
- en: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, ngram * decoder_sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的每一层的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器预测流的注意力权重，在注意力softmax之后，用于计算加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, encoder_sequence_length,
    decoder_sequence_length)`.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算加权平均值。
- en: The [ProphetNetForCausalLM](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '[ProphetNetForCausalLM](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
