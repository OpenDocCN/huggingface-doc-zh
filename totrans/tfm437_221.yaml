- en: ProphetNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/prophetnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/prophetnet)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/224.1c5e954b.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">[![Models](../Images/29825340e5ec090ff93b526bc34bf2f3.png)](https://huggingface.co/models?filter=prophetnet)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/prophetnet-large-uncased)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram
    for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu
    Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang,
    Ming Zhou on 13 Jan, 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: ProphetNet is an encoder-decoder model and can predict n-future tokens for “ngram”
    language modeling instead of just the next token.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this paper, we present a new sequence-to-sequence pretraining model called
    ProphetNet, which introduces a novel self-supervised objective named future n-gram
    prediction and the proposed n-stream self-attention mechanism. Instead of the
    optimization of one-step ahead prediction in traditional sequence-to-sequence
    model, the ProphetNet is optimized by n-step ahead prediction which predicts the
    next n tokens simultaneously based on previous context tokens at each time step.
    The future n-gram prediction explicitly encourages the model to plan for the future
    tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet
    using a base scale dataset (16GB) and a large scale dataset (160GB) respectively.
    Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks
    for abstractive summarization and question generation tasks. Experimental results
    show that ProphetNet achieves new state-of-the-art results on all these datasets
    compared to the models using the same scale pretraining corpus.*'
  prefs: []
  type: TYPE_NORMAL
- en: The Authors’ code can be found [here](https://github.com/microsoft/ProphetNet).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ProphetNet is a model with absolute position embeddings so it’s usually advised
    to pad the inputs on the right rather than the left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model architecture is based on the original Transformer, but replaces the
    “standard” self-attention mechanism in the decoder by a a main self-attention
    mechanism and a self and n-stream (predict) self-attention mechanism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Translation task guide](../tasks/translation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Summarization task guide](../tasks/summarization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ProphetNetConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/configuration_prophetnet.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '( activation_dropout: Optional = 0.1 activation_function: Union = ''gelu''
    vocab_size: Optional = 30522 hidden_size: Optional = 1024 encoder_ffn_dim: Optional
    = 4096 num_encoder_layers: Optional = 12 num_encoder_attention_heads: Optional
    = 16 decoder_ffn_dim: Optional = 4096 num_decoder_layers: Optional = 12 num_decoder_attention_heads:
    Optional = 16 attention_dropout: Optional = 0.1 dropout: Optional = 0.1 max_position_embeddings:
    Optional = 512 init_std: Optional = 0.02 is_encoder_decoder: Optional = True add_cross_attention:
    Optional = True decoder_start_token_id: Optional = 0 ngram: Optional = 2 num_buckets:
    Optional = 32 relative_max_distance: Optional = 128 disable_ngram_loss: Optional
    = False eps: Optional = 0.0 use_cache: Optional = True pad_token_id: Optional
    = 0 bos_token_id: Optional = 1 eos_token_id: Optional = 2 **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**activation_dropout** (`float`, *optional*, defaults to 0.1) — The dropout
    ratio for activations inside the fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**activation_function** (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 30522) — Vocabulary size of
    the ProphetNET model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 1024) — Dimensionality of the
    layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_ffn_dim** (`int`, *optional*, defaults to 4096) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_encoder_layers** (`int`, *optional*, defaults to 12) — Number of encoder
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_encoder_attention_heads** (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_ffn_dim** (`int`, *optional*, defaults to 4096) — Dimensionality
    of the `intermediate` (often named feed-forward) layer in decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_decoder_layers** (`int`, *optional*, defaults to 12) — Number of decoder
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_decoder_attention_heads** (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_dropout** (`float`, *optional*, defaults to 0.1) — The dropout
    ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout** (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_position_embeddings** (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init_std** (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_cross_attention** (`bool`, *optional*, defaults to `True`) — Whether
    cross-attention layers should be added to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_encoder_decoder** (`bool`, *optional*, defaults to `True`) — Whether this
    is an encoder/decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_id** (`int`, *optional*, defaults to 1) — Padding token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token_id** (`int`, *optional*, defaults to 0) — Beginning of stream token
    id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token_id** (`int`, *optional*, defaults to 2) — End of stream token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ngram** (`int`, *optional*, defaults to 2) — Number of future tokens to predict.
    Set to 1 to be same as traditional Language model to predict next first token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_buckets** (`int`, *optional*, defaults to 32) — The number of buckets
    to use for each attention layer. This is for relative position calculation. See
    the [T5 paper](see [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**relative_max_distance** (`int`, *optional*, defaults to 128) — Relative distances
    greater than this number will be put into the last same bucket. This is for relative
    position calculation. See the [T5 paper](see [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**disable_ngram_loss** (`bool`, *optional*, defaults to `False`) — Whether
    be trained predicting only the next first token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eps** (`float`, *optional*, defaults to 0.0) — Controls the `epsilon` parameter
    value for label smoothing in the loss calculation. If set to 0, no label smoothing
    is performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*, defaults to `True`) — Whether or not the
    model should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel).
    It is used to instantiate a ProphetNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ProphetNet [microsoft/prophetnet-large-uncased](https://huggingface.co/microsoft/prophetnet-large-uncased)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ProphetNetTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L287)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vocab_file: str do_lower_case: Optional = True do_basic_tokenize: Optional
    = True never_split: Optional = None unk_token: Optional = ''[UNK]'' sep_token:
    Optional = ''[SEP]'' x_sep_token: Optional = ''[X_SEP]'' pad_token: Optional =
    ''[PAD]'' mask_token: Optional = ''[MASK]'' tokenize_chinese_chars: Optional =
    True strip_accents: Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — File containing the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_lower_case** (`bool`, *optional*, defaults to `True`) — Whether or not
    to lowercase the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_basic_tokenize** (`bool`, *optional*, defaults to `True`) — Whether or
    not to do basic tokenization before WordPiece.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**never_split** (`Iterable`, *optional*) — Collection of tokens which will
    never be split during tokenization. Only has an effect when `do_basic_tokenize=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sep_token** (`str`, *optional*, defaults to `"[SEP]"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**x_sep_token** (`str`, *optional*, defaults to `"[X_SEP]"`) — Special second
    separator token, which can be generated by [ProphetNetForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration).
    It is used to separate bullet-point like sentences in summarization, *e.g.*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"[PAD]"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask_token** (`str`, *optional*, defaults to `"[MASK]"`) — The token used
    for masking values. This is the token used when training this model with masked
    language modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenize_chinese_chars** (`bool`, *optional*, defaults to `True`) — Whether
    or not to tokenize Chinese characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This should likely be deactivated for Japanese (see this [issue](https://github.com/huggingface/transformers/issues/328)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**strip_accents** (`bool`, *optional*) — Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original BERT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a ProphetNetTokenizer. Based on WordPiece.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L496)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A BERT sequence has the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `[CLS] X [SEP]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair of sequences: `[CLS] A [SEP] B [SEP]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### convert_tokens_to_string'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L413)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tokens: str )'
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of tokens (string) in a single string.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L448)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. A ProphetNet
  prefs: []
  type: TYPE_NORMAL
- en: 'sequence pair mask has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  prefs: []
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/tokenization_prophetnet.py#L418)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    Optional = False ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) —
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: ProphetNet specific outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L252)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None logits: FloatTensor = None logits_ngram: Optional =
    None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_ngram_hidden_states:
    Optional = None decoder_attentions: Optional = None decoder_ngram_attentions:
    Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional
    = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits_ngram** (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`. Attentions weights of the
    encoder, after the attention softmax, used to compute the weighted average in
    the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for sequence-to-sequence language models outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L337)'
  prefs: []
  type: TYPE_NORMAL
- en: '( last_hidden_state: FloatTensor last_hidden_state_ngram: Optional = None past_key_values:
    Optional = None decoder_hidden_states: Optional = None decoder_ngram_hidden_states:
    Optional = None decoder_attentions: Optional = None decoder_ngram_attentions:
    Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional
    = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**last_hidden_state_ngram** (`torch.FloatTensor` of shape `(batch_size,ngram
    * decoder_sequence_length, config.vocab_size)`, *optional*) — Sequence of predict
    stream hidden-states at the output of the last layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Base class for model encoder’s outputs that also contains : pre-computed hidden
    states that can speed up sequential decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L423)'
  prefs: []
  type: TYPE_NORMAL
- en: '( last_hidden_state: FloatTensor last_hidden_state_ngram: Optional = None past_key_values:
    Optional = None hidden_states: Optional = None hidden_states_ngram: Optional =
    None attentions: Optional = None ngram_attentions: Optional = None cross_attentions:
    Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**last_hidden_state_ngram** (`torch.FloatTensor` of shape `(batch_size, ngram
    * decoder_sequence_length, config.vocab_size)`) — Sequence of predict stream hidden-states
    at the output of the last layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L483)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loss: Optional = None logits: FloatTensor = None logits_ngram: Optional =
    None past_key_values: Optional = None hidden_states: Optional = None hidden_states_ngram:
    Optional = None attentions: Optional = None ngram_attentions: Optional = None
    cross_attentions: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits_ngram** (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  prefs: []
  type: TYPE_NORMAL
- en: ProphetNetModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1726)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ProphetNetConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare ProphetNet Model outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1769)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids:
    Optional = None decoder_attention_mask: Optional = None head_mask: Optional =
    None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None
    encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds:
    Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None
    output_attentions: Optional = None output_hidden_states: Optional = None return_dict:
    Optional = None ) → [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_input_ids** (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ProphetNet uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attention_mask** (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_head_mask** (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attn_head_mask** (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_outputs** (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple
    consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**last_hidden_state_ngram** (`torch.FloatTensor` of shape `(batch_size,ngram
    * decoder_sequence_length, config.vocab_size)`, *optional*) — Sequence of predict
    stream hidden-states at the output of the last layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ProphetNetModel](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ProphetNetEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetEncoder'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1224)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ProphetNetConfig word_embeddings: Embedding = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standalone encoder part of the ProphetNetModel. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'word_embeddings (`torch.nn.Embeddings` of shape `(config.vocab_size, config.hidden_size)`,
    *optional*): The word embedding parameters. This can be used to initialize [ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    with pre-defined word embeddings instead of randomly initialized word embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1258)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional
    = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None ) → [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ProphetNetDecoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetDecoder'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1358)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ProphetNetConfig word_embeddings: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standalone decoder part of the ProphetNetModel. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'word_embeddings (`torch.nn.Embeddings` of shape `(config.vocab_size, config.hidden_size)`,
    *optional*): The word embedding parameters. This can be used to initialize [ProphetNetEncoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetEncoder)
    with pre-defined word embeddings instead of randomly initialized word embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1399)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states:
    Optional = None encoder_attention_mask: Optional = None head_mask: Optional =
    None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds:
    Optional = None use_cache: Optional = None output_attentions: Optional = None
    output_hidden_states: Optional = None return_dict: Optional = None ) → [transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on the padding token indices
    of the encoder input. This mask is used in the cross-attention if the model is
    configured as a decoder. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attn_head_mask** (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    hidden_size)`) — Sequence of main stream hidden-states at the output of the last
    layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**last_hidden_state_ngram** (`torch.FloatTensor` of shape `(batch_size, ngram
    * decoder_sequence_length, config.vocab_size)`) — Sequence of predict stream hidden-states
    at the output of the last layer of the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ProphetNetDecoder](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetDecoder)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ProphetNetForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetForConditionalGeneration'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1860)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ProphetNetConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ProphetNet Model with a language modeling head. Can be used for sequence
    generation tasks. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L1891)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids:
    Optional = None decoder_attention_mask: Optional = None head_mask: Optional =
    None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None
    encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds:
    Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None
    use_cache: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None ) → [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_input_ids** (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ProphetNet uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attention_mask** (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_head_mask** (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attn_head_mask** (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_outputs** (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple
    consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits_ngram** (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, encoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, encoder_sequence_length)`. Attentions weights of the
    encoder, after the attention softmax, used to compute the weighted average in
    the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ProphetNetForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ProphetNetForCausalLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ProphetNetForCausalLM'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L2076)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ProphetNetConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ProphetNetConfig](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standalone decoder part of the ProphetNetModel with a lm head on top. The
    model can be used for causal language modeling. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet).
    Checkpoints were converted from original Fairseq checkpoints. For more information
    on the checkpoint conversion, please take a look at the file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matters related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/prophetnet/modeling_prophetnet.py#L2126)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states:
    Optional = None encoder_attention_mask: Optional = None head_mask: Optional =
    None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds:
    Optional = None labels: Optional = None use_cache: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None ) → [transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on the padding token indices
    of the encoder input. This mask is used in the cross-attention if the model is
    configured as a decoder. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attn_head_mask** (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden-states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the left-to-right language modeling loss (next word prediction).
    Indices should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring)
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels n `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`ProphenetConfig`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the main stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits_ngram** (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length,
    config.vocab_size)`) — Prediction scores of the predict stream language modeling
    head (scores for each vocabulary token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_attn_heads,
    decoder_sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, decoder_sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of main stream of the decoder at the output of each layer plus
    the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, ngram * decoder_sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the predict stream of the decoder at the output of each layer
    plus the initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_attn_heads, decoder_sequence_length,
    decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ngram_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    decoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the predict stream of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,
    encoder_sequence_length, decoder_sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the cross-attention layer of the decoder, after the attention
    softmax, used to compute the weighted average in the
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ProphetNetForCausalLM](/docs/transformers/v4.37.2/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
