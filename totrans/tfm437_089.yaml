- en: How to create a custom pipeline?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/add_new_pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/add_new_pipeline)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/4.ae880155.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: In this guide, we will see how to create a custom pipeline and share it on the
    [Hub](https://hf.co/models) or add it to the ü§ó Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, you need to decide the raw entries the pipeline will be
    able to take. It can be strings, raw bytes, dictionaries or whatever seems to
    be the most likely desired input. Try to keep these inputs as pure Python as possible
    as it makes compatibility easier (even through other languages via JSON). Those
    will be the `inputs` of the pipeline (`preprocess`).
  prefs: []
  type: TYPE_NORMAL
- en: Then define the `outputs`. Same policy as the `inputs`. The simpler, the better.
    Those will be the outputs of `postprocess` method.
  prefs: []
  type: TYPE_NORMAL
- en: Start by inheriting the base class `Pipeline` with the 4 methods needed to implement
    `preprocess`, `_forward`, `postprocess`, and `_sanitize_parameters`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The structure of this breakdown is to support relatively seamless support for
    CPU/GPU, while supporting doing pre/postprocessing on the CPU on different threads
  prefs: []
  type: TYPE_NORMAL
- en: '`preprocess` will take the originally defined inputs, and turn them into something
    feedable to the model. It might contain more information and is usually a `Dict`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`_forward` is the implementation detail and is not meant to be called directly.
    `forward` is the preferred called method as it contains safeguards to make sure
    everything is working on the expected device. If anything is linked to a real
    model it belongs in the `_forward` method, anything else is in the preprocess/postprocess.'
  prefs: []
  type: TYPE_NORMAL
- en: '`postprocess` methods will take the output of `_forward` and turn it into the
    final output that was decided earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: '`_sanitize_parameters` exists to allow users to pass any parameters whenever
    they wish, be it at initialization time `pipeline(...., maybe_arg=4)` or at call
    time `pipe = pipeline(...); output = pipe(...., maybe_arg=4)`.'
  prefs: []
  type: TYPE_NORMAL
- en: The returns of `_sanitize_parameters` are the 3 dicts of kwargs that will be
    passed directly to `preprocess`, `_forward`, and `postprocess`. Don‚Äôt fill anything
    if the caller didn‚Äôt call with any extra parameter. That allows to keep the default
    arguments in the function definition which is always more ‚Äúnatural‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: A classic example would be a `top_k` argument in the post processing in classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In order to achieve that, we‚Äôll update our `postprocess` method with a default
    parameter to `5`. and edit `_sanitize_parameters` to allow this new parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Try to keep the inputs/outputs very simple and ideally JSON-serializable as
    it makes the pipeline usage very easy without requiring users to understand new
    kinds of objects. It‚Äôs also relatively common to support many different types
    of arguments for ease of use (audio files, which can be filenames, URLs or pure
    bytes)
  prefs: []
  type: TYPE_NORMAL
- en: Adding it to the list of supported tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To register your `new-task` to the list of supported tasks, you have to add
    it to the `PIPELINE_REGISTRY`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can specify a default model if you want, in which case it should come with
    a specific revision (which can be the name of a branch or a commit hash, here
    we took `"abcdef"`) as well as the type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Share your pipeline on the Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To share your custom pipeline on the Hub, you just have to save the custom
    code of your `Pipeline` subclass in a python file. For instance, let‚Äôs say we
    want to use a custom pipeline for sentence pair classification like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation is framework agnostic, and will work for PyTorch and TensorFlow
    models. If we have saved this in a file named `pair_classification.py`, we can
    then import it and register it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, we can use it with a pretrained model. For instance `sgugger/finetuned-bert-mrpc`
    has been fine-tuned on the MRPC dataset, which classifies pairs of sentences as
    paraphrases or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can share it on the Hub by using the `save_pretrained` method in a
    `Repository`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will copy the file where you defined `PairClassificationPipeline` inside
    the folder `"test-dynamic-pipeline"`, along with saving the model and tokenizer
    of the pipeline, before pushing everything into the repository `{your_username}/test-dynamic-pipeline`.
    After that, anyone can use it as long as they provide the option `trust_remote_code=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Add the pipeline to ü§ó Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to contribute your pipeline to ü§ó Transformers, you will need to
    add a new module in the `pipelines` submodule with the code of your pipeline,
    then add it to the list of tasks defined in `pipelines/__init__.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Then you will need to add tests. Create a new file `tests/test_pipelines_MY_PIPELINE.py`
    with examples of the other tests.
  prefs: []
  type: TYPE_NORMAL
- en: The `run_pipeline_test` function will be very generic and run on small random
    models on every possible architecture as defined by `model_mapping` and `tf_model_mapping`.
  prefs: []
  type: TYPE_NORMAL
- en: This is very important to test future compatibility, meaning if someone adds
    a new model for `XXXForQuestionAnswering` then the pipeline test will attempt
    to run on it. Because the models are random it‚Äôs impossible to check for actual
    values, that‚Äôs why there is a helper `ANY` that will simply attempt to match the
    output of the pipeline TYPE.
  prefs: []
  type: TYPE_NORMAL
- en: You also *need* to implement 2 (ideally 4) tests.
  prefs: []
  type: TYPE_NORMAL
- en: '`test_small_model_pt` : Define 1 small model for this pipeline (doesn‚Äôt matter
    if the results don‚Äôt make sense) and test the pipeline outputs. The results should
    be the same as `test_small_model_tf`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_small_model_tf` : Define 1 small model for this pipeline (doesn‚Äôt matter
    if the results don‚Äôt make sense) and test the pipeline outputs. The results should
    be the same as `test_small_model_pt`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_large_model_pt` (`optional`): Tests the pipeline on a real pipeline where
    the results are supposed to make sense. These tests are slow and should be marked
    as such. Here the goal is to showcase the pipeline and to make sure there is no
    drift in future releases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_large_model_tf` (`optional`): Tests the pipeline on a real pipeline where
    the results are supposed to make sense. These tests are slow and should be marked
    as such. Here the goal is to showcase the pipeline and to make sure there is no
    drift in future releases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
