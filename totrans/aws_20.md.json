["```py\noptimum-cli export neuron \\\n  --model bert-base-uncased \\\n  --sequence_length 128 \\\n  --batch_size 1 \\\n  bert_neuron/\n```", "```py\noptimum-cli export neuron --help\n```", "```py\npip install optimum[neuronx]\n```", "```py\npip install optimum[neuron]\n```", "```py\noptimum-cli export neuron --help\n\nusage: optimum-cli export neuron [-h] -m MODEL [--task TASK] [--atol ATOL] [--cache_dir CACHE_DIR] [--trust-remote-code]\n                                 [--compiler_workdir COMPILER_WORKDIR] [--disable-validation] [--auto_cast {none,matmul,all}]\n                                 [--auto_cast_type {bf16,fp16,tf32}] [--dynamic-batch-size] [--num_cores NUM_CORES] [--unet UNET]\n                                 [--output_hidden_states] [--output_attentions] [--batch_size BATCH_SIZE]\n                                 [--sequence_length SEQUENCE_LENGTH] [--num_beams NUM_BEAMS] [--num_choices NUM_CHOICES]\n                                 [--num_channels NUM_CHANNELS] [--width WIDTH] [--height HEIGHT]\n                                 [--num_images_per_prompt NUM_IMAGES_PER_PROMPT] [-O1 | -O2 | -O3]\n                                 output\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -O1                   Enables the core performance optimizations in the compiler, while also minimizing compile time.\n  -O2                   [Default] Provides the best balance between model performance and compile time.\n  -O3                   May provide additional model execution performance but may incur longer compile times and higher host\n                        memory usage during model compilation.\n\nRequired arguments:\n  -m MODEL, --model MODEL\n                        Model ID on huggingface.co or path on disk to load model from.\n  output                Path indicating the directory where to store generated Neuronx compiled TorchScript model.\n\nOptional arguments:\n  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model.\n                        Available tasks depend on the model, but are among: ['audio-classification', 'audio-frame-\n                        classification', 'audio-xvector', 'automatic-speech-recognition', 'conversational', 'depth-estimation',\n                        'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-image',\n                        'image-to-text', 'mask-generation', 'masked-im', 'multiple-choice', 'object-detection', 'question-\n                        answering', 'semantic-segmentation', 'text-to-audio', 'text-generation', 'text2text-generation', 'text-\n                        classification', 'token-classification', 'zero-shot-image-classification', 'zero-shot-object-detection',\n                        'stable-diffusion', 'stable-diffusion-xl'].\n  --atol ATOL           If specified, the absolute difference tolerance when validating the model. Otherwise, the default atol\n                        for the model will be used.\n  --cache_dir CACHE_DIR\n                        Path indicating where to store cache.\n  --trust-remote-code   Allow to use custom code for the modeling hosted in the model repository. This option should only be set\n                        for repositories you trust and in which you have read the code, as it will execute on your local machine\n                        arbitrary code present in the model repository.\n  --compiler_workdir COMPILER_WORKDIR\n                        Path indicating the directory where to store intermediary files generated by Neuronx compiler.\n  --disable-validation  Whether to disable the validation of inference on neuron device compared to the outputs of original\n                        PyTorch model on CPU.\n  --auto_cast {none,matmul,all}\n                        Whether to cast operations from FP32 to lower precision to speed up the inference. Can be `\"none\"`,\n                        `\"matmul\"` or `\"all\"`.\n  --auto_cast_type {bf16,fp16,tf32}\n                        The data type to cast FP32 operations to when auto-cast mode is enabled. Can be `\"bf16\"`, `\"fp16\"` or\n                        `\"tf32\"`.\n  --dynamic-batch-size  Enable dynamic batch size for neuron compiled model. If this option is enabled, the input batch size can\n                        be a multiple of the batch size during the compilation, but it comes with a potential tradeoff in terms\n                        of latency.\n  --num_cores NUM_CORES\n                        The number of cores on which the model should be deployed (text-generation only).\n  --unet UNET           UNet model ID on huggingface.co or path on disk to load model from. This will replace the unet in the\n                        original Stable Diffusion pipeline.\n  --output_hidden_states\n                        Whether or not for the traced model to return the hidden states of all layers.\n  --output_attentions   Whether or not for the traced model to return the attentions tensors of all attention layers.\n\nInput shapes:\n  --batch_size BATCH_SIZE\n                        Batch size that the Neuronx-cc compiler exported model will be able to take as input.\n  --sequence_length SEQUENCE_LENGTH\n                        Sequence length that the Neuronx-cc compiler exported model will be able to take as input.\n  --num_beams NUM_BEAMS\n                        Number of beams for beam search that the Neuronx-cc compiler exported model will be able to take as\n                        input.\n  --num_choices NUM_CHOICES\n                        Only for the multiple-choice task. Num choices that the Neuronx-cc compiler exported model will be able\n                        to take as input.\n  --num_channels NUM_CHANNELS\n                        Image tasks only. Number of channels that the Neuronx-cc compiler exported model will be able to take as\n                        input.\n  --width WIDTH         Image tasks only. Width that the Neuronx-cc compiler exported model will be able to take as input.\n  --height HEIGHT       Image tasks only. Height that the Neuronx-cc compiler exported model will be able to take as input.\n  --num_images_per_prompt NUM_IMAGES_PER_PROMPT\n                        Stable diffusion only. Number of images per prompt that the Neuronx-cc compiler exported model will be\n                        able to take as input.\n\n```", "```py\noptimum-cli export neuron --model distilbert-base-uncased-distilled-squad --batch_size 1 --sequence_length 16 distilbert_base_uncased_squad_neuron/\n```", "```py\nValidating Neuron model...\n        -[\u2713] Neuron model output names match reference model (last_hidden_state)\n        - Validating Neuron Model output \"last_hidden_state\":\n                -[\u2713] (1, 16, 32) matches (1, 16, 32)\n                -[\u2713] all values close (atol: 0.0001)\nThe Neuronx export succeeded and the exported model was saved at: distilbert_base_uncased_squad_neuron/\n```", "```py\noptimum-cli export neuron --model local_path --task question-answering --batch_size 1 --sequence_length 16 --dynamic-batch-size distilbert_base_uncased_squad_neuron/\n```", "```py\n>>> from optimum.neuron import NeuronModelForSequenceClassification\n\n>>> input_shapes = {\"batch_size\": 1, \"sequence_length\": 64}  # mandatory shapes\n>>> model = NeuronModelForSequenceClassification.from_pretrained(\n...   \"distilbert-base-uncased-finetuned-sst-2-english\", export=True, **input_shapes\n... )\n\n# Save the model\n>>> model.save_pretrained(\"./distilbert-base-uncased-finetuned-sst-2-english_neuron/\")\n```", "```py\n>>> from transformers import AutoTokenizer\n>>> from optimum.neuron import NeuronModelForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"./distilbert-base-uncased-finetuned-sst-2-english_neuron/\")\n>>> model = NeuronModelForSequenceClassification.from_pretrained(\"./distilbert-base-uncased-finetuned-sst-2-english_neuron/\")\n\n>>> inputs = tokenizer(\"Hamilton is considered to be the best musical of human history.\", return_tensors=\"pt\")\n>>> logits = model(**inputs).logits\n>>> print(model.config.id2label[logits.argmax().item()])\n'POSITIVE'\n```", "```py\noptimum-cli export neuron --model stabilityai/stable-diffusion-2-1-base \\\n  --task stable-diffusion \\\n  --batch_size 1 \\\n  --height 512 `# height in pixels of generated image, eg. 512, 768` \\\n  --width 512 `# width in pixels of generated image, eg. 512, 768` \\\n  --num_images_per_prompt 4 `# number of images to generate per prompt, defaults to 1` \\\n  --auto_cast matmul `# cast only matrix multiplication operations` \\\n  --auto_cast_type bf16 `# cast operations from FP32 to BF16` \\\n  sd_neuron/\n```", "```py\noptimum-cli export neuron --model stabilityai/stable-diffusion-xl-base-1.0 \\\n  --task stable-diffusion-xl \\\n  --batch_size 1 \\\n  --height 1024 `# height in pixels of generated image, eg. 768, 1024` \\\n  --width 1024 `# width in pixels of generated image, eg. 768, 1024` \\\n  --num_images_per_prompt 4 `# number of images to generate per prompt, defaults to 1` \\\n  --auto_cast matmul `# cast only matrix multiplication operations` \\\n  --auto_cast_type bf16 `# cast operations from FP32 to BF16` \\\n  sd_neuron/\n```", "```py\n>>> from optimum.exporters.tasks import TasksManager\n>>> from optimum.exporters.neuron.model_configs import *  # Register neuron specific configs to the TasksManager\n\n>>> distilbert_tasks = list(TasksManager.get_supported_tasks_for_model_type(\"distilbert\", \"neuron\").keys())\n>>> print(distilbert_tasks)\n['feature-extraction', 'fill-mask', 'multiple-choice', 'question-answering', 'text-classification', 'token-classification']\n```"]