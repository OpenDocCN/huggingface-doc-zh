["```py\nfrom peft import LoraConfig\n\nconfig = LoraConfig(init_lora_weights=\"gaussian\", ...)\n```", "```py\nfrom peft import LoraConfig\n\nconfig = LoraConfig(init_lora_weights=False, ...)\n```", "```py\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\n\nbase_model = AutoModelForCausalLM.from_pretrained(...)  # don't quantize here\nloftq_config = LoftQConfig(loftq_bits=4, ...)           # set 4bit quantization\nlora_config = LoraConfig(..., init_lora_weights=\"loftq\", loftq_config=loftq_config)\npeft_model = get_peft_model(base_model, lora_config)\n```", "```py\nfrom peft import LoraConfig\n\nconfig = LoraConfig(use_rslora=True, ...)\n```", "```py\nconfig = LoraConfig(target_modules=\"all-linear\", ...)\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\nmodel.merge_and_unload()\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\nmodel.merge_adapter()\n\n# unmerge the LoRA layers from the base model\nmodel.unmerge_adapter()\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n```", "```py\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id, adapter_name=\"sft\")\n```", "```py\nweighted_adapter_name = \"sft-dpo\"\nmodel.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")\nmodel.add_weighted_adapter(\n    adapters=[\"sft\", \"dpo\"],\n    weights=[0.7, 0.3],\n    adapter_name=weighted_adapter_name,\n    combination_type=\"linear\"\n)\nmodel.set_adapter(weighted_adapter_name)\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\nprompt = \"Hey, are you conscious? Can you talk to me?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n\nwith torch.no_grad():\n    generate_ids = model.generate(**inputs, max_length=30)\noutputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nprint(outputs)\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\n\n# load different adapter\nmodel.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")\n\n# set adapter as active\nmodel.set_adapter(\"dpo\")\n```", "```py\n# unload adapter\nmodel.unload()\n\n# delete adapter\nmodel.delete_adapter(\"dpo\")\n```"]