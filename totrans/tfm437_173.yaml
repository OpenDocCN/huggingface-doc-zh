- en: Fuyu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/fuyu](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/fuyu)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and
    authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus
    Odena, Arushi Somani, Sağnak Taşırlar.
  prefs: []
  type: TYPE_NORMAL
- en: The authors introduced Fuyu-8B, a decoder-only multimodal model based on the
    classic transformers architecture, with query and key normalization. A linear
    encoder is added to create multimodal embeddings from image inputs.
  prefs: []
  type: TYPE_NORMAL
- en: By treating image tokens like text tokens and using a special image-newline
    character, the model knows when an image line ends. Image positional embeddings
    are removed. This avoids the need for different training phases for various image
    resolutions. With 8 billion parameters and licensed under CC-BY-NC, Fuyu-8B is
    notable for its ability to handle both text and images, its impressive context
    size of 16K, and its overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `Fuyu` models were trained using `bfloat16`, but the original inference
    uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'`
    which will be used by the `AutoModel` API to cast the checkpoints from `torch.float32`
    to `torch.float16`.
  prefs: []
  type: TYPE_NORMAL
- en: The `dtype` of the online weights is mostly irrelevant, unless you are using
    `torch_dtype="auto"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`. The reason is that the model will first be downloaded
    ( using the `dtype` of the checkpoints online) then it will be cast to the default
    `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype`
    they want, and if they don’t it will be `torch.float32`.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning the model in `float16` is not recommended and known to produce `nan`,
    as such the model should be fine-tuned in `bfloat16`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tips:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert the model, you need to clone the original repository using `git
    clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For the chat model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, model can be loaded via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Inputs need to be passed through a specific Processor to have the correct formats.
    A processor requires an image_processor and a tokenizer. Hence, inputs can be
    loaded via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This model was contributed by [Molbap](https://huggingface.co/Molbap). The original
    code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).
  prefs: []
  type: TYPE_NORMAL
- en: Fuyu uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports
    bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.
    The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The authors suggest to use the following prompt for image captioning: `f"Generate
    a coco-style caption.\\n"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FuyuConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FuyuConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/configuration_fuyu.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 262144) — Vocabulary size of the
    Fuyu model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [FuyuForCausalLM](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuForCausalLM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimension of the hidden
    representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 16384) — Dimension of the
    MLP representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 36) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 64) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu2"`) — The
    non-linear activation function (function or string) in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 16384) — The maximum
    sequence length that this model might ever be used with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 300) — The input image size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 30) — The input vision transformer
    encoding patch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The input image number
    of channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the rms normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`. Whether to tie weight embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether to
    tie input and output embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rope_theta` (`float`, *optional*, defaults to 25000.0) — The base period of
    the RoPE embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rope_scaling` (`Dict`, *optional*) — Dictionary containing the scaling configuration
    for the RoPE embeddings. Currently supports two scaling strategies: linear and
    dynamic. Their scaling factor must be a float greater than 1\. The expected format
    is `{"type": strategy name, "factor": scaling factor}`. When using this flag,
    don’t update `max_position_embeddings` to the expected new maximum. See the following
    thread for more information on how these scaling strategies behave: [https://www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/).
    This is an experimental feature, subject to breaking API changes in future versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qk_layernorm` (`bool`, *optional*, defaults to `True`) — Whether or not to
    normalize the Queries and Keys after projecting the hidden states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    after applying the MLP to the hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    after computing the attention scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`partial_rotary_factor` (`float`, *optional*, defaults to 0.5) — Percentage
    of the query and keys which will have rotary embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) — The id of the *beginning-of-sequence*
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*, defaults to 2) — The id
    of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence*
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize the `language[PRE5]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '>>> from transformers import FuyuConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> # Initializing a Fuyu fuyu-7b style configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> configuration = FuyuConfig()'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '( config: FuyuConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '( input_ids: LongTensor = None image_patches: Tensor = None image_patches_indices:
    Tensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values:
    Optional = None inputs_embeds: Optional = None use_cache: Optional = None labels:
    Optional = None output_attentions: Optional = None output_hidden_states: Optional
    = None return_dict: Optional = None ) → export const metadata = ''undefined'';transformers.modeling_outputs.CausalLMOutputWithPast
    or tuple(torch.FloatTensor)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '>>> from transformers import FuyuProcessor, FuyuForCausalLM'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from PIL import Image'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> import requests'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> processor = FuyuProcessor.from_pretrained("adept/fuyu-8b")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> model = FuyuForCausalLM.from_pretrained("adept/fuyu-8b")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> image = Image.open(requests.get(url, stream=True).raw)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> prompt = "Generate a coco-style caption.\n"'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> inputs = processor(text=text_prompt, images=image, return_tensors="pt")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> outputs = model(**inputs)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> generated_ids = model.generate(**model_inputs, max_new_tokens=7)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> generation_text = processor.batch_decode(generated_ids, skip_special_tokens=True)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(generation_text)'
  prefs: []
  type: TYPE_NORMAL
- en: '''A bus parked on the side of a road.'''
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '( do_resize: bool = True size: Optional = None resample: Resampling = <Resampling.BILINEAR:
    2> do_pad: bool = True padding_value: float = 1.0 padding_mode: str = ''constant''
    do_normalize: bool = True image_mean: Union = 0.5 image_std: Union = 0.5 do_rescale:
    bool = True rescale_factor: float = 0.00392156862745098 patch_size: Optional =
    None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ( images **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ( image_processor tokenizer )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '( text = None images = None add_special_tokens: bool = True return_attention_mask:
    bool = True padding: Union = False truncation: Union = None max_length: Optional
    = None stride: int = 0 pad_to_multiple_of: Optional = None return_overflowing_tokens:
    bool = False return_special_tokens_mask: bool = False return_offsets_mapping:
    bool = False return_token_type_ids: bool = False return_length: bool = False verbose:
    bool = True return_tensors: Union = None **kwargs ) → export const metadata =
    ''undefined'';FuyuBatchEncoding'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`) — The sequence or batch of sequences to be encoded.
    Each sequence can be a string or a list of strings (pretokenized string). If the
    sequences are provided as list of strings (pretokenized), you must set `is_split_into_words=True`
    (to lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`images` (`PIL.Image.Image`, `List[PIL.Image.Image]`) — The image or batch
    of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
    tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape
    (C, H, W), where C is a number of channels, H and W are image height and width.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`FuyuBatchEncoding`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `FuyuBatchEncoding` with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` — Tensor of token ids to be fed to a model. Returned when `text`
    is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_patches` — List of Tensor of image patches. Returned when `images` is
    not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_patches_indices` — Tensor of indices where patch embeddings have to
    be inserted by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model when `return_attention_mask=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to prepare for the model one or several sequences(s) and image(s).
    This method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast’s
    [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    if `text` is not `None` to encode the text. To prepare the image(s), this method
    forwards the `images` and `kwargs` arguments to FuyuImageProcessor’s [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    if `images` is not `None`. Please refer to the doctsring of the above two methods
    for more information.
  prefs: []
  type: TYPE_NORMAL
