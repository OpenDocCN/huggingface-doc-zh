- en: Fuyu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/fuyu](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/fuyu)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/136.972bfe5f.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and
    authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus
    Odena, Arushi Somani, Sağnak Taşırlar.
  prefs: []
  type: TYPE_NORMAL
- en: The authors introduced Fuyu-8B, a decoder-only multimodal model based on the
    classic transformers architecture, with query and key normalization. A linear
    encoder is added to create multimodal embeddings from image inputs.
  prefs: []
  type: TYPE_NORMAL
- en: By treating image tokens like text tokens and using a special image-newline
    character, the model knows when an image line ends. Image positional embeddings
    are removed. This avoids the need for different training phases for various image
    resolutions. With 8 billion parameters and licensed under CC-BY-NC, Fuyu-8B is
    notable for its ability to handle both text and images, its impressive context
    size of 16K, and its overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `Fuyu` models were trained using `bfloat16`, but the original inference
    uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'`
    which will be used by the `AutoModel` API to cast the checkpoints from `torch.float32`
    to `torch.float16`.
  prefs: []
  type: TYPE_NORMAL
- en: The `dtype` of the online weights is mostly irrelevant, unless you are using
    `torch_dtype="auto"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`. The reason is that the model will first be downloaded
    ( using the `dtype` of the checkpoints online) then it will be cast to the default
    `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype`
    they want, and if they don’t it will be `torch.float32`.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning the model in `float16` is not recommended and known to produce `nan`,
    as such the model should be fine-tuned in `bfloat16`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tips:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert the model, you need to clone the original repository using `git
    clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For the chat model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, model can be loaded via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Inputs need to be passed through a specific Processor to have the correct formats.
    A processor requires an image_processor and a tokenizer. Hence, inputs can be
    loaded via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This model was contributed by [Molbap](https://huggingface.co/Molbap). The original
    code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).
  prefs: []
  type: TYPE_NORMAL
- en: Fuyu uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports
    bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.
    The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The authors suggest to use the following prompt for image captioning: `f"Generate
    a coco-style caption.\\n"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FuyuConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.FuyuConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/configuration_fuyu.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_size = 262144 hidden_size = 4096 intermediate_size = 16384 num_hidden_layers
    = 36 num_attention_heads = 64 hidden_act = 'relu2' max_position_embeddings = 16384
    image_size = 300 patch_size = 30 num_channels = 3 initializer_range = 0.02 layer_norm_eps
    = 1e-05 use_cache = True tie_word_embeddings = False rope_theta = 25000.0 rope_scaling
    = None qk_layernorm = True hidden_dropout = 0.0 attention_dropout = 0.0 partial_rotary_factor
    = 0.5 pad_token_id = None bos_token_id = 1 eos_token_id = 2 text_config = None
    **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 262144) — Vocabulary size of
    the Fuyu model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [FuyuForCausalLM](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuForCausalLM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 4096) — Dimension of the hidden
    representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intermediate_size** (`int`, *optional*, defaults to 16384) — Dimension of
    the MLP representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hidden_layers** (`int`, *optional*, defaults to 36) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 64) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `function`, *optional*, defaults to `"relu2"`) — The
    non-linear activation function (function or string) in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_position_embeddings** (`int`, *optional*, defaults to 16384) — The maximum
    sequence length that this model might ever be used with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_size** (`int`, *optional*, defaults to 300) — The input image size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_size** (`int`, *optional*, defaults to 30) — The input vision transformer
    encoding patch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_channels** (`int`, *optional*, defaults to 3) — The input image number
    of channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the rms normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*, defaults to `True`) — Whether or not the
    model should return the last key/values attentions (not used by all models). Only
    relevant if `config.is_decoder=True`. Whether to tie weight embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tie_word_embeddings** (`bool`, *optional*, defaults to `False`) — Whether
    to tie input and output embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rope_theta** (`float`, *optional*, defaults to 25000.0) — The base period
    of the RoPE embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rope_scaling** (`Dict`, *optional*) — Dictionary containing the scaling configuration
    for the RoPE embeddings. Currently supports two scaling strategies: linear and
    dynamic. Their scaling factor must be a float greater than 1\. The expected format
    is `{"type": strategy name, "factor": scaling factor}`. When using this flag,
    don’t update `max_position_embeddings` to the expected new maximum. See the following
    thread for more information on how these scaling strategies behave: [https://www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/).
    This is an experimental feature, subject to breaking API changes in future versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qk_layernorm** (`bool`, *optional*, defaults to `True`) — Whether or not
    to normalize the Queries and Keys after projecting the hidden states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_dropout** (`float`, *optional*, defaults to 0.0) — The dropout ratio
    after applying the MLP to the hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_dropout** (`float`, *optional*, defaults to 0.0) — The dropout
    ratio after computing the attention scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**partial_rotary_factor** (`float`, *optional*, defaults to 0.5) — Percentage
    of the query and keys which will have rotary embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_id** (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token_id** (`int`, *optional*, defaults to 1) — The id of the *beginning-of-sequence*
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token_id** (`Union[int, List[int]]`, *optional*, defaults to 2) — The
    id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence*
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_config** (`dict`, *optional*) — Dictionary of configuration options
    used to initialize the `language[PRE4]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '>>> from transformers import FuyuConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> # Initializing a Fuyu fuyu-7b style configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> configuration = FuyuConfig()'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '>>> from transformers import FuyuProcessor, FuyuForCausalLM'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from PIL import Image'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> import requests'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> processor = FuyuProcessor.from_pretrained("adept/fuyu-8b")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> model = FuyuForCausalLM.from_pretrained("adept/fuyu-8b")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> image = Image.open(requests.get(url, stream=True).raw)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> prompt = "Generate a coco-style caption.\n"'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> inputs = processor(text=text_prompt, images=image, return_tensors="pt")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> outputs = model(**inputs)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> generated_ids = model.generate(**model_inputs, max_new_tokens=7)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> generation_text = processor.batch_decode(generated_ids, skip_special_tokens=True)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(generation_text)'
  prefs: []
  type: TYPE_NORMAL
- en: '''A bus parked on the side of a road.'''
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: FuyuImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.FuyuImageProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/image_processing_fuyu.py#L180)'
  prefs: []
  type: TYPE_NORMAL
- en: '( do_resize: bool = True size: Optional = None resample: Resampling = <Resampling.BILINEAR:
    2> do_pad: bool = True padding_value: float = 1.0 padding_mode: str = ''constant''
    do_normalize: bool = True image_mean: Union = 0.5 image_std: Union = 0.5 do_rescale:
    bool = True rescale_factor: float = 0.00392156862745098 patch_size: Optional =
    None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**do_resize** (`bool`, *optional*, defaults to `True`) — Whether to resize
    the image to `size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size** (`Dict[str, int]`, *optional*, defaults to `{"height" -- 1080, "width":
    1920}`): Dictionary in the format `{"height": int, "width": int}` specifying the
    size of the output image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resample** (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_pad** (`bool`, *optional*, defaults to `True`) — Whether to pad the image
    to `size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding_value** (`float`, *optional*, defaults to 1.0) — The value to pad
    the image with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding_mode** (`str`, *optional*, defaults to `"constant"`) — The padding
    mode to use when padding the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_normalize** (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_mean** (`float`, *optional*, defaults to 0.5) — The mean to use when
    normalizing the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_std** (`float`, *optional*, defaults to 0.5) — The standard deviation
    to use when normalizing the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_rescale** (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale_factor** (`float`, *optional*, defaults to `1 / 255`) — The factor
    to use when rescaling the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_size** (`Dict[str, int]`, *optional*, defaults to `{"height" -- 30,
    "width": 30}`): Dictionary in the format `{"height": int, "width": int}` specifying
    the size of the patches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This class should handle the image processing part before the main FuyuForCausalLM.
    In particular, it should handle:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing Images: Taking a batch of images as input. If the images are variable-sized,
    it resizes them based on the desired patch dimensions. The image output is always
    img_h, img_w of (1080, 1920)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, it patches up these images using the patchify_image function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Creating Image Input IDs: For each patch, a placeholder ID is given to identify
    where these patches belong in a token sequence. For variable-sized images, each
    line of patches is terminated with a newline ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Patch Indices: For each image patch, the code maintains an index where
    these patches should be inserted in a token stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  prefs: []
  type: TYPE_NORMAL
- en: ( images **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess an image or a batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: FuyuProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.FuyuProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/processing_fuyu.py#L309)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image_processor tokenizer )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image_processor** ([FuyuImageProcessor](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuImageProcessor))
    — The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** ([LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast))
    — The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Fuyu processor which wraps a Fuyu image processor and a Llama tokenizer
    into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[FuyuProcessor](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuProcessor)
    offers all the functionalities of [FuyuImageProcessor](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuImageProcessor)
    and [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuProcessor.__call__)
    and `decode()` for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/processing_fuyu.py#L451)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text = None images = None add_special_tokens: bool = True return_attention_mask:
    bool = True padding: Union = False truncation: Union = None max_length: Optional
    = None stride: int = 0 pad_to_multiple_of: Optional = None return_overflowing_tokens:
    bool = False return_special_tokens_mask: bool = False return_offsets_mapping:
    bool = False return_token_type_ids: bool = False return_length: bool = False verbose:
    bool = True return_tensors: Union = None **kwargs ) → `FuyuBatchEncoding`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`, `List[str]`) — The sequence or batch of sequences to be encoded.
    Each sequence can be a string or a list of strings (pretokenized string). If the
    sequences are provided as list of strings (pretokenized), you must set `is_split_into_words=True`
    (to lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**images** (`PIL.Image.Image`, `List[PIL.Image.Image]`) — The image or batch
    of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
    tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape
    (C, H, W), where C is a number of channels, H and W are image height and width.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`FuyuBatchEncoding`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `FuyuBatchEncoding` with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** — Tensor of token ids to be fed to a model. Returned when `text`
    is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_patches** — List of Tensor of image patches. Returned when `images`
    is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_patches_indices** — Tensor of indices where patch embeddings have to
    be inserted by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** — List of indices specifying which tokens should be attended
    to by the model when `return_attention_mask=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to prepare for the model one or several sequences(s) and image(s).
    This method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast’s
    [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    if `text` is not `None` to encode the text. To prepare the image(s), this method
    forwards the `images` and `kwargs` arguments to FuyuImageProcessor’s [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    if `images` is not `None`. Please refer to the doctsring of the above two methods
    for more information.
  prefs: []
  type: TYPE_NORMAL
