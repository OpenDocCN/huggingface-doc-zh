# YOLOS

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/yolos](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/yolos)

## æ¦‚è¿°

YOLOSæ¨¡å‹æ˜¯ç”±Yuxin Fangã€Bencheng Liaoã€Xinggang Wangã€Jiemin Fangã€Jiyang Qiã€Rui Wuã€Jianwei Niuã€Wenyu Liuåœ¨[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666)ä¸­æå‡ºçš„ã€‚YOLOSå»ºè®®ä»…åˆ©ç”¨æ™®é€šçš„[Vision Transformer (ViT)](vit)è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå—åˆ°DETRçš„å¯å‘ã€‚ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨åŸºæœ¬å¤§å°çš„ä»…ç¼–ç å™¨Transformerä¹Ÿå¯ä»¥åœ¨COCOä¸Šå®ç°42 APï¼Œç±»ä¼¼äºDETRå’Œæ›´å¤æ‚çš„æ¡†æ¶ï¼Œå¦‚Faster R-CNNã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

Transformerèƒ½å¦ä»çº¯åºåˆ—åˆ°åºåˆ—çš„è§’åº¦æ‰§è¡Œ2Då¯¹è±¡å’ŒåŒºåŸŸçº§åˆ«çš„è¯†åˆ«ï¼Œè€Œå¯¹2Dç©ºé—´ç»“æ„çš„äº†è§£å¾ˆå°‘ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†You Only Look at One Sequence (YOLOS)ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŸºäºæ™®é€šVision Transformerçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œåªè¿›è¡Œäº†æœ€å°‘çš„ä¿®æ”¹ã€åŒºåŸŸå…ˆéªŒä»¥åŠç›®æ ‡ä»»åŠ¡çš„å½’çº³åå·®ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…åœ¨ä¸­ç­‰è§„æ¨¡çš„ImageNet-1kæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„YOLOSæ¨¡å‹å·²ç»å¯ä»¥åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„COCOç›®æ ‡æ£€æµ‹åŸºå‡†ä¸Šå–å¾—ç›¸å½“æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œç›´æ¥é‡‡ç”¨BERT-Baseæ¶æ„çš„YOLOS-Baseåœ¨COCO valä¸Šå¯ä»¥è·å¾—42.0çš„box APã€‚æˆ‘ä»¬è¿˜é€šè¿‡YOLOSè®¨è®ºäº†å½“å‰é¢„è®­ç»ƒæ–¹æ¡ˆå’ŒTransformerè§†è§‰æ¨¡å‹æ‰©å±•ç­–ç•¥çš„å½±å“ä»¥åŠå±€é™æ€§ã€‚

![drawing](../Images/b49c1370806db7d0d200d783ed143c9f.png) YOLOSæ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2106.00666)ã€‚

æ­¤æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/hustvl/YOLOS)æ‰¾åˆ°ã€‚

## èµ„æº

å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨YOLOSã€‚

ç›®æ ‡æ£€æµ‹

+   æ‰€æœ‰æ¼”ç¤ºæ¨æ–­+å¾®è°ƒ[YolosForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosForObjectDetection)åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šçš„ç¤ºä¾‹ç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/YOLOS)æ‰¾åˆ°ã€‚

+   å¦è¯·å‚é˜…ï¼š[ç›®æ ‡æ£€æµ‹ä»»åŠ¡æŒ‡å—](../tasks/object_detection)

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

ä½¿ç”¨[YolosImageProcessor](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosImageProcessor)æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒï¼ˆå’Œå¯é€‰ç›®æ ‡ï¼‰ã€‚ä¸[DETR](detr)ç›¸åï¼ŒYOLOSä¸éœ€è¦åˆ›å»º`pixel_mask`ã€‚

## YolosConfig

### `class transformers.YolosConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/configuration_yolos.py#L35)

```py
( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = [512, 864] patch_size = 16 num_channels = 3 qkv_bias = True num_detection_tokens = 100 use_mid_position_embeddings = True auxiliary_loss = False class_cost = 1 bbox_cost = 5 giou_cost = 2 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 eos_coefficient = 0.1 **kwargs )
```

å‚æ•°

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º768ï¼‰â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º12ï¼‰â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º12ï¼‰â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3072ï¼‰â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act`ï¼ˆ`str`æˆ–`function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚

+   `hidden_dropout_prob`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `initializer_range` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `image_size` (`List[int]`, *optional*, é»˜è®¤ä¸º`[512, 864]`) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size` (`int`, *optional*, é»˜è®¤ä¸º16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `num_channels` (`int`, *optional*, é»˜è®¤ä¸º3) â€” è¾“å…¥é€šé“çš„æ•°é‡ã€‚

+   `qkv_bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åç½®ã€‚

+   `num_detection_tokens` (`int`, *optional*, é»˜è®¤ä¸º100) â€” æ£€æµ‹ä»¤ç‰Œçš„æ•°é‡ã€‚

+   `use_mid_position_embeddings` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä½¿ç”¨ä¸­é—´å±‚ä½ç½®ç¼–ç ã€‚

+   `auxiliary_loss` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä½¿ç”¨è¾…åŠ©è§£ç æŸå¤±ï¼ˆæ¯ä¸ªè§£ç å™¨å±‚çš„æŸå¤±ï¼‰ã€‚

+   `class_cost` (`float`, *optional*, é»˜è®¤ä¸º1) â€” åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­åˆ†ç±»é”™è¯¯çš„ç›¸å¯¹æƒé‡ã€‚

+   `bbox_cost` (`float`, *optional*, é»˜è®¤ä¸º5) â€” åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­è¾¹ç•Œæ¡†åæ ‡çš„L1è¯¯å·®çš„ç›¸å¯¹æƒé‡ã€‚

+   YolosImageProcessor

+   `bbox_loss_coefficient` (`float`, *optional*, é»˜è®¤ä¸º5) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­L1è¾¹ç•Œæ¡†æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚

+   `giou_loss_coefficient` (`float`, *optional*, é»˜è®¤ä¸º2) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­å¹¿ä¹‰IoUæŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚

+   `eos_coefficient` (`float`, *optional*, é»˜è®¤ä¸º0.1) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­â€œæ— å¯¹è±¡â€ç±»çš„ç›¸å¯¹åˆ†ç±»æƒé‡ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[YolosModel](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªYOLOSæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºYOLOS [hustvl/yolos-base](https://huggingface.co/hustvl/yolos-base)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import YolosConfig, YolosModel

>>> # Initializing a YOLOS hustvl/yolos-base style configuration
>>> configuration = YolosConfig()

>>> # Initializing a model (with random weights) from the hustvl/yolos-base style configuration
>>> model = YolosModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## `giou_cost` (`float`, *optional*, é»˜è®¤ä¸º2) â€” å¹¿ä¹‰IoUæŸå¤±åœ¨åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­è¾¹ç•Œæ¡†çš„ç›¸å¯¹æƒé‡ã€‚

### `class transformers.YolosImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/image_processing_yolos.py#L668)

```py
( format: Union = <AnnotationFormat.COCO_DETECTION: 'coco_detection'> do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )
```

å‚æ•°

+   `format` (`str`, *optional*, é»˜è®¤ä¸º`"coco_detection"`) â€” æ³¨é‡Šçš„æ•°æ®æ ¼å¼ã€‚ä¸º`"coco_detection"`æˆ–â€œcoco_panopticâ€ä¹‹ä¸€ã€‚

+   `do_resize` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ§åˆ¶æ˜¯å¦å°†å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ç»´åº¦è°ƒæ•´ä¸ºæŒ‡å®šçš„`size`ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_resize`å‚æ•°è¦†ç›–ã€‚

+   `size` (`Dict[str, int]` *optional*, é»˜è®¤ä¸º`{"shortest_edge" -- 800, "longest_edge": 1333}`): è°ƒæ•´å¤§å°åçš„å›¾åƒï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ç»´åº¦ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`size`å‚æ•°è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`, *optional*, é»˜è®¤ä¸º`PILImageResampling.BILINEAR`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚

+   `do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ§åˆ¶æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹`rescale_factor`é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_rescale`å‚æ•°è¦†ç›–ã€‚

+   `rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒè¦ä½¿ç”¨çš„æ¯”ä¾‹å› å­ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`rescale_factor`å‚æ•°è¦†ç›–ã€‚do_normalize â€” æ§åˆ¶æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_normalize`å‚æ•°è¦†ç›–ã€‚

+   `image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`) â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„å‡å€¼ã€‚å¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–æ¯ä¸ªé€šé“çš„å€¼åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_mean`å‚æ•°è¦†ç›–ã€‚

+   `image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`) â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„æ ‡å‡†å·®å€¼ã€‚å¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–æ¯ä¸ªé€šé“çš„å€¼åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_std`å‚æ•°è¦†ç›–ã€‚

+   `do_pad` (`bool`, *optional*, defaults to `True`) â€” æ§åˆ¶æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå¡«å……ä»¥é€‚åº”æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒå¹¶åˆ›å»ºåƒç´ æ©æ¨¡ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_pad`å‚æ•°è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ªDetrå›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/image_processing_yolos.py#L1006)

```py
( images: Union annotations: Union = None return_segmentation_masks: bool = None masks_path: Union = None do_resize: Optional = None size: Optional = None resample = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None format: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–åƒç´ å€¼èŒƒå›´ä»0åˆ°255çš„å›¾åƒæ‰¹æ¬¡ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚

+   `annotations` (`AnnotationType` or `List[AnnotationType]`, *optional*) â€” ä¸å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡å…³è”çš„æ³¨é‡Šåˆ—è¡¨ã€‚å¦‚æœæ³¨é‡Šç”¨äºç›®æ ‡æ£€æµ‹ï¼Œæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹é”®çš„å­—å…¸ï¼š

    +   â€œimage_idâ€ (`int`): å›¾åƒçš„IDã€‚

    +   â€œannotationsâ€ (`List[Dict]`): å›¾åƒçš„æ³¨é‡Šåˆ—è¡¨ã€‚æ¯ä¸ªæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ã€‚ä¸€ä¸ªå›¾åƒå¯ä»¥æ²¡æœ‰æ³¨é‡Šï¼Œæ­¤æ—¶åˆ—è¡¨åº”ä¸ºç©ºã€‚å¦‚æœæ³¨é‡Šç”¨äºåˆ†å‰²ï¼Œæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹é”®çš„å­—å…¸ï¼š

    +   â€œimage_idâ€ (`int`): å›¾åƒçš„IDã€‚

    +   â€œsegments_infoâ€ (`List[Dict]`): å›¾åƒçš„æ®µåˆ—è¡¨ã€‚æ¯ä¸ªæ®µåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ã€‚ä¸€ä¸ªå›¾åƒå¯ä»¥æ²¡æœ‰æ®µï¼Œæ­¤æ—¶åˆ—è¡¨åº”ä¸ºç©ºã€‚

    +   â€œfile_nameâ€ (`str`): å›¾åƒçš„æ–‡ä»¶åã€‚

+   `return_segmentation_masks` (`bool`, *optional*, defaults to self.return_segmentation_masks) â€” æ˜¯å¦è¿”å›åˆ†å‰²æ©æ¨¡ã€‚

+   `masks_path` (`str` or `pathlib.Path`, *optional*) â€” åŒ…å«åˆ†å‰²æ©æ¨¡çš„ç›®å½•è·¯å¾„ã€‚

+   `do_resize` (`bool`, *optional*, defaults to self.do_resize) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`, *optional*, defaults to self.size) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚

+   `resample` (`PILImageResampling`, *optional*, defaults to self.resample) â€” è°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚

+   `do_rescale` (`bool`, *optional*, defaults to self.do_rescale) â€” æ˜¯å¦é‡æ–°ç¼©æ”¾å›¾åƒã€‚

+   `rescale_factor` (`float`, *optional*, defaults to self.rescale_factor) â€” è°ƒæ•´å›¾åƒæ—¶ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚

+   `do_normalize` (`bool`, *optional*, defaults to self.do_normalize) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean` (`float` or `List[float]`, *optional*, defaults to self.image_mean) â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„å‡å€¼ã€‚

+   `image_std` (`float` or `List[float]`, *optional*, defaults to self.image_std) â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚

+   `do_pad` (`bool`, *optional*, defaults to self.do_pad) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå¡«å……ã€‚

+   `format` (`str` or `AnnotationFormat`, *optional*, defaults to self.format) â€” æ³¨é‡Šçš„æ ¼å¼ã€‚

+   `return_tensors` (`str` or `TensorType`, *optional*, defaults to self.return_tensors) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¦‚æœä¸º`None`ï¼Œå°†è¿”å›å›¾åƒåˆ—è¡¨ã€‚

+   `data_format` (`str` æˆ– `ChannelDimension`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.data_format) â€” å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä¸è¾“å…¥å›¾åƒç›¸åŒã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé€šé“ç»´åº¦æ ¼å¼å°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ (num_channels, height, width) æ ¼å¼ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ (height, width, num_channels) æ ¼å¼ã€‚

    +   `"none"` æˆ– `ChannelDimension.NONE`ï¼šå›¾åƒä»¥ (height, width) æ ¼å¼ã€‚

å¯¹å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿å¯ä»¥è¢«æ¨¡å‹ä½¿ç”¨ã€‚

#### `pad`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/image_processing_yolos.py#L951)

```py
( images: List constant_values: Union = 0 return_pixel_mask: bool = False return_tensors: Union = None data_format: Optional = None input_data_format: Union = None )
```

å‚æ•°

+   `image` (`np.ndarray`) â€” è¦å¡«å……çš„å›¾åƒã€‚

+   `constant_values` (`float` æˆ– `Iterable[float]`ï¼Œ*å¯é€‰*) â€” å¦‚æœ `mode` æ˜¯ `"constant"`ï¼Œåˆ™ç”¨äºå¡«å……çš„å€¼ã€‚

+   `return_pixel_mask` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å›åƒç´ æ©ç ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`ï¼šè¿”å›ä¸€ä¸ª `tf.Tensor` ç±»å‹çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`ï¼šè¿”å›ä¸€ä¸ª `torch.Tensor` ç±»å‹çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`ï¼šè¿”å›ä¸€ä¸ª `np.ndarray` ç±»å‹çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`ï¼šè¿”å›ä¸€ä¸ª `jax.numpy.ndarray` ç±»å‹çš„æ‰¹æ¬¡ã€‚

+   `data_format` (`str` æˆ– `ChannelDimension`ï¼Œ*å¯é€‰*) â€” å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä¸è¾“å…¥å›¾åƒç›¸åŒã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†è¢«æ¨æ–­ã€‚

å°†å›¾åƒæ‰¹æ¬¡å¡«å……åˆ°å›¾åƒçš„åº•éƒ¨å’Œå³ä¾§ï¼Œç”¨é›¶å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§é«˜åº¦å’Œå®½åº¦çš„å¤§å°ï¼Œå¹¶å¯é€‰æ‹©è¿”å›å®ƒä»¬å¯¹åº”çš„åƒç´ æ©ç ã€‚

#### `post_process_object_detection`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/image_processing_yolos.py#L1274)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs` (`YolosObjectDetectionOutput`) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `threshold` (`float`ï¼Œ*å¯é€‰*) â€” ä¿ç•™å¯¹è±¡æ£€æµ‹é¢„æµ‹çš„åˆ†æ•°é˜ˆå€¼ã€‚

+   `target_sizes` (`torch.Tensor` æˆ– `List[Tuple[int, int]]`ï¼Œ*å¯é€‰*) â€” å½¢çŠ¶ä¸º `(batch_size, 2)` çš„å¼ é‡æˆ–åŒ…å«æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾åƒç›®æ ‡å¤§å° `(height, width)` çš„å…ƒç»„åˆ—è¡¨ (`Tuple[int, int]`)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹é¢„æµ‹çš„æ‰¹æ¬¡ä¸­å›¾åƒçš„åˆ†æ•°ã€æ ‡ç­¾å’Œæ¡†ã€‚

å°† [YolosForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosForObjectDetection) çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ€ç»ˆçš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º (top_left_x, top_left_y, bottom_right_x, bottom_right_y)ã€‚ä»…æ”¯æŒ PyTorchã€‚

## YolosFeatureExtractor

### `class transformers.YolosFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/feature_extraction_yolos.py#L36)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚

#### `pad`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/image_processing_yolos.py#L951)

```py
( images: List constant_values: Union = 0 return_pixel_mask: bool = False return_tensors: Union = None data_format: Optional = None input_data_format: Union = None )
```

å‚æ•°

+   `image` (`np.ndarray`) â€” è¦å¡«å……çš„å›¾åƒã€‚

+   `constant_values` (`float` æˆ– `Iterable[float]`ï¼Œ*å¯é€‰*) â€” å¦‚æœ `mode` æ˜¯ `"constant"`ï¼Œåˆ™ç”¨äºå¡«å……çš„å€¼ã€‚

+   `return_pixel_mask` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å›åƒç´ æ©ç ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`ï¼šè¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`ï¼šè¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`ï¼šè¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`ï¼šè¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹å¤„ç†ã€‚

+   `data_format` (`str` æˆ– `ChannelDimension`ï¼Œ*å¯é€‰*) â€” å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä¸è¾“å…¥å›¾åƒç›¸åŒã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œå°†è¢«æ¨æ–­ã€‚

å°†å›¾åƒæ‰¹å¤„ç†å¡«å……åˆ°å›¾åƒçš„åº•éƒ¨å’Œå³ä¾§ï¼Œç”¨é›¶å¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€å¤§é«˜åº¦å’Œå®½åº¦çš„å¤§å°ï¼Œå¹¶å¯é€‰æ‹©è¿”å›ç›¸åº”çš„åƒç´ æ©ç ã€‚

#### `post_process_object_detection`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/image_processing_yolos.py#L1274)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs` (`YolosObjectDetectionOutput`) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `threshold` (`float`ï¼Œ*å¯é€‰*) â€” ä¿ç•™å¯¹è±¡æ£€æµ‹é¢„æµ‹çš„åˆ†æ•°é˜ˆå€¼ã€‚

+   `target_sizes` (`torch.Tensor` æˆ– `List[Tuple[int, int]]`ï¼Œ*å¯é€‰*) â€” å½¢çŠ¶ä¸º `(batch_size, 2)` çš„å¼ é‡æˆ–åŒ…å«æ‰¹å¤„ç†ä¸­æ¯ä¸ªå›¾åƒçš„ç›®æ ‡å¤§å° `(height, width)` çš„å…ƒç»„åˆ—è¡¨ (`Tuple[int, int]`)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ‰¹å¤„ç†ä¸­å›¾åƒçš„é¢„æµ‹å¾—åˆ†ã€æ ‡ç­¾å’Œæ¡†ã€‚

å°† [YolosForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosForObjectDetection) çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ€ç»ˆè¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º (å·¦ä¸Šè§’ xã€å·¦ä¸Šè§’ yã€å³ä¸‹è§’ xã€å³ä¸‹è§’ y)ã€‚ä»…æ”¯æŒ PyTorchã€‚

## YolosModel

### `class transformers.YolosModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/modeling_yolos.py#L583)

```py
( config: YolosConfig add_pooling_layer: bool = True )
```

å‚æ•°

+   `config` ([YolosConfig](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ YOLOS æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨åœ¨é¡¶éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/modeling_yolos.py#L615)

```py
( pixel_values: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ [YolosImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨æ˜¯ `masked`ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–ä¸€ä¸ª`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®([YolosConfig](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosConfig))å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” ç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åçš„åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¯¹äºBERTç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œtanhæ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[YolosModel](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, YolosModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("hustvl/yolos-small")
>>> model = YolosModel.from_pretrained("hustvl/yolos-small")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 3401, 384]
```

## YolosForObjectDetection

### `class transformers.YolosForObjectDetection`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/modeling_yolos.py#L689)

```py
( config: YolosConfig )
```

å‚æ•°

+   `config`ï¼ˆ[YolosConfig](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosConfig)ï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

YOLOSæ¨¡å‹ï¼ˆåŒ…å«ViTç¼–ç å™¨ï¼‰ï¼Œé¡¶éƒ¨å¸¦æœ‰ç›®æ ‡æ£€æµ‹å¤´ï¼Œç”¨äºè¯¸å¦‚COCOæ£€æµ‹ä¹‹ç±»çš„ä»»åŠ¡ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/yolos/modeling_yolos.py#L722)

```py
( pixel_values: FloatTensor labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.yolos.modeling_yolos.YolosObjectDetectionOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[YolosImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels` (`List[Dict]` of len `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—äºŒåˆ†åŒ¹é…æŸå¤±çš„æ ‡ç­¾ã€‚å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸è‡³å°‘åŒ…å«ä»¥ä¸‹2ä¸ªé”®ï¼š`'class_labels'`å’Œ`'boxes'`ï¼ˆåˆ†åˆ«æ˜¯æ‰¹å¤„ç†ä¸­å›¾åƒçš„ç±»åˆ«æ ‡ç­¾å’Œè¾¹ç•Œæ¡†ï¼‰ã€‚ç±»åˆ«æ ‡ç­¾æœ¬èº«åº”è¯¥æ˜¯é•¿åº¦ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†çš„æ•°é‡,)`çš„`torch.LongTensor`ï¼Œè€Œæ¡†æ˜¯å½¢çŠ¶ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†çš„æ•°é‡, 4)`çš„`torch.FloatTensor`ã€‚

è¿”å›

`transformers.models.yolos.modeling_yolos.YolosObjectDetectionOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.yolos.modeling_yolos.YolosObjectDetectionOutput`æˆ–ä¸€ä¸ªå…ƒç»„`torch.FloatTensor`ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[YolosConfig](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” æ€»æŸå¤±ï¼Œä½œä¸ºç±»åˆ«é¢„æµ‹çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾¹ç•Œæ¡†æŸå¤±çš„çº¿æ€§ç»„åˆã€‚åè€…è¢«å®šä¹‰ä¸ºL1æŸå¤±å’Œå¹¿ä¹‰æ¯”ä¾‹ä¸å˜IoUæŸå¤±çš„çº¿æ€§ç»„åˆã€‚

+   `loss_dict` (`Dict`, *optional*) â€” åŒ…å«å„ä¸ªæŸå¤±çš„å­—å…¸ã€‚ç”¨äºè®°å½•ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†ç±»logitsï¼ˆåŒ…æ‹¬æ— å¯¹è±¡ï¼‰ã€‚

+   `pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„å½’ä¸€åŒ–æ¡†åæ ‡ï¼Œè¡¨ç¤ºä¸ºï¼ˆä¸­å¿ƒ_xï¼Œä¸­å¿ƒ_yï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰ã€‚è¿™äº›å€¼åœ¨[0, 1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œç›¸å¯¹äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªå•ç‹¬å›¾åƒçš„å¤§å°ï¼ˆå¿½ç•¥å¯èƒ½çš„å¡«å……ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`post_process()`æ¥æ£€ç´¢æœªå½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†ã€‚

+   `auxiliary_outputs` (`list[Dict]`, *optional*) â€” å¯é€‰ï¼Œä»…åœ¨æ¿€æ´»è¾…åŠ©æŸå¤±ï¼ˆå³`config.auxiliary_loss`è®¾ç½®ä¸º`True`ï¼‰å¹¶æä¾›æ ‡ç­¾æ—¶è¿”å›ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªè§£ç å™¨å±‚çš„ä¸Šè¿°ä¸¤ä¸ªé”®ï¼ˆ`logits`å’Œ`pred_boxes`ï¼‰çš„å­—å…¸åˆ—è¡¨ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[YolosForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/yolos#transformers.YolosForObjectDetection)çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, AutoModelForObjectDetection
>>> import torch
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("hustvl/yolos-tiny")
>>> model = AutoModelForObjectDetection.from_pretrained("hustvl/yolos-tiny")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> target_sizes = torch.tensor([image.size[::-1]])
>>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[
...     0
... ]

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected remote with confidence 0.994 at location [46.96, 72.61, 181.02, 119.73]
Detected remote with confidence 0.975 at location [340.66, 79.19, 372.59, 192.65]
Detected cat with confidence 0.984 at location [12.27, 54.25, 319.42, 470.99]
Detected remote with confidence 0.922 at location [41.66, 71.96, 178.7, 120.33]
Detected cat with confidence 0.914 at location [342.34, 21.48, 638.64, 372.46]
```
