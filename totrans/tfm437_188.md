# Llama2

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2)

## 概述

Llama2模型是由Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez Madian Khabsa、Isabel Kloumann、Artem Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushka rMishra、Igor Molybog、Yixin Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing EllenTan、Binh Tang、Ross Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela Fan、Melanie Kambadur、Sharan Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov、Thomas Scialom提出的，它是一个包含从7B到70B参数的基础语言模型的集合，具有为聊天应用程序调优的检查点！

论文的摘要如下：

*在这项工作中，我们开发并发布了Llama 2，这是一组预训练和调优的大型语言模型（LLMs），规模从70亿到700亿参数不等。我们调优的LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上优于开源聊天模型，并根据我们的人类评估，对于帮助和安全性，可能是封闭源模型的合适替代品。我们提供了关于我们对Llama 2-Chat进行调优和安全改进方法的详细描述，以便使社区能够在我们的工作基础上构建并促进LLMs的负责任发展。*

查看所有Llama2模型检查点[这里](https://huggingface.co/models?search=llama2)。该模型由[Arthur Zucker](https://huggingface.co/ArthurZ)贡献，[Lysandre Debut](https://huggingface.co/lysandre)也有贡献。Hugging Face中的实现代码基于GPT-NeoX [这里](https://github.com/EleutherAI/gpt-neox)。作者的原始代码可以在[这里](https://github.com/facebookresearch/llama)找到。

## 使用提示

`Llama2`模型是使用`bfloat16`进行训练的，但原始推断使用`float16`。Hub上上传的检查点使用`torch_dtype='float16'`，`AutoModel` API将使用它将检查点从`torch.float32`转换为`torch.float16`。

在线权重的`dtype`大多不相关，除非您在使用`model = AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`初始化模型时使用`torch_dtype="auto"`。原因是模型将首先被下载（使用在线检查点的`dtype`），然后将被转换为`torch`的默认`dtype`（变为`torch.float32`），最后，如果配置中提供了`torch_dtype`，则将使用它。

不建议在`float16`中训练模型，已知会产生`nan`；因此，模型应该在`bfloat16`中进行训练。

提示：

+   Llama2模型的权重可以通过填写[此表格](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)获得

+   该架构与第一个Llama非常相似，增加了Grouped Query Attention（GQA），参考这篇[论文](https://arxiv.org/pdf/2305.13245.pdf)

+   将 `config.pretraining_tp` 设置为与 1 不同的值将激活线性层的更准确但更慢的计算，这应该更好地匹配原始对数。

+   原始模型使用 `pad_id = -1`，这意味着没有填充标记。我们不能使用相同的逻辑，确保使用 `tokenizer.add_special_tokens({"pad_token":"<pad>"})` 添加一个填充标记，并相应调整令牌嵌入。您还应该设置 `model.config.pad_token_id`。模型的 `embed_tokens` 层使用 `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)` 进行初始化，这确保了对填充标记进行编码将输出零，因此在初始化时传递它是推荐的。

+   填写表格并获得模型检查点访问权限后，您应该能够使用已转换的检查点。否则，如果您正在转换自己的模型，请随时使用 [转换脚本](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)。可以使用以下（示例）命令调用脚本：

```py
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

+   转换后，可以通过以下方式加载模型和分词器：

```py
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

请注意，执行脚本需要足够的 CPU RAM 以在 float16 精度中托管整个模型（即使最大版本分为多个检查点，每个检查点都包含模型的每个权重的一部分，因此我们需要将它们全部加载到 RAM 中）。对于 75B 模型，因此需要 145GB 的 RAM。

+   LLaMA 分词器是基于 [sentencepiece](https://github.com/google/sentencepiece) 的 BPE 模型。sentencepiece 的一个特点是，在解码序列时，如果第一个令牌是单词的开头（例如“Banana”），分词器不会在字符串前面添加前缀空格。

+   通过 `attn_implementation="flash_attention_2"` 使用 Flash Attention 2 时，不要将 `torch_dtype` 传递给 `from_pretrained` 类方法，并使用自动混合精度训练。当使用 `Trainer` 时，只需将 `fp16` 或 `bf16` 指定为 `True`。否则，请确保您使用 `torch.autocast`。这是必需的，因为 Flash Attention 仅支持 `fp16` 和 `bf16` 数据类型。

## 资源

一个官方 Hugging Face 和社区（由 🌎 表示）资源列表，可帮助您开始使用 LLaMA2。如果您有兴趣提交资源以包含在此处，请随时打开一个 Pull Request，我们将对其进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。

+   [Llama 2 已发布 - 在 Hugging Face 上获取](https://huggingface.co/blog/llama2)，关于 Llama 2 及如何与 🤗 Transformers 和 🤗 PEFT 一起使用的博客文章。

+   [LLaMA 2 - 您需要的所有资源](https://www.philschmid.de/llama-2)，一个相关资源的汇编，用于了解 LLaMA 2 并快速入门。

文本生成

+   一个关于如何在 Google Colab 中使用 QLoRA 和 4 位精度对 Llama 2 进行微调的 [笔记本](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)。🌎

+   一个关于如何使用 4 位 QLoRA 对“Llama-v2-7b-guanaco”模型进行微调并从 PDF 中生成问答数据集的 [笔记本](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing)。🌎

文本分类

+   一个关于如何使用 QLoRa、TRL 和韩文文本分类数据集对 Llama 2 模型进行微调的 [笔记本](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing)。🌎🇰🇷

⚗️ 优化

+   [使用 DPO 对 Llama 2 进行微调](https://huggingface.co/blog/dpo-trl)，一个指南，介绍如何使用 TRL 库的 DPO 方法对特定数据集上的 Llama 2 进行微调。

+   [扩展指南：指导调整 Llama 2](https://www.philschmid.de/instruction-tune-llama-2)，一个指南，用于训练 Llama 2 从输入生成指令，将模型从遵循指令转变为给出指令。

+   一个[笔记本](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing)，介绍如何在个人计算机上使用 QLoRa 和 TRL 对 Llama 2 模型进行微调。

⚡️ 推理

+   一个[笔记本](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)，介绍如何使用 AutoGPTQ 库中的 GPTQ 对 Llama 2 模型进行量化。

+   一个[笔记本](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing)，介绍如何在本地计算机或 Google Colab 上运行带有 4 位量化的 Llama 2 Chat Model。

🚀 部署

+   [在亚马逊 SageMaker 上对 LLaMA 2 (7-70B) 进行微调](https://www.philschmid.de/sagemaker-llama2-qlora)，从设置到 QLoRA 微调和部署的完整指南。

+   [在亚马逊 SageMaker 上部署 Llama 2 7B/13B/70B](https://www.philschmid.de/sagemaker-llama-llm)，使用 Hugging Face 的 LLM DLC 容器进行安全和可扩展部署的指南。

## LlamaConfig

### `class transformers.LlamaConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/configuration_llama.py#L31)

```py
( vocab_size = 32000 hidden_size = 4096 intermediate_size = 11008 num_hidden_layers = 32 num_attention_heads = 32 num_key_value_heads = None hidden_act = 'silu' max_position_embeddings = 2048 initializer_range = 0.02 rms_norm_eps = 1e-06 use_cache = True pad_token_id = None bos_token_id = 1 eos_token_id = 2 pretraining_tp = 1 tie_word_embeddings = False rope_theta = 10000.0 rope_scaling = None attention_bias = False attention_dropout = 0.0 **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为 32000) — LLaMA 模型的词汇量。定义了在调用 [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel) 时可以表示的不同令牌数量。

+   `hidden_size` (`int`, *可选*, 默认为 4096) — 隐藏表示的维度。

+   `intermediate_size` (`int`, *可选*, 默认为 11008) — MLP 表示的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为 32) — Transformer 解码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*, 默认为 32) — Transformer 解码器中每个注意力层的注意力头数量。

+   `num_key_value_heads` (`int`, *可选*) — 这是应该用于实现 Grouped Query Attention 的 key_value heads 的数量。如果 `num_key_value_heads=num_attention_heads`，模型将使用 Multi Head Attention (MHA)，如果 `num_key_value_heads=1`，模型将使用 Multi Query Attention (MQA)，否则使用 GQA。将多头检查点转换为 GQA 检查点时，每个组键和值头应通过对该组中所有原始头进行均值池化来构建。有关更多详细信息，请查看[此论文](https://arxiv.org/pdf/2305.13245.pdf)。如果未指定，将默认为` num_attention_heads`。

+   `hidden_act` (`str` 或 `function`, *可选*, 默认为 `"silu"`) — 解码器中的非线性激活函数（函数或字符串）。

+   `max_position_embeddings` (`int`, *可选*, 默认为 2048) — 此模型可能使用的最大序列长度。Llama 1 支持最多 2048 个令牌，Llama 2 支持最多 4096 个令牌，CodeLlama 支持最多 16384 个令牌。

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `rms_norm_eps` (`float`, *可选*, 默认为 1e-06) — rms normalization 层使用的 epsilon。

+   `use_cache` (`bool`, *可选*, 默认为 `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在 `config.is_decoder=True` 时相关。

+   `pad_token_id` (`int`, *可选*) — 填充令牌 id。

+   `bos_token_id` (`int`, *可选*, 默认为 1) — 流的开始令牌 id。

+   `eos_token_id` (`int`, *可选*, 默认为 2) — 流的结束令牌 id。

+   `pretraining_tp` (`int`, *可选*, 默认为 1) — 实验性功能。在预训练期间使用的张量并行性等级。请参考[此文档](https://huggingface.co/docs/transformers/parallelism)以了解更多信息。此值对于确保预训练结果的精确可重现性是必要的。请参考[此问题](https://github.com/pytorch/pytorch/issues/76232)。

+   `tie_word_embeddings`（`bool`，*可选*，默认为`False`）— 是否绑定权重嵌入

+   `rope_theta`（`float`，*可选*，默认为10000.0）— RoPE嵌入的基本周期。

+   `rope_scaling`（`Dict`，*可选*）— 包含RoPE嵌入的缩放配置的字典。目前支持两种缩放策略：线性和动态。它们的缩放因子必须是大于1的浮点数。预期格式为`{"type": 策略名称, "factor": 缩放因子}`。使用此标志时，不要将`max_position_embeddings`更新为预期的新最大值。有关这些缩放策略行为的更多信息，请参阅以下主题：[https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)。这是一个实验性功能，可能在未来版本中发生破坏性API更改。

+   `attention_bias`（`bool`，默认为`False`，*可选*，默认为`False`）— 在自注意力期间的查询、键、值和输出投影层中是否使用偏置。

+   `attention_dropout`（`float`，*可选*，默认为0.0）— 注意力概率的dropout比率。

这是用于存储[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)配置的配置类。它用于根据指定的参数实例化一个LLaMA模型，定义模型架构。使用默认值实例化配置将产生类似于LLaMA-7B的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

```py
>>> from transformers import LlamaModel, LlamaConfig

>>> # Initializing a LLaMA llama-7b style configuration
>>> configuration = LlamaConfig()

>>> # Initializing a model from the llama-7b style configuration
>>> model = LlamaModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LlamaTokenizer

### `class transformers.LlamaTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L66)

```py
( vocab_file unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' pad_token = None sp_model_kwargs: Optional = None add_bos_token = True add_eos_token = False clean_up_tokenization_spaces = False use_default_system_prompt = False spaces_between_special_tokens = False legacy = None **kwargs )
```

参数

+   `vocab_file`（`str`）— 词汇文件的路径。

+   `unk_token`（`str`或`tokenizers.AddedToken`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `bos_token`（`str`或`tokenizers.AddedToken`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可用作序列分类器标记。

+   `eos_token`（`str`或`tokenizers.AddedToken`，*可选*，默认为`"</s>"`）— 序列结束标记。

+   `pad_token`（`str`或`tokenizers.AddedToken`，*可选*）— 用于使令牌数组大小相同以进行批处理的特殊令牌。然后将被注意力机制或损失计算忽略。

+   `sp_model_kwargs`（`Dict[str, Any]`，`Optional`，*可选*）— 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`：启用子词正则化。

    +   `nbest_size`：unigram的抽样参数。对于BPE-Dropout无效。

        +   `nbest_size = {0,1}`：不执行抽样。

        +   `nbest_size > 1`：从nbest_size结果中抽样。

        +   `nbest_size < 0`：假设nbest_size为无限大，并使用前向过滤和后向抽样算法从所有假设（格子）中抽样。

    +   `alpha`：unigram抽样的平滑参数，以及BPE-dropout合并操作的dropout概率。

+   `add_bos_token`（`bool`，*可选*，默认为`True`）— 是否在序列开头添加`bos_token`。

+   `add_eos_token`（`bool`，*可选*，默认为`False`）— 是否在序列末尾添加`eos_token`。

+   `clean_up_tokenization_spaces`（`bool`，*可选*，默认为`False`）— 是否在解码后清除空格，清除包括删除额外空格等潜在工件。

+   `use_default_system_prompt`（`bool`，*可选*，默认为`False`）— 是否应使用Llama的默认系统提示。

+   `spaces_between_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在特殊标记之间添加空格。

+   `legacy`（`bool`，*可选*）— 是否应使用分词器的`legacy`行为。在合并＃24622和＃25224之前的遗留版本中，包括修复正确处理出现在特殊标记之后的标记的问题。一个简单的例子：

    +   `legacy=True`：

构建一个Llama分词器。基于字节级字节对编码。默认填充标记未设置，因为原始模型中没有填充标记。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L333)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L344)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经使用模型的特殊标记格式化。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列ID。在使用分词器的`prepare_for_model`方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L381)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。

返回

`List[int]`

根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。一个ALBERT

序列对掩码的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果token_ids_1为None，则仅返回掩码的第一部分（0s）。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L306)

```py
( save_directory filename_prefix: Optional = None ) → export const metadata = 'undefined';Tuple(str)
```

参数

+   `save_directory`（`str`）— 保存词汇表的目录。

返回

`Tuple(str)`

保存的文件路径。

将词汇表和特殊标记文件保存到目录中。

## LlamaTokenizerFast

### `class transformers.LlamaTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L57)

```py
( vocab_file = None tokenizer_file = None clean_up_tokenization_spaces = False unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' add_bos_token = True add_eos_token = False use_default_system_prompt = False **kwargs )
```

参数

+   `vocab_file`（`str`，*可选*）— [SentencePiece](https://github.com/google/sentencepiece)文件（通常具有.model扩展名），其中包含实例化分词器所需的词汇表。

+   `tokenizer_file`（`str`，*可选*）— [tokenizers](https://github.com/huggingface/tokenizers)文件（通常具有.json扩展名），其中包含加载分词器所需的所有内容。

+   `clean_up_tokenization_spaces`（`bool`，*可选*，默认为`False`）— 是否在解码后清除空格，清除包括删除额外空格等潜在工件。

+   `unk_token`（`str`或`tokenizers.AddedToken`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `bos_token`（`str`或`tokenizers.AddedToken`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

+   `eos_token`（`str`或`tokenizers.AddedToken`，*可选*，默认为`"</s>"`）— 序列结束标记。

+   `add_bos_token`（`bool`，*可选*，默认为`True`）— 是否在序列开头添加`bos_token`。

+   `add_eos_token`（`bool`，*可选*，默认为`False`）— 是否在序列末尾添加`eos_token`。

+   `use_default_system_prompt`（`bool`，*可选*，默认为`False`）— 是否使用Llama的默认系统提示。

构建一个Llama分词器。基于字节级字节对编码。

这主要使用ByteFallback和无规范化。

```py
>>> from transformers import LlamaTokenizerFast

>>> tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
>>> tokenizer.encode("Hello this is a test")
[1, 15043, 445, 338, 263, 1243]
```

如果要更改`bos_token`或`eos_token`，请确保在初始化模型时指定它们，或调用`tokenizer.update_post_processor()`以确保后处理正确完成（否则编码序列的第一个标记和最后一个标记的值将不正确）。有关更多详细信息，请查看[后处理器]（[https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors)）文档。

此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L272)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';A list of integers in the range [0, 1]
```

参数

+   `token_ids_0`（`List[int]`）— 第一个序列的id列表。

+   `token_ids_1`（`List[int]`，*可选*）— 第二个序列的id列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经格式化为模型的特殊标记。

返回

一个范围在[0, 1]内的整数列表

1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列id。在使用tokenizer的`prepare_for_model`或`encode_plus`方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— 第一个标记化序列。

+   `token_ids_1`（`List[int]`，*可选*）— 第二个标记化序列。

返回

`List[int]`

标记类型id。

创建与传递的序列对应的标记类型ID。[什么是标记类型ID？](../glossary#token-type-ids)

如果模型有特殊的构建方式，应该在子类中重写此方法。

#### `update_post_processor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L146)

```py
( )
```

更新底层的后处理器，使用当前的`bos_token`和`eos_token`。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L190)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LlamaModel

### `class transformers.LlamaModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L939)

```py
( config: LlamaConfig )
```

参数

+   `config`（[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。配置 — LlamaConfig

裸的LLaMA模型输出原始隐藏状态，没有特定的头部。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

Transformer解码器由*config.num_hidden_layers*层组成。每一层都是一个`LlamaDecoderLayer`。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L974)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被掩盖的标记，

    +   0表示被掩盖的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    如果使用`past_key_values`，则可能只需要输入最后的`input_ids`（参见`past_key_values`）。

    如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。

    +   1表示头部未被掩盖，

    +   0表示头部被掩盖。

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `past_key_values`（`Cache`或`tuple(tuple(torch.FloatTensor))`，*可选*）— 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在先前解码阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。

    允许两种格式：

    +   一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；

    +   长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。这也被称为传统的缓存格式。

    该模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统的缓存格式。

    如果使用`past_key_values`，用户可以选择仅输入最后的`input_ids`（这些没有将它们的过去键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*） — 可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `use_cache`（`bool`，*可选*） — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## LlamaForCausalLM

### `class transformers.LlamaForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1106)

```py
( config )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1136)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*） — 避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   对于未被“掩盖”的标记，为1，

    +   0表示被“掩盖”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    如果使用`past_key_values`，可选择仅输入最后的`input_ids`（参见`past_key_values`）。

    如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。

    +   1表示头部未被“掩盖”，

    +   0表示头部被“掩盖”。

+   `position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `past_key_values`（`Cache`或`tuple(tuple(torch.FloatTensor))`，*可选*）— 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在解码的先前阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。

    允许两种格式：

    +   一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；

    +   长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。这也被称为传统的缓存格式。

    模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统的缓存格式。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`input_ids`（那些没有将它们的过去键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。

+   `inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

    参数 — 标签（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）：用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`范围内，或者为-100（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

返回

[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)）和输入的不同元素。

+   `loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回）— 语言建模损失（用于下一个标记预测）。

+   `logits`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量）

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[LlamaForCausalLM](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForCausalLM)的前向方法重写了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, LlamaForCausalLM

>>> model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
>>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

>>> prompt = "Hey, are you conscious? Can you talk to me?"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(inputs.input_ids, max_length=30)
>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
```

## LlamaForSequenceClassification

### `class transformers.LlamaForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1295)

```py
( config )
```

参数

+   `config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有顶部序列分类头（线性层）的LLaMa模型变换器。

[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)使用最后一个标记来进行分类，就像其他因果模型（例如GPT-2）一样。

由于它对最后一个标记进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，则在每行中找到不是填充标记的最后一个标记。如果未定义`pad_token_id`，则在批次的每行中简单地取最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此执行相同操作（在批次的每行中取最后一个值）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1326)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下，如果提供了填充，则将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。

    [注意力掩码是什么？](../glossary#attention-mask)

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    如果使用了`past_key_values`，则可以选择仅输入最后的`input_ids`（参见`past_key_values`）。

    如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `past_key_values`（`Cache`或`tuple(tuple(torch.FloatTensor))`，*可选*）- 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在先前解码阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。

    允许两种格式：

    +   一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；

    +   长度为`config.n_layers`的`tuple(torch.FloatTensor)`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。这也被称为传统的缓存格式。

    模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统的缓存格式。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`input_ids`（即那些没有将其过去的键值状态提供给该模型的输入）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 中。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
