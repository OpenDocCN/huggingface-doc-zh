# Llama2

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2)

## æ¦‚è¿°

Llama2æ¨¡å‹æ˜¯ç”±Hugo Touvronã€Louis Martinã€Kevin Stoneã€Peter Albertã€Amjad Almahairiã€Yasmine Babaeiã€Nikolay Bashlykovã€Soumya Batraã€Prajjwal Bhargavaã€Shruti Bhosaleã€Dan Bikelã€Lukas Blecherã€Cristian Canton Ferrerã€Moya Chenã€Guillem Cucurullã€David Esiobuã€Jude Fernandesã€Jeremy Fuã€Wenyin Fuã€Brian Fullerã€Cynthia Gaoã€Vedanuj Goswamiã€Naman Goyalã€Anthony Hartshornã€Saghar Hosseiniã€Rui Houã€Hakan Inanã€Marcin Kardasã€Viktor Kerkez Madian Khabsaã€Isabel Kloumannã€Artem Korenevã€Punit Singh Kouraã€Marie-Anne Lachauxã€Thibaut Lavrilã€Jenya Leeã€Diana Liskovichã€Yinghai Luã€Yuning Maoã€Xavier Martinetã€Todor Mihaylovã€Pushka rMishraã€Igor Molybogã€Yixin Nieã€Andrew Poultonã€Jeremy Reizensteinã€Rashi Rungtaã€Kalyan Saladiã€Alan Scheltenã€Ruan Silvaã€Eric Michael Smithã€Ranjan Subramanianã€Xiaoqing EllenTanã€Binh Tangã€Ross Taylorã€Adina Williamsã€Jian Xiang Kuanã€Puxin Xuã€Zheng Yanã€Iliyan Zarovã€Yuchen Zhangã€Angela Fanã€Melanie Kambadurã€Sharan Narangã€Aurelien Rodriguezã€Robert Stojnicã€Sergey Edunovã€Thomas Scialomæå‡ºçš„ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å«ä»7Båˆ°70Bå‚æ•°çš„åŸºç¡€è¯­è¨€æ¨¡å‹çš„é›†åˆï¼Œå…·æœ‰ä¸ºèŠå¤©åº”ç”¨ç¨‹åºè°ƒä¼˜çš„æ£€æŸ¥ç‚¹ï¼

è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº†Llama 2ï¼Œè¿™æ˜¯ä¸€ç»„é¢„è®­ç»ƒå’Œè°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè§„æ¨¡ä»70äº¿åˆ°700äº¿å‚æ•°ä¸ç­‰ã€‚æˆ‘ä»¬è°ƒä¼˜çš„LLMsï¼Œç§°ä¸ºLlama 2-Chatï¼Œé’ˆå¯¹å¯¹è¯ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ‘ä»¬æµ‹è¯•çš„å¤§å¤šæ•°åŸºå‡†ä¸Šä¼˜äºå¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„äººç±»è¯„ä¼°ï¼Œå¯¹äºå¸®åŠ©å’Œå®‰å…¨æ€§ï¼Œå¯èƒ½æ˜¯å°é—­æºæ¨¡å‹çš„åˆé€‚æ›¿ä»£å“ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæˆ‘ä»¬å¯¹Llama 2-Chatè¿›è¡Œè°ƒä¼˜å’Œå®‰å…¨æ”¹è¿›æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œä»¥ä¾¿ä½¿ç¤¾åŒºèƒ½å¤Ÿåœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šæ„å»ºå¹¶ä¿ƒè¿›LLMsçš„è´Ÿè´£ä»»å‘å±•ã€‚*

æŸ¥çœ‹æ‰€æœ‰Llama2æ¨¡å‹æ£€æŸ¥ç‚¹[è¿™é‡Œ](https://huggingface.co/models?search=llama2)ã€‚è¯¥æ¨¡å‹ç”±[Arthur Zucker](https://huggingface.co/ArthurZ)è´¡çŒ®ï¼Œ[Lysandre Debut](https://huggingface.co/lysandre)ä¹Ÿæœ‰è´¡çŒ®ã€‚Hugging Faceä¸­çš„å®ç°ä»£ç åŸºäºGPT-NeoX [è¿™é‡Œ](https://github.com/EleutherAI/gpt-neox)ã€‚ä½œè€…çš„åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/llama)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

`Llama2`æ¨¡å‹æ˜¯ä½¿ç”¨`bfloat16`è¿›è¡Œè®­ç»ƒçš„ï¼Œä½†åŸå§‹æ¨æ–­ä½¿ç”¨`float16`ã€‚Hubä¸Šä¸Šä¼ çš„æ£€æŸ¥ç‚¹ä½¿ç”¨`torch_dtype='float16'`ï¼Œ`AutoModel` APIå°†ä½¿ç”¨å®ƒå°†æ£€æŸ¥ç‚¹ä»`torch.float32`è½¬æ¢ä¸º`torch.float16`ã€‚

åœ¨çº¿æƒé‡çš„`dtype`å¤§å¤šä¸ç›¸å…³ï¼Œé™¤éæ‚¨åœ¨ä½¿ç”¨`model = AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`åˆå§‹åŒ–æ¨¡å‹æ—¶ä½¿ç”¨`torch_dtype="auto"`ã€‚åŸå› æ˜¯æ¨¡å‹å°†é¦–å…ˆè¢«ä¸‹è½½ï¼ˆä½¿ç”¨åœ¨çº¿æ£€æŸ¥ç‚¹çš„`dtype`ï¼‰ï¼Œç„¶åå°†è¢«è½¬æ¢ä¸º`torch`çš„é»˜è®¤`dtype`ï¼ˆå˜ä¸º`torch.float32`ï¼‰ï¼Œæœ€åï¼Œå¦‚æœé…ç½®ä¸­æä¾›äº†`torch_dtype`ï¼Œåˆ™å°†ä½¿ç”¨å®ƒã€‚

ä¸å»ºè®®åœ¨`float16`ä¸­è®­ç»ƒæ¨¡å‹ï¼Œå·²çŸ¥ä¼šäº§ç”Ÿ`nan`ï¼›å› æ­¤ï¼Œæ¨¡å‹åº”è¯¥åœ¨`bfloat16`ä¸­è¿›è¡Œè®­ç»ƒã€‚

æç¤ºï¼š

+   Llama2æ¨¡å‹çš„æƒé‡å¯ä»¥é€šè¿‡å¡«å†™[æ­¤è¡¨æ ¼](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)è·å¾—

+   è¯¥æ¶æ„ä¸ç¬¬ä¸€ä¸ªLlamaéå¸¸ç›¸ä¼¼ï¼Œå¢åŠ äº†Grouped Query Attentionï¼ˆGQAï¼‰ï¼Œå‚è€ƒè¿™ç¯‡[è®ºæ–‡](https://arxiv.org/pdf/2305.13245.pdf)

+   å°† `config.pretraining_tp` è®¾ç½®ä¸ºä¸ 1 ä¸åŒçš„å€¼å°†æ¿€æ´»çº¿æ€§å±‚çš„æ›´å‡†ç¡®ä½†æ›´æ…¢çš„è®¡ç®—ï¼Œè¿™åº”è¯¥æ›´å¥½åœ°åŒ¹é…åŸå§‹å¯¹æ•°ã€‚

+   åŸå§‹æ¨¡å‹ä½¿ç”¨ `pad_id = -1`ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰å¡«å……æ ‡è®°ã€‚æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼Œç¡®ä¿ä½¿ç”¨ `tokenizer.add_special_tokens({"pad_token":"<pad>"})` æ·»åŠ ä¸€ä¸ªå¡«å……æ ‡è®°ï¼Œå¹¶ç›¸åº”è°ƒæ•´ä»¤ç‰ŒåµŒå…¥ã€‚æ‚¨è¿˜åº”è¯¥è®¾ç½® `model.config.pad_token_id`ã€‚æ¨¡å‹çš„ `embed_tokens` å±‚ä½¿ç”¨ `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)` è¿›è¡Œåˆå§‹åŒ–ï¼Œè¿™ç¡®ä¿äº†å¯¹å¡«å……æ ‡è®°è¿›è¡Œç¼–ç å°†è¾“å‡ºé›¶ï¼Œå› æ­¤åœ¨åˆå§‹åŒ–æ—¶ä¼ é€’å®ƒæ˜¯æ¨èçš„ã€‚

+   å¡«å†™è¡¨æ ¼å¹¶è·å¾—æ¨¡å‹æ£€æŸ¥ç‚¹è®¿é—®æƒé™åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨å·²è½¬æ¢çš„æ£€æŸ¥ç‚¹ã€‚å¦åˆ™ï¼Œå¦‚æœæ‚¨æ­£åœ¨è½¬æ¢è‡ªå·±çš„æ¨¡å‹ï¼Œè¯·éšæ—¶ä½¿ç”¨ [è½¬æ¢è„šæœ¬](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ï¼ˆç¤ºä¾‹ï¼‰å‘½ä»¤è°ƒç”¨è„šæœ¬ï¼š

```py
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

+   è½¬æ¢åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š

```py
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

è¯·æ³¨æ„ï¼Œæ‰§è¡Œè„šæœ¬éœ€è¦è¶³å¤Ÿçš„ CPU RAM ä»¥åœ¨ float16 ç²¾åº¦ä¸­æ‰˜ç®¡æ•´ä¸ªæ¨¡å‹ï¼ˆå³ä½¿æœ€å¤§ç‰ˆæœ¬åˆ†ä¸ºå¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œæ¯ä¸ªæ£€æŸ¥ç‚¹éƒ½åŒ…å«æ¨¡å‹çš„æ¯ä¸ªæƒé‡çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å…¨éƒ¨åŠ è½½åˆ° RAM ä¸­ï¼‰ã€‚å¯¹äº 75B æ¨¡å‹ï¼Œå› æ­¤éœ€è¦ 145GB çš„ RAMã€‚

+   LLaMA åˆ†è¯å™¨æ˜¯åŸºäº [sentencepiece](https://github.com/google/sentencepiece) çš„ BPE æ¨¡å‹ã€‚sentencepiece çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯ï¼Œåœ¨è§£ç åºåˆ—æ—¶ï¼Œå¦‚æœç¬¬ä¸€ä¸ªä»¤ç‰Œæ˜¯å•è¯çš„å¼€å¤´ï¼ˆä¾‹å¦‚â€œBananaâ€ï¼‰ï¼Œåˆ†è¯å™¨ä¸ä¼šåœ¨å­—ç¬¦ä¸²å‰é¢æ·»åŠ å‰ç¼€ç©ºæ ¼ã€‚

+   é€šè¿‡ `attn_implementation="flash_attention_2"` ä½¿ç”¨ Flash Attention 2 æ—¶ï¼Œä¸è¦å°† `torch_dtype` ä¼ é€’ç»™ `from_pretrained` ç±»æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒã€‚å½“ä½¿ç”¨ `Trainer` æ—¶ï¼Œåªéœ€å°† `fp16` æˆ– `bf16` æŒ‡å®šä¸º `True`ã€‚å¦åˆ™ï¼Œè¯·ç¡®ä¿æ‚¨ä½¿ç”¨ `torch.autocast`ã€‚è¿™æ˜¯å¿…éœ€çš„ï¼Œå› ä¸º Flash Attention ä»…æ”¯æŒ `fp16` å’Œ `bf16` æ•°æ®ç±»å‹ã€‚

## èµ„æº

ä¸€ä¸ªå®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ LLaMA2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

+   [Llama 2 å·²å‘å¸ƒ - åœ¨ Hugging Face ä¸Šè·å–](https://huggingface.co/blog/llama2)ï¼Œå…³äº Llama 2 åŠå¦‚ä½•ä¸ ğŸ¤— Transformers å’Œ ğŸ¤— PEFT ä¸€èµ·ä½¿ç”¨çš„åšå®¢æ–‡ç« ã€‚

+   [LLaMA 2 - æ‚¨éœ€è¦çš„æ‰€æœ‰èµ„æº](https://www.philschmid.de/llama-2)ï¼Œä¸€ä¸ªç›¸å…³èµ„æºçš„æ±‡ç¼–ï¼Œç”¨äºäº†è§£ LLaMA 2 å¹¶å¿«é€Ÿå…¥é—¨ã€‚

æ–‡æœ¬ç”Ÿæˆ

+   ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨ Google Colab ä¸­ä½¿ç”¨ QLoRA å’Œ 4 ä½ç²¾åº¦å¯¹ Llama 2 è¿›è¡Œå¾®è°ƒçš„ [ç¬”è®°æœ¬](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)ã€‚ğŸŒ

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ 4 ä½ QLoRA å¯¹â€œLlama-v2-7b-guanacoâ€æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶ä» PDF ä¸­ç”Ÿæˆé—®ç­”æ•°æ®é›†çš„ [ç¬”è®°æœ¬](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing)ã€‚ğŸŒ

æ–‡æœ¬åˆ†ç±»

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ QLoRaã€TRL å’ŒéŸ©æ–‡æ–‡æœ¬åˆ†ç±»æ•°æ®é›†å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ [ç¬”è®°æœ¬](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing)ã€‚ğŸŒğŸ‡°ğŸ‡·

âš—ï¸ ä¼˜åŒ–

+   [ä½¿ç”¨ DPO å¯¹ Llama 2 è¿›è¡Œå¾®è°ƒ](https://huggingface.co/blog/dpo-trl)ï¼Œä¸€ä¸ªæŒ‡å—ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨ TRL åº“çš„ DPO æ–¹æ³•å¯¹ç‰¹å®šæ•°æ®é›†ä¸Šçš„ Llama 2 è¿›è¡Œå¾®è°ƒã€‚

+   [æ‰©å±•æŒ‡å—ï¼šæŒ‡å¯¼è°ƒæ•´ Llama 2](https://www.philschmid.de/instruction-tune-llama-2)ï¼Œä¸€ä¸ªæŒ‡å—ï¼Œç”¨äºè®­ç»ƒ Llama 2 ä»è¾“å…¥ç”ŸæˆæŒ‡ä»¤ï¼Œå°†æ¨¡å‹ä»éµå¾ªæŒ‡ä»¤è½¬å˜ä¸ºç»™å‡ºæŒ‡ä»¤ã€‚

+   ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing)ï¼Œä»‹ç»å¦‚ä½•åœ¨ä¸ªäººè®¡ç®—æœºä¸Šä½¿ç”¨ QLoRa å’Œ TRL å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

âš¡ï¸ æ¨ç†

+   ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨ AutoGPTQ åº“ä¸­çš„ GPTQ å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚

+   ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing)ï¼Œä»‹ç»å¦‚ä½•åœ¨æœ¬åœ°è®¡ç®—æœºæˆ– Google Colab ä¸Šè¿è¡Œå¸¦æœ‰ 4 ä½é‡åŒ–çš„ Llama 2 Chat Modelã€‚

ğŸš€ éƒ¨ç½²

+   [åœ¨äºšé©¬é€Š SageMaker ä¸Šå¯¹ LLaMA 2 (7-70B) è¿›è¡Œå¾®è°ƒ](https://www.philschmid.de/sagemaker-llama2-qlora)ï¼Œä»è®¾ç½®åˆ° QLoRA å¾®è°ƒå’Œéƒ¨ç½²çš„å®Œæ•´æŒ‡å—ã€‚

+   [åœ¨äºšé©¬é€Š SageMaker ä¸Šéƒ¨ç½² Llama 2 7B/13B/70B](https://www.philschmid.de/sagemaker-llama-llm)ï¼Œä½¿ç”¨ Hugging Face çš„ LLM DLC å®¹å™¨è¿›è¡Œå®‰å…¨å’Œå¯æ‰©å±•éƒ¨ç½²çš„æŒ‡å—ã€‚

## LlamaConfig

### `class transformers.LlamaConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/configuration_llama.py#L31)

```py
( vocab_size = 32000 hidden_size = 4096 intermediate_size = 11008 num_hidden_layers = 32 num_attention_heads = 32 num_key_value_heads = None hidden_act = 'silu' max_position_embeddings = 2048 initializer_range = 0.02 rms_norm_eps = 1e-06 use_cache = True pad_token_id = None bos_token_id = 1 eos_token_id = 2 pretraining_tp = 1 tie_word_embeddings = False rope_theta = 10000.0 rope_scaling = None attention_bias = False attention_dropout = 0.0 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 32000) â€” LLaMA æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel) æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚

+   `hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 4096) â€” éšè—è¡¨ç¤ºçš„ç»´åº¦ã€‚

+   `intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 11008) â€” MLP è¡¨ç¤ºçš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 32) â€” Transformer è§£ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 32) â€” Transformer è§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `num_key_value_heads` (`int`, *å¯é€‰*) â€” è¿™æ˜¯åº”è¯¥ç”¨äºå®ç° Grouped Query Attention çš„ key_value heads çš„æ•°é‡ã€‚å¦‚æœ `num_key_value_heads=num_attention_heads`ï¼Œæ¨¡å‹å°†ä½¿ç”¨ Multi Head Attention (MHA)ï¼Œå¦‚æœ `num_key_value_heads=1`ï¼Œæ¨¡å‹å°†ä½¿ç”¨ Multi Query Attention (MQA)ï¼Œå¦åˆ™ä½¿ç”¨ GQAã€‚å°†å¤šå¤´æ£€æŸ¥ç‚¹è½¬æ¢ä¸º GQA æ£€æŸ¥ç‚¹æ—¶ï¼Œæ¯ä¸ªç»„é”®å’Œå€¼å¤´åº”é€šè¿‡å¯¹è¯¥ç»„ä¸­æ‰€æœ‰åŸå§‹å¤´è¿›è¡Œå‡å€¼æ± åŒ–æ¥æ„å»ºã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ­¤è®ºæ–‡](https://arxiv.org/pdf/2305.13245.pdf)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†é»˜è®¤ä¸º` num_attention_heads`ã€‚

+   `hidden_act` (`str` æˆ– `function`, *å¯é€‰*, é»˜è®¤ä¸º `"silu"`) â€” è§£ç å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚

+   `max_position_embeddings` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2048) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚Llama 1 æ”¯æŒæœ€å¤š 2048 ä¸ªä»¤ç‰Œï¼ŒLlama 2 æ”¯æŒæœ€å¤š 4096 ä¸ªä»¤ç‰Œï¼ŒCodeLlama æ”¯æŒæœ€å¤š 16384 ä¸ªä»¤ç‰Œã€‚

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `rms_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-06) â€” rms normalization å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨ `config.is_decoder=True` æ—¶ç›¸å…³ã€‚

+   `pad_token_id` (`int`, *å¯é€‰*) â€” å¡«å……ä»¤ç‰Œ idã€‚

+   `bos_token_id` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æµçš„å¼€å§‹ä»¤ç‰Œ idã€‚

+   `eos_token_id` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2) â€” æµçš„ç»“æŸä»¤ç‰Œ idã€‚

+   `pretraining_tp` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” å®éªŒæ€§åŠŸèƒ½ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å¼ é‡å¹¶è¡Œæ€§ç­‰çº§ã€‚è¯·å‚è€ƒ[æ­¤æ–‡æ¡£](https://huggingface.co/docs/transformers/parallelism)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ­¤å€¼å¯¹äºç¡®ä¿é¢„è®­ç»ƒç»“æœçš„ç²¾ç¡®å¯é‡ç°æ€§æ˜¯å¿…è¦çš„ã€‚è¯·å‚è€ƒ[æ­¤é—®é¢˜](https://github.com/pytorch/pytorch/issues/76232)ã€‚

+   `tie_word_embeddings`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ç»‘å®šæƒé‡åµŒå…¥

+   `rope_theta`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º10000.0ï¼‰â€” RoPEåµŒå…¥çš„åŸºæœ¬å‘¨æœŸã€‚

+   `rope_scaling`ï¼ˆ`Dict`ï¼Œ*å¯é€‰*ï¼‰â€” åŒ…å«RoPEåµŒå…¥çš„ç¼©æ”¾é…ç½®çš„å­—å…¸ã€‚ç›®å‰æ”¯æŒä¸¤ç§ç¼©æ”¾ç­–ç•¥ï¼šçº¿æ€§å’ŒåŠ¨æ€ã€‚å®ƒä»¬çš„ç¼©æ”¾å› å­å¿…é¡»æ˜¯å¤§äº1çš„æµ®ç‚¹æ•°ã€‚é¢„æœŸæ ¼å¼ä¸º`{"type": ç­–ç•¥åç§°, "factor": ç¼©æ”¾å› å­}`ã€‚ä½¿ç”¨æ­¤æ ‡å¿—æ—¶ï¼Œä¸è¦å°†`max_position_embeddings`æ›´æ–°ä¸ºé¢„æœŸçš„æ–°æœ€å¤§å€¼ã€‚æœ‰å…³è¿™äº›ç¼©æ”¾ç­–ç•¥è¡Œä¸ºçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹ä¸»é¢˜ï¼š[https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿç ´åæ€§APIæ›´æ”¹ã€‚

+   `attention_bias`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” åœ¨è‡ªæ³¨æ„åŠ›æœŸé—´çš„æŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºæŠ•å½±å±‚ä¸­æ˜¯å¦ä½¿ç”¨åç½®ã€‚

+   `attention_dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªLLaMAæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLLaMA-7Bçš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

```py
>>> from transformers import LlamaModel, LlamaConfig

>>> # Initializing a LLaMA llama-7b style configuration
>>> configuration = LlamaConfig()

>>> # Initializing a model from the llama-7b style configuration
>>> model = LlamaModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LlamaTokenizer

### `class transformers.LlamaTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L66)

```py
( vocab_file unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' pad_token = None sp_model_kwargs: Optional = None add_bos_token = True add_eos_token = False clean_up_tokenization_spaces = False use_default_system_prompt = False spaces_between_special_tokens = False legacy = None **kwargs )
```

å‚æ•°

+   `vocab_file`ï¼ˆ`str`ï¼‰â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `unk_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `bos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<s>"`ï¼‰â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚

+   `eos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"</s>"`ï¼‰â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `pad_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿ä»¤ç‰Œæ•°ç»„å¤§å°ç›¸åŒä»¥è¿›è¡Œæ‰¹å¤„ç†çš„ç‰¹æ®Šä»¤ç‰Œã€‚ç„¶åå°†è¢«æ³¨æ„åŠ›æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚

+   `sp_model_kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ`Optional`ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™`SentencePieceProcessor.__init__()`æ–¹æ³•ã€‚[SentencePieceçš„PythonåŒ…è£…å™¨](https://github.com/google/sentencepiece/tree/master/python)å¯ç”¨äºè®¾ç½®ï¼š

    +   `enable_sampling`ï¼šå¯ç”¨å­è¯æ­£åˆ™åŒ–ã€‚

    +   `nbest_size`ï¼šunigramçš„æŠ½æ ·å‚æ•°ã€‚å¯¹äºBPE-Dropoutæ— æ•ˆã€‚

        +   `nbest_size = {0,1}`ï¼šä¸æ‰§è¡ŒæŠ½æ ·ã€‚

        +   `nbest_size > 1`ï¼šä»nbest_sizeç»“æœä¸­æŠ½æ ·ã€‚

        +   `nbest_size < 0`ï¼šå‡è®¾nbest_sizeä¸ºæ— é™å¤§ï¼Œå¹¶ä½¿ç”¨å‰å‘è¿‡æ»¤å’Œåå‘æŠ½æ ·ç®—æ³•ä»æ‰€æœ‰å‡è®¾ï¼ˆæ ¼å­ï¼‰ä¸­æŠ½æ ·ã€‚

    +   `alpha`ï¼šunigramæŠ½æ ·çš„å¹³æ»‘å‚æ•°ï¼Œä»¥åŠBPE-dropoutåˆå¹¶æ“ä½œçš„dropoutæ¦‚ç‡ã€‚

+   `add_bos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—å¼€å¤´æ·»åŠ `bos_token`ã€‚

+   `add_eos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ `eos_token`ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç åæ¸…é™¤ç©ºæ ¼ï¼Œæ¸…é™¤åŒ…æ‹¬åˆ é™¤é¢å¤–ç©ºæ ¼ç­‰æ½œåœ¨å·¥ä»¶ã€‚

+   `use_default_system_prompt`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åº”ä½¿ç”¨Llamaçš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚

+   `spaces_between_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨ç‰¹æ®Šæ ‡è®°ä¹‹é—´æ·»åŠ ç©ºæ ¼ã€‚

+   `legacy`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦åº”ä½¿ç”¨åˆ†è¯å™¨çš„`legacy`è¡Œä¸ºã€‚åœ¨åˆå¹¶ï¼ƒ24622å’Œï¼ƒ25224ä¹‹å‰çš„é—ç•™ç‰ˆæœ¬ä¸­ï¼ŒåŒ…æ‹¬ä¿®å¤æ­£ç¡®å¤„ç†å‡ºç°åœ¨ç‰¹æ®Šæ ‡è®°ä¹‹åçš„æ ‡è®°çš„é—®é¢˜ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

    +   `legacy=True`ï¼š

æ„å»ºä¸€ä¸ªLlamaåˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚é»˜è®¤å¡«å……æ ‡è®°æœªè®¾ç½®ï¼Œå› ä¸ºåŸå§‹æ¨¡å‹ä¸­æ²¡æœ‰å¡«å……æ ‡è®°ã€‚

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L333)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L344)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” IDåˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ªIDåˆ—è¡¨ã€‚

+   `already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»ä½¿ç”¨æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°æ ¼å¼åŒ–ã€‚

è¿”å›

`List[int]`

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º[0, 1]ï¼š1è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—IDã€‚åœ¨ä½¿ç”¨åˆ†è¯å™¨çš„`prepare_for_model`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L381)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” IDåˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ªIDåˆ—è¡¨ã€‚

è¿”å›

`List[int]`

æ ¹æ®ç»™å®šåºåˆ—çš„[æ ‡è®°ç±»å‹ID](../glossary#token-type-ids)åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚ä¸€ä¸ªALBERT

åºåˆ—å¯¹æ©ç çš„æ ¼å¼å¦‚ä¸‹ï¼š

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

å¦‚æœtoken_ids_1ä¸ºNoneï¼Œåˆ™ä»…è¿”å›æ©ç çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ0sï¼‰ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L306)

```py
( save_directory filename_prefix: Optional = None ) â†’ export const metadata = 'undefined';Tuple(str)
```

å‚æ•°

+   `save_directory`ï¼ˆ`str`ï¼‰â€” ä¿å­˜è¯æ±‡è¡¨çš„ç›®å½•ã€‚

è¿”å›

`Tuple(str)`

ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ã€‚

å°†è¯æ±‡è¡¨å’Œç‰¹æ®Šæ ‡è®°æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ä¸­ã€‚

## LlamaTokenizerFast

### `class transformers.LlamaTokenizerFast`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L57)

```py
( vocab_file = None tokenizer_file = None clean_up_tokenization_spaces = False unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' add_bos_token = True add_eos_token = False use_default_system_prompt = False **kwargs )
```

å‚æ•°

+   `vocab_file`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” [SentencePiece](https://github.com/google/sentencepiece)æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰.modelæ‰©å±•åï¼‰ï¼Œå…¶ä¸­åŒ…å«å®ä¾‹åŒ–åˆ†è¯å™¨æ‰€éœ€çš„è¯æ±‡è¡¨ã€‚

+   `tokenizer_file`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” [tokenizers](https://github.com/huggingface/tokenizers)æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰.jsonæ‰©å±•åï¼‰ï¼Œå…¶ä¸­åŒ…å«åŠ è½½åˆ†è¯å™¨æ‰€éœ€çš„æ‰€æœ‰å†…å®¹ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç åæ¸…é™¤ç©ºæ ¼ï¼Œæ¸…é™¤åŒ…æ‹¬åˆ é™¤é¢å¤–ç©ºæ ¼ç­‰æ½œåœ¨å·¥ä»¶ã€‚

+   `unk_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `bos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<s>"`ï¼‰â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ä»¥ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚

+   `eos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"</s>"`ï¼‰â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `add_bos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—å¼€å¤´æ·»åŠ `bos_token`ã€‚

+   `add_eos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ `eos_token`ã€‚

+   `use_default_system_prompt`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨Llamaçš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚

æ„å»ºä¸€ä¸ªLlamaåˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚

è¿™ä¸»è¦ä½¿ç”¨ByteFallbackå’Œæ— è§„èŒƒåŒ–ã€‚

```py
>>> from transformers import LlamaTokenizerFast

>>> tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
>>> tokenizer.encode("Hello this is a test")
[1, 15043, 445, 338, 263, 1243]
```

å¦‚æœè¦æ›´æ”¹`bos_token`æˆ–`eos_token`ï¼Œè¯·ç¡®ä¿åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶æŒ‡å®šå®ƒä»¬ï¼Œæˆ–è°ƒç”¨`tokenizer.update_post_processor()`ä»¥ç¡®ä¿åå¤„ç†æ­£ç¡®å®Œæˆï¼ˆå¦åˆ™ç¼–ç åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°å’Œæœ€åä¸€ä¸ªæ ‡è®°çš„å€¼å°†ä¸æ­£ç¡®ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[åå¤„ç†å™¨]ï¼ˆ[https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors)ï¼‰æ–‡æ¡£ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L272)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';A list of integers in the range [0, 1]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªåºåˆ—çš„idåˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªåºåˆ—çš„idåˆ—è¡¨ã€‚

+   `already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

ä¸€ä¸ªèŒƒå›´åœ¨[0, 1]å†…çš„æ•´æ•°åˆ—è¡¨

1è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—idã€‚åœ¨ä½¿ç”¨tokenizerçš„`prepare_for_model`æˆ–`encode_plus`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

è¿”å›

`List[int]`

æ ‡è®°ç±»å‹idã€‚

åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹IDã€‚[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

å¦‚æœæ¨¡å‹æœ‰ç‰¹æ®Šçš„æ„å»ºæ–¹å¼ï¼Œåº”è¯¥åœ¨å­ç±»ä¸­é‡å†™æ­¤æ–¹æ³•ã€‚

#### `update_post_processor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L146)

```py
( )
```

æ›´æ–°åº•å±‚çš„åå¤„ç†å™¨ï¼Œä½¿ç”¨å½“å‰çš„`bos_token`å’Œ`eos_token`ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L190)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LlamaModel

### `class transformers.LlamaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L939)

```py
( config: LlamaConfig )
```

å‚æ•°

+   `config`ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚é…ç½® â€” LlamaConfig

è£¸çš„LLaMAæ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

Transformerè§£ç å™¨ç”±*config.num_hidden_layers*å±‚ç»„æˆã€‚æ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ª`LlamaDecoderLayer`ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L974)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºæœªè¢«æ©ç›–çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºè¢«æ©ç›–çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¦è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    è¯¥æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## LlamaForCausalLM

### `class transformers.LlamaForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1106)

```py
( config )
```

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1136)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰ â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   å¯¹äºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ï¼Œä¸º1ï¼Œ

    +   0è¡¨ç¤ºè¢«â€œæ©ç›–â€çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç›–â€ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚

+   `position_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨è§£ç çš„å…ˆå‰é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

    å‚æ•° â€” æ ‡ç­¾ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰ï¼šç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚

è¿”å›

[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼‰

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[LlamaForCausalLM](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForCausalLM)çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, LlamaForCausalLM

>>> model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
>>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

>>> prompt = "Hey, are you conscious? Can you talk to me?"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(inputs.input_ids, max_length=30)
>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
```

## LlamaForSequenceClassification

### `class transformers.LlamaForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1295)

```py
( config )
```

å‚æ•°

+   `config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰é¡¶éƒ¨åºåˆ—åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰çš„LLaMaæ¨¡å‹å˜æ¢å™¨ã€‚

[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)ä½¿ç”¨æœ€åä¸€ä¸ªæ ‡è®°æ¥è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æœæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-2ï¼‰ä¸€æ ·ã€‚

ç”±äºå®ƒå¯¹æœ€åä¸€ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤éœ€è¦çŸ¥é“æœ€åä¸€ä¸ªæ ‡è®°çš„ä½ç½®ã€‚å¦‚æœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œåˆ™åœ¨æ¯è¡Œä¸­æ‰¾åˆ°ä¸æ˜¯å¡«å……æ ‡è®°çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚å¦‚æœæœªå®šä¹‰`pad_token_id`ï¼Œåˆ™åœ¨æ‰¹æ¬¡çš„æ¯è¡Œä¸­ç®€å•åœ°å–æœ€åä¸€ä¸ªå€¼ã€‚ç”±äºåœ¨ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶æ— æ³•çŒœæµ‹å¡«å……æ ‡è®°ï¼Œå› æ­¤æ‰§è¡Œç›¸åŒæ“ä½œï¼ˆåœ¨æ‰¹æ¬¡çš„æ¯è¡Œä¸­å–æœ€åä¸€ä¸ªå€¼ï¼‰ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1326)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›äº†å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    [æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰- é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå³é‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™è¯¥æ¨¡å‹çš„è¾“å…¥ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` ä¸­ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
