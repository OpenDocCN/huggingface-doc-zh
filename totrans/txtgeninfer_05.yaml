- en: Supported Models and Hardware
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持的模型和硬件
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/supported_models](https://huggingface.co/docs/text-generation-inference/supported_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-generation-inference/supported_models](https://huggingface.co/docs/text-generation-inference/supported_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference enables serving optimized models on specific hardware
    for the highest performance. The following sections list which models are hardware
    are supported.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成推理使得在特定硬件上为最高性能提供优化模型成为可能。以下部分列出了支持哪些模型和硬件。
- en: Supported Models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的模型
- en: The following models are optimized and can be served with TGI, which uses custom
    CUDA kernels for better inference. You can add the flag `--disable-custom-kernels`
    at the end of the `docker run` command if you wish to disable them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型已经优化，并可以通过TGI提供服务，TGI使用自定义的CUDA内核以获得更好的推理效果。如果您希望禁用它们，可以在`docker run`命令的末尾添加标志`--disable-custom-kernels`。
- en: '[BLOOM](https://huggingface.co/bigscience/bloom)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BLOOM](https://huggingface.co/bigscience/bloom)'
- en: '[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)'
- en: '[Galactica](https://huggingface.co/facebook/galactica-120b)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Galactica](https://huggingface.co/facebook/galactica-120b)'
- en: '[GPT-Neox](https://huggingface.co/EleutherAI/gpt-neox-20b)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-Neox](https://huggingface.co/EleutherAI/gpt-neox-20b)'
- en: '[Llama](https://github.com/facebookresearch/llama)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama](https://github.com/facebookresearch/llama)'
- en: '[OPT](https://huggingface.co/facebook/opt-66b)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OPT](https://huggingface.co/facebook/opt-66b)'
- en: '[SantaCoder](https://huggingface.co/bigcode/santacoder)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SantaCoder](https://huggingface.co/bigcode/santacoder)'
- en: '[Starcoder](https://huggingface.co/bigcode/starcoder)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Starcoder](https://huggingface.co/bigcode/starcoder)'
- en: '[Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)'
- en: '[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)'
- en: '[MPT](https://huggingface.co/mosaicml/mpt-30b)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MPT](https://huggingface.co/mosaicml/mpt-30b)'
- en: '[Llama V2](https://huggingface.co/meta-llama)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama V2](https://huggingface.co/meta-llama)'
- en: '[Code Llama](https://huggingface.co/codellama)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Code Llama](https://huggingface.co/codellama)'
- en: '[Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)'
- en: '[Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)'
- en: '[Phi](https://huggingface.co/microsoft/phi-2)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Phi](https://huggingface.co/microsoft/phi-2)'
- en: 'If the above list lacks the model you would like to serve, depending on the
    model’s pipeline type, you can try to initialize and serve the model anyways to
    see how well it performs, but performance isn’t guaranteed for non-optimized models:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述列表中缺少您想要提供的模型，可以根据模型的管道类型尝试初始化和提供模型，以查看其性能如何，但对于非优化模型，性能不能保证。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you wish to serve a supported model that already exists on a local folder,
    just point to the local folder.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望提供一个已经存在于本地文件夹中的支持模型，只需指向本地文件夹。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Supported Hardware
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的硬件
- en: TGI optimized models are supported on NVIDIA [A100](https://www.nvidia.com/en-us/data-center/a100/),
    [A10G](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) and [T4](https://www.nvidia.com/en-us/data-center/tesla-t4/)
    GPUs with CUDA 12.2+. Note that you have to install [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    to use it. For other NVIDIA GPUs, continuous batching will still apply, but some
    operations like flash attention and paged attention will not be executed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TGI优化模型支持在CUDA 12.2+的NVIDIA [A100](https://www.nvidia.com/en-us/data-center/a100/)、[A10G](https://www.nvidia.com/en-us/data-center/products/a10-gpu/)和[T4](https://www.nvidia.com/en-us/data-center/tesla-t4/)
    GPU上运行。请注意，您必须安装[NVIDIA容器工具包](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)才能使用它。对于其他NVIDIA
    GPU，仍将应用连续批处理，但一些操作，如flash attention和分页注意力将不会执行。
- en: 'TGI also has support of ROCm-enabled AMD Instinct MI210 and MI250 GPUs, with
    paged attention, GPTQ quantization, flash attention v2 support. The following
    features are currently not supported in the ROCm version of TGI, and the supported
    may be extended in the future:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TGI还支持启用ROCm的AMD Instinct MI210和MI250 GPU，具有分页注意力、GPTQ量化、flash attention v2支持。以下功能目前在ROCm版本的TGI中不受支持，支持可能会在未来扩展：
- en: Loading [AWQ](https://huggingface.co/docs/transformers/quantization#awq) checkpoints.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载[AWQ](https://huggingface.co/docs/transformers/quantization#awq)检查点。
- en: Flash [layer norm kernel](https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flash [层归一化内核](https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm)
- en: Kernel for slinding window attention (Mistral)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑动窗口注意力的内核（Mistral）
- en: 'TGI is also supported on the following AI hardware accelerators:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TGI还支持以下人工智能硬件加速器：
- en: '*Habana first-gen Gaudi and Gaudi2:* check out this [repository](https://github.com/huggingface/tgi-gaudi)
    to serve models with TGI on Gaudi and Gaudi2 with [Optimum Habana](https://huggingface.co/docs/optimum/habana/index)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Habana第一代Gaudi和Gaudi2:* 查看这个[存储库](https://github.com/huggingface/tgi-gaudi)，以使用TGI在Gaudi和Gaudi2上为模型提供服务，使用[Optimum
    Habana](https://huggingface.co/docs/optimum/habana/index)'
- en: '*AWS Inferentia2:* check out this [guide](https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference)
    on how to serve models with TGI on Inferentia2.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS Inferentia2:* 查看这个[指南](https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference)，了解如何在Inferentia2上使用TGI为模型提供服务。'
