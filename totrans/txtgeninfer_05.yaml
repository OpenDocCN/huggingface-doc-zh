- en: Supported Models and Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/supported_models](https://huggingface.co/docs/text-generation-inference/supported_models)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference enables serving optimized models on specific hardware
    for the highest performance. The following sections list which models are hardware
    are supported.
  prefs: []
  type: TYPE_NORMAL
- en: Supported Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following models are optimized and can be served with TGI, which uses custom
    CUDA kernels for better inference. You can add the flag `--disable-custom-kernels`
    at the end of the `docker run` command if you wish to disable them.
  prefs: []
  type: TYPE_NORMAL
- en: '[BLOOM](https://huggingface.co/bigscience/bloom)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Galactica](https://huggingface.co/facebook/galactica-120b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-Neox](https://huggingface.co/EleutherAI/gpt-neox-20b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Llama](https://github.com/facebookresearch/llama)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OPT](https://huggingface.co/facebook/opt-66b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SantaCoder](https://huggingface.co/bigcode/santacoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Starcoder](https://huggingface.co/bigcode/starcoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MPT](https://huggingface.co/mosaicml/mpt-30b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Llama V2](https://huggingface.co/meta-llama)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Code Llama](https://huggingface.co/codellama)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Phi](https://huggingface.co/microsoft/phi-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the above list lacks the model you would like to serve, depending on the
    model’s pipeline type, you can try to initialize and serve the model anyways to
    see how well it performs, but performance isn’t guaranteed for non-optimized models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you wish to serve a supported model that already exists on a local folder,
    just point to the local folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Supported Hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TGI optimized models are supported on NVIDIA [A100](https://www.nvidia.com/en-us/data-center/a100/),
    [A10G](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) and [T4](https://www.nvidia.com/en-us/data-center/tesla-t4/)
    GPUs with CUDA 12.2+. Note that you have to install [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    to use it. For other NVIDIA GPUs, continuous batching will still apply, but some
    operations like flash attention and paged attention will not be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'TGI also has support of ROCm-enabled AMD Instinct MI210 and MI250 GPUs, with
    paged attention, GPTQ quantization, flash attention v2 support. The following
    features are currently not supported in the ROCm version of TGI, and the supported
    may be extended in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading [AWQ](https://huggingface.co/docs/transformers/quantization#awq) checkpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flash [layer norm kernel](https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel for slinding window attention (Mistral)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TGI is also supported on the following AI hardware accelerators:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Habana first-gen Gaudi and Gaudi2:* check out this [repository](https://github.com/huggingface/tgi-gaudi)
    to serve models with TGI on Gaudi and Gaudi2 with [Optimum Habana](https://huggingface.co/docs/optimum/habana/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS Inferentia2:* check out this [guide](https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference)
    on how to serve models with TGI on Inferentia2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
