["```py\n( image_size = 224 patch_size = 16 num_channels = 3 num_frames = 16 tubelet_size = 2 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 qkv_bias = True use_mean_pooling = True decoder_num_attention_heads = 6 decoder_hidden_size = 384 decoder_num_hidden_layers = 4 decoder_intermediate_size = 1536 norm_pix_loss = True **kwargs )\n```", "```py\n>>> from transformers import VideoMAEConfig, VideoMAEModel\n\n>>> # Initializing a VideoMAE videomae-base style configuration\n>>> configuration = VideoMAEConfig()\n\n>>> # Randomly initializing a model from the configuration\n>>> model = VideoMAEModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )\n```", "```py\n( videos: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config )\n```", "```py\n( pixel_values: FloatTensor bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import av\n>>> import numpy as np\n\n>>> from transformers import AutoImageProcessor, VideoMAEModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 16 frames\n>>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n>>> model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n\n>>> # prepare video for the model\n>>> inputs = image_processor(list(video), return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 1568, 768]\n```", "```py\n( config )\n```", "```py\n( pixel_values: FloatTensor bool_masked_pos: BoolTensor head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.videomae.modeling_videomae.VideoMAEForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, VideoMAEForPreTraining\n>>> import numpy as np\n>>> import torch\n\n>>> num_frames = 16\n>>> video = list(np.random.randint(0, 256, (num_frames, 3, 224, 224)))\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n>>> model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")\n\n>>> pixel_values = image_processor(video, return_tensors=\"pt\").pixel_values\n\n>>> num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n>>> seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n>>> bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n\n>>> outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n>>> loss = outputs.loss\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 16 frames\n>>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n>>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n\n>>> inputs = image_processor(list(video), return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n...     logits = outputs.logits\n\n>>> # model predicts one of the 400 Kinetics-400 classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\neating spaghetti\n```"]