- en: Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练器
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class provides an API for feature-complete training in PyTorch, and it supports
    distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/),
    [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html)
    for PyTorch. [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    goes hand-in-hand with the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    class, which offers a wide range of options to customize how a model is trained.
    Together, these two classes provide a complete training API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    类提供了一个用于在PyTorch中进行完整特征训练的API，并支持在多个GPU/TPU上进行分布式训练，支持[NVIDIA GPUs](https://nvidia.github.io/apex/)的混合精度，[AMD
    GPUs](https://rocm.docs.amd.com/en/latest/rocm.html)，以及PyTorch的[`torch.amp`](https://pytorch.org/docs/stable/amp.html)。[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    与[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类相辅相成，后者提供了广泛的选项来自定义模型的训练方式。这两个类一起提供了一个完整的训练API。'
- en: '[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    and [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)
    inherit from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and `TrainingArgument` classes and they’re adapted for training models for sequence-to-sequence
    tasks such as summarization or translation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    和 [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)
    继承自[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)和`TrainingArgument`类，它们适用于用于序列到序列任务（如摘要或翻译）的模型训练。'
- en: 'The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class is optimized for 🤗 Transformers models and can have surprising behaviors
    when used with other models. When using it with your own model, make sure:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    类针对🤗 Transformers模型进行了优化，当与其他模型一起使用时可能会有一些意外行为。在使用自己的模型时，请确保：'
- en: your model always return tuples or subclasses of [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型始终返回元组或[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)的子类
- en: your model can compute the loss if a `labels` argument is provided and that
    loss is returned as the first element of the tuple (if your model returns tuples)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供了`labels`参数并且该损失作为元组的第一个元素返回（如果您的模型返回元组），则您的模型可以计算损失
- en: your model can accept multiple label arguments (use `label_names` in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    to indicate their name to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    but none of them should be named `"label"`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型可以接受多个标签参数（在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中使用`label_names`指示它们的名称给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)），但它们中没有一个应该被命名为`"label"`
- en: Trainer
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练器
- en: '### `class transformers.Trainer`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Trainer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L236)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L236)'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or `torch.nn.Module`, *optional*) — The model to train, evaluate or use for predictions.
    If not provided, a `model_init` must be passed.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 `torch.nn.Module`, *可选*) — 用于训练、评估或用于预测的模型。如果未提供，则必须传递一个`model_init`。'
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is optimized to work with the [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    provided by the library. You can still use your own models defined as `torch.nn.Module`
    as long as they work the same way as the 🤗 Transformers models.'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    被优化为与库提供的[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)一起使用。只要您的模型与🤗
    Transformers模型的工作方式相同，您仍然可以使用自己定义的`torch.nn.Module`模型。'
- en: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    *optional*) — The arguments to tweak for training. Will default to a basic instance
    of [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    with the `output_dir` set to a directory named *tmp_trainer* in the current directory
    if not provided.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    *可选*) — 用于调整训练的参数。如果未提供，将默认为一个基本的[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)实例，其中`output_dir`设置为当前目录中名为*tmp_trainer*的目录。'
- en: '`data_collator` (`DataCollator`, *optional*) — The function to use to form
    a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default
    to [default_data_collator()](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.default_data_collator)
    if no `tokenizer` is provided, an instance of [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)
    otherwise.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator` (`DataCollator`, *可选*) — 用于从`train_dataset`或`eval_dataset`的元素列表中形成批次的函数。如果未提供`tokenizer`，将默认为[default_data_collator()](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.default_data_collator)，否则将默认为[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)的实例。'
- en: '`train_dataset` (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`,
    *optional*) — The dataset to use for training. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset`（`torch.utils.data.Dataset`或`torch.utils.data.IterableDataset`，*可选*）—
    用于训练的数据集。如果是[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。'
- en: Note that if it’s a `torch.utils.data.IterableDataset` with some randomization
    and you are training in a distributed fashion, your iterable dataset should either
    use a internal attribute `generator` that is a `torch.Generator` for the randomization
    that must be identical on all processes (and the Trainer will manually set the
    seed of this `generator` at each epoch) or have a `set_epoch()` method that internally
    sets the seed of the RNGs used.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，如果是带有一些随机化的`torch.utils.data.IterableDataset`，并且您正在以分布式方式进行训练，您的可迭代数据集应该使用一个内部属性`generator`，该属性是一个`torch.Generator`，用于在所有进程上保持相同的随机化（并且Trainer将在每个epoch手动设置此`generator`的种子），或者具有一个在内部设置用于随机数生成器的种子的`set_epoch()`方法。
- en: '`eval_dataset` (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]),
    *optional*) — The dataset to use for evaluation. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    If it is a dictionary, it will evaluate on each dataset prepending the dictionary
    key to the metric name.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset`（Union[`torch.utils.data.Dataset`，Dict[str，`torch.utils.data.Dataset`]），*可选*）—
    用于评估的数据集。如果是[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。如果是字典，则将在每个数据集上评估，并在度量名称之前添加字典键。'
- en: '`tokenizer` ([PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase),
    *optional*) — The tokenizer used to preprocess the data. If provided, will be
    used to automatically pad the inputs to the maximum length when batching inputs,
    and it will be saved along the model to make it easier to rerun an interrupted
    training or reuse the fine-tuned model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)，*可选*）—
    用于预处理数据的分词器。如果提供，将用于在批处理输入时自动填充输入到最大长度，并将保存在模型中，以便更容易重新运行中断的训练或重用微调的模型。'
- en: '`model_init` (`Callable[[], PreTrainedModel]`, *optional*) — A function that
    instantiates the model to be used. If provided, each call to [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    will start from a new instance of the model as given by this function.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_init`（`Callable[[], PreTrainedModel]`，*可选*）— 用于实例化要使用的模型的函数。如果提供，每次调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)都将从此函数给出的模型的新实例开始。'
- en: The function may have zero argument, or a single one containing the optuna/Ray
    Tune/SigOpt trial object, to be able to choose different architectures according
    to hyper parameters (such as layer count, sizes of inner layers, dropout probabilities
    etc).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数可能没有参数，或者包含一个参数，其中包含optuna/Ray Tune/SigOpt试验对象，以便根据超参数（例如层计数、内部层大小、丢失概率等）选择不同的架构。
- en: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *optional*) — The function
    that will be used to compute metrics at evaluation. Must take a [EvalPrediction](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.EvalPrediction)
    and return a dictionary string to metric values.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics`（`Callable[[EvalPrediction], Dict]`，*可选*）— 用于在评估时计算指标的函数。必须接受[EvalPrediction](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.EvalPrediction)并返回一个字符串字典以表示指标值。'
- en: '`callbacks` (List of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback),
    *optional*) — A list of callbacks to customize the training loop. Will add those
    to the list of default callbacks detailed in [here](callback).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks`（[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表，*可选*）—
    用于自定义训练循环的回调列表。将添加到[此处](callback)详细说明的默认回调列表中。'
- en: If you want to remove one of the default callbacks used, use the [Trainer.remove_callback()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.remove_callback)
    method.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要删除使用的默认回调之一，请使用[Trainer.remove_callback()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.remove_callback)方法。
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`,
    *optional*, defaults to `(None, None)`) — A tuple containing the optimizer and
    the scheduler to use. Will default to an instance of [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    on your model and a scheduler given by [get_linear_schedule_with_warmup()](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)
    controlled by `args`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizers`（`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`，*可选*，默认为`(None,
    None)`）— 包含要使用的优化器和调度器的元组。将默认为您的模型上的[AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)实例和由[get_linear_schedule_with_warmup()](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)控制的调度器，由`args`指定。'
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`,
    *optional*) — A function that preprocess the logits right before caching them
    at each evaluation step. Must take two tensors, the logits and the labels, and
    return the logits once processed as desired. The modifications made by this function
    will be reflected in the predictions received by `compute_metrics`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_logits_for_metrics`（`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`，*可选*）—
    用于在每个评估步骤之前预处理logits的函数。必须接受两个张量，logits和标签，并根据需要返回处理后的logits。此函数所做的修改将反映在`compute_metrics`接收到的预测中。'
- en: Note that the labels (second parameter) will be `None` if the dataset does not
    have them.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，如果数据集没有标签，标签（第二个参数）将为`None`。
- en: Trainer is a simple but feature-complete training and eval loop for PyTorch,
    optimized for 🤗 Transformers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer是一个简单但功能完备的PyTorch训练和评估循环，专为🤗 Transformers优化。
- en: 'Important attributes:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 重要属性：
- en: '`model` — Always points to the core model. If using a transformers model, it
    will be a [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    subclass.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` - 始终指向核心模型。如果使用transformers模型，它将是一个[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)子类。'
- en: '`model_wrapped` — Always points to the most external model in case one or more
    other modules wrap the original model. This is the model that should be used for
    the forward pass. For example, under `DeepSpeed`, the inner model is wrapped in
    `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner
    model hasn’t been wrapped, then `self.model_wrapped` is the same as `self.model`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_wrapped` - 始终指向最外部的模型，以防一个或多个其他模块包装原始模型。这是应该用于前向传递的模型。例如，在`DeepSpeed`下，内部模型被包装在`DeepSpeed`中，然后再次包装在`torch.nn.DistributedDataParallel`中。如果内部模型没有被包装，那么`self.model_wrapped`与`self.model`相同。'
- en: '`is_model_parallel` — Whether or not a model has been switched to a model parallel
    mode (different from data parallelism, this means some of the model layers are
    split on different GPUs).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_model_parallel` - 模型是否已切换到模型并行模式（与数据并行不同，这意味着一些模型层在不同的GPU上分割）。 '
- en: '`place_model_on_device` — Whether or not to automatically place the model on
    the device - it will be set to `False` if model parallel or deepspeed is used,
    or if the default `TrainingArguments.place_model_on_device` is overridden to return
    `False` .'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`place_model_on_device` - 是否自动将模型放置在设备上 - 如果使用模型并行或deepspeed，或者如果默认的`TrainingArguments.place_model_on_device`被覆盖为返回`False`，则将其设置为`False`。'
- en: '`is_in_train` — Whether or not a model is currently running `train` (e.g. when
    `evaluate` is called while in `train`)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_in_train` - 模型当前是否在运行`train`（例如，在`train`中调用`evaluate`时）'
- en: '#### `add_callback`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_callback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L654)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L654)'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`callback` (`type` or [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    — A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    class or an instance of a [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    In the first case, will instantiate a member of that class.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`（`type`或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)）-
    一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)类或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)的实例。在第一种情况下，将实例化该类的成员。'
- en: Add a callback to the current list of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 向当前的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表中添加一个回调。
- en: '#### `autocast_smart_context_manager`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `autocast_smart_context_manager`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2734)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2734)'
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A helper wrapper that creates an appropriate context manager for `autocast`
    while feeding it the desired arguments, depending on the situation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个帮助器包装器，为`autocast`创建适当的上下文管理器，并根据情况提供所需的参数。
- en: '#### `compute_loss`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_loss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2785)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2785)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How the loss is computed by Trainer. By default, all models return the loss
    in the first element.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer如何计算损失。默认情况下，所有模型都在第一个元素中返回损失。
- en: Subclass and override for custom behavior.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 子类和覆盖以进行自定义行为。
- en: '#### `compute_loss_context_manager`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_loss_context_manager`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2728)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2728)'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A helper wrapper to group together context managers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个帮助器包装器，用于将上下文管理器组合在一起。
- en: '#### `create_model_card`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_model_card`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3564)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3564)'
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`language` (`str`, *optional*) — The language of the model (if applicable)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`（`str`，*可选*）- 模型的语言（如果适用）'
- en: '`license` (`str`, *optional*) — The license of the model. Will default to the
    license of the pretrained model used, if the original model given to the `Trainer`
    comes from a repo on the Hub.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`license`（`str`，*可选*）- 模型的许可证。如果原始模型给定给`Trainer`来自Hub上的repo，则将默认为使用的预训练模型的许可证。'
- en: '`tags` (`str` or `List[str]`, *optional*) — Some tags to be included in the
    metadata of the model card.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`（`str`或`List[str]`，*可选*）- 要包含在模型卡元数据中的一些标签。'
- en: '`model_name` (`str`, *optional*) — The name of the model.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`（`str`，*可选*）- 模型的名称。'
- en: '`finetuned_from` (`str`, *optional*) — The name of the model used to fine-tune
    this one (if applicable). Will default to the name of the repo of the original
    model given to the `Trainer` (if it comes from the Hub).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`finetuned_from`（`str`，*可选*）- 用于微调此模型的模型的名称（如果适用）。如果原始模型给定给`Trainer`来自Hub，则将默认为原始模型的repo的名称。'
- en: '`tasks` (`str` or `List[str]`, *optional*) — One or several task identifiers,
    to be included in the metadata of the model card.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tasks`（`str`或`List[str]`，*可选*）- 一个或多个任务标识符，将包含在模型卡的元数据中。'
- en: '`dataset_tags` (`str` or `List[str]`, *optional*) — One or several dataset
    tags, to be included in the metadata of the model card.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_tags`（`str`或`List[str]`，*可选*）- 一个或多个数据集标签，将包含在模型卡的元数据中。'
- en: '`dataset` (`str` or `List[str]`, *optional*) — One or several dataset identifiers,
    to be included in the metadata of the model card.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset`（`str`或`List[str]`，*可选*）- 一个或多个数据集标识符，将包含在模型卡的元数据中。'
- en: '`dataset_args` (`str` or `List[str]`, *optional*) — One or several dataset
    arguments, to be included in the metadata of the model card.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_args` (`str` 或 `List[str]`, *可选*) — 一个或多个数据集参数，将包含在模型卡的元数据中。'
- en: Creates a draft of a model card using the information available to the `Trainer`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Trainer`可用的信息创建一个模型卡的草稿。
- en: '#### `create_optimizer`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_optimizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L929)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L929)'
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setup the optimizer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 设置优化器。
- en: We provide a reasonable default that works well. If you want to use something
    else, you can pass a tuple in the Trainer’s init through `optimizers`, or subclass
    and override this method in a subclass.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个合理的默认值，效果很好。如果您想使用其他内容，可以通过`optimizers`在Trainer的init中传递一个元组，或者在子类中重写此方法。
- en: '#### `create_optimizer_and_scheduler`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 创建优化器和调度器
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L902)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L902)'
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Setup the optimizer and the learning rate scheduler.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 设置优化器和学习率调度器。
- en: We provide a reasonable default that works well. If you want to use something
    else, you can pass a tuple in the Trainer’s init through `optimizers`, or subclass
    and override this method (or `create_optimizer` and/or `create_scheduler`) in
    a subclass.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个合理的默认值，效果很好。如果您想使用其他内容，可以通过`optimizers`在Trainer的init中传递一个元组，或者在子类中重写此方法（或`create_optimizer`和/或`create_scheduler`）。
- en: '#### `create_scheduler`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_scheduler`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1109)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1109)'
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_training_steps` (int) — The number of training steps to do.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps` (int) — 要执行的训练步骤数。'
- en: Setup the scheduler. The optimizer of the trainer must have been set up either
    before this method is called or passed as an argument.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 设置调度器。训练器的优化器必须在调用此方法之前设置好，或者作为参数传递。
- en: '#### `evaluate`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3031)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3031)'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`eval_dataset` (Union[`Dataset`, Dict[str, `Dataset`]), *optional*) — Pass
    a dataset if you wish to override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    If it is a dictionary, it will evaluate on each dataset, prepending the dictionary
    key to the metric name. Datasets must implement the `__len__` method.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (Union[`Dataset`, Dict[str, `Dataset`]), *可选*) — 如果要覆盖`self.eval_dataset`，请传递一个数据集。如果是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则`model.forward()`方法不接受的列将自动删除。如果是一个字典，它将对每个数据集进行评估，并在度量名称前加上字典键。数据集必须实现`__len__`方法。'
- en: 'If you pass a dictionary with names of datasets as keys and datasets as values,
    evaluate will run separate evaluations on each dataset. This can be useful to
    monitor how training affects other datasets or simply to get a more fine-grained
    evaluation. When used with `load_best_model_at_end`, make sure `metric_for_best_model`
    references exactly one of the datasets. If you, for example, pass in `{"data1":
    data1, "data2": data2}` for two datasets `data1` and `data2`, you could specify
    `metric_for_best_model="eval_data1_loss"` for using the loss on `data1` and `metric_for_best_model="eval_data1_loss"`
    for the loss on `data2`.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如果您传递一个以数据集名称为键、数据集为值的字典，评估将在每个数据集上单独运行。这对于监视训练如何影响其他数据集或仅仅获得更精细的评估很有用。当与`load_best_model_at_end`一起使用时，请确保`metric_for_best_model`引用确切地一个数据集。例如，如果为两个数据集`data1`和`data2`传递`{"data1":
    data1, "data2": data2}`，则可以指定`metric_for_best_model="eval_data1_loss"`来使用`data1`上的损失，以及`metric_for_best_model="eval_data1_loss"`来使用`data2`上的损失。'
- en: '`ignore_keys` (`List[str]`, *optional*) — A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys` (`List[str]`, *可选*) — 在模型输出中应该被忽略的键的列表（如果它是一个字典）。'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"eval"`) — An optional
    prefix to be used as the metrics key prefix. For example the metrics “bleu” will
    be named “eval_bleu” if the prefix is “eval” (default)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix` (`str`, *可选*, 默认为`"eval"`) — 用作指标键前缀的可选前缀。例如，如果前缀是“eval”（默认），则指标“bleu”将被命名为“eval_bleu”。'
- en: Run evaluation and returns metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运行评估并返回指标。
- en: The calling script will be responsible for providing a method to compute metrics,
    as they are task-dependent (pass it to the init `compute_metrics` argument).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 调用脚本将负责提供计算指标的方法，因为它们是任务相关的（将其传递给`compute_metrics`参数进行初始化）。
- en: You can also subclass and override this method to inject custom behavior.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以子类化并重写此方法以注入自定义行为。
- en: '#### `evaluation_loop`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluation_loop`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3191)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3191)'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 预测/评估循环，由`Trainer.evaluate()`和`Trainer.predict()`共享。
- en: Works both with or without labels.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用带有或不带有标签的工作。
- en: '#### `floating_point_ops`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `floating_point_ops`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3529)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3529)'
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — The inputs and targets of
    the model.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — 模型的输入和目标。'
- en: Returns
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: The number of floating-point operations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点运算的数量。
- en: For models that inherit from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    uses that method to compute the number of floating point operations for every
    backward + forward pass. If using another model, either implement such a method
    in the model or subclass and override this method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，使用该方法计算每次反向+前向传递的浮点运算次数。如果使用另一个模型，要么在模型中实现这样的方法，要么子类化并重写此方法。
- en: '#### `get_decay_parameter_names`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_decay_parameter_names`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L918)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L918)'
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Get all parameter names that weight decay will be applied to
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 获取将应用权重衰减的所有参数名称
- en: Note that some models implement their own layernorm instead of calling nn.LayerNorm,
    weight decay could still apply to those modules since this function only filter
    out instance of nn.LayerNorm
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些模型实现了自己的layernorm而不是调用nn.LayerNorm，因此这个函数只过滤出nn.LayerNorm的实例
- en: '#### `get_eval_dataloader`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_eval_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L834)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L834)'
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`eval_dataset` (`torch.utils.data.Dataset`, *optional*) — If provided, will
    override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (`torch.utils.data.Dataset`, *可选*) — 如果提供，将覆盖`self.eval_dataset`。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，那些`model.forward()`方法不接受的列将被自动移除。它必须实现`__len__`。'
- en: Returns the evaluation `~torch.utils.data.DataLoader`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 返回评估`~torch.utils.data.DataLoader`。
- en: Subclass and override this method if you want to inject some custom behavior.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要注入一些自定义行为，请子类化并重写此方法。
- en: '#### `get_optimizer_cls_and_kwargs`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_optimizer_cls_and_kwargs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L977)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L977)'
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`transformers.training_args.TrainingArguments`) — The training arguments
    for the training session.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`transformers.training_args.TrainingArguments`) — 训练会话的训练参数。'
- en: Returns the optimizer class and optimizer parameters based on the training arguments.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根据训练参数返回优化器类和优化器参数。
- en: '#### `get_test_dataloader`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_test_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L869)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L869)'
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`test_dataset` (`torch.utils.data.Dataset`, *optional*) — The test dataset
    to use. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dataset` (`torch.utils.data.Dataset`, *可选*) — 要使用的测试数据集。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，那些`model.forward()`方法不接受的列将被自动移除。它必须实现`__len__`。'
- en: Returns the test `~torch.utils.data.DataLoader`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 返回测试`~torch.utils.data.DataLoader`。
- en: Subclass and override this method if you want to inject some custom behavior.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要注入一些自定义行为，请子类化并重写此方法。
- en: '#### `get_train_dataloader`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_train_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L778)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L778)'
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Returns the training `~torch.utils.data.DataLoader`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 返回训练`~torch.utils.data.DataLoader`。
- en: Will use no sampler if `train_dataset` does not implement `__len__`, a random
    sampler (adapted to distributed training if necessary) otherwise.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`train_dataset`不实现`__len__`，则不使用采样器，否则使用随机采样器（必要时适应分布式训练）。
- en: Subclass and override this method if you want to inject some custom behavior.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要注入一些自定义行为，请子类化并重写此方法。
- en: '#### `hyperparameter_search`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `hyperparameter_search`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2596)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2596)'
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hp_space` (`Callable[["optuna.Trial"], Dict[str, float]]`, *optional*) — A
    function that defines the hyperparameter search space. Will default to `default_hp_space_optuna()`
    or `default_hp_space_ray()` or `default_hp_space_sigopt()` depending on your backend.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp_space` (`Callable[["optuna.Trial"], Dict[str, float]]`, *可选*) — 定义超参数搜索空间的函数。将根据您的后端默认为`default_hp_space_optuna()`或`default_hp_space_ray()`或`default_hp_space_sigopt()`。'
- en: '`compute_objective` (`Callable[[Dict[str, float]], float]`, *optional*) — A
    function computing the objective to minimize or maximize from the metrics returned
    by the `evaluate` method. Will default to `default_compute_objective()`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_objective` (`Callable[[Dict[str, float]], float]`, *可选*) — 一个计算要最小化或最大化的目标的函数，从`evaluate`方法返回的指标中计算。将默认为`default_compute_objective()`。'
- en: '`n_trials` (`int`, *optional*, defaults to 100) — The number of trial runs
    to test.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_trials` (`int`, *可选*, 默认为100) — 测试运行的次数。'
- en: '`direction` (`str` or `List[str]`, *optional*, defaults to `"minimize"`) —
    If it’s single objective optimization, direction is `str`, can be `"minimize"`
    or `"maximize"`, you should pick `"minimize"` when optimizing the validation loss,
    `"maximize"` when optimizing one or several metrics. If it’s multi objectives
    optimization, direction is `List[str]`, can be List of `"minimize"` and `"maximize"`,
    you should pick `"minimize"` when optimizing the validation loss, `"maximize"`
    when optimizing one or several metrics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`direction` (`str` 或 `List[str]`, *可选*, 默认为`"minimize"`) — 如果是单目标优化，方向是`str`，可以是`"minimize"`或`"maximize"`，当优化验证损失时应选择`"minimize"`，当优化一个或多个指标时应选择`"maximize"`。如果是多目标优化，方向是`List[str]`，可以是`"minimize"`和`"maximize"`的列表，当优化验证损失时应选择`"minimize"`，当优化一个或多个指标时应选择`"maximize"`。'
- en: '`backend` (`str` or `~training_utils.HPSearchBackend`, *optional*) — The backend
    to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt,
    depending on which one is installed. If all are installed, will default to optuna.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`（`str`或`~training_utils.HPSearchBackend`，*可选*）—用于超参数搜索的后端。将默认为optuna、Ray
    Tune或SigOpt，取决于安装了哪个。如果所有都安装了，将默认为optuna。'
- en: '`hp_name` (`Callable[["optuna.Trial"], str]]`, *optional*) — A function that
    defines the trial/run name. Will default to None.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp_name`（`Callable[["optuna.Trial"], str]`，*可选*）—定义试验/运行名称的函数。默认为None。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to `optuna.create_study` or `ray.tune.run`. For more information see:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）—传递给`optuna.create_study`或`ray.tune.run`的其他关键字参数。更多信息请参见：'
- en: the documentation of [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[optuna.create_study的文档](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)'
- en: the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tune.run的文档](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)'
- en: the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[sigopt的文档](https://app.sigopt.com/docs/endpoints/experiments/create)'
- en: Returns
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[`trainer_utils.BestRun`或`List[trainer_utils.BestRun]`]'
- en: All the information about the best run or best runs for multi-objective optimization.
    Experiment summary can be found in `run_summary` attribute for Ray backend.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有关多目标优化的最佳运行或最佳运行的所有信息。实验摘要可以在Ray后端的`run_summary`属性中找到。
- en: Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The
    optimized quantity is determined by `compute_objective`, which defaults to a function
    returning the evaluation loss when no metric is provided, the sum of all metrics
    otherwise.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`optuna`、`Ray Tune`或`SigOpt`启动超参数搜索。优化的数量由`compute_objective`确定，默认情况下，当没有提供指标时返回评估损失的函数，否则返回所有指标的总和。
- en: 'To use this method, you need to have provided a `model_init` when initializing
    your [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):
    we need to reinitialize the model at each new run. This is incompatible with the
    `optimizers` argument, so you need to subclass [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and override the method [create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)
    for custom optimizer/scheduler.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种方法，您需要在初始化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)时提供一个`model_init`：我们需要在每次新运行时重新初始化模型。这与`optimizers`参数不兼容，因此您需要子类化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)并重写方法[create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)以获得自定义优化器/调度器。
- en: '#### `init_hf_repo`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `init_hf_repo`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3547)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3547)'
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Initializes a git repo in `self.args.hub_model_id`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在`self.args.hub_model_id`中初始化git存储库。
- en: '#### `is_local_process_zero`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `is_local_process_zero`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2822)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2822)'
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Whether or not this process is the local (e.g., on one machine if training in
    a distributed fashion on several machines) main process.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此进程是否为本地（例如，在多台机器上以分布式方式进行训练时，如果是主要进程，则为一台机器上的进程）。
- en: '#### `is_world_process_zero`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `is_world_process_zero`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2829)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2829)'
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Whether or not this process is the global main process (when training in a distributed
    fashion on several machines, this is only going to be `True` for one process).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此进程是否为全局主进程（在多台机器上以分布式方式进行训练时，只有一个进程会是`True`）。
- en: '#### `log`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `log`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2675)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2675)'
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`logs` (`Dict[str, float]`) — The values to log.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs`（`Dict[str, float]`）—要记录的值。'
- en: Log `logs` on the various objects watching training.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 记录`logs`在观察训练的各种对象。
- en: Subclass and override this method to inject custom behavior.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 子类化并重写此方法以注入自定义行为。
- en: '#### `log_metrics`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `log_metrics`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L905)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L905)'
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`split` (`str`) — Mode/split name: one of `train`, `eval`, `test`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split`（`str`）—模式/分割名称：`train`、`eval`、`test`之一'
- en: '`metrics` (`Dict[str, float]`) — The metrics returned from train/evaluate/predictmetrics:
    metrics dict'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics`（`Dict[str, float]`）—来自train/evaluate/predictmetrics的指标：指标字典'
- en: Log metrics in a specially formatted way
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以特殊格式记录指标
- en: Under distributed environment this is done only for a process with rank 0.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境下，这仅针对排名为0的进程执行。
- en: 'Notes on memory reports:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内存报告的注意事项：
- en: In order to get memory usage report you need to install `psutil`. You can do
    that with `pip install psutil`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得内存使用报告，您需要安装`psutil`。您可以使用`pip install psutil`来安装。
- en: 'Now when this method is run, you will see a report that will include: :'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当运行此方法时，您将看到一个包含的报告：：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Understanding the reports:**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解报告：**'
- en: the first segment, e.g., `train__`, tells you which stage the metrics are for.
    Reports starting with `init_` will be added to the first stage that gets run.
    So that if only evaluation is run, the memory usage for the `__init__` will be
    reported along with the `eval_` metrics.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，第一部分，例如`train__`，告诉您指标所属的阶段。以`init_`开头的报告将添加到运行的第一个阶段。因此，如果只运行评估，则将报告`__init__`的内存使用情况以及`eval_`指标。
- en: the third segment, is either `cpu` or `gpu`, tells you whether it’s the general
    RAM or the gpu0 memory metric.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三部分，是`cpu`或`gpu`，告诉您它是通用RAM还是gpu0内存指标。
- en: '`*_alloc_delta` - is the difference in the used/allocated memory counter between
    the end and the start of the stage - it can be negative if a function released
    more memory than it allocated.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*_alloc_delta` - 是阶段结束和开始时使用/分配内存计数器之间的差异 - 如果函数释放的内存多于分配的内存，则可能为负数。'
- en: '`*_peaked_delta` - is any extra memory that was consumed and then freed - relative
    to the current allocated memory counter - it is never negative. When you look
    at the metrics of any stage you add up `alloc_delta` + `peaked_delta` and you
    know how much memory was needed to complete that stage.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*_peaked_delta` - 是任何额外消耗然后释放的内存 - 相对于当前分配的内存计数器 - 它永远不会是负数。当您查看任何阶段的指标时，您将`alloc_delta`
    + `peaked_delta`相加，就知道完成该阶段需要多少内存。'
- en: The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu).
    Typically this is enough since the main process does the bulk of work, but it
    could be not quite so if model parallel is used and then other GPUs may use a
    different amount of gpu memory. This is also not the same under DataParallel where
    gpu0 may require much more memory than the rest since it stores the gradient and
    optimizer states for all participating GPUS. Perhaps in the future these reports
    will evolve to measure those too.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 仅对rank 0和gpu 0的进程进行报告（如果有gpu）。通常这已经足够了，因为主进程完成大部分工作，但如果使用模型并行，情况可能不太一样，其他GPU可能使用不同数量的gpu内存。在DataParallel下也不同，因为gpu0可能需要比其他GPU更多的内存，因为它存储了所有参与GPU的梯度和优化器状态。也许在未来，这些报告将发展到测量这些内容。
- en: The CPU RAM metric measures RSS (Resident Set Size) includes both the memory
    which is unique to the process and the memory shared with other processes. It
    is important to note that it does not include swapped out memory, so the reports
    could be imprecise.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: CPU RAM指标测量RSS（Resident Set Size），包括进程独有的内存和与其他进程共享的内存。重要的是要注意，它不包括被交换出的内存，因此报告可能不够精确。
- en: The CPU peak memory is measured using a sampling thread. Due to python’s GIL
    it may miss some of the peak memory if that thread didn’t get a chance to run
    when the highest memory was used. Therefore this report can be less than reality.
    Using `tracemalloc` would have reported the exact peak memory, but it doesn’t
    report memory allocations outside of python. So if some C++ CUDA extension allocated
    its own memory it won’t be reported. And therefore it was dropped in favor of
    the memory sampling approach, which reads the current process memory usage.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: CPU峰值内存是使用采样线程测量的。由于python的GIL，如果该线程在使用最高内存时没有运行的机会，它可能会错过一些峰值内存。因此，这份报告可能小于实际情况。使用`tracemalloc`将报告准确的峰值内存，但它不会报告python之外的内存分配。因此，如果某个C++
    CUDA扩展分配了自己的内存，它将不会被报告。因此，它被放弃，以支持内存采样方法，该方法读取当前进程的内存使用情况。
- en: The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()`
    and `torch.cuda.max_memory_allocated()`. This metric reports only “deltas” for
    pytorch-specific allocations, as `torch.cuda` memory management system doesn’t
    track any memory allocated outside of pytorch. For example, the very first cuda
    call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GPU分配和峰值内存报告是通过`torch.cuda.memory_allocated()`和`torch.cuda.max_memory_allocated()`完成的。这个指标仅报告pytorch特定分配的“增量”，因为`torch.cuda`内存管理系统不跟踪pytorch之外分配的任何内存。例如，第一个cuda调用通常加载CUDA内核，可能占用0.5到2GB的GPU内存。
- en: Note that this tracker doesn’t account for memory allocations outside of [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)’s
    `__init__`, `train`, `evaluate` and `predict` calls.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此跟踪器不考虑[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的`__init__`、`train`、`evaluate`和`predict`调用之外的内存分配。
- en: Because `evaluation` calls may happen during `train`, we can’t handle nested
    invocations because `torch.cuda.max_memory_allocated` is a single counter, so
    if it gets reset by a nested eval call, `train`’s tracker will report incorrect
    info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266)
    gets resolved it will be possible to change this class to be re-entrant. Until
    then we will only track the outer level of `train`, `evaluate` and `predict` methods.
    Which means that if `eval` is called during `train`, it’s the latter that will
    account for its memory usage and that of the former.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`evaluation`调用可能发生在`train`期间，我们无法处理嵌套调用，因为`torch.cuda.max_memory_allocated`是一个计数器，所以如果它被嵌套的eval调用重置，`train`的跟踪器将报告不正确的信息。如果这个[pytorch问题](https://github.com/pytorch/pytorch/issues/16266)得到解决，将有可能将这个类改为可重入。在那之前，我们只会跟踪`train`、`evaluate`和`predict`方法的外层级别。这意味着如果在`train`期间调用`eval`，后者将记录其内存使用情况以及前者的内存使用情况。
- en: This also means that if any other tool that is used along the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    calls `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be
    invalid. And the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will disrupt the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats`
    themselves.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着如果在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)调用期间使用任何其他工具`torch.cuda.reset_peak_memory_stats`，则gpu峰值内存统计数据可能无效。而且[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将破坏任何依赖于自己调用`torch.cuda.reset_peak_memory_stats`的工具的正常行为。
- en: For best performance you may want to consider turning the memory profiling off
    for production runs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，您可能希望在生产运行中关闭内存分析。
- en: '#### `metrics_format`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `metrics_format`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L879)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L879)'
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`metrics` (`Dict[str, float]`) — The metrics returned from train/evaluate/predict'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` (`Dict[str, float]`) — 训练/评估/预测返回的指标'
- en: Returns
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: metrics (`Dict[str, float]`)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: metrics (`Dict[str, float]`)
- en: The reformatted metrics
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 重格式化的指标
- en: Reformat Trainer metrics values to a human-readable format
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 将Trainer指标值重新格式化为人类可读的格式
- en: '#### `num_examples`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_examples`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1128)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1128)'
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Helper to get number of samples in a `~torch.utils.data.DataLoader` by accessing
    its dataset. When dataloader.dataset does not exist or has no length, estimates
    as best it can
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问其数据集来获取`~torch.utils.data.DataLoader`中样本数的帮助程序。当数据加载器数据集不存在或没有长度时，尽可能估计
- en: '#### `num_tokens`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1142)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1142)'
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Helper to get number of tokens in a `~torch.utils.data.DataLoader` by enumerating
    dataloader.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过枚举数据加载器来获取`~torch.utils.data.DataLoader`中的标记数的帮助程序。
- en: '#### `pop_callback`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pop_callback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L665)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L665)'
- en: '[PRE27]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`callback` (`type` or [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    — A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    class or an instance of a [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    In the first case, will pop the first member of that class found in the list of
    callbacks.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`（`type`或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)）-
    [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)类或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)的实例。在第一种情况下，将弹出在回调列表中找到的该类的第一个成员。'
- en: Returns
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)'
- en: The callback removed, if found.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到，将删除回调。
- en: Remove a callback from the current list of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    and returns it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从当前的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表中删除回调并返回它。
- en: If the callback is not found, returns `None` (and no error is raised).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未找到回调，则返回`None`（不会引发错误）。
- en: '#### `predict`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3129)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3129)'
- en: '[PRE28]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`test_dataset` (`Dataset`) — Dataset to run the predictions on. If it is an
    `datasets.Dataset`, columns not accepted by the `model.forward()` method are automatically
    removed. Has to implement the method `__len__`'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dataset`（`Dataset`）- 要在其上运行预测的数据集。如果它是`datasets.Dataset`，则不被`model.forward()`方法接受的列将自动删除。必须实现方法`__len__`'
- en: '`ignore_keys` (`List[str]`, *optional*) — A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys`（`List[str]`，*可选*）- 模型输出中应忽略的键列表（如果它是字典）。'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"test"`) — An optional
    prefix to be used as the metrics key prefix. For example the metrics “bleu” will
    be named “test_bleu” if the prefix is “test” (default)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix`（`str`，*可选*，默认为`"test"`）- 用作指标键前缀的可选前缀。例如，如果前缀是“test”（默认），则指标“bleu”将被命名为“test_bleu”。'
- en: Run prediction and returns predictions and potential metrics.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 运行预测并返回预测和潜在指标。
- en: Depending on the dataset and your use case, your test dataset may contain labels.
    In that case, this method will also return metrics, like in `evaluate()`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集和用例，您的测试数据集可能包含标签。在这种情况下，此方法还将返回指标，就像在`evaluate()`中一样。
- en: If your predictions or labels have different sequence length (for instance because
    you’re doing dynamic padding in a token classification task) the predictions will
    be padded (on the right) to allow for concatenation into one array. The padding
    index is -100.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的预测或标签具有不同的序列长度（例如，因为您在标记分类任务中进行动态填充），则预测将被填充（在右侧），以允许连接到一个数组中。填充索引为-100。
- en: 'Returns: *NamedTuple* A namedtuple with the following keys:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：*NamedTuple* 具有以下键的命名元组：
- en: 'predictions (`np.ndarray`): The predictions on `test_dataset`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测（`np.ndarray`）：在`test_dataset`上的预测。
- en: 'label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained
    some).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: label_ids（`np.ndarray`，*可选*）：标签（如果数据集包含）。
- en: 'metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics
    (if the dataset contained labels).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标（`Dict[str, float]`，*可选*）：潜在的指标字典（如果数据集包含标签）。
- en: '#### `prediction_loop`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prediction_loop`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3766)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3766)'
- en: '[PRE29]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 预测/评估循环，由`Trainer.evaluate()`和`Trainer.predict()`共享。
- en: Works both with or without labels.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否有标签，都可以使用。
- en: '#### `prediction_step`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prediction_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3424)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3424)'
- en: '[PRE30]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`nn.Module`) — The model to evaluate.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`nn.Module`）- 要评估的模型。'
- en: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — The inputs and targets of
    the model.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（`Dict[str, Union[torch.Tensor, Any]]`）- 模型的输入和目标。'
- en: The dictionary will be unpacked before being fed to the model. Most models expect
    the targets under the argument `labels`. Check your model’s documentation for
    all accepted arguments.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在馈送模型之前，字典将被解包。大多数模型希望目标在参数`labels`下。检查您模型的文档以获取所有接受的参数。
- en: '`prediction_loss_only` (`bool`) — Whether or not to return the loss only.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_loss_only`（`bool`）- 是否仅返回损失。'
- en: '`ignore_keys` (`List[str]`, *optional*) — A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys`（`List[str]`，*可选*）- 模型输出中应忽略的键列表（如果它是字典）。'
- en: Returns
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
- en: A tuple with the loss, logits and labels (each being optional).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含损失、logits 和标签的元组（每个都是可选的）。
- en: Perform an evaluation step on `model` using `inputs`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `inputs` 在 `model` 上执行评估步骤。
- en: Subclass and override to inject custom behavior.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 子类和覆盖以注入自定义行为。
- en: '#### `propagate_args_to_deepspeed`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `propagate_args_to_deepspeed`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L4012)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L4012)'
- en: '[PRE31]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Sets values in the deepspeed plugin based on the Trainer args
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Trainer 参数在 deepspeed 插件中设置值
- en: '#### `push_to_hub`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3702)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3702)'
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`commit_message` (`str`, *optional*, defaults to `"End of training"`) — Message
    to commit while pushing.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`, *可选*, 默认为 `"End of training"`) — 推送时要提交的消息。'
- en: '`blocking` (`bool`, *optional*, defaults to `True`) — Whether the function
    should return only when the `git push` has finished.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blocking` (`bool`, *可选*, 默认为 `True`) — 函数是否应该在 `git push` 完成后才返回。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to [create_model_card()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_model_card).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 传递给 [create_model_card()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_model_card)
    的额外关键字参数。'
- en: Upload `self.model` and `self.tokenizer` to the 🤗 model hub on the repo `self.args.hub_model_id`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `self.model` 和 `self.tokenizer` 上传到 🤗 模型中心，存储库为 `self.args.hub_model_id`。
- en: '#### `remove_callback`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `remove_callback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L681)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L681)'
- en: '[PRE33]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`callback` (`type` or [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    — A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    class or an instance of a [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    In the first case, will remove the first member of that class found in the list
    of callbacks.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`回调` (`类型` 或 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    — 一个 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    类或一个 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    的实例。在第一种情况下，将删除在回调列表中找到的该类的第一个成员。'
- en: Remove a callback from the current list of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 从当前的 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    列表中删除一个回调。
- en: '#### `save_metrics`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_metrics`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L995)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L995)'
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`split` (`str`) — Mode/split name: one of `train`, `eval`, `test`, `all`'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split` (`str`) — 模式/拆分名称：`train`, `eval`, `test`, `all` 中的一个'
- en: '`metrics` (`Dict[str, float]`) — The metrics returned from train/evaluate/predict'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` (`Dict[str, float]`) — 从训练/评估/预测返回的指标'
- en: '`combined` (`bool`, *optional*, defaults to `True`) — Creates combined metrics
    by updating `all_results.json` with metrics of this call'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combined` (`bool`, *可选*, 默认为 `True`) — 通过更新 `all_results.json` 创建组合指标，其中包括此调用的指标。'
- en: Save metrics into a json file for that split, e.g. `train_results.json`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 将指标保存到一个 json 文件中，例如 `train_results.json`。
- en: Under distributed environment this is done only for a process with rank 0.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境下，这仅适用于秩为0的进程。
- en: To understand the metrics please read the docstring of [log_metrics()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log_metrics).
    The only difference is that raw unformatted numbers are saved in the current method.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解指标，请阅读 [log_metrics()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log_metrics)
    的文档字符串。唯一的区别是原始未格式化的数字保存在当前方法中。
- en: '#### `save_model`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2841)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2841)'
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Will save the model, so you can reload it using `from_pretrained()`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将保存模型，因此您可以使用 `from_pretrained()` 重新加载它。
- en: Will only save from the main process.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 仅从主进程保存。
- en: '#### `save_state`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_state`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L1033)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L1033)'
- en: '[PRE36]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Saves the Trainer state, since Trainer.save_model saves only the tokenizer with
    the model
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 保存 Trainer 状态，因为 Trainer.save_model 仅保存了模型的 tokenizer
- en: Under distributed environment this is done only for a process with rank 0.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境下，这仅适用于秩为0的进程。
- en: '#### `train`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `train`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1438)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1438)'
- en: '[PRE37]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`resume_from_checkpoint` (`str` or `bool`, *optional*) — If a `str`, local
    path to a saved checkpoint as saved by a previous instance of [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If a `bool` and equals `True`, load the last checkpoint in *args.output_dir* as
    saved by a previous instance of [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If present, training will resume from the model/optimizer/scheduler states loaded
    here.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_from_checkpoint` (`str` 或 `bool`, *可选*) — 如果是 `str`，则是由之前的 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    实例保存的本地路径的检查点。如果是 `bool` 并且等于 `True`，则加载由之前的 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    实例保存在 *args.output_dir* 中的最后一个检查点。如果存在，训练将从此处加载的模型/优化器/调度器状态恢复。'
- en: '`trial` (`optuna.Trial` or `Dict[str, Any]`, *optional*) — The trial run or
    the hyperparameter dictionary for hyperparameter search.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trial` (`optuna.Trial` 或 `Dict[str, Any]`, *可选*) — 运行试验或用于超参数搜索的超参数字典。'
- en: '`ignore_keys_for_eval` (`List[str]`, *optional*) — A list of keys in the output
    of your model (if it is a dictionary) that should be ignored when gathering predictions
    for evaluation during the training.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys_for_eval` (`List[str]`, *可选*) — 您的模型输出中应在评估期间忽略的键的列表（如果它是一个字典）。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments used
    to hide deprecated arguments'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 用于隐藏已弃用参数的附加关键字参数'
- en: Main training entry point.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 主要训练入口。
- en: '#### `training_step`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `training_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2746)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2746)'
- en: '[PRE38]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`nn.Module`) — The model to train.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`nn.Module`) — 要训练的模型。'
- en: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — The inputs and targets of
    the model.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — 模型的输入和目标。'
- en: The dictionary will be unpacked before being fed to the model. Most models expect
    the targets under the argument `labels`. Check your model’s documentation for
    all accepted arguments.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字典将在馈送到模型之前解包。大多数模型期望目标在参数`labels`下。检查您模型的文档以获取所有接受的参数。
- en: Returns
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: The tensor with training loss on this batch.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这批次的训练损失的张量。
- en: Perform a training step on a batch of inputs.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对一批输入执行训练步骤。
- en: Subclass and override to inject custom behavior.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 子类和覆盖以注入自定义行为。
- en: Seq2SeqTrainer
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2SeqTrainer
- en: '### `class transformers.Seq2SeqTrainer`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Seq2SeqTrainer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L41)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L41)'
- en: '[PRE39]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `evaluate`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L112)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L112)'
- en: '[PRE40]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`eval_dataset` (`Dataset`, *optional*) — Pass a dataset if you wish to override
    `self.eval_dataset`. If it is an [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement the `__len__` method.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (`Dataset`, *可选*) — 如果要覆盖`self.eval_dataset`，请传递一个数据集。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。它必须实现`__len__`方法。'
- en: '`ignore_keys` (`List[str]`, *optional*) — A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys` (`List[str]`, *可选*) — 您的模型输出中应在收集预测时忽略的键的列表。'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"eval"`) — An optional
    prefix to be used as the metrics key prefix. For example the metrics “bleu” will
    be named “eval_bleu” if the prefix is `"eval"` (default)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix` (`str`, *可选*, 默认为`"eval"`) — 用作指标键前缀的可选前缀。例如，如果前缀是`"eval"`（默认），则指标“bleu”将被命名为“eval_bleu”。'
- en: '`max_length` (`int`, *optional*) — The maximum target length to use when predicting
    with the generate method.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 在使用`generate`方法进行预测时使用的最大目标长度。'
- en: '`num_beams` (`int`, *optional*) — Number of beams for beam search that will
    be used when predicting with the generate method. 1 means no beam search. gen_kwargs
    — Additional `generate` specific kwargs.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`, *可选*) — 在使用`generate`方法进行预测时将用于束搜索的束数。1表示没有束搜索。gen_kwargs
    — 附加的`generate`特定kwargs。'
- en: Run evaluation and returns metrics.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 运行评估并返回指标。
- en: The calling script will be responsible for providing a method to compute metrics,
    as they are task-dependent (pass it to the init `compute_metrics` argument).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 调用脚本将负责提供计算指标的方法，因为它们是任务相关的（将其传递给init `compute_metrics`参数）。
- en: You can also subclass and override this method to inject custom behavior.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以子类化并覆盖此方法以注入自定义行为。
- en: '#### `predict`'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L168)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L168)'
- en: '[PRE41]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`test_dataset` (`Dataset`) — Dataset to run the predictions on. If it is a
    [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    Has to implement the method `__len__`'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dataset` (`Dataset`) — 要在其上运行预测的数据集。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。必须实现`__len__`方法'
- en: '`ignore_keys` (`List[str]`, *optional*) — A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys` (`List[str]`, *可选*) — 您的模型输出中应在收集预测时忽略的键的列表。'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"eval"`) — An optional
    prefix to be used as the metrics key prefix. For example the metrics “bleu” will
    be named “eval_bleu” if the prefix is `"eval"` (default)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix` (`str`, *可选*, 默认为`"eval"`) — 用作指标键前缀的可选前缀。例如，如果前缀是`"eval"`（默认），则指标“bleu”将被命名为“eval_bleu”。'
- en: '`max_length` (`int`, *optional*) — The maximum target length to use when predicting
    with the generate method.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 在使用`generate`方法进行预测时使用的最大目标长度。'
- en: '`num_beams` (`int`, *optional*) — Number of beams for beam search that will
    be used when predicting with the generate method. 1 means no beam search. gen_kwargs
    — Additional `generate` specific kwargs.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`, *可选*) — 在使用`generate`方法进行预测时将用于束搜索的束数。1表示没有束搜索。gen_kwargs
    — 附加的`generate`特定kwargs。'
- en: Run prediction and returns predictions and potential metrics.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 运行预测并返回预测和潜在指标。
- en: Depending on the dataset and your use case, your test dataset may contain labels.
    In that case, this method will also return metrics, like in `evaluate()`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集和您的用例，您的测试数据集可能包含标签。在这种情况下，此方法还将返回指标，就像在`evaluate()`中一样。
- en: If your predictions or labels have different sequence lengths (for instance
    because you’re doing dynamic padding in a token classification task) the predictions
    will be padded (on the right) to allow for concatenation into one array. The padding
    index is -100.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的预测或标签具有不同的序列长度（例如，因为您在标记分类任务中进行动态填充），则预测将被填充（在右侧）以允许连接成一个数组。填充索引为 -100。
- en: 'Returns: *NamedTuple* A namedtuple with the following keys:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '返回: *NamedTuple* 具有以下键的命名元组:'
- en: 'predictions (`np.ndarray`): The predictions on `test_dataset`.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'predictions (`np.ndarray`): 在 `test_dataset` 上的预测。'
- en: 'label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained
    some).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'label_ids (`np.ndarray`, *optional*): 标签（如果数据集包含标签）。'
- en: 'metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics
    (if the dataset contained labels).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'metrics (`Dict[str, float]`, *optional*): 潜在的指标字典（如果数据集包含标签）。'
- en: TrainingArguments
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainingArguments
- en: '### `class transformers.TrainingArguments`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainingArguments`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161)'
- en: '[PRE42]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`output_dir` (`str`) — The output directory where the model predictions and
    checkpoints will be written.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir` (`str`) — 模型预测和检查点将被写入的输出目录。'
- en: '`overwrite_output_dir` (`bool`, *optional*, defaults to `False`) — If `True`,
    overwrite the content of the output directory. Use this to continue training if
    `output_dir` points to a checkpoint directory.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overwrite_output_dir` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，则覆盖输出目录的内容。使用此选项继续训练，如果
    `output_dir` 指向检查点目录。'
- en: '`do_train` (`bool`, *optional*, defaults to `False`) — Whether to run training
    or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_train` (`bool`, *optional*, 默认为 `False`) — 是否运行训练。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是用于您的训练/评估脚本。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples)
    获取更多详细信息。'
- en: '`do_eval` (`bool`, *optional*) — Whether to run evaluation on the validation
    set or not. Will be set to `True` if `evaluation_strategy` is different from `"no"`.
    This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_eval` (`bool`, *optional*) — 是否在验证集上运行评估。如果 `evaluation_strategy` 与 `"no"`
    不同，则将设置为 `True`。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是用于您的训练/评估脚本。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples)
    获取更多详细信息。'
- en: '`do_predict` (`bool`, *optional*, defaults to `False`) — Whether to run predictions
    on the test set or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_predict` (`bool`, *optional*, 默认为 `False`) — 是否在测试集上运行预测。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是用于您的训练/评估脚本。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples)
    获取更多详细信息。'
- en: '`evaluation_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) — The evaluation strategy to adopt during training.
    Possible values are:'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluation_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, 默认为 `"no"`) — 训练期间采用的评估策略。可能的值为:'
- en: '`"no"`: No evaluation is done during training.'
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练期间不进行评估。'
- en: '`"steps"`: Evaluation is done (and logged) every `eval_steps`.'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每 `eval_steps` 次进行评估（并记录日志）。'
- en: '`"epoch"`: Evaluation is done at the end of each epoch.'
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 每个时期结束时进行评估。'
- en: '`prediction_loss_only` (`bool`, *optional*, defaults to `False`) — When performing
    evaluation and generating predictions, only returns the loss.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_loss_only` (`bool`, *optional*, 默认为 `False`) — 在进行评估和生成预测时，仅返回损失。'
- en: '`per_device_train_batch_size` (`int`, *optional*, defaults to 8) — The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for training.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_train_batch_size` (`int`, *optional*, 默认为 8) — 用于训练的每个 GPU/XPU/TPU/MPS/NPU
    核心/CPU 的批处理大小。'
- en: '`per_device_eval_batch_size` (`int`, *optional*, defaults to 8) — The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_eval_batch_size` (`int`, *optional*, 默认为 8) — 用于评估的每个 GPU/XPU/TPU/MPS/NPU
    核心/CPU 的批处理大小。'
- en: '`gradient_accumulation_steps` (`int`, *optional*, defaults to 1) — Number of
    updates steps to accumulate the gradients for, before performing a backward/update
    pass.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` (`int`, *optional*, 默认为 1) — 累积梯度的更新步数，然后执行反向/更新传递。'
- en: When using gradient accumulation, one step is counted as one step with backward
    pass. Therefore, logging, evaluation, save will be conducted every `gradient_accumulation_steps
    * xxx_step` training examples.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用梯度累积时，一个步骤被计为一个带有反向传递的步骤。因此，每 `gradient_accumulation_steps * xxx_step` 训练示例将进行日志记录、评估和保存。
- en: '`eval_accumulation_steps` (`int`, *optional*) — Number of predictions steps
    to accumulate the output tensors for, before moving the results to the CPU. If
    left unset, the whole predictions are accumulated on GPU/NPU/TPU before being
    moved to the CPU (faster but requires more memory).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_accumulation_steps` (`int`, *optional*) — 在将结果移动到 CPU 之前，累积输出张量的预测步数。如果未设置，整个预测将在
    GPU/NPU/TPU 上累积后再移动到 CPU（速度更快但需要更多内存）。'
- en: '`eval_delay` (`float`, *optional*) — Number of epochs or steps to wait for
    before the first evaluation can be performed, depending on the evaluation_strategy.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_delay` (`float`, *optional*) — 在进行第一次评估之前等待的时期或步数，取决于 `evaluation_strategy`。'
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) — The initial learning
    rate for [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` (`float`, *optional*, defaults to 5e-5) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的初始学习率。'
- en: '`weight_decay` (`float`, *optional*, defaults to 0) — The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights in [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` (`float`, *optional*, defaults to 0) — 应用的权重衰减（如果不为零）到 [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器中的所有层，除了所有偏置和 LayerNorm 权重。'
- en: '`adam_beta1` (`float`, *optional*, defaults to 0.9) — The beta1 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta1` (`float`, *optional*, defaults to 0.9) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的 beta1 超参数。'
- en: '`adam_beta2` (`float`, *optional*, defaults to 0.999) — The beta2 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta2` (`float`, *optional*, defaults to 0.999) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的 beta2 超参数。'
- en: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) — The epsilon hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的 epsilon 超参数。'
- en: '`max_grad_norm` (`float`, *optional*, defaults to 1.0) — Maximum gradient norm
    (for gradient clipping).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_grad_norm` (`float`, *optional*, defaults to 1.0) — 最大梯度范数（用于梯度裁剪）。'
- en: '`num_train_epochs(float,` *optional*, defaults to 3.0) — Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_epochs(float,` *optional*, defaults to 3.0) — 执行的总训练时代数（如果不是整数，则在停止训练之前执行最后一个时代的小数部分百分比）。'
- en: '`max_steps` (`int`, *optional*, defaults to -1) — If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *optional*, defaults to -1) — 如果设置为正数，则执行的总训练步数。覆盖 `num_train_epochs`。对于有限的数据集，训练通过数据集（如果所有数据都用完）重复进行，直到达到
    `max_steps`。'
- en: '`lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) — The scheduler type to use. See the documentation
    of [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    for all possible values.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) — 要使用的调度器类型。查看 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    的文档以获取所有可能的值。'
- en: '`lr_scheduler_kwargs` (‘dict’, *optional*, defaults to {}) — The extra arguments
    for the lr_scheduler. See the documentation of each scheduler for possible values.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_kwargs` (‘dict’, *optional*, defaults to {}) — lr_scheduler 的额外参数。查看每个调度器的文档以获取可能的值。'
- en: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) — Ratio of total training
    steps used for a linear warmup from 0 to `learning_rate`.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) — 用于从 0 线性预热到 `learning_rate`
    的总训练步数的比率。'
- en: '`warmup_steps` (`int`, *optional*, defaults to 0) — Number of steps used for
    a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`, *optional*, defaults to 0) — 用于从 0 线性预热到 `learning_rate`
    的步骤数。覆盖 `warmup_ratio` 的任何效果。'
- en: '`log_level` (`str`, *optional*, defaults to `passive`) — Logger log level to
    use on the main process. Possible choices are the log levels as strings: ‘debug’,
    ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t
    set anything and keeps the current log level for the Transformers library (which
    will be `"warning"` by default).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level` (`str`, *optional*, defaults to `passive`) — 主进程使用的记录器日志级别。可能的选择是字符串形式的日志级别：''debug''、''info''、''warning''、''error''和''critical''，以及一个''passive''级别，它不设置任何内容，并保持Transformers库的当前日志级别（默认为`"warning"`）。'
- en: '`log_level_replica` (`str`, *optional*, defaults to `"warning"`) — Logger log
    level to use on replicas. Same choices as `log_level`”'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level_replica` (`str`, *optional*, defaults to `"warning"`) — 副本使用的记录器日志级别。与
    `log_level` 相同的选择。'
- en: '`log_on_each_node` (`bool`, *optional*, defaults to `True`) — In multinode
    distributed training, whether to log using `log_level` once per node, or only
    on the main node.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_on_each_node` (`bool`, *optional*, defaults to `True`) — 在多节点分布式训练中，是否每个节点使用
    `log_level` 进行日志记录，还是仅在主节点上进行。'
- en: '`logging_dir` (`str`, *optional*) — [TensorBoard](https://www.tensorflow.org/tensorboard)
    log directory. Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_dir` (`str`, *optional*) — [TensorBoard](https://www.tensorflow.org/tensorboard)
    日志目录。默认为 *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***。'
- en: '`logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — The logging strategy to adopt during training.
    Possible values are:'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — 训练期间采用的日志记录策略。可能的值有：'
- en: '`"no"`: No logging is done during training.'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练期间不进行日志记录。'
- en: '`"epoch"`: Logging is done at the end of each epoch.'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 每个时代结束时进行日志记录。'
- en: '`"steps"`: Logging is done every `logging_steps`.'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每 `logging_steps` 进行日志记录。'
- en: '`logging_first_step` (`bool`, *optional*, defaults to `False`) — Whether to
    log and evaluate the first `global_step` or not.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_first_step` (`bool`, *optional*, defaults to `False`) — 是否记录和评估第一个
    `global_step`。'
- en: '`logging_steps` (`int` or `float`, *optional*, defaults to 500) — Number of
    update steps between two logs if `logging_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_steps` (`int` or `float`, *optional*, defaults to 500) — 如果 `logging_strategy="steps"`，则在两个日志之间的更新步骤数。应为整数或范围为
    `[0,1)` 的浮点数。如果小于 1，将被解释为总训练步骤的比率。'
- en: '`logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) — Whether
    to filter `nan` and `inf` losses for logging. If set to `True` the loss of every
    step that is `nan` or `inf` is filtered and the average loss of the current logging
    window is taken instead.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) — 是否过滤用于记录的
    `nan` 和 `inf` 损失。如果设置为 `True`，则会过滤每个步骤的损失，如果为 `nan` 或 `inf`，则取当前日志窗口的平均损失。 '
- en: '`logging_nan_inf_filter` only influences the logging of loss values, it does
    not change the behavior the gradient is computed or applied to the model.'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` 仅影响损失值的记录，不会改变梯度的计算或应用于模型的行为。'
- en: '`save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — The checkpoint save strategy to adopt during
    training. Possible values are:'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — 训练期间采用的检查点保存策略。可能的值有：'
- en: '`"no"`: No save is done during training.'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练期间不进行保存。'
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 在每个时期结束时保存。'
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每 `save_steps` 保存一次。'
- en: '`save_steps` (`int` or `float`, *optional*, defaults to 500) — Number of updates
    steps before two checkpoint saves if `save_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps` (`int` or `float`, *optional*, defaults to 500) — 如果 `save_strategy="steps"`，在两次检查点保存之前的更新步骤数。应为整数或范围为
    `[0,1)` 的浮点数。如果小于 1，将被解释为总训练步骤的比率。'
- en: '`save_total_limit` (`int`, *optional*) — If a value is passed, will limit the
    total amount of checkpoints. Deletes the older checkpoints in `output_dir`. When
    `load_best_model_at_end` is enabled, the “best” checkpoint according to `metric_for_best_model`
    will always be retained in addition to the most recent ones. For example, for
    `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will
    always be retained alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`,
    it is possible that two checkpoints are saved: the last one and the best one (if
    they are different).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_total_limit` (`int`, *optional*) — 如果传递了一个值，将限制检查点的总量。删除 `output_dir`
    中的旧检查点。当启用 `load_best_model_at_end` 时，根据 `metric_for_best_model` 的“最佳”检查点将始终保留在最近的检查点之外。例如，对于
    `save_total_limit=5` 和 `load_best_model_at_end`，最后四个检查点将始终与最佳模型一起保留。当 `save_total_limit=1`
    和 `load_best_model_at_end` 时，可能保存两个检查点：最后一个和最佳一个（如果它们不同）。'
- en: '`save_safetensors` (`bool`, *optional*, defaults to `True`) — Use [safetensors](https://huggingface.co/docs/safetensors)
    saving and loading for state dicts instead of default `torch.load` and `torch.save`.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_safetensors` (`bool`, *optional*, defaults to `True`) — 使用 [safetensors](https://huggingface.co/docs/safetensors)
    保存和加载状态字典，而不是默认的 `torch.load` 和 `torch.save`。'
- en: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) — When doing
    multi-node distributed training, whether to save models and checkpoints on each
    node, or only on the main one.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) — 在进行多节点分布式训练时，是否在每个节点上保存模型和检查点，还是只在主节点上保存。'
- en: This should not be activated when the different nodes use the same storage as
    the files will be saved with the same names for each node.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当不同节点使用相同存储时，不应激活此选项，因为文件将以相同名称保存在每个节点上。
- en: '`save_only_model` (`bool`, *optional*, defaults to `False`) — When checkpointing,
    whether to only save the model, or also the optimizer, scheduler & rng state.
    Note that when this is true, you won’t be able to resume training from checkpoint.
    This enables you to save storage by not storing the optimizer, scheduler & rng
    state. You can only load the model using `from_pretrained` with this option set
    to `True`.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_only_model` (`bool`, *optional*, defaults to `False`) — 在进行检查点时，是否仅保存模型，还是同时保存优化器、调度器和
    rng 状态。请注意，当此选项为 true 时，您将无法从检查点恢复训练。这样可以通过不存储优化器、调度器和 rng 状态来节省存储空间。您只能使用 `from_pretrained`
    加载模型，并将此选项设置为 `True`。'
- en: '`use_cpu` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    cpu. If set to False, we will use cuda or mps device if available.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cpu` (`bool`, *optional*, defaults to `False`) — 是否使用 cpu。如果设置为 False，将使用
    cuda 或 mps 设备（如果可用）。'
- en: '`seed` (`int`, *optional*, defaults to 42) — Random seed that will be set at
    the beginning of training. To ensure reproducibility across runs, use the `~Trainer.model_init`
    function to instantiate the model if it has some randomly initialized parameters.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`, *optional*, defaults to 42) — 在训练开始时设置的随机种子。为了确保多次运行的可重现性，请使用
    `~Trainer.model_init` 函数来实例化模型，如果模型具有一些随机初始化的参数。'
- en: '`data_seed` (`int`, *optional*) — Random seed to be used with data samplers.
    If not set, random generators for data sampling will use the same seed as `seed`.
    This can be used to ensure reproducibility of data sampling, independent of the
    model seed.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_seed` (`int`, *optional*) — 用于数据采样器的随机种子。如果未设置，用于数据采样的随机生成器将使用与 `seed`
    相同的种子。这可用于确保数据采样的可重现性，独立于模型种子。'
- en: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) — Whether or not
    to use PyTorch jit trace for inference.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) — 是否使用 PyTorch jit
    trace 进行推断。'
- en: '`use_ipex` (`bool`, *optional*, defaults to `False`) — Use Intel extension
    for PyTorch when it is available. [IPEX installation](https://github.com/intel/intel-extension-for-pytorch).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_ipex` (`bool`, *optional*, defaults to `False`) — 在可用时使用 PyTorch 的 Intel
    扩展。[IPEX 安装](https://github.com/intel/intel-extension-for-pytorch)。'
- en: '`bf16` (`bool`, *optional*, defaults to `False`) — Whether to use bf16 16-bit
    (mixed) precision training instead of 32-bit training. Requires Ampere or higher
    NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental
    API and it may change.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16` (`bool`, *optional*, defaults to `False`) — 是否使用bf16 16位（混合）精度训练，而不是32位训练。需要安培或更高的NVIDIA架构，或者使用CPU（use_cpu）或Ascend
    NPU。这是一个实验性API，可能会更改。'
- en: '`fp16` (`bool`, *optional*, defaults to `False`) — Whether to use fp16 16-bit
    (mixed) precision training instead of 32-bit training.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16` (`bool`, *optional*, defaults to `False`) — 是否使用fp16 16位（混合）精度训练，而不是32位训练。'
- en: '`fp16_opt_level` (`str`, *optional*, defaults to ‘O1’) — For `fp16` training,
    Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details
    on the [Apex documentation](https://nvidia.github.io/apex/amp).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_opt_level` (`str`, *optional*, defaults to ‘O1’) — 对于`fp16`训练，选择在[‘O0’,
    ‘O1’, ‘O2’, 和 ‘O3’]中的Apex AMP优化级别。有关详细信息，请参阅[Apex文档](https://nvidia.github.io/apex/amp)。'
- en: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) — This argument is
    deprecated. Use `half_precision_backend` instead.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) — 此参数已弃用。请改用`half_precision_backend`。'
- en: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) — The backend
    to use for mixed precision training. Must be one of `"auto", "apex", "cpu_amp"`.
    `"auto"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected,
    while the other choices will force the requested backend.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) — 用于混合精度训练的后端。必须是`"auto",
    "apex", "cpu_amp"`之一。`"auto"`将根据检测到的PyTorch版本使用CPU/CUDA AMP或APEX，而其他选择将强制使用请求的后端。'
- en: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) — Whether to use
    full bfloat16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values. This is an experimental API and it may change.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的bfloat16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。这是一个实验性API，可能会更改。'
- en: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) — Whether to use
    full float16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的float16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。'
- en: '`tf32` (`bool`, *optional*) — Whether to enable the TF32 mode, available in
    Ampere and newer GPU architectures. The default value depends on PyTorch’s version
    default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer
    to the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation.
    This is an experimental API and it may change.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf32` (`bool`, *optional*) — 是否启用TF32模式，适用于安培和更新的GPU架构。默认值取决于PyTorch版本的`torch.backends.cuda.matmul.allow_tf32`默认值。有关更多详细信息，请参阅[TF32](https://huggingface.co/docs/transformers/performance#tf32)文档。这是一个实验性API，可能会更改。'
- en: '`local_rank` (`int`, *optional*, defaults to -1) — Rank of the process during
    distributed training.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_rank` (`int`, *optional*, defaults to -1`) — 分布式训练过程中进程的排名。'
- en: '`ddp_backend` (`str`, *optional*) — The backend to use for distributed training.
    Must be one of `"nccl"`, `"mpi"`, `"ccl"`, `"gloo"`, `"hccl"`.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_backend` (`str`, *optional*) — 用于分布式训练的后端。必须是`"nccl"`, `"mpi"`, `"ccl"`,
    `"gloo"`, `"hccl"`之一。'
- en: '`tpu_num_cores` (`int`, *optional*) — When training on TPU, the number of TPU
    cores (automatically passed by launcher script).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tpu_num_cores` (`int`, *optional*) — 在TPU上训练时，TPU核心的数量（由启动脚本自动传递）。'
- en: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) — Whether
    to drop the last incomplete batch (if the length of the dataset is not divisible
    by the batch size) or not.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) — 是否丢弃最后一个不完整的批次（如果数据集的长度不是批次大小的整数倍）。'
- en: '`eval_steps` (`int` or `float`, *optional*) — Number of update steps between
    two evaluations if `evaluation_strategy="steps"`. Will default to the same value
    as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`.
    If smaller than 1, will be interpreted as ratio of total training steps.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps` (`int` or `float`, *optional*) — 如果`evaluation_strategy="steps"`，则两次评估之间的更新步数。如果未设置，将默认为与`logging_steps`相同的值。应为范围为`[0,1)`的整数或浮点数。如果小于1，则将解释为总训练步数的比率。'
- en: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) — Number of subprocesses
    to use for data loading (PyTorch only). 0 means that the data will be loaded in
    the main process.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) — 用于数据加载的子进程数（仅适用于PyTorch）。0表示数据将在主进程中加载。'
- en: '`past_index` (`int`, *optional*, defaults to -1) — Some models like [TransformerXL](../model_doc/transformerxl)
    or [XLNet](../model_doc/xlnet) can make use of the past hidden states for their
    predictions. If this argument is set to a positive int, the `Trainer` will use
    the corresponding output (usually index 2) as the past state and feed it to the
    model at the next training step under the keyword argument `mems`.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_index` (`int`, *optional*, defaults to -1`) — 一些模型（如[TransformerXL](../model_doc/transformerxl)或[XLNet](../model_doc/xlnet)）可以利用过去的隐藏状态进行预测。如果将此参数设置为正整数，则`Trainer`将使用相应的输出（通常为索引2）作为过去状态，并在下一个训练步骤中将其作为关键字参数`mems`提供给模型。'
- en: '`run_name` (`str`, *optional*) — A descriptor for the run. Typically used for
    [wandb](https://www.wandb.com/) and [mlflow](https://www.mlflow.org/) logging.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run_name` (`str`, *optional*) — 运行的描述符。通常用于[wandb](https://www.wandb.com/)和[mlflow](https://www.mlflow.org/)日志记录。'
- en: '`disable_tqdm` (`bool`, *optional*) — Whether or not to disable the tqdm progress
    bars and table of metrics produced by `~notebook.NotebookTrainingTracker` in Jupyter
    Notebooks. Will default to `True` if the logging level is set to warn or lower
    (default), `False` otherwise.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_tqdm` (`bool`, *optional*) — 是否禁用Jupyter笔记本中由`~notebook.NotebookTrainingTracker`生成的tqdm进度条和指标表。如果日志级别设置为warn或更低（默认值），则默认为`True`，否则为`False`。'
- en: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) — Whether
    or not to automatically remove the columns unused by the model forward method.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) — 是否自动删除模型前向方法未使用的列。'
- en: '`label_names` (`List[str]`, *optional*) — The list of keys in your dictionary
    of inputs that correspond to the labels.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_names` (`List[str]`, *可选*) — 您的输入字典中与标签对应的键列表。'
- en: Will eventually default to the list of argument names accepted by the model
    that contain the word “label”, except if the model used is one of the `XxxForQuestionAnswering`
    in which case it will also include the `["start_positions", "end_positions"]`
    keys.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终将默认为模型接受的参数名称列表，其中包含单词“label”，除非使用的模型是 `XxxForQuestionAnswering` 之一，那么还将包括
    `["start_positions", "end_positions"]` 键。
- en: '`load_best_model_at_end` (`bool`, *optional*, defaults to `False`) — Whether
    or not to load the best model found during training at the end of training. When
    this option is enabled, the best checkpoint will always be saved. See [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)
    for more.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_best_model_at_end` (`bool`, *可选*, 默认为 `False`) — 是否在训练结束时加载找到的最佳模型。启用此选项时，将始终保存最佳检查点。有关更多信息，请参阅
    [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)。'
- en: When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`,
    and in the case it is “steps”, `save_steps` must be a round multiple of `eval_steps`.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当设置为 `True` 时，参数 `save_strategy` 需要与 `evaluation_strategy` 相同，并且在其为 “steps”
    的情况下，`save_steps` 必须是 `eval_steps` 的整数倍。
- en: '`metric_for_best_model` (`str`, *optional*) — Use in conjunction with `load_best_model_at_end`
    to specify the metric to use to compare two different models. Must be the name
    of a metric returned by the evaluation with or without the prefix `"eval_"`. Will
    default to `"loss"` if unspecified and `load_best_model_at_end=True` (to use the
    evaluation loss).'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_for_best_model` (`str`, *可选*) — 与 `load_best_model_at_end` 结合使用，指定用于比较两个不同模型的指标。必须是评估返回的指标的名称，带有或不带有前缀
    `"eval_"`。如果未指定且 `load_best_model_at_end=True`，将默认为 `"loss"`（使用评估损失）。'
- en: If you set this value, `greater_is_better` will default to `True`. Don’t forget
    to set it to `False` if your metric is better when lower.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果设置了此值，`greater_is_better` 将默认为 `True`。不要忘记，如果您的指标在较低时更好，则将其设置为 `False`。
- en: '`greater_is_better` (`bool`, *optional*) — Use in conjunction with `load_best_model_at_end`
    and `metric_for_best_model` to specify if better models should have a greater
    metric or not. Will default to:'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`greater_is_better` (`bool`, *可选*) — 与 `load_best_model_at_end` 和 `metric_for_best_model`
    结合使用，指定更好的模型是否应具有更大的指标。默认为：'
- en: '`True` if `metric_for_best_model` is set to a value that isn’t `"loss"` or
    `"eval_loss"`.'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `metric_for_best_model` 设置为不是 `"loss"` 或 `"eval_loss"` 的值，则为 `True`。
- en: '`False` if `metric_for_best_model` is not set, or set to `"loss"` or `"eval_loss"`.'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果未设置 `metric_for_best_model`，或设置为 `"loss"` 或 `"eval_loss"`，则为 `False`。
- en: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) — When resuming
    training, whether or not to skip the epochs and batches to get the data loading
    at the same stage as in the previous training. If set to `True`, the training
    will begin faster (as that skipping step can take a long time) but will not yield
    the same results as the interrupted training would have.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_data_skip` (`bool`, *可选*, 默认为 `False`) — 在恢复训练时，是否跳过批次以使数据加载与先前训练中的阶段相同。如果设置为
    `True`，训练将更快开始（因为跳过步骤可能需要很长时间），但不会产生与中断训练相同的结果。'
- en: '`fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''''`)
    — Use PyTorch Distributed Parallel Training (in distributed training only).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp` (`bool`, `str` 或 `FSDPOption` 列表, *可选*, 默认为 `''''`) — 使用 PyTorch 分布式并行训练（仅在分布式训练中）。'
- en: 'A list of options along the following:'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是一系列选项：
- en: '`"full_shard"`: Shard parameters, gradients and optimizer states.'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"full_shard"`: 分片参数、梯度和优化器状态。'
- en: '`"shard_grad_op"`: Shard optimizer states and gradients.'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"shard_grad_op"`: 分片优化器状态和梯度。'
- en: '`"hybrid_shard"`: Apply `FULL_SHARD` within a node, and replicate parameters
    across nodes.'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard"`: 在节点内应用 `FULL_SHARD`，并在节点之间复制参数。'
- en: '`"hybrid_shard_zero2"`: Apply `SHARD_GRAD_OP` within a node, and replicate
    parameters across nodes.'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard_zero2"`: 在节点内应用 `SHARD_GRAD_OP`，并在节点之间复制参数。'
- en: '`"offload"`: Offload parameters and gradients to CPUs (only compatible with
    `"full_shard"` and `"shard_grad_op"`).'
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"offload"`: 将参数和梯度卸载到 CPU（仅与 `"full_shard"` 和 `"shard_grad_op"` 兼容）。'
- en: '`"auto_wrap"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.'
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"auto_wrap"`: 使用 `default_auto_wrap_policy` 自动递归包装层与 FSDP。'
- en: '`fsdp_config` (`str` or `dict`, *optional*) — Config to be used with fsdp (Pytorch
    Distributed Parallel Training). The value is either a location of fsdp json config
    file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp_config` (`str` 或 `dict`, *可选*) — 用于 fsdp（Pytorch 分布式并行训练）的配置。该值可以是 fsdp
    json 配置文件的位置（例如，`fsdp_config.json`）或已加载的 json 文件作为 `dict`。'
- en: 'A List of config and its options:'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置及其选项列表：
- en: 'min_num_params (`int`, *optional*, defaults to `0`): FSDP’s minimum number
    of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'min_num_params (`int`, *可选*, 默认为 `0`): FSDP 默认自动包装的参数最小数量。 （仅在传递 `fsdp` 字段时有用）。'
- en: 'transformer_layer_cls_to_wrap (`List[str]`, *optional*): List of transformer
    layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`, `T5Block`
    … (useful only when `fsdp` flag is passed).'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'transformer_layer_cls_to_wrap (`List[str]`, *可选*): 要包装的 transformer 层类名称列表（区分大小写），例如，`BertLayer`、`GPTJBlock`、`T5Block`
    …（仅在传递 `fsdp` 标志时有用）。'
- en: backward_prefetch (`str`, *optional*) FSDP’s backward prefetch mode. Controls
    when to prefetch next set of parameters (useful only when `fsdp` field is passed).
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: backward_prefetch (`str`, *可选*) FSDP 的后向预取模式。控制何时预取下一组参数（仅在传递 `fsdp` 字段时有用）。
- en: 'A list of options along the following:'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是一系列选项：
- en: '`"backward_pre"` : Prefetches the next set of parameters before the current
    set of parameter’s gradient computation.'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_pre"` : 在当前参数的梯度计算之前，预取下一组参数。'
- en: '`"backward_post"` : This prefetches the next set of parameters after the current
    set of parameter’s gradient computation.'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_post"` : 在当前参数的梯度计算之后，预取下一组参数。'
- en: forward_prefetch (`bool`, *optional*, defaults to `False`) FSDP’s forward prefetch
    mode (useful only when `fsdp` field is passed). If `"True"`, then FSDP explicitly
    prefetches the next upcoming all-gather while executing in the forward pass.
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: forward_prefetch（`bool`，*可选*，默认为`False`）FSDP的前向预取模式（仅在传递`fsdp`字段时有用）。如果为`"True"`，则FSDP在执行前向传递时明确预取下一个即将到来的全聚集。
- en: limit_all_gathers (`bool`, *optional*, defaults to `False`) FSDP’s limit_all_gathers
    (useful only when `fsdp` field is passed). If `"True"`, FSDP explicitly synchronizes
    the CPU thread to prevent too many in-flight all-gathers.
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: limit_all_gathers（`bool`，*可选*，默认为`False`）FSDP的limit_all_gathers（仅在传递`fsdp`字段时有用）。如果为`"True"`，FSDP明确同步CPU线程，以防止太多的in-flight
    all-gathers。
- en: use_orig_params (`bool`, *optional*, defaults to `True`) If `"True"`, allows
    non-uniform `requires_grad` during init, which means support for interspersed
    frozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning.
    Please refer this [blog]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: use_orig_params（`bool`，*可选*，默认为`True`）如果为`"True"`，允许在初始化期间使用非均匀的`requires_grad`，这意味着支持交替冻结和可训练的参数。在参数高效微调等情况下很有用。请参考这个[博客]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
- en: sync_module_states (`bool`, *optional*, defaults to `True`) If `"True"`, each
    individually wrapped FSDP unit will broadcast module parameters from rank 0 to
    ensure they are the same across all ranks after initialization
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: sync_module_states（`bool`，*可选*，默认为`True`）如果为`"True"`，每个单独包装的FSDP单元将从rank 0广播模块参数，以确保它们在初始化后在所有rank中是相同的
- en: 'activation_checkpointing (`bool`, *optional*, defaults to `False`): If `"True"`,
    activation checkpointing is a technique to reduce memory usage by clearing activations
    of certain layers and recomputing them during a backward pass. Effectively, this
    trades extra computation time for reduced memory usage.'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: activation_checkpointing（`bool`，*可选*，默认为`False`）：如果为`"True"`，激活检查点是一种通过清除某些层的激活并在向后传递期间重新计算它们来减少内存使用的技术。实际上，这是以额外的计算时间换取减少内存使用。
- en: 'xla (`bool`, *optional*, defaults to `False`): Whether to use PyTorch/XLA Fully
    Sharded Data Parallel Training. This is an experimental feature and its API may
    evolve in the future.'
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla（`bool`，*可选*，默认为`False`）：是否使用PyTorch/XLA完全分片数据并行训练。这是一个实验性功能，其API可能会在未来发生变化。
- en: xla_fsdp_settings (`dict`, *optional*) The value is a dictionary which stores
    the XLA FSDP wrapping parameters.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_settings（`dict`，*可选*）该值是一个存储XLA FSDP包装参数的字典。
- en: For a complete list of options, please see [here](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关完整的选项列表，请参见[这里](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)。
- en: 'xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`): Will use gradient
    checkpointing over each nested XLA FSDP wrapped layer. This setting can only be
    used when the xla flag is set to true, and an auto wrapping policy is specified
    through fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_grad_ckpt（`bool`，*可选*，默认为`False`）：将在每个嵌套的XLA FSDP包装层上使用梯度检查点。此设置仅在将xla标志设置为true并通过fsdp_min_num_params或fsdp_transformer_layer_cls_to_wrap指定自动包装策略时才能使用。
- en: '`deepspeed` (`str` or `dict`, *optional*) — Use [Deepspeed](https://github.com/microsoft/deepspeed).
    This is an experimental feature and its API may evolve in the future. The value
    is either the location of DeepSpeed json config file (e.g., `ds_config.json`)
    or an already loaded json file as a `dict`”'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepspeed`（`str`或`dict`，*可选*）— 使用[Deepspeed](https://github.com/microsoft/deepspeed)。这是一个实验性功能，其API可能会在未来发生变化。该值可以是DeepSpeed
    json配置文件的位置（例如，`ds_config.json`）或已加载的json文件作为`dict`”'
- en: '`label_smoothing_factor` (`float`, *optional*, defaults to 0.0) — The label
    smoothing factor to use. Zero means no label smoothing, otherwise the underlying
    onehot-encoded labels are changed from 0s and 1s to `label_smoothing_factor/num_labels`
    and `1 - label_smoothing_factor + label_smoothing_factor/num_labels` respectively.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing_factor`（`float`，*可选*，默认为0.0）— 要使用的标签平滑因子。零表示不进行标签平滑，否则底层的onehot编码标签将从0和1更改为`label_smoothing_factor/num_labels`和`1
    - label_smoothing_factor + label_smoothing_factor/num_labels`。'
- en: '`debug` (`str` or list of `DebugOption`, *optional*, defaults to `""`) — Enable
    one or more debug features. This is an experimental feature.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug`（`str`或`DebugOption`列表，*可选*，默认为`""`）— 启用一个或多个调试功能。这是一个实验性功能。'
- en: 'Possible options are:'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的选项包括：
- en: '`"underflow_overflow"`: detects overflow in model’s input/outputs and reports
    the last frames that led to the event'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"underflow_overflow"`：检测模型输入/输出中的溢出并报告导致事件的最后帧'
- en: '`"tpu_metrics_debug"`: print debug metrics on TPU'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"tpu_metrics_debug"`：在TPU上打印调试指标'
- en: The options should be separated by whitespaces.
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选项应该用空格分隔。
- en: '`optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    — The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused,
    adamw_anyprecision or adafactor.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim`（`str`或`training_args.OptimizerNames`，*可选*，默认为`"adamw_torch"`）— 要使用的优化器：adamw_hf、adamw_torch、adamw_torch_fused、adamw_apex_fused、adamw_anyprecision或adafactor。'
- en: '`optim_args` (`str`, *optional*) — Optional arguments that are supplied to
    AnyPrecisionAdamW.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim_args`（`str`，*可选*）— 供AnyPrecisionAdamW提供的可选参数。'
- en: '`group_by_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to group together samples of roughly the same length in the training dataset (to
    minimize padding applied and be more efficient). Only useful if applying dynamic
    padding.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_by_length`（`bool`，*可选*，默认为`False`）— 是否在训练数据集中将大致相同长度的样本分组在一起（以最小化应用的填充并提高效率）。仅在应用动态填充时有用。'
- en: '`length_column_name` (`str`, *optional*, defaults to `"length"`) — Column name
    for precomputed lengths. If the column exists, grouping by length will use these
    values rather than computing them on train startup. Ignored unless `group_by_length`
    is `True` and the dataset is an instance of `Dataset`.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_column_name` (`str`, *optional*, defaults to `"length"`) — 预先计算长度的列名。如果该列存在，按长度分组将使用这些值而不是在训练启动时计算它们。仅在
    `group_by_length` 为 `True` 且数据集是 `Dataset` 的实例时才会被忽略。'
- en: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — The list
    of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
    `"clearml"`, `"codecarbon"`, `"comet_ml"`, `"dagshub"`, `"dvclive"`, `"flyte"`,
    `"mlflow"`, `"neptune"`, `"tensorboard"`, and `"wandb"`. Use `"all"` to report
    to all integrations installed, `"none"` for no integrations.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — 报告结果和日志的集成列表。支持的平台有
    `"azure_ml"`、`"clearml"`、`"codecarbon"`、`"comet_ml"`、`"dagshub"`、`"dvclive"`、`"flyte"`、`"mlflow"`、`"neptune"`、`"tensorboard"`
    和 `"wandb"`。使用 `"all"` 报告到所有已安装的集成，使用 `"none"` 不报告到任何集成。'
- en: '`ddp_find_unused_parameters` (`bool`, *optional*) — When using distributed
    training, the value of the flag `find_unused_parameters` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_find_unused_parameters` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel`
    的 `find_unused_parameters` 标志的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。'
- en: '`ddp_bucket_cap_mb` (`int`, *optional*) — When using distributed training,
    the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_bucket_cap_mb` (`int`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel`
    的 `bucket_cap_mb` 标志的值。'
- en: '`ddp_broadcast_buffers` (`bool`, *optional*) — When using distributed training,
    the value of the flag `broadcast_buffers` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_broadcast_buffers` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel`
    的 `broadcast_buffers` 标志的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。'
- en: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) — Whether
    you want to pin memory in data loaders or not. Will default to `True`.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) — 是否要在数据加载器中固定内存。默认为
    `True`。'
- en: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) —
    If True, the data loader will not shut down the worker processes after a dataset
    has been consumed once. This allows to maintain the workers Dataset instances
    alive. Can potentially speed up training, but will increase RAM usage. Will default
    to `False`.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) —
    如果为 True，则数据加载器在数据集被消耗一次后不会关闭工作进程。这允许保持工作进程的数据集实例处于活动状态。可能会加快训练速度，但会增加内存使用量。默认为
    `False`。'
- en: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) — Whether to
    skip adding of memory profiler reports to metrics. This is skipped by default
    because it slows down the training and evaluation speed.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) — 是否跳过将内存分析器报告添加到指标中。默认情况下会跳过这一步，因为它会减慢训练和评估速度。'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push the model to the Hub every time the model is saved. If this is activated,
    `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`)
    and the content will be pushed each time a save is triggered (depending on your
    `save_strategy`). Calling [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    will also trigger a push.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — 是否在每次保存模型时将模型推送到
    Hub。如果激活了此选项，`output_dir` 将开始一个与仓库同步的 git 目录（由 `hub_model_id` 确定），并且每次触发保存时都会推送内容（取决于您的
    `save_strategy`）。调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    也会触发推送。'
- en: If `output_dir` exists, it needs to be a local clone of the repository to which
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will be pushed.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 `output_dir` 存在，则它需要是将 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    将要推送到的仓库的本地克隆。
- en: '`resume_from_checkpoint` (`str`, *optional*) — The path to a folder with a
    valid checkpoint for your model. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_from_checkpoint` (`str`, *optional*) — 您的模型的有效检查点所在文件夹的路径。这个参数不会直接被
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是打算由您的训练/评估脚本使用。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples)
    以获取更多详细信息。'
- en: '`hub_model_id` (`str`, *optional*) — The name of the repository to keep in
    sync with the local *output_dir*. It can be a simple model ID in which case the
    model will be pushed in your namespace. Otherwise it should be the whole repository
    name, for instance `"user_name/model"`, which allows you to push to an organization
    you are a member of with `"organization_name/model"`. Will default to `user_name/output_dir_name`
    with *output_dir_name* being the name of `output_dir`.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_model_id` (`str`, *optional*) — 要与本地 *output_dir* 保持同步的仓库名称。它可以是一个简单的模型
    ID，此时模型将被推送到您的命名空间。否则，它应该是整个仓库名称，例如 `"user_name/model"`，这样您就可以推送到您所属的组织，如 `"organization_name/model"`。默认为
    `user_name/output_dir_name`，其中 *output_dir_name* 是 `output_dir` 的名称。'
- en: Will default to the name of `output_dir`.
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认为 `output_dir` 的名称。
- en: '`hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    — Defines the scope of what is pushed to the Hub and when. Possible values are:'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    — 定义推送到 Hub 的范围和时间。可能的值有：'
- en: '`"end"`: push the model, its configuration, the tokenizer (if passed along
    to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card when the [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    method is called.'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"end"`: 当调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    方法时，会推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。'
- en: '`"every_save"`: push the model, its configuration, the tokenizer (if passed
    along to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card each time there is a model save. The pushes are asynchronous
    to not block training, and in case the save are very frequent, a new push is only
    attempted if the previous one is finished. A last push is made with the final
    model at the end of training.'
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"every_save"`: 每次保存模型时，都会推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。推送是异步的，以避免阻塞训练，如果保存非常频繁，则只有在上一个推送完成后才会尝试新的推送。在训练结束时，会使用最终模型进行最后一次推送。'
- en: '`"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed
    in a subfolder named last-checkpoint, allowing you to resume training easily with
    `trainer.train(resume_from_checkpoint="last-checkpoint")`.'
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"checkpoint"`: 类似于 `"every_save"`，但最新的检查点也会被推送到名为 last-checkpoint 的子文件夹中，使您可以通过
    `trainer.train(resume_from_checkpoint="last-checkpoint")` 轻松恢复训练。'
- en: '`"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like
    they appear in the output folder (so you will get one checkpoint folder per folder
    in your final repository)'
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all_checkpoints"`: 类似于 `"checkpoint"`，但所有检查点都像它们出现在输出文件夹中一样被推送（因此您将在最终存储库中获得一个检查点文件夹）。'
- en: '`hub_token` (`str`, *optional*) — The token to use to push the model to the
    Hub. Will default to the token in the cache folder obtained with `huggingface-cli
    login`.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_token` (`str`, *optional*) — 用于将模型推送到 Hub 的令牌。将默认使用通过 `huggingface-cli
    login` 获取的缓存文件夹中的令牌。'
- en: '`hub_private_repo` (`bool`, *optional*, defaults to `False`) — If True, the
    Hub repo will be set to private.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_private_repo` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则 Hub 存储库将设置为私有。'
- en: '`hub_always_push` (`bool`, *optional*, defaults to `False`) — Unless this is
    `True`, the `Trainer` will skip pushing a checkpoint when the previous push is
    not finished.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_always_push` (`bool`, *optional*, 默认为 `False`) — 除非为 `True`，否则当上一个推送未完成时，`Trainer`
    将跳过推送检查点。'
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `False`) — If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则使用梯度检查点来节省内存，但会导致反向传播速度变慢。'
- en: '`gradient_checkpointing_kwargs` (`dict`, *optional*, defaults to `None`) —
    Key word arguments to be passed to the `gradient_checkpointing_enable` method.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing_kwargs` (`dict`, *optional*, 默认为 `None`) — 要传递给 `gradient_checkpointing_enable`
    方法的关键字参数。'
- en: '`include_inputs_for_metrics` (`bool`, *optional*, defaults to `False`) — Whether
    or not the inputs will be passed to the `compute_metrics` function. This is intended
    for metrics that need inputs, predictions and references for scoring calculation
    in Metric class.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_inputs_for_metrics` (`bool`, *optional*, 默认为 `False`) — 是否将输入传递给 `compute_metrics`
    函数。这适用于需要输入、预测和参考值进行评分计算的指标类。'
- en: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) — Whether
    to find a batch size that will fit into memory automatically through exponential
    decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed
    (`pip install accelerate`)'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_find_batch_size` (`bool`, *optional*, 默认为 `False`) — 是否通过指数衰减自动找到适合内存的批处理大小，避免
    CUDA 内存不足错误。需要安装 accelerate (`pip install accelerate`)。'
- en: '`full_determinism` (`bool`, *optional*, defaults to `False`) — If `True`, [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    is called instead of [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    to ensure reproducible results in distributed training. Important: this will negatively
    impact the performance, so only use it for debugging.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`full_determinism` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，将调用 [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    而不是 [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    以确保在分布式训练中获得可重现的结果。重要提示：这将对性能产生负面影响，因此仅用于调试目的。'
- en: '`torchdynamo` (`str`, *optional*) — If set, the backend compiler for TorchDynamo.
    Possible choices are `"eager"`, `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`,
    `"aot_cudagraphs"`, `"ofi"`, `"fx2trt"`, `"onnxrt"` and `"ipex"`.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchdynamo` (`str`, *optional*) — 如果设置，TorchDynamo 的后端编译器。可能的选择包括 `"eager"`,
    `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`, `"aot_cudagraphs"`,
    `"ofi"`, `"fx2trt"`, `"onnxrt"` 和 `"ipex"`。'
- en: '`ray_scope` (`str`, *optional*, defaults to `"last"`) — The scope to use when
    doing hyperparameter search with Ray. By default, `"last"` will be used. Ray will
    then use the last checkpoint of all trials, compare those, and select the best
    one. However, other options are also available. See the [Ray documentation](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)
    for more options.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray_scope` (`str`, *optional*, 默认为 `"last"`) — 在使用 Ray 进行超参数搜索时要使用的范围。默认情况下，将使用
    `"last"`。然后，Ray 将使用所有试验的最后一个检查点，比较它们，并选择最佳的一个。但是，也有其他选项可用。有关更多选项，请参阅 [Ray 文档](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)。'
- en: '`ddp_timeout` (`int`, *optional*, defaults to 1800) — The timeout for `torch.distributed.init_process_group`
    calls, used to avoid GPU socket timeouts when performing slow operations in distributed
    runnings. Please refer the [PyTorch documentation] ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))
    for more information.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_timeout` (`int`, *可选*, 默认为1800) — `torch.distributed.init_process_group`调用的超时时间，用于避免在分布式运行中执行缓慢操作时出现GPU套接字超时。请参考[PyTorch文档]
    ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))
    获取更多信息。'
- en: '`use_mps_device` (`bool`, *optional*, defaults to `False`) — This argument
    is deprecated.`mps` device will be used if it is available similar to `cuda` device.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mps_device` (`bool`, *可选*, 默认为 `False`) — 此参数已弃用。如果可用，将使用`mps`设备，类似于`cuda`设备。'
- en: '`torch_compile` (`bool`, *optional*, defaults to `False`) — Whether or not
    to compile the model using PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile` (`bool`, *可选*, 默认为 `False`) — 是否使用 PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/)
    编译模型。'
- en: This will use the best defaults for the [`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).
    You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode`
    but we don’t guarantee any of them will work as the support is progressively rolled
    in in PyTorch.
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将使用[`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile)的最佳默认值。您可以使用参数`torch_compile_backend`和`torch_compile_mode`自定义默认值，但我们不能保证它们中的任何一个会起作用，因为支持逐渐在PyTorch中推出。
- en: This flag and the whole compile API is experimental and subject to change in
    future releases.
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个标志和整个编译API是实验性的，并可能在未来版本中发生变化。
- en: '`torch_compile_backend` (`str`, *optional*) — The backend to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_backend` (`str`, *可选*) — 在`torch.compile`中使用的后端。如果设置为任何值，`torch_compile`将设置为`True`。'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。
- en: This flag is experimental and subject to change in future releases.
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个标志是实验性的，并可能在未来版本中发生变化。
- en: '`torch_compile_mode` (`str`, *optional*) — The mode to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_mode` (`str`, *可选*) — 在`torch.compile`中使用的模式。如果设置为任何值，`torch_compile`将设置为`True`。'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。
- en: This flag is experimental and subject to change in future releases.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个标志是实验性的，并可能在未来版本中发生变化。
- en: '`split_batches` (`bool`, *optional*) — Whether or not the accelerator should
    split the batches yielded by the dataloaders across the devices during distributed
    training. If'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_batches` (`bool`, *可选*) — 是否加速器在分布式训练期间应该将数据加载器产生的批次分配到设备上。如果'
- en: set to `True`, the actual batch size used will be the same on any kind of distributed
    processes, but it must be a
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置为`True`，实际使用的批量大小将在任何类型的分布式进程上相同，但必须是
- en: round multiple of the number of processes you are using (such as GPUs).
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将使用您正在使用的进程数量的倍数（例如GPU）进行四舍五入。
- en: '`include_tokens_per_second` (`bool`, *optional*) — Whether or not to compute
    the number of tokens per second per device for training speed metrics.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_tokens_per_second` (`bool`, *可选*) — 是否计算每个设备每秒的标记数以获取训练速度指标。'
- en: This will iterate over the entire training dataloader once beforehand,
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在事先迭代整个训练数据加载器一次，
- en: and will slow down the entire process.
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并且会减慢整个过程。
- en: '`include_num_input_tokens_seen` (`bool`, *optional*) — Whether or not to track
    the number of input tokens seen throughout training.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_num_input_tokens_seen` (`bool`, *可选*) — 是否跟踪整个训练过程中看到的输入标记数。'
- en: May be slower in distributed training as gather operations must be called.
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在分布式训练中可能会较慢，因为必须调用gather操作。
- en: '`neftune_noise_alpha` (`Optional[float]`) — If not `None`, this will activate
    NEFTune noise embeddings. This can drastically improve model performance for instruction
    fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914)
    and the [original code](https://github.com/neelsjain/NEFTune). Support transformers
    `PreTrainedModel` and also `PeftModel` from peft.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neftune_noise_alpha` (`Optional[float]`) — 如果不是`None`，这将激活NEFTune噪声嵌入。这可以极大地提高指令微调的模型性能。查看[原始论文](https://arxiv.org/abs/2310.05914)和[原始代码](https://github.com/neelsjain/NEFTune)。支持transformers
    `PreTrainedModel`和`PeftModel`。'
- en: TrainingArguments is the subset of the arguments we use in our example scripts
    **which relate to the training loop itself**.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: TrainingArguments是我们在示例脚本中使用的与训练循环本身相关的参数的子集。
- en: Using [HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)
    we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse)
    arguments that can be specified on the command line.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)，我们可以将这个类转换为可以在命令行上指定的[argparse](https://docs.python.org/3/library/argparse#module-argparse)参数。
- en: '#### `get_process_log_level`'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_process_log_level`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2028)'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2028)'
- en: '[PRE43]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Returns the log level to be used depending on whether this process is the main
    process of node 0, main process of node non-0, or a non-main process.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个进程是节点0的主进程、非0节点的主进程还是非主进程，返回要使用的日志级别。
- en: For the main process the log level defaults to the logging level set (`logging.WARNING`
    if you didn’t do anything) unless overridden by `log_level` argument.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主进程，日志级别默认为设置的日志级别（如果您没有做任何操作，则为`logging.WARNING`），除非被`log_level`参数覆盖。
- en: For the replica processes the log level defaults to `logging.WARNING` unless
    overridden by `log_level_replica` argument.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 对于副本进程，默认的日志级别为`logging.WARNING`，除非被`log_level_replica`参数覆盖。
- en: The choice between the main and replica process settings is made according to
    the return value of `should_log`.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`should_log`的返回值来选择主进程和副本进程的设置。
- en: '#### `get_warmup_steps`'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_warmup_steps`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2117)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2117)'
- en: '[PRE44]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Get number of steps used for a linear warmup.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 获取用于线性预热的步数。
- en: '#### `main_process_first`'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `main_process_first`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2066)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2066)'
- en: '[PRE45]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`local` (`bool`, *optional*, defaults to `True`) — if `True` first means process
    of rank 0 of each node if `False` first means process of rank 0 of node rank 0
    In multi-node environment with a shared filesystem you most likely will want to
    use `local=False` so that only the main process of the first node will do the
    processing. If however, the filesystem is not shared, then the main process of
    each node will need to do the processing, which is the default behavior.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local` (`bool`, *optional*, defaults to `True`) — 如果为`True`，则首先处理每个节点的排名为0的进程，如果为`False`，则首先处理排名为0的节点0的进程。在具有共享文件系统的多节点环境中，您很可能会想要使用`local=False`，以便只有第一个节点的主进程会进行处理。但是，如果文件系统不共享，则每个节点的主进程将需要进行处理，这是默认行为。'
- en: '`desc` (`str`, *optional*, defaults to `"work"`) — a work description to be
    used in debug logs'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desc` (`str`, *optional*, defaults to `"work"`) — 用于调试日志中的工作描述'
- en: A context manager for torch distributed environment where on needs to do something
    on the main process, while blocking replicas, and when it’s finished releasing
    the replicas.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 用于torch分布式环境的上下文管理器，在主进程上需要执行某些操作，同时阻塞副本，完成后释放副本。
- en: One such use is for `datasets`’s `map` feature which to be efficient should
    be run once on the main process, which upon completion saves a cached version
    of results and which then automatically gets loaded by the replicas.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种用法是用于`datasets`的`map`功能，为了有效率，应该在主进程上运行一次，完成后保存结果的缓存版本，然后副本会自动加载。
- en: '#### `set_dataloader`'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2629)'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2629)'
- en: '[PRE46]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`drop_last` (`bool`, *optional*, defaults to `False`) — Whether to drop the
    last incomplete batch (if the length of the dataset is not divisible by the batch
    size) or not.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_last` (`bool`, *optional*, defaults to `False`) — 是否丢弃最后一个不完整的批次（如果数据集的长度不可被批次大小整除）。'
- en: '`num_workers` (`int`, *optional*, defaults to 0) — Number of subprocesses to
    use for data loading (PyTorch only). 0 means that the data will be loaded in the
    main process.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, defaults to 0) — 用于数据加载的子进程数量（仅适用于PyTorch）。0表示数据将在主进程中加载。'
- en: '`pin_memory` (`bool`, *optional*, defaults to `True`) — Whether you want to
    pin memory in data loaders or not. Will default to `True`.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pin_memory` (`bool`, *optional*, defaults to `True`) — 是否要在数据加载器中固定内存。默认为`True`。'
- en: '`persistent_workers` (`bool`, *optional*, defaults to `False`) — If True, the
    data loader will not shut down the worker processes after a dataset has been consumed
    once. This allows to maintain the workers Dataset instances alive. Can potentially
    speed up training, but will increase RAM usage. Will default to `False`.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persistent_workers` (`bool`, *optional*, defaults to `False`) — 如果为True，则数据加载器在数据集被消耗一次后不会关闭工作进程。这允许保持工作进程的数据集实例保持活动状态。可能会加快训练速度，但会增加内存使用量。默认为`False`。'
- en: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) — Whether
    to find a batch size that will fit into memory automatically through exponential
    decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed
    (`pip install accelerate`)'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) — 是否通过指数衰减自动找到适合内存的批次大小，避免CUDA内存不足错误。需要安装accelerate（`pip
    install accelerate`）'
- en: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) — When resuming
    training, whether or not to skip the epochs and batches to get the data loading
    at the same stage as in the previous training. If set to `True`, the training
    will begin faster (as that skipping step can take a long time) but will not yield
    the same results as the interrupted training would have.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) — 在恢复训练时，是否跳过批次和轮次以使数据加载处于与先前训练相同阶段。如果设置为`True`，训练将更快开始（因为跳过步骤可能需要很长时间），但不会产生与中断训练相同的结果。'
- en: '`sampler_seed` (`int`, *optional*) — Random seed to be used with data samplers.
    If not set, random generators for data sampling will use the same seed as `self.seed`.
    This can be used to ensure reproducibility of data sampling, independent of the
    model seed.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampler_seed` (`int`, *optional*) — 用于数据采样器的随机种子。如果未设置，则数据采样的随机生成器将使用与`self.seed`相同的种子。这可用于确保数据采样的可重复性，独立于模型种子。'
- en: A method that regroups all arguments linked to the dataloaders creation.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与数据加载器创建相关的参数重新组合的方法。
- en: 'Example:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE47]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#### `set_evaluate`'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2238)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2238)'
- en: '[PRE48]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) — The evaluation strategy to adopt during training.
    Possible values are:'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` (`str`或[IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) — 训练过程中采用的评估策略。可能的值为：'
- en: '`"no"`: No evaluation is done during training.'
  id: totrans-567
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练过程中不进行评估。'
- en: '`"steps"`: Evaluation is done (and logged) every `steps`.'
  id: totrans-568
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每`steps`进行评估（并记录日志）。'
- en: '`"epoch"`: Evaluation is done at the end of each epoch.'
  id: totrans-569
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 每个时代结束时进行评估。'
- en: Setting a `strategy` different from `"no"` will set `self.do_eval` to `True`.
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置与`"no"`不同的`strategy`将`self.do_eval`设置为`True`。
- en: '`steps` (`int`, *optional*, defaults to 500) — Number of update steps between
    two evaluations if `strategy="steps"`.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` (`int`, *optional*, 默认为500) — 如果`strategy="steps"`，两次评估之间的更新步数。'
- en: '`batch_size` (`int` *optional*, defaults to 8) — The batch size per device
    (GPU/TPU core/CPU…) used for evaluation.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int` *optional*, 默认为8) — 用于评估的每个设备（GPU/TPU核心/CPU...）的批量大小。'
- en: '`accumulation_steps` (`int`, *optional*) — Number of predictions steps to accumulate
    the output tensors for, before moving the results to the CPU. If left unset, the
    whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster
    but requires more memory).'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accumulation_steps` (`int`, *optional*) — 在将结果移动到CPU之前，累积输出张量的预测步数。如果未设置，整个预测将在GPU/TPU上累积后移至CPU（速度更快但需要更多内存）。'
- en: '`delay` (`float`, *optional*) — Number of epochs or steps to wait for before
    the first evaluation can be performed, depending on the evaluation_strategy.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delay` (`float`, *optional*) — 等待进行第一次评估的周期数或步数，取决于评估策略。'
- en: '`loss_only` (`bool`, *optional*, defaults to `False`) — Ignores all outputs
    except the loss.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_only` (`bool`, *optional*, 默认为`False`) — 仅忽略损失以外的所有输出。'
- en: '`jit_mode` (`bool`, *optional*) — Whether or not to use PyTorch jit trace for
    inference.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jit_mode` (`bool`, *optional*) — 是否使用PyTorch jit跟踪进行推断。'
- en: A method that regroups all arguments linked to evaluation.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与评估相关的参数分组的方法。
- en: 'Example:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE49]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `set_logging`'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_logging`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2388)'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2388)'
- en: '[PRE50]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — The logging strategy to adopt during training.
    Possible values are:'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, 默认为`"steps"`) — 训练期间采用的日志记录策略。可能的值有：'
- en: '`"no"`: No save is done during training.'
  id: totrans-585
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练期间不保存。'
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-586
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 在每个周期结束时保存。'
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-587
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每`save_steps`保存一次。'
- en: '`steps` (`int`, *optional*, defaults to 500) — Number of update steps between
    two logs if `strategy="steps"`.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` (`int`, *optional*, 默认为500) — 如果`strategy="steps"`，两次日志记录之间的更新步数。'
- en: '`level` (`str`, *optional*, defaults to `"passive"`) — Logger log level to
    use on the main process. Possible choices are the log levels as strings: `"debug"`,
    `"info"`, `"warning"`, `"error"` and `"critical"`, plus a `"passive"` level which
    doesn’t set anything and lets the application set the level.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level` (`str`, *optional*, 默认为`"passive"`) — 用于主进程的记录器日志级别。可能的选择是字符串形式的日志级别：`"debug"`、`"info"`、`"warning"`、`"error"`和`"critical"`，以及一个不设置任何内容并让应用程序设置级别的`"passive"`级别。'
- en: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — The list
    of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
    `"clearml"`, `"codecarbon"`, `"comet_ml"`, `"dagshub"`, `"dvclive"`, `"flyte"`,
    `"mlflow"`, `"neptune"`, `"tensorboard"`, and `"wandb"`. Use `"all"` to report
    to all integrations installed, `"none"` for no integrations.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to` (`str` 或 `List[str]`, *optional*, 默认为`"all"`) — 报告结果和日志的集成列表。支持的平台有`"azure_ml"`、`"clearml"`、`"codecarbon"`、`"comet_ml"`、`"dagshub"`、`"dvclive"`、`"flyte"`、`"mlflow"`、`"neptune"`、`"tensorboard"`和`"wandb"`。使用`"all"`报告所有已安装的集成，使用`"none"`不报告任何集成。'
- en: '`first_step` (`bool`, *optional*, defaults to `False`) — Whether to log and
    evaluate the first `global_step` or not.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first_step` (`bool`, *optional*, 默认为`False`) — 是否记录和评估第一个`global_step`。'
- en: '`nan_inf_filter` (`bool`, *optional*, defaults to `True`) — Whether to filter
    `nan` and `inf` losses for logging. If set to `True` the loss of every step that
    is `nan` or `inf` is filtered and the average loss of the current logging window
    is taken instead.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nan_inf_filter` (`bool`, *optional*, 默认为`True`) — 是否过滤用于日志记录的`nan`和`inf`损失。如果设置为`True`，则过滤每个步骤的`nan`或`inf`损失，并取代当前日志窗口的平均损失。'
- en: '`nan_inf_filter` only influences the logging of loss values, it does not change
    the behavior the gradient is computed or applied to the model.'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`nan_inf_filter`仅影响损失值的日志记录，不会改变计算梯度或将梯度应用于模型的行为。'
- en: '`on_each_node` (`bool`, *optional*, defaults to `True`) — In multinode distributed
    training, whether to log using `log_level` once per node, or only on the main
    node.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_each_node` (`bool`, *optional*, 默认为`True`) — 在多节点分布式训练中，是否每个节点仅使用`log_level`一次进行日志记录，或仅在主节点上进行日志记录。'
- en: '`replica_level` (`str`, *optional*, defaults to `"passive"`) — Logger log level
    to use on replicas. Same choices as `log_level`'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replica_level` (`str`, *optional*, 默认为`"passive"`) — 用于副本的记录器日志级别。与`log_level`相同的选择。'
- en: A method that regroups all arguments linked to logging.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与日志记录相关的参数分组的方法。
- en: 'Example:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE51]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `set_lr_scheduler`'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_lr_scheduler`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2584)'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2584)'
- en: '[PRE52]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`name` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) — The scheduler type to use. See the documentation
    of [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    for all possible values.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str` 或 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, 默认为`"linear"`) — 要使用的调度程序类型。查看[SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)的文档以获取所有可能的值。'
- en: '`num_epochs(float,` *optional*, defaults to 3.0) — Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_epochs(float,` *optional*, 默认为3.0) — 要执行的总训练周期数（如果不是整数，则在停止训练之前执行最后一个周期的小数部分百分比）。'
- en: '`max_steps` (`int`, *optional*, defaults to -1) — If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *可选*, 默认为 -1) — 如果设置为正数，则执行的总训练步数。覆盖`num_train_epochs`。对于有限的数据集，如果所有数据都用完，则通过数据集重复训练，直到达到`max_steps`。'
- en: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) — Ratio of total training
    steps used for a linear warmup from 0 to `learning_rate`.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_ratio` (`float`, *可选*, 默认为 0.0) — 用于从0到`learning_rate`进行线性预热的总训练步骤的比率。'
- en: '`warmup_steps` (`int`, *optional*, defaults to 0) — Number of steps used for
    a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`, *可选*, 默认为 0) — 用于从0到`learning_rate`进行线性预热的步骤数。覆盖`warmup_ratio`的任何效果。'
- en: A method that regroups all arguments linked to the learning rate scheduler and
    its hyperparameters.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将所有与学习率调度器及其超参数相关联的参数重新分组的方法。
- en: 'Example:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE53]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '#### `set_optimizer`'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_optimizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2533)'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2533)'
- en: '[PRE54]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`name` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    — The optimizer to use: `"adamw_hf"`, `"adamw_torch"`, `"adamw_torch_fused"`,
    `"adamw_apex_fused"`, `"adamw_anyprecision"` or `"adafactor"`.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str` 或 `training_args.OptimizerNames`, *可选*, 默认为 `"adamw_torch"`)
    — 要使用的优化器："adamw_hf"、"adamw_torch"、"adamw_torch_fused"、"adamw_apex_fused"、"adamw_anyprecision"或"adafactor"。'
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) — The initial learning
    rate.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` (`float`, *可选*, 默认为 5e-5) — 初始学习率。'
- en: '`weight_decay` (`float`, *optional*, defaults to 0) — The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` (`float`, *可选*, 默认为 0) — 应用的权重衰减（如果不为零）到所有层，除了所有偏置和LayerNorm权重。'
- en: '`beta1` (`float`, *optional*, defaults to 0.9) — The beta1 hyperparameter for
    the adam optimizer or its variants.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta1` (`float`, *可选*, 默认为 0.9) — Adam优化器或其变种的beta1超参数。'
- en: '`beta2` (`float`, *optional*, defaults to 0.999) — The beta2 hyperparameter
    for the adam optimizer or its variants.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta2` (`float`, *可选*, 默认为 0.999) — Adam优化器或其变种的beta2超参数。'
- en: '`epsilon` (`float`, *optional*, defaults to 1e-8) — The epsilon hyperparameter
    for the adam optimizer or its variants.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon` (`float`, *可选*, 默认为 1e-8) — Adam优化器或其变种的epsilon超参数。'
- en: '`args` (`str`, *optional*) — Optional arguments that are supplied to AnyPrecisionAdamW
    (only useful when `optim="adamw_anyprecision"`).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`str`, *可选*) — 提供给AnyPrecisionAdamW的可选参数（仅在`optim="adamw_anyprecision"`时有用）。'
- en: A method that regroups all arguments linked to the optimizer and its hyperparameters.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将所有与优化器及其超参数相关联的参数重新分组的方法。
- en: 'Example:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE55]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '#### `set_push_to_hub`'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2463)'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2463)'
- en: '[PRE56]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_id` (`str`) — The name of the repository to keep in sync with the local
    *output_dir*. It can be a simple model ID in which case the model will be pushed
    in your namespace. Otherwise it should be the whole repository name, for instance
    `"user_name/model"`, which allows you to push to an organization you are a member
    of with `"organization_name/model"`.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_id` (`str`) — 与本地*output_dir*同步的存储库的名称。它可以是一个简单的模型ID，此时模型将被推送到您的命名空间。否则，它应该是整个存储库名称，例如`"user_name/model"`，这样您就可以将其推送到您是成员的组织中，例如`"organization_name/model"`。'
- en: '`strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    — Defines the scope of what is pushed to the Hub and when. Possible values are:'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` (`str` 或 `HubStrategy`, *可选*, 默认为 `"every_save"`) — 定义推送到Hub的范围和时间。可能的值为：'
- en: '`"end"`: push the model, its configuration, the tokenizer (if passed along
    to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card when the [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    method is called.'
  id: totrans-631
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"end"`: 当调用[save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)方法时，推送模型、其配置、分词器（如果传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。'
- en: '`"every_save"`: push the model, its configuration, the tokenizer (if passed
    along to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card each time there is a model save. The pushes are asynchronous
    to not block training, and in case the save are very frequent, a new push is only
    attempted if the previous one is finished. A last push is made with the final
    model at the end of training.'
  id: totrans-632
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"every_save"`: 每次保存模型时，推送模型、其配置、分词器（如果传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。推送是异步的，以避免阻塞训练，如果保存非常频繁，则只有在上一个推送完成后才会尝试新的推送。在训练结束时，使用最终模型进行最后一次推送。'
- en: '`"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed
    in a subfolder named last-checkpoint, allowing you to resume training easily with
    `trainer.train(resume_from_checkpoint="last-checkpoint")`.'
  id: totrans-633
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"checkpoint"`: 类似于`"every_save"`，但最新的检查点也被推送到名为last-checkpoint的子文件夹中，这样您可以轻松地使用`trainer.train(resume_from_checkpoint="last-checkpoint")`恢复训练。'
- en: '`"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like
    they appear in the output folder (so you will get one checkpoint folder per folder
    in your final repository)'
  id: totrans-634
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all_checkpoints"`: 类似于`"checkpoint"`，但所有检查点都像它们出现在输出文件夹中一样被推送（因此您将在最终存储库中的每个文件夹中获得一个检查点文件夹）。'
- en: '`token` (`str`, *optional*) — The token to use to push the model to the Hub.
    Will default to the token in the cache folder obtained with `huggingface-cli login`.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str`, *可选*) — 用于将模型推送到Hub的令牌。将默认使用通过`huggingface-cli login`获得的缓存文件夹中的令牌。'
- en: '`private_repo` (`bool`, *optional*, defaults to `False`) — If True, the Hub
    repo will be set to private.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private_repo` (`bool`, *可选*, 默认为 `False`) — 如果为True，则Hub存储库将设置为私有。'
- en: '`always_push` (`bool`, *optional*, defaults to `False`) — Unless this is `True`,
    the `Trainer` will skip pushing a checkpoint when the previous push is not finished.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: always_push（`bool`，*可选*，默认为`False`）— 除非为`True`，否则当上一次推送未完成时，`Trainer`将跳过推送检查点。
- en: A method that regroups all arguments linked to synchronizing checkpoints with
    the Hub.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与与Hub同步检查点相关的参数进行分组的方法。
- en: Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir`
    will begin a git directory synced with the repo (determined by `model_id`) and
    the content will be pushed each time a save is triggered (depending on`self.save_strategy`).
    Calling [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    will also trigger a push.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此方法将设置`self.push_to_hub`为`True`，这意味着`output_dir`将开始一个与存储库同步的git目录（由`model_id`确定），并且每次触发保存时将推送内容（取决于`self.save_strategy`）。调用[save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)也将触发推送。
- en: 'Example:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE57]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `set_save`'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_save`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2339)'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2339)'
- en: '[PRE58]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — The checkpoint save strategy to adopt during
    training. Possible values are:'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: strategy（`str`或[IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)，*可选*，默认为“steps”）—
    训练期间采用的检查点保存策略。可能的值为：
- en: '`"no"`: No save is done during training.'
  id: totrans-647
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “no”：在训练期间不进行保存。
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-648
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “epoch”：在每个时代结束时保存。
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-649
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “steps”：每`save_steps`保存一次。
- en: '`steps` (`int`, *optional*, defaults to 500) — Number of updates steps before
    two checkpoint saves if `strategy="steps"`.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps`（`int`，*可选*，默认为500）— 如果`strategy="steps"`，则在两个检查点保存之前的更新步骤数。'
- en: '`total_limit` (`int`, *optional*) — If a value is passed, will limit the total
    amount of checkpoints. Deletes the older checkpoints in `output_dir`.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: total_limit（`int`，*可选*）— 如果传递了一个值，将限制检查点的总量。删除`output_dir`中的旧检查点。
- en: '`on_each_node` (`bool`, *optional*, defaults to `False`) — When doing multi-node
    distributed training, whether to save models and checkpoints on each node, or
    only on the main one.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: on_each_node（`bool`，*可选*，默认为`False`）— 在进行多节点分布式训练时，是否在每个节点上保存模型和检查点，还是仅在主节点上保存。
- en: This should not be activated when the different nodes use the same storage as
    the files will be saved with the same names for each node.
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当不同节点使用相同存储时，不应激活此选项，因为文件将以每个节点相同的名称保存。
- en: A method that regroups all arguments linked to checkpoint saving.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与检查点保存相关的参数进行分组的方法。
- en: 'Example:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE59]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '#### `set_testing`'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_testing`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2299)'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2299)'
- en: '[PRE60]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_size` (`int` *optional*, defaults to 8) — The batch size per device
    (GPU/TPU core/CPU…) used for testing.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_size（`int` *可选*，默认为8）— 用于测试的每个设备（GPU/TPU核心/CPU…）的批量大小。
- en: '`loss_only` (`bool`, *optional*, defaults to `False`) — Ignores all outputs
    except the loss.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: loss_only（`bool`，*可选*，默认为`False`）— 除了损失之外，忽略所有输出。
- en: '`jit_mode` (`bool`, *optional*) — Whether or not to use PyTorch jit trace for
    inference.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: jit_mode（`bool`，*可选*）— 是否使用PyTorch jit跟踪进行推断。
- en: A method that regroups all basic arguments linked to testing on a held-out dataset.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与在保留数据集上进行测试相关的基本参数进行分组的方法。
- en: Calling this method will automatically set `self.do_predict` to `True`.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此方法将自动将`self.do_predict`设置为`True`。
- en: 'Example:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE61]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `set_training`'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_training`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2163)'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2163)'
- en: '[PRE62]'
  id: totrans-670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) — The initial learning
    rate for the optimizer.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learning_rate（`float`，*可选*，默认为5e-5）— 优化器的初始学习率。
- en: '`batch_size` (`int` *optional*, defaults to 8) — The batch size per device
    (GPU/TPU core/CPU…) used for training.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_size（`int` *可选*，默认为8）— 用于训练的每个设备（GPU/TPU核心/CPU…）的批量大小。
- en: '`weight_decay` (`float`, *optional*, defaults to 0) — The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights in the optimizer.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: weight_decay（`float`，*可选*，默认为0）— 应用的权重衰减（如果不为零）到优化器中除所有偏置和LayerNorm权重之外的所有层。
- en: '`num_train_epochs(float,` *optional*, defaults to 3.0) — Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_train_epochs（float，*可选*，默认为3.0）— 要执行的总训练时期数（如果不是整数，则在停止训练之前执行最后一个时期的小数部分百分比）。
- en: '`max_steps` (`int`, *optional*, defaults to -1) — If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: max_steps（`int`，*可选*，默认为-1）— 如果设置为正数，则执行的总训练步数。覆盖`num_train_epochs`。对于有限的数据集，如果所有数据都用完，则通过数据集重复训练，直到达到`max_steps`。
- en: '`gradient_accumulation_steps` (`int`, *optional*, defaults to 1) — Number of
    updates steps to accumulate the gradients for, before performing a backward/update
    pass.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gradient_accumulation_steps（`int`，*可选*，默认为1）— 在执行向后/更新传递之前，累积梯度的更新步骤数。
- en: When using gradient accumulation, one step is counted as one step with backward
    pass. Therefore, logging, evaluation, save will be conducted every `gradient_accumulation_steps
    * xxx_step` training examples.
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用梯度累积时，一个步骤被计为一个带有向后传递的步骤。因此，每`gradient_accumulation_steps * xxx_step`个训练示例将进行日志记录、评估和保存。
- en: '`seed` (`int`, *optional*, defaults to 42) — Random seed that will be set at
    the beginning of training. To ensure reproducibility across runs, use the `~Trainer.model_init`
    function to instantiate the model if it has some randomly initialized parameters.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`, *optional*, 默认为 42) — 将在训练开始时设置的随机种子。为了确保在不同运行之间的可重现性，请使用 `~Trainer.model_init`
    函数来实例化模型，如果模型有一些随机初始化的参数。'
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `False`) — If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则使用梯度检查点来节省内存，但会降低向后传递的速度。'
- en: A method that regroups all basic arguments linked to the training.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有与训练相关的基本参数重新组合的方法。
- en: Calling this method will automatically set `self.do_train` to `True`.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此方法将自动将 `self.do_train` 设置为 `True`。
- en: 'Example:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE63]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '#### `to_dict`'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2126)'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2126)'
- en: '[PRE64]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Serializes this instance while replace `Enum` by their values (for JSON serialization
    support). It obfuscates the token values by removing their value.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 将此实例序列化，同时用它们的值替换 `Enum`（用于 JSON 序列化支持）。通过删除它们的值来混淆令牌值。
- en: '#### `to_json_string`'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_json_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2143)'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2143)'
- en: '[PRE65]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Serializes this instance to a JSON string.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 将此实例序列化为 JSON 字符串。
- en: '#### `to_sanitized_dict`'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_sanitized_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2149)'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2149)'
- en: '[PRE66]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Sanitized serialization to use with TensorBoard’s hparams
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 经过清理的序列化，可与 TensorBoard 的 hparams 一起使用
- en: Seq2SeqTrainingArguments
  id: totrans-697
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2SeqTrainingArguments
- en: '### `class transformers.Seq2SeqTrainingArguments`'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Seq2SeqTrainingArguments`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L28)'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L28)'
- en: '[PRE67]'
  id: totrans-700
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`output_dir` (`str`) — The output directory where the model predictions and
    checkpoints will be written.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir` (`str`) — 模型预测和检查点将被写入的输出目录。'
- en: '`overwrite_output_dir` (`bool`, *optional*, defaults to `False`) — If `True`,
    overwrite the content of the output directory. Use this to continue training if
    `output_dir` points to a checkpoint directory.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overwrite_output_dir` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，则覆盖输出目录的内容。如果
    `output_dir` 指向一个检查点目录，则使用此选项继续训练。'
- en: '`do_train` (`bool`, *optional*, defaults to `False`) — Whether to run training
    or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_train` (`bool`, *optional*, 默认为 `False`) — 是否运行训练。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。'
- en: '`do_eval` (`bool`, *optional*) — Whether to run evaluation on the validation
    set or not. Will be set to `True` if `evaluation_strategy` is different from `"no"`.
    This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_eval` (`bool`, *optional*) — 是否在验证集上运行评估。如果 `evaluation_strategy` 与 `"no"`
    不同，则将设置为 `True`。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。'
- en: '`do_predict` (`bool`, *optional*, defaults to `False`) — Whether to run predictions
    on the test set or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_predict` (`bool`, *optional*, 默认为 `False`) — 是否在测试集上运行预测。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。'
- en: '`evaluation_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) — The evaluation strategy to adopt during training.
    Possible values are:'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluation_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, 默认为 `"no"`) — 训练期间采用的评估策略。可能的值有：'
- en: '`"no"`: No evaluation is done during training.'
  id: totrans-708
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 在训练期间不进行评估。'
- en: '`"steps"`: Evaluation is done (and logged) every `eval_steps`.'
  id: totrans-709
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每 `eval_steps` 进行一次评估（并记录）。'
- en: '`"epoch"`: Evaluation is done at the end of each epoch.'
  id: totrans-710
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 在每个时代结束时进行评估。'
- en: '`prediction_loss_only` (`bool`, *optional*, defaults to `False`) — When performing
    evaluation and generating predictions, only returns the loss.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_loss_only` (`bool`, *optional*, 默认为 `False`) — 在进行评估和生成预测时，仅返回损失。'
- en: '`per_device_train_batch_size` (`int`, *optional*, defaults to 8) — The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for training.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_train_batch_size` (`int`, *optional*, 默认为 8) — 训练时每个 GPU/XPU/TPU/MPS/NPU
    核心/CPU 的批量大小。'
- en: '`per_device_eval_batch_size` (`int`, *optional*, defaults to 8) — The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_eval_batch_size` (`int`, *optional*, 默认为 8) — 评估时每个 GPU/XPU/TPU/MPS/NPU
    核心/CPU 的批量大小。'
- en: '`gradient_accumulation_steps` (`int`, *optional*, defaults to 1) — Number of
    updates steps to accumulate the gradients for, before performing a backward/update
    pass.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` (`int`, *optional*, 默认为 1) — 在执行向后/更新传递之前，累积梯度的更新步骤数。'
- en: When using gradient accumulation, one step is counted as one step with backward
    pass. Therefore, logging, evaluation, save will be conducted every `gradient_accumulation_steps
    * xxx_step` training examples.
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用梯度累积时，一个步骤被计为一个带有反向传播的步骤。因此，每 `gradient_accumulation_steps * xxx_step` 训练示例将进行记录、评估、保存。
- en: '`eval_accumulation_steps` (`int`, *optional*) — Number of predictions steps
    to accumulate the output tensors for, before moving the results to the CPU. If
    left unset, the whole predictions are accumulated on GPU/NPU/TPU before being
    moved to the CPU (faster but requires more memory).'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_accumulation_steps` (`int`, *可选*) — 在将结果移动到 CPU 之前，累积输出张量的预测步数。如果未设置，整个预测将在
    GPU/NPU/TPU 上累积后再移动到 CPU（更快但需要更多内存）。'
- en: '`eval_delay` (`float`, *optional*) — Number of epochs or steps to wait for
    before the first evaluation can be performed, depending on the evaluation_strategy.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_delay` (`float`, *可选*) — 在进行第一次评估之前等待的周期数或步数，具体取决于 evaluation_strategy。'
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) — The initial learning
    rate for [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` (`float`, *可选*, 默认为 5e-5) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的初始学习率。'
- en: '`weight_decay` (`float`, *optional*, defaults to 0) — The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights in [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` (`float`, *可选*, 默认为 0) — 要应用的权重衰减（如果不为零）到所有层，除了 [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器中的所有偏置和 LayerNorm 权重。'
- en: '`adam_beta1` (`float`, *optional*, defaults to 0.9) — The beta1 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta1` (`float`, *可选*, 默认为 0.9) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的 beta1 超参数。'
- en: '`adam_beta2` (`float`, *optional*, defaults to 0.999) — The beta2 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta2` (`float`, *可选*, 默认为 0.999) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的 beta2 超参数。'
- en: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) — The epsilon hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_epsilon` (`float`, *可选*, 默认为 1e-8) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    优化器的 epsilon 超参数。'
- en: '`max_grad_norm` (`float`, *optional*, defaults to 1.0) — Maximum gradient norm
    (for gradient clipping).'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_grad_norm` (`float`, *可选*, 默认为 1.0) — 最大梯度范数（用于梯度裁剪）。'
- en: '`num_train_epochs(float,` *optional*, defaults to 3.0) — Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_epochs(float,` *可选*, 默认为 3.0) — 要执行的总训练周期数（如果不是整数，则在停止训练之前执行最后一个周期的小数部分百分比）。'
- en: '`max_steps` (`int`, *optional*, defaults to -1) — If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *可选*, 默认为 -1) — 如果设置为正数，则执行的总训练步数。覆盖 `num_train_epochs`。对于有限的数据集，如果所有数据都用完，则通过数据集重新进行训练，直到达到
    `max_steps`。'
- en: '`lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) — The scheduler type to use. See the documentation
    of [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    for all possible values.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_type` (`str` 或 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *可选*, 默认为 `"linear"`) — 要使用的调度器类型。查看 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    的文档以获取所有可能的值。'
- en: '`lr_scheduler_kwargs` (‘dict’, *optional*, defaults to {}) — The extra arguments
    for the lr_scheduler. See the documentation of each scheduler for possible values.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_kwargs`（‘dict’, *可选*, 默认为 {}） — lr_scheduler 的额外参数。查看每个调度器的文档以获取可能的值。'
- en: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) — Ratio of total training
    steps used for a linear warmup from 0 to `learning_rate`.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_ratio` (`float`, *可选*, 默认为 0.0) — 用于从 0 到 `learning_rate` 进行线性预热的总训练步数的比率。'
- en: '`warmup_steps` (`int`, *optional*, defaults to 0) — Number of steps used for
    a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`, *可选*, 默认为 0) — 用于从 0 到 `learning_rate` 进行线性预热的步数。覆盖任何
    `warmup_ratio` 的效果。'
- en: '`log_level` (`str`, *optional*, defaults to `passive`) — Logger log level to
    use on the main process. Possible choices are the log levels as strings: ‘debug’,
    ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t
    set anything and keeps the current log level for the Transformers library (which
    will be `"warning"` by default).'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level` (`str`, *可选*, 默认为 `passive`) — 要在主进程上使用的记录器日志级别。可能的选择是字符串形式的日志级别：‘debug’、‘info’、‘warning’、‘error’
    和 ‘critical’，以及一个‘passive’级别，它不设置任何内容并保持 Transformers 库的当前日志级别（默认为 `"warning"`）。'
- en: '`log_level_replica` (`str`, *optional*, defaults to `"warning"`) — Logger log
    level to use on replicas. Same choices as `log_level`”'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level_replica` (`str`, *可选*, 默认为 `"warning"`) — 用于副本的记录器日志级别。与 `log_level`
    相同的选择”'
- en: '`log_on_each_node` (`bool`, *optional*, defaults to `True`) — In multinode
    distributed training, whether to log using `log_level` once per node, or only
    on the main node.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_on_each_node` (`bool`, *可选*, 默认为 `True`) — 在多节点分布式训练中，是否每个节点使用 `log_level`
    进行记录，或仅在主节点上进行记录。'
- en: '`logging_dir` (`str`, *optional*) — [TensorBoard](https://www.tensorflow.org/tensorboard)
    log directory. Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_dir` (`str`, *可选*) — [TensorBoard](https://www.tensorflow.org/tensorboard)
    日志目录。将默认为 *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***。'
- en: '`logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — The logging strategy to adopt during training.
    Possible values are:'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)，*可选*，默认为
    `"steps"`) — 训练过程中采用的日志记录策略。可能的取值有：'
- en: '`"no"`: No logging is done during training.'
  id: totrans-735
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练过程中不进行日志记录。'
- en: '`"epoch"`: Logging is done at the end of each epoch.'
  id: totrans-736
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 每个时代结束时进行日志记录。'
- en: '`"steps"`: Logging is done every `logging_steps`.'
  id: totrans-737
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每 `logging_steps` 步进行日志记录。'
- en: '`logging_first_step` (`bool`, *optional*, defaults to `False`) — Whether to
    log and evaluate the first `global_step` or not.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_first_step` (`bool`，*可选*，默认为 `False`) — 是否记录和评估第一个 `global_step`。'
- en: '`logging_steps` (`int` or `float`, *optional*, defaults to 500) — Number of
    update steps between two logs if `logging_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_steps` (`int` 或 `float`，*可选*，默认为 500) — 如果 `logging_strategy="steps"`，则在两次日志之间的更新步数。应为整数或范围为
    `[0,1)` 的浮点数。如果小于 1，则将被解释为总训练步数的比率。'
- en: '`logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) — Whether
    to filter `nan` and `inf` losses for logging. If set to `True` the loss of every
    step that is `nan` or `inf` is filtered and the average loss of the current logging
    window is taken instead.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` (`bool`，*可选*，默认为 `True`) — 是否过滤用于记录的 `nan` 和 `inf`
    损失。如果设置为 `True`，则会过滤每个步骤的损失值为 `nan` 或 `inf`，并取当前日志窗口的平均损失值。 '
- en: '`logging_nan_inf_filter` only influences the logging of loss values, it does
    not change the behavior the gradient is computed or applied to the model.'
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` 仅影响损失值的记录，不会改变梯度的计算或应用于模型的行为。'
- en: '`save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) — The checkpoint save strategy to adopt during
    training. Possible values are:'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)，*可选*，默认为
    `"steps"`) — 训练过程中采用的检查点保存策略。可能的取值有：'
- en: '`"no"`: No save is done during training.'
  id: totrans-743
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: 训练过程中不进行保存。'
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-744
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: 每个时代结束时保存。'
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-745
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: 每 `save_steps` 步保存一次。'
- en: '`save_steps` (`int` or `float`, *optional*, defaults to 500) — Number of updates
    steps before two checkpoint saves if `save_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps` (`int` 或 `float`，*可选*，默认为 500) — 如果 `save_strategy="steps"`，则在两次检查点保存之间的更新步数。应为整数或范围为
    `[0,1)` 的浮点数。如果小于 1，则将被解释为总训练步数的比率。'
- en: '`save_total_limit` (`int`, *optional*) — If a value is passed, will limit the
    total amount of checkpoints. Deletes the older checkpoints in `output_dir`. When
    `load_best_model_at_end` is enabled, the “best” checkpoint according to `metric_for_best_model`
    will always be retained in addition to the most recent ones. For example, for
    `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will
    always be retained alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`,
    it is possible that two checkpoints are saved: the last one and the best one (if
    they are different).'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_total_limit` (`int`，*可选*) — 如果传递了一个值，将限制检查点的总量。删除 `output_dir` 中的旧检查点。当启用
    `load_best_model_at_end` 时，“最佳”检查点始终会保留，而且还会保留最近的检查点。例如，对于 `save_total_limit=5`
    和 `load_best_model_at_end`，最后四个检查点将始终与最佳模型一起保留。当 `save_total_limit=1` 和 `load_best_model_at_end`
    时，可能保存两个检查点：最后一个和最佳一个（如果它们不同）。'
- en: '`save_safetensors` (`bool`, *optional*, defaults to `True`) — Use [safetensors](https://huggingface.co/docs/safetensors)
    saving and loading for state dicts instead of default `torch.load` and `torch.save`.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_safetensors` (`bool`，*可选*，默认为 `True`) — 使用 [safetensors](https://huggingface.co/docs/safetensors)
    保存和加载状态字典，而不是默认的 `torch.load` 和 `torch.save`。'
- en: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) — When doing
    multi-node distributed training, whether to save models and checkpoints on each
    node, or only on the main one.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_on_each_node` (`bool`，*可选*，默认为 `False`) — 在进行多节点分布式训练时，是否在每个节点上保存模型和检查点，还是仅在主节点上保存。'
- en: This should not be activated when the different nodes use the same storage as
    the files will be saved with the same names for each node.
  id: totrans-750
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当不同节点使用相同存储时，不应激活此选项，因为文件将以相同名称保存在每个节点上。
- en: '`save_only_model` (`bool`, *optional*, defaults to `False`) — When checkpointing,
    whether to only save the model, or also the optimizer, scheduler & rng state.
    Note that when this is true, you won’t be able to resume training from checkpoint.
    This enables you to save storage by not storing the optimizer, scheduler & rng
    state. You can only load the model using `from_pretrained` with this option set
    to `True`.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_only_model` (`bool`，*可选*，默认为 `False`) — 在检查点时，是否仅保存模型，还是同时保存优化器、调度器和
    RNG 状态。请注意，当此选项为真时，您将无法从检查点恢复训练。这样可以通过不存储优化器、调度器和 RNG 状态来节省存储空间。您只能使用 `from_pretrained`
    加载模型，并将此选项设置为 `True`。'
- en: '`use_cpu` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    cpu. If set to False, we will use cuda or mps device if available.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cpu` (`bool`，*可选*，默认为 `False`) — 是否使用 CPU。如果设置为 False，将使用 cuda 或 mps 设备（如果可用）。'
- en: '`seed` (`int`, *optional*, defaults to 42) — Random seed that will be set at
    the beginning of training. To ensure reproducibility across runs, use the `~Trainer.model_init`
    function to instantiate the model if it has some randomly initialized parameters.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`，*可选*，默认为 42) — 在训练开始时设置的随机种子。为了确保跨运行的可重现性，请使用 `~Trainer.model_init`
    函数来实例化模型，如果模型具有一些随机初始化的参数。'
- en: '`data_seed` (`int`, *optional*) — Random seed to be used with data samplers.
    If not set, random generators for data sampling will use the same seed as `seed`.
    This can be used to ensure reproducibility of data sampling, independent of the
    model seed.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_seed` (`int`, *optional*) — 用于数据采样的随机种子。如果未设置，数据采样的随机生成器将使用与`seed`相同的种子。这可用于确保数据采样的可重现性，与模型种子无关。'
- en: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) — Whether or not
    to use PyTorch jit trace for inference.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) — 是否使用PyTorch jit跟踪进行推断。'
- en: '`use_ipex` (`bool`, *optional*, defaults to `False`) — Use Intel extension
    for PyTorch when it is available. [IPEX installation](https://github.com/intel/intel-extension-for-pytorch).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_ipex` (`bool`, *optional*, defaults to `False`) — 在PyTorch可用时使用Intel扩展。[IPEX安装](https://github.com/intel/intel-extension-for-pytorch)。'
- en: '`bf16` (`bool`, *optional*, defaults to `False`) — Whether to use bf16 16-bit
    (mixed) precision training instead of 32-bit training. Requires Ampere or higher
    NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental
    API and it may change.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16` (`bool`, *optional*, defaults to `False`) — 是否使用bf16 16位（混合）精度训练，而不是32位训练。需要安普尔或更高的NVIDIA架构或使用CPU（use_cpu）或Ascend
    NPU。这是一个实验性的API，可能会发生变化。'
- en: '`fp16` (`bool`, *optional*, defaults to `False`) — Whether to use fp16 16-bit
    (mixed) precision training instead of 32-bit training.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16` (`bool`, *optional*, defaults to `False`) — 是否使用fp16 16位（混合）精度训练，而不是32位训练。'
- en: '`fp16_opt_level` (`str`, *optional*, defaults to ‘O1’) — For `fp16` training,
    Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details
    on the [Apex documentation](https://nvidia.github.io/apex/amp).'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_opt_level` (`str`, *optional*, defaults to ‘O1’) — 对于`fp16`训练，选择在[‘O0’,
    ‘O1’, ‘O2’, 和 ‘O3’]中的Apex AMP优化级别。有关详细信息，请参阅[Apex文档](https://nvidia.github.io/apex/amp)。'
- en: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) — This argument is
    deprecated. Use `half_precision_backend` instead.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) — 此参数已弃用。请改用`half_precision_backend`。'
- en: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) — The backend
    to use for mixed precision training. Must be one of `"auto", "apex", "cpu_amp"`.
    `"auto"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected,
    while the other choices will force the requested backend.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) — 用于混合精度训练的后端。必须是`"auto",
    "apex", "cpu_amp"`之一。`"auto"`将根据检测到的PyTorch版本使用CPU/CUDA AMP或APEX，而其他选择将强制使用请求的后端。'
- en: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) — Whether to use
    full bfloat16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values. This is an experimental API and it may change.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的bfloat16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。这是一个实验性的API，可能会发生变化。'
- en: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) — Whether to use
    full float16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的float16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。'
- en: '`tf32` (`bool`, *optional*) — Whether to enable the TF32 mode, available in
    Ampere and newer GPU architectures. The default value depends on PyTorch’s version
    default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer
    to the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation.
    This is an experimental API and it may change.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf32` (`bool`, *optional*) — 是否启用TF32模式，适用于Ampere和更新的GPU架构。默认值取决于PyTorch的版本默认值`torch.backends.cuda.matmul.allow_tf32`。有关更多详细信息，请参阅[TF32](https://huggingface.co/docs/transformers/performance#tf32)文档。这是一个实验性的API，可能会发生变化。'
- en: '`local_rank` (`int`, *optional*, defaults to -1) — Rank of the process during
    distributed training.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_rank` (`int`, *optional*, defaults to -1) — 分布式训练期间进程的排名。'
- en: '`ddp_backend` (`str`, *optional*) — The backend to use for distributed training.
    Must be one of `"nccl"`, `"mpi"`, `"ccl"`, `"gloo"`, `"hccl"`.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_backend` (`str`, *optional*) — 用于分布式训练的后端。必须是`"nccl"`, `"mpi"`, `"ccl"`,
    `"gloo"`, `"hccl"`之一。'
- en: '`tpu_num_cores` (`int`, *optional*) — When training on TPU, the number of TPU
    cores (automatically passed by launcher script).'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tpu_num_cores` (`int`, *optional*) — 在TPU上训练时，TPU核心的数量（由启动脚本自动传递）。'
- en: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) — Whether
    to drop the last incomplete batch (if the length of the dataset is not divisible
    by the batch size) or not.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) — 是否丢弃最后一个不完整的批次（如果数据集的长度不能被批次大小整除）。'
- en: '`eval_steps` (`int` or `float`, *optional*) — Number of update steps between
    two evaluations if `evaluation_strategy="steps"`. Will default to the same value
    as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`.
    If smaller than 1, will be interpreted as ratio of total training steps.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps` (`int` or `float`, *optional*) — 如果`evaluation_strategy="steps"`，则两次评估之间的更新步数。如果未设置，将默认为与`logging_steps`相同的值。应为范围在`[0,1)`的整数或浮点数。如果小于1，将被解释为总训练步数的比率。'
- en: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) — Number of subprocesses
    to use for data loading (PyTorch only). 0 means that the data will be loaded in
    the main process.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) — 用于数据加载的子进程数（仅适用于PyTorch）。0表示数据将在主进程中加载。'
- en: '`past_index` (`int`, *optional*, defaults to -1) — Some models like [TransformerXL](../model_doc/transformerxl)
    or [XLNet](../model_doc/xlnet) can make use of the past hidden states for their
    predictions. If this argument is set to a positive int, the `Trainer` will use
    the corresponding output (usually index 2) as the past state and feed it to the
    model at the next training step under the keyword argument `mems`.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_index` (`int`, *optional*, defaults to -1) — 一些模型（如[TransformerXL](../model_doc/transformerxl)或[XLNet](../model_doc/xlnet)）可以利用过去的隐藏状态进行预测。如果将此参数设置为正整数，则`Trainer`将使用相应的输出（通常为索引2）作为过去状态，并在下一个训练步骤中将其作为关键字参数`mems`提供给模型。'
- en: '`run_name` (`str`, *optional*) — A descriptor for the run. Typically used for
    [wandb](https://www.wandb.com/) and [mlflow](https://www.mlflow.org/) logging.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run_name` (`str`, *optional*) — 运行的描述符。通常用于[wandb](https://www.wandb.com/)和[mlflow](https://www.mlflow.org/)日志记录。'
- en: '`disable_tqdm` (`bool`, *optional*) — Whether or not to disable the tqdm progress
    bars and table of metrics produced by `~notebook.NotebookTrainingTracker` in Jupyter
    Notebooks. Will default to `True` if the logging level is set to warn or lower
    (default), `False` otherwise.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_tqdm` (`bool`, *optional*) — 是否禁用Jupyter笔记本中`~notebook.NotebookTrainingTracker`生成的tqdm进度条和指标表。如果日志级别设置为warn或更低（默认），则默认为`True`，否则为`False`。'
- en: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) — Whether
    or not to automatically remove the columns unused by the model forward method.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) — 是否自动删除模型前向方法未使用的列。'
- en: '`label_names` (`List[str]`, *optional*) — The list of keys in your dictionary
    of inputs that correspond to the labels.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_names` (`List[str]`, *optional*) — 您的输入字典中对应于标签的键列表。'
- en: Will eventually default to the list of argument names accepted by the model
    that contain the word “label”, except if the model used is one of the `XxxForQuestionAnswering`
    in which case it will also include the `["start_positions", "end_positions"]`
    keys.
  id: totrans-776
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终将默认为模型接受的参数名称列表，其中包含单词“label”，除非使用的模型是`XxxForQuestionAnswering`之一，在这种情况下还将包括`["start_positions",
    "end_positions"]`键。
- en: '`load_best_model_at_end` (`bool`, *optional*, defaults to `False`) — Whether
    or not to load the best model found during training at the end of training. When
    this option is enabled, the best checkpoint will always be saved. See [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)
    for more.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_best_model_at_end` (`bool`, *optional*, defaults to `False`) — 是否在训练结束时加载训练过程中找到的最佳模型。启用此选项时，最佳检查点将始终被保存。更多信息请参见[`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)。'
- en: When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`,
    and in the case it is “steps”, `save_steps` must be a round multiple of `eval_steps`.
  id: totrans-778
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置为`True`时，参数`save_strategy`需要与`evaluation_strategy`相同，如果是“steps”，则`save_steps`必须是`eval_steps`的整数倍。
- en: '`metric_for_best_model` (`str`, *optional*) — Use in conjunction with `load_best_model_at_end`
    to specify the metric to use to compare two different models. Must be the name
    of a metric returned by the evaluation with or without the prefix `"eval_"`. Will
    default to `"loss"` if unspecified and `load_best_model_at_end=True` (to use the
    evaluation loss).'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_for_best_model` (`str`, *optional*) — 与`load_best_model_at_end`一起使用，指定用于比较两个不同模型的度量标准。必须是评估返回的度量的名称，带有或不带有前缀`"eval_"`。如果未指定且`load_best_model_at_end=True`（使用评估损失），将默认为`"loss"`。'
- en: If you set this value, `greater_is_better` will default to `True`. Don’t forget
    to set it to `False` if your metric is better when lower.
  id: totrans-780
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果设置了此值，`greater_is_better`将默认为`True`。如果您的度量标准较低时更好，请不要忘记将其设置为`False`。
- en: '`greater_is_better` (`bool`, *optional*) — Use in conjunction with `load_best_model_at_end`
    and `metric_for_best_model` to specify if better models should have a greater
    metric or not. Will default to:'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`greater_is_better` (`bool`, *optional*) — 与`load_best_model_at_end`和`metric_for_best_model`一起使用，指定更好的模型是否应具有更大的度量标准。默认为：'
- en: '`True` if `metric_for_best_model` is set to a value that isn’t `"loss"` or
    `"eval_loss"`.'
  id: totrans-782
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`metric_for_best_model`设置为不是`"loss"`或`"eval_loss"`的值，则为`True`。
- en: '`False` if `metric_for_best_model` is not set, or set to `"loss"` or `"eval_loss"`.'
  id: totrans-783
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果未设置`metric_for_best_model`，或设置为`"loss"`或`"eval_loss"`，则为`False`。
- en: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) — When resuming
    training, whether or not to skip the epochs and batches to get the data loading
    at the same stage as in the previous training. If set to `True`, the training
    will begin faster (as that skipping step can take a long time) but will not yield
    the same results as the interrupted training would have.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) — 恢复训练时，是否跳过批次和轮次以使数据加载与先前训练的阶段相同。如果设置为`True`，训练将更快开始（因为跳过步骤可能需要很长时间），但不会产生与中断训练相同的结果。'
- en: '`fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''''`)
    — Use PyTorch Distributed Parallel Training (in distributed training only).'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''''`)
    — 使用PyTorch分布式并行训练（仅在分布式训练中）。'
- en: 'A list of options along the following:'
  id: totrans-786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下选项列表：
- en: '`"full_shard"`: Shard parameters, gradients and optimizer states.'
  id: totrans-787
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"full_shard"`: 分片参数、梯度和优化器状态。'
- en: '`"shard_grad_op"`: Shard optimizer states and gradients.'
  id: totrans-788
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"shard_grad_op"`: 分片优化器状态和梯度。'
- en: '`"hybrid_shard"`: Apply `FULL_SHARD` within a node, and replicate parameters
    across nodes.'
  id: totrans-789
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard"`: 在节点内应用`FULL_SHARD`，并在节点之间复制参数。'
- en: '`"hybrid_shard_zero2"`: Apply `SHARD_GRAD_OP` within a node, and replicate
    parameters across nodes.'
  id: totrans-790
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard_zero2"`: 在节点内应用`SHARD_GRAD_OP`，并在节点之间复制参数。'
- en: '`"offload"`: Offload parameters and gradients to CPUs (only compatible with
    `"full_shard"` and `"shard_grad_op"`).'
  id: totrans-791
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"offload"`: 将参数和梯度卸载到CPU（仅与`"full_shard"`和`"shard_grad_op"`兼容）。'
- en: '`"auto_wrap"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.'
  id: totrans-792
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"auto_wrap"`: 使用`default_auto_wrap_policy`自动递归包装层与FSDP。'
- en: '`fsdp_config` (`str` or `dict`, *optional*) — Config to be used with fsdp (Pytorch
    Distributed Parallel Training). The value is either a location of fsdp json config
    file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp_config` (`str` or `dict`, *optional*) — 用于fsdp（Pytorch分布式并行训练）的配置。该值可以是fsdp
    json配置文件的位置（例如，`fsdp_config.json`）或已加载的json文件作为`dict`。'
- en: 'A List of config and its options:'
  id: totrans-794
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置及其选项列表：
- en: 'min_num_params (`int`, *optional*, defaults to `0`): FSDP’s minimum number
    of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).'
  id: totrans-795
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'min_num_params (`int`, *optional*, defaults to `0`): FSDP的默认自动包装的参数最小数量。（仅在传递`fsdp`字段时有用）。'
- en: 'transformer_layer_cls_to_wrap (`List[str]`, *optional*): List of transformer
    layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`, `T5Block`
    … (useful only when `fsdp` flag is passed).'
  id: totrans-796
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformer_layer_cls_to_wrap（`List[str]`，*可选*）：要包装的transformer层类名称列表（区分大小写），例如，`BertLayer`，`GPTJBlock`，`T5Block`
    …（仅在传递`fsdp`标志时有用）。
- en: backward_prefetch (`str`, *optional*) FSDP’s backward prefetch mode. Controls
    when to prefetch next set of parameters (useful only when `fsdp` field is passed).
  id: totrans-797
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: backward_prefetch（`str`，*可选*）FSDP的后向预取模式。控制何时预取下一组参数（仅在传递`fsdp`字段时有用）。
- en: 'A list of options along the following:'
  id: totrans-798
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是一系列选项：
- en: '`"backward_pre"` : Prefetches the next set of parameters before the current
    set of parameter’s gradient computation.'
  id: totrans-799
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_pre"`：在当前参数梯度计算之前预取下一组参数。'
- en: '`"backward_post"` : This prefetches the next set of parameters after the current
    set of parameter’s gradient computation.'
  id: totrans-800
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_post"`：在当前参数梯度计算之后预取下一组参数。'
- en: forward_prefetch (`bool`, *optional*, defaults to `False`) FSDP’s forward prefetch
    mode (useful only when `fsdp` field is passed). If `"True"`, then FSDP explicitly
    prefetches the next upcoming all-gather while executing in the forward pass.
  id: totrans-801
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: forward_prefetch（`bool`，*可选*，默认为`False`）FSDP的前向预取模式（仅在传递`fsdp`字段时有用）。如果为`"True"`，则FSDP会在前向传递中显式预取下一个即将到来的all-gather。
- en: limit_all_gathers (`bool`, *optional*, defaults to `False`) FSDP’s limit_all_gathers
    (useful only when `fsdp` field is passed). If `"True"`, FSDP explicitly synchronizes
    the CPU thread to prevent too many in-flight all-gathers.
  id: totrans-802
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: limit_all_gathers（`bool`，*可选*，默认为`False`）FSDP的limit_all_gathers（仅在传递`fsdp`字段时有用）。如果为`"True"`，FSDP会显式同步CPU线程，以防止太多的in-flight
    all-gathers。
- en: use_orig_params (`bool`, *optional*, defaults to `True`) If `"True"`, allows
    non-uniform `requires_grad` during init, which means support for interspersed
    frozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning.
    Please refer this [blog]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
  id: totrans-803
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: use_orig_params（`bool`，*可选*，默认为`True`）如果为`"True"`，允许在初始化期间使用非均匀的`requires_grad`，这意味着支持交替冻结和可训练参数。在参数高效微调等情况下非常有用。请参考此[博客]（[https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
- en: sync_module_states (`bool`, *optional*, defaults to `True`) If `"True"`, each
    individually wrapped FSDP unit will broadcast module parameters from rank 0 to
    ensure they are the same across all ranks after initialization
  id: totrans-804
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: sync_module_states（`bool`，*可选*，默认为`True`）如果为`"True"`，每个单独包装的FSDP单元将从rank 0广播模块参数，以确保在初始化后所有rank中的参数相同
- en: 'activation_checkpointing (`bool`, *optional*, defaults to `False`): If `"True"`,
    activation checkpointing is a technique to reduce memory usage by clearing activations
    of certain layers and recomputing them during a backward pass. Effectively, this
    trades extra computation time for reduced memory usage.'
  id: totrans-805
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: activation_checkpointing（`bool`，*可选*，默认为`False`）：如果为`"True"`，激活检查点是一种通过清除某些层的激活并在反向传递期间重新计算它们来减少内存使用的技术。实际上，这是以额外的计算时间换取减少内存使用。
- en: 'xla (`bool`, *optional*, defaults to `False`): Whether to use PyTorch/XLA Fully
    Sharded Data Parallel Training. This is an experimental feature and its API may
    evolve in the future.'
  id: totrans-806
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla（`bool`，*可选*，默认为`False`）：是否使用PyTorch/XLA完全分片数据并行训练。这是一个实验性功能，其API可能会在未来发生变化。
- en: xla_fsdp_settings (`dict`, *optional*) The value is a dictionary which stores
    the XLA FSDP wrapping parameters.
  id: totrans-807
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_settings（`dict`，*可选*）该值是一个存储XLA FSDP包装参数的字典。
- en: For a complete list of options, please see [here](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).
  id: totrans-808
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关所有选项的完整列表，请参见[此处](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)。
- en: 'xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`): Will use gradient
    checkpointing over each nested XLA FSDP wrapped layer. This setting can only be
    used when the xla flag is set to true, and an auto wrapping policy is specified
    through fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.'
  id: totrans-809
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_grad_ckpt（`bool`，*可选*，默认为`False`）：将在每个嵌套的XLA FSDP包装层上使用梯度检查点。只有在将xla标志设置为true，并通过fsdp_min_num_params或fsdp_transformer_layer_cls_to_wrap指定了自动包装策略时才能使用此设置。
- en: '`deepspeed` (`str` or `dict`, *optional*) — Use [Deepspeed](https://github.com/microsoft/deepspeed).
    This is an experimental feature and its API may evolve in the future. The value
    is either the location of DeepSpeed json config file (e.g., `ds_config.json`)
    or an already loaded json file as a `dict`”'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepspeed`（`str`或`dict`，*可选*）—使用[Deepspeed](https://github.com/microsoft/deepspeed)。这是一个实验性功能，其API可能会在未来发生变化。该值可以是DeepSpeed
    json配置文件的位置（例如，`ds_config.json`）或已加载的json文件作为`dict`”'
- en: '`label_smoothing_factor` (`float`, *optional*, defaults to 0.0) — The label
    smoothing factor to use. Zero means no label smoothing, otherwise the underlying
    onehot-encoded labels are changed from 0s and 1s to `label_smoothing_factor/num_labels`
    and `1 - label_smoothing_factor + label_smoothing_factor/num_labels` respectively.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing_factor`（`float`，*可选*，默认为0.0）—要使用的标签平滑因子。零表示不进行标签平滑，否则基础的onehot编码标签将从0和1更改为`label_smoothing_factor/num_labels`和`1
    - label_smoothing_factor + label_smoothing_factor/num_labels`。'
- en: '`debug` (`str` or list of `DebugOption`, *optional*, defaults to `""`) — Enable
    one or more debug features. This is an experimental feature.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug`（`str`或`DebugOption`列表，*可选*，默认为`""`）—启用一个或多个调试功能。这是一个实验性功能。'
- en: 'Possible options are:'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的选项有：
- en: '`"underflow_overflow"`: detects overflow in model’s input/outputs and reports
    the last frames that led to the event'
  id: totrans-814
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"underflow_overflow"`：检测模型输入/输出中的溢出并报告导致事件的最后帧'
- en: '`"tpu_metrics_debug"`: print debug metrics on TPU'
  id: totrans-815
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"tpu_metrics_debug"`：在TPU上打印调试指标'
- en: The options should be separated by whitespaces.
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选项应该用空格分隔。
- en: '`optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    — The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused,
    adamw_anyprecision or adafactor.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    — 要使用的优化器：adamw_hf、adamw_torch、adamw_torch_fused、adamw_apex_fused、adamw_anyprecision
    或 adafactor。'
- en: '`optim_args` (`str`, *optional*) — Optional arguments that are supplied to
    AnyPrecisionAdamW.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim_args` (`str`, *optional*) — 提供给 AnyPrecisionAdamW 的可选参数。'
- en: '`group_by_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to group together samples of roughly the same length in the training dataset (to
    minimize padding applied and be more efficient). Only useful if applying dynamic
    padding.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_by_length` (`bool`, *optional*, defaults to `False`) — 是否在训练数据集中将大致相同长度的样本分组在一起（以最小化填充并提高效率）。仅在应用动态填充时有用。'
- en: '`length_column_name` (`str`, *optional*, defaults to `"length"`) — Column name
    for precomputed lengths. If the column exists, grouping by length will use these
    values rather than computing them on train startup. Ignored unless `group_by_length`
    is `True` and the dataset is an instance of `Dataset`.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_column_name` (`str`, *optional*, defaults to `"length"`) — 预先计算长度的列名。如果该列存在，则按长度分组将使用这些值而不是在训练启动时计算它们。仅在
    `group_by_length` 为 `True` 且数据集是 `Dataset` 的实例时才会被忽略。'
- en: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — The list
    of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
    `"clearml"`, `"codecarbon"`, `"comet_ml"`, `"dagshub"`, `"dvclive"`, `"flyte"`,
    `"mlflow"`, `"neptune"`, `"tensorboard"`, and `"wandb"`. Use `"all"` to report
    to all integrations installed, `"none"` for no integrations.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — 报告结果和日志的集成列表。支持的平台有
    `"azure_ml"`、`"clearml"`、`"codecarbon"`、`"comet_ml"`、`"dagshub"`、`"dvclive"`、`"flyte"`、`"mlflow"`、`"neptune"`、`"tensorboard"`
    和 `"wandb"`。使用 `"all"` 报告到所有已安装的集成，使用 `"none"` 不报告到任何集成。'
- en: '`ddp_find_unused_parameters` (`bool`, *optional*) — When using distributed
    training, the value of the flag `find_unused_parameters` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_find_unused_parameters` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel`
    的标志 `find_unused_parameters` 的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。'
- en: '`ddp_bucket_cap_mb` (`int`, *optional*) — When using distributed training,
    the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_bucket_cap_mb` (`int`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel`
    的标志 `bucket_cap_mb` 的值。'
- en: '`ddp_broadcast_buffers` (`bool`, *optional*) — When using distributed training,
    the value of the flag `broadcast_buffers` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_broadcast_buffers` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel`
    的标志 `broadcast_buffers` 的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。'
- en: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) — Whether
    you want to pin memory in data loaders or not. Will default to `True`.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) — 是否要在数据加载器中固定内存。默认为
    `True`。'
- en: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) —
    If True, the data loader will not shut down the worker processes after a dataset
    has been consumed once. This allows to maintain the workers Dataset instances
    alive. Can potentially speed up training, but will increase RAM usage. Will default
    to `False`.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) —
    如果为 True，则数据加载器在数据集被消耗一次后不会关闭工作进程。这允许保持工作人员数据集实例的活动状态。可能会加快训练速度，但会增加 RAM 使用量。默认为
    `False`。'
- en: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) — Whether to
    skip adding of memory profiler reports to metrics. This is skipped by default
    because it slows down the training and evaluation speed.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) — 是否跳过将内存分析报告添加到指标中。默认跳过此步骤，因为它会减慢训练和评估速度。'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push the model to the Hub every time the model is saved. If this is activated,
    `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`)
    and the content will be pushed each time a save is triggered (depending on your
    `save_strategy`). Calling [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    will also trigger a push.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — 每次保存模型时是否将模型推送到 Hub。如果激活了此选项，`output_dir`
    将开始一个与存储库同步的 git 目录（由 `hub_model_id` 确定），并且每次触发保存时都会推送内容（取决于您的 `save_strategy`）。调用
    [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    也会触发推送。'
- en: If `output_dir` exists, it needs to be a local clone of the repository to which
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will be pushed.
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 `output_dir` 存在，则需要是将 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    推送到的存储库的本地克隆。
- en: '`resume_from_checkpoint` (`str`, *optional*) — The path to a folder with a
    valid checkpoint for your model. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    it’s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_from_checkpoint` (`str`, *optional*) — 您的模型的有效检查点所在文件夹的路径。此参数不会直接被
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。'
- en: '`hub_model_id` (`str`, *optional*) — The name of the repository to keep in
    sync with the local *output_dir*. It can be a simple model ID in which case the
    model will be pushed in your namespace. Otherwise it should be the whole repository
    name, for instance `"user_name/model"`, which allows you to push to an organization
    you are a member of with `"organization_name/model"`. Will default to `user_name/output_dir_name`
    with *output_dir_name* being the name of `output_dir`.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_model_id` (`str`, *optional*) — 与本地 *output_dir* 同步的存储库的名称。它可以是一个简单的模型
    ID，此时模型将被推送到您的命名空间。否则，它应该是整个存储库名称，例如 `"user_name/model"`，这样您就可以推送到您所属的组织，例如 `"organization_name/model"`。将默认为
    `user_name/output_dir_name`，其中 *output_dir_name* 是 `output_dir` 的名称。'
- en: Will default to the name of `output_dir`.
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将默认为 `output_dir` 的名称。
- en: '`hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    — Defines the scope of what is pushed to the Hub and when. Possible values are:'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_strategy` (`str` 或 `HubStrategy`, *optional*, 默认为 `"every_save"`) — 定义推送到
    Hub 的范围和时间。可能的值有：'
- en: '`"end"`: push the model, its configuration, the tokenizer (if passed along
    to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card when the [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    method is called.'
  id: totrans-834
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"end"`: 当调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    方法时，推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡片的草稿。'
- en: '`"every_save"`: push the model, its configuration, the tokenizer (if passed
    along to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card each time there is a model save. The pushes are asynchronous
    to not block training, and in case the save are very frequent, a new push is only
    attempted if the previous one is finished. A last push is made with the final
    model at the end of training.'
  id: totrans-835
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"every_save"`: 每次保存模型时，推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡片的草稿。推送是异步的，以避免阻塞训练，如果保存非常频繁，则只有在上一个推送完成后才会尝试新的推送。在训练结束时，使用最终模型进行最后一次推送。'
- en: '`"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed
    in a subfolder named last-checkpoint, allowing you to resume training easily with
    `trainer.train(resume_from_checkpoint="last-checkpoint")`.'
  id: totrans-836
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"checkpoint"`: 类似于 `"every_save"`，但最新的检查点也会被推送到名为 last-checkpoint 的子文件夹中，这样您可以轻松地使用
    `trainer.train(resume_from_checkpoint="last-checkpoint")` 恢复训练。'
- en: '`"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like
    they appear in the output folder (so you will get one checkpoint folder per folder
    in your final repository)'
  id: totrans-837
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all_checkpoints"`: 类似于 `"checkpoint"`，但所有检查点都像它们出现在输出文件夹中一样被推送（因此您将在最终存储库中获得一个检查点文件夹）。'
- en: '`hub_token` (`str`, *optional*) — The token to use to push the model to the
    Hub. Will default to the token in the cache folder obtained with `huggingface-cli
    login`.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_token` (`str`, *optional*) — 用于将模型推送到 Hub 的令牌。将默认为使用 `huggingface-cli
    login` 获取的缓存文件夹中的令牌。'
- en: '`hub_private_repo` (`bool`, *optional*, defaults to `False`) — If True, the
    Hub repo will be set to private.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_private_repo` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则 Hub 存储库将设置为私有。'
- en: '`hub_always_push` (`bool`, *optional*, defaults to `False`) — Unless this is
    `True`, the `Trainer` will skip pushing a checkpoint when the previous push is
    not finished.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_always_push` (`bool`, *optional*, 默认为 `False`) — 除非为 `True`，否则 `Trainer`
    在上一个推送未完成时将跳过推送检查点。'
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `False`) — If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则使用梯度检查点来节省内存，但会导致反向传播速度变慢。'
- en: '`gradient_checkpointing_kwargs` (`dict`, *optional*, defaults to `None`) —
    Key word arguments to be passed to the `gradient_checkpointing_enable` method.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing_kwargs` (`dict`, *optional*, 默认为 `None`) — 要传递给 `gradient_checkpointing_enable`
    方法的关键字参数。'
- en: '`include_inputs_for_metrics` (`bool`, *optional*, defaults to `False`) — Whether
    or not the inputs will be passed to the `compute_metrics` function. This is intended
    for metrics that need inputs, predictions and references for scoring calculation
    in Metric class.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_inputs_for_metrics` (`bool`, *optional*, 默认为 `False`) — 是否将输入传递给 `compute_metrics`
    函数。这适用于需要输入、预测和参考值进行评分计算的指标类。'
- en: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) — Whether
    to find a batch size that will fit into memory automatically through exponential
    decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed
    (`pip install accelerate`)'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_find_batch_size` (`bool`, *optional*, 默认为 `False`) — 是否通过指数衰减自动找到适合内存的批量大小，避免
    CUDA 内存不足错误。需要安装 accelerate (`pip install accelerate`)。'
- en: '`full_determinism` (`bool`, *optional*, defaults to `False`) — If `True`, [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    is called instead of [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    to ensure reproducible results in distributed training. Important: this will negatively
    impact the performance, so only use it for debugging.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`full_determinism` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，将调用 [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    而不是 [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    来确保在分布式训练中获得可重现的结果。重要提示：这会对性能产生负面影响，因此只能用于调试目的。'
- en: '`torchdynamo` (`str`, *optional*) — If set, the backend compiler for TorchDynamo.
    Possible choices are `"eager"`, `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`,
    `"aot_cudagraphs"`, `"ofi"`, `"fx2trt"`, `"onnxrt"` and `"ipex"`.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchdynamo` (`str`, *optional*) — 如果设置，TorchDynamo 的后端编译器。可能的选择是 `"eager"`,
    `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`, `"aot_cudagraphs"`,
    `"ofi"`, `"fx2trt"`, `"onnxrt"` 和 `"ipex"`。'
- en: '`ray_scope` (`str`, *optional*, defaults to `"last"`) — The scope to use when
    doing hyperparameter search with Ray. By default, `"last"` will be used. Ray will
    then use the last checkpoint of all trials, compare those, and select the best
    one. However, other options are also available. See the [Ray documentation](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)
    for more options.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray_scope`（`str`，*可选*，默认为`"last"`）— 在使用Ray进行超参数搜索时要使用的范围。默认情况下，将使用`"last"`。然后，Ray将使用所有试验的最后一个检查点，进行比较并选择最佳的一个。但也有其他选项可用。查看[Ray文档](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)以获取更多选项。'
- en: '`ddp_timeout` (`int`, *optional*, defaults to 1800) — The timeout for `torch.distributed.init_process_group`
    calls, used to avoid GPU socket timeouts when performing slow operations in distributed
    runnings. Please refer the [PyTorch documentation] ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))
    for more information.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_timeout`（`int`，*可选*，默认为1800）— `torch.distributed.init_process_group`调用的超时时间，用于避免在分布式运行中执行缓慢操作时发生GPU套接字超时。请参考[PyTorch文档]([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))以获取更多信息。'
- en: '`use_mps_device` (`bool`, *optional*, defaults to `False`) — This argument
    is deprecated.`mps` device will be used if it is available similar to `cuda` device.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mps_device`（`bool`，*可选*，默认为`False`）— 此参数已弃用。如果可用，将使用`mps`设备，类似于`cuda`设备。'
- en: '`torch_compile` (`bool`, *optional*, defaults to `False`) — Whether or not
    to compile the model using PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile`（`bool`，*可选*，默认为`False`）— 是否使用PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/)编译模型。'
- en: This will use the best defaults for the [`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).
    You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode`
    but we don’t guarantee any of them will work as the support is progressively rolled
    in in PyTorch.
  id: totrans-851
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将使用[`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile)的最佳默认值。您可以使用参数`torch_compile_backend`和`torch_compile_mode`自定义默认值，但我们不能保证它们中的任何一个会起作用，因为支持逐步在PyTorch中推出。
- en: This flag and the whole compile API is experimental and subject to change in
    future releases.
  id: totrans-852
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此标志和整个编译API是实验性的，可能会在未来的版本中发生变化。
- en: '`torch_compile_backend` (`str`, *optional*) — The backend to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_backend`（`str`，*可选*）— 在`torch.compile`中要使用的后端。如果设置为任何值，`torch_compile`将被设置为`True`。'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-854
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。
- en: This flag is experimental and subject to change in future releases.
  id: totrans-855
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此标志是实验性的，可能会在未来的版本中发生变化。
- en: '`torch_compile_mode` (`str`, *optional*) — The mode to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_mode`（`str`，*可选*）— 在`torch.compile`中要使用的模式。如果设置为任何值，`torch_compile`将被设置为`True`。'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-857
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。
- en: This flag is experimental and subject to change in future releases.
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此标志是实验性的，可能会在未来的版本中发生变化。
- en: '`split_batches` (`bool`, *optional*) — Whether or not the accelerator should
    split the batches yielded by the dataloaders across the devices during distributed
    training. If'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_batches`（`bool`，*可选*）— 在分布式训练期间，加速器是否应该在设备之间分割数据加载器产生的批次。如果'
- en: set to `True`, the actual batch size used will be the same on any kind of distributed
    processes, but it must be a
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置为`True`，实际使用的批量大小将在任何类型的分布式进程上相同，但必须是
- en: round multiple of the number of processes you are using (such as GPUs).
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将多个进程的数量（例如GPU）的倍数四舍五入。
- en: '`include_tokens_per_second` (`bool`, *optional*) — Whether or not to compute
    the number of tokens per second per device for training speed metrics.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_tokens_per_second`（`bool`，*可选*）— 是否计算每个设备每秒的标记数，用于训练速度指标。'
- en: This will iterate over the entire training dataloader once beforehand,
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在训练数据加载器之前迭代整个训练数据加载器一次，
- en: and will slow down the entire process.
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并且会减慢整个过程。
- en: '`include_num_input_tokens_seen` (`bool`, *optional*) — Whether or not to track
    the number of input tokens seen throughout training.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_num_input_tokens_seen`（`bool`，*可选*）— 是否要跟踪整个训练过程中看到的输入标记数量。'
- en: May be slower in distributed training as gather operations must be called.
  id: totrans-866
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在分布式训练中可能会较慢，因为必须调用gather操作。
- en: '`neftune_noise_alpha` (`Optional[float]`) — If not `None`, this will activate
    NEFTune noise embeddings. This can drastically improve model performance for instruction
    fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914)
    and the [original code](https://github.com/neelsjain/NEFTune). Support transformers
    `PreTrainedModel` and also `PeftModel` from peft.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neftune_noise_alpha`（`Optional[float]`）— 如果不是`None`，将激活NEFTune噪声嵌入。这可以极大地提高指导微调的模型性能。查看[原始论文](https://arxiv.org/abs/2310.05914)和[原始代码](https://github.com/neelsjain/NEFTune)。支持transformers的`PreTrainedModel`和peft的`PeftModel`。'
- en: '`sortish_sampler` (`bool`, *optional*, defaults to `False`) — Whether to use
    a *sortish sampler* or not. Only possible if the underlying datasets are *Seq2SeqDataset*
    for now but will become generally available in the near future.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sortish_sampler`（`bool`，*可选*，默认为`False`）— 是否使用*sortish sampler*。目前仅在底层数据集为*Seq2SeqDataset*时才可能，但将在不久的将来普遍可用。'
- en: It sorts the inputs according to lengths in order to minimize the padding size,
    with a bit of randomness for the training set.
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据长度对输入进行排序，以最小化填充大小，并在训练集中加入一些随机性。
- en: '`predict_with_generate` (`bool`, *optional*, defaults to `False`) — Whether
    to use generate to calculate generative metrics (ROUGE, BLEU).'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_with_generate`（`bool`，*可选*，默认为`False`）— 是否使用生成来计算生成指标（ROUGE，BLEU）。'
- en: '`generation_max_length` (`int`, *optional*) — The `max_length` to use on each
    evaluation loop when `predict_with_generate=True`. Will default to the `max_length`
    value of the model configuration.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_max_length` (`int`, *optional*) — 在`predict_with_generate=True`时，在每个评估循环中使用的`max_length`。将默认为模型配置的`max_length`值。'
- en: '`generation_num_beams` (`int`, *optional*) — The `num_beams` to use on each
    evaluation loop when `predict_with_generate=True`. Will default to the `num_beams`
    value of the model configuration.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_num_beams` (`int`, *optional*) — 在`predict_with_generate=True`时，在每个评估循环中使用的`num_beams`。将默认为模型配置的`num_beams`值。'
- en: '`generation_config` (`str` or `Path` or [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig),
    *optional*) — Allows to load a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    from the `from_pretrained` method. This can be either:'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`str`或`Path`或[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig),
    *optional*) — 允许从`from_pretrained`方法加载一个[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。这可以是：'
- en: a string, the *model id* of a pretrained model configuration hosted inside a
    model repo on huggingface.co. Valid model ids can be located at the root-level,
    like `bert-base-uncased`, or namespaced under a user or organization name, like
    `dbmdz/bert-base-german-cased`.
  id: totrans-874
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，预训练模型配置的*模型id*，托管在huggingface.co上的模型存储库内。有效的模型id可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: a path to a *directory* containing a configuration file saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-875
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)方法保存的配置文件，例如，`./my_model_directory/`。
- en: a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    object.
  id: totrans-876
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)对象。
- en: TrainingArguments is the subset of the arguments we use in our example scripts
    **which relate to the training loop itself**.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: TrainingArguments是我们在示例脚本中使用的与训练循环本身相关的参数的子集。
- en: Using [HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)
    we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse)
    arguments that can be specified on the command line.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)，我们可以将这个类转换为可以在命令行上指定的[argparse](https://docs.python.org/3/library/argparse#module-argparse)参数。
- en: '#### `to_dict`'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L87)'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L87)'
- en: '[PRE68]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Serializes this instance while replace `Enum` by their values and `GenerationConfig`
    by dictionaries (for JSON serialization support). It obfuscates the token values
    by removing their value.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 将此实例序列化，将`Enum`替换为它们的值，将`GenerationConfig`替换为字典（用于JSON序列化支持）。通过删除其值来混淆标记值。
