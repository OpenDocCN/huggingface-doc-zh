- en: Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class provides an API for feature-complete training in PyTorch, and it supports
    distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/),
    [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html)
    for PyTorch. [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    goes hand-in-hand with the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    class, which offers a wide range of options to customize how a model is trained.
    Together, these two classes provide a complete training API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ç±»æä¾›äº†ä¸€ä¸ªç”¨äºåœ¨PyTorchä¸­è¿›è¡Œå®Œæ•´ç‰¹å¾è®­ç»ƒçš„APIï¼Œå¹¶æ”¯æŒåœ¨å¤šä¸ªGPU/TPUä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒ[NVIDIA GPUs](https://nvidia.github.io/apex/)çš„æ··åˆç²¾åº¦ï¼Œ[AMD
    GPUs](https://rocm.docs.amd.com/en/latest/rocm.html)ï¼Œä»¥åŠPyTorchçš„[`torch.amp`](https://pytorch.org/docs/stable/amp.html)ã€‚[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä¸[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ç±»ç›¸è¾…ç›¸æˆï¼Œåè€…æä¾›äº†å¹¿æ³›çš„é€‰é¡¹æ¥è‡ªå®šä¹‰æ¨¡å‹çš„è®­ç»ƒæ–¹å¼ã€‚è¿™ä¸¤ä¸ªç±»ä¸€èµ·æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒAPIã€‚'
- en: '[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    and [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)
    inherit from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and `TrainingArgument` classes and theyâ€™re adapted for training models for sequence-to-sequence
    tasks such as summarization or translation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    å’Œ [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)
    ç»§æ‰¿è‡ª[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å’Œ`TrainingArgument`ç±»ï¼Œå®ƒä»¬é€‚ç”¨äºç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦æˆ–ç¿»è¯‘ï¼‰çš„æ¨¡å‹è®­ç»ƒã€‚'
- en: 'The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class is optimized for ğŸ¤— Transformers models and can have surprising behaviors
    when used with other models. When using it with your own model, make sure:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ç±»é’ˆå¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå½“ä¸å…¶ä»–æ¨¡å‹ä¸€èµ·ä½¿ç”¨æ—¶å¯èƒ½ä¼šæœ‰ä¸€äº›æ„å¤–è¡Œä¸ºã€‚åœ¨ä½¿ç”¨è‡ªå·±çš„æ¨¡å‹æ—¶ï¼Œè¯·ç¡®ä¿ï¼š'
- en: your model always return tuples or subclasses of [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨çš„æ¨¡å‹å§‹ç»ˆè¿”å›å…ƒç»„æˆ–[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)çš„å­ç±»
- en: your model can compute the loss if a `labels` argument is provided and that
    loss is returned as the first element of the tuple (if your model returns tuples)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæä¾›äº†`labels`å‚æ•°å¹¶ä¸”è¯¥æŸå¤±ä½œä¸ºå…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ è¿”å›ï¼ˆå¦‚æœæ‚¨çš„æ¨¡å‹è¿”å›å…ƒç»„ï¼‰ï¼Œåˆ™æ‚¨çš„æ¨¡å‹å¯ä»¥è®¡ç®—æŸå¤±
- en: your model can accept multiple label arguments (use `label_names` in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    to indicate their name to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    but none of them should be named `"label"`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨çš„æ¨¡å‹å¯ä»¥æ¥å—å¤šä¸ªæ ‡ç­¾å‚æ•°ï¼ˆåœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­ä½¿ç”¨`label_names`æŒ‡ç¤ºå®ƒä»¬çš„åç§°ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ï¼Œä½†å®ƒä»¬ä¸­æ²¡æœ‰ä¸€ä¸ªåº”è¯¥è¢«å‘½åä¸º`"label"`
- en: Trainer
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨
- en: '### `class transformers.Trainer`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Trainer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L236)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L236)'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or `torch.nn.Module`, *optional*) â€” The model to train, evaluate or use for predictions.
    If not provided, a `model_init` must be passed.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    æˆ– `torch.nn.Module`, *å¯é€‰*) â€” ç”¨äºè®­ç»ƒã€è¯„ä¼°æˆ–ç”¨äºé¢„æµ‹çš„æ¨¡å‹ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å¿…é¡»ä¼ é€’ä¸€ä¸ª`model_init`ã€‚'
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is optimized to work with the [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    provided by the library. You can still use your own models defined as `torch.nn.Module`
    as long as they work the same way as the ğŸ¤— Transformers models.'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    è¢«ä¼˜åŒ–ä¸ºä¸åº“æä¾›çš„[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ä¸€èµ·ä½¿ç”¨ã€‚åªè¦æ‚¨çš„æ¨¡å‹ä¸ğŸ¤—
    Transformersæ¨¡å‹çš„å·¥ä½œæ–¹å¼ç›¸åŒï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨è‡ªå·±å®šä¹‰çš„`torch.nn.Module`æ¨¡å‹ã€‚'
- en: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    *optional*) â€” The arguments to tweak for training. Will default to a basic instance
    of [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    with the `output_dir` set to a directory named *tmp_trainer* in the current directory
    if not provided.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    *å¯é€‰*) â€” ç”¨äºè°ƒæ•´è®­ç»ƒçš„å‚æ•°ã€‚å¦‚æœæœªæä¾›ï¼Œå°†é»˜è®¤ä¸ºä¸€ä¸ªåŸºæœ¬çš„[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å®ä¾‹ï¼Œå…¶ä¸­`output_dir`è®¾ç½®ä¸ºå½“å‰ç›®å½•ä¸­åä¸º*tmp_trainer*çš„ç›®å½•ã€‚'
- en: '`data_collator` (`DataCollator`, *optional*) â€” The function to use to form
    a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default
    to [default_data_collator()](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.default_data_collator)
    if no `tokenizer` is provided, an instance of [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)
    otherwise.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator` (`DataCollator`, *å¯é€‰*) â€” ç”¨äºä»`train_dataset`æˆ–`eval_dataset`çš„å…ƒç´ åˆ—è¡¨ä¸­å½¢æˆæ‰¹æ¬¡çš„å‡½æ•°ã€‚å¦‚æœæœªæä¾›`tokenizer`ï¼Œå°†é»˜è®¤ä¸º[default_data_collator()](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.default_data_collator)ï¼Œå¦åˆ™å°†é»˜è®¤ä¸º[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)çš„å®ä¾‹ã€‚'
- en: '`train_dataset` (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`,
    *optional*) â€” The dataset to use for training. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset`ï¼ˆ`torch.utils.data.Dataset`æˆ–`torch.utils.data.IterableDataset`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚å¦‚æœæ˜¯[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è‡ªåŠ¨åˆ é™¤ã€‚'
- en: Note that if itâ€™s a `torch.utils.data.IterableDataset` with some randomization
    and you are training in a distributed fashion, your iterable dataset should either
    use a internal attribute `generator` that is a `torch.Generator` for the randomization
    that must be identical on all processes (and the Trainer will manually set the
    seed of this `generator` at each epoch) or have a `set_epoch()` method that internally
    sets the seed of the RNGs used.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœæ˜¯å¸¦æœ‰ä¸€äº›éšæœºåŒ–çš„`torch.utils.data.IterableDataset`ï¼Œå¹¶ä¸”æ‚¨æ­£åœ¨ä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæ‚¨çš„å¯è¿­ä»£æ•°æ®é›†åº”è¯¥ä½¿ç”¨ä¸€ä¸ªå†…éƒ¨å±æ€§`generator`ï¼Œè¯¥å±æ€§æ˜¯ä¸€ä¸ª`torch.Generator`ï¼Œç”¨äºåœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šä¿æŒç›¸åŒçš„éšæœºåŒ–ï¼ˆå¹¶ä¸”Trainerå°†åœ¨æ¯ä¸ªepochæ‰‹åŠ¨è®¾ç½®æ­¤`generator`çš„ç§å­ï¼‰ï¼Œæˆ–è€…å…·æœ‰ä¸€ä¸ªåœ¨å†…éƒ¨è®¾ç½®ç”¨äºéšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­çš„`set_epoch()`æ–¹æ³•ã€‚
- en: '`eval_dataset` (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]),
    *optional*) â€” The dataset to use for evaluation. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    If it is a dictionary, it will evaluate on each dataset prepending the dictionary
    key to the metric name.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset`ï¼ˆUnion[`torch.utils.data.Dataset`ï¼ŒDict[strï¼Œ`torch.utils.data.Dataset`]ï¼‰ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºè¯„ä¼°çš„æ•°æ®é›†ã€‚å¦‚æœæ˜¯[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è‡ªåŠ¨åˆ é™¤ã€‚å¦‚æœæ˜¯å­—å…¸ï¼Œåˆ™å°†åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå¹¶åœ¨åº¦é‡åç§°ä¹‹å‰æ·»åŠ å­—å…¸é”®ã€‚'
- en: '`tokenizer` ([PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase),
    *optional*) â€” The tokenizer used to preprocess the data. If provided, will be
    used to automatically pad the inputs to the maximum length when batching inputs,
    and it will be saved along the model to make it easier to rerun an interrupted
    training or reuse the fine-tuned model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¢„å¤„ç†æ•°æ®çš„åˆ†è¯å™¨ã€‚å¦‚æœæä¾›ï¼Œå°†ç”¨äºåœ¨æ‰¹å¤„ç†è¾“å…¥æ—¶è‡ªåŠ¨å¡«å……è¾“å…¥åˆ°æœ€å¤§é•¿åº¦ï¼Œå¹¶å°†ä¿å­˜åœ¨æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿æ›´å®¹æ˜“é‡æ–°è¿è¡Œä¸­æ–­çš„è®­ç»ƒæˆ–é‡ç”¨å¾®è°ƒçš„æ¨¡å‹ã€‚'
- en: '`model_init` (`Callable[[], PreTrainedModel]`, *optional*) â€” A function that
    instantiates the model to be used. If provided, each call to [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    will start from a new instance of the model as given by this function.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_init`ï¼ˆ`Callable[[], PreTrainedModel]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå®ä¾‹åŒ–è¦ä½¿ç”¨çš„æ¨¡å‹çš„å‡½æ•°ã€‚å¦‚æœæä¾›ï¼Œæ¯æ¬¡è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)éƒ½å°†ä»æ­¤å‡½æ•°ç»™å‡ºçš„æ¨¡å‹çš„æ–°å®ä¾‹å¼€å§‹ã€‚'
- en: The function may have zero argument, or a single one containing the optuna/Ray
    Tune/SigOpt trial object, to be able to choose different architectures according
    to hyper parameters (such as layer count, sizes of inner layers, dropout probabilities
    etc).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°å¯èƒ½æ²¡æœ‰å‚æ•°ï¼Œæˆ–è€…åŒ…å«ä¸€ä¸ªå‚æ•°ï¼Œå…¶ä¸­åŒ…å«optuna/Ray Tune/SigOptè¯•éªŒå¯¹è±¡ï¼Œä»¥ä¾¿æ ¹æ®è¶…å‚æ•°ï¼ˆä¾‹å¦‚å±‚è®¡æ•°ã€å†…éƒ¨å±‚å¤§å°ã€ä¸¢å¤±æ¦‚ç‡ç­‰ï¼‰é€‰æ‹©ä¸åŒçš„æ¶æ„ã€‚
- en: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *optional*) â€” The function
    that will be used to compute metrics at evaluation. Must take a [EvalPrediction](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.EvalPrediction)
    and return a dictionary string to metric values.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics`ï¼ˆ`Callable[[EvalPrediction], Dict]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºåœ¨è¯„ä¼°æ—¶è®¡ç®—æŒ‡æ ‡çš„å‡½æ•°ã€‚å¿…é¡»æ¥å—[EvalPrediction](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.EvalPrediction)å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²å­—å…¸ä»¥è¡¨ç¤ºæŒ‡æ ‡å€¼ã€‚'
- en: '`callbacks` (List of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback),
    *optional*) â€” A list of callbacks to customize the training loop. Will add those
    to the list of default callbacks detailed in [here](callback).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks`ï¼ˆ[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯çš„å›è°ƒåˆ—è¡¨ã€‚å°†æ·»åŠ åˆ°[æ­¤å¤„](callback)è¯¦ç»†è¯´æ˜çš„é»˜è®¤å›è°ƒåˆ—è¡¨ä¸­ã€‚'
- en: If you want to remove one of the default callbacks used, use the [Trainer.remove_callback()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.remove_callback)
    method.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦åˆ é™¤ä½¿ç”¨çš„é»˜è®¤å›è°ƒä¹‹ä¸€ï¼Œè¯·ä½¿ç”¨[Trainer.remove_callback()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.remove_callback)æ–¹æ³•ã€‚
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`,
    *optional*, defaults to `(None, None)`) â€” A tuple containing the optimizer and
    the scheduler to use. Will default to an instance of [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    on your model and a scheduler given by [get_linear_schedule_with_warmup()](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)
    controlled by `args`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizers`ï¼ˆ`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(None,
    None)`ï¼‰â€” åŒ…å«è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çš„å…ƒç»„ã€‚å°†é»˜è®¤ä¸ºæ‚¨çš„æ¨¡å‹ä¸Šçš„[AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)å®ä¾‹å’Œç”±[get_linear_schedule_with_warmup()](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)æ§åˆ¶çš„è°ƒåº¦å™¨ï¼Œç”±`args`æŒ‡å®šã€‚'
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`,
    *optional*) â€” A function that preprocess the logits right before caching them
    at each evaluation step. Must take two tensors, the logits and the labels, and
    return the logits once processed as desired. The modifications made by this function
    will be reflected in the predictions received by `compute_metrics`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_logits_for_metrics`ï¼ˆ`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºåœ¨æ¯ä¸ªè¯„ä¼°æ­¥éª¤ä¹‹å‰é¢„å¤„ç†logitsçš„å‡½æ•°ã€‚å¿…é¡»æ¥å—ä¸¤ä¸ªå¼ é‡ï¼Œlogitså’Œæ ‡ç­¾ï¼Œå¹¶æ ¹æ®éœ€è¦è¿”å›å¤„ç†åçš„logitsã€‚æ­¤å‡½æ•°æ‰€åšçš„ä¿®æ”¹å°†åæ˜ åœ¨`compute_metrics`æ¥æ”¶åˆ°çš„é¢„æµ‹ä¸­ã€‚'
- en: Note that the labels (second parameter) will be `None` if the dataset does not
    have them.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœæ•°æ®é›†æ²¡æœ‰æ ‡ç­¾ï¼Œæ ‡ç­¾ï¼ˆç¬¬äºŒä¸ªå‚æ•°ï¼‰å°†ä¸º`None`ã€‚
- en: Trainer is a simple but feature-complete training and eval loop for PyTorch,
    optimized for ğŸ¤— Transformers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Traineræ˜¯ä¸€ä¸ªç®€å•ä½†åŠŸèƒ½å®Œå¤‡çš„PyTorchè®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ï¼Œä¸“ä¸ºğŸ¤— Transformersä¼˜åŒ–ã€‚
- en: 'Important attributes:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦å±æ€§ï¼š
- en: '`model` â€” Always points to the core model. If using a transformers model, it
    will be a [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    subclass.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` - å§‹ç»ˆæŒ‡å‘æ ¸å¿ƒæ¨¡å‹ã€‚å¦‚æœä½¿ç”¨transformersæ¨¡å‹ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)å­ç±»ã€‚'
- en: '`model_wrapped` â€” Always points to the most external model in case one or more
    other modules wrap the original model. This is the model that should be used for
    the forward pass. For example, under `DeepSpeed`, the inner model is wrapped in
    `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner
    model hasnâ€™t been wrapped, then `self.model_wrapped` is the same as `self.model`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_wrapped` - å§‹ç»ˆæŒ‡å‘æœ€å¤–éƒ¨çš„æ¨¡å‹ï¼Œä»¥é˜²ä¸€ä¸ªæˆ–å¤šä¸ªå…¶ä»–æ¨¡å—åŒ…è£…åŸå§‹æ¨¡å‹ã€‚è¿™æ˜¯åº”è¯¥ç”¨äºå‰å‘ä¼ é€’çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨`DeepSpeed`ä¸‹ï¼Œå†…éƒ¨æ¨¡å‹è¢«åŒ…è£…åœ¨`DeepSpeed`ä¸­ï¼Œç„¶åå†æ¬¡åŒ…è£…åœ¨`torch.nn.DistributedDataParallel`ä¸­ã€‚å¦‚æœå†…éƒ¨æ¨¡å‹æ²¡æœ‰è¢«åŒ…è£…ï¼Œé‚£ä¹ˆ`self.model_wrapped`ä¸`self.model`ç›¸åŒã€‚'
- en: '`is_model_parallel` â€” Whether or not a model has been switched to a model parallel
    mode (different from data parallelism, this means some of the model layers are
    split on different GPUs).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_model_parallel` - æ¨¡å‹æ˜¯å¦å·²åˆ‡æ¢åˆ°æ¨¡å‹å¹¶è¡Œæ¨¡å¼ï¼ˆä¸æ•°æ®å¹¶è¡Œä¸åŒï¼Œè¿™æ„å‘³ç€ä¸€äº›æ¨¡å‹å±‚åœ¨ä¸åŒçš„GPUä¸Šåˆ†å‰²ï¼‰ã€‚ '
- en: '`place_model_on_device` â€” Whether or not to automatically place the model on
    the device - it will be set to `False` if model parallel or deepspeed is used,
    or if the default `TrainingArguments.place_model_on_device` is overridden to return
    `False` .'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`place_model_on_device` - æ˜¯å¦è‡ªåŠ¨å°†æ¨¡å‹æ”¾ç½®åœ¨è®¾å¤‡ä¸Š - å¦‚æœä½¿ç”¨æ¨¡å‹å¹¶è¡Œæˆ–deepspeedï¼Œæˆ–è€…å¦‚æœé»˜è®¤çš„`TrainingArguments.place_model_on_device`è¢«è¦†ç›–ä¸ºè¿”å›`False`ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º`False`ã€‚'
- en: '`is_in_train` â€” Whether or not a model is currently running `train` (e.g. when
    `evaluate` is called while in `train`)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_in_train` - æ¨¡å‹å½“å‰æ˜¯å¦åœ¨è¿è¡Œ`train`ï¼ˆä¾‹å¦‚ï¼Œåœ¨`train`ä¸­è°ƒç”¨`evaluate`æ—¶ï¼‰'
- en: '#### `add_callback`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_callback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L654)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L654)'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`callback` (`type` or [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    â€” A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    class or an instance of a [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    In the first case, will instantiate a member of that class.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`ï¼ˆ`type`æˆ–[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼‰-
    ä¸€ä¸ª[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ç±»æˆ–[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)çš„å®ä¾‹ã€‚åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œå°†å®ä¾‹åŒ–è¯¥ç±»çš„æˆå‘˜ã€‚'
- en: Add a callback to the current list of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å‘å½“å‰çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªå›è°ƒã€‚
- en: '#### `autocast_smart_context_manager`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `autocast_smart_context_manager`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2734)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2734)'
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A helper wrapper that creates an appropriate context manager for `autocast`
    while feeding it the desired arguments, depending on the situation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸®åŠ©å™¨åŒ…è£…å™¨ï¼Œä¸º`autocast`åˆ›å»ºé€‚å½“çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå¹¶æ ¹æ®æƒ…å†µæä¾›æ‰€éœ€çš„å‚æ•°ã€‚
- en: '#### `compute_loss`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_loss`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2785)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2785)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How the loss is computed by Trainer. By default, all models return the loss
    in the first element.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Trainerå¦‚ä½•è®¡ç®—æŸå¤±ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨ç¬¬ä¸€ä¸ªå…ƒç´ ä¸­è¿”å›æŸå¤±ã€‚
- en: Subclass and override for custom behavior.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å­ç±»å’Œè¦†ç›–ä»¥è¿›è¡Œè‡ªå®šä¹‰è¡Œä¸ºã€‚
- en: '#### `compute_loss_context_manager`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_loss_context_manager`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2728)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2728)'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A helper wrapper to group together context managers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸®åŠ©å™¨åŒ…è£…å™¨ï¼Œç”¨äºå°†ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç»„åˆåœ¨ä¸€èµ·ã€‚
- en: '#### `create_model_card`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_model_card`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3564)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3564)'
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`language` (`str`, *optional*) â€” The language of the model (if applicable)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹çš„è¯­è¨€ï¼ˆå¦‚æœé€‚ç”¨ï¼‰'
- en: '`license` (`str`, *optional*) â€” The license of the model. Will default to the
    license of the pretrained model used, if the original model given to the `Trainer`
    comes from a repo on the Hub.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`license`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹çš„è®¸å¯è¯ã€‚å¦‚æœåŸå§‹æ¨¡å‹ç»™å®šç»™`Trainer`æ¥è‡ªHubä¸Šçš„repoï¼Œåˆ™å°†é»˜è®¤ä¸ºä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹çš„è®¸å¯è¯ã€‚'
- en: '`tags` (`str` or `List[str]`, *optional*) â€” Some tags to be included in the
    metadata of the model card.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰- è¦åŒ…å«åœ¨æ¨¡å‹å¡å…ƒæ•°æ®ä¸­çš„ä¸€äº›æ ‡ç­¾ã€‚'
- en: '`model_name` (`str`, *optional*) â€” The name of the model.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹çš„åç§°ã€‚'
- en: '`finetuned_from` (`str`, *optional*) â€” The name of the model used to fine-tune
    this one (if applicable). Will default to the name of the repo of the original
    model given to the `Trainer` (if it comes from the Hub).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`finetuned_from`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå¾®è°ƒæ­¤æ¨¡å‹çš„æ¨¡å‹çš„åç§°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚å¦‚æœåŸå§‹æ¨¡å‹ç»™å®šç»™`Trainer`æ¥è‡ªHubï¼Œåˆ™å°†é»˜è®¤ä¸ºåŸå§‹æ¨¡å‹çš„repoçš„åç§°ã€‚'
- en: '`tasks` (`str` or `List[str]`, *optional*) â€” One or several task identifiers,
    to be included in the metadata of the model card.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tasks`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰- ä¸€ä¸ªæˆ–å¤šä¸ªä»»åŠ¡æ ‡è¯†ç¬¦ï¼Œå°†åŒ…å«åœ¨æ¨¡å‹å¡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: '`dataset_tags` (`str` or `List[str]`, *optional*) â€” One or several dataset
    tags, to be included in the metadata of the model card.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_tags`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰- ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†æ ‡ç­¾ï¼Œå°†åŒ…å«åœ¨æ¨¡å‹å¡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: '`dataset` (`str` or `List[str]`, *optional*) â€” One or several dataset identifiers,
    to be included in the metadata of the model card.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰- ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†æ ‡è¯†ç¬¦ï¼Œå°†åŒ…å«åœ¨æ¨¡å‹å¡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: '`dataset_args` (`str` or `List[str]`, *optional*) â€” One or several dataset
    arguments, to be included in the metadata of the model card.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_args` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†å‚æ•°ï¼Œå°†åŒ…å«åœ¨æ¨¡å‹å¡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: Creates a draft of a model card using the information available to the `Trainer`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Trainer`å¯ç”¨çš„ä¿¡æ¯åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¡çš„è‰ç¨¿ã€‚
- en: '#### `create_optimizer`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_optimizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L929)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L929)'
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setup the optimizer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¼˜åŒ–å™¨ã€‚
- en: We provide a reasonable default that works well. If you want to use something
    else, you can pass a tuple in the Trainerâ€™s init through `optimizers`, or subclass
    and override this method in a subclass.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ï¼Œæ•ˆæœå¾ˆå¥½ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å…¶ä»–å†…å®¹ï¼Œå¯ä»¥é€šè¿‡`optimizers`åœ¨Trainerçš„initä¸­ä¼ é€’ä¸€ä¸ªå…ƒç»„ï¼Œæˆ–è€…åœ¨å­ç±»ä¸­é‡å†™æ­¤æ–¹æ³•ã€‚
- en: '#### `create_optimizer_and_scheduler`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L902)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L902)'
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Setup the optimizer and the learning rate scheduler.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
- en: We provide a reasonable default that works well. If you want to use something
    else, you can pass a tuple in the Trainerâ€™s init through `optimizers`, or subclass
    and override this method (or `create_optimizer` and/or `create_scheduler`) in
    a subclass.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ï¼Œæ•ˆæœå¾ˆå¥½ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å…¶ä»–å†…å®¹ï¼Œå¯ä»¥é€šè¿‡`optimizers`åœ¨Trainerçš„initä¸­ä¼ é€’ä¸€ä¸ªå…ƒç»„ï¼Œæˆ–è€…åœ¨å­ç±»ä¸­é‡å†™æ­¤æ–¹æ³•ï¼ˆæˆ–`create_optimizer`å’Œ/æˆ–`create_scheduler`ï¼‰ã€‚
- en: '#### `create_scheduler`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_scheduler`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1109)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1109)'
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`num_training_steps` (int) â€” The number of training steps to do.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps` (int) â€” è¦æ‰§è¡Œçš„è®­ç»ƒæ­¥éª¤æ•°ã€‚'
- en: Setup the scheduler. The optimizer of the trainer must have been set up either
    before this method is called or passed as an argument.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®è°ƒåº¦å™¨ã€‚è®­ç»ƒå™¨çš„ä¼˜åŒ–å™¨å¿…é¡»åœ¨è°ƒç”¨æ­¤æ–¹æ³•ä¹‹å‰è®¾ç½®å¥½ï¼Œæˆ–è€…ä½œä¸ºå‚æ•°ä¼ é€’ã€‚
- en: '#### `evaluate`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3031)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3031)'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`eval_dataset` (Union[`Dataset`, Dict[str, `Dataset`]), *optional*) â€” Pass
    a dataset if you wish to override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    If it is a dictionary, it will evaluate on each dataset, prepending the dictionary
    key to the metric name. Datasets must implement the `__len__` method.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (Union[`Dataset`, Dict[str, `Dataset`]), *å¯é€‰*) â€” å¦‚æœè¦è¦†ç›–`self.eval_dataset`ï¼Œè¯·ä¼ é€’ä¸€ä¸ªæ•°æ®é›†ã€‚å¦‚æœæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™`model.forward()`æ–¹æ³•ä¸æ¥å—çš„åˆ—å°†è‡ªåŠ¨åˆ é™¤ã€‚å¦‚æœæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå®ƒå°†å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨åº¦é‡åç§°å‰åŠ ä¸Šå­—å…¸é”®ã€‚æ•°æ®é›†å¿…é¡»å®ç°`__len__`æ–¹æ³•ã€‚'
- en: 'If you pass a dictionary with names of datasets as keys and datasets as values,
    evaluate will run separate evaluations on each dataset. This can be useful to
    monitor how training affects other datasets or simply to get a more fine-grained
    evaluation. When used with `load_best_model_at_end`, make sure `metric_for_best_model`
    references exactly one of the datasets. If you, for example, pass in `{"data1":
    data1, "data2": data2}` for two datasets `data1` and `data2`, you could specify
    `metric_for_best_model="eval_data1_loss"` for using the loss on `data1` and `metric_for_best_model="eval_data1_loss"`
    for the loss on `data2`.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å¦‚æœæ‚¨ä¼ é€’ä¸€ä¸ªä»¥æ•°æ®é›†åç§°ä¸ºé”®ã€æ•°æ®é›†ä¸ºå€¼çš„å­—å…¸ï¼Œè¯„ä¼°å°†åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šå•ç‹¬è¿è¡Œã€‚è¿™å¯¹äºç›‘è§†è®­ç»ƒå¦‚ä½•å½±å“å…¶ä»–æ•°æ®é›†æˆ–ä»…ä»…è·å¾—æ›´ç²¾ç»†çš„è¯„ä¼°å¾ˆæœ‰ç”¨ã€‚å½“ä¸`load_best_model_at_end`ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œè¯·ç¡®ä¿`metric_for_best_model`å¼•ç”¨ç¡®åˆ‡åœ°ä¸€ä¸ªæ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸ºä¸¤ä¸ªæ•°æ®é›†`data1`å’Œ`data2`ä¼ é€’`{"data1":
    data1, "data2": data2}`ï¼Œåˆ™å¯ä»¥æŒ‡å®š`metric_for_best_model="eval_data1_loss"`æ¥ä½¿ç”¨`data1`ä¸Šçš„æŸå¤±ï¼Œä»¥åŠ`metric_for_best_model="eval_data1_loss"`æ¥ä½¿ç”¨`data2`ä¸Šçš„æŸå¤±ã€‚'
- en: '`ignore_keys` (`List[str]`, *optional*) â€” A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys` (`List[str]`, *å¯é€‰*) â€” åœ¨æ¨¡å‹è¾“å‡ºä¸­åº”è¯¥è¢«å¿½ç•¥çš„é”®çš„åˆ—è¡¨ï¼ˆå¦‚æœå®ƒæ˜¯ä¸€ä¸ªå­—å…¸ï¼‰ã€‚'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"eval"`) â€” An optional
    prefix to be used as the metrics key prefix. For example the metrics â€œbleuâ€ will
    be named â€œeval_bleuâ€ if the prefix is â€œevalâ€ (default)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"eval"`) â€” ç”¨ä½œæŒ‡æ ‡é”®å‰ç¼€çš„å¯é€‰å‰ç¼€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‰ç¼€æ˜¯â€œevalâ€ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™æŒ‡æ ‡â€œbleuâ€å°†è¢«å‘½åä¸ºâ€œeval_bleuâ€ã€‚'
- en: Run evaluation and returns metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œè¯„ä¼°å¹¶è¿”å›æŒ‡æ ‡ã€‚
- en: The calling script will be responsible for providing a method to compute metrics,
    as they are task-dependent (pass it to the init `compute_metrics` argument).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨è„šæœ¬å°†è´Ÿè´£æä¾›è®¡ç®—æŒ‡æ ‡çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä»»åŠ¡ç›¸å…³çš„ï¼ˆå°†å…¶ä¼ é€’ç»™`compute_metrics`å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼‰ã€‚
- en: You can also subclass and override this method to inject custom behavior.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ä»¥æ³¨å…¥è‡ªå®šä¹‰è¡Œä¸ºã€‚
- en: '#### `evaluation_loop`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluation_loop`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3191)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3191)'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹/è¯„ä¼°å¾ªç¯ï¼Œç”±`Trainer.evaluate()`å’Œ`Trainer.predict()`å…±äº«ã€‚
- en: Works both with or without labels.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨å¸¦æœ‰æˆ–ä¸å¸¦æœ‰æ ‡ç­¾çš„å·¥ä½œã€‚
- en: '#### `floating_point_ops`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `floating_point_ops`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3529)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3529)'
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) â€” The inputs and targets of
    the model.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) â€” æ¨¡å‹çš„è¾“å…¥å’Œç›®æ ‡ã€‚'
- en: Returns
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: The number of floating-point operations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æµ®ç‚¹è¿ç®—çš„æ•°é‡ã€‚
- en: For models that inherit from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    uses that method to compute the number of floating point operations for every
    backward + forward pass. If using another model, either implement such a method
    in the model or subclass and override this method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)çš„æ¨¡å‹ï¼Œä½¿ç”¨è¯¥æ–¹æ³•è®¡ç®—æ¯æ¬¡åå‘+å‰å‘ä¼ é€’çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚å¦‚æœä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹ï¼Œè¦ä¹ˆåœ¨æ¨¡å‹ä¸­å®ç°è¿™æ ·çš„æ–¹æ³•ï¼Œè¦ä¹ˆå­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ã€‚
- en: '#### `get_decay_parameter_names`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_decay_parameter_names`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L918)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L918)'
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Get all parameter names that weight decay will be applied to
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å°†åº”ç”¨æƒé‡è¡°å‡çš„æ‰€æœ‰å‚æ•°åç§°
- en: Note that some models implement their own layernorm instead of calling nn.LayerNorm,
    weight decay could still apply to those modules since this function only filter
    out instance of nn.LayerNorm
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸€äº›æ¨¡å‹å®ç°äº†è‡ªå·±çš„layernormè€Œä¸æ˜¯è°ƒç”¨nn.LayerNormï¼Œå› æ­¤è¿™ä¸ªå‡½æ•°åªè¿‡æ»¤å‡ºnn.LayerNormçš„å®ä¾‹
- en: '#### `get_eval_dataloader`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_eval_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L834)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L834)'
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`eval_dataset` (`torch.utils.data.Dataset`, *optional*) â€” If provided, will
    override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (`torch.utils.data.Dataset`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼Œå°†è¦†ç›–`self.eval_dataset`ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œé‚£äº›`model.forward()`æ–¹æ³•ä¸æ¥å—çš„åˆ—å°†è¢«è‡ªåŠ¨ç§»é™¤ã€‚å®ƒå¿…é¡»å®ç°`__len__`ã€‚'
- en: Returns the evaluation `~torch.utils.data.DataLoader`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›è¯„ä¼°`~torch.utils.data.DataLoader`ã€‚
- en: Subclass and override this method if you want to inject some custom behavior.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦æ³¨å…¥ä¸€äº›è‡ªå®šä¹‰è¡Œä¸ºï¼Œè¯·å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ã€‚
- en: '#### `get_optimizer_cls_and_kwargs`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_optimizer_cls_and_kwargs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L977)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L977)'
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`args` (`transformers.training_args.TrainingArguments`) â€” The training arguments
    for the training session.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`transformers.training_args.TrainingArguments`) â€” è®­ç»ƒä¼šè¯çš„è®­ç»ƒå‚æ•°ã€‚'
- en: Returns the optimizer class and optimizer parameters based on the training arguments.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è®­ç»ƒå‚æ•°è¿”å›ä¼˜åŒ–å™¨ç±»å’Œä¼˜åŒ–å™¨å‚æ•°ã€‚
- en: '#### `get_test_dataloader`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_test_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L869)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L869)'
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`test_dataset` (`torch.utils.data.Dataset`, *optional*) â€” The test dataset
    to use. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dataset` (`torch.utils.data.Dataset`, *å¯é€‰*) â€” è¦ä½¿ç”¨çš„æµ‹è¯•æ•°æ®é›†ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œé‚£äº›`model.forward()`æ–¹æ³•ä¸æ¥å—çš„åˆ—å°†è¢«è‡ªåŠ¨ç§»é™¤ã€‚å®ƒå¿…é¡»å®ç°`__len__`ã€‚'
- en: Returns the test `~torch.utils.data.DataLoader`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æµ‹è¯•`~torch.utils.data.DataLoader`ã€‚
- en: Subclass and override this method if you want to inject some custom behavior.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦æ³¨å…¥ä¸€äº›è‡ªå®šä¹‰è¡Œä¸ºï¼Œè¯·å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ã€‚
- en: '#### `get_train_dataloader`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_train_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L778)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L778)'
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Returns the training `~torch.utils.data.DataLoader`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›è®­ç»ƒ`~torch.utils.data.DataLoader`ã€‚
- en: Will use no sampler if `train_dataset` does not implement `__len__`, a random
    sampler (adapted to distributed training if necessary) otherwise.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`train_dataset`ä¸å®ç°`__len__`ï¼Œåˆ™ä¸ä½¿ç”¨é‡‡æ ·å™¨ï¼Œå¦åˆ™ä½¿ç”¨éšæœºé‡‡æ ·å™¨ï¼ˆå¿…è¦æ—¶é€‚åº”åˆ†å¸ƒå¼è®­ç»ƒï¼‰ã€‚
- en: Subclass and override this method if you want to inject some custom behavior.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦æ³¨å…¥ä¸€äº›è‡ªå®šä¹‰è¡Œä¸ºï¼Œè¯·å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ã€‚
- en: '#### `hyperparameter_search`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `hyperparameter_search`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2596)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2596)'
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hp_space` (`Callable[["optuna.Trial"], Dict[str, float]]`, *optional*) â€” A
    function that defines the hyperparameter search space. Will default to `default_hp_space_optuna()`
    or `default_hp_space_ray()` or `default_hp_space_sigopt()` depending on your backend.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp_space` (`Callable[["optuna.Trial"], Dict[str, float]]`, *å¯é€‰*) â€” å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´çš„å‡½æ•°ã€‚å°†æ ¹æ®æ‚¨çš„åç«¯é»˜è®¤ä¸º`default_hp_space_optuna()`æˆ–`default_hp_space_ray()`æˆ–`default_hp_space_sigopt()`ã€‚'
- en: '`compute_objective` (`Callable[[Dict[str, float]], float]`, *optional*) â€” A
    function computing the objective to minimize or maximize from the metrics returned
    by the `evaluate` method. Will default to `default_compute_objective()`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_objective` (`Callable[[Dict[str, float]], float]`, *å¯é€‰*) â€” ä¸€ä¸ªè®¡ç®—è¦æœ€å°åŒ–æˆ–æœ€å¤§åŒ–çš„ç›®æ ‡çš„å‡½æ•°ï¼Œä»`evaluate`æ–¹æ³•è¿”å›çš„æŒ‡æ ‡ä¸­è®¡ç®—ã€‚å°†é»˜è®¤ä¸º`default_compute_objective()`ã€‚'
- en: '`n_trials` (`int`, *optional*, defaults to 100) â€” The number of trial runs
    to test.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_trials` (`int`, *å¯é€‰*, é»˜è®¤ä¸º100) â€” æµ‹è¯•è¿è¡Œçš„æ¬¡æ•°ã€‚'
- en: '`direction` (`str` or `List[str]`, *optional*, defaults to `"minimize"`) â€”
    If itâ€™s single objective optimization, direction is `str`, can be `"minimize"`
    or `"maximize"`, you should pick `"minimize"` when optimizing the validation loss,
    `"maximize"` when optimizing one or several metrics. If itâ€™s multi objectives
    optimization, direction is `List[str]`, can be List of `"minimize"` and `"maximize"`,
    you should pick `"minimize"` when optimizing the validation loss, `"maximize"`
    when optimizing one or several metrics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`direction` (`str` æˆ– `List[str]`, *å¯é€‰*, é»˜è®¤ä¸º`"minimize"`) â€” å¦‚æœæ˜¯å•ç›®æ ‡ä¼˜åŒ–ï¼Œæ–¹å‘æ˜¯`str`ï¼Œå¯ä»¥æ˜¯`"minimize"`æˆ–`"maximize"`ï¼Œå½“ä¼˜åŒ–éªŒè¯æŸå¤±æ—¶åº”é€‰æ‹©`"minimize"`ï¼Œå½“ä¼˜åŒ–ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‡æ ‡æ—¶åº”é€‰æ‹©`"maximize"`ã€‚å¦‚æœæ˜¯å¤šç›®æ ‡ä¼˜åŒ–ï¼Œæ–¹å‘æ˜¯`List[str]`ï¼Œå¯ä»¥æ˜¯`"minimize"`å’Œ`"maximize"`çš„åˆ—è¡¨ï¼Œå½“ä¼˜åŒ–éªŒè¯æŸå¤±æ—¶åº”é€‰æ‹©`"minimize"`ï¼Œå½“ä¼˜åŒ–ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‡æ ‡æ—¶åº”é€‰æ‹©`"maximize"`ã€‚'
- en: '`backend` (`str` or `~training_utils.HPSearchBackend`, *optional*) â€” The backend
    to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt,
    depending on which one is installed. If all are installed, will default to optuna.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`ï¼ˆ`str`æˆ–`~training_utils.HPSearchBackend`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºè¶…å‚æ•°æœç´¢çš„åç«¯ã€‚å°†é»˜è®¤ä¸ºoptunaã€Ray
    Tuneæˆ–SigOptï¼Œå–å†³äºå®‰è£…äº†å“ªä¸ªã€‚å¦‚æœæ‰€æœ‰éƒ½å®‰è£…äº†ï¼Œå°†é»˜è®¤ä¸ºoptunaã€‚'
- en: '`hp_name` (`Callable[["optuna.Trial"], str]]`, *optional*) â€” A function that
    defines the trial/run name. Will default to None.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp_name`ï¼ˆ`Callable[["optuna.Trial"], str]`ï¼Œ*å¯é€‰*ï¼‰â€”å®šä¹‰è¯•éªŒ/è¿è¡Œåç§°çš„å‡½æ•°ã€‚é»˜è®¤ä¸ºNoneã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional keyword arguments passed
    along to `optuna.create_study` or `ray.tune.run`. For more information see:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¼ é€’ç»™`optuna.create_study`æˆ–`ray.tune.run`çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§ï¼š'
- en: the documentation of [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[optuna.create_studyçš„æ–‡æ¡£](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)'
- en: the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tune.runçš„æ–‡æ¡£](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)'
- en: the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[sigoptçš„æ–‡æ¡£](https://app.sigopt.com/docs/endpoints/experiments/create)'
- en: Returns
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[`trainer_utils.BestRun`æˆ–`List[trainer_utils.BestRun]`]'
- en: All the information about the best run or best runs for multi-objective optimization.
    Experiment summary can be found in `run_summary` attribute for Ray backend.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å¤šç›®æ ‡ä¼˜åŒ–çš„æœ€ä½³è¿è¡Œæˆ–æœ€ä½³è¿è¡Œçš„æ‰€æœ‰ä¿¡æ¯ã€‚å®éªŒæ‘˜è¦å¯ä»¥åœ¨Rayåç«¯çš„`run_summary`å±æ€§ä¸­æ‰¾åˆ°ã€‚
- en: Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The
    optimized quantity is determined by `compute_objective`, which defaults to a function
    returning the evaluation loss when no metric is provided, the sum of all metrics
    otherwise.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`optuna`ã€`Ray Tune`æˆ–`SigOpt`å¯åŠ¨è¶…å‚æ•°æœç´¢ã€‚ä¼˜åŒ–çš„æ•°é‡ç”±`compute_objective`ç¡®å®šï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå½“æ²¡æœ‰æä¾›æŒ‡æ ‡æ—¶è¿”å›è¯„ä¼°æŸå¤±çš„å‡½æ•°ï¼Œå¦åˆ™è¿”å›æ‰€æœ‰æŒ‡æ ‡çš„æ€»å’Œã€‚
- en: 'To use this method, you need to have provided a `model_init` when initializing
    your [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):
    we need to reinitialize the model at each new run. This is incompatible with the
    `optimizers` argument, so you need to subclass [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and override the method [create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)
    for custom optimizer/scheduler.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨éœ€è¦åœ¨åˆå§‹åŒ–[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ—¶æä¾›ä¸€ä¸ª`model_init`ï¼šæˆ‘ä»¬éœ€è¦åœ¨æ¯æ¬¡æ–°è¿è¡Œæ—¶é‡æ–°åˆå§‹åŒ–æ¨¡å‹ã€‚è¿™ä¸`optimizers`å‚æ•°ä¸å…¼å®¹ï¼Œå› æ­¤æ‚¨éœ€è¦å­ç±»åŒ–[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¹¶é‡å†™æ–¹æ³•[create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)ä»¥è·å¾—è‡ªå®šä¹‰ä¼˜åŒ–å™¨/è°ƒåº¦å™¨ã€‚
- en: '#### `init_hf_repo`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `init_hf_repo`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3547)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3547)'
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Initializes a git repo in `self.args.hub_model_id`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`self.args.hub_model_id`ä¸­åˆå§‹åŒ–gitå­˜å‚¨åº“ã€‚
- en: '#### `is_local_process_zero`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `is_local_process_zero`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2822)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2822)'
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Whether or not this process is the local (e.g., on one machine if training in
    a distributed fashion on several machines) main process.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤è¿›ç¨‹æ˜¯å¦ä¸ºæœ¬åœ°ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¤šå°æœºå™¨ä¸Šä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¦‚æœæ˜¯ä¸»è¦è¿›ç¨‹ï¼Œåˆ™ä¸ºä¸€å°æœºå™¨ä¸Šçš„è¿›ç¨‹ï¼‰ã€‚
- en: '#### `is_world_process_zero`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `is_world_process_zero`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2829)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2829)'
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Whether or not this process is the global main process (when training in a distributed
    fashion on several machines, this is only going to be `True` for one process).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤è¿›ç¨‹æ˜¯å¦ä¸ºå…¨å±€ä¸»è¿›ç¨‹ï¼ˆåœ¨å¤šå°æœºå™¨ä¸Šä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œè®­ç»ƒæ—¶ï¼Œåªæœ‰ä¸€ä¸ªè¿›ç¨‹ä¼šæ˜¯`True`ï¼‰ã€‚
- en: '#### `log`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `log`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2675)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2675)'
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`logs` (`Dict[str, float]`) â€” The values to log.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs`ï¼ˆ`Dict[str, float]`ï¼‰â€”è¦è®°å½•çš„å€¼ã€‚'
- en: Log `logs` on the various objects watching training.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å½•`logs`åœ¨è§‚å¯Ÿè®­ç»ƒçš„å„ç§å¯¹è±¡ã€‚
- en: Subclass and override this method to inject custom behavior.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ä»¥æ³¨å…¥è‡ªå®šä¹‰è¡Œä¸ºã€‚
- en: '#### `log_metrics`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `log_metrics`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L905)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L905)'
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`split` (`str`) â€” Mode/split name: one of `train`, `eval`, `test`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split`ï¼ˆ`str`ï¼‰â€”æ¨¡å¼/åˆ†å‰²åç§°ï¼š`train`ã€`eval`ã€`test`ä¹‹ä¸€'
- en: '`metrics` (`Dict[str, float]`) â€” The metrics returned from train/evaluate/predictmetrics:
    metrics dict'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics`ï¼ˆ`Dict[str, float]`ï¼‰â€”æ¥è‡ªtrain/evaluate/predictmetricsçš„æŒ‡æ ‡ï¼šæŒ‡æ ‡å­—å…¸'
- en: Log metrics in a specially formatted way
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ç‰¹æ®Šæ ¼å¼è®°å½•æŒ‡æ ‡
- en: Under distributed environment this is done only for a process with rank 0.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè¿™ä»…é’ˆå¯¹æ’åä¸º0çš„è¿›ç¨‹æ‰§è¡Œã€‚
- en: 'Notes on memory reports:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå†…å­˜æŠ¥å‘Šçš„æ³¨æ„äº‹é¡¹ï¼š
- en: In order to get memory usage report you need to install `psutil`. You can do
    that with `pip install psutil`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—å†…å­˜ä½¿ç”¨æŠ¥å‘Šï¼Œæ‚¨éœ€è¦å®‰è£…`psutil`ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`pip install psutil`æ¥å®‰è£…ã€‚
- en: 'Now when this method is run, you will see a report that will include: :'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å½“è¿è¡Œæ­¤æ–¹æ³•æ—¶ï¼Œæ‚¨å°†çœ‹åˆ°ä¸€ä¸ªåŒ…å«çš„æŠ¥å‘Šï¼šï¼š
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Understanding the reports:**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç†è§£æŠ¥å‘Šï¼š**'
- en: the first segment, e.g., `train__`, tells you which stage the metrics are for.
    Reports starting with `init_` will be added to the first stage that gets run.
    So that if only evaluation is run, the memory usage for the `__init__` will be
    reported along with the `eval_` metrics.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œç¬¬ä¸€éƒ¨åˆ†ï¼Œä¾‹å¦‚`train__`ï¼Œå‘Šè¯‰æ‚¨æŒ‡æ ‡æ‰€å±çš„é˜¶æ®µã€‚ä»¥`init_`å¼€å¤´çš„æŠ¥å‘Šå°†æ·»åŠ åˆ°è¿è¡Œçš„ç¬¬ä¸€ä¸ªé˜¶æ®µã€‚å› æ­¤ï¼Œå¦‚æœåªè¿è¡Œè¯„ä¼°ï¼Œåˆ™å°†æŠ¥å‘Š`__init__`çš„å†…å­˜ä½¿ç”¨æƒ…å†µä»¥åŠ`eval_`æŒ‡æ ‡ã€‚
- en: the third segment, is either `cpu` or `gpu`, tells you whether itâ€™s the general
    RAM or the gpu0 memory metric.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†ï¼Œæ˜¯`cpu`æˆ–`gpu`ï¼Œå‘Šè¯‰æ‚¨å®ƒæ˜¯é€šç”¨RAMè¿˜æ˜¯gpu0å†…å­˜æŒ‡æ ‡ã€‚
- en: '`*_alloc_delta` - is the difference in the used/allocated memory counter between
    the end and the start of the stage - it can be negative if a function released
    more memory than it allocated.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*_alloc_delta` - æ˜¯é˜¶æ®µç»“æŸå’Œå¼€å§‹æ—¶ä½¿ç”¨/åˆ†é…å†…å­˜è®¡æ•°å™¨ä¹‹é—´çš„å·®å¼‚ - å¦‚æœå‡½æ•°é‡Šæ”¾çš„å†…å­˜å¤šäºåˆ†é…çš„å†…å­˜ï¼Œåˆ™å¯èƒ½ä¸ºè´Ÿæ•°ã€‚'
- en: '`*_peaked_delta` - is any extra memory that was consumed and then freed - relative
    to the current allocated memory counter - it is never negative. When you look
    at the metrics of any stage you add up `alloc_delta` + `peaked_delta` and you
    know how much memory was needed to complete that stage.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*_peaked_delta` - æ˜¯ä»»ä½•é¢å¤–æ¶ˆè€—ç„¶åé‡Šæ”¾çš„å†…å­˜ - ç›¸å¯¹äºå½“å‰åˆ†é…çš„å†…å­˜è®¡æ•°å™¨ - å®ƒæ°¸è¿œä¸ä¼šæ˜¯è´Ÿæ•°ã€‚å½“æ‚¨æŸ¥çœ‹ä»»ä½•é˜¶æ®µçš„æŒ‡æ ‡æ—¶ï¼Œæ‚¨å°†`alloc_delta`
    + `peaked_delta`ç›¸åŠ ï¼Œå°±çŸ¥é“å®Œæˆè¯¥é˜¶æ®µéœ€è¦å¤šå°‘å†…å­˜ã€‚'
- en: The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu).
    Typically this is enough since the main process does the bulk of work, but it
    could be not quite so if model parallel is used and then other GPUs may use a
    different amount of gpu memory. This is also not the same under DataParallel where
    gpu0 may require much more memory than the rest since it stores the gradient and
    optimizer states for all participating GPUS. Perhaps in the future these reports
    will evolve to measure those too.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…å¯¹rank 0å’Œgpu 0çš„è¿›ç¨‹è¿›è¡ŒæŠ¥å‘Šï¼ˆå¦‚æœæœ‰gpuï¼‰ã€‚é€šå¸¸è¿™å·²ç»è¶³å¤Ÿäº†ï¼Œå› ä¸ºä¸»è¿›ç¨‹å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œï¼Œä½†å¦‚æœä½¿ç”¨æ¨¡å‹å¹¶è¡Œï¼Œæƒ…å†µå¯èƒ½ä¸å¤ªä¸€æ ·ï¼Œå…¶ä»–GPUå¯èƒ½ä½¿ç”¨ä¸åŒæ•°é‡çš„gpuå†…å­˜ã€‚åœ¨DataParallelä¸‹ä¹Ÿä¸åŒï¼Œå› ä¸ºgpu0å¯èƒ½éœ€è¦æ¯”å…¶ä»–GPUæ›´å¤šçš„å†…å­˜ï¼Œå› ä¸ºå®ƒå­˜å‚¨äº†æ‰€æœ‰å‚ä¸GPUçš„æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚ä¹Ÿè®¸åœ¨æœªæ¥ï¼Œè¿™äº›æŠ¥å‘Šå°†å‘å±•åˆ°æµ‹é‡è¿™äº›å†…å®¹ã€‚
- en: The CPU RAM metric measures RSS (Resident Set Size) includes both the memory
    which is unique to the process and the memory shared with other processes. It
    is important to note that it does not include swapped out memory, so the reports
    could be imprecise.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: CPU RAMæŒ‡æ ‡æµ‹é‡RSSï¼ˆResident Set Sizeï¼‰ï¼ŒåŒ…æ‹¬è¿›ç¨‹ç‹¬æœ‰çš„å†…å­˜å’Œä¸å…¶ä»–è¿›ç¨‹å…±äº«çš„å†…å­˜ã€‚é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œå®ƒä¸åŒ…æ‹¬è¢«äº¤æ¢å‡ºçš„å†…å­˜ï¼Œå› æ­¤æŠ¥å‘Šå¯èƒ½ä¸å¤Ÿç²¾ç¡®ã€‚
- en: The CPU peak memory is measured using a sampling thread. Due to pythonâ€™s GIL
    it may miss some of the peak memory if that thread didnâ€™t get a chance to run
    when the highest memory was used. Therefore this report can be less than reality.
    Using `tracemalloc` would have reported the exact peak memory, but it doesnâ€™t
    report memory allocations outside of python. So if some C++ CUDA extension allocated
    its own memory it wonâ€™t be reported. And therefore it was dropped in favor of
    the memory sampling approach, which reads the current process memory usage.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: CPUå³°å€¼å†…å­˜æ˜¯ä½¿ç”¨é‡‡æ ·çº¿ç¨‹æµ‹é‡çš„ã€‚ç”±äºpythonçš„GILï¼Œå¦‚æœè¯¥çº¿ç¨‹åœ¨ä½¿ç”¨æœ€é«˜å†…å­˜æ—¶æ²¡æœ‰è¿è¡Œçš„æœºä¼šï¼Œå®ƒå¯èƒ½ä¼šé”™è¿‡ä¸€äº›å³°å€¼å†…å­˜ã€‚å› æ­¤ï¼Œè¿™ä»½æŠ¥å‘Šå¯èƒ½å°äºå®é™…æƒ…å†µã€‚ä½¿ç”¨`tracemalloc`å°†æŠ¥å‘Šå‡†ç¡®çš„å³°å€¼å†…å­˜ï¼Œä½†å®ƒä¸ä¼šæŠ¥å‘Špythonä¹‹å¤–çš„å†…å­˜åˆ†é…ã€‚å› æ­¤ï¼Œå¦‚æœæŸä¸ªC++
    CUDAæ‰©å±•åˆ†é…äº†è‡ªå·±çš„å†…å­˜ï¼Œå®ƒå°†ä¸ä¼šè¢«æŠ¥å‘Šã€‚å› æ­¤ï¼Œå®ƒè¢«æ”¾å¼ƒï¼Œä»¥æ”¯æŒå†…å­˜é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¯»å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚
- en: The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()`
    and `torch.cuda.max_memory_allocated()`. This metric reports only â€œdeltasâ€ for
    pytorch-specific allocations, as `torch.cuda` memory management system doesnâ€™t
    track any memory allocated outside of pytorch. For example, the very first cuda
    call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GPUåˆ†é…å’Œå³°å€¼å†…å­˜æŠ¥å‘Šæ˜¯é€šè¿‡`torch.cuda.memory_allocated()`å’Œ`torch.cuda.max_memory_allocated()`å®Œæˆçš„ã€‚è¿™ä¸ªæŒ‡æ ‡ä»…æŠ¥å‘Špytorchç‰¹å®šåˆ†é…çš„â€œå¢é‡â€ï¼Œå› ä¸º`torch.cuda`å†…å­˜ç®¡ç†ç³»ç»Ÿä¸è·Ÿè¸ªpytorchä¹‹å¤–åˆ†é…çš„ä»»ä½•å†…å­˜ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªcudaè°ƒç”¨é€šå¸¸åŠ è½½CUDAå†…æ ¸ï¼Œå¯èƒ½å ç”¨0.5åˆ°2GBçš„GPUå†…å­˜ã€‚
- en: Note that this tracker doesnâ€™t account for memory allocations outside of [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)â€™s
    `__init__`, `train`, `evaluate` and `predict` calls.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ­¤è·Ÿè¸ªå™¨ä¸è€ƒè™‘[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)çš„`__init__`ã€`train`ã€`evaluate`å’Œ`predict`è°ƒç”¨ä¹‹å¤–çš„å†…å­˜åˆ†é…ã€‚
- en: Because `evaluation` calls may happen during `train`, we canâ€™t handle nested
    invocations because `torch.cuda.max_memory_allocated` is a single counter, so
    if it gets reset by a nested eval call, `train`â€™s tracker will report incorrect
    info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266)
    gets resolved it will be possible to change this class to be re-entrant. Until
    then we will only track the outer level of `train`, `evaluate` and `predict` methods.
    Which means that if `eval` is called during `train`, itâ€™s the latter that will
    account for its memory usage and that of the former.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸º`evaluation`è°ƒç”¨å¯èƒ½å‘ç”Ÿåœ¨`train`æœŸé—´ï¼Œæˆ‘ä»¬æ— æ³•å¤„ç†åµŒå¥—è°ƒç”¨ï¼Œå› ä¸º`torch.cuda.max_memory_allocated`æ˜¯ä¸€ä¸ªè®¡æ•°å™¨ï¼Œæ‰€ä»¥å¦‚æœå®ƒè¢«åµŒå¥—çš„evalè°ƒç”¨é‡ç½®ï¼Œ`train`çš„è·Ÿè¸ªå™¨å°†æŠ¥å‘Šä¸æ­£ç¡®çš„ä¿¡æ¯ã€‚å¦‚æœè¿™ä¸ª[pytorché—®é¢˜](https://github.com/pytorch/pytorch/issues/16266)å¾—åˆ°è§£å†³ï¼Œå°†æœ‰å¯èƒ½å°†è¿™ä¸ªç±»æ”¹ä¸ºå¯é‡å…¥ã€‚åœ¨é‚£ä¹‹å‰ï¼Œæˆ‘ä»¬åªä¼šè·Ÿè¸ª`train`ã€`evaluate`å’Œ`predict`æ–¹æ³•çš„å¤–å±‚çº§åˆ«ã€‚è¿™æ„å‘³ç€å¦‚æœåœ¨`train`æœŸé—´è°ƒç”¨`eval`ï¼Œåè€…å°†è®°å½•å…¶å†…å­˜ä½¿ç”¨æƒ…å†µä»¥åŠå‰è€…çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚
- en: This also means that if any other tool that is used along the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    calls `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be
    invalid. And the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will disrupt the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats`
    themselves.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæ„å‘³ç€å¦‚æœåœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)è°ƒç”¨æœŸé—´ä½¿ç”¨ä»»ä½•å…¶ä»–å·¥å…·`torch.cuda.reset_peak_memory_stats`ï¼Œåˆ™gpuå³°å€¼å†…å­˜ç»Ÿè®¡æ•°æ®å¯èƒ½æ— æ•ˆã€‚è€Œä¸”[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†ç ´åä»»ä½•ä¾èµ–äºè‡ªå·±è°ƒç”¨`torch.cuda.reset_peak_memory_stats`çš„å·¥å…·çš„æ­£å¸¸è¡Œä¸ºã€‚
- en: For best performance you may want to consider turning the memory profiling off
    for production runs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨ç”Ÿäº§è¿è¡Œä¸­å…³é—­å†…å­˜åˆ†æã€‚
- en: '#### `metrics_format`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `metrics_format`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L879)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L879)'
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`metrics` (`Dict[str, float]`) â€” The metrics returned from train/evaluate/predict'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` (`Dict[str, float]`) â€” è®­ç»ƒ/è¯„ä¼°/é¢„æµ‹è¿”å›çš„æŒ‡æ ‡'
- en: Returns
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: metrics (`Dict[str, float]`)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: metrics (`Dict[str, float]`)
- en: The reformatted metrics
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ ¼å¼åŒ–çš„æŒ‡æ ‡
- en: Reformat Trainer metrics values to a human-readable format
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: å°†TraineræŒ‡æ ‡å€¼é‡æ–°æ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»çš„æ ¼å¼
- en: '#### `num_examples`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_examples`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1128)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1128)'
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Helper to get number of samples in a `~torch.utils.data.DataLoader` by accessing
    its dataset. When dataloader.dataset does not exist or has no length, estimates
    as best it can
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¿é—®å…¶æ•°æ®é›†æ¥è·å–`~torch.utils.data.DataLoader`ä¸­æ ·æœ¬æ•°çš„å¸®åŠ©ç¨‹åºã€‚å½“æ•°æ®åŠ è½½å™¨æ•°æ®é›†ä¸å­˜åœ¨æˆ–æ²¡æœ‰é•¿åº¦æ—¶ï¼Œå°½å¯èƒ½ä¼°è®¡
- en: '#### `num_tokens`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1142)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1142)'
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Helper to get number of tokens in a `~torch.utils.data.DataLoader` by enumerating
    dataloader.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æšä¸¾æ•°æ®åŠ è½½å™¨æ¥è·å–`~torch.utils.data.DataLoader`ä¸­çš„æ ‡è®°æ•°çš„å¸®åŠ©ç¨‹åºã€‚
- en: '#### `pop_callback`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pop_callback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L665)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L665)'
- en: '[PRE27]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`callback` (`type` or [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    â€” A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    class or an instance of a [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    In the first case, will pop the first member of that class found in the list of
    callbacks.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`ï¼ˆ`type`æˆ–[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ï¼‰-
    [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)ç±»æˆ–[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)çš„å®ä¾‹ã€‚åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œå°†å¼¹å‡ºåœ¨å›è°ƒåˆ—è¡¨ä¸­æ‰¾åˆ°çš„è¯¥ç±»çš„ç¬¬ä¸€ä¸ªæˆå‘˜ã€‚'
- en: Returns
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)'
- en: The callback removed, if found.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¾åˆ°ï¼Œå°†åˆ é™¤å›è°ƒã€‚
- en: Remove a callback from the current list of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    and returns it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å½“å‰çš„[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)åˆ—è¡¨ä¸­åˆ é™¤å›è°ƒå¹¶è¿”å›å®ƒã€‚
- en: If the callback is not found, returns `None` (and no error is raised).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœªæ‰¾åˆ°å›è°ƒï¼Œåˆ™è¿”å›`None`ï¼ˆä¸ä¼šå¼•å‘é”™è¯¯ï¼‰ã€‚
- en: '#### `predict`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3129)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3129)'
- en: '[PRE28]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`test_dataset` (`Dataset`) â€” Dataset to run the predictions on. If it is an
    `datasets.Dataset`, columns not accepted by the `model.forward()` method are automatically
    removed. Has to implement the method `__len__`'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dataset`ï¼ˆ`Dataset`ï¼‰- è¦åœ¨å…¶ä¸Šè¿è¡Œé¢„æµ‹çš„æ•°æ®é›†ã€‚å¦‚æœå®ƒæ˜¯`datasets.Dataset`ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è‡ªåŠ¨åˆ é™¤ã€‚å¿…é¡»å®ç°æ–¹æ³•`__len__`'
- en: '`ignore_keys` (`List[str]`, *optional*) â€” A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹è¾“å‡ºä¸­åº”å¿½ç•¥çš„é”®åˆ—è¡¨ï¼ˆå¦‚æœå®ƒæ˜¯å­—å…¸ï¼‰ã€‚'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"test"`) â€” An optional
    prefix to be used as the metrics key prefix. For example the metrics â€œbleuâ€ will
    be named â€œtest_bleuâ€ if the prefix is â€œtestâ€ (default)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"test"`ï¼‰- ç”¨ä½œæŒ‡æ ‡é”®å‰ç¼€çš„å¯é€‰å‰ç¼€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‰ç¼€æ˜¯â€œtestâ€ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™æŒ‡æ ‡â€œbleuâ€å°†è¢«å‘½åä¸ºâ€œtest_bleuâ€ã€‚'
- en: Run prediction and returns predictions and potential metrics.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œé¢„æµ‹å¹¶è¿”å›é¢„æµ‹å’Œæ½œåœ¨æŒ‡æ ‡ã€‚
- en: Depending on the dataset and your use case, your test dataset may contain labels.
    In that case, this method will also return metrics, like in `evaluate()`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ•°æ®é›†å’Œç”¨ä¾‹ï¼Œæ‚¨çš„æµ‹è¯•æ•°æ®é›†å¯èƒ½åŒ…å«æ ‡ç­¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•è¿˜å°†è¿”å›æŒ‡æ ‡ï¼Œå°±åƒåœ¨`evaluate()`ä¸­ä¸€æ ·ã€‚
- en: If your predictions or labels have different sequence length (for instance because
    youâ€™re doing dynamic padding in a token classification task) the predictions will
    be padded (on the right) to allow for concatenation into one array. The padding
    index is -100.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„é¢„æµ‹æˆ–æ ‡ç­¾å…·æœ‰ä¸åŒçš„åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œå› ä¸ºæ‚¨åœ¨æ ‡è®°åˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡ŒåŠ¨æ€å¡«å……ï¼‰ï¼Œåˆ™é¢„æµ‹å°†è¢«å¡«å……ï¼ˆåœ¨å³ä¾§ï¼‰ï¼Œä»¥å…è®¸è¿æ¥åˆ°ä¸€ä¸ªæ•°ç»„ä¸­ã€‚å¡«å……ç´¢å¼•ä¸º-100ã€‚
- en: 'Returns: *NamedTuple* A namedtuple with the following keys:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ï¼š*NamedTuple* å…·æœ‰ä»¥ä¸‹é”®çš„å‘½åå…ƒç»„ï¼š
- en: 'predictions (`np.ndarray`): The predictions on `test_dataset`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹ï¼ˆ`np.ndarray`ï¼‰ï¼šåœ¨`test_dataset`ä¸Šçš„é¢„æµ‹ã€‚
- en: 'label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained
    some).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: label_idsï¼ˆ`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰ï¼šæ ‡ç­¾ï¼ˆå¦‚æœæ•°æ®é›†åŒ…å«ï¼‰ã€‚
- en: 'metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics
    (if the dataset contained labels).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡æ ‡ï¼ˆ`Dict[str, float]`ï¼Œ*å¯é€‰*ï¼‰ï¼šæ½œåœ¨çš„æŒ‡æ ‡å­—å…¸ï¼ˆå¦‚æœæ•°æ®é›†åŒ…å«æ ‡ç­¾ï¼‰ã€‚
- en: '#### `prediction_loop`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prediction_loop`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3766)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3766)'
- en: '[PRE29]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹/è¯„ä¼°å¾ªç¯ï¼Œç”±`Trainer.evaluate()`å’Œ`Trainer.predict()`å…±äº«ã€‚
- en: Works both with or without labels.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ˜¯å¦æœ‰æ ‡ç­¾ï¼Œéƒ½å¯ä»¥ä½¿ç”¨ã€‚
- en: '#### `prediction_step`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prediction_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3424)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3424)'
- en: '[PRE30]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`nn.Module`) â€” The model to evaluate.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ï¼ˆ`nn.Module`ï¼‰- è¦è¯„ä¼°çš„æ¨¡å‹ã€‚'
- en: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) â€” The inputs and targets of
    the model.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`ï¼ˆ`Dict[str, Union[torch.Tensor, Any]]`ï¼‰- æ¨¡å‹çš„è¾“å…¥å’Œç›®æ ‡ã€‚'
- en: The dictionary will be unpacked before being fed to the model. Most models expect
    the targets under the argument `labels`. Check your modelâ€™s documentation for
    all accepted arguments.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é¦ˆé€æ¨¡å‹ä¹‹å‰ï¼Œå­—å…¸å°†è¢«è§£åŒ…ã€‚å¤§å¤šæ•°æ¨¡å‹å¸Œæœ›ç›®æ ‡åœ¨å‚æ•°`labels`ä¸‹ã€‚æ£€æŸ¥æ‚¨æ¨¡å‹çš„æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æ¥å—çš„å‚æ•°ã€‚
- en: '`prediction_loss_only` (`bool`) â€” Whether or not to return the loss only.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_loss_only`ï¼ˆ`bool`ï¼‰- æ˜¯å¦ä»…è¿”å›æŸå¤±ã€‚'
- en: '`ignore_keys` (`List[str]`, *optional*) â€” A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹è¾“å‡ºä¸­åº”å¿½ç•¥çš„é”®åˆ—è¡¨ï¼ˆå¦‚æœå®ƒæ˜¯å­—å…¸ï¼‰ã€‚'
- en: Returns
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
- en: A tuple with the loss, logits and labels (each being optional).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«æŸå¤±ã€logits å’Œæ ‡ç­¾çš„å…ƒç»„ï¼ˆæ¯ä¸ªéƒ½æ˜¯å¯é€‰çš„ï¼‰ã€‚
- en: Perform an evaluation step on `model` using `inputs`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `inputs` åœ¨ `model` ä¸Šæ‰§è¡Œè¯„ä¼°æ­¥éª¤ã€‚
- en: Subclass and override to inject custom behavior.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: å­ç±»å’Œè¦†ç›–ä»¥æ³¨å…¥è‡ªå®šä¹‰è¡Œä¸ºã€‚
- en: '#### `propagate_args_to_deepspeed`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `propagate_args_to_deepspeed`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L4012)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L4012)'
- en: '[PRE31]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Sets values in the deepspeed plugin based on the Trainer args
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ® Trainer å‚æ•°åœ¨ deepspeed æ’ä»¶ä¸­è®¾ç½®å€¼
- en: '#### `push_to_hub`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3702)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3702)'
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`commit_message` (`str`, *optional*, defaults to `"End of training"`) â€” Message
    to commit while pushing.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"End of training"`) â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚'
- en: '`blocking` (`bool`, *optional*, defaults to `True`) â€” Whether the function
    should return only when the `git push` has finished.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blocking` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” å‡½æ•°æ˜¯å¦åº”è¯¥åœ¨ `git push` å®Œæˆåæ‰è¿”å›ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional keyword arguments passed
    along to [create_model_card()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_model_card).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ä¼ é€’ç»™ [create_model_card()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_model_card)
    çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Upload `self.model` and `self.tokenizer` to the ğŸ¤— model hub on the repo `self.args.hub_model_id`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å°† `self.model` å’Œ `self.tokenizer` ä¸Šä¼ åˆ° ğŸ¤— æ¨¡å‹ä¸­å¿ƒï¼Œå­˜å‚¨åº“ä¸º `self.args.hub_model_id`ã€‚
- en: '#### `remove_callback`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `remove_callback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L681)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L681)'
- en: '[PRE33]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`callback` (`type` or [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    â€” A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    class or an instance of a [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    In the first case, will remove the first member of that class found in the list
    of callbacks.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å›è°ƒ` (`ç±»å‹` æˆ– [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback))
    â€” ä¸€ä¸ª [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    ç±»æˆ–ä¸€ä¸ª [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    çš„å®ä¾‹ã€‚åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œå°†åˆ é™¤åœ¨å›è°ƒåˆ—è¡¨ä¸­æ‰¾åˆ°çš„è¯¥ç±»çš„ç¬¬ä¸€ä¸ªæˆå‘˜ã€‚'
- en: Remove a callback from the current list of [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å½“å‰çš„ [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    åˆ—è¡¨ä¸­åˆ é™¤ä¸€ä¸ªå›è°ƒã€‚
- en: '#### `save_metrics`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_metrics`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L995)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L995)'
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`split` (`str`) â€” Mode/split name: one of `train`, `eval`, `test`, `all`'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split` (`str`) â€” æ¨¡å¼/æ‹†åˆ†åç§°ï¼š`train`, `eval`, `test`, `all` ä¸­çš„ä¸€ä¸ª'
- en: '`metrics` (`Dict[str, float]`) â€” The metrics returned from train/evaluate/predict'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` (`Dict[str, float]`) â€” ä»è®­ç»ƒ/è¯„ä¼°/é¢„æµ‹è¿”å›çš„æŒ‡æ ‡'
- en: '`combined` (`bool`, *optional*, defaults to `True`) â€” Creates combined metrics
    by updating `all_results.json` with metrics of this call'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combined` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” é€šè¿‡æ›´æ–° `all_results.json` åˆ›å»ºç»„åˆæŒ‡æ ‡ï¼Œå…¶ä¸­åŒ…æ‹¬æ­¤è°ƒç”¨çš„æŒ‡æ ‡ã€‚'
- en: Save metrics into a json file for that split, e.g. `train_results.json`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æŒ‡æ ‡ä¿å­˜åˆ°ä¸€ä¸ª json æ–‡ä»¶ä¸­ï¼Œä¾‹å¦‚ `train_results.json`ã€‚
- en: Under distributed environment this is done only for a process with rank 0.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè¿™ä»…é€‚ç”¨äºç§©ä¸º0çš„è¿›ç¨‹ã€‚
- en: To understand the metrics please read the docstring of [log_metrics()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log_metrics).
    The only difference is that raw unformatted numbers are saved in the current method.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æŒ‡æ ‡ï¼Œè¯·é˜…è¯» [log_metrics()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log_metrics)
    çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯åŸå§‹æœªæ ¼å¼åŒ–çš„æ•°å­—ä¿å­˜åœ¨å½“å‰æ–¹æ³•ä¸­ã€‚
- en: '#### `save_model`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2841)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2841)'
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Will save the model, so you can reload it using `from_pretrained()`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¿å­˜æ¨¡å‹ï¼Œå› æ­¤æ‚¨å¯ä»¥ä½¿ç”¨ `from_pretrained()` é‡æ–°åŠ è½½å®ƒã€‚
- en: Will only save from the main process.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä»ä¸»è¿›ç¨‹ä¿å­˜ã€‚
- en: '#### `save_state`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_state`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L1033)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L1033)'
- en: '[PRE36]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Saves the Trainer state, since Trainer.save_model saves only the tokenizer with
    the model
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜ Trainer çŠ¶æ€ï¼Œå› ä¸º Trainer.save_model ä»…ä¿å­˜äº†æ¨¡å‹çš„ tokenizer
- en: Under distributed environment this is done only for a process with rank 0.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè¿™ä»…é€‚ç”¨äºç§©ä¸º0çš„è¿›ç¨‹ã€‚
- en: '#### `train`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `train`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1438)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1438)'
- en: '[PRE37]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`resume_from_checkpoint` (`str` or `bool`, *optional*) â€” If a `str`, local
    path to a saved checkpoint as saved by a previous instance of [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If a `bool` and equals `True`, load the last checkpoint in *args.output_dir* as
    saved by a previous instance of [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If present, training will resume from the model/optimizer/scheduler states loaded
    here.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_from_checkpoint` (`str` æˆ– `bool`, *å¯é€‰*) â€” å¦‚æœæ˜¯ `str`ï¼Œåˆ™æ˜¯ç”±ä¹‹å‰çš„ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    å®ä¾‹ä¿å­˜çš„æœ¬åœ°è·¯å¾„çš„æ£€æŸ¥ç‚¹ã€‚å¦‚æœæ˜¯ `bool` å¹¶ä¸”ç­‰äº `True`ï¼Œåˆ™åŠ è½½ç”±ä¹‹å‰çš„ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    å®ä¾‹ä¿å­˜åœ¨ *args.output_dir* ä¸­çš„æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚å¦‚æœå­˜åœ¨ï¼Œè®­ç»ƒå°†ä»æ­¤å¤„åŠ è½½çš„æ¨¡å‹/ä¼˜åŒ–å™¨/è°ƒåº¦å™¨çŠ¶æ€æ¢å¤ã€‚'
- en: '`trial` (`optuna.Trial` or `Dict[str, Any]`, *optional*) â€” The trial run or
    the hyperparameter dictionary for hyperparameter search.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trial` (`optuna.Trial` æˆ– `Dict[str, Any]`, *å¯é€‰*) â€” è¿è¡Œè¯•éªŒæˆ–ç”¨äºè¶…å‚æ•°æœç´¢çš„è¶…å‚æ•°å­—å…¸ã€‚'
- en: '`ignore_keys_for_eval` (`List[str]`, *optional*) â€” A list of keys in the output
    of your model (if it is a dictionary) that should be ignored when gathering predictions
    for evaluation during the training.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys_for_eval` (`List[str]`, *å¯é€‰*) â€” æ‚¨çš„æ¨¡å‹è¾“å‡ºä¸­åº”åœ¨è¯„ä¼°æœŸé—´å¿½ç•¥çš„é”®çš„åˆ—è¡¨ï¼ˆå¦‚æœå®ƒæ˜¯ä¸€ä¸ªå­—å…¸ï¼‰ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional keyword arguments used
    to hide deprecated arguments'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ç”¨äºéšè—å·²å¼ƒç”¨å‚æ•°çš„é™„åŠ å…³é”®å­—å‚æ•°'
- en: Main training entry point.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦è®­ç»ƒå…¥å£ã€‚
- en: '#### `training_step`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `training_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2746)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2746)'
- en: '[PRE38]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`nn.Module`) â€” The model to train.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`nn.Module`) â€” è¦è®­ç»ƒçš„æ¨¡å‹ã€‚'
- en: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) â€” The inputs and targets of
    the model.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` (`Dict[str, Union[torch.Tensor, Any]]`) â€” æ¨¡å‹çš„è¾“å…¥å’Œç›®æ ‡ã€‚'
- en: The dictionary will be unpacked before being fed to the model. Most models expect
    the targets under the argument `labels`. Check your modelâ€™s documentation for
    all accepted arguments.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å­—å…¸å°†åœ¨é¦ˆé€åˆ°æ¨¡å‹ä¹‹å‰è§£åŒ…ã€‚å¤§å¤šæ•°æ¨¡å‹æœŸæœ›ç›®æ ‡åœ¨å‚æ•°`labels`ä¸‹ã€‚æ£€æŸ¥æ‚¨æ¨¡å‹çš„æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æ¥å—çš„å‚æ•°ã€‚
- en: Returns
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.Tensor`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: The tensor with training loss on this batch.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ‰¹æ¬¡çš„è®­ç»ƒæŸå¤±çš„å¼ é‡ã€‚
- en: Perform a training step on a batch of inputs.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸€æ‰¹è¾“å…¥æ‰§è¡Œè®­ç»ƒæ­¥éª¤ã€‚
- en: Subclass and override to inject custom behavior.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: å­ç±»å’Œè¦†ç›–ä»¥æ³¨å…¥è‡ªå®šä¹‰è¡Œä¸ºã€‚
- en: Seq2SeqTrainer
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2SeqTrainer
- en: '### `class transformers.Seq2SeqTrainer`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Seq2SeqTrainer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L41)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L41)'
- en: '[PRE39]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `evaluate`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L112)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L112)'
- en: '[PRE40]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`eval_dataset` (`Dataset`, *optional*) â€” Pass a dataset if you wish to override
    `self.eval_dataset`. If it is an [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement the `__len__` method.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (`Dataset`, *å¯é€‰*) â€” å¦‚æœè¦è¦†ç›–`self.eval_dataset`ï¼Œè¯·ä¼ é€’ä¸€ä¸ªæ•°æ®é›†ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è‡ªåŠ¨åˆ é™¤ã€‚å®ƒå¿…é¡»å®ç°`__len__`æ–¹æ³•ã€‚'
- en: '`ignore_keys` (`List[str]`, *optional*) â€” A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys` (`List[str]`, *å¯é€‰*) â€” æ‚¨çš„æ¨¡å‹è¾“å‡ºä¸­åº”åœ¨æ”¶é›†é¢„æµ‹æ—¶å¿½ç•¥çš„é”®çš„åˆ—è¡¨ã€‚'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"eval"`) â€” An optional
    prefix to be used as the metrics key prefix. For example the metrics â€œbleuâ€ will
    be named â€œeval_bleuâ€ if the prefix is `"eval"` (default)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"eval"`) â€” ç”¨ä½œæŒ‡æ ‡é”®å‰ç¼€çš„å¯é€‰å‰ç¼€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‰ç¼€æ˜¯`"eval"`ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™æŒ‡æ ‡â€œbleuâ€å°†è¢«å‘½åä¸ºâ€œeval_bleuâ€ã€‚'
- en: '`max_length` (`int`, *optional*) â€” The maximum target length to use when predicting
    with the generate method.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *å¯é€‰*) â€” åœ¨ä½¿ç”¨`generate`æ–¹æ³•è¿›è¡Œé¢„æµ‹æ—¶ä½¿ç”¨çš„æœ€å¤§ç›®æ ‡é•¿åº¦ã€‚'
- en: '`num_beams` (`int`, *optional*) â€” Number of beams for beam search that will
    be used when predicting with the generate method. 1 means no beam search. gen_kwargs
    â€” Additional `generate` specific kwargs.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`, *å¯é€‰*) â€” åœ¨ä½¿ç”¨`generate`æ–¹æ³•è¿›è¡Œé¢„æµ‹æ—¶å°†ç”¨äºæŸæœç´¢çš„æŸæ•°ã€‚1è¡¨ç¤ºæ²¡æœ‰æŸæœç´¢ã€‚gen_kwargs
    â€” é™„åŠ çš„`generate`ç‰¹å®škwargsã€‚'
- en: Run evaluation and returns metrics.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œè¯„ä¼°å¹¶è¿”å›æŒ‡æ ‡ã€‚
- en: The calling script will be responsible for providing a method to compute metrics,
    as they are task-dependent (pass it to the init `compute_metrics` argument).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨è„šæœ¬å°†è´Ÿè´£æä¾›è®¡ç®—æŒ‡æ ‡çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä»»åŠ¡ç›¸å…³çš„ï¼ˆå°†å…¶ä¼ é€’ç»™init `compute_metrics`å‚æ•°ï¼‰ã€‚
- en: You can also subclass and override this method to inject custom behavior.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å­ç±»åŒ–å¹¶è¦†ç›–æ­¤æ–¹æ³•ä»¥æ³¨å…¥è‡ªå®šä¹‰è¡Œä¸ºã€‚
- en: '#### `predict`'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L168)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L168)'
- en: '[PRE41]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`test_dataset` (`Dataset`) â€” Dataset to run the predictions on. If it is a
    [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    Has to implement the method `__len__`'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dataset` (`Dataset`) â€” è¦åœ¨å…¶ä¸Šè¿è¡Œé¢„æµ‹çš„æ•°æ®é›†ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è‡ªåŠ¨åˆ é™¤ã€‚å¿…é¡»å®ç°`__len__`æ–¹æ³•'
- en: '`ignore_keys` (`List[str]`, *optional*) â€” A list of keys in the output of your
    model (if it is a dictionary) that should be ignored when gathering predictions.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_keys` (`List[str]`, *å¯é€‰*) â€” æ‚¨çš„æ¨¡å‹è¾“å‡ºä¸­åº”åœ¨æ”¶é›†é¢„æµ‹æ—¶å¿½ç•¥çš„é”®çš„åˆ—è¡¨ã€‚'
- en: '`metric_key_prefix` (`str`, *optional*, defaults to `"eval"`) â€” An optional
    prefix to be used as the metrics key prefix. For example the metrics â€œbleuâ€ will
    be named â€œeval_bleuâ€ if the prefix is `"eval"` (default)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_key_prefix` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"eval"`) â€” ç”¨ä½œæŒ‡æ ‡é”®å‰ç¼€çš„å¯é€‰å‰ç¼€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‰ç¼€æ˜¯`"eval"`ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™æŒ‡æ ‡â€œbleuâ€å°†è¢«å‘½åä¸ºâ€œeval_bleuâ€ã€‚'
- en: '`max_length` (`int`, *optional*) â€” The maximum target length to use when predicting
    with the generate method.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *å¯é€‰*) â€” åœ¨ä½¿ç”¨`generate`æ–¹æ³•è¿›è¡Œé¢„æµ‹æ—¶ä½¿ç”¨çš„æœ€å¤§ç›®æ ‡é•¿åº¦ã€‚'
- en: '`num_beams` (`int`, *optional*) â€” Number of beams for beam search that will
    be used when predicting with the generate method. 1 means no beam search. gen_kwargs
    â€” Additional `generate` specific kwargs.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`, *å¯é€‰*) â€” åœ¨ä½¿ç”¨`generate`æ–¹æ³•è¿›è¡Œé¢„æµ‹æ—¶å°†ç”¨äºæŸæœç´¢çš„æŸæ•°ã€‚1è¡¨ç¤ºæ²¡æœ‰æŸæœç´¢ã€‚gen_kwargs
    â€” é™„åŠ çš„`generate`ç‰¹å®škwargsã€‚'
- en: Run prediction and returns predictions and potential metrics.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œé¢„æµ‹å¹¶è¿”å›é¢„æµ‹å’Œæ½œåœ¨æŒ‡æ ‡ã€‚
- en: Depending on the dataset and your use case, your test dataset may contain labels.
    In that case, this method will also return metrics, like in `evaluate()`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ•°æ®é›†å’Œæ‚¨çš„ç”¨ä¾‹ï¼Œæ‚¨çš„æµ‹è¯•æ•°æ®é›†å¯èƒ½åŒ…å«æ ‡ç­¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•è¿˜å°†è¿”å›æŒ‡æ ‡ï¼Œå°±åƒåœ¨`evaluate()`ä¸­ä¸€æ ·ã€‚
- en: If your predictions or labels have different sequence lengths (for instance
    because youâ€™re doing dynamic padding in a token classification task) the predictions
    will be padded (on the right) to allow for concatenation into one array. The padding
    index is -100.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„é¢„æµ‹æˆ–æ ‡ç­¾å…·æœ‰ä¸åŒçš„åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œå› ä¸ºæ‚¨åœ¨æ ‡è®°åˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡ŒåŠ¨æ€å¡«å……ï¼‰ï¼Œåˆ™é¢„æµ‹å°†è¢«å¡«å……ï¼ˆåœ¨å³ä¾§ï¼‰ä»¥å…è®¸è¿æ¥æˆä¸€ä¸ªæ•°ç»„ã€‚å¡«å……ç´¢å¼•ä¸º -100ã€‚
- en: 'Returns: *NamedTuple* A namedtuple with the following keys:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿”å›: *NamedTuple* å…·æœ‰ä»¥ä¸‹é”®çš„å‘½åå…ƒç»„:'
- en: 'predictions (`np.ndarray`): The predictions on `test_dataset`.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'predictions (`np.ndarray`): åœ¨ `test_dataset` ä¸Šçš„é¢„æµ‹ã€‚'
- en: 'label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained
    some).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'label_ids (`np.ndarray`, *optional*): æ ‡ç­¾ï¼ˆå¦‚æœæ•°æ®é›†åŒ…å«æ ‡ç­¾ï¼‰ã€‚'
- en: 'metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics
    (if the dataset contained labels).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'metrics (`Dict[str, float]`, *optional*): æ½œåœ¨çš„æŒ‡æ ‡å­—å…¸ï¼ˆå¦‚æœæ•°æ®é›†åŒ…å«æ ‡ç­¾ï¼‰ã€‚'
- en: TrainingArguments
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainingArguments
- en: '### `class transformers.TrainingArguments`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainingArguments`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161)'
- en: '[PRE42]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`output_dir` (`str`) â€” The output directory where the model predictions and
    checkpoints will be written.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir` (`str`) â€” æ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹å°†è¢«å†™å…¥çš„è¾“å‡ºç›®å½•ã€‚'
- en: '`overwrite_output_dir` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    overwrite the content of the output directory. Use this to continue training if
    `output_dir` points to a checkpoint directory.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overwrite_output_dir` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œåˆ™è¦†ç›–è¾“å‡ºç›®å½•çš„å†…å®¹ã€‚ä½¿ç”¨æ­¤é€‰é¡¹ç»§ç»­è®­ç»ƒï¼Œå¦‚æœ
    `output_dir` æŒ‡å‘æ£€æŸ¥ç‚¹ç›®å½•ã€‚'
- en: '`do_train` (`bool`, *optional*, defaults to `False`) â€” Whether to run training
    or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_train` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿è¡Œè®­ç»ƒã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢« [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯ç”¨äºæ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ã€‚æŸ¥çœ‹ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)
    è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚'
- en: '`do_eval` (`bool`, *optional*) â€” Whether to run evaluation on the validation
    set or not. Will be set to `True` if `evaluation_strategy` is different from `"no"`.
    This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_eval` (`bool`, *optional*) â€” æ˜¯å¦åœ¨éªŒè¯é›†ä¸Šè¿è¡Œè¯„ä¼°ã€‚å¦‚æœ `evaluation_strategy` ä¸ `"no"`
    ä¸åŒï¼Œåˆ™å°†è®¾ç½®ä¸º `True`ã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢« [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯ç”¨äºæ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ã€‚æŸ¥çœ‹ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)
    è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚'
- en: '`do_predict` (`bool`, *optional*, defaults to `False`) â€” Whether to run predictions
    on the test set or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_predict` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œé¢„æµ‹ã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢« [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯ç”¨äºæ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ã€‚æŸ¥çœ‹ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)
    è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚'
- en: '`evaluation_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) â€” The evaluation strategy to adopt during training.
    Possible values are:'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluation_strategy` (`str` æˆ– [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, é»˜è®¤ä¸º `"no"`) â€” è®­ç»ƒæœŸé—´é‡‡ç”¨çš„è¯„ä¼°ç­–ç•¥ã€‚å¯èƒ½çš„å€¼ä¸º:'
- en: '`"no"`: No evaluation is done during training.'
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒæœŸé—´ä¸è¿›è¡Œè¯„ä¼°ã€‚'
- en: '`"steps"`: Evaluation is done (and logged) every `eval_steps`.'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯ `eval_steps` æ¬¡è¿›è¡Œè¯„ä¼°ï¼ˆå¹¶è®°å½•æ—¥å¿—ï¼‰ã€‚'
- en: '`"epoch"`: Evaluation is done at the end of each epoch.'
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚'
- en: '`prediction_loss_only` (`bool`, *optional*, defaults to `False`) â€” When performing
    evaluation and generating predictions, only returns the loss.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_loss_only` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” åœ¨è¿›è¡Œè¯„ä¼°å’Œç”Ÿæˆé¢„æµ‹æ—¶ï¼Œä»…è¿”å›æŸå¤±ã€‚'
- en: '`per_device_train_batch_size` (`int`, *optional*, defaults to 8) â€” The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for training.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_train_batch_size` (`int`, *optional*, é»˜è®¤ä¸º 8) â€” ç”¨äºè®­ç»ƒçš„æ¯ä¸ª GPU/XPU/TPU/MPS/NPU
    æ ¸å¿ƒ/CPU çš„æ‰¹å¤„ç†å¤§å°ã€‚'
- en: '`per_device_eval_batch_size` (`int`, *optional*, defaults to 8) â€” The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_eval_batch_size` (`int`, *optional*, é»˜è®¤ä¸º 8) â€” ç”¨äºè¯„ä¼°çš„æ¯ä¸ª GPU/XPU/TPU/MPS/NPU
    æ ¸å¿ƒ/CPU çš„æ‰¹å¤„ç†å¤§å°ã€‚'
- en: '`gradient_accumulation_steps` (`int`, *optional*, defaults to 1) â€” Number of
    updates steps to accumulate the gradients for, before performing a backward/update
    pass.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` (`int`, *optional*, é»˜è®¤ä¸º 1) â€” ç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥æ•°ï¼Œç„¶åæ‰§è¡Œåå‘/æ›´æ–°ä¼ é€’ã€‚'
- en: When using gradient accumulation, one step is counted as one step with backward
    pass. Therefore, logging, evaluation, save will be conducted every `gradient_accumulation_steps
    * xxx_step` training examples.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œä¸€ä¸ªæ­¥éª¤è¢«è®¡ä¸ºä¸€ä¸ªå¸¦æœ‰åå‘ä¼ é€’çš„æ­¥éª¤ã€‚å› æ­¤ï¼Œæ¯ `gradient_accumulation_steps * xxx_step` è®­ç»ƒç¤ºä¾‹å°†è¿›è¡Œæ—¥å¿—è®°å½•ã€è¯„ä¼°å’Œä¿å­˜ã€‚
- en: '`eval_accumulation_steps` (`int`, *optional*) â€” Number of predictions steps
    to accumulate the output tensors for, before moving the results to the CPU. If
    left unset, the whole predictions are accumulated on GPU/NPU/TPU before being
    moved to the CPU (faster but requires more memory).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_accumulation_steps` (`int`, *optional*) â€” åœ¨å°†ç»“æœç§»åŠ¨åˆ° CPU ä¹‹å‰ï¼Œç´¯ç§¯è¾“å‡ºå¼ é‡çš„é¢„æµ‹æ­¥æ•°ã€‚å¦‚æœæœªè®¾ç½®ï¼Œæ•´ä¸ªé¢„æµ‹å°†åœ¨
    GPU/NPU/TPU ä¸Šç´¯ç§¯åå†ç§»åŠ¨åˆ° CPUï¼ˆé€Ÿåº¦æ›´å¿«ä½†éœ€è¦æ›´å¤šå†…å­˜ï¼‰ã€‚'
- en: '`eval_delay` (`float`, *optional*) â€” Number of epochs or steps to wait for
    before the first evaluation can be performed, depending on the evaluation_strategy.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_delay` (`float`, *optional*) â€” åœ¨è¿›è¡Œç¬¬ä¸€æ¬¡è¯„ä¼°ä¹‹å‰ç­‰å¾…çš„æ—¶æœŸæˆ–æ­¥æ•°ï¼Œå–å†³äº `evaluation_strategy`ã€‚'
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) â€” The initial learning
    rate for [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` (`float`, *optional*, defaults to 5e-5) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ã€‚'
- en: '`weight_decay` (`float`, *optional*, defaults to 0) â€” The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights in [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` (`float`, *optional*, defaults to 0) â€” åº”ç”¨çš„æƒé‡è¡°å‡ï¼ˆå¦‚æœä¸ä¸ºé›¶ï¼‰åˆ° [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨ä¸­çš„æ‰€æœ‰å±‚ï¼Œé™¤äº†æ‰€æœ‰åç½®å’Œ LayerNorm æƒé‡ã€‚'
- en: '`adam_beta1` (`float`, *optional*, defaults to 0.9) â€” The beta1 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta1` (`float`, *optional*, defaults to 0.9) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„ beta1 è¶…å‚æ•°ã€‚'
- en: '`adam_beta2` (`float`, *optional*, defaults to 0.999) â€” The beta2 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta2` (`float`, *optional*, defaults to 0.999) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„ beta2 è¶…å‚æ•°ã€‚'
- en: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) â€” The epsilon hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„ epsilon è¶…å‚æ•°ã€‚'
- en: '`max_grad_norm` (`float`, *optional*, defaults to 1.0) â€” Maximum gradient norm
    (for gradient clipping).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_grad_norm` (`float`, *optional*, defaults to 1.0) â€” æœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºæ¢¯åº¦è£å‰ªï¼‰ã€‚'
- en: '`num_train_epochs(float,` *optional*, defaults to 3.0) â€” Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_epochs(float,` *optional*, defaults to 3.0) â€” æ‰§è¡Œçš„æ€»è®­ç»ƒæ—¶ä»£æ•°ï¼ˆå¦‚æœä¸æ˜¯æ•´æ•°ï¼Œåˆ™åœ¨åœæ­¢è®­ç»ƒä¹‹å‰æ‰§è¡Œæœ€åä¸€ä¸ªæ—¶ä»£çš„å°æ•°éƒ¨åˆ†ç™¾åˆ†æ¯”ï¼‰ã€‚'
- en: '`max_steps` (`int`, *optional*, defaults to -1) â€” If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *optional*, defaults to -1) â€” å¦‚æœè®¾ç½®ä¸ºæ­£æ•°ï¼Œåˆ™æ‰§è¡Œçš„æ€»è®­ç»ƒæ­¥æ•°ã€‚è¦†ç›– `num_train_epochs`ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é›†ï¼Œè®­ç»ƒé€šè¿‡æ•°æ®é›†ï¼ˆå¦‚æœæ‰€æœ‰æ•°æ®éƒ½ç”¨å®Œï¼‰é‡å¤è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°
    `max_steps`ã€‚'
- en: '`lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) â€” The scheduler type to use. See the documentation
    of [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    for all possible values.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) â€” è¦ä½¿ç”¨çš„è°ƒåº¦å™¨ç±»å‹ã€‚æŸ¥çœ‹ [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    çš„æ–‡æ¡£ä»¥è·å–æ‰€æœ‰å¯èƒ½çš„å€¼ã€‚'
- en: '`lr_scheduler_kwargs` (â€˜dictâ€™, *optional*, defaults to {}) â€” The extra arguments
    for the lr_scheduler. See the documentation of each scheduler for possible values.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_kwargs` (â€˜dictâ€™, *optional*, defaults to {}) â€” lr_scheduler çš„é¢å¤–å‚æ•°ã€‚æŸ¥çœ‹æ¯ä¸ªè°ƒåº¦å™¨çš„æ–‡æ¡£ä»¥è·å–å¯èƒ½çš„å€¼ã€‚'
- en: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) â€” Ratio of total training
    steps used for a linear warmup from 0 to `learning_rate`.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) â€” ç”¨äºä» 0 çº¿æ€§é¢„çƒ­åˆ° `learning_rate`
    çš„æ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ç‡ã€‚'
- en: '`warmup_steps` (`int`, *optional*, defaults to 0) â€” Number of steps used for
    a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`, *optional*, defaults to 0) â€” ç”¨äºä» 0 çº¿æ€§é¢„çƒ­åˆ° `learning_rate`
    çš„æ­¥éª¤æ•°ã€‚è¦†ç›– `warmup_ratio` çš„ä»»ä½•æ•ˆæœã€‚'
- en: '`log_level` (`str`, *optional*, defaults to `passive`) â€” Logger log level to
    use on the main process. Possible choices are the log levels as strings: â€˜debugâ€™,
    â€˜infoâ€™, â€˜warningâ€™, â€˜errorâ€™ and â€˜criticalâ€™, plus a â€˜passiveâ€™ level which doesnâ€™t
    set anything and keeps the current log level for the Transformers library (which
    will be `"warning"` by default).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level` (`str`, *optional*, defaults to `passive`) â€” ä¸»è¿›ç¨‹ä½¿ç”¨çš„è®°å½•å™¨æ—¥å¿—çº§åˆ«ã€‚å¯èƒ½çš„é€‰æ‹©æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ—¥å¿—çº§åˆ«ï¼š''debug''ã€''info''ã€''warning''ã€''error''å’Œ''critical''ï¼Œä»¥åŠä¸€ä¸ª''passive''çº§åˆ«ï¼Œå®ƒä¸è®¾ç½®ä»»ä½•å†…å®¹ï¼Œå¹¶ä¿æŒTransformersåº“çš„å½“å‰æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤ä¸º`"warning"`ï¼‰ã€‚'
- en: '`log_level_replica` (`str`, *optional*, defaults to `"warning"`) â€” Logger log
    level to use on replicas. Same choices as `log_level`â€'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level_replica` (`str`, *optional*, defaults to `"warning"`) â€” å‰¯æœ¬ä½¿ç”¨çš„è®°å½•å™¨æ—¥å¿—çº§åˆ«ã€‚ä¸
    `log_level` ç›¸åŒçš„é€‰æ‹©ã€‚'
- en: '`log_on_each_node` (`bool`, *optional*, defaults to `True`) â€” In multinode
    distributed training, whether to log using `log_level` once per node, or only
    on the main node.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_on_each_node` (`bool`, *optional*, defaults to `True`) â€” åœ¨å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ˜¯å¦æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨
    `log_level` è¿›è¡Œæ—¥å¿—è®°å½•ï¼Œè¿˜æ˜¯ä»…åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿›è¡Œã€‚'
- en: '`logging_dir` (`str`, *optional*) â€” [TensorBoard](https://www.tensorflow.org/tensorboard)
    log directory. Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_dir` (`str`, *optional*) â€” [TensorBoard](https://www.tensorflow.org/tensorboard)
    æ—¥å¿—ç›®å½•ã€‚é»˜è®¤ä¸º *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***ã€‚'
- en: '`logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” The logging strategy to adopt during training.
    Possible values are:'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” è®­ç»ƒæœŸé—´é‡‡ç”¨çš„æ—¥å¿—è®°å½•ç­–ç•¥ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'
- en: '`"no"`: No logging is done during training.'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒæœŸé—´ä¸è¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`"epoch"`: Logging is done at the end of each epoch.'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶è¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`"steps"`: Logging is done every `logging_steps`.'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯ `logging_steps` è¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`logging_first_step` (`bool`, *optional*, defaults to `False`) â€” Whether to
    log and evaluate the first `global_step` or not.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_first_step` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦è®°å½•å’Œè¯„ä¼°ç¬¬ä¸€ä¸ª
    `global_step`ã€‚'
- en: '`logging_steps` (`int` or `float`, *optional*, defaults to 500) â€” Number of
    update steps between two logs if `logging_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_steps` (`int` or `float`, *optional*, defaults to 500) â€” å¦‚æœ `logging_strategy="steps"`ï¼Œåˆ™åœ¨ä¸¤ä¸ªæ—¥å¿—ä¹‹é—´çš„æ›´æ–°æ­¥éª¤æ•°ã€‚åº”ä¸ºæ•´æ•°æˆ–èŒƒå›´ä¸º
    `[0,1)` çš„æµ®ç‚¹æ•°ã€‚å¦‚æœå°äº 1ï¼Œå°†è¢«è§£é‡Šä¸ºæ€»è®­ç»ƒæ­¥éª¤çš„æ¯”ç‡ã€‚'
- en: '`logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) â€” Whether
    to filter `nan` and `inf` losses for logging. If set to `True` the loss of every
    step that is `nan` or `inf` is filtered and the average loss of the current logging
    window is taken instead.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¿‡æ»¤ç”¨äºè®°å½•çš„
    `nan` å’Œ `inf` æŸå¤±ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™ä¼šè¿‡æ»¤æ¯ä¸ªæ­¥éª¤çš„æŸå¤±ï¼Œå¦‚æœä¸º `nan` æˆ– `inf`ï¼Œåˆ™å–å½“å‰æ—¥å¿—çª—å£çš„å¹³å‡æŸå¤±ã€‚ '
- en: '`logging_nan_inf_filter` only influences the logging of loss values, it does
    not change the behavior the gradient is computed or applied to the model.'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` ä»…å½±å“æŸå¤±å€¼çš„è®°å½•ï¼Œä¸ä¼šæ”¹å˜æ¢¯åº¦çš„è®¡ç®—æˆ–åº”ç”¨äºæ¨¡å‹çš„è¡Œä¸ºã€‚'
- en: '`save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” The checkpoint save strategy to adopt during
    training. Possible values are:'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” è®­ç»ƒæœŸé—´é‡‡ç”¨çš„æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'
- en: '`"no"`: No save is done during training.'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä¿å­˜ã€‚'
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶ä¿å­˜ã€‚'
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯ `save_steps` ä¿å­˜ä¸€æ¬¡ã€‚'
- en: '`save_steps` (`int` or `float`, *optional*, defaults to 500) â€” Number of updates
    steps before two checkpoint saves if `save_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps` (`int` or `float`, *optional*, defaults to 500) â€” å¦‚æœ `save_strategy="steps"`ï¼Œåœ¨ä¸¤æ¬¡æ£€æŸ¥ç‚¹ä¿å­˜ä¹‹å‰çš„æ›´æ–°æ­¥éª¤æ•°ã€‚åº”ä¸ºæ•´æ•°æˆ–èŒƒå›´ä¸º
    `[0,1)` çš„æµ®ç‚¹æ•°ã€‚å¦‚æœå°äº 1ï¼Œå°†è¢«è§£é‡Šä¸ºæ€»è®­ç»ƒæ­¥éª¤çš„æ¯”ç‡ã€‚'
- en: '`save_total_limit` (`int`, *optional*) â€” If a value is passed, will limit the
    total amount of checkpoints. Deletes the older checkpoints in `output_dir`. When
    `load_best_model_at_end` is enabled, the â€œbestâ€ checkpoint according to `metric_for_best_model`
    will always be retained in addition to the most recent ones. For example, for
    `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will
    always be retained alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`,
    it is possible that two checkpoints are saved: the last one and the best one (if
    they are different).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_total_limit` (`int`, *optional*) â€” å¦‚æœä¼ é€’äº†ä¸€ä¸ªå€¼ï¼Œå°†é™åˆ¶æ£€æŸ¥ç‚¹çš„æ€»é‡ã€‚åˆ é™¤ `output_dir`
    ä¸­çš„æ—§æ£€æŸ¥ç‚¹ã€‚å½“å¯ç”¨ `load_best_model_at_end` æ—¶ï¼Œæ ¹æ® `metric_for_best_model` çš„â€œæœ€ä½³â€æ£€æŸ¥ç‚¹å°†å§‹ç»ˆä¿ç•™åœ¨æœ€è¿‘çš„æ£€æŸ¥ç‚¹ä¹‹å¤–ã€‚ä¾‹å¦‚ï¼Œå¯¹äº
    `save_total_limit=5` å’Œ `load_best_model_at_end`ï¼Œæœ€åå››ä¸ªæ£€æŸ¥ç‚¹å°†å§‹ç»ˆä¸æœ€ä½³æ¨¡å‹ä¸€èµ·ä¿ç•™ã€‚å½“ `save_total_limit=1`
    å’Œ `load_best_model_at_end` æ—¶ï¼Œå¯èƒ½ä¿å­˜ä¸¤ä¸ªæ£€æŸ¥ç‚¹ï¼šæœ€åä¸€ä¸ªå’Œæœ€ä½³ä¸€ä¸ªï¼ˆå¦‚æœå®ƒä»¬ä¸åŒï¼‰ã€‚'
- en: '`save_safetensors` (`bool`, *optional*, defaults to `True`) â€” Use [safetensors](https://huggingface.co/docs/safetensors)
    saving and loading for state dicts instead of default `torch.load` and `torch.save`.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_safetensors` (`bool`, *optional*, defaults to `True`) â€” ä½¿ç”¨ [safetensors](https://huggingface.co/docs/safetensors)
    ä¿å­˜å’ŒåŠ è½½çŠ¶æ€å­—å…¸ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„ `torch.load` å’Œ `torch.save`ã€‚'
- en: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) â€” When doing
    multi-node distributed training, whether to save models and checkpoints on each
    node, or only on the main one.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) â€” åœ¨è¿›è¡Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œæ˜¯å¦åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ï¼Œè¿˜æ˜¯åªåœ¨ä¸»èŠ‚ç‚¹ä¸Šä¿å­˜ã€‚'
- en: This should not be activated when the different nodes use the same storage as
    the files will be saved with the same names for each node.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ä¸åŒèŠ‚ç‚¹ä½¿ç”¨ç›¸åŒå­˜å‚¨æ—¶ï¼Œä¸åº”æ¿€æ´»æ­¤é€‰é¡¹ï¼Œå› ä¸ºæ–‡ä»¶å°†ä»¥ç›¸åŒåç§°ä¿å­˜åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šã€‚
- en: '`save_only_model` (`bool`, *optional*, defaults to `False`) â€” When checkpointing,
    whether to only save the model, or also the optimizer, scheduler & rng state.
    Note that when this is true, you wonâ€™t be able to resume training from checkpoint.
    This enables you to save storage by not storing the optimizer, scheduler & rng
    state. You can only load the model using `from_pretrained` with this option set
    to `True`.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_only_model` (`bool`, *optional*, defaults to `False`) â€” åœ¨è¿›è¡Œæ£€æŸ¥ç‚¹æ—¶ï¼Œæ˜¯å¦ä»…ä¿å­˜æ¨¡å‹ï¼Œè¿˜æ˜¯åŒæ—¶ä¿å­˜ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œ
    rng çŠ¶æ€ã€‚è¯·æ³¨æ„ï¼Œå½“æ­¤é€‰é¡¹ä¸º true æ—¶ï¼Œæ‚¨å°†æ— æ³•ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚è¿™æ ·å¯ä»¥é€šè¿‡ä¸å­˜å‚¨ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œ rng çŠ¶æ€æ¥èŠ‚çœå­˜å‚¨ç©ºé—´ã€‚æ‚¨åªèƒ½ä½¿ç”¨ `from_pretrained`
    åŠ è½½æ¨¡å‹ï¼Œå¹¶å°†æ­¤é€‰é¡¹è®¾ç½®ä¸º `True`ã€‚'
- en: '`use_cpu` (`bool`, *optional*, defaults to `False`) â€” Whether or not to use
    cpu. If set to False, we will use cuda or mps device if available.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cpu` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨ cpuã€‚å¦‚æœè®¾ç½®ä¸º Falseï¼Œå°†ä½¿ç”¨
    cuda æˆ– mps è®¾å¤‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚'
- en: '`seed` (`int`, *optional*, defaults to 42) â€” Random seed that will be set at
    the beginning of training. To ensure reproducibility across runs, use the `~Trainer.model_init`
    function to instantiate the model if it has some randomly initialized parameters.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`, *optional*, defaults to 42) â€” åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®çš„éšæœºç§å­ã€‚ä¸ºäº†ç¡®ä¿å¤šæ¬¡è¿è¡Œçš„å¯é‡ç°æ€§ï¼Œè¯·ä½¿ç”¨
    `~Trainer.model_init` å‡½æ•°æ¥å®ä¾‹åŒ–æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹å…·æœ‰ä¸€äº›éšæœºåˆå§‹åŒ–çš„å‚æ•°ã€‚'
- en: '`data_seed` (`int`, *optional*) â€” Random seed to be used with data samplers.
    If not set, random generators for data sampling will use the same seed as `seed`.
    This can be used to ensure reproducibility of data sampling, independent of the
    model seed.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_seed` (`int`, *optional*) â€” ç”¨äºæ•°æ®é‡‡æ ·å™¨çš„éšæœºç§å­ã€‚å¦‚æœæœªè®¾ç½®ï¼Œç”¨äºæ•°æ®é‡‡æ ·çš„éšæœºç”Ÿæˆå™¨å°†ä½¿ç”¨ä¸ `seed`
    ç›¸åŒçš„ç§å­ã€‚è¿™å¯ç”¨äºç¡®ä¿æ•°æ®é‡‡æ ·çš„å¯é‡ç°æ€§ï¼Œç‹¬ç«‹äºæ¨¡å‹ç§å­ã€‚'
- en: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to use PyTorch jit trace for inference.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨ PyTorch jit
    trace è¿›è¡Œæ¨æ–­ã€‚'
- en: '`use_ipex` (`bool`, *optional*, defaults to `False`) â€” Use Intel extension
    for PyTorch when it is available. [IPEX installation](https://github.com/intel/intel-extension-for-pytorch).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_ipex` (`bool`, *optional*, defaults to `False`) â€” åœ¨å¯ç”¨æ—¶ä½¿ç”¨ PyTorch çš„ Intel
    æ‰©å±•ã€‚[IPEX å®‰è£…](https://github.com/intel/intel-extension-for-pytorch)ã€‚'
- en: '`bf16` (`bool`, *optional*, defaults to `False`) â€” Whether to use bf16 16-bit
    (mixed) precision training instead of 32-bit training. Requires Ampere or higher
    NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental
    API and it may change.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨bf16 16ä½ï¼ˆæ··åˆï¼‰ç²¾åº¦è®­ç»ƒï¼Œè€Œä¸æ˜¯32ä½è®­ç»ƒã€‚éœ€è¦å®‰åŸ¹æˆ–æ›´é«˜çš„NVIDIAæ¶æ„ï¼Œæˆ–è€…ä½¿ç”¨CPUï¼ˆuse_cpuï¼‰æˆ–Ascend
    NPUã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§APIï¼Œå¯èƒ½ä¼šæ›´æ”¹ã€‚'
- en: '`fp16` (`bool`, *optional*, defaults to `False`) â€” Whether to use fp16 16-bit
    (mixed) precision training instead of 32-bit training.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨fp16 16ä½ï¼ˆæ··åˆï¼‰ç²¾åº¦è®­ç»ƒï¼Œè€Œä¸æ˜¯32ä½è®­ç»ƒã€‚'
- en: '`fp16_opt_level` (`str`, *optional*, defaults to â€˜O1â€™) â€” For `fp16` training,
    Apex AMP optimization level selected in [â€˜O0â€™, â€˜O1â€™, â€˜O2â€™, and â€˜O3â€™]. See details
    on the [Apex documentation](https://nvidia.github.io/apex/amp).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_opt_level` (`str`, *optional*, defaults to â€˜O1â€™) â€” å¯¹äº`fp16`è®­ç»ƒï¼Œé€‰æ‹©åœ¨[â€˜O0â€™,
    â€˜O1â€™, â€˜O2â€™, å’Œ â€˜O3â€™]ä¸­çš„Apex AMPä¼˜åŒ–çº§åˆ«ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Apexæ–‡æ¡£](https://nvidia.github.io/apex/amp)ã€‚'
- en: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) â€” This argument is
    deprecated. Use `half_precision_backend` instead.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) â€” æ­¤å‚æ•°å·²å¼ƒç”¨ã€‚è¯·æ”¹ç”¨`half_precision_backend`ã€‚'
- en: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) â€” The backend
    to use for mixed precision training. Must be one of `"auto", "apex", "cpu_amp"`.
    `"auto"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected,
    while the other choices will force the requested backend.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) â€” ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒçš„åç«¯ã€‚å¿…é¡»æ˜¯`"auto",
    "apex", "cpu_amp"`ä¹‹ä¸€ã€‚`"auto"`å°†æ ¹æ®æ£€æµ‹åˆ°çš„PyTorchç‰ˆæœ¬ä½¿ç”¨CPU/CUDA AMPæˆ–APEXï¼Œè€Œå…¶ä»–é€‰æ‹©å°†å¼ºåˆ¶ä½¿ç”¨è¯·æ±‚çš„åç«¯ã€‚'
- en: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    full bfloat16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values. This is an experimental API and it may change.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨å®Œæ•´çš„bfloat16è¯„ä¼°ï¼Œè€Œä¸æ˜¯32ä½ã€‚è¿™å°†æ›´å¿«ï¼ŒèŠ‚çœå†…å­˜ï¼Œä½†å¯èƒ½ä¼šæŸå®³æŒ‡æ ‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§APIï¼Œå¯èƒ½ä¼šæ›´æ”¹ã€‚'
- en: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    full float16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨å®Œæ•´çš„float16è¯„ä¼°ï¼Œè€Œä¸æ˜¯32ä½ã€‚è¿™å°†æ›´å¿«ï¼ŒèŠ‚çœå†…å­˜ï¼Œä½†å¯èƒ½ä¼šæŸå®³æŒ‡æ ‡å€¼ã€‚'
- en: '`tf32` (`bool`, *optional*) â€” Whether to enable the TF32 mode, available in
    Ampere and newer GPU architectures. The default value depends on PyTorchâ€™s version
    default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer
    to the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation.
    This is an experimental API and it may change.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf32` (`bool`, *optional*) â€” æ˜¯å¦å¯ç”¨TF32æ¨¡å¼ï¼Œé€‚ç”¨äºå®‰åŸ¹å’Œæ›´æ–°çš„GPUæ¶æ„ã€‚é»˜è®¤å€¼å–å†³äºPyTorchç‰ˆæœ¬çš„`torch.backends.cuda.matmul.allow_tf32`é»˜è®¤å€¼ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[TF32](https://huggingface.co/docs/transformers/performance#tf32)æ–‡æ¡£ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§APIï¼Œå¯èƒ½ä¼šæ›´æ”¹ã€‚'
- en: '`local_rank` (`int`, *optional*, defaults to -1) â€” Rank of the process during
    distributed training.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_rank` (`int`, *optional*, defaults to -1`) â€” åˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹ä¸­è¿›ç¨‹çš„æ’åã€‚'
- en: '`ddp_backend` (`str`, *optional*) â€” The backend to use for distributed training.
    Must be one of `"nccl"`, `"mpi"`, `"ccl"`, `"gloo"`, `"hccl"`.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_backend` (`str`, *optional*) â€” ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„åç«¯ã€‚å¿…é¡»æ˜¯`"nccl"`, `"mpi"`, `"ccl"`,
    `"gloo"`, `"hccl"`ä¹‹ä¸€ã€‚'
- en: '`tpu_num_cores` (`int`, *optional*) â€” When training on TPU, the number of TPU
    cores (automatically passed by launcher script).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tpu_num_cores` (`int`, *optional*) â€” åœ¨TPUä¸Šè®­ç»ƒæ—¶ï¼ŒTPUæ ¸å¿ƒçš„æ•°é‡ï¼ˆç”±å¯åŠ¨è„šæœ¬è‡ªåŠ¨ä¼ é€’ï¼‰ã€‚'
- en: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) â€” Whether
    to drop the last incomplete batch (if the length of the dataset is not divisible
    by the batch size) or not.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ï¼ˆå¦‚æœæ•°æ®é›†çš„é•¿åº¦ä¸æ˜¯æ‰¹æ¬¡å¤§å°çš„æ•´æ•°å€ï¼‰ã€‚'
- en: '`eval_steps` (`int` or `float`, *optional*) â€” Number of update steps between
    two evaluations if `evaluation_strategy="steps"`. Will default to the same value
    as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`.
    If smaller than 1, will be interpreted as ratio of total training steps.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps` (`int` or `float`, *optional*) â€” å¦‚æœ`evaluation_strategy="steps"`ï¼Œåˆ™ä¸¤æ¬¡è¯„ä¼°ä¹‹é—´çš„æ›´æ–°æ­¥æ•°ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºä¸`logging_steps`ç›¸åŒçš„å€¼ã€‚åº”ä¸ºèŒƒå›´ä¸º`[0,1)`çš„æ•´æ•°æˆ–æµ®ç‚¹æ•°ã€‚å¦‚æœå°äº1ï¼Œåˆ™å°†è§£é‡Šä¸ºæ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ç‡ã€‚'
- en: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) â€” Number of subprocesses
    to use for data loading (PyTorch only). 0 means that the data will be loaded in
    the main process.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) â€” ç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°ï¼ˆä»…é€‚ç”¨äºPyTorchï¼‰ã€‚0è¡¨ç¤ºæ•°æ®å°†åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½ã€‚'
- en: '`past_index` (`int`, *optional*, defaults to -1) â€” Some models like [TransformerXL](../model_doc/transformerxl)
    or [XLNet](../model_doc/xlnet) can make use of the past hidden states for their
    predictions. If this argument is set to a positive int, the `Trainer` will use
    the corresponding output (usually index 2) as the past state and feed it to the
    model at the next training step under the keyword argument `mems`.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_index` (`int`, *optional*, defaults to -1`) â€” ä¸€äº›æ¨¡å‹ï¼ˆå¦‚[TransformerXL](../model_doc/transformerxl)æˆ–[XLNet](../model_doc/xlnet)ï¼‰å¯ä»¥åˆ©ç”¨è¿‡å»çš„éšè—çŠ¶æ€è¿›è¡Œé¢„æµ‹ã€‚å¦‚æœå°†æ­¤å‚æ•°è®¾ç½®ä¸ºæ­£æ•´æ•°ï¼Œåˆ™`Trainer`å°†ä½¿ç”¨ç›¸åº”çš„è¾“å‡ºï¼ˆé€šå¸¸ä¸ºç´¢å¼•2ï¼‰ä½œä¸ºè¿‡å»çŠ¶æ€ï¼Œå¹¶åœ¨ä¸‹ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­å°†å…¶ä½œä¸ºå…³é”®å­—å‚æ•°`mems`æä¾›ç»™æ¨¡å‹ã€‚'
- en: '`run_name` (`str`, *optional*) â€” A descriptor for the run. Typically used for
    [wandb](https://www.wandb.com/) and [mlflow](https://www.mlflow.org/) logging.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run_name` (`str`, *optional*) â€” è¿è¡Œçš„æè¿°ç¬¦ã€‚é€šå¸¸ç”¨äº[wandb](https://www.wandb.com/)å’Œ[mlflow](https://www.mlflow.org/)æ—¥å¿—è®°å½•ã€‚'
- en: '`disable_tqdm` (`bool`, *optional*) â€” Whether or not to disable the tqdm progress
    bars and table of metrics produced by `~notebook.NotebookTrainingTracker` in Jupyter
    Notebooks. Will default to `True` if the logging level is set to warn or lower
    (default), `False` otherwise.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_tqdm` (`bool`, *optional*) â€” æ˜¯å¦ç¦ç”¨Jupyterç¬”è®°æœ¬ä¸­ç”±`~notebook.NotebookTrainingTracker`ç”Ÿæˆçš„tqdmè¿›åº¦æ¡å’ŒæŒ‡æ ‡è¡¨ã€‚å¦‚æœæ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºwarnæˆ–æ›´ä½ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œåˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚'
- en: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to automatically remove the columns unused by the model forward method.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è‡ªåŠ¨åˆ é™¤æ¨¡å‹å‰å‘æ–¹æ³•æœªä½¿ç”¨çš„åˆ—ã€‚'
- en: '`label_names` (`List[str]`, *optional*) â€” The list of keys in your dictionary
    of inputs that correspond to the labels.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_names` (`List[str]`, *å¯é€‰*) â€” æ‚¨çš„è¾“å…¥å­—å…¸ä¸­ä¸æ ‡ç­¾å¯¹åº”çš„é”®åˆ—è¡¨ã€‚'
- en: Will eventually default to the list of argument names accepted by the model
    that contain the word â€œlabelâ€, except if the model used is one of the `XxxForQuestionAnswering`
    in which case it will also include the `["start_positions", "end_positions"]`
    keys.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå°†é»˜è®¤ä¸ºæ¨¡å‹æ¥å—çš„å‚æ•°åç§°åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å•è¯â€œlabelâ€ï¼Œé™¤éä½¿ç”¨çš„æ¨¡å‹æ˜¯ `XxxForQuestionAnswering` ä¹‹ä¸€ï¼Œé‚£ä¹ˆè¿˜å°†åŒ…æ‹¬
    `["start_positions", "end_positions"]` é”®ã€‚
- en: '`load_best_model_at_end` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to load the best model found during training at the end of training. When
    this option is enabled, the best checkpoint will always be saved. See [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)
    for more.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_best_model_at_end` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œå°†å§‹ç»ˆä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
    [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)ã€‚'
- en: When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`,
    and in the case it is â€œstepsâ€, `save_steps` must be a round multiple of `eval_steps`.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“è®¾ç½®ä¸º `True` æ—¶ï¼Œå‚æ•° `save_strategy` éœ€è¦ä¸ `evaluation_strategy` ç›¸åŒï¼Œå¹¶ä¸”åœ¨å…¶ä¸º â€œstepsâ€
    çš„æƒ…å†µä¸‹ï¼Œ`save_steps` å¿…é¡»æ˜¯ `eval_steps` çš„æ•´æ•°å€ã€‚
- en: '`metric_for_best_model` (`str`, *optional*) â€” Use in conjunction with `load_best_model_at_end`
    to specify the metric to use to compare two different models. Must be the name
    of a metric returned by the evaluation with or without the prefix `"eval_"`. Will
    default to `"loss"` if unspecified and `load_best_model_at_end=True` (to use the
    evaluation loss).'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_for_best_model` (`str`, *å¯é€‰*) â€” ä¸ `load_best_model_at_end` ç»“åˆä½¿ç”¨ï¼ŒæŒ‡å®šç”¨äºæ¯”è¾ƒä¸¤ä¸ªä¸åŒæ¨¡å‹çš„æŒ‡æ ‡ã€‚å¿…é¡»æ˜¯è¯„ä¼°è¿”å›çš„æŒ‡æ ‡çš„åç§°ï¼Œå¸¦æœ‰æˆ–ä¸å¸¦æœ‰å‰ç¼€
    `"eval_"`ã€‚å¦‚æœæœªæŒ‡å®šä¸” `load_best_model_at_end=True`ï¼Œå°†é»˜è®¤ä¸º `"loss"`ï¼ˆä½¿ç”¨è¯„ä¼°æŸå¤±ï¼‰ã€‚'
- en: If you set this value, `greater_is_better` will default to `True`. Donâ€™t forget
    to set it to `False` if your metric is better when lower.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè®¾ç½®äº†æ­¤å€¼ï¼Œ`greater_is_better` å°†é»˜è®¤ä¸º `True`ã€‚ä¸è¦å¿˜è®°ï¼Œå¦‚æœæ‚¨çš„æŒ‡æ ‡åœ¨è¾ƒä½æ—¶æ›´å¥½ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º `False`ã€‚
- en: '`greater_is_better` (`bool`, *optional*) â€” Use in conjunction with `load_best_model_at_end`
    and `metric_for_best_model` to specify if better models should have a greater
    metric or not. Will default to:'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`greater_is_better` (`bool`, *å¯é€‰*) â€” ä¸ `load_best_model_at_end` å’Œ `metric_for_best_model`
    ç»“åˆä½¿ç”¨ï¼ŒæŒ‡å®šæ›´å¥½çš„æ¨¡å‹æ˜¯å¦åº”å…·æœ‰æ›´å¤§çš„æŒ‡æ ‡ã€‚é»˜è®¤ä¸ºï¼š'
- en: '`True` if `metric_for_best_model` is set to a value that isnâ€™t `"loss"` or
    `"eval_loss"`.'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœ `metric_for_best_model` è®¾ç½®ä¸ºä¸æ˜¯ `"loss"` æˆ– `"eval_loss"` çš„å€¼ï¼Œåˆ™ä¸º `True`ã€‚
- en: '`False` if `metric_for_best_model` is not set, or set to `"loss"` or `"eval_loss"`.'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½® `metric_for_best_model`ï¼Œæˆ–è®¾ç½®ä¸º `"loss"` æˆ– `"eval_loss"`ï¼Œåˆ™ä¸º `False`ã€‚
- en: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) â€” When resuming
    training, whether or not to skip the epochs and batches to get the data loading
    at the same stage as in the previous training. If set to `True`, the training
    will begin faster (as that skipping step can take a long time) but will not yield
    the same results as the interrupted training would have.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_data_skip` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” åœ¨æ¢å¤è®­ç»ƒæ—¶ï¼Œæ˜¯å¦è·³è¿‡æ‰¹æ¬¡ä»¥ä½¿æ•°æ®åŠ è½½ä¸å…ˆå‰è®­ç»ƒä¸­çš„é˜¶æ®µç›¸åŒã€‚å¦‚æœè®¾ç½®ä¸º
    `True`ï¼Œè®­ç»ƒå°†æ›´å¿«å¼€å§‹ï¼ˆå› ä¸ºè·³è¿‡æ­¥éª¤å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰ï¼Œä½†ä¸ä¼šäº§ç”Ÿä¸ä¸­æ–­è®­ç»ƒç›¸åŒçš„ç»“æœã€‚'
- en: '`fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''''`)
    â€” Use PyTorch Distributed Parallel Training (in distributed training only).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp` (`bool`, `str` æˆ– `FSDPOption` åˆ—è¡¨, *å¯é€‰*, é»˜è®¤ä¸º `''''`) â€” ä½¿ç”¨ PyTorch åˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒï¼ˆä»…åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼‰ã€‚'
- en: 'A list of options along the following:'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ç³»åˆ—é€‰é¡¹ï¼š
- en: '`"full_shard"`: Shard parameters, gradients and optimizer states.'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"full_shard"`: åˆ†ç‰‡å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚'
- en: '`"shard_grad_op"`: Shard optimizer states and gradients.'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"shard_grad_op"`: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ã€‚'
- en: '`"hybrid_shard"`: Apply `FULL_SHARD` within a node, and replicate parameters
    across nodes.'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard"`: åœ¨èŠ‚ç‚¹å†…åº”ç”¨ `FULL_SHARD`ï¼Œå¹¶åœ¨èŠ‚ç‚¹ä¹‹é—´å¤åˆ¶å‚æ•°ã€‚'
- en: '`"hybrid_shard_zero2"`: Apply `SHARD_GRAD_OP` within a node, and replicate
    parameters across nodes.'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard_zero2"`: åœ¨èŠ‚ç‚¹å†…åº”ç”¨ `SHARD_GRAD_OP`ï¼Œå¹¶åœ¨èŠ‚ç‚¹ä¹‹é—´å¤åˆ¶å‚æ•°ã€‚'
- en: '`"offload"`: Offload parameters and gradients to CPUs (only compatible with
    `"full_shard"` and `"shard_grad_op"`).'
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"offload"`: å°†å‚æ•°å’Œæ¢¯åº¦å¸è½½åˆ° CPUï¼ˆä»…ä¸ `"full_shard"` å’Œ `"shard_grad_op"` å…¼å®¹ï¼‰ã€‚'
- en: '`"auto_wrap"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.'
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"auto_wrap"`: ä½¿ç”¨ `default_auto_wrap_policy` è‡ªåŠ¨é€’å½’åŒ…è£…å±‚ä¸ FSDPã€‚'
- en: '`fsdp_config` (`str` or `dict`, *optional*) â€” Config to be used with fsdp (Pytorch
    Distributed Parallel Training). The value is either a location of fsdp json config
    file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp_config` (`str` æˆ– `dict`, *å¯é€‰*) â€” ç”¨äº fsdpï¼ˆPytorch åˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒï¼‰çš„é…ç½®ã€‚è¯¥å€¼å¯ä»¥æ˜¯ fsdp
    json é…ç½®æ–‡ä»¶çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œ`fsdp_config.json`ï¼‰æˆ–å·²åŠ è½½çš„ json æ–‡ä»¶ä½œä¸º `dict`ã€‚'
- en: 'A List of config and its options:'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é…ç½®åŠå…¶é€‰é¡¹åˆ—è¡¨ï¼š
- en: 'min_num_params (`int`, *optional*, defaults to `0`): FSDPâ€™s minimum number
    of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'min_num_params (`int`, *å¯é€‰*, é»˜è®¤ä¸º `0`): FSDP é»˜è®¤è‡ªåŠ¨åŒ…è£…çš„å‚æ•°æœ€å°æ•°é‡ã€‚ ï¼ˆä»…åœ¨ä¼ é€’ `fsdp` å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚'
- en: 'transformer_layer_cls_to_wrap (`List[str]`, *optional*): List of transformer
    layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`, `T5Block`
    â€¦ (useful only when `fsdp` flag is passed).'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'transformer_layer_cls_to_wrap (`List[str]`, *å¯é€‰*): è¦åŒ…è£…çš„ transformer å±‚ç±»åç§°åˆ—è¡¨ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼Œä¾‹å¦‚ï¼Œ`BertLayer`ã€`GPTJBlock`ã€`T5Block`
    â€¦ï¼ˆä»…åœ¨ä¼ é€’ `fsdp` æ ‡å¿—æ—¶æœ‰ç”¨ï¼‰ã€‚'
- en: backward_prefetch (`str`, *optional*) FSDPâ€™s backward prefetch mode. Controls
    when to prefetch next set of parameters (useful only when `fsdp` field is passed).
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: backward_prefetch (`str`, *å¯é€‰*) FSDP çš„åå‘é¢„å–æ¨¡å¼ã€‚æ§åˆ¶ä½•æ—¶é¢„å–ä¸‹ä¸€ç»„å‚æ•°ï¼ˆä»…åœ¨ä¼ é€’ `fsdp` å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚
- en: 'A list of options along the following:'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ç³»åˆ—é€‰é¡¹ï¼š
- en: '`"backward_pre"` : Prefetches the next set of parameters before the current
    set of parameterâ€™s gradient computation.'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_pre"` : åœ¨å½“å‰å‚æ•°çš„æ¢¯åº¦è®¡ç®—ä¹‹å‰ï¼Œé¢„å–ä¸‹ä¸€ç»„å‚æ•°ã€‚'
- en: '`"backward_post"` : This prefetches the next set of parameters after the current
    set of parameterâ€™s gradient computation.'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_post"` : åœ¨å½“å‰å‚æ•°çš„æ¢¯åº¦è®¡ç®—ä¹‹åï¼Œé¢„å–ä¸‹ä¸€ç»„å‚æ•°ã€‚'
- en: forward_prefetch (`bool`, *optional*, defaults to `False`) FSDPâ€™s forward prefetch
    mode (useful only when `fsdp` field is passed). If `"True"`, then FSDP explicitly
    prefetches the next upcoming all-gather while executing in the forward pass.
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: forward_prefetchï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰FSDPçš„å‰å‘é¢„å–æ¨¡å¼ï¼ˆä»…åœ¨ä¼ é€’`fsdp`å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚å¦‚æœä¸º`"True"`ï¼Œåˆ™FSDPåœ¨æ‰§è¡Œå‰å‘ä¼ é€’æ—¶æ˜ç¡®é¢„å–ä¸‹ä¸€ä¸ªå³å°†åˆ°æ¥çš„å…¨èšé›†ã€‚
- en: limit_all_gathers (`bool`, *optional*, defaults to `False`) FSDPâ€™s limit_all_gathers
    (useful only when `fsdp` field is passed). If `"True"`, FSDP explicitly synchronizes
    the CPU thread to prevent too many in-flight all-gathers.
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: limit_all_gathersï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰FSDPçš„limit_all_gathersï¼ˆä»…åœ¨ä¼ é€’`fsdp`å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚å¦‚æœä¸º`"True"`ï¼ŒFSDPæ˜ç¡®åŒæ­¥CPUçº¿ç¨‹ï¼Œä»¥é˜²æ­¢å¤ªå¤šçš„in-flight
    all-gathersã€‚
- en: use_orig_params (`bool`, *optional*, defaults to `True`) If `"True"`, allows
    non-uniform `requires_grad` during init, which means support for interspersed
    frozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning.
    Please refer this [blog]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: use_orig_paramsï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰å¦‚æœä¸º`"True"`ï¼Œå…è®¸åœ¨åˆå§‹åŒ–æœŸé—´ä½¿ç”¨éå‡åŒ€çš„`requires_grad`ï¼Œè¿™æ„å‘³ç€æ”¯æŒäº¤æ›¿å†»ç»“å’Œå¯è®­ç»ƒçš„å‚æ•°ã€‚åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒç­‰æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ã€‚è¯·å‚è€ƒè¿™ä¸ª[åšå®¢]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
- en: sync_module_states (`bool`, *optional*, defaults to `True`) If `"True"`, each
    individually wrapped FSDP unit will broadcast module parameters from rank 0 to
    ensure they are the same across all ranks after initialization
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: sync_module_statesï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰å¦‚æœä¸º`"True"`ï¼Œæ¯ä¸ªå•ç‹¬åŒ…è£…çš„FSDPå•å…ƒå°†ä»rank 0å¹¿æ’­æ¨¡å—å‚æ•°ï¼Œä»¥ç¡®ä¿å®ƒä»¬åœ¨åˆå§‹åŒ–ååœ¨æ‰€æœ‰rankä¸­æ˜¯ç›¸åŒçš„
- en: 'activation_checkpointing (`bool`, *optional*, defaults to `False`): If `"True"`,
    activation checkpointing is a technique to reduce memory usage by clearing activations
    of certain layers and recomputing them during a backward pass. Effectively, this
    trades extra computation time for reduced memory usage.'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: activation_checkpointingï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šå¦‚æœä¸º`"True"`ï¼Œæ¿€æ´»æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§é€šè¿‡æ¸…é™¤æŸäº›å±‚çš„æ¿€æ´»å¹¶åœ¨å‘åä¼ é€’æœŸé—´é‡æ–°è®¡ç®—å®ƒä»¬æ¥å‡å°‘å†…å­˜ä½¿ç”¨çš„æŠ€æœ¯ã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯ä»¥é¢å¤–çš„è®¡ç®—æ—¶é—´æ¢å–å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: 'xla (`bool`, *optional*, defaults to `False`): Whether to use PyTorch/XLA Fully
    Sharded Data Parallel Training. This is an experimental feature and its API may
    evolve in the future.'
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xlaï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šæ˜¯å¦ä½¿ç”¨PyTorch/XLAå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå…¶APIå¯èƒ½ä¼šåœ¨æœªæ¥å‘ç”Ÿå˜åŒ–ã€‚
- en: xla_fsdp_settings (`dict`, *optional*) The value is a dictionary which stores
    the XLA FSDP wrapping parameters.
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_settingsï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰è¯¥å€¼æ˜¯ä¸€ä¸ªå­˜å‚¨XLA FSDPåŒ…è£…å‚æ•°çš„å­—å…¸ã€‚
- en: For a complete list of options, please see [here](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ‰å…³å®Œæ•´çš„é€‰é¡¹åˆ—è¡¨ï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)ã€‚
- en: 'xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`): Will use gradient
    checkpointing over each nested XLA FSDP wrapped layer. This setting can only be
    used when the xla flag is set to true, and an auto wrapping policy is specified
    through fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_grad_ckptï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šå°†åœ¨æ¯ä¸ªåµŒå¥—çš„XLA FSDPåŒ…è£…å±‚ä¸Šä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚æ­¤è®¾ç½®ä»…åœ¨å°†xlaæ ‡å¿—è®¾ç½®ä¸ºtrueå¹¶é€šè¿‡fsdp_min_num_paramsæˆ–fsdp_transformer_layer_cls_to_wrapæŒ‡å®šè‡ªåŠ¨åŒ…è£…ç­–ç•¥æ—¶æ‰èƒ½ä½¿ç”¨ã€‚
- en: '`deepspeed` (`str` or `dict`, *optional*) â€” Use [Deepspeed](https://github.com/microsoft/deepspeed).
    This is an experimental feature and its API may evolve in the future. The value
    is either the location of DeepSpeed json config file (e.g., `ds_config.json`)
    or an already loaded json file as a `dict`â€'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepspeed`ï¼ˆ`str`æˆ–`dict`ï¼Œ*å¯é€‰*ï¼‰â€” ä½¿ç”¨[Deepspeed](https://github.com/microsoft/deepspeed)ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå…¶APIå¯èƒ½ä¼šåœ¨æœªæ¥å‘ç”Ÿå˜åŒ–ã€‚è¯¥å€¼å¯ä»¥æ˜¯DeepSpeed
    jsoné…ç½®æ–‡ä»¶çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œ`ds_config.json`ï¼‰æˆ–å·²åŠ è½½çš„jsonæ–‡ä»¶ä½œä¸º`dict`â€'
- en: '`label_smoothing_factor` (`float`, *optional*, defaults to 0.0) â€” The label
    smoothing factor to use. Zero means no label smoothing, otherwise the underlying
    onehot-encoded labels are changed from 0s and 1s to `label_smoothing_factor/num_labels`
    and `1 - label_smoothing_factor + label_smoothing_factor/num_labels` respectively.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” è¦ä½¿ç”¨çš„æ ‡ç­¾å¹³æ»‘å› å­ã€‚é›¶è¡¨ç¤ºä¸è¿›è¡Œæ ‡ç­¾å¹³æ»‘ï¼Œå¦åˆ™åº•å±‚çš„onehotç¼–ç æ ‡ç­¾å°†ä»0å’Œ1æ›´æ”¹ä¸º`label_smoothing_factor/num_labels`å’Œ`1
    - label_smoothing_factor + label_smoothing_factor/num_labels`ã€‚'
- en: '`debug` (`str` or list of `DebugOption`, *optional*, defaults to `""`) â€” Enable
    one or more debug features. This is an experimental feature.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug`ï¼ˆ`str`æˆ–`DebugOption`åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`""`ï¼‰â€” å¯ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªè°ƒè¯•åŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ã€‚'
- en: 'Possible options are:'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯èƒ½çš„é€‰é¡¹åŒ…æ‹¬ï¼š
- en: '`"underflow_overflow"`: detects overflow in modelâ€™s input/outputs and reports
    the last frames that led to the event'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"underflow_overflow"`ï¼šæ£€æµ‹æ¨¡å‹è¾“å…¥/è¾“å‡ºä¸­çš„æº¢å‡ºå¹¶æŠ¥å‘Šå¯¼è‡´äº‹ä»¶çš„æœ€åå¸§'
- en: '`"tpu_metrics_debug"`: print debug metrics on TPU'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"tpu_metrics_debug"`ï¼šåœ¨TPUä¸Šæ‰“å°è°ƒè¯•æŒ‡æ ‡'
- en: The options should be separated by whitespaces.
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é€‰é¡¹åº”è¯¥ç”¨ç©ºæ ¼åˆ†éš”ã€‚
- en: '`optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    â€” The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused,
    adamw_anyprecision or adafactor.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim`ï¼ˆ`str`æˆ–`training_args.OptimizerNames`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"adamw_torch"`ï¼‰â€” è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼šadamw_hfã€adamw_torchã€adamw_torch_fusedã€adamw_apex_fusedã€adamw_anyprecisionæˆ–adafactorã€‚'
- en: '`optim_args` (`str`, *optional*) â€” Optional arguments that are supplied to
    AnyPrecisionAdamW.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim_args`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” ä¾›AnyPrecisionAdamWæä¾›çš„å¯é€‰å‚æ•°ã€‚'
- en: '`group_by_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to group together samples of roughly the same length in the training dataset (to
    minimize padding applied and be more efficient). Only useful if applying dynamic
    padding.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_by_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å°†å¤§è‡´ç›¸åŒé•¿åº¦çš„æ ·æœ¬åˆ†ç»„åœ¨ä¸€èµ·ï¼ˆä»¥æœ€å°åŒ–åº”ç”¨çš„å¡«å……å¹¶æé«˜æ•ˆç‡ï¼‰ã€‚ä»…åœ¨åº”ç”¨åŠ¨æ€å¡«å……æ—¶æœ‰ç”¨ã€‚'
- en: '`length_column_name` (`str`, *optional*, defaults to `"length"`) â€” Column name
    for precomputed lengths. If the column exists, grouping by length will use these
    values rather than computing them on train startup. Ignored unless `group_by_length`
    is `True` and the dataset is an instance of `Dataset`.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_column_name` (`str`, *optional*, defaults to `"length"`) â€” é¢„å…ˆè®¡ç®—é•¿åº¦çš„åˆ—åã€‚å¦‚æœè¯¥åˆ—å­˜åœ¨ï¼ŒæŒ‰é•¿åº¦åˆ†ç»„å°†ä½¿ç”¨è¿™äº›å€¼è€Œä¸æ˜¯åœ¨è®­ç»ƒå¯åŠ¨æ—¶è®¡ç®—å®ƒä»¬ã€‚ä»…åœ¨
    `group_by_length` ä¸º `True` ä¸”æ•°æ®é›†æ˜¯ `Dataset` çš„å®ä¾‹æ—¶æ‰ä¼šè¢«å¿½ç•¥ã€‚'
- en: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) â€” The list
    of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
    `"clearml"`, `"codecarbon"`, `"comet_ml"`, `"dagshub"`, `"dvclive"`, `"flyte"`,
    `"mlflow"`, `"neptune"`, `"tensorboard"`, and `"wandb"`. Use `"all"` to report
    to all integrations installed, `"none"` for no integrations.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) â€” æŠ¥å‘Šç»“æœå’Œæ—¥å¿—çš„é›†æˆåˆ—è¡¨ã€‚æ”¯æŒçš„å¹³å°æœ‰
    `"azure_ml"`ã€`"clearml"`ã€`"codecarbon"`ã€`"comet_ml"`ã€`"dagshub"`ã€`"dvclive"`ã€`"flyte"`ã€`"mlflow"`ã€`"neptune"`ã€`"tensorboard"`
    å’Œ `"wandb"`ã€‚ä½¿ç”¨ `"all"` æŠ¥å‘Šåˆ°æ‰€æœ‰å·²å®‰è£…çš„é›†æˆï¼Œä½¿ç”¨ `"none"` ä¸æŠ¥å‘Šåˆ°ä»»ä½•é›†æˆã€‚'
- en: '`ddp_find_unused_parameters` (`bool`, *optional*) â€” When using distributed
    training, the value of the flag `find_unused_parameters` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_find_unused_parameters` (`bool`, *optional*) â€” åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™ `DistributedDataParallel`
    çš„ `find_unused_parameters` æ ‡å¿—çš„å€¼ã€‚å¦‚æœä½¿ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™é»˜è®¤ä¸º `False`ï¼Œå¦åˆ™ä¸º `True`ã€‚'
- en: '`ddp_bucket_cap_mb` (`int`, *optional*) â€” When using distributed training,
    the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_bucket_cap_mb` (`int`, *optional*) â€” åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™ `DistributedDataParallel`
    çš„ `bucket_cap_mb` æ ‡å¿—çš„å€¼ã€‚'
- en: '`ddp_broadcast_buffers` (`bool`, *optional*) â€” When using distributed training,
    the value of the flag `broadcast_buffers` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_broadcast_buffers` (`bool`, *optional*) â€” åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™ `DistributedDataParallel`
    çš„ `broadcast_buffers` æ ‡å¿—çš„å€¼ã€‚å¦‚æœä½¿ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™é»˜è®¤ä¸º `False`ï¼Œå¦åˆ™ä¸º `True`ã€‚'
- en: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) â€” Whether
    you want to pin memory in data loaders or not. Will default to `True`.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¦åœ¨æ•°æ®åŠ è½½å™¨ä¸­å›ºå®šå†…å­˜ã€‚é»˜è®¤ä¸º
    `True`ã€‚'
- en: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) â€”
    If True, the data loader will not shut down the worker processes after a dataset
    has been consumed once. This allows to maintain the workers Dataset instances
    alive. Can potentially speed up training, but will increase RAM usage. Will default
    to `False`.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) â€”
    å¦‚æœä¸º Trueï¼Œåˆ™æ•°æ®åŠ è½½å™¨åœ¨æ•°æ®é›†è¢«æ¶ˆè€—ä¸€æ¬¡åä¸ä¼šå…³é—­å·¥ä½œè¿›ç¨‹ã€‚è¿™å…è®¸ä¿æŒå·¥ä½œè¿›ç¨‹çš„æ•°æ®é›†å®ä¾‹å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚å¯èƒ½ä¼šåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡ã€‚é»˜è®¤ä¸º
    `False`ã€‚'
- en: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) â€” Whether to
    skip adding of memory profiler reports to metrics. This is skipped by default
    because it slows down the training and evaluation speed.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è·³è¿‡å°†å†…å­˜åˆ†æå™¨æŠ¥å‘Šæ·»åŠ åˆ°æŒ‡æ ‡ä¸­ã€‚é»˜è®¤æƒ…å†µä¸‹ä¼šè·³è¿‡è¿™ä¸€æ­¥ï¼Œå› ä¸ºå®ƒä¼šå‡æ…¢è®­ç»ƒå’Œè¯„ä¼°é€Ÿåº¦ã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push the model to the Hub every time the model is saved. If this is activated,
    `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`)
    and the content will be pushed each time a save is triggered (depending on your
    `save_strategy`). Calling [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    will also trigger a push.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶å°†æ¨¡å‹æ¨é€åˆ°
    Hubã€‚å¦‚æœæ¿€æ´»äº†æ­¤é€‰é¡¹ï¼Œ`output_dir` å°†å¼€å§‹ä¸€ä¸ªä¸ä»“åº“åŒæ­¥çš„ git ç›®å½•ï¼ˆç”± `hub_model_id` ç¡®å®šï¼‰ï¼Œå¹¶ä¸”æ¯æ¬¡è§¦å‘ä¿å­˜æ—¶éƒ½ä¼šæ¨é€å†…å®¹ï¼ˆå–å†³äºæ‚¨çš„
    `save_strategy`ï¼‰ã€‚è°ƒç”¨ [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    ä¹Ÿä¼šè§¦å‘æ¨é€ã€‚'
- en: If `output_dir` exists, it needs to be a local clone of the repository to which
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will be pushed.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ `output_dir` å­˜åœ¨ï¼Œåˆ™å®ƒéœ€è¦æ˜¯å°† [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    å°†è¦æ¨é€åˆ°çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚
- en: '`resume_from_checkpoint` (`str`, *optional*) â€” The path to a folder with a
    valid checkpoint for your model. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_from_checkpoint` (`str`, *optional*) â€” æ‚¨çš„æ¨¡å‹çš„æœ‰æ•ˆæ£€æŸ¥ç‚¹æ‰€åœ¨æ–‡ä»¶å¤¹çš„è·¯å¾„ã€‚è¿™ä¸ªå‚æ•°ä¸ä¼šç›´æ¥è¢«
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯æ‰“ç®—ç”±æ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ä½¿ç”¨ã€‚æŸ¥çœ‹ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)
    ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚'
- en: '`hub_model_id` (`str`, *optional*) â€” The name of the repository to keep in
    sync with the local *output_dir*. It can be a simple model ID in which case the
    model will be pushed in your namespace. Otherwise it should be the whole repository
    name, for instance `"user_name/model"`, which allows you to push to an organization
    you are a member of with `"organization_name/model"`. Will default to `user_name/output_dir_name`
    with *output_dir_name* being the name of `output_dir`.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_model_id` (`str`, *optional*) â€” è¦ä¸æœ¬åœ° *output_dir* ä¿æŒåŒæ­¥çš„ä»“åº“åç§°ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹
    IDï¼Œæ­¤æ—¶æ¨¡å‹å°†è¢«æ¨é€åˆ°æ‚¨çš„å‘½åç©ºé—´ã€‚å¦åˆ™ï¼Œå®ƒåº”è¯¥æ˜¯æ•´ä¸ªä»“åº“åç§°ï¼Œä¾‹å¦‚ `"user_name/model"`ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥æ¨é€åˆ°æ‚¨æ‰€å±çš„ç»„ç»‡ï¼Œå¦‚ `"organization_name/model"`ã€‚é»˜è®¤ä¸º
    `user_name/output_dir_name`ï¼Œå…¶ä¸­ *output_dir_name* æ˜¯ `output_dir` çš„åç§°ã€‚'
- en: Will default to the name of `output_dir`.
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é»˜è®¤ä¸º `output_dir` çš„åç§°ã€‚
- en: '`hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    â€” Defines the scope of what is pushed to the Hub and when. Possible values are:'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    â€” å®šä¹‰æ¨é€åˆ° Hub çš„èŒƒå›´å’Œæ—¶é—´ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'
- en: '`"end"`: push the model, its configuration, the tokenizer (if passed along
    to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card when the [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    method is called.'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"end"`: å½“è°ƒç”¨ [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    æ–¹æ³•æ—¶ï¼Œä¼šæ¨é€æ¨¡å‹ã€å…¶é…ç½®ã€åˆ†è¯å™¨ï¼ˆå¦‚æœä¼ é€’ç»™ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ä»¥åŠæ¨¡å‹å¡çš„è‰ç¨¿ã€‚'
- en: '`"every_save"`: push the model, its configuration, the tokenizer (if passed
    along to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card each time there is a model save. The pushes are asynchronous
    to not block training, and in case the save are very frequent, a new push is only
    attempted if the previous one is finished. A last push is made with the final
    model at the end of training.'
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"every_save"`: æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼Œéƒ½ä¼šæ¨é€æ¨¡å‹ã€å…¶é…ç½®ã€åˆ†è¯å™¨ï¼ˆå¦‚æœä¼ é€’ç»™ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ä»¥åŠæ¨¡å‹å¡çš„è‰ç¨¿ã€‚æ¨é€æ˜¯å¼‚æ­¥çš„ï¼Œä»¥é¿å…é˜»å¡è®­ç»ƒï¼Œå¦‚æœä¿å­˜éå¸¸é¢‘ç¹ï¼Œåˆ™åªæœ‰åœ¨ä¸Šä¸€ä¸ªæ¨é€å®Œæˆåæ‰ä¼šå°è¯•æ–°çš„æ¨é€ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œä¼šä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œæœ€åä¸€æ¬¡æ¨é€ã€‚'
- en: '`"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed
    in a subfolder named last-checkpoint, allowing you to resume training easily with
    `trainer.train(resume_from_checkpoint="last-checkpoint")`.'
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"checkpoint"`: ç±»ä¼¼äº `"every_save"`ï¼Œä½†æœ€æ–°çš„æ£€æŸ¥ç‚¹ä¹Ÿä¼šè¢«æ¨é€åˆ°åä¸º last-checkpoint çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œä½¿æ‚¨å¯ä»¥é€šè¿‡
    `trainer.train(resume_from_checkpoint="last-checkpoint")` è½»æ¾æ¢å¤è®­ç»ƒã€‚'
- en: '`"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like
    they appear in the output folder (so you will get one checkpoint folder per folder
    in your final repository)'
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all_checkpoints"`: ç±»ä¼¼äº `"checkpoint"`ï¼Œä½†æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½åƒå®ƒä»¬å‡ºç°åœ¨è¾“å‡ºæ–‡ä»¶å¤¹ä¸­ä¸€æ ·è¢«æ¨é€ï¼ˆå› æ­¤æ‚¨å°†åœ¨æœ€ç»ˆå­˜å‚¨åº“ä¸­è·å¾—ä¸€ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ï¼‰ã€‚'
- en: '`hub_token` (`str`, *optional*) â€” The token to use to push the model to the
    Hub. Will default to the token in the cache folder obtained with `huggingface-cli
    login`.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_token` (`str`, *optional*) â€” ç”¨äºå°†æ¨¡å‹æ¨é€åˆ° Hub çš„ä»¤ç‰Œã€‚å°†é»˜è®¤ä½¿ç”¨é€šè¿‡ `huggingface-cli
    login` è·å–çš„ç¼“å­˜æ–‡ä»¶å¤¹ä¸­çš„ä»¤ç‰Œã€‚'
- en: '`hub_private_repo` (`bool`, *optional*, defaults to `False`) â€” If True, the
    Hub repo will be set to private.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_private_repo` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™ Hub å­˜å‚¨åº“å°†è®¾ç½®ä¸ºç§æœ‰ã€‚'
- en: '`hub_always_push` (`bool`, *optional*, defaults to `False`) â€” Unless this is
    `True`, the `Trainer` will skip pushing a checkpoint when the previous push is
    not finished.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_always_push` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” é™¤éä¸º `True`ï¼Œå¦åˆ™å½“ä¸Šä¸€ä¸ªæ¨é€æœªå®Œæˆæ—¶ï¼Œ`Trainer`
    å°†è·³è¿‡æ¨é€æ£€æŸ¥ç‚¹ã€‚'
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `False`) â€” If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜ï¼Œä½†ä¼šå¯¼è‡´åå‘ä¼ æ’­é€Ÿåº¦å˜æ…¢ã€‚'
- en: '`gradient_checkpointing_kwargs` (`dict`, *optional*, defaults to `None`) â€”
    Key word arguments to be passed to the `gradient_checkpointing_enable` method.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing_kwargs` (`dict`, *optional*, é»˜è®¤ä¸º `None`) â€” è¦ä¼ é€’ç»™ `gradient_checkpointing_enable`
    æ–¹æ³•çš„å…³é”®å­—å‚æ•°ã€‚'
- en: '`include_inputs_for_metrics` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the inputs will be passed to the `compute_metrics` function. This is intended
    for metrics that need inputs, predictions and references for scoring calculation
    in Metric class.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_inputs_for_metrics` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°†è¾“å…¥ä¼ é€’ç»™ `compute_metrics`
    å‡½æ•°ã€‚è¿™é€‚ç”¨äºéœ€è¦è¾“å…¥ã€é¢„æµ‹å’Œå‚è€ƒå€¼è¿›è¡Œè¯„åˆ†è®¡ç®—çš„æŒ‡æ ‡ç±»ã€‚'
- en: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) â€” Whether
    to find a batch size that will fit into memory automatically through exponential
    decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed
    (`pip install accelerate`)'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_find_batch_size` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦é€šè¿‡æŒ‡æ•°è¡°å‡è‡ªåŠ¨æ‰¾åˆ°é€‚åˆå†…å­˜çš„æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…
    CUDA å†…å­˜ä¸è¶³é”™è¯¯ã€‚éœ€è¦å®‰è£… accelerate (`pip install accelerate`)ã€‚'
- en: '`full_determinism` (`bool`, *optional*, defaults to `False`) â€” If `True`, [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    is called instead of [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    to ensure reproducible results in distributed training. Important: this will negatively
    impact the performance, so only use it for debugging.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`full_determinism` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œå°†è°ƒç”¨ [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    è€Œä¸æ˜¯ [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    ä»¥ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­è·å¾—å¯é‡ç°çš„ç»“æœã€‚é‡è¦æç¤ºï¼šè¿™å°†å¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå› æ­¤ä»…ç”¨äºè°ƒè¯•ç›®çš„ã€‚'
- en: '`torchdynamo` (`str`, *optional*) â€” If set, the backend compiler for TorchDynamo.
    Possible choices are `"eager"`, `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`,
    `"aot_cudagraphs"`, `"ofi"`, `"fx2trt"`, `"onnxrt"` and `"ipex"`.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchdynamo` (`str`, *optional*) â€” å¦‚æœè®¾ç½®ï¼ŒTorchDynamo çš„åç«¯ç¼–è¯‘å™¨ã€‚å¯èƒ½çš„é€‰æ‹©åŒ…æ‹¬ `"eager"`,
    `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`, `"aot_cudagraphs"`,
    `"ofi"`, `"fx2trt"`, `"onnxrt"` å’Œ `"ipex"`ã€‚'
- en: '`ray_scope` (`str`, *optional*, defaults to `"last"`) â€” The scope to use when
    doing hyperparameter search with Ray. By default, `"last"` will be used. Ray will
    then use the last checkpoint of all trials, compare those, and select the best
    one. However, other options are also available. See the [Ray documentation](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)
    for more options.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray_scope` (`str`, *optional*, é»˜è®¤ä¸º `"last"`) â€” åœ¨ä½¿ç”¨ Ray è¿›è¡Œè¶…å‚æ•°æœç´¢æ—¶è¦ä½¿ç”¨çš„èŒƒå›´ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå°†ä½¿ç”¨
    `"last"`ã€‚ç„¶åï¼ŒRay å°†ä½¿ç”¨æ‰€æœ‰è¯•éªŒçš„æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œæ¯”è¾ƒå®ƒä»¬ï¼Œå¹¶é€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªã€‚ä½†æ˜¯ï¼Œä¹Ÿæœ‰å…¶ä»–é€‰é¡¹å¯ç”¨ã€‚æœ‰å…³æ›´å¤šé€‰é¡¹ï¼Œè¯·å‚é˜… [Ray æ–‡æ¡£](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)ã€‚'
- en: '`ddp_timeout` (`int`, *optional*, defaults to 1800) â€” The timeout for `torch.distributed.init_process_group`
    calls, used to avoid GPU socket timeouts when performing slow operations in distributed
    runnings. Please refer the [PyTorch documentation] ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))
    for more information.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_timeout` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1800) â€” `torch.distributed.init_process_group`è°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼Œç”¨äºé¿å…åœ¨åˆ†å¸ƒå¼è¿è¡Œä¸­æ‰§è¡Œç¼“æ…¢æ“ä½œæ—¶å‡ºç°GPUå¥—æ¥å­—è¶…æ—¶ã€‚è¯·å‚è€ƒ[PyTorchæ–‡æ¡£]
    ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))
    è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '`use_mps_device` (`bool`, *optional*, defaults to `False`) â€” This argument
    is deprecated.`mps` device will be used if it is available similar to `cuda` device.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mps_device` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ­¤å‚æ•°å·²å¼ƒç”¨ã€‚å¦‚æœå¯ç”¨ï¼Œå°†ä½¿ç”¨`mps`è®¾å¤‡ï¼Œç±»ä¼¼äº`cuda`è®¾å¤‡ã€‚'
- en: '`torch_compile` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to compile the model using PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/)
    ç¼–è¯‘æ¨¡å‹ã€‚'
- en: This will use the best defaults for the [`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).
    You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode`
    but we donâ€™t guarantee any of them will work as the support is progressively rolled
    in in PyTorch.
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å°†ä½¿ç”¨[`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile)çš„æœ€ä½³é»˜è®¤å€¼ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å‚æ•°`torch_compile_backend`å’Œ`torch_compile_mode`è‡ªå®šä¹‰é»˜è®¤å€¼ï¼Œä½†æˆ‘ä»¬ä¸èƒ½ä¿è¯å®ƒä»¬ä¸­çš„ä»»ä½•ä¸€ä¸ªä¼šèµ·ä½œç”¨ï¼Œå› ä¸ºæ”¯æŒé€æ¸åœ¨PyTorchä¸­æ¨å‡ºã€‚
- en: This flag and the whole compile API is experimental and subject to change in
    future releases.
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ ‡å¿—å’Œæ•´ä¸ªç¼–è¯‘APIæ˜¯å®éªŒæ€§çš„ï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: '`torch_compile_backend` (`str`, *optional*) â€” The backend to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_backend` (`str`, *å¯é€‰*) â€” åœ¨`torch.compile`ä¸­ä½¿ç”¨çš„åç«¯ã€‚å¦‚æœè®¾ç½®ä¸ºä»»ä½•å€¼ï¼Œ`torch_compile`å°†è®¾ç½®ä¸º`True`ã€‚'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–å¯èƒ½çš„å€¼ï¼Œå¹¶æ³¨æ„å®ƒä»¬å¯èƒ½ä¼šéšç€PyTorchç‰ˆæœ¬çš„å˜åŒ–è€Œæ”¹å˜ã€‚
- en: This flag is experimental and subject to change in future releases.
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ ‡å¿—æ˜¯å®éªŒæ€§çš„ï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: '`torch_compile_mode` (`str`, *optional*) â€” The mode to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_mode` (`str`, *å¯é€‰*) â€” åœ¨`torch.compile`ä¸­ä½¿ç”¨çš„æ¨¡å¼ã€‚å¦‚æœè®¾ç½®ä¸ºä»»ä½•å€¼ï¼Œ`torch_compile`å°†è®¾ç½®ä¸º`True`ã€‚'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–å¯èƒ½çš„å€¼ï¼Œå¹¶æ³¨æ„å®ƒä»¬å¯èƒ½ä¼šéšç€PyTorchç‰ˆæœ¬çš„å˜åŒ–è€Œæ”¹å˜ã€‚
- en: This flag is experimental and subject to change in future releases.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ ‡å¿—æ˜¯å®éªŒæ€§çš„ï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: '`split_batches` (`bool`, *optional*) â€” Whether or not the accelerator should
    split the batches yielded by the dataloaders across the devices during distributed
    training. If'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_batches` (`bool`, *å¯é€‰*) â€” æ˜¯å¦åŠ é€Ÿå™¨åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´åº”è¯¥å°†æ•°æ®åŠ è½½å™¨äº§ç”Ÿçš„æ‰¹æ¬¡åˆ†é…åˆ°è®¾å¤‡ä¸Šã€‚å¦‚æœ'
- en: set to `True`, the actual batch size used will be the same on any kind of distributed
    processes, but it must be a
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸º`True`ï¼Œå®é™…ä½¿ç”¨çš„æ‰¹é‡å¤§å°å°†åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è¿›ç¨‹ä¸Šç›¸åŒï¼Œä½†å¿…é¡»æ˜¯
- en: round multiple of the number of processes you are using (such as GPUs).
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°†ä½¿ç”¨æ‚¨æ­£åœ¨ä½¿ç”¨çš„è¿›ç¨‹æ•°é‡çš„å€æ•°ï¼ˆä¾‹å¦‚GPUï¼‰è¿›è¡Œå››èˆäº”å…¥ã€‚
- en: '`include_tokens_per_second` (`bool`, *optional*) â€” Whether or not to compute
    the number of tokens per second per device for training speed metrics.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_tokens_per_second` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è®¡ç®—æ¯ä¸ªè®¾å¤‡æ¯ç§’çš„æ ‡è®°æ•°ä»¥è·å–è®­ç»ƒé€Ÿåº¦æŒ‡æ ‡ã€‚'
- en: This will iterate over the entire training dataloader once beforehand,
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å°†åœ¨äº‹å…ˆè¿­ä»£æ•´ä¸ªè®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸€æ¬¡ï¼Œ
- en: and will slow down the entire process.
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¹¶ä¸”ä¼šå‡æ…¢æ•´ä¸ªè¿‡ç¨‹ã€‚
- en: '`include_num_input_tokens_seen` (`bool`, *optional*) â€” Whether or not to track
    the number of input tokens seen throughout training.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_num_input_tokens_seen` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è·Ÿè¸ªæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„è¾“å…¥æ ‡è®°æ•°ã€‚'
- en: May be slower in distributed training as gather operations must be called.
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯èƒ½ä¼šè¾ƒæ…¢ï¼Œå› ä¸ºå¿…é¡»è°ƒç”¨gatheræ“ä½œã€‚
- en: '`neftune_noise_alpha` (`Optional[float]`) â€” If not `None`, this will activate
    NEFTune noise embeddings. This can drastically improve model performance for instruction
    fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914)
    and the [original code](https://github.com/neelsjain/NEFTune). Support transformers
    `PreTrainedModel` and also `PeftModel` from peft.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neftune_noise_alpha` (`Optional[float]`) â€” å¦‚æœä¸æ˜¯`None`ï¼Œè¿™å°†æ¿€æ´»NEFTuneå™ªå£°åµŒå…¥ã€‚è¿™å¯ä»¥æå¤§åœ°æé«˜æŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹æ€§èƒ½ã€‚æŸ¥çœ‹[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2310.05914)å’Œ[åŸå§‹ä»£ç ](https://github.com/neelsjain/NEFTune)ã€‚æ”¯æŒtransformers
    `PreTrainedModel`å’Œ`PeftModel`ã€‚'
- en: TrainingArguments is the subset of the arguments we use in our example scripts
    **which relate to the training loop itself**.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: TrainingArgumentsæ˜¯æˆ‘ä»¬åœ¨ç¤ºä¾‹è„šæœ¬ä¸­ä½¿ç”¨çš„ä¸è®­ç»ƒå¾ªç¯æœ¬èº«ç›¸å…³çš„å‚æ•°çš„å­é›†ã€‚
- en: Using [HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)
    we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse)
    arguments that can be specified on the command line.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªç±»è½¬æ¢ä¸ºå¯ä»¥åœ¨å‘½ä»¤è¡Œä¸ŠæŒ‡å®šçš„[argparse](https://docs.python.org/3/library/argparse#module-argparse)å‚æ•°ã€‚
- en: '#### `get_process_log_level`'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_process_log_level`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2028)'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2028)'
- en: '[PRE43]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Returns the log level to be used depending on whether this process is the main
    process of node 0, main process of node non-0, or a non-main process.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¿™ä¸ªè¿›ç¨‹æ˜¯èŠ‚ç‚¹0çš„ä¸»è¿›ç¨‹ã€é0èŠ‚ç‚¹çš„ä¸»è¿›ç¨‹è¿˜æ˜¯éä¸»è¿›ç¨‹ï¼Œè¿”å›è¦ä½¿ç”¨çš„æ—¥å¿—çº§åˆ«ã€‚
- en: For the main process the log level defaults to the logging level set (`logging.WARNING`
    if you didnâ€™t do anything) unless overridden by `log_level` argument.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸»è¿›ç¨‹ï¼Œæ—¥å¿—çº§åˆ«é»˜è®¤ä¸ºè®¾ç½®çš„æ—¥å¿—çº§åˆ«ï¼ˆå¦‚æœæ‚¨æ²¡æœ‰åšä»»ä½•æ“ä½œï¼Œåˆ™ä¸º`logging.WARNING`ï¼‰ï¼Œé™¤éè¢«`log_level`å‚æ•°è¦†ç›–ã€‚
- en: For the replica processes the log level defaults to `logging.WARNING` unless
    overridden by `log_level_replica` argument.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‰¯æœ¬è¿›ç¨‹ï¼Œé»˜è®¤çš„æ—¥å¿—çº§åˆ«ä¸º`logging.WARNING`ï¼Œé™¤éè¢«`log_level_replica`å‚æ•°è¦†ç›–ã€‚
- en: The choice between the main and replica process settings is made according to
    the return value of `should_log`.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®`should_log`çš„è¿”å›å€¼æ¥é€‰æ‹©ä¸»è¿›ç¨‹å’Œå‰¯æœ¬è¿›ç¨‹çš„è®¾ç½®ã€‚
- en: '#### `get_warmup_steps`'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_warmup_steps`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2117)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2117)'
- en: '[PRE44]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Get number of steps used for a linear warmup.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ç”¨äºçº¿æ€§é¢„çƒ­çš„æ­¥æ•°ã€‚
- en: '#### `main_process_first`'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `main_process_first`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2066)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2066)'
- en: '[PRE45]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`local` (`bool`, *optional*, defaults to `True`) â€” if `True` first means process
    of rank 0 of each node if `False` first means process of rank 0 of node rank 0
    In multi-node environment with a shared filesystem you most likely will want to
    use `local=False` so that only the main process of the first node will do the
    processing. If however, the filesystem is not shared, then the main process of
    each node will need to do the processing, which is the default behavior.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local` (`bool`, *optional*, defaults to `True`) â€” å¦‚æœä¸º`True`ï¼Œåˆ™é¦–å…ˆå¤„ç†æ¯ä¸ªèŠ‚ç‚¹çš„æ’åä¸º0çš„è¿›ç¨‹ï¼Œå¦‚æœä¸º`False`ï¼Œåˆ™é¦–å…ˆå¤„ç†æ’åä¸º0çš„èŠ‚ç‚¹0çš„è¿›ç¨‹ã€‚åœ¨å…·æœ‰å…±äº«æ–‡ä»¶ç³»ç»Ÿçš„å¤šèŠ‚ç‚¹ç¯å¢ƒä¸­ï¼Œæ‚¨å¾ˆå¯èƒ½ä¼šæƒ³è¦ä½¿ç”¨`local=False`ï¼Œä»¥ä¾¿åªæœ‰ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸»è¿›ç¨‹ä¼šè¿›è¡Œå¤„ç†ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ–‡ä»¶ç³»ç»Ÿä¸å…±äº«ï¼Œåˆ™æ¯ä¸ªèŠ‚ç‚¹çš„ä¸»è¿›ç¨‹å°†éœ€è¦è¿›è¡Œå¤„ç†ï¼Œè¿™æ˜¯é»˜è®¤è¡Œä¸ºã€‚'
- en: '`desc` (`str`, *optional*, defaults to `"work"`) â€” a work description to be
    used in debug logs'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desc` (`str`, *optional*, defaults to `"work"`) â€” ç”¨äºè°ƒè¯•æ—¥å¿—ä¸­çš„å·¥ä½œæè¿°'
- en: A context manager for torch distributed environment where on needs to do something
    on the main process, while blocking replicas, and when itâ€™s finished releasing
    the replicas.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºtorchåˆ†å¸ƒå¼ç¯å¢ƒçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œåœ¨ä¸»è¿›ç¨‹ä¸Šéœ€è¦æ‰§è¡ŒæŸäº›æ“ä½œï¼ŒåŒæ—¶é˜»å¡å‰¯æœ¬ï¼Œå®Œæˆåé‡Šæ”¾å‰¯æœ¬ã€‚
- en: One such use is for `datasets`â€™s `map` feature which to be efficient should
    be run once on the main process, which upon completion saves a cached version
    of results and which then automatically gets loaded by the replicas.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ç§ç”¨æ³•æ˜¯ç”¨äº`datasets`çš„`map`åŠŸèƒ½ï¼Œä¸ºäº†æœ‰æ•ˆç‡ï¼Œåº”è¯¥åœ¨ä¸»è¿›ç¨‹ä¸Šè¿è¡Œä¸€æ¬¡ï¼Œå®Œæˆåä¿å­˜ç»“æœçš„ç¼“å­˜ç‰ˆæœ¬ï¼Œç„¶åå‰¯æœ¬ä¼šè‡ªåŠ¨åŠ è½½ã€‚
- en: '#### `set_dataloader`'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_dataloader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2629)'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2629)'
- en: '[PRE46]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`drop_last` (`bool`, *optional*, defaults to `False`) â€” Whether to drop the
    last incomplete batch (if the length of the dataset is not divisible by the batch
    size) or not.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_last` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ï¼ˆå¦‚æœæ•°æ®é›†çš„é•¿åº¦ä¸å¯è¢«æ‰¹æ¬¡å¤§å°æ•´é™¤ï¼‰ã€‚'
- en: '`num_workers` (`int`, *optional*, defaults to 0) â€” Number of subprocesses to
    use for data loading (PyTorch only). 0 means that the data will be loaded in the
    main process.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, defaults to 0) â€” ç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡ï¼ˆä»…é€‚ç”¨äºPyTorchï¼‰ã€‚0è¡¨ç¤ºæ•°æ®å°†åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½ã€‚'
- en: '`pin_memory` (`bool`, *optional*, defaults to `True`) â€” Whether you want to
    pin memory in data loaders or not. Will default to `True`.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pin_memory` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¦åœ¨æ•°æ®åŠ è½½å™¨ä¸­å›ºå®šå†…å­˜ã€‚é»˜è®¤ä¸º`True`ã€‚'
- en: '`persistent_workers` (`bool`, *optional*, defaults to `False`) â€” If True, the
    data loader will not shut down the worker processes after a dataset has been consumed
    once. This allows to maintain the workers Dataset instances alive. Can potentially
    speed up training, but will increase RAM usage. Will default to `False`.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persistent_workers` (`bool`, *optional*, defaults to `False`) â€” å¦‚æœä¸ºTrueï¼Œåˆ™æ•°æ®åŠ è½½å™¨åœ¨æ•°æ®é›†è¢«æ¶ˆè€—ä¸€æ¬¡åä¸ä¼šå…³é—­å·¥ä½œè¿›ç¨‹ã€‚è¿™å…è®¸ä¿æŒå·¥ä½œè¿›ç¨‹çš„æ•°æ®é›†å®ä¾‹ä¿æŒæ´»åŠ¨çŠ¶æ€ã€‚å¯èƒ½ä¼šåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡ã€‚é»˜è®¤ä¸º`False`ã€‚'
- en: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) â€” Whether
    to find a batch size that will fit into memory automatically through exponential
    decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed
    (`pip install accelerate`)'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦é€šè¿‡æŒ‡æ•°è¡°å‡è‡ªåŠ¨æ‰¾åˆ°é€‚åˆå†…å­˜çš„æ‰¹æ¬¡å¤§å°ï¼Œé¿å…CUDAå†…å­˜ä¸è¶³é”™è¯¯ã€‚éœ€è¦å®‰è£…accelerateï¼ˆ`pip
    install accelerate`ï¼‰'
- en: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) â€” When resuming
    training, whether or not to skip the epochs and batches to get the data loading
    at the same stage as in the previous training. If set to `True`, the training
    will begin faster (as that skipping step can take a long time) but will not yield
    the same results as the interrupted training would have.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) â€” åœ¨æ¢å¤è®­ç»ƒæ—¶ï¼Œæ˜¯å¦è·³è¿‡æ‰¹æ¬¡å’Œè½®æ¬¡ä»¥ä½¿æ•°æ®åŠ è½½å¤„äºä¸å…ˆå‰è®­ç»ƒç›¸åŒé˜¶æ®µã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œè®­ç»ƒå°†æ›´å¿«å¼€å§‹ï¼ˆå› ä¸ºè·³è¿‡æ­¥éª¤å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰ï¼Œä½†ä¸ä¼šäº§ç”Ÿä¸ä¸­æ–­è®­ç»ƒç›¸åŒçš„ç»“æœã€‚'
- en: '`sampler_seed` (`int`, *optional*) â€” Random seed to be used with data samplers.
    If not set, random generators for data sampling will use the same seed as `self.seed`.
    This can be used to ensure reproducibility of data sampling, independent of the
    model seed.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampler_seed` (`int`, *optional*) â€” ç”¨äºæ•°æ®é‡‡æ ·å™¨çš„éšæœºç§å­ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™æ•°æ®é‡‡æ ·çš„éšæœºç”Ÿæˆå™¨å°†ä½¿ç”¨ä¸`self.seed`ç›¸åŒçš„ç§å­ã€‚è¿™å¯ç”¨äºç¡®ä¿æ•°æ®é‡‡æ ·çš„å¯é‡å¤æ€§ï¼Œç‹¬ç«‹äºæ¨¡å‹ç§å­ã€‚'
- en: A method that regroups all arguments linked to the dataloaders creation.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸æ•°æ®åŠ è½½å™¨åˆ›å»ºç›¸å…³çš„å‚æ•°é‡æ–°ç»„åˆçš„æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE47]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#### `set_evaluate`'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2238)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2238)'
- en: '[PRE48]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) â€” The evaluation strategy to adopt during training.
    Possible values are:'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` (`str`æˆ–[IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) â€” è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨çš„è¯„ä¼°ç­–ç•¥ã€‚å¯èƒ½çš„å€¼ä¸ºï¼š'
- en: '`"no"`: No evaluation is done during training.'
  id: totrans-567
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒè¿‡ç¨‹ä¸­ä¸è¿›è¡Œè¯„ä¼°ã€‚'
- en: '`"steps"`: Evaluation is done (and logged) every `steps`.'
  id: totrans-568
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯`steps`è¿›è¡Œè¯„ä¼°ï¼ˆå¹¶è®°å½•æ—¥å¿—ï¼‰ã€‚'
- en: '`"epoch"`: Evaluation is done at the end of each epoch.'
  id: totrans-569
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚'
- en: Setting a `strategy` different from `"no"` will set `self.do_eval` to `True`.
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸`"no"`ä¸åŒçš„`strategy`å°†`self.do_eval`è®¾ç½®ä¸º`True`ã€‚
- en: '`steps` (`int`, *optional*, defaults to 500) â€” Number of update steps between
    two evaluations if `strategy="steps"`.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` (`int`, *optional*, é»˜è®¤ä¸º500) â€” å¦‚æœ`strategy="steps"`ï¼Œä¸¤æ¬¡è¯„ä¼°ä¹‹é—´çš„æ›´æ–°æ­¥æ•°ã€‚'
- en: '`batch_size` (`int` *optional*, defaults to 8) â€” The batch size per device
    (GPU/TPU core/CPUâ€¦) used for evaluation.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int` *optional*, é»˜è®¤ä¸º8) â€” ç”¨äºè¯„ä¼°çš„æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/TPUæ ¸å¿ƒ/CPU...ï¼‰çš„æ‰¹é‡å¤§å°ã€‚'
- en: '`accumulation_steps` (`int`, *optional*) â€” Number of predictions steps to accumulate
    the output tensors for, before moving the results to the CPU. If left unset, the
    whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster
    but requires more memory).'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accumulation_steps` (`int`, *optional*) â€” åœ¨å°†ç»“æœç§»åŠ¨åˆ°CPUä¹‹å‰ï¼Œç´¯ç§¯è¾“å‡ºå¼ é‡çš„é¢„æµ‹æ­¥æ•°ã€‚å¦‚æœæœªè®¾ç½®ï¼Œæ•´ä¸ªé¢„æµ‹å°†åœ¨GPU/TPUä¸Šç´¯ç§¯åç§»è‡³CPUï¼ˆé€Ÿåº¦æ›´å¿«ä½†éœ€è¦æ›´å¤šå†…å­˜ï¼‰ã€‚'
- en: '`delay` (`float`, *optional*) â€” Number of epochs or steps to wait for before
    the first evaluation can be performed, depending on the evaluation_strategy.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delay` (`float`, *optional*) â€” ç­‰å¾…è¿›è¡Œç¬¬ä¸€æ¬¡è¯„ä¼°çš„å‘¨æœŸæ•°æˆ–æ­¥æ•°ï¼Œå–å†³äºè¯„ä¼°ç­–ç•¥ã€‚'
- en: '`loss_only` (`bool`, *optional*, defaults to `False`) â€” Ignores all outputs
    except the loss.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_only` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” ä»…å¿½ç•¥æŸå¤±ä»¥å¤–çš„æ‰€æœ‰è¾“å‡ºã€‚'
- en: '`jit_mode` (`bool`, *optional*) â€” Whether or not to use PyTorch jit trace for
    inference.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jit_mode` (`bool`, *optional*) â€” æ˜¯å¦ä½¿ç”¨PyTorch jitè·Ÿè¸ªè¿›è¡Œæ¨æ–­ã€‚'
- en: A method that regroups all arguments linked to evaluation.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸è¯„ä¼°ç›¸å…³çš„å‚æ•°åˆ†ç»„çš„æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE49]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `set_logging`'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_logging`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2388)'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2388)'
- en: '[PRE50]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” The logging strategy to adopt during training.
    Possible values are:'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` (`str` æˆ– [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, é»˜è®¤ä¸º`"steps"`) â€” è®­ç»ƒæœŸé—´é‡‡ç”¨çš„æ—¥å¿—è®°å½•ç­–ç•¥ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'
- en: '`"no"`: No save is done during training.'
  id: totrans-585
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒæœŸé—´ä¸ä¿å­˜ã€‚'
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-586
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: åœ¨æ¯ä¸ªå‘¨æœŸç»“æŸæ—¶ä¿å­˜ã€‚'
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-587
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯`save_steps`ä¿å­˜ä¸€æ¬¡ã€‚'
- en: '`steps` (`int`, *optional*, defaults to 500) â€” Number of update steps between
    two logs if `strategy="steps"`.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` (`int`, *optional*, é»˜è®¤ä¸º500) â€” å¦‚æœ`strategy="steps"`ï¼Œä¸¤æ¬¡æ—¥å¿—è®°å½•ä¹‹é—´çš„æ›´æ–°æ­¥æ•°ã€‚'
- en: '`level` (`str`, *optional*, defaults to `"passive"`) â€” Logger log level to
    use on the main process. Possible choices are the log levels as strings: `"debug"`,
    `"info"`, `"warning"`, `"error"` and `"critical"`, plus a `"passive"` level which
    doesnâ€™t set anything and lets the application set the level.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level` (`str`, *optional*, é»˜è®¤ä¸º`"passive"`) â€” ç”¨äºä¸»è¿›ç¨‹çš„è®°å½•å™¨æ—¥å¿—çº§åˆ«ã€‚å¯èƒ½çš„é€‰æ‹©æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ—¥å¿—çº§åˆ«ï¼š`"debug"`ã€`"info"`ã€`"warning"`ã€`"error"`å’Œ`"critical"`ï¼Œä»¥åŠä¸€ä¸ªä¸è®¾ç½®ä»»ä½•å†…å®¹å¹¶è®©åº”ç”¨ç¨‹åºè®¾ç½®çº§åˆ«çš„`"passive"`çº§åˆ«ã€‚'
- en: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) â€” The list
    of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
    `"clearml"`, `"codecarbon"`, `"comet_ml"`, `"dagshub"`, `"dvclive"`, `"flyte"`,
    `"mlflow"`, `"neptune"`, `"tensorboard"`, and `"wandb"`. Use `"all"` to report
    to all integrations installed, `"none"` for no integrations.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to` (`str` æˆ– `List[str]`, *optional*, é»˜è®¤ä¸º`"all"`) â€” æŠ¥å‘Šç»“æœå’Œæ—¥å¿—çš„é›†æˆåˆ—è¡¨ã€‚æ”¯æŒçš„å¹³å°æœ‰`"azure_ml"`ã€`"clearml"`ã€`"codecarbon"`ã€`"comet_ml"`ã€`"dagshub"`ã€`"dvclive"`ã€`"flyte"`ã€`"mlflow"`ã€`"neptune"`ã€`"tensorboard"`å’Œ`"wandb"`ã€‚ä½¿ç”¨`"all"`æŠ¥å‘Šæ‰€æœ‰å·²å®‰è£…çš„é›†æˆï¼Œä½¿ç”¨`"none"`ä¸æŠ¥å‘Šä»»ä½•é›†æˆã€‚'
- en: '`first_step` (`bool`, *optional*, defaults to `False`) â€” Whether to log and
    evaluate the first `global_step` or not.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first_step` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è®°å½•å’Œè¯„ä¼°ç¬¬ä¸€ä¸ª`global_step`ã€‚'
- en: '`nan_inf_filter` (`bool`, *optional*, defaults to `True`) â€” Whether to filter
    `nan` and `inf` losses for logging. If set to `True` the loss of every step that
    is `nan` or `inf` is filtered and the average loss of the current logging window
    is taken instead.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nan_inf_filter` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿‡æ»¤ç”¨äºæ—¥å¿—è®°å½•çš„`nan`å’Œ`inf`æŸå¤±ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿‡æ»¤æ¯ä¸ªæ­¥éª¤çš„`nan`æˆ–`inf`æŸå¤±ï¼Œå¹¶å–ä»£å½“å‰æ—¥å¿—çª—å£çš„å¹³å‡æŸå¤±ã€‚'
- en: '`nan_inf_filter` only influences the logging of loss values, it does not change
    the behavior the gradient is computed or applied to the model.'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`nan_inf_filter`ä»…å½±å“æŸå¤±å€¼çš„æ—¥å¿—è®°å½•ï¼Œä¸ä¼šæ”¹å˜è®¡ç®—æ¢¯åº¦æˆ–å°†æ¢¯åº¦åº”ç”¨äºæ¨¡å‹çš„è¡Œä¸ºã€‚'
- en: '`on_each_node` (`bool`, *optional*, defaults to `True`) â€” In multinode distributed
    training, whether to log using `log_level` once per node, or only on the main
    node.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_each_node` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” åœ¨å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ˜¯å¦æ¯ä¸ªèŠ‚ç‚¹ä»…ä½¿ç”¨`log_level`ä¸€æ¬¡è¿›è¡Œæ—¥å¿—è®°å½•ï¼Œæˆ–ä»…åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`replica_level` (`str`, *optional*, defaults to `"passive"`) â€” Logger log level
    to use on replicas. Same choices as `log_level`'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replica_level` (`str`, *optional*, é»˜è®¤ä¸º`"passive"`) â€” ç”¨äºå‰¯æœ¬çš„è®°å½•å™¨æ—¥å¿—çº§åˆ«ã€‚ä¸`log_level`ç›¸åŒçš„é€‰æ‹©ã€‚'
- en: A method that regroups all arguments linked to logging.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸æ—¥å¿—è®°å½•ç›¸å…³çš„å‚æ•°åˆ†ç»„çš„æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE51]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `set_lr_scheduler`'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_lr_scheduler`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2584)'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2584)'
- en: '[PRE52]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`name` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) â€” The scheduler type to use. See the documentation
    of [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    for all possible values.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str` æˆ– [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, é»˜è®¤ä¸º`"linear"`) â€” è¦ä½¿ç”¨çš„è°ƒåº¦ç¨‹åºç±»å‹ã€‚æŸ¥çœ‹[SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)çš„æ–‡æ¡£ä»¥è·å–æ‰€æœ‰å¯èƒ½çš„å€¼ã€‚'
- en: '`num_epochs(float,` *optional*, defaults to 3.0) â€” Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_epochs(float,` *optional*, é»˜è®¤ä¸º3.0) â€” è¦æ‰§è¡Œçš„æ€»è®­ç»ƒå‘¨æœŸæ•°ï¼ˆå¦‚æœä¸æ˜¯æ•´æ•°ï¼Œåˆ™åœ¨åœæ­¢è®­ç»ƒä¹‹å‰æ‰§è¡Œæœ€åä¸€ä¸ªå‘¨æœŸçš„å°æ•°éƒ¨åˆ†ç™¾åˆ†æ¯”ï¼‰ã€‚'
- en: '`max_steps` (`int`, *optional*, defaults to -1) â€” If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º -1) â€” å¦‚æœè®¾ç½®ä¸ºæ­£æ•°ï¼Œåˆ™æ‰§è¡Œçš„æ€»è®­ç»ƒæ­¥æ•°ã€‚è¦†ç›–`num_train_epochs`ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é›†ï¼Œå¦‚æœæ‰€æœ‰æ•°æ®éƒ½ç”¨å®Œï¼Œåˆ™é€šè¿‡æ•°æ®é›†é‡å¤è®­ç»ƒï¼Œç›´åˆ°è¾¾åˆ°`max_steps`ã€‚'
- en: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) â€” Ratio of total training
    steps used for a linear warmup from 0 to `learning_rate`.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_ratio` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” ç”¨äºä»0åˆ°`learning_rate`è¿›è¡Œçº¿æ€§é¢„çƒ­çš„æ€»è®­ç»ƒæ­¥éª¤çš„æ¯”ç‡ã€‚'
- en: '`warmup_steps` (`int`, *optional*, defaults to 0) â€” Number of steps used for
    a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” ç”¨äºä»0åˆ°`learning_rate`è¿›è¡Œçº¿æ€§é¢„çƒ­çš„æ­¥éª¤æ•°ã€‚è¦†ç›–`warmup_ratio`çš„ä»»ä½•æ•ˆæœã€‚'
- en: A method that regroups all arguments linked to the learning rate scheduler and
    its hyperparameters.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ‰€æœ‰ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨åŠå…¶è¶…å‚æ•°ç›¸å…³è”çš„å‚æ•°é‡æ–°åˆ†ç»„çš„æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE53]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '#### `set_optimizer`'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_optimizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2533)'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2533)'
- en: '[PRE54]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`name` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    â€” The optimizer to use: `"adamw_hf"`, `"adamw_torch"`, `"adamw_torch_fused"`,
    `"adamw_apex_fused"`, `"adamw_anyprecision"` or `"adafactor"`.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str` æˆ– `training_args.OptimizerNames`, *å¯é€‰*, é»˜è®¤ä¸º `"adamw_torch"`)
    â€” è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼š"adamw_hf"ã€"adamw_torch"ã€"adamw_torch_fused"ã€"adamw_apex_fused"ã€"adamw_anyprecision"æˆ–"adafactor"ã€‚'
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) â€” The initial learning
    rate.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 5e-5) â€” åˆå§‹å­¦ä¹ ç‡ã€‚'
- en: '`weight_decay` (`float`, *optional*, defaults to 0) â€” The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” åº”ç”¨çš„æƒé‡è¡°å‡ï¼ˆå¦‚æœä¸ä¸ºé›¶ï¼‰åˆ°æ‰€æœ‰å±‚ï¼Œé™¤äº†æ‰€æœ‰åç½®å’ŒLayerNormæƒé‡ã€‚'
- en: '`beta1` (`float`, *optional*, defaults to 0.9) â€” The beta1 hyperparameter for
    the adam optimizer or its variants.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta1` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.9) â€” Adamä¼˜åŒ–å™¨æˆ–å…¶å˜ç§çš„beta1è¶…å‚æ•°ã€‚'
- en: '`beta2` (`float`, *optional*, defaults to 0.999) â€” The beta2 hyperparameter
    for the adam optimizer or its variants.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta2` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.999) â€” Adamä¼˜åŒ–å™¨æˆ–å…¶å˜ç§çš„beta2è¶…å‚æ•°ã€‚'
- en: '`epsilon` (`float`, *optional*, defaults to 1e-8) â€” The epsilon hyperparameter
    for the adam optimizer or its variants.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-8) â€” Adamä¼˜åŒ–å™¨æˆ–å…¶å˜ç§çš„epsilonè¶…å‚æ•°ã€‚'
- en: '`args` (`str`, *optional*) â€” Optional arguments that are supplied to AnyPrecisionAdamW
    (only useful when `optim="adamw_anyprecision"`).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`str`, *å¯é€‰*) â€” æä¾›ç»™AnyPrecisionAdamWçš„å¯é€‰å‚æ•°ï¼ˆä»…åœ¨`optim="adamw_anyprecision"`æ—¶æœ‰ç”¨ï¼‰ã€‚'
- en: A method that regroups all arguments linked to the optimizer and its hyperparameters.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†æ‰€æœ‰ä¸ä¼˜åŒ–å™¨åŠå…¶è¶…å‚æ•°ç›¸å…³è”çš„å‚æ•°é‡æ–°åˆ†ç»„çš„æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE55]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '#### `set_push_to_hub`'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2463)'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2463)'
- en: '[PRE56]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model_id` (`str`) â€” The name of the repository to keep in sync with the local
    *output_dir*. It can be a simple model ID in which case the model will be pushed
    in your namespace. Otherwise it should be the whole repository name, for instance
    `"user_name/model"`, which allows you to push to an organization you are a member
    of with `"organization_name/model"`.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_id` (`str`) â€” ä¸æœ¬åœ°*output_dir*åŒæ­¥çš„å­˜å‚¨åº“çš„åç§°ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹IDï¼Œæ­¤æ—¶æ¨¡å‹å°†è¢«æ¨é€åˆ°æ‚¨çš„å‘½åç©ºé—´ã€‚å¦åˆ™ï¼Œå®ƒåº”è¯¥æ˜¯æ•´ä¸ªå­˜å‚¨åº“åç§°ï¼Œä¾‹å¦‚`"user_name/model"`ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥å°†å…¶æ¨é€åˆ°æ‚¨æ˜¯æˆå‘˜çš„ç»„ç»‡ä¸­ï¼Œä¾‹å¦‚`"organization_name/model"`ã€‚'
- en: '`strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    â€” Defines the scope of what is pushed to the Hub and when. Possible values are:'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` (`str` æˆ– `HubStrategy`, *å¯é€‰*, é»˜è®¤ä¸º `"every_save"`) â€” å®šä¹‰æ¨é€åˆ°Hubçš„èŒƒå›´å’Œæ—¶é—´ã€‚å¯èƒ½çš„å€¼ä¸ºï¼š'
- en: '`"end"`: push the model, its configuration, the tokenizer (if passed along
    to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card when the [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    method is called.'
  id: totrans-631
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"end"`: å½“è°ƒç”¨[save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)æ–¹æ³•æ—¶ï¼Œæ¨é€æ¨¡å‹ã€å…¶é…ç½®ã€åˆ†è¯å™¨ï¼ˆå¦‚æœä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ä»¥åŠæ¨¡å‹å¡çš„è‰ç¨¿ã€‚'
- en: '`"every_save"`: push the model, its configuration, the tokenizer (if passed
    along to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card each time there is a model save. The pushes are asynchronous
    to not block training, and in case the save are very frequent, a new push is only
    attempted if the previous one is finished. A last push is made with the final
    model at the end of training.'
  id: totrans-632
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"every_save"`: æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼Œæ¨é€æ¨¡å‹ã€å…¶é…ç½®ã€åˆ†è¯å™¨ï¼ˆå¦‚æœä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ä»¥åŠæ¨¡å‹å¡çš„è‰ç¨¿ã€‚æ¨é€æ˜¯å¼‚æ­¥çš„ï¼Œä»¥é¿å…é˜»å¡è®­ç»ƒï¼Œå¦‚æœä¿å­˜éå¸¸é¢‘ç¹ï¼Œåˆ™åªæœ‰åœ¨ä¸Šä¸€ä¸ªæ¨é€å®Œæˆåæ‰ä¼šå°è¯•æ–°çš„æ¨é€ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œæœ€åä¸€æ¬¡æ¨é€ã€‚'
- en: '`"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed
    in a subfolder named last-checkpoint, allowing you to resume training easily with
    `trainer.train(resume_from_checkpoint="last-checkpoint")`.'
  id: totrans-633
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"checkpoint"`: ç±»ä¼¼äº`"every_save"`ï¼Œä½†æœ€æ–°çš„æ£€æŸ¥ç‚¹ä¹Ÿè¢«æ¨é€åˆ°åä¸ºlast-checkpointçš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œè¿™æ ·æ‚¨å¯ä»¥è½»æ¾åœ°ä½¿ç”¨`trainer.train(resume_from_checkpoint="last-checkpoint")`æ¢å¤è®­ç»ƒã€‚'
- en: '`"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like
    they appear in the output folder (so you will get one checkpoint folder per folder
    in your final repository)'
  id: totrans-634
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all_checkpoints"`: ç±»ä¼¼äº`"checkpoint"`ï¼Œä½†æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½åƒå®ƒä»¬å‡ºç°åœ¨è¾“å‡ºæ–‡ä»¶å¤¹ä¸­ä¸€æ ·è¢«æ¨é€ï¼ˆå› æ­¤æ‚¨å°†åœ¨æœ€ç»ˆå­˜å‚¨åº“ä¸­çš„æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­è·å¾—ä¸€ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ï¼‰ã€‚'
- en: '`token` (`str`, *optional*) â€” The token to use to push the model to the Hub.
    Will default to the token in the cache folder obtained with `huggingface-cli login`.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str`, *å¯é€‰*) â€” ç”¨äºå°†æ¨¡å‹æ¨é€åˆ°Hubçš„ä»¤ç‰Œã€‚å°†é»˜è®¤ä½¿ç”¨é€šè¿‡`huggingface-cli login`è·å¾—çš„ç¼“å­˜æ–‡ä»¶å¤¹ä¸­çš„ä»¤ç‰Œã€‚'
- en: '`private_repo` (`bool`, *optional*, defaults to `False`) â€” If True, the Hub
    repo will be set to private.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private_repo` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸ºTrueï¼Œåˆ™Hubå­˜å‚¨åº“å°†è®¾ç½®ä¸ºç§æœ‰ã€‚'
- en: '`always_push` (`bool`, *optional*, defaults to `False`) â€” Unless this is `True`,
    the `Trainer` will skip pushing a checkpoint when the previous push is not finished.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: always_pushï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” é™¤éä¸º`True`ï¼Œå¦åˆ™å½“ä¸Šä¸€æ¬¡æ¨é€æœªå®Œæˆæ—¶ï¼Œ`Trainer`å°†è·³è¿‡æ¨é€æ£€æŸ¥ç‚¹ã€‚
- en: A method that regroups all arguments linked to synchronizing checkpoints with
    the Hub.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸ä¸HubåŒæ­¥æ£€æŸ¥ç‚¹ç›¸å…³çš„å‚æ•°è¿›è¡Œåˆ†ç»„çš„æ–¹æ³•ã€‚
- en: Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir`
    will begin a git directory synced with the repo (determined by `model_id`) and
    the content will be pushed each time a save is triggered (depending on`self.save_strategy`).
    Calling [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    will also trigger a push.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨æ­¤æ–¹æ³•å°†è®¾ç½®`self.push_to_hub`ä¸º`True`ï¼Œè¿™æ„å‘³ç€`output_dir`å°†å¼€å§‹ä¸€ä¸ªä¸å­˜å‚¨åº“åŒæ­¥çš„gitç›®å½•ï¼ˆç”±`model_id`ç¡®å®šï¼‰ï¼Œå¹¶ä¸”æ¯æ¬¡è§¦å‘ä¿å­˜æ—¶å°†æ¨é€å†…å®¹ï¼ˆå–å†³äº`self.save_strategy`ï¼‰ã€‚è°ƒç”¨[save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)ä¹Ÿå°†è§¦å‘æ¨é€ã€‚
- en: 'Example:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE57]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `set_save`'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_save`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2339)'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2339)'
- en: '[PRE58]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” The checkpoint save strategy to adopt during
    training. Possible values are:'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: strategyï¼ˆ`str`æˆ–[IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œstepsâ€ï¼‰â€”
    è®­ç»ƒæœŸé—´é‡‡ç”¨çš„æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥ã€‚å¯èƒ½çš„å€¼ä¸ºï¼š
- en: '`"no"`: No save is done during training.'
  id: totrans-647
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä¿å­˜ã€‚
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-648
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œepochâ€ï¼šåœ¨æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶ä¿å­˜ã€‚
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-649
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œstepsâ€ï¼šæ¯`save_steps`ä¿å­˜ä¸€æ¬¡ã€‚
- en: '`steps` (`int`, *optional*, defaults to 500) â€” Number of updates steps before
    two checkpoint saves if `strategy="steps"`.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º500ï¼‰â€” å¦‚æœ`strategy="steps"`ï¼Œåˆ™åœ¨ä¸¤ä¸ªæ£€æŸ¥ç‚¹ä¿å­˜ä¹‹å‰çš„æ›´æ–°æ­¥éª¤æ•°ã€‚'
- en: '`total_limit` (`int`, *optional*) â€” If a value is passed, will limit the total
    amount of checkpoints. Deletes the older checkpoints in `output_dir`.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: total_limitï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœä¼ é€’äº†ä¸€ä¸ªå€¼ï¼Œå°†é™åˆ¶æ£€æŸ¥ç‚¹çš„æ€»é‡ã€‚åˆ é™¤`output_dir`ä¸­çš„æ—§æ£€æŸ¥ç‚¹ã€‚
- en: '`on_each_node` (`bool`, *optional*, defaults to `False`) â€” When doing multi-node
    distributed training, whether to save models and checkpoints on each node, or
    only on the main one.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: on_each_nodeï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” åœ¨è¿›è¡Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œæ˜¯å¦åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ï¼Œè¿˜æ˜¯ä»…åœ¨ä¸»èŠ‚ç‚¹ä¸Šä¿å­˜ã€‚
- en: This should not be activated when the different nodes use the same storage as
    the files will be saved with the same names for each node.
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ä¸åŒèŠ‚ç‚¹ä½¿ç”¨ç›¸åŒå­˜å‚¨æ—¶ï¼Œä¸åº”æ¿€æ´»æ­¤é€‰é¡¹ï¼Œå› ä¸ºæ–‡ä»¶å°†ä»¥æ¯ä¸ªèŠ‚ç‚¹ç›¸åŒçš„åç§°ä¿å­˜ã€‚
- en: A method that regroups all arguments linked to checkpoint saving.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸æ£€æŸ¥ç‚¹ä¿å­˜ç›¸å…³çš„å‚æ•°è¿›è¡Œåˆ†ç»„çš„æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE59]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '#### `set_testing`'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_testing`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2299)'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2299)'
- en: '[PRE60]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_size` (`int` *optional*, defaults to 8) â€” The batch size per device
    (GPU/TPU core/CPUâ€¦) used for testing.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_sizeï¼ˆ`int` *å¯é€‰*ï¼Œé»˜è®¤ä¸º8ï¼‰â€” ç”¨äºæµ‹è¯•çš„æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/TPUæ ¸å¿ƒ/CPUâ€¦ï¼‰çš„æ‰¹é‡å¤§å°ã€‚
- en: '`loss_only` (`bool`, *optional*, defaults to `False`) â€” Ignores all outputs
    except the loss.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: loss_onlyï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” é™¤äº†æŸå¤±ä¹‹å¤–ï¼Œå¿½ç•¥æ‰€æœ‰è¾“å‡ºã€‚
- en: '`jit_mode` (`bool`, *optional*) â€” Whether or not to use PyTorch jit trace for
    inference.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: jit_modeï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä½¿ç”¨PyTorch jitè·Ÿè¸ªè¿›è¡Œæ¨æ–­ã€‚
- en: A method that regroups all basic arguments linked to testing on a held-out dataset.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸åœ¨ä¿ç•™æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ç›¸å…³çš„åŸºæœ¬å‚æ•°è¿›è¡Œåˆ†ç»„çš„æ–¹æ³•ã€‚
- en: Calling this method will automatically set `self.do_predict` to `True`.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨æ­¤æ–¹æ³•å°†è‡ªåŠ¨å°†`self.do_predict`è®¾ç½®ä¸º`True`ã€‚
- en: 'Example:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE61]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `set_training`'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_training`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2163)'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2163)'
- en: '[PRE62]'
  id: totrans-670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) â€” The initial learning
    rate for the optimizer.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learning_rateï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º5e-5ï¼‰â€” ä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ã€‚
- en: '`batch_size` (`int` *optional*, defaults to 8) â€” The batch size per device
    (GPU/TPU core/CPUâ€¦) used for training.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_sizeï¼ˆ`int` *å¯é€‰*ï¼Œé»˜è®¤ä¸º8ï¼‰â€” ç”¨äºè®­ç»ƒçš„æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/TPUæ ¸å¿ƒ/CPUâ€¦ï¼‰çš„æ‰¹é‡å¤§å°ã€‚
- en: '`weight_decay` (`float`, *optional*, defaults to 0) â€” The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights in the optimizer.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: weight_decayï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” åº”ç”¨çš„æƒé‡è¡°å‡ï¼ˆå¦‚æœä¸ä¸ºé›¶ï¼‰åˆ°ä¼˜åŒ–å™¨ä¸­é™¤æ‰€æœ‰åç½®å’ŒLayerNormæƒé‡ä¹‹å¤–çš„æ‰€æœ‰å±‚ã€‚
- en: '`num_train_epochs(float,` *optional*, defaults to 3.0) â€” Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_train_epochsï¼ˆfloatï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3.0ï¼‰â€” è¦æ‰§è¡Œçš„æ€»è®­ç»ƒæ—¶æœŸæ•°ï¼ˆå¦‚æœä¸æ˜¯æ•´æ•°ï¼Œåˆ™åœ¨åœæ­¢è®­ç»ƒä¹‹å‰æ‰§è¡Œæœ€åä¸€ä¸ªæ—¶æœŸçš„å°æ•°éƒ¨åˆ†ç™¾åˆ†æ¯”ï¼‰ã€‚
- en: '`max_steps` (`int`, *optional*, defaults to -1) â€” If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: max_stepsï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º-1ï¼‰â€” å¦‚æœè®¾ç½®ä¸ºæ­£æ•°ï¼Œåˆ™æ‰§è¡Œçš„æ€»è®­ç»ƒæ­¥æ•°ã€‚è¦†ç›–`num_train_epochs`ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é›†ï¼Œå¦‚æœæ‰€æœ‰æ•°æ®éƒ½ç”¨å®Œï¼Œåˆ™é€šè¿‡æ•°æ®é›†é‡å¤è®­ç»ƒï¼Œç›´åˆ°è¾¾åˆ°`max_steps`ã€‚
- en: '`gradient_accumulation_steps` (`int`, *optional*, defaults to 1) â€” Number of
    updates steps to accumulate the gradients for, before performing a backward/update
    pass.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gradient_accumulation_stepsï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” åœ¨æ‰§è¡Œå‘å/æ›´æ–°ä¼ é€’ä¹‹å‰ï¼Œç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚
- en: When using gradient accumulation, one step is counted as one step with backward
    pass. Therefore, logging, evaluation, save will be conducted every `gradient_accumulation_steps
    * xxx_step` training examples.
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œä¸€ä¸ªæ­¥éª¤è¢«è®¡ä¸ºä¸€ä¸ªå¸¦æœ‰å‘åä¼ é€’çš„æ­¥éª¤ã€‚å› æ­¤ï¼Œæ¯`gradient_accumulation_steps * xxx_step`ä¸ªè®­ç»ƒç¤ºä¾‹å°†è¿›è¡Œæ—¥å¿—è®°å½•ã€è¯„ä¼°å’Œä¿å­˜ã€‚
- en: '`seed` (`int`, *optional*, defaults to 42) â€” Random seed that will be set at
    the beginning of training. To ensure reproducibility across runs, use the `~Trainer.model_init`
    function to instantiate the model if it has some randomly initialized parameters.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`, *optional*, é»˜è®¤ä¸º 42) â€” å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®çš„éšæœºç§å­ã€‚ä¸ºäº†ç¡®ä¿åœ¨ä¸åŒè¿è¡Œä¹‹é—´çš„å¯é‡ç°æ€§ï¼Œè¯·ä½¿ç”¨ `~Trainer.model_init`
    å‡½æ•°æ¥å®ä¾‹åŒ–æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€äº›éšæœºåˆå§‹åŒ–çš„å‚æ•°ã€‚'
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `False`) â€” If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜ï¼Œä½†ä¼šé™ä½å‘åä¼ é€’çš„é€Ÿåº¦ã€‚'
- en: A method that regroups all basic arguments linked to the training.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸è®­ç»ƒç›¸å…³çš„åŸºæœ¬å‚æ•°é‡æ–°ç»„åˆçš„æ–¹æ³•ã€‚
- en: Calling this method will automatically set `self.do_train` to `True`.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨æ­¤æ–¹æ³•å°†è‡ªåŠ¨å°† `self.do_train` è®¾ç½®ä¸º `True`ã€‚
- en: 'Example:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE63]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '#### `to_dict`'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2126)'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2126)'
- en: '[PRE64]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Serializes this instance while replace `Enum` by their values (for JSON serialization
    support). It obfuscates the token values by removing their value.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å®ä¾‹åºåˆ—åŒ–ï¼ŒåŒæ—¶ç”¨å®ƒä»¬çš„å€¼æ›¿æ¢ `Enum`ï¼ˆç”¨äº JSON åºåˆ—åŒ–æ”¯æŒï¼‰ã€‚é€šè¿‡åˆ é™¤å®ƒä»¬çš„å€¼æ¥æ··æ·†ä»¤ç‰Œå€¼ã€‚
- en: '#### `to_json_string`'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_json_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2143)'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2143)'
- en: '[PRE65]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Serializes this instance to a JSON string.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å®ä¾‹åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ã€‚
- en: '#### `to_sanitized_dict`'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_sanitized_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2149)'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2149)'
- en: '[PRE66]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Sanitized serialization to use with TensorBoardâ€™s hparams
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡æ¸…ç†çš„åºåˆ—åŒ–ï¼Œå¯ä¸ TensorBoard çš„ hparams ä¸€èµ·ä½¿ç”¨
- en: Seq2SeqTrainingArguments
  id: totrans-697
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2SeqTrainingArguments
- en: '### `class transformers.Seq2SeqTrainingArguments`'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Seq2SeqTrainingArguments`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L28)'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L28)'
- en: '[PRE67]'
  id: totrans-700
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`output_dir` (`str`) â€” The output directory where the model predictions and
    checkpoints will be written.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir` (`str`) â€” æ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹å°†è¢«å†™å…¥çš„è¾“å‡ºç›®å½•ã€‚'
- en: '`overwrite_output_dir` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    overwrite the content of the output directory. Use this to continue training if
    `output_dir` points to a checkpoint directory.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overwrite_output_dir` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œåˆ™è¦†ç›–è¾“å‡ºç›®å½•çš„å†…å®¹ã€‚å¦‚æœ
    `output_dir` æŒ‡å‘ä¸€ä¸ªæ£€æŸ¥ç‚¹ç›®å½•ï¼Œåˆ™ä½¿ç”¨æ­¤é€‰é¡¹ç»§ç»­è®­ç»ƒã€‚'
- en: '`do_train` (`bool`, *optional*, defaults to `False`) â€” Whether to run training
    or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_train` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿è¡Œè®­ç»ƒã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢« [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯æ‰“ç®—ç”±æ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ä½¿ç”¨ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)ã€‚'
- en: '`do_eval` (`bool`, *optional*) â€” Whether to run evaluation on the validation
    set or not. Will be set to `True` if `evaluation_strategy` is different from `"no"`.
    This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_eval` (`bool`, *optional*) â€” æ˜¯å¦åœ¨éªŒè¯é›†ä¸Šè¿è¡Œè¯„ä¼°ã€‚å¦‚æœ `evaluation_strategy` ä¸ `"no"`
    ä¸åŒï¼Œåˆ™å°†è®¾ç½®ä¸º `True`ã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢« [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯æ‰“ç®—ç”±æ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ä½¿ç”¨ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)ã€‚'
- en: '`do_predict` (`bool`, *optional*, defaults to `False`) â€” Whether to run predictions
    on the test set or not. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_predict` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œé¢„æµ‹ã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢« [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯æ‰“ç®—ç”±æ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ä½¿ç”¨ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)ã€‚'
- en: '`evaluation_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"no"`) â€” The evaluation strategy to adopt during training.
    Possible values are:'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluation_strategy` (`str` æˆ– [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, é»˜è®¤ä¸º `"no"`) â€” è®­ç»ƒæœŸé—´é‡‡ç”¨çš„è¯„ä¼°ç­–ç•¥ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'
- en: '`"no"`: No evaluation is done during training.'
  id: totrans-708
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: åœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œè¯„ä¼°ã€‚'
- en: '`"steps"`: Evaluation is done (and logged) every `eval_steps`.'
  id: totrans-709
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯ `eval_steps` è¿›è¡Œä¸€æ¬¡è¯„ä¼°ï¼ˆå¹¶è®°å½•ï¼‰ã€‚'
- en: '`"epoch"`: Evaluation is done at the end of each epoch.'
  id: totrans-710
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: åœ¨æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚'
- en: '`prediction_loss_only` (`bool`, *optional*, defaults to `False`) â€” When performing
    evaluation and generating predictions, only returns the loss.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_loss_only` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” åœ¨è¿›è¡Œè¯„ä¼°å’Œç”Ÿæˆé¢„æµ‹æ—¶ï¼Œä»…è¿”å›æŸå¤±ã€‚'
- en: '`per_device_train_batch_size` (`int`, *optional*, defaults to 8) â€” The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for training.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_train_batch_size` (`int`, *optional*, é»˜è®¤ä¸º 8) â€” è®­ç»ƒæ—¶æ¯ä¸ª GPU/XPU/TPU/MPS/NPU
    æ ¸å¿ƒ/CPU çš„æ‰¹é‡å¤§å°ã€‚'
- en: '`per_device_eval_batch_size` (`int`, *optional*, defaults to 8) â€” The batch
    size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_eval_batch_size` (`int`, *optional*, é»˜è®¤ä¸º 8) â€” è¯„ä¼°æ—¶æ¯ä¸ª GPU/XPU/TPU/MPS/NPU
    æ ¸å¿ƒ/CPU çš„æ‰¹é‡å¤§å°ã€‚'
- en: '`gradient_accumulation_steps` (`int`, *optional*, defaults to 1) â€” Number of
    updates steps to accumulate the gradients for, before performing a backward/update
    pass.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` (`int`, *optional*, é»˜è®¤ä¸º 1) â€” åœ¨æ‰§è¡Œå‘å/æ›´æ–°ä¼ é€’ä¹‹å‰ï¼Œç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚'
- en: When using gradient accumulation, one step is counted as one step with backward
    pass. Therefore, logging, evaluation, save will be conducted every `gradient_accumulation_steps
    * xxx_step` training examples.
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œä¸€ä¸ªæ­¥éª¤è¢«è®¡ä¸ºä¸€ä¸ªå¸¦æœ‰åå‘ä¼ æ’­çš„æ­¥éª¤ã€‚å› æ­¤ï¼Œæ¯ `gradient_accumulation_steps * xxx_step` è®­ç»ƒç¤ºä¾‹å°†è¿›è¡Œè®°å½•ã€è¯„ä¼°ã€ä¿å­˜ã€‚
- en: '`eval_accumulation_steps` (`int`, *optional*) â€” Number of predictions steps
    to accumulate the output tensors for, before moving the results to the CPU. If
    left unset, the whole predictions are accumulated on GPU/NPU/TPU before being
    moved to the CPU (faster but requires more memory).'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_accumulation_steps` (`int`, *å¯é€‰*) â€” åœ¨å°†ç»“æœç§»åŠ¨åˆ° CPU ä¹‹å‰ï¼Œç´¯ç§¯è¾“å‡ºå¼ é‡çš„é¢„æµ‹æ­¥æ•°ã€‚å¦‚æœæœªè®¾ç½®ï¼Œæ•´ä¸ªé¢„æµ‹å°†åœ¨
    GPU/NPU/TPU ä¸Šç´¯ç§¯åå†ç§»åŠ¨åˆ° CPUï¼ˆæ›´å¿«ä½†éœ€è¦æ›´å¤šå†…å­˜ï¼‰ã€‚'
- en: '`eval_delay` (`float`, *optional*) â€” Number of epochs or steps to wait for
    before the first evaluation can be performed, depending on the evaluation_strategy.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_delay` (`float`, *å¯é€‰*) â€” åœ¨è¿›è¡Œç¬¬ä¸€æ¬¡è¯„ä¼°ä¹‹å‰ç­‰å¾…çš„å‘¨æœŸæ•°æˆ–æ­¥æ•°ï¼Œå…·ä½“å–å†³äº evaluation_strategyã€‚'
- en: '`learning_rate` (`float`, *optional*, defaults to 5e-5) â€” The initial learning
    rate for [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 5e-5) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ã€‚'
- en: '`weight_decay` (`float`, *optional*, defaults to 0) â€” The weight decay to apply
    (if not zero) to all layers except all bias and LayerNorm weights in [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” è¦åº”ç”¨çš„æƒé‡è¡°å‡ï¼ˆå¦‚æœä¸ä¸ºé›¶ï¼‰åˆ°æ‰€æœ‰å±‚ï¼Œé™¤äº† [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨ä¸­çš„æ‰€æœ‰åç½®å’Œ LayerNorm æƒé‡ã€‚'
- en: '`adam_beta1` (`float`, *optional*, defaults to 0.9) â€” The beta1 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta1` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.9) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„ beta1 è¶…å‚æ•°ã€‚'
- en: '`adam_beta2` (`float`, *optional*, defaults to 0.999) â€” The beta2 hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta2` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.999) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„ beta2 è¶…å‚æ•°ã€‚'
- en: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) â€” The epsilon hyperparameter
    for the [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    optimizer.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_epsilon` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-8) â€” [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)
    ä¼˜åŒ–å™¨çš„ epsilon è¶…å‚æ•°ã€‚'
- en: '`max_grad_norm` (`float`, *optional*, defaults to 1.0) â€” Maximum gradient norm
    (for gradient clipping).'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_grad_norm` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1.0) â€” æœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºæ¢¯åº¦è£å‰ªï¼‰ã€‚'
- en: '`num_train_epochs(float,` *optional*, defaults to 3.0) â€” Total number of training
    epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_epochs(float,` *å¯é€‰*, é»˜è®¤ä¸º 3.0) â€” è¦æ‰§è¡Œçš„æ€»è®­ç»ƒå‘¨æœŸæ•°ï¼ˆå¦‚æœä¸æ˜¯æ•´æ•°ï¼Œåˆ™åœ¨åœæ­¢è®­ç»ƒä¹‹å‰æ‰§è¡Œæœ€åä¸€ä¸ªå‘¨æœŸçš„å°æ•°éƒ¨åˆ†ç™¾åˆ†æ¯”ï¼‰ã€‚'
- en: '`max_steps` (`int`, *optional*, defaults to -1) â€” If set to a positive number,
    the total number of training steps to perform. Overrides `num_train_epochs`. For
    a finite dataset, training is reiterated through the dataset (if all data is exhausted)
    until `max_steps` is reached.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º -1) â€” å¦‚æœè®¾ç½®ä¸ºæ­£æ•°ï¼Œåˆ™æ‰§è¡Œçš„æ€»è®­ç»ƒæ­¥æ•°ã€‚è¦†ç›– `num_train_epochs`ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é›†ï¼Œå¦‚æœæ‰€æœ‰æ•°æ®éƒ½ç”¨å®Œï¼Œåˆ™é€šè¿‡æ•°æ®é›†é‡æ–°è¿›è¡Œè®­ç»ƒï¼Œç›´åˆ°è¾¾åˆ°
    `max_steps`ã€‚'
- en: '`lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *optional*, defaults to `"linear"`) â€” The scheduler type to use. See the documentation
    of [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    for all possible values.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_type` (`str` æˆ– [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType),
    *å¯é€‰*, é»˜è®¤ä¸º `"linear"`) â€” è¦ä½¿ç”¨çš„è°ƒåº¦å™¨ç±»å‹ã€‚æŸ¥çœ‹ [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)
    çš„æ–‡æ¡£ä»¥è·å–æ‰€æœ‰å¯èƒ½çš„å€¼ã€‚'
- en: '`lr_scheduler_kwargs` (â€˜dictâ€™, *optional*, defaults to {}) â€” The extra arguments
    for the lr_scheduler. See the documentation of each scheduler for possible values.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler_kwargs`ï¼ˆâ€˜dictâ€™, *å¯é€‰*, é»˜è®¤ä¸º {}ï¼‰ â€” lr_scheduler çš„é¢å¤–å‚æ•°ã€‚æŸ¥çœ‹æ¯ä¸ªè°ƒåº¦å™¨çš„æ–‡æ¡£ä»¥è·å–å¯èƒ½çš„å€¼ã€‚'
- en: '`warmup_ratio` (`float`, *optional*, defaults to 0.0) â€” Ratio of total training
    steps used for a linear warmup from 0 to `learning_rate`.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_ratio` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” ç”¨äºä» 0 åˆ° `learning_rate` è¿›è¡Œçº¿æ€§é¢„çƒ­çš„æ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ç‡ã€‚'
- en: '`warmup_steps` (`int`, *optional*, defaults to 0) â€” Number of steps used for
    a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” ç”¨äºä» 0 åˆ° `learning_rate` è¿›è¡Œçº¿æ€§é¢„çƒ­çš„æ­¥æ•°ã€‚è¦†ç›–ä»»ä½•
    `warmup_ratio` çš„æ•ˆæœã€‚'
- en: '`log_level` (`str`, *optional*, defaults to `passive`) â€” Logger log level to
    use on the main process. Possible choices are the log levels as strings: â€˜debugâ€™,
    â€˜infoâ€™, â€˜warningâ€™, â€˜errorâ€™ and â€˜criticalâ€™, plus a â€˜passiveâ€™ level which doesnâ€™t
    set anything and keeps the current log level for the Transformers library (which
    will be `"warning"` by default).'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `passive`) â€” è¦åœ¨ä¸»è¿›ç¨‹ä¸Šä½¿ç”¨çš„è®°å½•å™¨æ—¥å¿—çº§åˆ«ã€‚å¯èƒ½çš„é€‰æ‹©æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ—¥å¿—çº§åˆ«ï¼šâ€˜debugâ€™ã€â€˜infoâ€™ã€â€˜warningâ€™ã€â€˜errorâ€™
    å’Œ â€˜criticalâ€™ï¼Œä»¥åŠä¸€ä¸ªâ€˜passiveâ€™çº§åˆ«ï¼Œå®ƒä¸è®¾ç½®ä»»ä½•å†…å®¹å¹¶ä¿æŒ Transformers åº“çš„å½“å‰æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤ä¸º `"warning"`ï¼‰ã€‚'
- en: '`log_level_replica` (`str`, *optional*, defaults to `"warning"`) â€” Logger log
    level to use on replicas. Same choices as `log_level`â€'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_level_replica` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"warning"`) â€” ç”¨äºå‰¯æœ¬çš„è®°å½•å™¨æ—¥å¿—çº§åˆ«ã€‚ä¸ `log_level`
    ç›¸åŒçš„é€‰æ‹©â€'
- en: '`log_on_each_node` (`bool`, *optional*, defaults to `True`) â€” In multinode
    distributed training, whether to log using `log_level` once per node, or only
    on the main node.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_on_each_node` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” åœ¨å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ˜¯å¦æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ `log_level`
    è¿›è¡Œè®°å½•ï¼Œæˆ–ä»…åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿›è¡Œè®°å½•ã€‚'
- en: '`logging_dir` (`str`, *optional*) â€” [TensorBoard](https://www.tensorflow.org/tensorboard)
    log directory. Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_dir` (`str`, *å¯é€‰*) â€” [TensorBoard](https://www.tensorflow.org/tensorboard)
    æ—¥å¿—ç›®å½•ã€‚å°†é»˜è®¤ä¸º *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***ã€‚'
- en: '`logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” The logging strategy to adopt during training.
    Possible values are:'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_strategy` (`str` æˆ– [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `"steps"`) â€” è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨çš„æ—¥å¿—è®°å½•ç­–ç•¥ã€‚å¯èƒ½çš„å–å€¼æœ‰ï¼š'
- en: '`"no"`: No logging is done during training.'
  id: totrans-735
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒè¿‡ç¨‹ä¸­ä¸è¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`"epoch"`: Logging is done at the end of each epoch.'
  id: totrans-736
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶è¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`"steps"`: Logging is done every `logging_steps`.'
  id: totrans-737
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯ `logging_steps` æ­¥è¿›è¡Œæ—¥å¿—è®°å½•ã€‚'
- en: '`logging_first_step` (`bool`, *optional*, defaults to `False`) â€” Whether to
    log and evaluate the first `global_step` or not.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_first_step` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦è®°å½•å’Œè¯„ä¼°ç¬¬ä¸€ä¸ª `global_step`ã€‚'
- en: '`logging_steps` (`int` or `float`, *optional*, defaults to 500) â€” Number of
    update steps between two logs if `logging_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_steps` (`int` æˆ– `float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 500) â€” å¦‚æœ `logging_strategy="steps"`ï¼Œåˆ™åœ¨ä¸¤æ¬¡æ—¥å¿—ä¹‹é—´çš„æ›´æ–°æ­¥æ•°ã€‚åº”ä¸ºæ•´æ•°æˆ–èŒƒå›´ä¸º
    `[0,1)` çš„æµ®ç‚¹æ•°ã€‚å¦‚æœå°äº 1ï¼Œåˆ™å°†è¢«è§£é‡Šä¸ºæ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ç‡ã€‚'
- en: '`logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) â€” Whether
    to filter `nan` and `inf` losses for logging. If set to `True` the loss of every
    step that is `nan` or `inf` is filtered and the average loss of the current logging
    window is taken instead.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿‡æ»¤ç”¨äºè®°å½•çš„ `nan` å’Œ `inf`
    æŸå¤±ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™ä¼šè¿‡æ»¤æ¯ä¸ªæ­¥éª¤çš„æŸå¤±å€¼ä¸º `nan` æˆ– `inf`ï¼Œå¹¶å–å½“å‰æ—¥å¿—çª—å£çš„å¹³å‡æŸå¤±å€¼ã€‚ '
- en: '`logging_nan_inf_filter` only influences the logging of loss values, it does
    not change the behavior the gradient is computed or applied to the model.'
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`logging_nan_inf_filter` ä»…å½±å“æŸå¤±å€¼çš„è®°å½•ï¼Œä¸ä¼šæ”¹å˜æ¢¯åº¦çš„è®¡ç®—æˆ–åº”ç”¨äºæ¨¡å‹çš„è¡Œä¸ºã€‚'
- en: '`save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy),
    *optional*, defaults to `"steps"`) â€” The checkpoint save strategy to adopt during
    training. Possible values are:'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_strategy` (`str` æˆ– [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `"steps"`) â€” è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨çš„æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥ã€‚å¯èƒ½çš„å–å€¼æœ‰ï¼š'
- en: '`"no"`: No save is done during training.'
  id: totrans-743
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"no"`: è®­ç»ƒè¿‡ç¨‹ä¸­ä¸è¿›è¡Œä¿å­˜ã€‚'
- en: '`"epoch"`: Save is done at the end of each epoch.'
  id: totrans-744
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"epoch"`: æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶ä¿å­˜ã€‚'
- en: '`"steps"`: Save is done every `save_steps`.'
  id: totrans-745
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"steps"`: æ¯ `save_steps` æ­¥ä¿å­˜ä¸€æ¬¡ã€‚'
- en: '`save_steps` (`int` or `float`, *optional*, defaults to 500) â€” Number of updates
    steps before two checkpoint saves if `save_strategy="steps"`. Should be an integer
    or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of
    total training steps.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps` (`int` æˆ– `float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 500) â€” å¦‚æœ `save_strategy="steps"`ï¼Œåˆ™åœ¨ä¸¤æ¬¡æ£€æŸ¥ç‚¹ä¿å­˜ä¹‹é—´çš„æ›´æ–°æ­¥æ•°ã€‚åº”ä¸ºæ•´æ•°æˆ–èŒƒå›´ä¸º
    `[0,1)` çš„æµ®ç‚¹æ•°ã€‚å¦‚æœå°äº 1ï¼Œåˆ™å°†è¢«è§£é‡Šä¸ºæ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ç‡ã€‚'
- en: '`save_total_limit` (`int`, *optional*) â€” If a value is passed, will limit the
    total amount of checkpoints. Deletes the older checkpoints in `output_dir`. When
    `load_best_model_at_end` is enabled, the â€œbestâ€ checkpoint according to `metric_for_best_model`
    will always be retained in addition to the most recent ones. For example, for
    `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will
    always be retained alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`,
    it is possible that two checkpoints are saved: the last one and the best one (if
    they are different).'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_total_limit` (`int`ï¼Œ*å¯é€‰*) â€” å¦‚æœä¼ é€’äº†ä¸€ä¸ªå€¼ï¼Œå°†é™åˆ¶æ£€æŸ¥ç‚¹çš„æ€»é‡ã€‚åˆ é™¤ `output_dir` ä¸­çš„æ—§æ£€æŸ¥ç‚¹ã€‚å½“å¯ç”¨
    `load_best_model_at_end` æ—¶ï¼Œâ€œæœ€ä½³â€æ£€æŸ¥ç‚¹å§‹ç»ˆä¼šä¿ç•™ï¼Œè€Œä¸”è¿˜ä¼šä¿ç•™æœ€è¿‘çš„æ£€æŸ¥ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äº `save_total_limit=5`
    å’Œ `load_best_model_at_end`ï¼Œæœ€åå››ä¸ªæ£€æŸ¥ç‚¹å°†å§‹ç»ˆä¸æœ€ä½³æ¨¡å‹ä¸€èµ·ä¿ç•™ã€‚å½“ `save_total_limit=1` å’Œ `load_best_model_at_end`
    æ—¶ï¼Œå¯èƒ½ä¿å­˜ä¸¤ä¸ªæ£€æŸ¥ç‚¹ï¼šæœ€åä¸€ä¸ªå’Œæœ€ä½³ä¸€ä¸ªï¼ˆå¦‚æœå®ƒä»¬ä¸åŒï¼‰ã€‚'
- en: '`save_safetensors` (`bool`, *optional*, defaults to `True`) â€” Use [safetensors](https://huggingface.co/docs/safetensors)
    saving and loading for state dicts instead of default `torch.load` and `torch.save`.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_safetensors` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” ä½¿ç”¨ [safetensors](https://huggingface.co/docs/safetensors)
    ä¿å­˜å’ŒåŠ è½½çŠ¶æ€å­—å…¸ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„ `torch.load` å’Œ `torch.save`ã€‚'
- en: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) â€” When doing
    multi-node distributed training, whether to save models and checkpoints on each
    node, or only on the main one.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_on_each_node` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” åœ¨è¿›è¡Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œæ˜¯å¦åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ï¼Œè¿˜æ˜¯ä»…åœ¨ä¸»èŠ‚ç‚¹ä¸Šä¿å­˜ã€‚'
- en: This should not be activated when the different nodes use the same storage as
    the files will be saved with the same names for each node.
  id: totrans-750
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ä¸åŒèŠ‚ç‚¹ä½¿ç”¨ç›¸åŒå­˜å‚¨æ—¶ï¼Œä¸åº”æ¿€æ´»æ­¤é€‰é¡¹ï¼Œå› ä¸ºæ–‡ä»¶å°†ä»¥ç›¸åŒåç§°ä¿å­˜åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šã€‚
- en: '`save_only_model` (`bool`, *optional*, defaults to `False`) â€” When checkpointing,
    whether to only save the model, or also the optimizer, scheduler & rng state.
    Note that when this is true, you wonâ€™t be able to resume training from checkpoint.
    This enables you to save storage by not storing the optimizer, scheduler & rng
    state. You can only load the model using `from_pretrained` with this option set
    to `True`.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_only_model` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” åœ¨æ£€æŸ¥ç‚¹æ—¶ï¼Œæ˜¯å¦ä»…ä¿å­˜æ¨¡å‹ï¼Œè¿˜æ˜¯åŒæ—¶ä¿å­˜ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œ
    RNG çŠ¶æ€ã€‚è¯·æ³¨æ„ï¼Œå½“æ­¤é€‰é¡¹ä¸ºçœŸæ—¶ï¼Œæ‚¨å°†æ— æ³•ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚è¿™æ ·å¯ä»¥é€šè¿‡ä¸å­˜å‚¨ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œ RNG çŠ¶æ€æ¥èŠ‚çœå­˜å‚¨ç©ºé—´ã€‚æ‚¨åªèƒ½ä½¿ç”¨ `from_pretrained`
    åŠ è½½æ¨¡å‹ï¼Œå¹¶å°†æ­¤é€‰é¡¹è®¾ç½®ä¸º `True`ã€‚'
- en: '`use_cpu` (`bool`, *optional*, defaults to `False`) â€” Whether or not to use
    cpu. If set to False, we will use cuda or mps device if available.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cpu` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ CPUã€‚å¦‚æœè®¾ç½®ä¸º Falseï¼Œå°†ä½¿ç”¨ cuda æˆ– mps è®¾å¤‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚'
- en: '`seed` (`int`, *optional*, defaults to 42) â€” Random seed that will be set at
    the beginning of training. To ensure reproducibility across runs, use the `~Trainer.model_init`
    function to instantiate the model if it has some randomly initialized parameters.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 42) â€” åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®çš„éšæœºç§å­ã€‚ä¸ºäº†ç¡®ä¿è·¨è¿è¡Œçš„å¯é‡ç°æ€§ï¼Œè¯·ä½¿ç”¨ `~Trainer.model_init`
    å‡½æ•°æ¥å®ä¾‹åŒ–æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹å…·æœ‰ä¸€äº›éšæœºåˆå§‹åŒ–çš„å‚æ•°ã€‚'
- en: '`data_seed` (`int`, *optional*) â€” Random seed to be used with data samplers.
    If not set, random generators for data sampling will use the same seed as `seed`.
    This can be used to ensure reproducibility of data sampling, independent of the
    model seed.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_seed` (`int`, *optional*) â€” ç”¨äºæ•°æ®é‡‡æ ·çš„éšæœºç§å­ã€‚å¦‚æœæœªè®¾ç½®ï¼Œæ•°æ®é‡‡æ ·çš„éšæœºç”Ÿæˆå™¨å°†ä½¿ç”¨ä¸`seed`ç›¸åŒçš„ç§å­ã€‚è¿™å¯ç”¨äºç¡®ä¿æ•°æ®é‡‡æ ·çš„å¯é‡ç°æ€§ï¼Œä¸æ¨¡å‹ç§å­æ— å…³ã€‚'
- en: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to use PyTorch jit trace for inference.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jit_mode_eval` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨PyTorch jitè·Ÿè¸ªè¿›è¡Œæ¨æ–­ã€‚'
- en: '`use_ipex` (`bool`, *optional*, defaults to `False`) â€” Use Intel extension
    for PyTorch when it is available. [IPEX installation](https://github.com/intel/intel-extension-for-pytorch).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_ipex` (`bool`, *optional*, defaults to `False`) â€” åœ¨PyTorchå¯ç”¨æ—¶ä½¿ç”¨Intelæ‰©å±•ã€‚[IPEXå®‰è£…](https://github.com/intel/intel-extension-for-pytorch)ã€‚'
- en: '`bf16` (`bool`, *optional*, defaults to `False`) â€” Whether to use bf16 16-bit
    (mixed) precision training instead of 32-bit training. Requires Ampere or higher
    NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental
    API and it may change.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨bf16 16ä½ï¼ˆæ··åˆï¼‰ç²¾åº¦è®­ç»ƒï¼Œè€Œä¸æ˜¯32ä½è®­ç»ƒã€‚éœ€è¦å®‰æ™®å°”æˆ–æ›´é«˜çš„NVIDIAæ¶æ„æˆ–ä½¿ç”¨CPUï¼ˆuse_cpuï¼‰æˆ–Ascend
    NPUã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§çš„APIï¼Œå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚'
- en: '`fp16` (`bool`, *optional*, defaults to `False`) â€” Whether to use fp16 16-bit
    (mixed) precision training instead of 32-bit training.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨fp16 16ä½ï¼ˆæ··åˆï¼‰ç²¾åº¦è®­ç»ƒï¼Œè€Œä¸æ˜¯32ä½è®­ç»ƒã€‚'
- en: '`fp16_opt_level` (`str`, *optional*, defaults to â€˜O1â€™) â€” For `fp16` training,
    Apex AMP optimization level selected in [â€˜O0â€™, â€˜O1â€™, â€˜O2â€™, and â€˜O3â€™]. See details
    on the [Apex documentation](https://nvidia.github.io/apex/amp).'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_opt_level` (`str`, *optional*, defaults to â€˜O1â€™) â€” å¯¹äº`fp16`è®­ç»ƒï¼Œé€‰æ‹©åœ¨[â€˜O0â€™,
    â€˜O1â€™, â€˜O2â€™, å’Œ â€˜O3â€™]ä¸­çš„Apex AMPä¼˜åŒ–çº§åˆ«ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Apexæ–‡æ¡£](https://nvidia.github.io/apex/amp)ã€‚'
- en: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) â€” This argument is
    deprecated. Use `half_precision_backend` instead.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_backend` (`str`, *optional*, defaults to `"auto"`) â€” æ­¤å‚æ•°å·²å¼ƒç”¨ã€‚è¯·æ”¹ç”¨`half_precision_backend`ã€‚'
- en: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) â€” The backend
    to use for mixed precision training. Must be one of `"auto", "apex", "cpu_amp"`.
    `"auto"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected,
    while the other choices will force the requested backend.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`half_precision_backend` (`str`, *optional*, defaults to `"auto"`) â€” ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒçš„åç«¯ã€‚å¿…é¡»æ˜¯`"auto",
    "apex", "cpu_amp"`ä¹‹ä¸€ã€‚`"auto"`å°†æ ¹æ®æ£€æµ‹åˆ°çš„PyTorchç‰ˆæœ¬ä½¿ç”¨CPU/CUDA AMPæˆ–APEXï¼Œè€Œå…¶ä»–é€‰æ‹©å°†å¼ºåˆ¶ä½¿ç”¨è¯·æ±‚çš„åç«¯ã€‚'
- en: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    full bfloat16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values. This is an experimental API and it may change.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bf16_full_eval` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨å®Œæ•´çš„bfloat16è¯„ä¼°ï¼Œè€Œä¸æ˜¯32ä½ã€‚è¿™å°†æ›´å¿«ï¼ŒèŠ‚çœå†…å­˜ï¼Œä½†å¯èƒ½ä¼šæŸå®³æŒ‡æ ‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§çš„APIï¼Œå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚'
- en: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    full float16 evaluation instead of 32-bit. This will be faster and save memory
    but can harm metric values.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_full_eval` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨å®Œæ•´çš„float16è¯„ä¼°ï¼Œè€Œä¸æ˜¯32ä½ã€‚è¿™å°†æ›´å¿«ï¼ŒèŠ‚çœå†…å­˜ï¼Œä½†å¯èƒ½ä¼šæŸå®³æŒ‡æ ‡å€¼ã€‚'
- en: '`tf32` (`bool`, *optional*) â€” Whether to enable the TF32 mode, available in
    Ampere and newer GPU architectures. The default value depends on PyTorchâ€™s version
    default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer
    to the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation.
    This is an experimental API and it may change.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf32` (`bool`, *optional*) â€” æ˜¯å¦å¯ç”¨TF32æ¨¡å¼ï¼Œé€‚ç”¨äºAmpereå’Œæ›´æ–°çš„GPUæ¶æ„ã€‚é»˜è®¤å€¼å–å†³äºPyTorchçš„ç‰ˆæœ¬é»˜è®¤å€¼`torch.backends.cuda.matmul.allow_tf32`ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[TF32](https://huggingface.co/docs/transformers/performance#tf32)æ–‡æ¡£ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§çš„APIï¼Œå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚'
- en: '`local_rank` (`int`, *optional*, defaults to -1) â€” Rank of the process during
    distributed training.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_rank` (`int`, *optional*, defaults to -1) â€” åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´è¿›ç¨‹çš„æ’åã€‚'
- en: '`ddp_backend` (`str`, *optional*) â€” The backend to use for distributed training.
    Must be one of `"nccl"`, `"mpi"`, `"ccl"`, `"gloo"`, `"hccl"`.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_backend` (`str`, *optional*) â€” ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„åç«¯ã€‚å¿…é¡»æ˜¯`"nccl"`, `"mpi"`, `"ccl"`,
    `"gloo"`, `"hccl"`ä¹‹ä¸€ã€‚'
- en: '`tpu_num_cores` (`int`, *optional*) â€” When training on TPU, the number of TPU
    cores (automatically passed by launcher script).'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tpu_num_cores` (`int`, *optional*) â€” åœ¨TPUä¸Šè®­ç»ƒæ—¶ï¼ŒTPUæ ¸å¿ƒçš„æ•°é‡ï¼ˆç”±å¯åŠ¨è„šæœ¬è‡ªåŠ¨ä¼ é€’ï¼‰ã€‚'
- en: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) â€” Whether
    to drop the last incomplete batch (if the length of the dataset is not divisible
    by the batch size) or not.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_drop_last` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ï¼ˆå¦‚æœæ•°æ®é›†çš„é•¿åº¦ä¸èƒ½è¢«æ‰¹æ¬¡å¤§å°æ•´é™¤ï¼‰ã€‚'
- en: '`eval_steps` (`int` or `float`, *optional*) â€” Number of update steps between
    two evaluations if `evaluation_strategy="steps"`. Will default to the same value
    as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`.
    If smaller than 1, will be interpreted as ratio of total training steps.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps` (`int` or `float`, *optional*) â€” å¦‚æœ`evaluation_strategy="steps"`ï¼Œåˆ™ä¸¤æ¬¡è¯„ä¼°ä¹‹é—´çš„æ›´æ–°æ­¥æ•°ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºä¸`logging_steps`ç›¸åŒçš„å€¼ã€‚åº”ä¸ºèŒƒå›´åœ¨`[0,1)`çš„æ•´æ•°æˆ–æµ®ç‚¹æ•°ã€‚å¦‚æœå°äº1ï¼Œå°†è¢«è§£é‡Šä¸ºæ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ç‡ã€‚'
- en: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) â€” Number of subprocesses
    to use for data loading (PyTorch only). 0 means that the data will be loaded in
    the main process.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_num_workers` (`int`, *optional*, defaults to 0) â€” ç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°ï¼ˆä»…é€‚ç”¨äºPyTorchï¼‰ã€‚0è¡¨ç¤ºæ•°æ®å°†åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½ã€‚'
- en: '`past_index` (`int`, *optional*, defaults to -1) â€” Some models like [TransformerXL](../model_doc/transformerxl)
    or [XLNet](../model_doc/xlnet) can make use of the past hidden states for their
    predictions. If this argument is set to a positive int, the `Trainer` will use
    the corresponding output (usually index 2) as the past state and feed it to the
    model at the next training step under the keyword argument `mems`.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_index` (`int`, *optional*, defaults to -1) â€” ä¸€äº›æ¨¡å‹ï¼ˆå¦‚[TransformerXL](../model_doc/transformerxl)æˆ–[XLNet](../model_doc/xlnet)ï¼‰å¯ä»¥åˆ©ç”¨è¿‡å»çš„éšè—çŠ¶æ€è¿›è¡Œé¢„æµ‹ã€‚å¦‚æœå°†æ­¤å‚æ•°è®¾ç½®ä¸ºæ­£æ•´æ•°ï¼Œåˆ™`Trainer`å°†ä½¿ç”¨ç›¸åº”çš„è¾“å‡ºï¼ˆé€šå¸¸ä¸ºç´¢å¼•2ï¼‰ä½œä¸ºè¿‡å»çŠ¶æ€ï¼Œå¹¶åœ¨ä¸‹ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­å°†å…¶ä½œä¸ºå…³é”®å­—å‚æ•°`mems`æä¾›ç»™æ¨¡å‹ã€‚'
- en: '`run_name` (`str`, *optional*) â€” A descriptor for the run. Typically used for
    [wandb](https://www.wandb.com/) and [mlflow](https://www.mlflow.org/) logging.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run_name` (`str`, *optional*) â€” è¿è¡Œçš„æè¿°ç¬¦ã€‚é€šå¸¸ç”¨äº[wandb](https://www.wandb.com/)å’Œ[mlflow](https://www.mlflow.org/)æ—¥å¿—è®°å½•ã€‚'
- en: '`disable_tqdm` (`bool`, *optional*) â€” Whether or not to disable the tqdm progress
    bars and table of metrics produced by `~notebook.NotebookTrainingTracker` in Jupyter
    Notebooks. Will default to `True` if the logging level is set to warn or lower
    (default), `False` otherwise.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_tqdm` (`bool`, *optional*) â€” æ˜¯å¦ç¦ç”¨Jupyterç¬”è®°æœ¬ä¸­`~notebook.NotebookTrainingTracker`ç”Ÿæˆçš„tqdmè¿›åº¦æ¡å’ŒæŒ‡æ ‡è¡¨ã€‚å¦‚æœæ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºwarnæˆ–æ›´ä½ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚'
- en: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to automatically remove the columns unused by the model forward method.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_unused_columns` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è‡ªåŠ¨åˆ é™¤æ¨¡å‹å‰å‘æ–¹æ³•æœªä½¿ç”¨çš„åˆ—ã€‚'
- en: '`label_names` (`List[str]`, *optional*) â€” The list of keys in your dictionary
    of inputs that correspond to the labels.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_names` (`List[str]`, *optional*) â€” æ‚¨çš„è¾“å…¥å­—å…¸ä¸­å¯¹åº”äºæ ‡ç­¾çš„é”®åˆ—è¡¨ã€‚'
- en: Will eventually default to the list of argument names accepted by the model
    that contain the word â€œlabelâ€, except if the model used is one of the `XxxForQuestionAnswering`
    in which case it will also include the `["start_positions", "end_positions"]`
    keys.
  id: totrans-776
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå°†é»˜è®¤ä¸ºæ¨¡å‹æ¥å—çš„å‚æ•°åç§°åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å•è¯â€œlabelâ€ï¼Œé™¤éä½¿ç”¨çš„æ¨¡å‹æ˜¯`XxxForQuestionAnswering`ä¹‹ä¸€ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹è¿˜å°†åŒ…æ‹¬`["start_positions",
    "end_positions"]`é”®ã€‚
- en: '`load_best_model_at_end` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to load the best model found during training at the end of training. When
    this option is enabled, the best checkpoint will always be saved. See [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)
    for more.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_best_model_at_end` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæœ€ä½³æ£€æŸ¥ç‚¹å°†å§‹ç»ˆè¢«ä¿å­˜ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§[`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)ã€‚'
- en: When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`,
    and in the case it is â€œstepsâ€, `save_steps` must be a round multiple of `eval_steps`.
  id: totrans-778
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸º`True`æ—¶ï¼Œå‚æ•°`save_strategy`éœ€è¦ä¸`evaluation_strategy`ç›¸åŒï¼Œå¦‚æœæ˜¯â€œstepsâ€ï¼Œåˆ™`save_steps`å¿…é¡»æ˜¯`eval_steps`çš„æ•´æ•°å€ã€‚
- en: '`metric_for_best_model` (`str`, *optional*) â€” Use in conjunction with `load_best_model_at_end`
    to specify the metric to use to compare two different models. Must be the name
    of a metric returned by the evaluation with or without the prefix `"eval_"`. Will
    default to `"loss"` if unspecified and `load_best_model_at_end=True` (to use the
    evaluation loss).'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_for_best_model` (`str`, *optional*) â€” ä¸`load_best_model_at_end`ä¸€èµ·ä½¿ç”¨ï¼ŒæŒ‡å®šç”¨äºæ¯”è¾ƒä¸¤ä¸ªä¸åŒæ¨¡å‹çš„åº¦é‡æ ‡å‡†ã€‚å¿…é¡»æ˜¯è¯„ä¼°è¿”å›çš„åº¦é‡çš„åç§°ï¼Œå¸¦æœ‰æˆ–ä¸å¸¦æœ‰å‰ç¼€`"eval_"`ã€‚å¦‚æœæœªæŒ‡å®šä¸”`load_best_model_at_end=True`ï¼ˆä½¿ç”¨è¯„ä¼°æŸå¤±ï¼‰ï¼Œå°†é»˜è®¤ä¸º`"loss"`ã€‚'
- en: If you set this value, `greater_is_better` will default to `True`. Donâ€™t forget
    to set it to `False` if your metric is better when lower.
  id: totrans-780
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè®¾ç½®äº†æ­¤å€¼ï¼Œ`greater_is_better`å°†é»˜è®¤ä¸º`True`ã€‚å¦‚æœæ‚¨çš„åº¦é‡æ ‡å‡†è¾ƒä½æ—¶æ›´å¥½ï¼Œè¯·ä¸è¦å¿˜è®°å°†å…¶è®¾ç½®ä¸º`False`ã€‚
- en: '`greater_is_better` (`bool`, *optional*) â€” Use in conjunction with `load_best_model_at_end`
    and `metric_for_best_model` to specify if better models should have a greater
    metric or not. Will default to:'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`greater_is_better` (`bool`, *optional*) â€” ä¸`load_best_model_at_end`å’Œ`metric_for_best_model`ä¸€èµ·ä½¿ç”¨ï¼ŒæŒ‡å®šæ›´å¥½çš„æ¨¡å‹æ˜¯å¦åº”å…·æœ‰æ›´å¤§çš„åº¦é‡æ ‡å‡†ã€‚é»˜è®¤ä¸ºï¼š'
- en: '`True` if `metric_for_best_model` is set to a value that isnâ€™t `"loss"` or
    `"eval_loss"`.'
  id: totrans-782
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœ`metric_for_best_model`è®¾ç½®ä¸ºä¸æ˜¯`"loss"`æˆ–`"eval_loss"`çš„å€¼ï¼Œåˆ™ä¸º`True`ã€‚
- en: '`False` if `metric_for_best_model` is not set, or set to `"loss"` or `"eval_loss"`.'
  id: totrans-783
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®`metric_for_best_model`ï¼Œæˆ–è®¾ç½®ä¸º`"loss"`æˆ–`"eval_loss"`ï¼Œåˆ™ä¸º`False`ã€‚
- en: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) â€” When resuming
    training, whether or not to skip the epochs and batches to get the data loading
    at the same stage as in the previous training. If set to `True`, the training
    will begin faster (as that skipping step can take a long time) but will not yield
    the same results as the interrupted training would have.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_data_skip` (`bool`, *optional*, defaults to `False`) â€” æ¢å¤è®­ç»ƒæ—¶ï¼Œæ˜¯å¦è·³è¿‡æ‰¹æ¬¡å’Œè½®æ¬¡ä»¥ä½¿æ•°æ®åŠ è½½ä¸å…ˆå‰è®­ç»ƒçš„é˜¶æ®µç›¸åŒã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œè®­ç»ƒå°†æ›´å¿«å¼€å§‹ï¼ˆå› ä¸ºè·³è¿‡æ­¥éª¤å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰ï¼Œä½†ä¸ä¼šäº§ç”Ÿä¸ä¸­æ–­è®­ç»ƒç›¸åŒçš„ç»“æœã€‚'
- en: '`fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''''`)
    â€” Use PyTorch Distributed Parallel Training (in distributed training only).'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''''`)
    â€” ä½¿ç”¨PyTorchåˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒï¼ˆä»…åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼‰ã€‚'
- en: 'A list of options along the following:'
  id: totrans-786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹é€‰é¡¹åˆ—è¡¨ï¼š
- en: '`"full_shard"`: Shard parameters, gradients and optimizer states.'
  id: totrans-787
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"full_shard"`: åˆ†ç‰‡å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚'
- en: '`"shard_grad_op"`: Shard optimizer states and gradients.'
  id: totrans-788
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"shard_grad_op"`: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ã€‚'
- en: '`"hybrid_shard"`: Apply `FULL_SHARD` within a node, and replicate parameters
    across nodes.'
  id: totrans-789
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard"`: åœ¨èŠ‚ç‚¹å†…åº”ç”¨`FULL_SHARD`ï¼Œå¹¶åœ¨èŠ‚ç‚¹ä¹‹é—´å¤åˆ¶å‚æ•°ã€‚'
- en: '`"hybrid_shard_zero2"`: Apply `SHARD_GRAD_OP` within a node, and replicate
    parameters across nodes.'
  id: totrans-790
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hybrid_shard_zero2"`: åœ¨èŠ‚ç‚¹å†…åº”ç”¨`SHARD_GRAD_OP`ï¼Œå¹¶åœ¨èŠ‚ç‚¹ä¹‹é—´å¤åˆ¶å‚æ•°ã€‚'
- en: '`"offload"`: Offload parameters and gradients to CPUs (only compatible with
    `"full_shard"` and `"shard_grad_op"`).'
  id: totrans-791
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"offload"`: å°†å‚æ•°å’Œæ¢¯åº¦å¸è½½åˆ°CPUï¼ˆä»…ä¸`"full_shard"`å’Œ`"shard_grad_op"`å…¼å®¹ï¼‰ã€‚'
- en: '`"auto_wrap"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.'
  id: totrans-792
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"auto_wrap"`: ä½¿ç”¨`default_auto_wrap_policy`è‡ªåŠ¨é€’å½’åŒ…è£…å±‚ä¸FSDPã€‚'
- en: '`fsdp_config` (`str` or `dict`, *optional*) â€” Config to be used with fsdp (Pytorch
    Distributed Parallel Training). The value is either a location of fsdp json config
    file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp_config` (`str` or `dict`, *optional*) â€” ç”¨äºfsdpï¼ˆPytorchåˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒï¼‰çš„é…ç½®ã€‚è¯¥å€¼å¯ä»¥æ˜¯fsdp
    jsoné…ç½®æ–‡ä»¶çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œ`fsdp_config.json`ï¼‰æˆ–å·²åŠ è½½çš„jsonæ–‡ä»¶ä½œä¸º`dict`ã€‚'
- en: 'A List of config and its options:'
  id: totrans-794
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é…ç½®åŠå…¶é€‰é¡¹åˆ—è¡¨ï¼š
- en: 'min_num_params (`int`, *optional*, defaults to `0`): FSDPâ€™s minimum number
    of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).'
  id: totrans-795
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'min_num_params (`int`, *optional*, defaults to `0`): FSDPçš„é»˜è®¤è‡ªåŠ¨åŒ…è£…çš„å‚æ•°æœ€å°æ•°é‡ã€‚ï¼ˆä»…åœ¨ä¼ é€’`fsdp`å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚'
- en: 'transformer_layer_cls_to_wrap (`List[str]`, *optional*): List of transformer
    layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`, `T5Block`
    â€¦ (useful only when `fsdp` flag is passed).'
  id: totrans-796
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformer_layer_cls_to_wrapï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰ï¼šè¦åŒ…è£…çš„transformerå±‚ç±»åç§°åˆ—è¡¨ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼Œä¾‹å¦‚ï¼Œ`BertLayer`ï¼Œ`GPTJBlock`ï¼Œ`T5Block`
    â€¦ï¼ˆä»…åœ¨ä¼ é€’`fsdp`æ ‡å¿—æ—¶æœ‰ç”¨ï¼‰ã€‚
- en: backward_prefetch (`str`, *optional*) FSDPâ€™s backward prefetch mode. Controls
    when to prefetch next set of parameters (useful only when `fsdp` field is passed).
  id: totrans-797
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: backward_prefetchï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰FSDPçš„åå‘é¢„å–æ¨¡å¼ã€‚æ§åˆ¶ä½•æ—¶é¢„å–ä¸‹ä¸€ç»„å‚æ•°ï¼ˆä»…åœ¨ä¼ é€’`fsdp`å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚
- en: 'A list of options along the following:'
  id: totrans-798
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ç³»åˆ—é€‰é¡¹ï¼š
- en: '`"backward_pre"` : Prefetches the next set of parameters before the current
    set of parameterâ€™s gradient computation.'
  id: totrans-799
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_pre"`ï¼šåœ¨å½“å‰å‚æ•°æ¢¯åº¦è®¡ç®—ä¹‹å‰é¢„å–ä¸‹ä¸€ç»„å‚æ•°ã€‚'
- en: '`"backward_post"` : This prefetches the next set of parameters after the current
    set of parameterâ€™s gradient computation.'
  id: totrans-800
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"backward_post"`ï¼šåœ¨å½“å‰å‚æ•°æ¢¯åº¦è®¡ç®—ä¹‹åé¢„å–ä¸‹ä¸€ç»„å‚æ•°ã€‚'
- en: forward_prefetch (`bool`, *optional*, defaults to `False`) FSDPâ€™s forward prefetch
    mode (useful only when `fsdp` field is passed). If `"True"`, then FSDP explicitly
    prefetches the next upcoming all-gather while executing in the forward pass.
  id: totrans-801
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: forward_prefetchï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰FSDPçš„å‰å‘é¢„å–æ¨¡å¼ï¼ˆä»…åœ¨ä¼ é€’`fsdp`å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚å¦‚æœä¸º`"True"`ï¼Œåˆ™FSDPä¼šåœ¨å‰å‘ä¼ é€’ä¸­æ˜¾å¼é¢„å–ä¸‹ä¸€ä¸ªå³å°†åˆ°æ¥çš„all-gatherã€‚
- en: limit_all_gathers (`bool`, *optional*, defaults to `False`) FSDPâ€™s limit_all_gathers
    (useful only when `fsdp` field is passed). If `"True"`, FSDP explicitly synchronizes
    the CPU thread to prevent too many in-flight all-gathers.
  id: totrans-802
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: limit_all_gathersï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰FSDPçš„limit_all_gathersï¼ˆä»…åœ¨ä¼ é€’`fsdp`å­—æ®µæ—¶æœ‰ç”¨ï¼‰ã€‚å¦‚æœä¸º`"True"`ï¼ŒFSDPä¼šæ˜¾å¼åŒæ­¥CPUçº¿ç¨‹ï¼Œä»¥é˜²æ­¢å¤ªå¤šçš„in-flight
    all-gathersã€‚
- en: use_orig_params (`bool`, *optional*, defaults to `True`) If `"True"`, allows
    non-uniform `requires_grad` during init, which means support for interspersed
    frozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning.
    Please refer this [blog]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
  id: totrans-803
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: use_orig_paramsï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰å¦‚æœä¸º`"True"`ï¼Œå…è®¸åœ¨åˆå§‹åŒ–æœŸé—´ä½¿ç”¨éå‡åŒ€çš„`requires_grad`ï¼Œè¿™æ„å‘³ç€æ”¯æŒäº¤æ›¿å†»ç»“å’Œå¯è®­ç»ƒå‚æ•°ã€‚åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒç­‰æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ã€‚è¯·å‚è€ƒæ­¤[åšå®¢]ï¼ˆ[https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
- en: sync_module_states (`bool`, *optional*, defaults to `True`) If `"True"`, each
    individually wrapped FSDP unit will broadcast module parameters from rank 0 to
    ensure they are the same across all ranks after initialization
  id: totrans-804
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: sync_module_statesï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰å¦‚æœä¸º`"True"`ï¼Œæ¯ä¸ªå•ç‹¬åŒ…è£…çš„FSDPå•å…ƒå°†ä»rank 0å¹¿æ’­æ¨¡å—å‚æ•°ï¼Œä»¥ç¡®ä¿åœ¨åˆå§‹åŒ–åæ‰€æœ‰rankä¸­çš„å‚æ•°ç›¸åŒ
- en: 'activation_checkpointing (`bool`, *optional*, defaults to `False`): If `"True"`,
    activation checkpointing is a technique to reduce memory usage by clearing activations
    of certain layers and recomputing them during a backward pass. Effectively, this
    trades extra computation time for reduced memory usage.'
  id: totrans-805
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: activation_checkpointingï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šå¦‚æœä¸º`"True"`ï¼Œæ¿€æ´»æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§é€šè¿‡æ¸…é™¤æŸäº›å±‚çš„æ¿€æ´»å¹¶åœ¨åå‘ä¼ é€’æœŸé—´é‡æ–°è®¡ç®—å®ƒä»¬æ¥å‡å°‘å†…å­˜ä½¿ç”¨çš„æŠ€æœ¯ã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯ä»¥é¢å¤–çš„è®¡ç®—æ—¶é—´æ¢å–å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: 'xla (`bool`, *optional*, defaults to `False`): Whether to use PyTorch/XLA Fully
    Sharded Data Parallel Training. This is an experimental feature and its API may
    evolve in the future.'
  id: totrans-806
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xlaï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šæ˜¯å¦ä½¿ç”¨PyTorch/XLAå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå…¶APIå¯èƒ½ä¼šåœ¨æœªæ¥å‘ç”Ÿå˜åŒ–ã€‚
- en: xla_fsdp_settings (`dict`, *optional*) The value is a dictionary which stores
    the XLA FSDP wrapping parameters.
  id: totrans-807
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_settingsï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰è¯¥å€¼æ˜¯ä¸€ä¸ªå­˜å‚¨XLA FSDPåŒ…è£…å‚æ•°çš„å­—å…¸ã€‚
- en: For a complete list of options, please see [here](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).
  id: totrans-808
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ‰å…³æ‰€æœ‰é€‰é¡¹çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚è§[æ­¤å¤„](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)ã€‚
- en: 'xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`): Will use gradient
    checkpointing over each nested XLA FSDP wrapped layer. This setting can only be
    used when the xla flag is set to true, and an auto wrapping policy is specified
    through fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.'
  id: totrans-809
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xla_fsdp_grad_ckptï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ï¼šå°†åœ¨æ¯ä¸ªåµŒå¥—çš„XLA FSDPåŒ…è£…å±‚ä¸Šä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚åªæœ‰åœ¨å°†xlaæ ‡å¿—è®¾ç½®ä¸ºtrueï¼Œå¹¶é€šè¿‡fsdp_min_num_paramsæˆ–fsdp_transformer_layer_cls_to_wrapæŒ‡å®šäº†è‡ªåŠ¨åŒ…è£…ç­–ç•¥æ—¶æ‰èƒ½ä½¿ç”¨æ­¤è®¾ç½®ã€‚
- en: '`deepspeed` (`str` or `dict`, *optional*) â€” Use [Deepspeed](https://github.com/microsoft/deepspeed).
    This is an experimental feature and its API may evolve in the future. The value
    is either the location of DeepSpeed json config file (e.g., `ds_config.json`)
    or an already loaded json file as a `dict`â€'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepspeed`ï¼ˆ`str`æˆ–`dict`ï¼Œ*å¯é€‰*ï¼‰â€”ä½¿ç”¨[Deepspeed](https://github.com/microsoft/deepspeed)ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå…¶APIå¯èƒ½ä¼šåœ¨æœªæ¥å‘ç”Ÿå˜åŒ–ã€‚è¯¥å€¼å¯ä»¥æ˜¯DeepSpeed
    jsoné…ç½®æ–‡ä»¶çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œ`ds_config.json`ï¼‰æˆ–å·²åŠ è½½çš„jsonæ–‡ä»¶ä½œä¸º`dict`â€'
- en: '`label_smoothing_factor` (`float`, *optional*, defaults to 0.0) â€” The label
    smoothing factor to use. Zero means no label smoothing, otherwise the underlying
    onehot-encoded labels are changed from 0s and 1s to `label_smoothing_factor/num_labels`
    and `1 - label_smoothing_factor + label_smoothing_factor/num_labels` respectively.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€”è¦ä½¿ç”¨çš„æ ‡ç­¾å¹³æ»‘å› å­ã€‚é›¶è¡¨ç¤ºä¸è¿›è¡Œæ ‡ç­¾å¹³æ»‘ï¼Œå¦åˆ™åŸºç¡€çš„onehotç¼–ç æ ‡ç­¾å°†ä»0å’Œ1æ›´æ”¹ä¸º`label_smoothing_factor/num_labels`å’Œ`1
    - label_smoothing_factor + label_smoothing_factor/num_labels`ã€‚'
- en: '`debug` (`str` or list of `DebugOption`, *optional*, defaults to `""`) â€” Enable
    one or more debug features. This is an experimental feature.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug`ï¼ˆ`str`æˆ–`DebugOption`åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`""`ï¼‰â€”å¯ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªè°ƒè¯•åŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ã€‚'
- en: 'Possible options are:'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯èƒ½çš„é€‰é¡¹æœ‰ï¼š
- en: '`"underflow_overflow"`: detects overflow in modelâ€™s input/outputs and reports
    the last frames that led to the event'
  id: totrans-814
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"underflow_overflow"`ï¼šæ£€æµ‹æ¨¡å‹è¾“å…¥/è¾“å‡ºä¸­çš„æº¢å‡ºå¹¶æŠ¥å‘Šå¯¼è‡´äº‹ä»¶çš„æœ€åå¸§'
- en: '`"tpu_metrics_debug"`: print debug metrics on TPU'
  id: totrans-815
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"tpu_metrics_debug"`ï¼šåœ¨TPUä¸Šæ‰“å°è°ƒè¯•æŒ‡æ ‡'
- en: The options should be separated by whitespaces.
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é€‰é¡¹åº”è¯¥ç”¨ç©ºæ ¼åˆ†éš”ã€‚
- en: '`optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    â€” The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused,
    adamw_anyprecision or adafactor.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`)
    â€” è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼šadamw_hfã€adamw_torchã€adamw_torch_fusedã€adamw_apex_fusedã€adamw_anyprecision
    æˆ– adafactorã€‚'
- en: '`optim_args` (`str`, *optional*) â€” Optional arguments that are supplied to
    AnyPrecisionAdamW.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optim_args` (`str`, *optional*) â€” æä¾›ç»™ AnyPrecisionAdamW çš„å¯é€‰å‚æ•°ã€‚'
- en: '`group_by_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to group together samples of roughly the same length in the training dataset (to
    minimize padding applied and be more efficient). Only useful if applying dynamic
    padding.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_by_length` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å°†å¤§è‡´ç›¸åŒé•¿åº¦çš„æ ·æœ¬åˆ†ç»„åœ¨ä¸€èµ·ï¼ˆä»¥æœ€å°åŒ–å¡«å……å¹¶æé«˜æ•ˆç‡ï¼‰ã€‚ä»…åœ¨åº”ç”¨åŠ¨æ€å¡«å……æ—¶æœ‰ç”¨ã€‚'
- en: '`length_column_name` (`str`, *optional*, defaults to `"length"`) â€” Column name
    for precomputed lengths. If the column exists, grouping by length will use these
    values rather than computing them on train startup. Ignored unless `group_by_length`
    is `True` and the dataset is an instance of `Dataset`.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_column_name` (`str`, *optional*, defaults to `"length"`) â€” é¢„å…ˆè®¡ç®—é•¿åº¦çš„åˆ—åã€‚å¦‚æœè¯¥åˆ—å­˜åœ¨ï¼Œåˆ™æŒ‰é•¿åº¦åˆ†ç»„å°†ä½¿ç”¨è¿™äº›å€¼è€Œä¸æ˜¯åœ¨è®­ç»ƒå¯åŠ¨æ—¶è®¡ç®—å®ƒä»¬ã€‚ä»…åœ¨
    `group_by_length` ä¸º `True` ä¸”æ•°æ®é›†æ˜¯ `Dataset` çš„å®ä¾‹æ—¶æ‰ä¼šè¢«å¿½ç•¥ã€‚'
- en: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) â€” The list
    of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
    `"clearml"`, `"codecarbon"`, `"comet_ml"`, `"dagshub"`, `"dvclive"`, `"flyte"`,
    `"mlflow"`, `"neptune"`, `"tensorboard"`, and `"wandb"`. Use `"all"` to report
    to all integrations installed, `"none"` for no integrations.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) â€” æŠ¥å‘Šç»“æœå’Œæ—¥å¿—çš„é›†æˆåˆ—è¡¨ã€‚æ”¯æŒçš„å¹³å°æœ‰
    `"azure_ml"`ã€`"clearml"`ã€`"codecarbon"`ã€`"comet_ml"`ã€`"dagshub"`ã€`"dvclive"`ã€`"flyte"`ã€`"mlflow"`ã€`"neptune"`ã€`"tensorboard"`
    å’Œ `"wandb"`ã€‚ä½¿ç”¨ `"all"` æŠ¥å‘Šåˆ°æ‰€æœ‰å·²å®‰è£…çš„é›†æˆï¼Œä½¿ç”¨ `"none"` ä¸æŠ¥å‘Šåˆ°ä»»ä½•é›†æˆã€‚'
- en: '`ddp_find_unused_parameters` (`bool`, *optional*) â€” When using distributed
    training, the value of the flag `find_unused_parameters` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_find_unused_parameters` (`bool`, *optional*) â€” åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™ `DistributedDataParallel`
    çš„æ ‡å¿— `find_unused_parameters` çš„å€¼ã€‚å¦‚æœä½¿ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™é»˜è®¤ä¸º `False`ï¼Œå¦åˆ™ä¸º `True`ã€‚'
- en: '`ddp_bucket_cap_mb` (`int`, *optional*) â€” When using distributed training,
    the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_bucket_cap_mb` (`int`, *optional*) â€” åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™ `DistributedDataParallel`
    çš„æ ‡å¿— `bucket_cap_mb` çš„å€¼ã€‚'
- en: '`ddp_broadcast_buffers` (`bool`, *optional*) â€” When using distributed training,
    the value of the flag `broadcast_buffers` passed to `DistributedDataParallel`.
    Will default to `False` if gradient checkpointing is used, `True` otherwise.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_broadcast_buffers` (`bool`, *optional*) â€” åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™ `DistributedDataParallel`
    çš„æ ‡å¿— `broadcast_buffers` çš„å€¼ã€‚å¦‚æœä½¿ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™é»˜è®¤ä¸º `False`ï¼Œå¦åˆ™ä¸º `True`ã€‚'
- en: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) â€” Whether
    you want to pin memory in data loaders or not. Will default to `True`.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¦åœ¨æ•°æ®åŠ è½½å™¨ä¸­å›ºå®šå†…å­˜ã€‚é»˜è®¤ä¸º
    `True`ã€‚'
- en: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) â€”
    If True, the data loader will not shut down the worker processes after a dataset
    has been consumed once. This allows to maintain the workers Dataset instances
    alive. Can potentially speed up training, but will increase RAM usage. Will default
    to `False`.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) â€”
    å¦‚æœä¸º Trueï¼Œåˆ™æ•°æ®åŠ è½½å™¨åœ¨æ•°æ®é›†è¢«æ¶ˆè€—ä¸€æ¬¡åä¸ä¼šå…³é—­å·¥ä½œè¿›ç¨‹ã€‚è¿™å…è®¸ä¿æŒå·¥ä½œäººå‘˜æ•°æ®é›†å®ä¾‹çš„æ´»åŠ¨çŠ¶æ€ã€‚å¯èƒ½ä¼šåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†ä¼šå¢åŠ  RAM ä½¿ç”¨é‡ã€‚é»˜è®¤ä¸º
    `False`ã€‚'
- en: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) â€” Whether to
    skip adding of memory profiler reports to metrics. This is skipped by default
    because it slows down the training and evaluation speed.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_memory_metrics` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è·³è¿‡å°†å†…å­˜åˆ†ææŠ¥å‘Šæ·»åŠ åˆ°æŒ‡æ ‡ä¸­ã€‚é»˜è®¤è·³è¿‡æ­¤æ­¥éª¤ï¼Œå› ä¸ºå®ƒä¼šå‡æ…¢è®­ç»ƒå’Œè¯„ä¼°é€Ÿåº¦ã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push the model to the Hub every time the model is saved. If this is activated,
    `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`)
    and the content will be pushed each time a save is triggered (depending on your
    `save_strategy`). Calling [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    will also trigger a push.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶æ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hubã€‚å¦‚æœæ¿€æ´»äº†æ­¤é€‰é¡¹ï¼Œ`output_dir`
    å°†å¼€å§‹ä¸€ä¸ªä¸å­˜å‚¨åº“åŒæ­¥çš„ git ç›®å½•ï¼ˆç”± `hub_model_id` ç¡®å®šï¼‰ï¼Œå¹¶ä¸”æ¯æ¬¡è§¦å‘ä¿å­˜æ—¶éƒ½ä¼šæ¨é€å†…å®¹ï¼ˆå–å†³äºæ‚¨çš„ `save_strategy`ï¼‰ã€‚è°ƒç”¨
    [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    ä¹Ÿä¼šè§¦å‘æ¨é€ã€‚'
- en: If `output_dir` exists, it needs to be a local clone of the repository to which
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will be pushed.
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ `output_dir` å­˜åœ¨ï¼Œåˆ™éœ€è¦æ˜¯å°† [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    æ¨é€åˆ°çš„å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†ã€‚
- en: '`resume_from_checkpoint` (`str`, *optional*) â€” The path to a folder with a
    valid checkpoint for your model. This argument is not directly used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    itâ€™s intended to be used by your training/evaluation scripts instead. See the
    [example scripts](https://github.com/huggingface/transformers/tree/main/examples)
    for more details.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_from_checkpoint` (`str`, *optional*) â€” æ‚¨çš„æ¨¡å‹çš„æœ‰æ•ˆæ£€æŸ¥ç‚¹æ‰€åœ¨æ–‡ä»¶å¤¹çš„è·¯å¾„ã€‚æ­¤å‚æ•°ä¸ä¼šç›´æ¥è¢«
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä½¿ç”¨ï¼Œè€Œæ˜¯æ‰“ç®—ç”±æ‚¨çš„è®­ç»ƒ/è¯„ä¼°è„šæœ¬ä½¿ç”¨ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)ã€‚'
- en: '`hub_model_id` (`str`, *optional*) â€” The name of the repository to keep in
    sync with the local *output_dir*. It can be a simple model ID in which case the
    model will be pushed in your namespace. Otherwise it should be the whole repository
    name, for instance `"user_name/model"`, which allows you to push to an organization
    you are a member of with `"organization_name/model"`. Will default to `user_name/output_dir_name`
    with *output_dir_name* being the name of `output_dir`.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_model_id` (`str`, *optional*) â€” ä¸æœ¬åœ° *output_dir* åŒæ­¥çš„å­˜å‚¨åº“çš„åç§°ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹
    IDï¼Œæ­¤æ—¶æ¨¡å‹å°†è¢«æ¨é€åˆ°æ‚¨çš„å‘½åç©ºé—´ã€‚å¦åˆ™ï¼Œå®ƒåº”è¯¥æ˜¯æ•´ä¸ªå­˜å‚¨åº“åç§°ï¼Œä¾‹å¦‚ `"user_name/model"`ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥æ¨é€åˆ°æ‚¨æ‰€å±çš„ç»„ç»‡ï¼Œä¾‹å¦‚ `"organization_name/model"`ã€‚å°†é»˜è®¤ä¸º
    `user_name/output_dir_name`ï¼Œå…¶ä¸­ *output_dir_name* æ˜¯ `output_dir` çš„åç§°ã€‚'
- en: Will default to the name of `output_dir`.
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°†é»˜è®¤ä¸º `output_dir` çš„åç§°ã€‚
- en: '`hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`)
    â€” Defines the scope of what is pushed to the Hub and when. Possible values are:'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_strategy` (`str` æˆ– `HubStrategy`, *optional*, é»˜è®¤ä¸º `"every_save"`) â€” å®šä¹‰æ¨é€åˆ°
    Hub çš„èŒƒå›´å’Œæ—¶é—´ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'
- en: '`"end"`: push the model, its configuration, the tokenizer (if passed along
    to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card when the [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    method is called.'
  id: totrans-834
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"end"`: å½“è°ƒç”¨ [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)
    æ–¹æ³•æ—¶ï¼Œæ¨é€æ¨¡å‹ã€å…¶é…ç½®ã€åˆ†è¯å™¨ï¼ˆå¦‚æœä¼ é€’ç»™ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ä»¥åŠæ¨¡å‹å¡ç‰‡çš„è‰ç¨¿ã€‚'
- en: '`"every_save"`: push the model, its configuration, the tokenizer (if passed
    along to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer))
    and a draft of a model card each time there is a model save. The pushes are asynchronous
    to not block training, and in case the save are very frequent, a new push is only
    attempted if the previous one is finished. A last push is made with the final
    model at the end of training.'
  id: totrans-835
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"every_save"`: æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼Œæ¨é€æ¨¡å‹ã€å…¶é…ç½®ã€åˆ†è¯å™¨ï¼ˆå¦‚æœä¼ é€’ç»™ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼‰ä»¥åŠæ¨¡å‹å¡ç‰‡çš„è‰ç¨¿ã€‚æ¨é€æ˜¯å¼‚æ­¥çš„ï¼Œä»¥é¿å…é˜»å¡è®­ç»ƒï¼Œå¦‚æœä¿å­˜éå¸¸é¢‘ç¹ï¼Œåˆ™åªæœ‰åœ¨ä¸Šä¸€ä¸ªæ¨é€å®Œæˆåæ‰ä¼šå°è¯•æ–°çš„æ¨é€ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œæœ€åä¸€æ¬¡æ¨é€ã€‚'
- en: '`"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed
    in a subfolder named last-checkpoint, allowing you to resume training easily with
    `trainer.train(resume_from_checkpoint="last-checkpoint")`.'
  id: totrans-836
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"checkpoint"`: ç±»ä¼¼äº `"every_save"`ï¼Œä½†æœ€æ–°çš„æ£€æŸ¥ç‚¹ä¹Ÿä¼šè¢«æ¨é€åˆ°åä¸º last-checkpoint çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œè¿™æ ·æ‚¨å¯ä»¥è½»æ¾åœ°ä½¿ç”¨
    `trainer.train(resume_from_checkpoint="last-checkpoint")` æ¢å¤è®­ç»ƒã€‚'
- en: '`"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like
    they appear in the output folder (so you will get one checkpoint folder per folder
    in your final repository)'
  id: totrans-837
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all_checkpoints"`: ç±»ä¼¼äº `"checkpoint"`ï¼Œä½†æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½åƒå®ƒä»¬å‡ºç°åœ¨è¾“å‡ºæ–‡ä»¶å¤¹ä¸­ä¸€æ ·è¢«æ¨é€ï¼ˆå› æ­¤æ‚¨å°†åœ¨æœ€ç»ˆå­˜å‚¨åº“ä¸­è·å¾—ä¸€ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ï¼‰ã€‚'
- en: '`hub_token` (`str`, *optional*) â€” The token to use to push the model to the
    Hub. Will default to the token in the cache folder obtained with `huggingface-cli
    login`.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_token` (`str`, *optional*) â€” ç”¨äºå°†æ¨¡å‹æ¨é€åˆ° Hub çš„ä»¤ç‰Œã€‚å°†é»˜è®¤ä¸ºä½¿ç”¨ `huggingface-cli
    login` è·å–çš„ç¼“å­˜æ–‡ä»¶å¤¹ä¸­çš„ä»¤ç‰Œã€‚'
- en: '`hub_private_repo` (`bool`, *optional*, defaults to `False`) â€” If True, the
    Hub repo will be set to private.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_private_repo` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™ Hub å­˜å‚¨åº“å°†è®¾ç½®ä¸ºç§æœ‰ã€‚'
- en: '`hub_always_push` (`bool`, *optional*, defaults to `False`) â€” Unless this is
    `True`, the `Trainer` will skip pushing a checkpoint when the previous push is
    not finished.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_always_push` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” é™¤éä¸º `True`ï¼Œå¦åˆ™ `Trainer`
    åœ¨ä¸Šä¸€ä¸ªæ¨é€æœªå®Œæˆæ—¶å°†è·³è¿‡æ¨é€æ£€æŸ¥ç‚¹ã€‚'
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `False`) â€” If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜ï¼Œä½†ä¼šå¯¼è‡´åå‘ä¼ æ’­é€Ÿåº¦å˜æ…¢ã€‚'
- en: '`gradient_checkpointing_kwargs` (`dict`, *optional*, defaults to `None`) â€”
    Key word arguments to be passed to the `gradient_checkpointing_enable` method.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing_kwargs` (`dict`, *optional*, é»˜è®¤ä¸º `None`) â€” è¦ä¼ é€’ç»™ `gradient_checkpointing_enable`
    æ–¹æ³•çš„å…³é”®å­—å‚æ•°ã€‚'
- en: '`include_inputs_for_metrics` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the inputs will be passed to the `compute_metrics` function. This is intended
    for metrics that need inputs, predictions and references for scoring calculation
    in Metric class.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_inputs_for_metrics` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°†è¾“å…¥ä¼ é€’ç»™ `compute_metrics`
    å‡½æ•°ã€‚è¿™é€‚ç”¨äºéœ€è¦è¾“å…¥ã€é¢„æµ‹å’Œå‚è€ƒå€¼è¿›è¡Œè¯„åˆ†è®¡ç®—çš„æŒ‡æ ‡ç±»ã€‚'
- en: '`auto_find_batch_size` (`bool`, *optional*, defaults to `False`) â€” Whether
    to find a batch size that will fit into memory automatically through exponential
    decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed
    (`pip install accelerate`)'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_find_batch_size` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦é€šè¿‡æŒ‡æ•°è¡°å‡è‡ªåŠ¨æ‰¾åˆ°é€‚åˆå†…å­˜çš„æ‰¹é‡å¤§å°ï¼Œé¿å…
    CUDA å†…å­˜ä¸è¶³é”™è¯¯ã€‚éœ€è¦å®‰è£… accelerate (`pip install accelerate`)ã€‚'
- en: '`full_determinism` (`bool`, *optional*, defaults to `False`) â€” If `True`, [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    is called instead of [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    to ensure reproducible results in distributed training. Important: this will negatively
    impact the performance, so only use it for debugging.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`full_determinism` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œå°†è°ƒç”¨ [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism)
    è€Œä¸æ˜¯ [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed)
    æ¥ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­è·å¾—å¯é‡ç°çš„ç»“æœã€‚é‡è¦æç¤ºï¼šè¿™ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå› æ­¤åªèƒ½ç”¨äºè°ƒè¯•ç›®çš„ã€‚'
- en: '`torchdynamo` (`str`, *optional*) â€” If set, the backend compiler for TorchDynamo.
    Possible choices are `"eager"`, `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`,
    `"aot_cudagraphs"`, `"ofi"`, `"fx2trt"`, `"onnxrt"` and `"ipex"`.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchdynamo` (`str`, *optional*) â€” å¦‚æœè®¾ç½®ï¼ŒTorchDynamo çš„åç«¯ç¼–è¯‘å™¨ã€‚å¯èƒ½çš„é€‰æ‹©æ˜¯ `"eager"`,
    `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`, `"aot_cudagraphs"`,
    `"ofi"`, `"fx2trt"`, `"onnxrt"` å’Œ `"ipex"`ã€‚'
- en: '`ray_scope` (`str`, *optional*, defaults to `"last"`) â€” The scope to use when
    doing hyperparameter search with Ray. By default, `"last"` will be used. Ray will
    then use the last checkpoint of all trials, compare those, and select the best
    one. However, other options are also available. See the [Ray documentation](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)
    for more options.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray_scope`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"last"`ï¼‰â€” åœ¨ä½¿ç”¨Rayè¿›è¡Œè¶…å‚æ•°æœç´¢æ—¶è¦ä½¿ç”¨çš„èŒƒå›´ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå°†ä½¿ç”¨`"last"`ã€‚ç„¶åï¼ŒRayå°†ä½¿ç”¨æ‰€æœ‰è¯•éªŒçš„æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œè¿›è¡Œæ¯”è¾ƒå¹¶é€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªã€‚ä½†ä¹Ÿæœ‰å…¶ä»–é€‰é¡¹å¯ç”¨ã€‚æŸ¥çœ‹[Rayæ–‡æ¡£](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)ä»¥è·å–æ›´å¤šé€‰é¡¹ã€‚'
- en: '`ddp_timeout` (`int`, *optional*, defaults to 1800) â€” The timeout for `torch.distributed.init_process_group`
    calls, used to avoid GPU socket timeouts when performing slow operations in distributed
    runnings. Please refer the [PyTorch documentation] ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))
    for more information.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddp_timeout`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1800ï¼‰â€” `torch.distributed.init_process_group`è°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼Œç”¨äºé¿å…åœ¨åˆ†å¸ƒå¼è¿è¡Œä¸­æ‰§è¡Œç¼“æ…¢æ“ä½œæ—¶å‘ç”ŸGPUå¥—æ¥å­—è¶…æ—¶ã€‚è¯·å‚è€ƒ[PyTorchæ–‡æ¡£]([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '`use_mps_device` (`bool`, *optional*, defaults to `False`) â€” This argument
    is deprecated.`mps` device will be used if it is available similar to `cuda` device.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mps_device`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ­¤å‚æ•°å·²å¼ƒç”¨ã€‚å¦‚æœå¯ç”¨ï¼Œå°†ä½¿ç”¨`mps`è®¾å¤‡ï¼Œç±»ä¼¼äº`cuda`è®¾å¤‡ã€‚'
- en: '`torch_compile` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to compile the model using PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/)ç¼–è¯‘æ¨¡å‹ã€‚'
- en: This will use the best defaults for the [`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).
    You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode`
    but we donâ€™t guarantee any of them will work as the support is progressively rolled
    in in PyTorch.
  id: totrans-851
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å°†ä½¿ç”¨[`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile)çš„æœ€ä½³é»˜è®¤å€¼ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å‚æ•°`torch_compile_backend`å’Œ`torch_compile_mode`è‡ªå®šä¹‰é»˜è®¤å€¼ï¼Œä½†æˆ‘ä»¬ä¸èƒ½ä¿è¯å®ƒä»¬ä¸­çš„ä»»ä½•ä¸€ä¸ªä¼šèµ·ä½œç”¨ï¼Œå› ä¸ºæ”¯æŒé€æ­¥åœ¨PyTorchä¸­æ¨å‡ºã€‚
- en: This flag and the whole compile API is experimental and subject to change in
    future releases.
  id: totrans-852
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ­¤æ ‡å¿—å’Œæ•´ä¸ªç¼–è¯‘APIæ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½ä¼šåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: '`torch_compile_backend` (`str`, *optional*) â€” The backend to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_backend`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨`torch.compile`ä¸­è¦ä½¿ç”¨çš„åç«¯ã€‚å¦‚æœè®¾ç½®ä¸ºä»»ä½•å€¼ï¼Œ`torch_compile`å°†è¢«è®¾ç½®ä¸º`True`ã€‚'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-854
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–å¯èƒ½çš„å€¼ï¼Œå¹¶æ³¨æ„å®ƒä»¬å¯èƒ½ä¼šéšç€PyTorchç‰ˆæœ¬çš„å˜åŒ–è€Œæ”¹å˜ã€‚
- en: This flag is experimental and subject to change in future releases.
  id: totrans-855
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ­¤æ ‡å¿—æ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½ä¼šåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: '`torch_compile_mode` (`str`, *optional*) â€” The mode to use in `torch.compile`.
    If set to any value, `torch_compile` will be set to `True`.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_compile_mode`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨`torch.compile`ä¸­è¦ä½¿ç”¨çš„æ¨¡å¼ã€‚å¦‚æœè®¾ç½®ä¸ºä»»ä½•å€¼ï¼Œ`torch_compile`å°†è¢«è®¾ç½®ä¸º`True`ã€‚'
- en: Refer to the PyTorch doc for possible values and note that they may change across
    PyTorch versions.
  id: totrans-857
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–å¯èƒ½çš„å€¼ï¼Œå¹¶æ³¨æ„å®ƒä»¬å¯èƒ½ä¼šéšç€PyTorchç‰ˆæœ¬çš„å˜åŒ–è€Œæ”¹å˜ã€‚
- en: This flag is experimental and subject to change in future releases.
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ­¤æ ‡å¿—æ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½ä¼šåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: '`split_batches` (`bool`, *optional*) â€” Whether or not the accelerator should
    split the batches yielded by the dataloaders across the devices during distributed
    training. If'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_batches`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´ï¼ŒåŠ é€Ÿå™¨æ˜¯å¦åº”è¯¥åœ¨è®¾å¤‡ä¹‹é—´åˆ†å‰²æ•°æ®åŠ è½½å™¨äº§ç”Ÿçš„æ‰¹æ¬¡ã€‚å¦‚æœ'
- en: set to `True`, the actual batch size used will be the same on any kind of distributed
    processes, but it must be a
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸º`True`ï¼Œå®é™…ä½¿ç”¨çš„æ‰¹é‡å¤§å°å°†åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è¿›ç¨‹ä¸Šç›¸åŒï¼Œä½†å¿…é¡»æ˜¯
- en: round multiple of the number of processes you are using (such as GPUs).
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°†å¤šä¸ªè¿›ç¨‹çš„æ•°é‡ï¼ˆä¾‹å¦‚GPUï¼‰çš„å€æ•°å››èˆäº”å…¥ã€‚
- en: '`include_tokens_per_second` (`bool`, *optional*) â€” Whether or not to compute
    the number of tokens per second per device for training speed metrics.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_tokens_per_second`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è®¡ç®—æ¯ä¸ªè®¾å¤‡æ¯ç§’çš„æ ‡è®°æ•°ï¼Œç”¨äºè®­ç»ƒé€Ÿåº¦æŒ‡æ ‡ã€‚'
- en: This will iterate over the entire training dataloader once beforehand,
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å°†åœ¨è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¹‹å‰è¿­ä»£æ•´ä¸ªè®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸€æ¬¡ï¼Œ
- en: and will slow down the entire process.
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¹¶ä¸”ä¼šå‡æ…¢æ•´ä¸ªè¿‡ç¨‹ã€‚
- en: '`include_num_input_tokens_seen` (`bool`, *optional*) â€” Whether or not to track
    the number of input tokens seen throughout training.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_num_input_tokens_seen`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¦è·Ÿè¸ªæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„è¾“å…¥æ ‡è®°æ•°é‡ã€‚'
- en: May be slower in distributed training as gather operations must be called.
  id: totrans-866
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯èƒ½ä¼šè¾ƒæ…¢ï¼Œå› ä¸ºå¿…é¡»è°ƒç”¨gatheræ“ä½œã€‚
- en: '`neftune_noise_alpha` (`Optional[float]`) â€” If not `None`, this will activate
    NEFTune noise embeddings. This can drastically improve model performance for instruction
    fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914)
    and the [original code](https://github.com/neelsjain/NEFTune). Support transformers
    `PreTrainedModel` and also `PeftModel` from peft.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neftune_noise_alpha`ï¼ˆ`Optional[float]`ï¼‰â€” å¦‚æœä¸æ˜¯`None`ï¼Œå°†æ¿€æ´»NEFTuneå™ªå£°åµŒå…¥ã€‚è¿™å¯ä»¥æå¤§åœ°æé«˜æŒ‡å¯¼å¾®è°ƒçš„æ¨¡å‹æ€§èƒ½ã€‚æŸ¥çœ‹[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2310.05914)å’Œ[åŸå§‹ä»£ç ](https://github.com/neelsjain/NEFTune)ã€‚æ”¯æŒtransformersçš„`PreTrainedModel`å’Œpeftçš„`PeftModel`ã€‚'
- en: '`sortish_sampler` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    a *sortish sampler* or not. Only possible if the underlying datasets are *Seq2SeqDataset*
    for now but will become generally available in the near future.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sortish_sampler`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨*sortish sampler*ã€‚ç›®å‰ä»…åœ¨åº•å±‚æ•°æ®é›†ä¸º*Seq2SeqDataset*æ—¶æ‰å¯èƒ½ï¼Œä½†å°†åœ¨ä¸ä¹…çš„å°†æ¥æ™®éå¯ç”¨ã€‚'
- en: It sorts the inputs according to lengths in order to minimize the padding size,
    with a bit of randomness for the training set.
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ ¹æ®é•¿åº¦å¯¹è¾“å…¥è¿›è¡Œæ’åºï¼Œä»¥æœ€å°åŒ–å¡«å……å¤§å°ï¼Œå¹¶åœ¨è®­ç»ƒé›†ä¸­åŠ å…¥ä¸€äº›éšæœºæ€§ã€‚
- en: '`predict_with_generate` (`bool`, *optional*, defaults to `False`) â€” Whether
    to use generate to calculate generative metrics (ROUGE, BLEU).'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_with_generate`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨ç”Ÿæˆæ¥è®¡ç®—ç”ŸæˆæŒ‡æ ‡ï¼ˆROUGEï¼ŒBLEUï¼‰ã€‚'
- en: '`generation_max_length` (`int`, *optional*) â€” The `max_length` to use on each
    evaluation loop when `predict_with_generate=True`. Will default to the `max_length`
    value of the model configuration.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_max_length` (`int`, *optional*) â€” åœ¨`predict_with_generate=True`æ—¶ï¼Œåœ¨æ¯ä¸ªè¯„ä¼°å¾ªç¯ä¸­ä½¿ç”¨çš„`max_length`ã€‚å°†é»˜è®¤ä¸ºæ¨¡å‹é…ç½®çš„`max_length`å€¼ã€‚'
- en: '`generation_num_beams` (`int`, *optional*) â€” The `num_beams` to use on each
    evaluation loop when `predict_with_generate=True`. Will default to the `num_beams`
    value of the model configuration.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_num_beams` (`int`, *optional*) â€” åœ¨`predict_with_generate=True`æ—¶ï¼Œåœ¨æ¯ä¸ªè¯„ä¼°å¾ªç¯ä¸­ä½¿ç”¨çš„`num_beams`ã€‚å°†é»˜è®¤ä¸ºæ¨¡å‹é…ç½®çš„`num_beams`å€¼ã€‚'
- en: '`generation_config` (`str` or `Path` or [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig),
    *optional*) â€” Allows to load a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    from the `from_pretrained` method. This can be either:'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`str`æˆ–`Path`æˆ–[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig),
    *optional*) â€” å…è®¸ä»`from_pretrained`æ–¹æ³•åŠ è½½ä¸€ä¸ª[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)ã€‚è¿™å¯ä»¥æ˜¯ï¼š'
- en: a string, the *model id* of a pretrained model configuration hosted inside a
    model repo on huggingface.co. Valid model ids can be located at the root-level,
    like `bert-base-uncased`, or namespaced under a user or organization name, like
    `dbmdz/bert-base-german-cased`.
  id: totrans-874
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„*æ¨¡å‹id*ï¼Œæ‰˜ç®¡åœ¨huggingface.coä¸Šçš„æ¨¡å‹å­˜å‚¨åº“å†…ã€‚æœ‰æ•ˆçš„æ¨¡å‹idå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´åŒ–ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚
- en: a path to a *directory* containing a configuration file saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-875
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª*ç›®å½•*çš„è·¯å¾„ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)æ–¹æ³•ä¿å­˜çš„é…ç½®æ–‡ä»¶ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚
- en: a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    object.
  id: totrans-876
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)å¯¹è±¡ã€‚
- en: TrainingArguments is the subset of the arguments we use in our example scripts
    **which relate to the training loop itself**.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: TrainingArgumentsæ˜¯æˆ‘ä»¬åœ¨ç¤ºä¾‹è„šæœ¬ä¸­ä½¿ç”¨çš„ä¸è®­ç»ƒå¾ªç¯æœ¬èº«ç›¸å…³çš„å‚æ•°çš„å­é›†ã€‚
- en: Using [HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)
    we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse)
    arguments that can be specified on the command line.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªç±»è½¬æ¢ä¸ºå¯ä»¥åœ¨å‘½ä»¤è¡Œä¸ŠæŒ‡å®šçš„[argparse](https://docs.python.org/3/library/argparse#module-argparse)å‚æ•°ã€‚
- en: '#### `to_dict`'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L87)'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L87)'
- en: '[PRE68]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Serializes this instance while replace `Enum` by their values and `GenerationConfig`
    by dictionaries (for JSON serialization support). It obfuscates the token values
    by removing their value.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å®ä¾‹åºåˆ—åŒ–ï¼Œå°†`Enum`æ›¿æ¢ä¸ºå®ƒä»¬çš„å€¼ï¼Œå°†`GenerationConfig`æ›¿æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºJSONåºåˆ—åŒ–æ”¯æŒï¼‰ã€‚é€šè¿‡åˆ é™¤å…¶å€¼æ¥æ··æ·†æ ‡è®°å€¼ã€‚
