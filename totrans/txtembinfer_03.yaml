- en: Quick Tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速浏览
- en: 'Original text: [https://huggingface.co/docs/text-embeddings-inference/quick_tour](https://huggingface.co/docs/text-embeddings-inference/quick_tour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-embeddings-inference/quick_tour](https://huggingface.co/docs/text-embeddings-inference/quick_tour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text Embeddings
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本嵌入
- en: The easiest way to get started with TEI is to use one of the official Docker
    containers (see [Supported models and hardware](supported_models) to choose the
    right container).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用TEI的最简单方法是使用官方的Docker容器之一（请参阅[支持的模型和硬件](supported_models)以选择正确的容器）。
- en: After making sure that your hardware is supported, install the [NVIDIA Container
    Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    if you plan on utilizing GPUs. NVIDIA drivers on your device need to be compatible
    with CUDA version 12.2 or higher.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的硬件受支持后，如果您计划使用GPU，则安装[NVIDIA容器工具包](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)。您设备上的NVIDIA驱动程序需要与CUDA版本12.2或更高版本兼容。
- en: Next, install Docker following their [installation instructions](https://docs.docker.com/get-docker/).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按照他们的[安装说明](https://docs.docker.com/get-docker/)安装Docker。
- en: 'Finally, deploy your model. Let’s say you want to use `BAAI/bge-large-en-v1.5`.
    Here’s how you can do this:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部署您的模型。假设您想使用`BAAI/bge-large-en-v1.5`。以下是您可以这样做的方法：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we pass a `revision=refs/pr/5`, because the `safetensors` variant of this
    model is currently in a pull request. We also recommend sharing a volume with
    the Docker container (`volume=$PWD/data`) to avoid downloading weights every run.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们传递了`revision=refs/pr/5`，因为此模型的`safetensors`变体目前在拉取请求中。我们还建议与Docker容器共享卷（`volume=$PWD/data`）以避免在每次运行时下载权重。
- en: 'Once you have deployed a model you can use the `embed` endpoint by sending
    requests:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您部署了模型，您可以通过发送请求来使用`embed`端点：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Re-rankers
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新排列器
- en: Re-rankers models are Sequence Classification cross-encoders models with a single
    class that scores the similarity between a query and a text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列器模型是具有单个类别的序列分类交叉编码器模型，用于评分查询和文本之间的相似性。
- en: See [this blogpost](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)
    by the LlamaIndex team to understand how you can use re-rankers models in your
    RAG pipeline to improve downstream performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅LlamaIndex团队的[此博文](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)以了解如何在RAG管道中使用重新排列器模型来提高下游性能。
- en: 'Let’s say you want to use `BAAI/bge-reranker-large`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想使用`BAAI/bge-reranker-large`：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you have deployed a model you can use the `rerank` endpoint to rank the
    similarity between a query and a list of texts:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您部署了模型，您可以使用`rerank`端点来对查询和文本列表之间的相似性进行排名：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Sequence Classification
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列分类
- en: 'You can also use classic Sequence Classification models like `SamLowe/roberta-base-go_emotions`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用经典的序列分类模型，比如`SamLowe/roberta-base-go_emotions`：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once you have deployed the model you can use the `predict` endpoint to get
    the emotions most associated with an input:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您部署了模型，您可以使用`predict`端点来获取与输入最相关的情绪：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
