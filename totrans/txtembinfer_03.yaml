- en: Quick Tour
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-embeddings-inference/quick_tour](https://huggingface.co/docs/text-embeddings-inference/quick_tour)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Text Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to get started with TEI is to use one of the official Docker
    containers (see [Supported models and hardware](supported_models) to choose the
    right container).
  prefs: []
  type: TYPE_NORMAL
- en: After making sure that your hardware is supported, install the [NVIDIA Container
    Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    if you plan on utilizing GPUs. NVIDIA drivers on your device need to be compatible
    with CUDA version 12.2 or higher.
  prefs: []
  type: TYPE_NORMAL
- en: Next, install Docker following their [installation instructions](https://docs.docker.com/get-docker/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, deploy your model. Let’s say you want to use `BAAI/bge-large-en-v1.5`.
    Here’s how you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here we pass a `revision=refs/pr/5`, because the `safetensors` variant of this
    model is currently in a pull request. We also recommend sharing a volume with
    the Docker container (`volume=$PWD/data`) to avoid downloading weights every run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have deployed a model you can use the `embed` endpoint by sending
    requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Re-rankers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Re-rankers models are Sequence Classification cross-encoders models with a single
    class that scores the similarity between a query and a text.
  prefs: []
  type: TYPE_NORMAL
- en: See [this blogpost](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)
    by the LlamaIndex team to understand how you can use re-rankers models in your
    RAG pipeline to improve downstream performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you want to use `BAAI/bge-reranker-large`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have deployed a model you can use the `rerank` endpoint to rank the
    similarity between a query and a list of texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Sequence Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also use classic Sequence Classification models like `SamLowe/roberta-base-go_emotions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have deployed the model you can use the `predict` endpoint to get
    the emotions most associated with an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
