- en: Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/api/tokenizer](https://huggingface.co/docs/tokenizers/api/tokenizer)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    PythonRustNode
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.Tokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([Model](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Model))
    — The core algorithm that this `Tokenizer` should be using.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Tokenizer` works as a pipeline. It processes some raw text as input and outputs
    an [Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding).
  prefs: []
  type: TYPE_NORMAL
- en: property decoder
  prefs: []
  type: TYPE_NORMAL
- en: The *optional* `Decoder` in use by the Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: property model
  prefs: []
  type: TYPE_NORMAL
- en: The [Model](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Model)
    in use by the Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: property normalizer
  prefs: []
  type: TYPE_NORMAL
- en: The *optional* [Normalizer](/docs/tokenizers/v0.13.4.rc2/en/api/normalizers#tokenizers.normalizers.Normalizer)
    in use by the Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: property padding
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: (`dict`, *optional*)
  prefs: []
  type: TYPE_NORMAL
- en: A dict with the current padding parameters if padding is enabled
  prefs: []
  type: TYPE_NORMAL
- en: Get the current padding parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*Cannot be set, use* `enable_padding()` *instead*'
  prefs: []
  type: TYPE_NORMAL
- en: property post_processor
  prefs: []
  type: TYPE_NORMAL
- en: The *optional* `PostProcessor` in use by the Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: property pre_tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: The *optional* [PreTokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer)
    in use by the Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: property truncation
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: (`dict`, *optional*)
  prefs: []
  type: TYPE_NORMAL
- en: A dict with the current truncation parameters if truncation is enabled
  prefs: []
  type: TYPE_NORMAL
- en: Get the currently set truncation parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*Cannot set, use* `enable_truncation()` *instead*'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_special_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokens` (A `List` of [AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)
    or `str`) — The list of special tokens we want to add to the vocabulary. Each
    token can either be a string or an instance of [AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)
    for more customization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of tokens that were created in the vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Add the given special tokens to the Tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: If these tokens are already part of the vocabulary, it just let the Tokenizer
    know about them. If they don’t exist, the Tokenizer creates them, giving them
    a new id.
  prefs: []
  type: TYPE_NORMAL
- en: These special tokens will never be processed by the model (ie won’t be split
    into multiple tokens), and they can be removed from the output when decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokens` (A `List` of [AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)
    or `str`) — The list of tokens we want to add to the vocabulary. Each token can
    be either a string or an instance of [AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)
    for more customization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of tokens that were created in the vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Add the given tokens to the vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: The given tokens are added only if they don’t already exist in the vocabulary.
    Each token then gets a new attributed id.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`ids` (A `List/Tuple` of `int`) — The list of ids that we want to decode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, defaults to `True`) — Whether the special tokens
    should be removed from the decoded string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The decoded string
  prefs: []
  type: TYPE_NORMAL
- en: Decode the given list of ids back to a string
  prefs: []
  type: TYPE_NORMAL
- en: This is used to decode anything coming back from a Language Model
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`List` of `List[int]`) — The batch of sequences we want to decode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, defaults to `True`) — Whether the special tokens
    should be removed from the decoded strings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[str]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of decoded strings
  prefs: []
  type: TYPE_NORMAL
- en: Decode a batch of ids back to their corresponding string
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_padding`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`direction` (`str`, *optional*, defaults to `right`) — The direction in which
    to pad. Can be either `right` or `left`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If specified, the padding length
    should always snap to the next multiple of the given value. For example if we
    were going to pad witha length of 250 but `pad_to_multiple_of=8` then we will
    pad to 256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_id` (`int`, defaults to 0) — The id to be used when padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_type_id` (`int`, defaults to 0) — The type id to be used when padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, defaults to `[PAD]`) — The pad token to be used when padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` (`int`, *optional*) — If specified, the length at which to pad. If
    not specified we pad using the size of the longest sequence in a batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable the padding
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_truncation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`) — The max length at which to truncate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*) — The length of the previous first sequence to
    be included in the overflowing sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`str`, *optional*, defaults to `longest_first`) — The strategy
    used to truncation. Can be one of `longest_first`, `only_first` or `only_second`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`direction` (`str`, defaults to `right`) — Truncate direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable truncation
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence` (`~tokenizers.InputSequence`) — The main input sequence we want
    to encode. This sequence can be either raw text or pre-tokenized, according to
    the `is_pretokenized` argument:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `is_pretokenized=False`: `TextInputSequence`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `is_pretokenized=True`: `PreTokenizedInputSequence()`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pair` (`~tokenizers.InputSequence`, *optional*) — An optional input sequence.
    The expected format is the same that for `sequence`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_pretokenized` (`bool`, defaults to `False`) — Whether the input is already
    pre-tokenized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, defaults to `True`) — Whether to add the special
    tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)'
  prefs: []
  type: TYPE_NORMAL
- en: The encoded result
  prefs: []
  type: TYPE_NORMAL
- en: Encode the given sequence and pair. This method can process raw text sequences
    as well as already pre-tokenized sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of the inputs that are accepted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#### `encode_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input` (A `List`/``Tuple` of `~tokenizers.EncodeInput`) — A list of single
    sequences or pair sequences to encode. Each sequence can be either raw text or
    pre-tokenized, according to the `is_pretokenized` argument:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `is_pretokenized=False`: `TextEncodeInput()`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `is_pretokenized=True`: `PreTokenizedEncodeInput()`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_pretokenized` (`bool`, defaults to `False`) — Whether the input is already
    pre-tokenized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, defaults to `True`) — Whether to add the special
    tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A `List` of [`~tokenizers.Encoding“]
  prefs: []
  type: TYPE_NORMAL
- en: The encoded batch
  prefs: []
  type: TYPE_NORMAL
- en: Encode the given batch of inputs. This method accept both raw text sequences
    as well as already pre-tokenized sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of the inputs that are accepted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_buffer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`buffer` (`bytes`) — A buffer containing a previously serialized [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: The new tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a new [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    from the given buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_file`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — A path to a local JSON file representing a previously serialized
    [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: The new tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a new [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    from the file at the given path.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`identifier` (`str`) — The identifier of a Model on the Hugging Face Hub, that
    contains a tokenizer.json file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, defaults to *main*) — A branch or commit id'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auth_token` (`str`, *optional*, defaults to *None*) — An optional auth token
    used to access private repositories on the Hugging Face Hub'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: The new tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a new [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    from an existing file on the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_str`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`json` (`str`) — A valid JSON string representing a previously serialized [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: The new tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a new [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    from the given JSON string.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_vocab`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`with_added_tokens` (`bool`, defaults to `True`) — Whether to include the added
    tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, int]`'
  prefs: []
  type: TYPE_NORMAL
- en: The vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Get the underlying vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_vocab_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`with_added_tokens` (`bool`, defaults to `True`) — Whether to include the added
    tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Get the size of the underlying vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: '#### `id_to_token`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`id` (`int`) — The id to convert'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Optional[str]`'
  prefs: []
  type: TYPE_NORMAL
- en: An optional token, `None` if out of vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Convert the given id to its corresponding token if it exists
  prefs: []
  type: TYPE_NORMAL
- en: '#### `no_padding`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Disable padding
  prefs: []
  type: TYPE_NORMAL
- en: '#### `no_truncation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Disable truncation
  prefs: []
  type: TYPE_NORMAL
- en: '#### `num_special_tokens_to_add`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the number of special tokens that would be added for single/pair sentences.
    :param is_pair: Boolean indicating if the input would be a single sentence or
    a pair :return:'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoding` ([Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding))
    — The [Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)
    corresponding to the main sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pair` ([Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding),
    *optional*) — An optional [Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)
    corresponding to the pair sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`) — Whether to add the special tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)'
  prefs: []
  type: TYPE_NORMAL
- en: The final post-processed encoding
  prefs: []
  type: TYPE_NORMAL
- en: Apply all the post-processing steps to the given encodings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Truncate according to the set truncation params (provided with `enable_truncation()`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the `PostProcessor`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pad according to the set padding params (provided with `enable_padding()`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### `save`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — A path to a file in which to save the serialized tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pretty` (`bool`, defaults to `True`) — Whether the JSON file should be pretty
    formatted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    to the file at the given path.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_str`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretty` (`bool`, defaults to `False`) — Whether the JSON string should be
    pretty formatted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: A string representing the serialized Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: Gets a serialized string representing this [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `token_to_id`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token` (`str`) — The token to convert'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Optional[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: An optional id, `None` if out of vocabulary
  prefs: []
  type: TYPE_NORMAL
- en: Convert the given token to its corresponding id if it exists
  prefs: []
  type: TYPE_NORMAL
- en: '#### `train`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`files` (`List[str]`) — A list of path to the files that we should use for
    training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trainer` (`~tokenizers.trainers.Trainer`, *optional*) — An optional trainer
    that should be used to train our Model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the Tokenizer using the given files.
  prefs: []
  type: TYPE_NORMAL
- en: Reads the files line by line, while keeping all the whitespace, even new lines.
    If you want to train from data store in-memory, you can check `train_from_iterator()`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `train_from_iterator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`iterator` (`Iterator`) — Any iterator over strings or list of strings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trainer` (`~tokenizers.trainers.Trainer`, *optional*) — An optional trainer
    that should be used to train our Model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` (`int`, *optional*) — The total number of sequences in the iterator.
    This is used to provide meaningful progress tracking'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the Tokenizer using the provided iterator.
  prefs: []
  type: TYPE_NORMAL
- en: You can provide anything that is a Python Iterator
  prefs: []
  type: TYPE_NORMAL
- en: A list of sequences `List[str]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A generator that yields `str` or `List[str]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Numpy array of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
