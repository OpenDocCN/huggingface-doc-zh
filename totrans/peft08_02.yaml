- en: PEFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/14.0ceaf05e.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting
    large pretrained models to various downstream applications without fine-tuning
    all of a modelâ€™s parameters because it is prohibitively costly. PEFT methods only
    fine-tune a small number of (extra) model parameters - significantly decreasing
    computational and storage costs - while yielding performance comparable to a fully
    fine-tuned model. This makes it more accessible to train and store large language
    models (LLMs) on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries
    to provide a faster and easier way to load, train, and use large models for inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[Get started'
  prefs: []
  type: TYPE_NORMAL
- en: Start here if you're new to ðŸ¤— PEFT to get an overview of the library's main
    features, and how to train a model with a PEFT method.](quicktour) [How-to guides
  prefs: []
  type: TYPE_NORMAL
- en: Practical guides demonstrating how to apply various PEFT methods across different
    types of tasks like image classification, causal language modeling, automatic
    speech recognition, and more. Learn how to use ðŸ¤— PEFT with the DeepSpeed and Fully
    Sharded Data Parallel scripts.](./task_guides/image_classification_lora) [Conceptual
    guides
  prefs: []
  type: TYPE_NORMAL
- en: Get a better theoretical understanding of how LoRA and various soft prompting
    methods help reduce the number of trainable parameters to make training more efficient.](./conceptual_guides/lora)
    [Reference
  prefs: []
  type: TYPE_NORMAL
- en: Technical descriptions of how ðŸ¤— PEFT classes and methods work.](./package_reference/config)
  prefs: []
  type: TYPE_NORMAL
- en: '[https://stevhliu-peft-methods.hf.space](https://stevhliu-peft-methods.hf.space)'
  prefs: []
  type: TYPE_NORMAL
