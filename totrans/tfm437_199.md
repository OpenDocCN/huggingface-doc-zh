# MegatronGPT2

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2)

## 概述

MegatronGPT2模型是由Mohammad Shoeybi、Mostofa Patwary、Raul Puri、Patrick LeGresley、Jared Casper和Bryan Catanzaro在[使用模型并行训练多十亿参数语言模型的Megatron-LM](https://arxiv.org/abs/1909.08053)中提出的。

论文摘要如下：

*最近在语言建模方面的工作表明，训练大型Transformer模型可以推动自然语言处理应用的最新技术。然而，非常大的模型可能由于内存限制而难以训练。在这项工作中，我们提出了训练非常大的Transformer模型的技术，并实现了一种简单高效的层内模型并行方法，使得可以训练具有数十亿参数的Transformer模型。我们的方法不需要新的编译器或库更改，与管道模型并行性正交且互补，并且可以通过在原生PyTorch中插入几个通信操作来完全实现。我们通过使用512个GPU收敛基于Transformer的模型，达到了83亿参数。与维持39 TeraFLOPs的强单GPU基线相比，我们在整个应用程序中保持了15.1 PetaFLOPs，其扩展效率为76%，这是峰值FLOPs的30%。为了证明大型语言模型可以进一步推动最新技术（SOTA），我们训练了一个83亿参数的Transformer语言模型，类似于GPT-2，以及一个39亿参数的类似BERT的模型。我们展示了在BERT类似模型中对层归一化的放置要特别注意，这对于随着模型规模增长而实现性能提升至关重要。使用GPT-2模型，我们在WikiText103（10.8，与15.8的SOTA困惑度相比）和LAMBADA（66.5%，与63.2%的SOTA准确率相比）数据集上取得了SOTA结果。我们的BERT模型在RACE数据集上取得了SOTA结果（90.9%，与89.4%的SOTA准确率相比）。*

这个模型是由[jdemouth](https://huggingface.co/jdemouth)贡献的。原始代码可以在[这里](https://github.com/NVIDIA/Megatron-LM)找到。该存储库包含了Megatron语言模型的多GPU和多节点实现。特别是，它包含了使用“张量并行”和“管道并行”技术的混合模型并行方法。

## 使用提示

我们提供了预训练的[GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m)检查点，用于评估或微调下游任务。

要访问这些检查点，首先[注册](https://ngc.nvidia.com/signup)并设置NVIDIA GPU云（NGC）注册表CLI。有关下载模型的更多文档，请参阅[NGC文档](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1)。

或者，您可以直接下载检查点：

```py
wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O
megatron_gpt2_345m_v0_0.zip
```

一旦您从NVIDIA GPU云（NGC）获得了检查点，您需要将其转换为Hugging Face Transformers GPT2实现可以轻松加载的格式。

以下命令允许您进行转换。我们假设文件夹`models/megatron_gpt2`包含`megatron_gpt2_345m_v0_0.zip`，并且该命令是从该文件夹运行的：

```py
python3 $PATH_TO_TRANSFORMERS/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip
```

MegatronGPT2架构与OpenAI GPT-2相同。有关配置类和其参数的信息，请参考[GPT-2文档](gpt2)。
