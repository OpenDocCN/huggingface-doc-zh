- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/trl/example_overview](https://huggingface.co/docs/trl/example_overview)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/trl/v0.7.10/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/start.d9a24ea1.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/singletons.9eef12cc.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/paths.1355483e.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/app.5bef33b8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/index.ded8f90d.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/0.abccdcd8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/7.0c03d860.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/CodeBlock.8580f3e8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/Heading.f027f30d.js">
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The examples should work in any of the following settings (with the same script):'
  prefs: []
  type: TYPE_NORMAL
- en: single GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multi GPUS (using PyTorch distributed mode)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multi GPUS (using DeepSpeed ZeRO-Offload stages 1, 2, & 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fp16 (mixed-precision), fp32 (normal precision), or bf16 (bfloat16 precision)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To run it in each of these various modes, first initialize the accelerate configuration
    with `accelerate config`
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE to train with a 4-bit or 8-bit model**, please run'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Accelerate Config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For all the examples, you‚Äôll need to generate a ü§ó Accelerate config file with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, it is encouraged to launch jobs with `accelerate launch`!
  prefs: []
  type: TYPE_NORMAL
- en: Maintained Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| File | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/scripts/sft.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)
    | This script shows how to use the `SFTTrainer` to fine tune a model or adapters
    into a target dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/scripts/reward_modeling.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py)
    | This script shows how to use the `RewardTrainer` to train a reward model on
    your own dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/scripts/ppo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py)
    | This script shows how to use the `PPOTrainer` to fine-tune a sentiment analysis
    model using IMDB dataset |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/scripts/ppo_multi_adapter.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo_multi_adapter.py)
    | This script shows how to use the `PPOTrainer` to train a single base model with
    multiple adapters. Requires you to run the example script with the reward model
    training beforehand. |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/scripts/stable_diffusion_tuning_example.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/stable_diffusion_tuning_example.py)
    | This script shows to use DDPOTrainer to fine-tune a stable diffusion model using
    reinforcement learning. |'
  prefs: []
  type: TYPE_TB
- en: 'Here are also some easier-to-run colab notebooks that you can use to get started
    with TRL:'
  prefs: []
  type: TYPE_NORMAL
- en: '| File | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/notebooks/best_of_n.ipynb`](https://github.com/huggingface/trl/tree/main/examples/notebooks/best_of_n.ipynb)
    | This notebook demonstrates how to use the ‚ÄúBest of N‚Äù sampling strategy using
    TRL when fine-tuning your model with PPO. |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/huggingface/trl/tree/main/examples/notebooks/gpt2-sentiment.ipynb)
    | This notebook demonstrates how to reproduce the GPT2 imdb sentiment tuning example
    on a jupyter notebook. |'
  prefs: []
  type: TYPE_TB
- en: '| [`examples/notebooks/gpt2-control.ipynb`](https://github.com/huggingface/trl/tree/main/examples/notebooks/gpt2-control.ipynb)
    | This notebook demonstrates how to reproduce the GPT2 sentiment control example
    on a jupyter notebook. |'
  prefs: []
  type: TYPE_TB
- en: 'We also have some other examples that are less maintained but can be used as
    a reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[research_projects](https://github.com/huggingface/trl/tree/main/examples/research_projects)**:
    Check out this folder to find the scripts used for some research projects that
    used TRL (LM de-toxification, Stack-Llama, etc.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the scripts can be run on multiple GPUs by providing the path of an ü§ó
    Accelerate config file when calling `accelerate launch`. To launch one of them
    on one or multiple GPUs, run the following command (swapping `{NUM_GPUS}` with
    the number of GPUs in your machine and `--all_arguments_of_the_script` with your
    arguments.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can also adjust the parameters of the ü§ó Accelerate config file to suit your
    needs (e.g. training in mixed precision).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training with DeepSpeed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most of the scripts can be run on multiple GPUs together with DeepSpeed ZeRO-{1,2,3}
    for efficient sharding of the optimizer states, gradients, and model weights.
    To do so, run following command (swapping `{NUM_GPUS}` with the number of GPUs
    in your machine, `--all_arguments_of_the_script` with your arguments, and `--deepspeed_config`
    with the path to the DeepSpeed config file such as `examples/deepspeed_configs/deepspeed_zero1.yaml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
