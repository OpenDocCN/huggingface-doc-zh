["```py\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\npython -m pip install torchvision tesseract\n```", "```py\ndef normalize_bbox(bbox, width, height):\n    return [\n        int(1000 * (bbox[0] / width)),\n        int(1000 * (bbox[1] / height)),\n        int(1000 * (bbox[2] / width)),\n        int(1000 * (bbox[3] / height)),\n    ]\n```", "```py\nfrom PIL import Image\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n)\n\nwidth, height = image.size\n```", "```py\nfrom transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor\n\nimage_processor = LayoutLMv2ImageProcessor()  # apply_ocr is set to True by default\ntokenizer = LayoutLMv2TokenizerFast.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\nprocessor = LayoutLMv2Processor(image_processor, tokenizer)\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nencoding = processor(\n    image, return_tensors=\"pt\"\n)  # you can also add all tokenizer parameters here such as padding, truncation\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nencoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nword_labels = [1, 2]\nencoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nquestion = \"What's his name?\"\nencoding = processor(image, question, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nquestion = \"What's his name?\"\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nencoding = processor(image, question, words, boxes=boxes, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\n>>> from transformers import LayoutLMv2Config, LayoutLMv2Model\n\n>>> # Initializing a LayoutLMv2 microsoft/layoutlmv2-base-uncased style configuration\n>>> configuration = LayoutLMv2Config()\n\n>>> # Initializing a model (with random weights) from the microsoft/layoutlmv2-base-uncased style configuration\n>>> model = LayoutLMv2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2Model, set_seed\n>>> from PIL import Image\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n>>> model = LayoutLMv2Model.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\n>>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\")\n>>> image_path = dataset[\"test\"][0][\"file\"]\n>>> image = Image.open(image_path).convert(\"RGB\")\n\n>>> encoding = processor(image, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> last_hidden_states = outputs.last_hidden_state\n\n>>> last_hidden_states.shape\ntorch.Size([1, 342, 768])\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2ForSequenceClassification, set_seed\n>>> from PIL import Image\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n\n>>> dataset = load_dataset(\"rvl_cdip\", split=\"train\", streaming=True)\n>>> data = next(iter(dataset))\n>>> image = data[\"image\"].convert(\"RGB\")\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n>>> model = LayoutLMv2ForSequenceClassification.from_pretrained(\n...     \"microsoft/layoutlmv2-base-uncased\", num_labels=dataset.info.features[\"label\"].num_classes\n... )\n\n>>> encoding = processor(image, return_tensors=\"pt\")\n>>> sequence_label = torch.tensor([data[\"label\"]])\n\n>>> outputs = model(**encoding, labels=sequence_label)\n\n>>> loss, logits = outputs.loss, outputs.logits\n>>> predicted_idx = logits.argmax(dim=-1).item()\n>>> predicted_answer = dataset.info.features[\"label\"].names[4]\n>>> predicted_idx, predicted_answer\n(4, 'advertisement')\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2ForTokenClassification, set_seed\n>>> from PIL import Image\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n\n>>> datasets = load_dataset(\"nielsr/funsd\", split=\"test\")\n>>> labels = datasets.features[\"ner_tags\"].feature.names\n>>> id2label = {v: k for v, k in enumerate(labels)}\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n>>> model = LayoutLMv2ForTokenClassification.from_pretrained(\n...     \"microsoft/layoutlmv2-base-uncased\", num_labels=len(labels)\n... )\n\n>>> data = datasets[0]\n>>> image = Image.open(data[\"image_path\"]).convert(\"RGB\")\n>>> words = data[\"words\"]\n>>> boxes = data[\"bboxes\"]  # make sure to normalize your bounding boxes\n>>> word_labels = data[\"ner_tags\"]\n>>> encoding = processor(\n...     image,\n...     words,\n...     boxes=boxes,\n...     word_labels=word_labels,\n...     padding=\"max_length\",\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n\n>>> outputs = model(**encoding)\n>>> logits, loss = outputs.logits, outputs.loss\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n>>> predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes[:5]\n['B-ANSWER', 'B-HEADER', 'B-HEADER', 'B-HEADER', 'B-HEADER']\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2ForQuestionAnswering, set_seed\n>>> import torch\n>>> from PIL import Image\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n>>> model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\n>>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\")\n>>> image_path = dataset[\"test\"][0][\"file\"]\n>>> image = Image.open(image_path).convert(\"RGB\")\n>>> question = \"When is coffee break?\"\n>>> encoding = processor(image, question, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> predicted_start_idx = outputs.start_logits.argmax(-1).item()\n>>> predicted_end_idx = outputs.end_logits.argmax(-1).item()\n>>> predicted_start_idx, predicted_end_idx\n(154, 287)\n\n>>> predicted_answer_tokens = encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1]\n>>> predicted_answer = processor.tokenizer.decode(predicted_answer_tokens)\n>>> predicted_answer  # results are not very good without further fine-tuning\n'council mem - bers conducted by trrf treasurer philip g. kuehn to get answers which the public ...\n```", "```py\n>>> target_start_index = torch.tensor([7])\n>>> target_end_index = torch.tensor([14])\n>>> outputs = model(**encoding, start_positions=target_start_index, end_positions=target_end_index)\n>>> predicted_answer_span_start = outputs.start_logits.argmax(-1).item()\n>>> predicted_answer_span_end = outputs.end_logits.argmax(-1).item()\n>>> predicted_answer_span_start, predicted_answer_span_end\n(154, 287)\n```"]