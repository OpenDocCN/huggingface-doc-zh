["```py\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\npython -m pip install torchvision tesseract\n```", "```py\ndef normalize_bbox(bbox, width, height):\n    return [\n        int(1000 * (bbox[0] / width)),\n        int(1000 * (bbox[1] / height)),\n        int(1000 * (bbox[2] / width)),\n        int(1000 * (bbox[3] / height)),\n    ]\n```", "```py\nfrom PIL import Image\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n)\n\nwidth, height = image.size\n```", "```py\nfrom transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor\n\nimage_processor = LayoutLMv2ImageProcessor()  # apply_ocr is set to True by default\ntokenizer = LayoutLMv2TokenizerFast.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\nprocessor = LayoutLMv2Processor(image_processor, tokenizer)\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nencoding = processor(\n    image, return_tensors=\"pt\"\n)  # you can also add all tokenizer parameters here such as padding, truncation\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nencoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nword_labels = [1, 2]\nencoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nquestion = \"What's his name?\"\nencoding = processor(image, question, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n    \"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nquestion = \"What's his name?\"\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nencoding = processor(image, question, words, boxes=boxes, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 max_2d_position_embeddings = 1024 max_rel_pos = 128 rel_pos_bins = 32 fast_qkv = True max_rel_2d_pos = 256 rel_2d_pos_bins = 64 convert_sync_batchnorm = True image_feature_pool_shape = [7, 7, 256] coordinate_size = 128 shape_size = 128 has_relative_attention_bias = True has_spatial_attention_bias = True has_visual_segment_embedding = False detectron2_config_args = None **kwargs )\n```", "```py\n>>> from transformers import LayoutLMv2Config, LayoutLMv2Model\n\n>>> # Initializing a LayoutLMv2 microsoft/layoutlmv2-base-uncased style configuration\n>>> configuration = LayoutLMv2Config()\n\n>>> # Initializing a model (with random weights) from the microsoft/layoutlmv2-base-uncased style configuration\n>>> model = LayoutLMv2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> apply_ocr: bool = True ocr_lang: Optional = None tesseract_config: Optional = '' **kwargs )\n```", "```py\n( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None apply_ocr: bool = None ocr_lang: Optional = None tesseract_config: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True tokenize_chinese_chars = True strip_accents = None model_max_length: int = 512 additional_special_tokens: Optional = None **kwargs )\n```", "```py\n( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( images text: Union = None text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = False max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2Model, set_seed\n>>> from PIL import Image\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n>>> model = LayoutLMv2Model.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\n>>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\")\n>>> image_path = dataset[\"test\"][0][\"file\"]\n>>> image = Image.open(image_path).convert(\"RGB\")\n\n>>> encoding = processor(image, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> last_hidden_states = outputs.last_hidden_state\n\n>>> last_hidden_states.shape\ntorch.Size([1, 342, 768])\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2ForSequenceClassification, set_seed\n>>> from PIL import Image\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n\n>>> dataset = load_dataset(\"rvl_cdip\", split=\"train\", streaming=True)\n>>> data = next(iter(dataset))\n>>> image = data[\"image\"].convert(\"RGB\")\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n>>> model = LayoutLMv2ForSequenceClassification.from_pretrained(\n...     \"microsoft/layoutlmv2-base-uncased\", num_labels=dataset.info.features[\"label\"].num_classes\n... )\n\n>>> encoding = processor(image, return_tensors=\"pt\")\n>>> sequence_label = torch.tensor([data[\"label\"]])\n\n>>> outputs = model(**encoding, labels=sequence_label)\n\n>>> loss, logits = outputs.loss, outputs.logits\n>>> predicted_idx = logits.argmax(dim=-1).item()\n>>> predicted_answer = dataset.info.features[\"label\"].names[4]\n>>> predicted_idx, predicted_answer\n(4, 'advertisement')\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2ForTokenClassification, set_seed\n>>> from PIL import Image\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n\n>>> datasets = load_dataset(\"nielsr/funsd\", split=\"test\")\n>>> labels = datasets.features[\"ner_tags\"].feature.names\n>>> id2label = {v: k for v, k in enumerate(labels)}\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n>>> model = LayoutLMv2ForTokenClassification.from_pretrained(\n...     \"microsoft/layoutlmv2-base-uncased\", num_labels=len(labels)\n... )\n\n>>> data = datasets[0]\n>>> image = Image.open(data[\"image_path\"]).convert(\"RGB\")\n>>> words = data[\"words\"]\n>>> boxes = data[\"bboxes\"]  # make sure to normalize your bounding boxes\n>>> word_labels = data[\"ner_tags\"]\n>>> encoding = processor(\n...     image,\n...     words,\n...     boxes=boxes,\n...     word_labels=word_labels,\n...     padding=\"max_length\",\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n\n>>> outputs = model(**encoding)\n>>> logits, loss = outputs.logits, outputs.loss\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n>>> predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes[:5]\n['B-ANSWER', 'B-HEADER', 'B-HEADER', 'B-HEADER', 'B-HEADER']\n```", "```py\n( config has_visual_segment_embedding = True )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, LayoutLMv2ForQuestionAnswering, set_seed\n>>> import torch\n>>> from PIL import Image\n>>> from datasets import load_dataset\n\n>>> set_seed(88)\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n>>> model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\n>>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\")\n>>> image_path = dataset[\"test\"][0][\"file\"]\n>>> image = Image.open(image_path).convert(\"RGB\")\n>>> question = \"When is coffee break?\"\n>>> encoding = processor(image, question, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)\n>>> predicted_start_idx = outputs.start_logits.argmax(-1).item()\n>>> predicted_end_idx = outputs.end_logits.argmax(-1).item()\n>>> predicted_start_idx, predicted_end_idx\n(154, 287)\n\n>>> predicted_answer_tokens = encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1]\n>>> predicted_answer = processor.tokenizer.decode(predicted_answer_tokens)\n>>> predicted_answer  # results are not very good without further fine-tuning\n'council mem - bers conducted by trrf treasurer philip g. kuehn to get answers which the public ...\n```", "```py\n>>> target_start_index = torch.tensor([7])\n>>> target_end_index = torch.tensor([14])\n>>> outputs = model(**encoding, start_positions=target_start_index, end_positions=target_end_index)\n>>> predicted_answer_span_start = outputs.start_logits.argmax(-1).item()\n>>> predicted_answer_span_end = outputs.end_logits.argmax(-1).item()\n>>> predicted_answer_span_start, predicted_answer_span_end\n(154, 287)\n```"]