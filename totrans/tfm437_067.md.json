["```py\npip install autoawq\n```", "```py\n{\n  \"_name_or_path\": \"/workspace/process/huggingfaceh4_zephyr-7b-alpha/source\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  ...\n  ...\n  ...\n  \"quantization_config\": {\n    \"quant_method\": \"awq\",\n    \"zero_point\": true,\n    \"group_size\": 128,\n    \"bits\": 4,\n    \"version\": \"gemm\"\n  }\n}\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-alpha-AWQ\", attn_implementation=\"flash_attention_2\", device_map=\"cuda:0\")\n```", "```py\nimport torch\nfrom transformers import AwqConfig, AutoModelForCausalLM\n\nmodel_id = \"TheBloke/Mistral-7B-OpenOrca-AWQ\"\n\nquantization_config = AwqConfig(\n    bits=4,\n    fuse_max_seq_len=512,\n    do_fuse=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)\n```", "```py\npip install auto-gptq\npip install git+https://github.com/huggingface/optimum.git\npip install git+https://github.com/huggingface/transformers.git\npip install --upgrade accelerate\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n\nmodel_id = \"facebook/opt-125m\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ngptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)\n```", "```py\ndataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\ngptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)\n```", "```py\nquantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=gptq_config)\n```", "```py\nquantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"30GiB\", 1: \"46GiB\", \"cpu\": \"30GiB\"}, quantization_config=gptq_config)\n```", "```py\nquantized_model.push_to_hub(\"opt-125m-gptq\")\ntokenizer.push_to_hub(\"opt-125m-gptq\")\n```", "```py\nquantized_model.save_pretrained(\"opt-125m-gptq\")\ntokenizer.save_pretrained(\"opt-125m-gptq\")\n\n# if quantized with device_map set\nquantized_model.to(\"cpu\")\nquantized_model.save_pretrained(\"opt-125m-gptq\")\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, GPTQConfig\n\ngptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\", quantization_config=gptq_config)\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, GPTQConfig\ngptq_config = GPTQConfig(bits=4, use_exllama=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"cpu\", quantization_config=gptq_config)\n```", "```py\npip install transformers accelerate bitsandbytes>0.37.0\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", device_map=\"auto\", load_in_8bit=True)\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, torch_dtype=torch.float32)\nmodel_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", device_map=\"auto\", load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n\nmodel.push_to_hub(\"bloom-560m-8bit\")\n```", "```py\nprint(model.get_memory_footprint())\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\n```", "```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n```", "```py\ndevice_map = {\n    \"transformer.word_embeddings\": 0,\n    \"transformer.word_embeddings_layernorm\": 0,\n    \"lm_head\": \"cpu\",\n    \"transformer.h\": 0,\n    \"transformer.ln_f\": 0,\n}\n```", "```py\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    \"bigscience/bloom-1b7\",\n    device_map=device_map,\n    quantization_config=quantization_config,\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\nquantization_config = BitsAndBytesConfig(\n    llm_int8_threshold=10,\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=device_map,\n    quantization_config=quantization_config,\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\nquantization_config = BitsAndBytesConfig(\n    llm_int8_skip_modules=[\"lm_head\"],\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=quantization_config,\n)\n```", "```py\nimport torch\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n```", "```py\nfrom transformers import BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n```", "```py\nfrom transformers import BitsAndBytesConfig\n\ndouble_quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b\", quantization_config=double_quant_config)\n```"]