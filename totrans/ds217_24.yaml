- en: Cloud storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/datasets/filesystems](https://huggingface.co/docs/datasets/filesystems)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/datasets/v2.17.0/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/start.146395b0.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/scheduler.bdbef820.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/singletons.98dc5b8b.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.8a885b74.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/paths.a483fec8.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/app.e612c4fb.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.c0aea24a.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/0.5e8dbda6.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/20.6ba3e9ce.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Tip.31005f7d.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/CodeBlock.6ccca92e.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Heading.2eb892cb.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ¤— Datasets supports access to cloud storage providers through a `fsspec` FileSystem
    implementations. You can save and load datasets from any cloud storage in a Pythonic
    way. Take a look at the following table for some example of supported cloud storage
    providers:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage provider | Filesystem implementation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |'
  prefs: []
  type: TYPE_TB
- en: '| Google Cloud Storage | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |'
  prefs: []
  type: TYPE_TB
- en: '| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Google Drive | [gdrivefs](https://github.com/intake/gdrivefs) |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle Cloud Storage | [ocifs](https://ocifs.readthedocs.io/en/latest/) |'
  prefs: []
  type: TYPE_TB
- en: This guide will show you how to save and load datasets with any cloud storage.
    Here are examples for S3, Google Cloud Storage, Azure Blob Storage, and Oracle
    Cloud Object Storage.
  prefs: []
  type: TYPE_NORMAL
- en: Set up your cloud storage FileSystem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon S3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the S3 FileSystem implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Define your credentials
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To use an anonymous connection, use `anon=True`. Otherwise, include your `aws_access_key_id`
    and `aws_secret_access_key` whenever you are interacting with a private S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Create your FileSystem instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Google Cloud Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the Google Cloud Storage implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Define your credentials
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Create your FileSystem instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Azure Blob Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the Azure Blob Storage implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Define your credentials
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Create your FileSystem instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Oracle Cloud Object Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the OCI FileSystem implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Define your credentials
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Create your FileSystem instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Load and Save your datasets using your cloud storage FileSystem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download and prepare a dataset into a cloud storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can download and prepare a dataset into your cloud storage by specifying
    a remote `output_dir` in `download_and_prepare`. Donâ€™t forget to use the previously
    defined `storage_options` containing your credentials to write into a private
    cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `download_and_prepare` method works in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: it first downloads the raw data files (if any) in your local cache. You can
    set your cache directory by passing `cache_dir` to [load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: then it generates the dataset in Arrow or Parquet format in your cloud storage
    by iterating over the raw data files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load a dataset builder from the Hugging Face Hub (see [how to load from the
    Hugging Face Hub](./loading#hugging-face-hub)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Load a dataset builder using a loading script (see [how to load a local loading
    script](./loading#local-loading-script)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Use your own data files (see [how to load local and remote files](./loading#local-and-remote-files)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It is highly recommended to save the files as compressed Parquet files to optimize
    I/O by specifying `file_format="parquet"`. Otherwise the dataset is saved as an
    uncompressed Arrow file.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also specify the size of the shards using `max_shard_size` (default
    is 500MB):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Dask
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dask is a parallel computing library and it has a pandas-like API for working
    with larger than memory Parquet datasets in parallel. Dask can use multiple threads
    or processes on a single machine, or a cluster of machines to process data in
    parallel. Dask supports local data but also data from a cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore you can load a dataset saved as sharded Parquet files in Dask with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can find more about dask dataframes in their [documentation](https://docs.dask.org/en/stable/dataframe.html).
  prefs: []
  type: TYPE_NORMAL
- en: Saving serialized datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After you have processed your dataset, you can save it to your cloud storage
    with [Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Remember to define your credentials in your [FileSystem instance](#set-up-your-cloud-storage-filesystem)
    `fs` whenever you are interacting with a private cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: Listing serialized datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'List files from a cloud storage with your FileSystem instance `fs`, using `fs.ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Load serialized datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you are ready to use your dataset again, reload it with [Dataset.load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
