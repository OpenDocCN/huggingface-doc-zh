# 文本到（RGB，深度）

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/ldm3d_diffusion`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/ldm3d_diffusion)

LDM3D 是由 Gabriela Ben Melech Stan、Diana Wofk、Scottie Fox、Alex Redden、Will Saxton、Jean Yu、Estelle Aflalo、Shao-Yen Tseng、Fabio Nonato、Matthias Muller 和 Vasudev Lal 在[LDM3D: Latent Diffusion Model for 3D](https://huggingface.co/papers/2305.10853)中提出的。LDM3D 从给定文本提示生成图像和深度图，与现有的文本到图像扩散模型（如 Stable Diffusion）不同，后者只生成图像。几乎具有相同数量的参数，LDM3D 成功创建了一个可以压缩 RGB 图像和深度图的潜在空间。

有两个可供使用的检查点：

+   [ldm3d-original](https://huggingface.co/Intel/ldm3d)。在[论文](https://arxiv.org/pdf/2305.10853.pdf)中使用的原始检查点

+   [ldm3d-4c](https://huggingface.co/Intel/ldm3d-4c)。LDM3D 的新版本，使用 4 通道输入而不是 6 通道输入，并在更高分辨率图像上进行微调。

论文摘要如下：

*这篇研究论文提出了一种用于从给定文本提示生成图像和深度图数据的三维潜扩散模型（LDM3D），允许用户从文本提示生成 RGBD 图像。LDM3D 模型在包含 RGB 图像、深度图和标题的元组数据集上进行了微调，并通过大量实验进行了验证。我们还开发了一个名为 DepthFusion 的应用程序，该应用程序使用生成的 RGB 图像和深度图来使用 TouchDesigner 创建沉浸式和交互式的 360 度视图体验。这项技术有潜力改变广泛的行业，从娱乐和游戏到建筑和设计。总的来说，这篇论文对生成式人工智能和计算机视觉领域做出了重要贡献，并展示了 LDM3D 和 DepthFusion 改变内容创作和数字体验的潜力。可以在[此网址](https://t.ly/tdi2)找到总结该方法的短视频。*

确保查看 Stable Diffusion Tips 部分，了解如何探索调度器速度和质量之间的权衡，以及如何有效地重用管道组件！

## StableDiffusionLDM3DPipeline

### `class diffusers.StableDiffusionLDM3DPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L84)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional requires_safety_checker: bool = True )
```

参数

+   `vae`（AutoencoderKL）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer））— 一个用于对文本进行标记化的`CLIPTokenizer`。

+   `unet`（UNet2DConditionModel、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 估计生成的图像是否可能被视为具有攻击性或有害的分类模块。请参考[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为 `safety_checker` 的输入。

用于文本到图像和 3D 生成的管道，使用 LDM3D。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   load_textual_inversion() 用于加载文本反演嵌入。

+   load_lora_weights() 用于加载 LoRA 权重

+   save_lora_weights() 用于保存 LoRA 权重

+   from_single_file() 用于加载 `.ckpt` 文件

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L573)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 49 guidance_scale: float = 5.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `height` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 5.0) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 指导在图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时被忽略（`guidance_scale < 1`）。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。ip_adapter_image —（`PipelineImageInput`，*可选*）：可选的图像输入，用于与 IP 适配器一起使用。

+   `output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的格式。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回 StableDiffusionPipelineOutput 而不是普通的元组。

+   `callback`（`Callable`，*可选*）— 在推断过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps`（`int`，*可选*，默认为 1）— 调用`callback`函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs`（`dict`，*可选*）— 如果指定，将传递给`AttentionProcessor`的 kwargs 字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的。

+   `clip_skip`（`int`，*可选*）— 在计算提示嵌入时要跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

返回

StableDiffusionPipelineOutput 或`tuple`

如果`return_dict`为`True`，将返回 StableDiffusionPipelineOutput，否则将返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不安全内容”（nsfw）的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> from diffusers import StableDiffusionLDM3DPipeline

>>> pipe = StableDiffusionLDM3DPipeline.from_pretrained("Intel/ldm3d-4c")
>>> pipe = pipe.to("cuda")

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> output = pipe(prompt)
>>> rgb_image, depth_image = output.rgb, output.depth
>>> rgb_image[0].save("astronaut_ldm3d_rgb.jpg")
>>> depth_image[0].save("astronaut_ldm3d_depth.png")
```

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L177)

```py
( )
```

禁用切片式 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L194)

```py
( )
```

禁用平铺式 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L169)

```py
( )
```

启用切片式 VAE 解码。启用此选项时，VAE 将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L185)

```py
( )
```

启用平铺的 VAE 解码。启用此选项时，VAE 将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L235)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。当不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成 negative_prompt_embeds。

+   `lora_scale` (`float`, *optional*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的 CLIP 层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## LDM3DPipelineOutput

### `class diffusers.pipelines.stable_diffusion_ldm3d.pipeline_stable_diffusion_ldm3d.LDM3DPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py#L62)

```py
( rgb: Union depth: Union nsfw_content_detected: Optional )
```

参数

+   `rgb` (`List[PIL.Image.Image]` or `np.ndarray`) — 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `depth` (`List[PIL.Image.Image]` or `np.ndarray`) — 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw），如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。

#### `__call__`

```py
( *args **kwargs )
```

调用自身作为函数。

# 升频器

[LDM3D-VR](https://arxiv.org/pdf/2311.03226.pdf) 是 LDM3D 的扩展版本。

该论文的摘要是：*潜在扩散模型已被证明在创建和操作视觉输出方面是最先进的。然而，据我们所知，与 RGB 一起生成深度图仍然受到限制。我们引入了 LDM3D-VR，这是一个针对虚拟现实开发的扩散模型套件，包括 LDM3D-pano 和 LDM3D-SR。这些模型使得可以基于文本提示生成全景 RGBD，并将低分辨率输入升级为高分辨率 RGBD。我们的模型是从包含全景/高分辨率 RGB 图像、深度图和标题的现有预训练模型微调而来。这两个模型与现有相关方法进行了评估*

有两个可用于使用的检查点：

+   [ldm3d-pano](https://huggingface.co/Intel/ldm3d-pano)。此检查点使得可以生成全景图像，并需要使用 StableDiffusionLDM3DPipeline 管道。

+   [ldm3d-sr](https://huggingface.co/Intel/ldm3d-sr)。此检查点使 RGB 和深度图像的提升成为可能。可以在原始 LDM3D 管道之后级联使用，使用来自 communauty 管道的 StableDiffusionUpscaleLDM3DPipeline。
