["```py\npip install sagemaker --upgrade\n```", "```py\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```", "```py\nimport sagemaker\nimport boto3\n\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```", "```py\nimport transformers\nimport datasets\nimport argparse\nimport os\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=32)\n    parser.add_argument(\"--model_name_or_path\", type=str)\n\n    # data, model, and output directories\n    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n```", "```py\nfrom sagemaker.huggingface import HuggingFace\n\n# hyperparameters which are passed to the training job\nhyperparameters={'epochs': 1,\n                 'per_device_train_batch_size': 32,\n                 'model_name_or_path': 'distilbert-base-uncased'\n                 }\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        hyperparameters = hyperparameters\n)\n```", "```py\nhuggingface_estimator.fit(\n  {'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',\n   'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'}\n)\n```", "```py\n/opt/conda/bin/python train.py --epochs 1 --model_name_or_path distilbert-base-uncased --per_device_train_batch_size 32\n```", "```py\nfrom sagemaker.s3 import S3Downloader\n\nS3Downloader.download(\n    s3_uri=huggingface_estimator.model_data, # S3 URI where the trained model is located\n    local_path='.',                          # local path where *.targ.gz is saved\n    sagemaker_session=sess                   # SageMaker session used for training the model\n)\n```", "```py\n# configuration for running training on smdistributed data parallel\ndistribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3dn.24xlarge',\n        instance_count=2,\n        role=role,\n        transformers_version='4.26.0',\n        pytorch_version='1.13.1',\n        py_version='py39',\n        hyperparameters = hyperparameters,\n        distribution = distribution\n)\n```", "```py\n# configuration for running training on smdistributed model parallel\nmpi_options = {\n    \"enabled\" : True,\n    \"processes_per_host\" : 8\n}\n\nsmp_options = {\n    \"enabled\":True,\n    \"parameters\": {\n        \"microbatches\": 4,\n        \"placement_strategy\": \"spread\",\n        \"pipeline\": \"interleaved\",\n        \"optimize\": \"speed\",\n        \"partitions\": 4,\n        \"ddp\": True,\n    }\n}\n\ndistribution={\n    \"smdistributed\": {\"modelparallel\": smp_options},\n    \"mpi\": mpi_options\n}\n\n # create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3dn.24xlarge',\n        instance_count=2,\n        role=role,\n        transformers_version='4.26.0',\n        pytorch_version='1.13.1',\n        py_version='py39',\n        hyperparameters = hyperparameters,\n        distribution = distribution\n)\n```", "```py\n# hyperparameters which are passed to the training job\nhyperparameters={'epochs': 1,\n                 'train_batch_size': 32,\n                 'model_name':'distilbert-base-uncased',\n                 'output_dir':'/opt/ml/checkpoints'\n                 }\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n\t    checkpoint_s3_uri=f's3://{sess.default_bucket()}/checkpoints'\n        use_spot_instances=True,\n        # max_wait should be equal to or greater than max_run in seconds\n        max_wait=3600,\n        max_run=1000,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        hyperparameters = hyperparameters\n)\n\n# Training seconds: 874\n# Billable seconds: 262\n# Managed Spot Training savings: 70.0%\n```", "```py\n# configure git settings\ngit_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.4.2'} # v4.4.2 refers to the transformers_version you use in the estimator\n\n # create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='run_glue.py',\n        source_dir='./examples/pytorch/text-classification',\n        git_config=git_config,\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        hyperparameters=hyperparameters\n)\n```", "```py\n# define metrics definitions\nmetric_definitions = [\n    {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n    {\"Name\": \"eval_accuracy\", \"Regex\": \"eval_accuracy.*=\\D*(.*?)$\"},\n    {\"Name\": \"eval_loss\", \"Regex\": \"eval_loss.*=\\D*(.*?)$\"},\n]\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        metric_definitions=metric_definitions,\n        hyperparameters = hyperparameters)\n```"]