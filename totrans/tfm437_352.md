# LayoutLMV2

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2)

## 概述

LayoutLMV2模型是由Yang Xu、Yiheng Xu、Tengchao Lv、Lei Cui、Furu Wei、Guoxin Wang、Yijuan Lu、Dinei Florencio、Cha Zhang、Wanxiang Che、Min Zhang、Lidong Zhou提出的[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)。LayoutLMV2改进了[LayoutLM](layoutlm)以获得跨多个文档图像理解基准的最新结果：

+   从扫描文档中提取信息：[FUNSD](https://guillaumejaume.github.io/FUNSD/)数据集（包含超过30,000个单词的199个带注释表格）、[CORD](https://github.com/clovaai/cord)数据集（包含800张用于训练的收据、100张用于验证和100张用于测试）、[SROIE](https://rrc.cvc.uab.es/?ch=13)数据集（包含626张用于训练和347张用于测试的收据）以及[Kleister-NDA](https://github.com/applicaai/kleister-nda)数据集（包含来自EDGAR数据库的非披露协议，包括254份用于训练、83份用于验证和203份用于测试的文件）。

+   文档图像分类：[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)数据集（包含40万张属于16个类别的图像）。

+   文档视觉问答：[DocVQA](https://arxiv.org/abs/2007.00398)数据集（包含在12,000多个文档图像上定义的5万个问题）。

该论文的摘要如下：

*文本和布局的预训练在各种视觉丰富的文档理解任务中已被证明是有效的，这是由于其有效的模型架构和大规模未标记的扫描/数字化文档的优势。在本文中，我们提出了LayoutLMv2，通过在多模态框架中预训练文本、布局和图像，利用了新的模型架构和预训练任务。具体来说，LayoutLMv2不仅使用现有的遮蔽视觉语言建模任务，还使用新的文本-图像对齐和文本-图像匹配任务在预训练阶段，从而更好地学习跨模态交互。同时，它还将空间感知自注意机制集成到Transformer架构中，使模型能够充分理解不同文本块之间的相对位置关系。实验结果表明，LayoutLMv2优于强基线，并在各种下游视觉丰富的文档理解任务中取得了新的最先进结果，包括FUNSD（0.7895 -> 0.8420）、CORD（0.9493 -> 0.9601）、SROIE（0.9524 -> 0.9781）、Kleister-NDA（0.834 -> 0.852）、RVL-CDIP（0.9443 -> 0.9564）和DocVQA（0.7295 -> 0.8672）。预训练的LayoutLMv2模型可以在此https URL上公开获取。*

LayoutLMv2依赖于`detectron2`、`torchvision`和`tesseract`。运行以下命令进行安装：

```py
python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
python -m pip install torchvision tesseract
```

（如果您正在开发LayoutLMv2，请注意通过doctests还需要安装这些包。）

## 使用提示

+   LayoutLMv1和LayoutLMv2之间的主要区别在于后者在预训练期间包含了视觉嵌入（而LayoutLMv1仅在微调期间添加了视觉嵌入）。

+   LayoutLMv2在自注意力层中添加了相对1D注意力偏置和空间2D注意力偏置到注意力分数中。详细信息可在[论文](https://arxiv.org/abs/2012.14740)的第5页找到。

+   可以在[此处](https://github.com/NielsRogge/Transformers-Tutorials)找到如何在RVL-CDIP、FUNSD、DocVQA、CORD上使用LayoutLMv2模型的演示笔记本。

+   LayoutLMv2使用Facebook AI的[Detectron2](https://github.com/facebookresearch/detectron2/)包作为其视觉骨干。查看[此链接](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)获取安装说明。

+   除了`input_ids`，[forward()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model.forward)还需要2个额外的输入，即`image`和`bbox`。`image`输入对应于文本标记出现的原始文档图像。模型期望每个文档图像的大小为224x224。这意味着如果您有一批文档图像，`image`应该是形状为(batch_size, 3, 224, 224)的张量。这可以是`torch.Tensor`或`Detectron2.structures.ImageList`。您不需要对通道进行归一化，因为模型会自行处理。需要注意的是，视觉主干期望BGR通道而不是RGB，因为Detectron2中的所有模型都是使用BGR格式进行预训练的。`bbox`输入是输入文本标记的边界框（即2D位置）。这与[LayoutLMModel](/docs/transformers/v4.37.2/en/model_doc/layoutlm#transformers.LayoutLMModel)相同。可以使用外部OCR引擎（例如Google的[Tesseract](https://github.com/tesseract-ocr/tesseract)）（有一个[Python包装器](https://pypi.org/project/pytesseract/)可用）来获取这些信息。每个边界框应采用(x0, y0, x1, y1)格式，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。请注意，首先需要将边界框归一化为0-1000的比例。要进行归一化，可以使用以下函数：

```py
def normalize_bbox(bbox, width, height):
    return [
        int(1000 * (bbox[0] / width)),
        int(1000 * (bbox[1] / height)),
        int(1000 * (bbox[2] / width)),
        int(1000 * (bbox[3] / height)),
    ]
```

这里，`width`和`height`对应于标记出现的原始文档的宽度和高度（在调整图像大小之前）。可以使用Python Image Library（PIL）库来获取这些信息，例如：

```py
from PIL import Image

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
)

width, height = image.size
```

然而，该模型包含一个全新的[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)，可用于直接为模型准备数据（包括在底层应用OCR）。更多信息可以在下面的“使用”部分找到。

+   在内部，[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)将通过其视觉主干发送`image`输入，以获得一个分辨率较低的特征图，其形状等于[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)的`image_feature_pool_shape`属性。然后将该特征图展平以获得一系列图像标记。由于特征图的大小默认为7x7，因此获得49个图像标记。然后将这些标记与文本标记连接，并通过Transformer编码器发送。这意味着模型的最后隐藏状态将具有长度为512 + 49 = 561，如果您将文本标记填充到最大长度。更一般地，最后的隐藏状态将具有形状`seq_length` + `image_feature_pool_shape[0]` * `config.image_feature_pool_shape[1]`。

+   在调用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)时，将打印一个警告，其中包含一长串未初始化的参数名称。这不是问题，因为这些参数是批量归一化统计数据，在自定义数据集上微调时将具有值。

+   如果要在分布式环境中训练模型，请确保在模型上调用`synchronize_batch_norm`，以便正确同步视觉主干的批量归一化层。

此外，还有LayoutXLM，这是LayoutLMv2的多语言版本。更多信息可以在[LayoutXLM的文档页面](layoutxlm)找到。

## 资源

官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用LayoutLMv2。如果您有兴趣提交资源以包含在此处，请随时提出拉取请求，我们将对其进行审查！资源应该展示一些新内容，而不是重复现有资源。

文本分类

+   关于如何在RVL-CDIP数据集上对LayoutLMv2进行微调以进行文本分类的笔记。

+   另请参阅：文本分类任务指南

问答

+   关于如何在DocVQA数据集上对LayoutLMv2进行问答微调的笔记。

+   另请参阅：问答任务指南

+   另请参阅：文档问答任务指南

标记分类

+   关于如何在CORD数据集上对LayoutLMv2进行微调以进行标记分类的笔记。

+   关于如何在FUNSD数据集上对LayoutLMv2进行微调以进行标记分类的笔记。

+   另请参阅：标记分类任务指南

## 用法：LayoutLMv2Processor

为模型准备数据的最简单方法是使用LayoutLMv2Processor，它在内部结合了图像处理器（LayoutLMv2ImageProcessor）和标记器（LayoutLMv2Tokenizer或LayoutLMv2TokenizerFast）。图像处理器处理图像模态，而标记器处理文本模态。处理器结合了两者，这对于像LayoutLMv2这样的多模态模型是理想的。请注意，如果您只想处理一个模态，仍然可以分别使用两者。

```py
from transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor

image_processor = LayoutLMv2ImageProcessor()  # apply_ocr is set to True by default
tokenizer = LayoutLMv2TokenizerFast.from_pretrained("microsoft/layoutlmv2-base-uncased")
processor = LayoutLMv2Processor(image_processor, tokenizer)
```

简而言之，可以将文档图像（以及可能的其他数据）提供给LayoutLMv2Processor，它将创建模型期望的输入。在内部，处理器首先使用LayoutLMv2ImageProcessor在图像上应用OCR，以获取单词列表和标准化边界框，并将图像调整大小以获得`image`输入。然后，单词和标准化边界框提供给LayoutLMv2Tokenizer或LayoutLMv2TokenizerFast，将它们转换为标记级别的`input_ids`、`attention_mask`、`token_type_ids`、`bbox`。可选地，可以向处理器提供单词标签，这些标签将转换为标记级别的`labels`。

[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)使用[PyTesseract](https://pypi.org/project/pytesseract/)，这是Google的Tesseract OCR引擎的Python封装。请注意，您仍然可以使用自己选择的OCR引擎，并自己提供单词和标准化框。这需要使用`apply_ocr`设置为`False`来初始化[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)。

总共有5个处理器支持的使用案例。下面我们列出它们。请注意，这些使用案例对批处理和非批处理输入都适用（我们为非批处理输入进行说明）。

使用案例1：文档图像分类（训练、推理）+标记分类（推理），apply_ocr=True

这是最简单的情况，处理器（实际上是图像处理器）将对图像执行OCR，以获取单词和标准化边界框。

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
encoding = processor(
    image, return_tensors="pt"
)  # you can also add all tokenizer parameters here such as padding, truncation
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

使用案例2：文档图像分类（训练、推理）+标记分类（推理），apply_ocr=False

如果想要自己执行OCR，可以将图像处理器初始化为`apply_ocr`设置为`False`。在这种情况下，应该自己向处理器提供单词和相应的（标准化的）边界框。

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
encoding = processor(image, words, boxes=boxes, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

使用案例3：标记分类（训练），apply_ocr=False

对于标记分类任务（如FUNSD、CORD、SROIE、Kleister-NDA），还可以提供相应的单词标签以训练模型。处理器将把这些转换为标记级别的`labels`。默认情况下，它只会标记单词的第一个词片，并用-100标记剩余的词片，这是PyTorch的CrossEntropyLoss的`ignore_index`。如果希望标记单词的所有词片，可以将分词器初始化为`only_label_first_subword`设置为`False`。

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
word_labels = [1, 2]
encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])
```

使用案例4：视觉问答（推理），apply_ocr=True

对于视觉问答任务（如DocVQA），您可以向处理器提供问题。默认情况下，处理器将在图像上应用OCR，并创建[CLS]问题标记[SEP]单词标记[SEP]。

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
question = "What's his name?"
encoding = processor(image, question, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

使用案例5：视觉问答（推理），apply_ocr=False

对于视觉问答任务（如DocVQA），您可以向处理器提供问题。如果您想自己执行OCR，可以向处理器提供自己的单词和（标准化的）边界框。

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
question = "What's his name?"
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
encoding = processor(image, question, words, boxes=boxes, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

## LayoutLMv2Config

### `class transformers.LayoutLMv2Config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/configuration_layoutlmv2.py#L34)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 max_2d_position_embeddings = 1024 max_rel_pos = 128 rel_pos_bins = 32 fast_qkv = True max_rel_2d_pos = 256 rel_2d_pos_bins = 64 convert_sync_batchnorm = True image_feature_pool_shape = [7, 7, 256] coordinate_size = 128 shape_size = 128 has_relative_attention_bias = True has_spatial_attention_bias = True has_visual_segment_embedding = False detectron2_config_args = None **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为30522）—LayoutLMv2模型的词汇量。定义了在调用[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)或`TFLayoutLMv2Model`时可以表示的不同标记数量。

+   `hidden_size`（`int`，*可选*，默认为768）—编码器层和池化器层的维度。

+   `num_hidden_layers`（`int`，*可选*，默认为12）—变换器编码器中的隐藏层数量。

+   `num_attention_heads`（`int`，*可选*，默认为12）—变换器编码器中每个注意力层的注意力头数量。

+   `intermediate_size`（`int`，*可选*，默认为3072）—变换器编码器中“中间”（即前馈）层的维度。

+   `hidden_act`（`str`或`function`，*可选*，默认为`"gelu"`）—编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob`（`float`，*可选*，默认为0.1）—嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为0.1) — 注意力概率的丢弃比率。

+   `max_position_embeddings` (`int`, *optional*, 默认为512) — 此模型可能使用的最大序列长度。通常设置为一个较大的值以防万一（例如512、1024或2048）。

+   `type_vocab_size` (`int`, *optional*, 默认为2) — 在调用[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)或`TFLayoutLMv2Model`时传递的`token_type_ids`的词汇表大小。

+   `initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。

+   `max_2d_position_embeddings` (`int`, *optional*, 默认为1024) — 2D位置嵌入可能使用的最大值。通常设置为一个较大的值以防万一（例如1024）。

+   `max_rel_pos` (`int`, *optional*, 默认为128) — 自注意力机制中要使用的相对位置的最大数量。

+   `rel_pos_bins` (`int`, *optional*, 默认为32) — 自注意力机制中要使用的相对位置桶的数量。

+   `fast_qkv` (`bool`, *optional*, 默认为`True`) — 是否在自注意力层中使用单个矩阵作为查询、键、值。

+   `max_rel_2d_pos` (`int`, *optional*, 默认为256) — 自注意力机制中使用的相对2D位置的最大数量。

+   `rel_2d_pos_bins` (`int`, *optional*, 默认为64) — 自注意力机制中的2D相对位置桶的数量。

+   `image_feature_pool_shape` (`List[int]`, *optional*, 默认为[7, 7, 256]) — 平均池化特征图的形状。

+   `coordinate_size` (`int`, *optional*, 默认为128) — 坐标嵌入的维度。

+   `shape_size` (`int`, *optional*, 默认为128) — 宽度和高度嵌入的维度。

+   `has_relative_attention_bias` (`bool`, *optional*, 默认为`True`) — 是否在自注意力机制中使用相对注意力偏置。

+   `has_spatial_attention_bias` (`bool`, *optional*, 默认为`True`) — 是否在自注意力机制中使用空间注意力偏置。

+   `has_visual_segment_embedding` (`bool`, *optional*, 默认为`False`) — 是否添加视觉段嵌入。

+   `detectron2_config_args` (`dict`, *optional*) — 包含Detectron2视觉骨干配置参数的字典。有关默认值的详细信息，请参阅[此文件](https://github.com/microsoft/unilm/blob/master/layoutlmft/layoutlmft/models/layoutlmv2/detectron2_config.py)。

这是用于存储[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)配置的配置类。它用于根据指定的参数实例化一个LayoutLMv2模型，定义模型架构。使用默认值实例化配置将产生类似于LayoutLMv2 [microsoft/layoutlmv2-base-uncased](https://huggingface.co/microsoft/layoutlmv2-base-uncased)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例:

```py
>>> from transformers import LayoutLMv2Config, LayoutLMv2Model

>>> # Initializing a LayoutLMv2 microsoft/layoutlmv2-base-uncased style configuration
>>> configuration = LayoutLMv2Config()

>>> # Initializing a model (with random weights) from the microsoft/layoutlmv2-base-uncased style configuration
>>> model = LayoutLMv2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LayoutLMv2FeatureExtractor

### `class transformers.LayoutLMv2FeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py#L28)

```py
( *args **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理图像或一批图像。

## LayoutLMv2ImageProcessor

### `class transformers.LayoutLMv2ImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L93)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> apply_ocr: bool = True ocr_lang: Optional = None tesseract_config: Optional = '' **kwargs )
```

参数

+   `do_resize` (`bool`, *可选*, 默认为 `True`) — 是否将图像的 (height, width) 尺寸调整为 `(size["height"], size["width"])`。可以被 `preprocess` 中的 `do_resize` 覆盖。

+   `size` (`Dict[str, int]` *可选*, 默认为 `{"height" -- 224, "width": 224}`): 调整大小后的图像尺寸。可以被 `preprocess` 中的 `size` 覆盖。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `Resampling.BILINEAR`) — 用于调整图像大小时使用的重采样滤波器。可以被 `preprocess` 方法中的 `resample` 参数覆盖。

+   `apply_ocr` (`bool`, *可选*, 默认为 `True`) — 是否应用 Tesseract OCR 引擎以获取单词 + 规范化边界框。可以被 `preprocess` 中的 `apply_ocr` 覆盖。

+   `ocr_lang` (`str`, *可选*) — 由其 ISO 代码指定的语言，用于 Tesseract OCR 引擎。默认情况下使用英语。可以被 `preprocess` 中的 `ocr_lang` 覆盖。

+   `tesseract_config` (`str`, *可选*, 默认为 `""`) — 转发到调用 Tesseract 时 `config` 参数的任何额外自定义配置标志。例如: ‘—psm 6’。可以被 `preprocess` 中的 `tesseract_config` 覆盖。

构建一个 LayoutLMv2 图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L189)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None apply_ocr: bool = None ocr_lang: Optional = None tesseract_config: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 调整大小后输出图像的期望尺寸。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `self.resample`) — 用于调整图像大小时使用的重采样滤波器。可以是枚举 `PIL.Image` 重采样滤波器之一。仅在 `do_resize` 设置为 `True` 时有效。

+   `apply_ocr` (`bool`, *可选*, 默认为 `self.apply_ocr`) — 是否应用 Tesseract OCR 引擎以获取单词 + 规范化边界框。

+   `ocr_lang` (`str`, *可选*, 默认为 `self.ocr_lang`) — 由其 ISO 代码指定的语言，用于 Tesseract OCR 引擎。默认情况下使用英语。

+   `tesseract_config` (`str`, *可选*, 默认为 `self.tesseract_config`) — 转发到调用 Tesseract 时 `config` 参数的任何额外自定义配置标志。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置: 返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`: 返回类型为 `tf.Tensor` 的批处理。

    +   `TensorType.PYTORCH` 或 `'pt'`: 返回类型为 `torch.Tensor` 的批处理。

    +   `TensorType.NUMPY` 或 `'np'`: 返回类型为 `np.ndarray` 的批处理。

    +   `TensorType.JAX` 或 `'jax'`: 返回类型为 `jax.numpy.ndarray` 的批处理。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`: 图像格式为 (num_channels, height, width)。

    +   `ChannelDimension.LAST`: 图像格式为 (height, width, num_channels)。

预处理图像或一批图像。

## LayoutLMv2Tokenizer

### `class transformers.LayoutLMv2Tokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L206)

```py
( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True tokenize_chinese_chars = True strip_accents = None model_max_length: int = 512 additional_special_tokens: Optional = None **kwargs )
```

构建一个 LayoutLMv2 分词器。基于 WordPiece。[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer) 可用于将单词、单词级边界框和可选单词标签转换为标记级的 `input_ids`、`attention_mask`、`token_type_ids`、`bbox` 和可选的 `labels`（用于标记分类）。

这个分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。

[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer) 运行端到端的分词：标点符号拆分和 wordpiece。它还将单词级边界框转换为标记级边界框。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L430)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text`（`str`、`List[str]`、`List[List[str]]`）— 要编码的序列或批次序列。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题）或一个字符串列表的列表（单词批次）。

+   `text_pair`（`List[str]`、`List[List[str]]`）— 要编码的序列或批次序列。每个序列应该是一个字符串列表（预分词的字符串）。

+   `boxes`（`List[List[int]]`、`List[List[List[int]]]`）— 单词级边界框。每个边界框应该被归一化为 0-1000 的比例。

+   `word_labels`（`List[int]`、`List[List[int]]`，*可选*）— 单词级整数标签（用于标记分类任务，如 FUNSD、CORD）。

+   `add_special_tokens`（`bool`，*可选*，默认为 `True`）— 是否使用相对于其模型的特殊标记对序列进行编码。

+   `padding`（`bool`、`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为 `False`）— 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：不进行填充（即，可以输出具有不同长度序列的批次）。

+   `truncation`（`bool`、`str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为 `False`）— 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。这将逐标记截断，如果提供了一对序列（或一批对序列），则会从较长序列中删除一个标记。

    +   `'only_first'`：截断到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`：截断到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不进行截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。

+   `max_length`（`int`，*可选*）— 控制截断/填充参数使用的最大长度。

    如果未设置或设置为`None`，则将使用预定义的模型最大长度，如果截断/填充参数之一需要最大长度。如果模型没有特定的最大输入长度（如XLNet），则截断/填充到最大长度将被停用。

+   `stride`（`int`，*可选*，默认为0）— 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出令牌将包含截断序列末尾的一些令牌，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠令牌的数量。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用张量核心特别有用。

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）— 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`：返回TensorFlow `tf.constant`对象。

    +   `'pt'`：返回PyTorch `torch.Tensor`对象。

    +   `'np'`：返回Numpy `np.ndarray`对象。

+   `return_token_type_ids`（`bool`，*可选*）— 是否返回令牌类型ID。如果保持默认设置，将根据特定分词器的默认值返回令牌类型ID，由`return_outputs`属性定义。

    令牌类型ID是什么？

+   `return_attention_mask`（`bool`，*可选*）— 是否返回注意力蒙版。如果保持默认设置，将根据特定分词器的默认值返回注意力蒙版，由`return_outputs`属性定义。

    注意力蒙版是什么？

+   `return_overflowing_tokens`（`bool`，*可选*，默认为`False`）— 是否返回溢出的令牌序列。如果提供了一对输入ID序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的令牌。

+   `return_special_tokens_mask`（`bool`，*可选*，默认为`False`）— 是否返回特殊令牌蒙版信息。

+   `return_offsets_mapping`（`bool`，*可选*，默认为`False`）— 是否返回每个令牌的`(char_start, char_end)`。

    这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。

+   `return_length`（`bool`，*可选*，默认为`False`）— 是否返回编码输入的长度。

+   `verbose`（`bool`，*可选*，默认为`True`）— 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要馈送到模型的令牌ID列表。

    输入ID是什么？

+   `bbox` — 要馈送到模型的边界框列表。

+   `token_type_ids` — 要馈送到模型的令牌类型ID列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    令牌类型ID是什么？

+   `attention_mask` — 指定哪些令牌应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    注意力蒙版是什么？

+   `labels` — 要馈送到模型的标签列表（当指定`word_labels`时）。

+   `overflowing_tokens` — 溢出令牌序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的标记数（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）。

对一个或多个序列或一个或多个序列对进行标记化和为模型准备，具有单词级别标准化边界框和可选标签。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L410)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LayoutLMv2TokenizerFast

### `class transformers.LayoutLMv2TokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L70)

```py
( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`, *optional*, defaults to `True`) — 在标记化时是否将输入转换为小写。

+   `unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。

+   `sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`) — 用于特殊[SEP]标记的边界框。

+   `pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。

+   `pad_token_label` (`int`, *optional*, defaults to -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。

+   `only_label_first_subword` (`bool`, *optional*, defaults to `True`) — 是否仅标记第一个子词，如果提供了单词标签。

+   `tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — 是否标记化中文字符。这可能应该在日语中停用（参见[此问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`, *optional*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始LayoutLMv2中的情况相同）。

构建一个“快速”LayoutLMv2分词器（由HuggingFace的*tokenizers*库支持）。基于WordPiece。

此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L179)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题）或一个字符串列表的列表（单词批次）。

+   `text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列应该是一个字符串列表（预标记化字符串）。

+   `boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级别的边界框。每个边界框应标准化为0-1000的比例。

+   `word_labels` (`List[int]`, `List[List[int]]`, *optional*) — 单词级别的整数标签（用于诸如FUNSD、CORD等标记分类任务）。

+   `add_special_tokens` (`bool`, *optional*, 默认为`True`) — 是否使用相对于其模型的特殊标记对序列进行编码。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, 默认为`False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 填充到由参数`max_length`指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False` 或 `'do_not_pad'`（默认）: 不填充（即，可以输出具有不同长度的序列的批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, 默认为`False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将逐标记截断，如果提供了一对序列（或一批序列），则从较长序列中删除一个标记。

    +   `'only_first'`: 截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则仅截断第一个序列。

    +   `'only_second'`: 截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则仅截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）: 不截断（即，可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length` (`int`, *optional*) — 控制截断/填充参数使用的最大长度。

    如果未设置或设置为`None`，则如果截断/填充参数中的一个需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *optional*, 默认为0) — 如果设置为一个数字，并且与`max_length`一起使用，当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）— 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`：返回 Numpy `np.ndarray` 对象。

+   `return_token_type_ids`（`bool`，*可选*）— 是否返回令牌类型ID。如果保持默认设置，将根据特定分词器的默认设置返回令牌类型ID，由`return_outputs`属性定义。

    [什么是令牌类型ID？](../glossary#token-type-ids)

+   `return_attention_mask`（`bool`，*可选*）— 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认设置返回注意力掩码，由`return_outputs`属性定义。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `return_overflowing_tokens`（`bool`，*可选*，默认为`False`）— 是否返回溢出的令牌序列。如果提供一对输入id序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的令牌。

+   `return_special_tokens_mask`（`bool`，*可选*，默认为`False`）— 是否返回特殊令牌掩码信息。

+   `return_offsets_mapping`（`bool`，*可选*，默认为`False`）— 是否返回每个令牌的`(char_start, char_end)`。

    这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。

+   `return_length`（`bool`，*可选*，默认为`False`）— 是否返回编码输入的长度。

+   `verbose`（`bool`，*可选*，默认为`True`）— 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

一个带有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的令牌id列表。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` — 要提供给模型的边界框列表。

+   `token_type_ids` — 要提供给模型的令牌类型id列表（当`return_token_type_ids=True`或者`self.model_input_names`中包含*`token_type_ids`*时）。

    [什么是令牌类型ID？](../glossary#token-type-ids)

+   `attention_mask` — 指定哪些令牌应该被模型关注的索引列表（当`return_attention_mask=True`或者`self.model_input_names`中包含*`attention_mask`*时）。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `labels` — 要提供给模型的标签列表（当指定`word_labels`时）。

+   `overflowing_tokens` — 溢出的令牌序列列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的令牌数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊令牌，0指定常规序列令牌（当`add_special_tokens=True`并且`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）。

对一个或多个序列或一个或多个序列对进行分词和准备模型，其中包含单词级别的归一化边界框和可选标签。

## LayoutLMv2Processor

### `class transformers.LayoutLMv2Processor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L27)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（`LayoutLMv2ImageProcessor`，*可选*）— [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)的实例。图像处理器是必需的输入。

+   `tokenizer`（`LayoutLMv2Tokenizer`或`LayoutLMv2TokenizerFast`，*可选*）— [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)或[LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)的实例。标记器是必需的输入。

构建一个LayoutLMv2处理器，将LayoutLMv2图像处理器和LayoutLMv2标记器合并为一个单一处理器。

[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)提供了准备模型数据所需的所有功能。

它首先使用[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)将文档图像调整为固定大小，并可选择应用OCR以获取单词和归一化边界框。然后将它们提供给[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)或[LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)，将单词和边界框转换为标记级别的`input_ids`、`attention_mask`、`token_type_ids`、`bbox`。可选地，可以提供整数`word_labels`，这些标签将转换为用于标记分类任务（如FUNSD、CORD）的标记级别`labels`。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L69)

```py
( images text: Union = None text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = False max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

此方法首先将`images`参数转发到[**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。如果[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)初始化时`apply_ocr`设置为`True`，它将获取的单词和边界框连同其他参数传递给[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)并返回输出，以及调整大小后的`images`。如果[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)初始化时`apply_ocr`设置为`False`，它将用户指定的单词（`text`/`text_pair`）和`boxes`连同其他参数传递给[__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)并返回输出，以及调整大小后的`images`。

请参考上述两个方法的文档字符串以获取更多信息。

## LayoutLMv2Model

### `class transformers.LayoutLMv2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L688)

```py
( config )
```

参数

+   `config`（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的LayoutLMv2模型变换器，输出没有特定头部的原始隐藏状态。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L802)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `((batch_size, sequence_length), 4)`, *optional*) — 每个输入序列标记的边界框。选择范围在`[0, config.max_2d_position_embeddings-1]`内。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

+   `image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` 或 `detectron.structures.ImageList`，其`tensors`的形状为`(batch_size, num_channels, height, width)`) — 文档图像的批处理。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   1对于未被`masked`的标记，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 指示输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围在`[0, config.max_position_embeddings - 1]`内。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的特定头部失效的掩码。选择在`[0, 1]`范围内的掩码值：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，则这很有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)）和输入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, LayoutLMv2Model, set_seed
>>> from PIL import Image
>>> import torch
>>> from datasets import load_dataset

>>> set_seed(88)

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
>>> model = LayoutLMv2Model.from_pretrained("microsoft/layoutlmv2-base-uncased")

>>> dataset = load_dataset("hf-internal-testing/fixtures_docvqa")
>>> image_path = dataset["test"][0]["file"]
>>> image = Image.open(image_path).convert("RGB")

>>> encoding = processor(image, return_tensors="pt")

>>> outputs = model(**encoding)
>>> last_hidden_states = outputs.last_hidden_state

>>> last_hidden_states.shape
torch.Size([1, 342, 768])
```

## LayoutLMv2ForSequenceClassification

### `class transformers.LayoutLMv2ForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L944)

```py
( config )
```

参数

+   `config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv2模型，顶部带有一个序列分类头（在[CLS]标记的最终隐藏状态、平均池化的初始视觉嵌入和平均池化的最终视觉嵌入的串联之上的线性层，例如用于文档图像分类任务，如[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)数据集。

这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L967)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`batch_size, sequence_length`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) — 每个输入序列标记的边界框。选择范围为 `[0, config.max_2d_position_embeddings-1]`。每个边界框应该是 (x0, y0, x1, y1) 格式的归一化版本，其中 (x0, y0) 对应于边界框左上角的位置，而 (x1, y1) 表示右下角的位置。

+   `image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` 或 `detectron.structures.ImageList`，其 `tensors` 的形状为 `(batch_size, num_channels, height, width)`) — 文档图像的批处理。

+   `attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 范围内：

    +   1 表示未被遮蔽的标记，

    +   0 表示被遮蔽的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在 `[0, 1]` 范围内：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的特定头部失效的掩码。掩码值选择在 `[0, 1]` 范围内：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 *input_ids* 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)）和输入而异的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回) — 分类（或如果 config.num_labels==1 则为回归）损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, LayoutLMv2ForSequenceClassification, set_seed
>>> from PIL import Image
>>> import torch
>>> from datasets import load_dataset

>>> set_seed(88)

>>> dataset = load_dataset("rvl_cdip", split="train", streaming=True)
>>> data = next(iter(dataset))
>>> image = data["image"].convert("RGB")

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
>>> model = LayoutLMv2ForSequenceClassification.from_pretrained(
...     "microsoft/layoutlmv2-base-uncased", num_labels=dataset.info.features["label"].num_classes
... )

>>> encoding = processor(image, return_tensors="pt")
>>> sequence_label = torch.tensor([data["label"]])

>>> outputs = model(**encoding, labels=sequence_label)

>>> loss, logits = outputs.loss, outputs.logits
>>> predicted_idx = logits.argmax(dim=-1).item()
>>> predicted_answer = dataset.info.features["label"].names[4]
>>> predicted_idx, predicted_answer
(4, 'advertisement')
```

## LayoutLMv2ForTokenClassification

### `class transformers.LayoutLMv2ForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1126)

```py
( config )
```

参数

+   `config`（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在LayoutLMv2模型的顶部具有标记分类头部（隐藏状态的文本部分上的线性层）的模型，例如用于序列标记（信息提取）任务的[FUNSD](https://guillaumejaume.github.io/FUNSD/)、[SROIE](https://rrc.cvc.uab.es/?ch=13)、[CORD](https://github.com/clovaai/cord)和[Kleister-NDA](https://github.com/applicaai/kleister-nda)。

该模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1149)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) — 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是一个规范化版本，格式为(x0, y0, x1, y1)，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

+   `image`（`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`或`detectron.structures.ImageList`，其`tensors`形状为`(batch_size, num_channels, height, width)`）— 批量文档图像。

+   `attention_mask`（`torch.FloatTensor`，形状为`batch_size, sequence_length`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    什么是注意力掩码？

+   `token_type_ids`（`torch.LongTensor`，形状为`batch_size, sequence_length`，*可选*）— 段标记索引，用于指示输入的第一部分和第二部分。索引选在`[0, 1]`之间：

    +   0 对应于*句子A*的标记。

    +   1 对应于*句子B*的标记。

    什么是标记类型ID？

+   `position_ids`（`torch.LongTensor`，形状为`batch_size, sequence_length`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置ID？

+   `head_mask`（`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示头部未被`masked`。

    +   0 表示头部被`masked`。

+   `inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于计算标记分类损失的标签。索引应在`[0, ..., config.num_labels - 1]`之间。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`。

一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)）和输入的不同元素。

+   `loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回）— 分类损失。

+   `logits`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.num_labels)`）— 分类分数（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出之一，+ 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, LayoutLMv2ForTokenClassification, set_seed
>>> from PIL import Image
>>> from datasets import load_dataset

>>> set_seed(88)

>>> datasets = load_dataset("nielsr/funsd", split="test")
>>> labels = datasets.features["ner_tags"].feature.names
>>> id2label = {v: k for v, k in enumerate(labels)}

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")
>>> model = LayoutLMv2ForTokenClassification.from_pretrained(
...     "microsoft/layoutlmv2-base-uncased", num_labels=len(labels)
... )

>>> data = datasets[0]
>>> image = Image.open(data["image_path"]).convert("RGB")
>>> words = data["words"]
>>> boxes = data["bboxes"]  # make sure to normalize your bounding boxes
>>> word_labels = data["ner_tags"]
>>> encoding = processor(
...     image,
...     words,
...     boxes=boxes,
...     word_labels=word_labels,
...     padding="max_length",
...     truncation=True,
...     return_tensors="pt",
... )

>>> outputs = model(**encoding)
>>> logits, loss = outputs.logits, outputs.loss

>>> predicted_token_class_ids = logits.argmax(-1)
>>> predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]
>>> predicted_tokens_classes[:5]
['B-ANSWER', 'B-HEADER', 'B-HEADER', 'B-HEADER', 'B-HEADER']
```

## LayoutLMv2ForQuestionAnswering

### `class transformers.LayoutLMv2ForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1258)

```py
( config has_visual_segment_embedding = True )
```

参数

+   `config`（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv2模型，在其顶部具有用于提取问答任务的跨度分类头，例如[DocVQA](https://rrc.cvc.uab.es/?ch=17)（在隐藏状态输出的文本部分顶部的线性层，用于计算`跨度起始对数`和`跨度结束对数`）。

这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1280)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`batch_size, sequence_length`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox`（形状为`(batch_size, sequence_length, 4)`的`torch.LongTensor`，*可选*） — 每个输入序列标记的边界框。在范围`[0, config.max_2d_position_embeddings-1]`中选择。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，而(x1, y1)表示边界框右下角的位置。

+   `image`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`或`detectron.structures.ImageList`，其`tensors`的形状为`(batch_size, num_channels, height, width)`） — 文档图像的批处理。

+   `attention_mask`（形状为`batch_size, sequence_length`的`torch.FloatTensor`，*可选*） — 避免在填充标记索引上执行注意力的蒙版。蒙版值选择在`[0, 1]`中：

    +   对于未被`掩码`的标记为1，

    +   对于被`掩码`的标记为0。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `token_type_ids`（形状为`batch_size, sequence_length`的`torch.LongTensor`，*可选*） — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`中：

    +   0对应于*句子A*的标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为 `batch_size, sequence_length`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部无效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将 *input_ids* 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `start_positions` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算标记跨度起始位置的位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度 (`sequence_length`)。超出序列范围的位置不会被考虑在内以计算损失。

+   `end_positions` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算标记跨度结束位置的位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度 (`sequence_length`)。超出序列范围的位置不会被考虑在内以计算损失。

返回

[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)）和输入的各种元素。

+   `损失` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供`labels`时返回) — 总跨度抽取损失是起始位置和结束位置的交叉熵之和。

+   `start_logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`) — 跨度起始分数（SoftMax之前）。

+   `end_logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`) — 跨度结束分数（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（如果模型具有嵌入层，则为嵌入的输出 + 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*，当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

[LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用 `Module` 实例而不是这个函数，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

在下面的示例中，我们给 LayoutLMv2 模型一个图像（包含文本）并向其提问。它会给出一个预测，即它认为答案在从图像中解析的文本中的位置。

```py
>>> from transformers import AutoProcessor, LayoutLMv2ForQuestionAnswering, set_seed
>>> import torch
>>> from PIL import Image
>>> from datasets import load_dataset

>>> set_seed(88)
>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
>>> model = LayoutLMv2ForQuestionAnswering.from_pretrained("microsoft/layoutlmv2-base-uncased")

>>> dataset = load_dataset("hf-internal-testing/fixtures_docvqa")
>>> image_path = dataset["test"][0]["file"]
>>> image = Image.open(image_path).convert("RGB")
>>> question = "When is coffee break?"
>>> encoding = processor(image, question, return_tensors="pt")

>>> outputs = model(**encoding)
>>> predicted_start_idx = outputs.start_logits.argmax(-1).item()
>>> predicted_end_idx = outputs.end_logits.argmax(-1).item()
>>> predicted_start_idx, predicted_end_idx
(154, 287)

>>> predicted_answer_tokens = encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1]
>>> predicted_answer = processor.tokenizer.decode(predicted_answer_tokens)
>>> predicted_answer  # results are not very good without further fine-tuning
'council mem - bers conducted by trrf treasurer philip g. kuehn to get answers which the public ...
```

```py
>>> target_start_index = torch.tensor([7])
>>> target_end_index = torch.tensor([14])
>>> outputs = model(**encoding, start_positions=target_start_index, end_positions=target_end_index)
>>> predicted_answer_span_start = outputs.start_logits.argmax(-1).item()
>>> predicted_answer_span_end = outputs.end_logits.argmax(-1).item()
>>> predicted_answer_span_start, predicted_answer_span_end
(154, 287)
```
