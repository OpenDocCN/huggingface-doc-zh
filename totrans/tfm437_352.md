# LayoutLMV2

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2)

## æ¦‚è¿°

LayoutLMV2æ¨¡å‹æ˜¯ç”±Yang Xuã€Yiheng Xuã€Tengchao Lvã€Lei Cuiã€Furu Weiã€Guoxin Wangã€Yijuan Luã€Dinei Florencioã€Cha Zhangã€Wanxiang Cheã€Min Zhangã€Lidong Zhouæå‡ºçš„[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)ã€‚LayoutLMV2æ”¹è¿›äº†[LayoutLM](layoutlm)ä»¥è·å¾—è·¨å¤šä¸ªæ–‡æ¡£å›¾åƒç†è§£åŸºå‡†çš„æœ€æ–°ç»“æœï¼š

+   ä»æ‰«ææ–‡æ¡£ä¸­æå–ä¿¡æ¯ï¼š[FUNSD](https://guillaumejaume.github.io/FUNSD/)æ•°æ®é›†ï¼ˆåŒ…å«è¶…è¿‡30,000ä¸ªå•è¯çš„199ä¸ªå¸¦æ³¨é‡Šè¡¨æ ¼ï¼‰ã€[CORD](https://github.com/clovaai/cord)æ•°æ®é›†ï¼ˆåŒ…å«800å¼ ç”¨äºè®­ç»ƒçš„æ”¶æ®ã€100å¼ ç”¨äºéªŒè¯å’Œ100å¼ ç”¨äºæµ‹è¯•ï¼‰ã€[SROIE](https://rrc.cvc.uab.es/?ch=13)æ•°æ®é›†ï¼ˆåŒ…å«626å¼ ç”¨äºè®­ç»ƒå’Œ347å¼ ç”¨äºæµ‹è¯•çš„æ”¶æ®ï¼‰ä»¥åŠ[Kleister-NDA](https://github.com/applicaai/kleister-nda)æ•°æ®é›†ï¼ˆåŒ…å«æ¥è‡ªEDGARæ•°æ®åº“çš„éæŠ«éœ²åè®®ï¼ŒåŒ…æ‹¬254ä»½ç”¨äºè®­ç»ƒã€83ä»½ç”¨äºéªŒè¯å’Œ203ä»½ç”¨äºæµ‹è¯•çš„æ–‡ä»¶ï¼‰ã€‚

+   æ–‡æ¡£å›¾åƒåˆ†ç±»ï¼š[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)æ•°æ®é›†ï¼ˆåŒ…å«40ä¸‡å¼ å±äº16ä¸ªç±»åˆ«çš„å›¾åƒï¼‰ã€‚

+   æ–‡æ¡£è§†è§‰é—®ç­”ï¼š[DocVQA](https://arxiv.org/abs/2007.00398)æ•°æ®é›†ï¼ˆåŒ…å«åœ¨12,000å¤šä¸ªæ–‡æ¡£å›¾åƒä¸Šå®šä¹‰çš„5ä¸‡ä¸ªé—®é¢˜ï¼‰ã€‚

è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æ–‡æœ¬å’Œå¸ƒå±€çš„é¢„è®­ç»ƒåœ¨å„ç§è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œè¿™æ˜¯ç”±äºå…¶æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å’Œå¤§è§„æ¨¡æœªæ ‡è®°çš„æ‰«æ/æ•°å­—åŒ–æ–‡æ¡£çš„ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LayoutLMv2ï¼Œé€šè¿‡åœ¨å¤šæ¨¡æ€æ¡†æ¶ä¸­é¢„è®­ç»ƒæ–‡æœ¬ã€å¸ƒå±€å’Œå›¾åƒï¼Œåˆ©ç”¨äº†æ–°çš„æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒLayoutLMv2ä¸ä»…ä½¿ç”¨ç°æœ‰çš„é®è”½è§†è§‰è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œè¿˜ä½¿ç”¨æ–°çš„æ–‡æœ¬-å›¾åƒå¯¹é½å’Œæ–‡æœ¬-å›¾åƒåŒ¹é…ä»»åŠ¡åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä»è€Œæ›´å¥½åœ°å­¦ä¹ è·¨æ¨¡æ€äº¤äº’ã€‚åŒæ—¶ï¼Œå®ƒè¿˜å°†ç©ºé—´æ„ŸçŸ¥è‡ªæ³¨æ„æœºåˆ¶é›†æˆåˆ°Transformeræ¶æ„ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†ç†è§£ä¸åŒæ–‡æœ¬å—ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutLMv2ä¼˜äºå¼ºåŸºçº¿ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼ŒåŒ…æ‹¬FUNSDï¼ˆ0.7895 -> 0.8420ï¼‰ã€CORDï¼ˆ0.9493 -> 0.9601ï¼‰ã€SROIEï¼ˆ0.9524 -> 0.9781ï¼‰ã€Kleister-NDAï¼ˆ0.834 -> 0.852ï¼‰ã€RVL-CDIPï¼ˆ0.9443 -> 0.9564ï¼‰å’ŒDocVQAï¼ˆ0.7295 -> 0.8672ï¼‰ã€‚é¢„è®­ç»ƒçš„LayoutLMv2æ¨¡å‹å¯ä»¥åœ¨æ­¤https URLä¸Šå…¬å¼€è·å–ã€‚*

LayoutLMv2ä¾èµ–äº`detectron2`ã€`torchvision`å’Œ`tesseract`ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š

```py
python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
python -m pip install torchvision tesseract
```

ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å¼€å‘LayoutLMv2ï¼Œè¯·æ³¨æ„é€šè¿‡doctestsè¿˜éœ€è¦å®‰è£…è¿™äº›åŒ…ã€‚ï¼‰

## ä½¿ç”¨æç¤º

+   LayoutLMv1å’ŒLayoutLMv2ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºåè€…åœ¨é¢„è®­ç»ƒæœŸé—´åŒ…å«äº†è§†è§‰åµŒå…¥ï¼ˆè€ŒLayoutLMv1ä»…åœ¨å¾®è°ƒæœŸé—´æ·»åŠ äº†è§†è§‰åµŒå…¥ï¼‰ã€‚

+   LayoutLMv2åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­æ·»åŠ äº†ç›¸å¯¹1Dæ³¨æ„åŠ›åç½®å’Œç©ºé—´2Dæ³¨æ„åŠ›åç½®åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸­ã€‚è¯¦ç»†ä¿¡æ¯å¯åœ¨[è®ºæ–‡](https://arxiv.org/abs/2012.14740)çš„ç¬¬5é¡µæ‰¾åˆ°ã€‚

+   å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials)æ‰¾åˆ°å¦‚ä½•åœ¨RVL-CDIPã€FUNSDã€DocVQAã€CORDä¸Šä½¿ç”¨LayoutLMv2æ¨¡å‹çš„æ¼”ç¤ºç¬”è®°æœ¬ã€‚

+   LayoutLMv2ä½¿ç”¨Facebook AIçš„[Detectron2](https://github.com/facebookresearch/detectron2/)åŒ…ä½œä¸ºå…¶è§†è§‰éª¨å¹²ã€‚æŸ¥çœ‹[æ­¤é“¾æ¥](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)è·å–å®‰è£…è¯´æ˜ã€‚

+   é™¤äº†`input_ids`ï¼Œ[forward()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model.forward)è¿˜éœ€è¦2ä¸ªé¢å¤–çš„è¾“å…¥ï¼Œå³`image`å’Œ`bbox`ã€‚`image`è¾“å…¥å¯¹åº”äºæ–‡æœ¬æ ‡è®°å‡ºç°çš„åŸå§‹æ–‡æ¡£å›¾åƒã€‚æ¨¡å‹æœŸæœ›æ¯ä¸ªæ–‡æ¡£å›¾åƒçš„å¤§å°ä¸º224x224ã€‚è¿™æ„å‘³ç€å¦‚æœæ‚¨æœ‰ä¸€æ‰¹æ–‡æ¡£å›¾åƒï¼Œ`image`åº”è¯¥æ˜¯å½¢çŠ¶ä¸º(batch_size, 3, 224, 224)çš„å¼ é‡ã€‚è¿™å¯ä»¥æ˜¯`torch.Tensor`æˆ–`Detectron2.structures.ImageList`ã€‚æ‚¨ä¸éœ€è¦å¯¹é€šé“è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› ä¸ºæ¨¡å‹ä¼šè‡ªè¡Œå¤„ç†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè§†è§‰ä¸»å¹²æœŸæœ›BGRé€šé“è€Œä¸æ˜¯RGBï¼Œå› ä¸ºDetectron2ä¸­çš„æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨BGRæ ¼å¼è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚`bbox`è¾“å…¥æ˜¯è¾“å…¥æ–‡æœ¬æ ‡è®°çš„è¾¹ç•Œæ¡†ï¼ˆå³2Dä½ç½®ï¼‰ã€‚è¿™ä¸[LayoutLMModel](/docs/transformers/v4.37.2/en/model_doc/layoutlm#transformers.LayoutLMModel)ç›¸åŒã€‚å¯ä»¥ä½¿ç”¨å¤–éƒ¨OCRå¼•æ“ï¼ˆä¾‹å¦‚Googleçš„[Tesseract](https://github.com/tesseract-ocr/tesseract)ï¼‰ï¼ˆæœ‰ä¸€ä¸ª[PythonåŒ…è£…å™¨](https://pypi.org/project/pytesseract/)å¯ç”¨ï¼‰æ¥è·å–è¿™äº›ä¿¡æ¯ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”é‡‡ç”¨(x0, y0, x1, y1)æ ¼å¼ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1)è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œé¦–å…ˆéœ€è¦å°†è¾¹ç•Œæ¡†å½’ä¸€åŒ–ä¸º0-1000çš„æ¯”ä¾‹ã€‚è¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

```py
def normalize_bbox(bbox, width, height):
    return [
        int(1000 * (bbox[0] / width)),
        int(1000 * (bbox[1] / height)),
        int(1000 * (bbox[2] / width)),
        int(1000 * (bbox[3] / height)),
    ]
```

è¿™é‡Œï¼Œ`width`å’Œ`height`å¯¹åº”äºæ ‡è®°å‡ºç°çš„åŸå§‹æ–‡æ¡£çš„å®½åº¦å’Œé«˜åº¦ï¼ˆåœ¨è°ƒæ•´å›¾åƒå¤§å°ä¹‹å‰ï¼‰ã€‚å¯ä»¥ä½¿ç”¨Python Image Libraryï¼ˆPILï¼‰åº“æ¥è·å–è¿™äº›ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š

```py
from PIL import Image

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
)

width, height = image.size
```

ç„¶è€Œï¼Œè¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªå…¨æ–°çš„[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)ï¼Œå¯ç”¨äºç›´æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®ï¼ˆåŒ…æ‹¬åœ¨åº•å±‚åº”ç”¨OCRï¼‰ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨ä¸‹é¢çš„â€œä½¿ç”¨â€éƒ¨åˆ†æ‰¾åˆ°ã€‚

+   åœ¨å†…éƒ¨ï¼Œ[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)å°†é€šè¿‡å…¶è§†è§‰ä¸»å¹²å‘é€`image`è¾“å…¥ï¼Œä»¥è·å¾—ä¸€ä¸ªåˆ†è¾¨ç‡è¾ƒä½çš„ç‰¹å¾å›¾ï¼Œå…¶å½¢çŠ¶ç­‰äº[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)çš„`image_feature_pool_shape`å±æ€§ã€‚ç„¶åå°†è¯¥ç‰¹å¾å›¾å±•å¹³ä»¥è·å¾—ä¸€ç³»åˆ—å›¾åƒæ ‡è®°ã€‚ç”±äºç‰¹å¾å›¾çš„å¤§å°é»˜è®¤ä¸º7x7ï¼Œå› æ­¤è·å¾—49ä¸ªå›¾åƒæ ‡è®°ã€‚ç„¶åå°†è¿™äº›æ ‡è®°ä¸æ–‡æœ¬æ ‡è®°è¿æ¥ï¼Œå¹¶é€šè¿‡Transformerç¼–ç å™¨å‘é€ã€‚è¿™æ„å‘³ç€æ¨¡å‹çš„æœ€åéšè—çŠ¶æ€å°†å…·æœ‰é•¿åº¦ä¸º512 + 49 = 561ï¼Œå¦‚æœæ‚¨å°†æ–‡æœ¬æ ‡è®°å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œæœ€åçš„éšè—çŠ¶æ€å°†å…·æœ‰å½¢çŠ¶`seq_length` + `image_feature_pool_shape[0]` * `config.image_feature_pool_shape[1]`ã€‚

+   åœ¨è°ƒç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ—¶ï¼Œå°†æ‰“å°ä¸€ä¸ªè­¦å‘Šï¼Œå…¶ä¸­åŒ…å«ä¸€é•¿ä¸²æœªåˆå§‹åŒ–çš„å‚æ•°åç§°ã€‚è¿™ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºè¿™äº›å‚æ•°æ˜¯æ‰¹é‡å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®ï¼Œåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒæ—¶å°†å…·æœ‰å€¼ã€‚

+   å¦‚æœè¦åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œè¯·ç¡®ä¿åœ¨æ¨¡å‹ä¸Šè°ƒç”¨`synchronize_batch_norm`ï¼Œä»¥ä¾¿æ­£ç¡®åŒæ­¥è§†è§‰ä¸»å¹²çš„æ‰¹é‡å½’ä¸€åŒ–å±‚ã€‚

æ­¤å¤–ï¼Œè¿˜æœ‰LayoutXLMï¼Œè¿™æ˜¯LayoutLMv2çš„å¤šè¯­è¨€ç‰ˆæœ¬ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[LayoutXLMçš„æ–‡æ¡£é¡µé¢](layoutxlm)æ‰¾åˆ°ã€‚

## èµ„æº

å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨LayoutLMv2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æå‡ºæ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

æ–‡æœ¬åˆ†ç±»

+   å…³äºå¦‚ä½•åœ¨RVL-CDIPæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¬”è®°ã€‚

+   å¦è¯·å‚é˜…ï¼šæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—

é—®ç­”

+   å…³äºå¦‚ä½•åœ¨DocVQAæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œé—®ç­”å¾®è°ƒçš„ç¬”è®°ã€‚

+   å¦è¯·å‚é˜…ï¼šé—®ç­”ä»»åŠ¡æŒ‡å—

+   å¦è¯·å‚é˜…ï¼šæ–‡æ¡£é—®ç­”ä»»åŠ¡æŒ‡å—

æ ‡è®°åˆ†ç±»

+   å…³äºå¦‚ä½•åœ¨CORDæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ ‡è®°åˆ†ç±»çš„ç¬”è®°ã€‚

+   å…³äºå¦‚ä½•åœ¨FUNSDæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ ‡è®°åˆ†ç±»çš„ç¬”è®°ã€‚

+   å¦è¯·å‚é˜…ï¼šæ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—

## ç”¨æ³•ï¼šLayoutLMv2Processor

ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨LayoutLMv2Processorï¼Œå®ƒåœ¨å†…éƒ¨ç»“åˆäº†å›¾åƒå¤„ç†å™¨ï¼ˆLayoutLMv2ImageProcessorï¼‰å’Œæ ‡è®°å™¨ï¼ˆLayoutLMv2Tokenizeræˆ–LayoutLMv2TokenizerFastï¼‰ã€‚å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒæ¨¡æ€ï¼Œè€Œæ ‡è®°å™¨å¤„ç†æ–‡æœ¬æ¨¡æ€ã€‚å¤„ç†å™¨ç»“åˆäº†ä¸¤è€…ï¼Œè¿™å¯¹äºåƒLayoutLMv2è¿™æ ·çš„å¤šæ¨¡æ€æ¨¡å‹æ˜¯ç†æƒ³çš„ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨åªæƒ³å¤„ç†ä¸€ä¸ªæ¨¡æ€ï¼Œä»ç„¶å¯ä»¥åˆ†åˆ«ä½¿ç”¨ä¸¤è€…ã€‚

```py
from transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor

image_processor = LayoutLMv2ImageProcessor()  # apply_ocr is set to True by default
tokenizer = LayoutLMv2TokenizerFast.from_pretrained("microsoft/layoutlmv2-base-uncased")
processor = LayoutLMv2Processor(image_processor, tokenizer)
```

ç®€è€Œè¨€ä¹‹ï¼Œå¯ä»¥å°†æ–‡æ¡£å›¾åƒï¼ˆä»¥åŠå¯èƒ½çš„å…¶ä»–æ•°æ®ï¼‰æä¾›ç»™LayoutLMv2Processorï¼Œå®ƒå°†åˆ›å»ºæ¨¡å‹æœŸæœ›çš„è¾“å…¥ã€‚åœ¨å†…éƒ¨ï¼Œå¤„ç†å™¨é¦–å…ˆä½¿ç”¨LayoutLMv2ImageProcessoråœ¨å›¾åƒä¸Šåº”ç”¨OCRï¼Œä»¥è·å–å•è¯åˆ—è¡¨å’Œæ ‡å‡†åŒ–è¾¹ç•Œæ¡†ï¼Œå¹¶å°†å›¾åƒè°ƒæ•´å¤§å°ä»¥è·å¾—`image`è¾“å…¥ã€‚ç„¶åï¼Œå•è¯å’Œæ ‡å‡†åŒ–è¾¹ç•Œæ¡†æä¾›ç»™LayoutLMv2Tokenizeræˆ–LayoutLMv2TokenizerFastï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`ã€‚å¯é€‰åœ°ï¼Œå¯ä»¥å‘å¤„ç†å™¨æä¾›å•è¯æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å°†è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`labels`ã€‚

[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)ä½¿ç”¨[PyTesseract](https://pypi.org/project/pytesseract/)ï¼Œè¿™æ˜¯Googleçš„Tesseract OCRå¼•æ“çš„Pythonå°è£…ã€‚è¯·æ³¨æ„ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨è‡ªå·±é€‰æ‹©çš„OCRå¼•æ“ï¼Œå¹¶è‡ªå·±æä¾›å•è¯å’Œæ ‡å‡†åŒ–æ¡†ã€‚è¿™éœ€è¦ä½¿ç”¨`apply_ocr`è®¾ç½®ä¸º`False`æ¥åˆå§‹åŒ–[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)ã€‚

æ€»å…±æœ‰5ä¸ªå¤„ç†å™¨æ”¯æŒçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºå®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›ä½¿ç”¨æ¡ˆä¾‹å¯¹æ‰¹å¤„ç†å’Œéæ‰¹å¤„ç†è¾“å…¥éƒ½é€‚ç”¨ï¼ˆæˆ‘ä»¬ä¸ºéæ‰¹å¤„ç†è¾“å…¥è¿›è¡Œè¯´æ˜ï¼‰ã€‚

ä½¿ç”¨æ¡ˆä¾‹1ï¼šæ–‡æ¡£å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒã€æ¨ç†ï¼‰+æ ‡è®°åˆ†ç±»ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=True

è¿™æ˜¯æœ€ç®€å•çš„æƒ…å†µï¼Œå¤„ç†å™¨ï¼ˆå®é™…ä¸Šæ˜¯å›¾åƒå¤„ç†å™¨ï¼‰å°†å¯¹å›¾åƒæ‰§è¡ŒOCRï¼Œä»¥è·å–å•è¯å’Œæ ‡å‡†åŒ–è¾¹ç•Œæ¡†ã€‚

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
encoding = processor(
    image, return_tensors="pt"
)  # you can also add all tokenizer parameters here such as padding, truncation
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

ä½¿ç”¨æ¡ˆä¾‹2ï¼šæ–‡æ¡£å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒã€æ¨ç†ï¼‰+æ ‡è®°åˆ†ç±»ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=False

å¦‚æœæƒ³è¦è‡ªå·±æ‰§è¡ŒOCRï¼Œå¯ä»¥å°†å›¾åƒå¤„ç†å™¨åˆå§‹åŒ–ä¸º`apply_ocr`è®¾ç½®ä¸º`False`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåº”è¯¥è‡ªå·±å‘å¤„ç†å™¨æä¾›å•è¯å’Œç›¸åº”çš„ï¼ˆæ ‡å‡†åŒ–çš„ï¼‰è¾¹ç•Œæ¡†ã€‚

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
encoding = processor(image, words, boxes=boxes, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

ä½¿ç”¨æ¡ˆä¾‹3ï¼šæ ‡è®°åˆ†ç±»ï¼ˆè®­ç»ƒï¼‰ï¼Œapply_ocr=False

å¯¹äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚FUNSDã€CORDã€SROIEã€Kleister-NDAï¼‰ï¼Œè¿˜å¯ä»¥æä¾›ç›¸åº”çš„å•è¯æ ‡ç­¾ä»¥è®­ç»ƒæ¨¡å‹ã€‚å¤„ç†å™¨å°†æŠŠè¿™äº›è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`labels`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒåªä¼šæ ‡è®°å•è¯çš„ç¬¬ä¸€ä¸ªè¯ç‰‡ï¼Œå¹¶ç”¨-100æ ‡è®°å‰©ä½™çš„è¯ç‰‡ï¼Œè¿™æ˜¯PyTorchçš„CrossEntropyLossçš„`ignore_index`ã€‚å¦‚æœå¸Œæœ›æ ‡è®°å•è¯çš„æ‰€æœ‰è¯ç‰‡ï¼Œå¯ä»¥å°†åˆ†è¯å™¨åˆå§‹åŒ–ä¸º`only_label_first_subword`è®¾ç½®ä¸º`False`ã€‚

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
word_labels = [1, 2]
encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])
```

ä½¿ç”¨æ¡ˆä¾‹4ï¼šè§†è§‰é—®ç­”ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=True

å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚DocVQAï¼‰ï¼Œæ‚¨å¯ä»¥å‘å¤„ç†å™¨æä¾›é—®é¢˜ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¤„ç†å™¨å°†åœ¨å›¾åƒä¸Šåº”ç”¨OCRï¼Œå¹¶åˆ›å»º[CLS]é—®é¢˜æ ‡è®°[SEP]å•è¯æ ‡è®°[SEP]ã€‚

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
question = "What's his name?"
encoding = processor(image, question, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

ä½¿ç”¨æ¡ˆä¾‹5ï¼šè§†è§‰é—®ç­”ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=False

å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚DocVQAï¼‰ï¼Œæ‚¨å¯ä»¥å‘å¤„ç†å™¨æä¾›é—®é¢˜ã€‚å¦‚æœæ‚¨æƒ³è‡ªå·±æ‰§è¡ŒOCRï¼Œå¯ä»¥å‘å¤„ç†å™¨æä¾›è‡ªå·±çš„å•è¯å’Œï¼ˆæ ‡å‡†åŒ–çš„ï¼‰è¾¹ç•Œæ¡†ã€‚

```py
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
question = "What's his name?"
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
encoding = processor(image, question, words, boxes=boxes, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

## LayoutLMv2Config

### `class transformers.LayoutLMv2Config`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/configuration_layoutlmv2.py#L34)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 max_2d_position_embeddings = 1024 max_rel_pos = 128 rel_pos_bins = 32 fast_qkv = True max_rel_2d_pos = 256 rel_2d_pos_bins = 64 convert_sync_batchnorm = True image_feature_pool_shape = [7, 7, 256] coordinate_size = 128 shape_size = 128 has_relative_attention_bias = True has_spatial_attention_bias = True has_visual_segment_embedding = False detectron2_config_args = None **kwargs )
```

å‚æ•°

+   `vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º30522ï¼‰â€”LayoutLMv2æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)æˆ–`TFLayoutLMv2Model`æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º768ï¼‰â€”ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º12ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º12ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `intermediate_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3072ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act`ï¼ˆ`str`æˆ–`function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€”ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚

+   `hidden_dropout_prob`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1ï¼‰â€”åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚

+   `type_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º2) â€” åœ¨è°ƒç”¨[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)æˆ–`TFLayoutLMv2Model`æ—¶ä¼ é€’çš„`token_type_ids`çš„è¯æ±‡è¡¨å¤§å°ã€‚

+   `initializer_range` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `max_2d_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º1024) â€” 2Dä½ç½®åµŒå…¥å¯èƒ½ä½¿ç”¨çš„æœ€å¤§å€¼ã€‚é€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚1024ï¼‰ã€‚

+   `max_rel_pos` (`int`, *optional*, é»˜è®¤ä¸º128) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­è¦ä½¿ç”¨çš„ç›¸å¯¹ä½ç½®çš„æœ€å¤§æ•°é‡ã€‚

+   `rel_pos_bins` (`int`, *optional*, é»˜è®¤ä¸º32) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­è¦ä½¿ç”¨çš„ç›¸å¯¹ä½ç½®æ¡¶çš„æ•°é‡ã€‚

+   `fast_qkv` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨å•ä¸ªçŸ©é˜µä½œä¸ºæŸ¥è¯¢ã€é”®ã€å€¼ã€‚

+   `max_rel_2d_pos` (`int`, *optional*, é»˜è®¤ä¸º256) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨çš„ç›¸å¯¹2Dä½ç½®çš„æœ€å¤§æ•°é‡ã€‚

+   `rel_2d_pos_bins` (`int`, *optional*, é»˜è®¤ä¸º64) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„2Dç›¸å¯¹ä½ç½®æ¡¶çš„æ•°é‡ã€‚

+   `image_feature_pool_shape` (`List[int]`, *optional*, é»˜è®¤ä¸º[7, 7, 256]) â€” å¹³å‡æ± åŒ–ç‰¹å¾å›¾çš„å½¢çŠ¶ã€‚

+   `coordinate_size` (`int`, *optional*, é»˜è®¤ä¸º128) â€” åæ ‡åµŒå…¥çš„ç»´åº¦ã€‚

+   `shape_size` (`int`, *optional*, é»˜è®¤ä¸º128) â€” å®½åº¦å’Œé«˜åº¦åµŒå…¥çš„ç»´åº¦ã€‚

+   `has_relative_attention_bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨ç›¸å¯¹æ³¨æ„åŠ›åç½®ã€‚

+   `has_spatial_attention_bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›åç½®ã€‚

+   `has_visual_segment_embedding` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ·»åŠ è§†è§‰æ®µåµŒå…¥ã€‚

+   `detectron2_config_args` (`dict`, *optional*) â€” åŒ…å«Detectron2è§†è§‰éª¨å¹²é…ç½®å‚æ•°çš„å­—å…¸ã€‚æœ‰å…³é»˜è®¤å€¼çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤æ–‡ä»¶](https://github.com/microsoft/unilm/blob/master/layoutlmft/layoutlmft/models/layoutlmv2/detectron2_config.py)ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªLayoutLMv2æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLayoutLMv2 [microsoft/layoutlmv2-base-uncased](https://huggingface.co/microsoft/layoutlmv2-base-uncased)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import LayoutLMv2Config, LayoutLMv2Model

>>> # Initializing a LayoutLMv2 microsoft/layoutlmv2-base-uncased style configuration
>>> configuration = LayoutLMv2Config()

>>> # Initializing a model (with random weights) from the microsoft/layoutlmv2-base-uncased style configuration
>>> model = LayoutLMv2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LayoutLMv2FeatureExtractor

### `class transformers.LayoutLMv2FeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py#L28)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## LayoutLMv2ImageProcessor

### `class transformers.LayoutLMv2ImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L93)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> apply_ocr: bool = True ocr_lang: Optional = None tesseract_config: Optional = '' **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ (height, width) å°ºå¯¸è°ƒæ•´ä¸º `(size["height"], size["width"])`ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `do_resize` è¦†ç›–ã€‚

+   `size` (`Dict[str, int]` *å¯é€‰*, é»˜è®¤ä¸º `{"height" -- 224, "width": 224}`): è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `size` è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º `Resampling.BILINEAR`) â€” ç”¨äºè°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¦†ç›–ã€‚

+   `apply_ocr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åº”ç”¨ Tesseract OCR å¼•æ“ä»¥è·å–å•è¯ + è§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `apply_ocr` è¦†ç›–ã€‚

+   `ocr_lang` (`str`, *å¯é€‰*) â€” ç”±å…¶ ISO ä»£ç æŒ‡å®šçš„è¯­è¨€ï¼Œç”¨äº Tesseract OCR å¼•æ“ã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨è‹±è¯­ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `ocr_lang` è¦†ç›–ã€‚

+   `tesseract_config` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `""`) â€” è½¬å‘åˆ°è°ƒç”¨ Tesseract æ—¶ `config` å‚æ•°çš„ä»»ä½•é¢å¤–è‡ªå®šä¹‰é…ç½®æ ‡å¿—ã€‚ä¾‹å¦‚: â€˜â€”psm 6â€™ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `tesseract_config` è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ª LayoutLMv2 å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L189)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None apply_ocr: bool = None ocr_lang: Optional = None tesseract_config: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º `self.size`) â€” è°ƒæ•´å¤§å°åè¾“å‡ºå›¾åƒçš„æœŸæœ›å°ºå¯¸ã€‚

+   `resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º `self.resample`) â€” ç”¨äºè°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯æšä¸¾ `PIL.Image` é‡é‡‡æ ·æ»¤æ³¢å™¨ä¹‹ä¸€ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `apply_ocr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.apply_ocr`) â€” æ˜¯å¦åº”ç”¨ Tesseract OCR å¼•æ“ä»¥è·å–å•è¯ + è§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚

+   `ocr_lang` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `self.ocr_lang`) â€” ç”±å…¶ ISO ä»£ç æŒ‡å®šçš„è¯­è¨€ï¼Œç”¨äº Tesseract OCR å¼•æ“ã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨è‹±è¯­ã€‚

+   `tesseract_config` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `self.tesseract_config`) â€” è½¬å‘åˆ°è°ƒç”¨ Tesseract æ—¶ `config` å‚æ•°çš„ä»»ä½•é¢å¤–è‡ªå®šä¹‰é…ç½®æ ‡å¿—ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`, *å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`: è¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹å¤„ç†ã€‚

+   `data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (num_channels, height, width)ã€‚

    +   `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (height, width, num_channels)ã€‚

é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## LayoutLMv2Tokenizer

### `class transformers.LayoutLMv2Tokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L206)

```py
( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True tokenize_chinese_chars = True strip_accents = None model_max_length: int = 512 additional_special_tokens: Optional = None **kwargs )
```

æ„å»ºä¸€ä¸ª LayoutLMv2 åˆ†è¯å™¨ã€‚åŸºäº WordPieceã€‚[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer) å¯ç”¨äºå°†å•è¯ã€å•è¯çº§è¾¹ç•Œæ¡†å’Œå¯é€‰å•è¯æ ‡ç­¾è½¬æ¢ä¸ºæ ‡è®°çº§çš„ `input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox` å’Œå¯é€‰çš„ `labels`ï¼ˆç”¨äºæ ‡è®°åˆ†ç±»ï¼‰ã€‚

è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer) è¿è¡Œç«¯åˆ°ç«¯çš„åˆ†è¯ï¼šæ ‡ç‚¹ç¬¦å·æ‹†åˆ†å’Œ wordpieceã€‚å®ƒè¿˜å°†å•è¯çº§è¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§è¾¹ç•Œæ¡†ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L430)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text`ï¼ˆ`str`ã€`List[str]`ã€`List[List[str]]`ï¼‰â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå•ä¸ªç¤ºä¾‹çš„å•è¯æˆ–ä¸€æ‰¹ç¤ºä¾‹çš„é—®é¢˜ï¼‰æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆå•è¯æ‰¹æ¬¡ï¼‰ã€‚

+   `text_pair`ï¼ˆ`List[str]`ã€`List[List[str]]`ï¼‰â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„å­—ç¬¦ä¸²ï¼‰ã€‚

+   `boxes`ï¼ˆ`List[List[int]]`ã€`List[List[List[int]]]`ï¼‰â€” å•è¯çº§è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥è¢«å½’ä¸€åŒ–ä¸º 0-1000 çš„æ¯”ä¾‹ã€‚

+   `word_labels`ï¼ˆ`List[int]`ã€`List[List[int]]`ï¼Œ*å¯é€‰*ï¼‰â€” å•è¯çº§æ•´æ•°æ ‡ç­¾ï¼ˆç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼Œå¦‚ FUNSDã€CORDï¼‰ã€‚

+   `add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚

+   `padding`ï¼ˆ`bool`ã€`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚

    +   `'max_length'`ï¼šå¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation`ï¼ˆ`bool`ã€`str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚è¿™å°†é€æ ‡è®°æˆªæ–­ï¼Œå¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä¼šä»è¾ƒé•¿åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œæˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—é•¿åº¦çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€éœ€è¦æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦å°†è¢«åœç”¨ã€‚

+   `stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºä»¤ç‰Œå°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›ä»¤ç‰Œï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰é‡å ä»¤ç‰Œçš„æ•°é‡ã€‚

+   `pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨å¼ é‡æ ¸å¿ƒç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚

+   `return_token_type_ids`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä»¤ç‰Œç±»å‹IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›ä»¤ç‰Œç±»å‹IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    ä»¤ç‰Œç±»å‹IDæ˜¯ä»€ä¹ˆï¼Ÿ

+   `return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›è’™ç‰ˆã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›è’™ç‰ˆï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    æ³¨æ„åŠ›è’™ç‰ˆæ˜¯ä»€ä¹ˆï¼Ÿ

+   `return_overflowing_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹è¾“å…¥IDåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„ä»¤ç‰Œã€‚

+   `return_special_tokens_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç‰¹æ®Šä»¤ç‰Œè’™ç‰ˆä¿¡æ¯ã€‚

+   `return_offsets_mapping`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æ¯ä¸ªä»¤ç‰Œçš„`(char_start, char_end)`ã€‚

    è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚

+   `return_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚

+   `verbose`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•

è¿”å›

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

å…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š

+   `input_ids` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰ŒIDåˆ—è¡¨ã€‚

    è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ

+   `bbox` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„è¾¹ç•Œæ¡†åˆ—è¡¨ã€‚

+   `token_type_ids` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰Œç±»å‹IDåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*`token_type_ids`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»¤ç‰Œç±»å‹IDæ˜¯ä»€ä¹ˆï¼Ÿ

+   `attention_mask` â€” æŒ‡å®šå“ªäº›ä»¤ç‰Œåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*`attention_mask`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    æ³¨æ„åŠ›è’™ç‰ˆæ˜¯ä»€ä¹ˆï¼Ÿ

+   `labels` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`word_labels`æ—¶ï¼‰ã€‚

+   `overflowing_tokens` â€” æº¢å‡ºä»¤ç‰Œåºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `num_truncated_tokens` â€” æˆªæ–­çš„æ ‡è®°æ•°ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—æ ‡è®°ï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚

+   `length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰ã€‚

å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œæ ‡è®°åŒ–å’Œä¸ºæ¨¡å‹å‡†å¤‡ï¼Œå…·æœ‰å•è¯çº§åˆ«æ ‡å‡†åŒ–è¾¹ç•Œæ¡†å’Œå¯é€‰æ ‡ç­¾ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L410)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LayoutLMv2TokenizerFast

### `class transformers.LayoutLMv2TokenizerFast`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L70)

```py
( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True tokenize_chinese_chars = True strip_accents = None **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚

+   `do_lower_case` (`bool`, *optional*, defaults to `True`) â€” åœ¨æ ‡è®°åŒ–æ—¶æ˜¯å¦å°†è¾“å…¥è½¬æ¢ä¸ºå°å†™ã€‚

+   `unk_token` (`str`, *optional*, defaults to `"[UNK]"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `sep_token` (`str`, *optional*, defaults to `"[SEP]"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œåœ¨ä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚å®ƒä¹Ÿç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚

+   `pad_token` (`str`, *optional*, defaults to `"[PAD]"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `cls_token` (`str`, *optional*, defaults to `"[CLS]"`) â€” åœ¨è¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆå¯¹æ•´ä¸ªåºåˆ—è€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼‰æ—¶ä½¿ç”¨çš„åˆ†ç±»å™¨æ ‡è®°ã€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚

+   `mask_token` (`str`, *optional*, defaults to `"[MASK]"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚

+   `cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[CLS]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚

+   `sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`) â€” ç”¨äºç‰¹æ®Š[SEP]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚

+   `pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[PAD]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚

+   `pad_token_label` (`int`, *optional*, defaults to -100) â€” ç”¨äºå¡«å……æ ‡è®°çš„æ ‡ç­¾ã€‚é»˜è®¤ä¸º-100ï¼Œè¿™æ˜¯PyTorchçš„CrossEntropyLossçš„`ignore_index`ã€‚

+   `only_label_first_subword` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä»…æ ‡è®°ç¬¬ä¸€ä¸ªå­è¯ï¼Œå¦‚æœæä¾›äº†å•è¯æ ‡ç­¾ã€‚

+   `tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦æ ‡è®°åŒ–ä¸­æ–‡å­—ç¬¦ã€‚è¿™å¯èƒ½åº”è¯¥åœ¨æ—¥è¯­ä¸­åœç”¨ï¼ˆå‚è§[æ­¤é—®é¢˜](https://github.com/huggingface/transformers/issues/328)ï¼‰ã€‚

+   `strip_accents` (`bool`, *optional*) â€” æ˜¯å¦å»é™¤æ‰€æœ‰é‡éŸ³ç¬¦å·ã€‚å¦‚æœæœªæŒ‡å®šæ­¤é€‰é¡¹ï¼Œåˆ™å°†ç”±`lowercase`çš„å€¼ç¡®å®šï¼ˆä¸åŸå§‹LayoutLMv2ä¸­çš„æƒ…å†µç›¸åŒï¼‰ã€‚

æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€LayoutLMv2åˆ†è¯å™¨ï¼ˆç”±HuggingFaceçš„*tokenizers*åº“æ”¯æŒï¼‰ã€‚åŸºäºWordPieceã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L179)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text` (`str`, `List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå•ä¸ªç¤ºä¾‹çš„å•è¯æˆ–ä¸€æ‰¹ç¤ºä¾‹çš„é—®é¢˜ï¼‰æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆå•è¯æ‰¹æ¬¡ï¼‰ã€‚

+   `text_pair` (`List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„æ ‡è®°åŒ–å­—ç¬¦ä¸²ï¼‰ã€‚

+   `boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” å•è¯çº§åˆ«çš„è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”æ ‡å‡†åŒ–ä¸º0-1000çš„æ¯”ä¾‹ã€‚

+   `word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” å•è¯çº§åˆ«çš„æ•´æ•°æ ‡ç­¾ï¼ˆç”¨äºè¯¸å¦‚FUNSDã€CORDç­‰æ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚

+   `add_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚

+   `padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚

    +   `'max_length'`: å¡«å……åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰: ä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦çš„åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`: æˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚è¿™å°†é€æ ‡è®°æˆªæ–­ï¼Œå¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™ä»è¾ƒé•¿åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`: æˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`: æˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰: ä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length` (`int`, *optional*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¸­çš„ä¸€ä¸ªéœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride` (`int`, *optional*, é»˜è®¤ä¸º0) â€” å¦‚æœè®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œå¹¶ä¸”ä¸`max_length`ä¸€èµ·ä½¿ç”¨ï¼Œå½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `pad_to_multiple_of` (`int`, *optional*) â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚

+   `return_token_type_ids`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä»¤ç‰Œç±»å‹IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›ä»¤ç‰Œç±»å‹IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    [ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `return_overflowing_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—ã€‚å¦‚æœæä¾›ä¸€å¯¹è¾“å…¥idåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„ä»¤ç‰Œã€‚

+   `return_special_tokens_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç‰¹æ®Šä»¤ç‰Œæ©ç ä¿¡æ¯ã€‚

+   `return_offsets_mapping`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æ¯ä¸ªä»¤ç‰Œçš„`(char_start, char_end)`ã€‚

    è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚

+   `return_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚

+   `verbose`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•

è¿”å›

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š

+   `input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„ä»¤ç‰Œidåˆ—è¡¨ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `bbox` â€” è¦æä¾›ç»™æ¨¡å‹çš„è¾¹ç•Œæ¡†åˆ—è¡¨ã€‚

+   `token_type_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„ä»¤ç‰Œç±»å‹idåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–è€…`self.model_input_names`ä¸­åŒ…å«*`token_type_ids`*æ—¶ï¼‰ã€‚

    [ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `attention_mask` â€” æŒ‡å®šå“ªäº›ä»¤ç‰Œåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–è€…`self.model_input_names`ä¸­åŒ…å«*`attention_mask`*æ—¶ï¼‰ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `labels` â€” è¦æä¾›ç»™æ¨¡å‹çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`word_labels`æ—¶ï¼‰ã€‚

+   `overflowing_tokens` â€” æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `num_truncated_tokens` â€” æˆªæ–­çš„ä»¤ç‰Œæ•°é‡ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šä»¤ç‰Œï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—ä»¤ç‰Œï¼ˆå½“`add_special_tokens=True`å¹¶ä¸”`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚

+   `length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰ã€‚

å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œåˆ†è¯å’Œå‡†å¤‡æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«å•è¯çº§åˆ«çš„å½’ä¸€åŒ–è¾¹ç•Œæ¡†å’Œå¯é€‰æ ‡ç­¾ã€‚

## LayoutLMv2Processor

### `class transformers.LayoutLMv2Processor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L27)

```py
( image_processor = None tokenizer = None **kwargs )
```

å‚æ•°

+   `image_processor`ï¼ˆ`LayoutLMv2ImageProcessor`ï¼Œ*å¯é€‰*ï¼‰â€” [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)çš„å®ä¾‹ã€‚å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer`ï¼ˆ`LayoutLMv2Tokenizer`æˆ–`LayoutLMv2TokenizerFast`ï¼Œ*å¯é€‰*ï¼‰â€” [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)æˆ–[LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)çš„å®ä¾‹ã€‚æ ‡è®°å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ªLayoutLMv2å¤„ç†å™¨ï¼Œå°†LayoutLMv2å›¾åƒå¤„ç†å™¨å’ŒLayoutLMv2æ ‡è®°å™¨åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚

[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)æä¾›äº†å‡†å¤‡æ¨¡å‹æ•°æ®æ‰€éœ€çš„æ‰€æœ‰åŠŸèƒ½ã€‚

å®ƒé¦–å…ˆä½¿ç”¨[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)å°†æ–‡æ¡£å›¾åƒè°ƒæ•´ä¸ºå›ºå®šå¤§å°ï¼Œå¹¶å¯é€‰æ‹©åº”ç”¨OCRä»¥è·å–å•è¯å’Œå½’ä¸€åŒ–è¾¹ç•Œæ¡†ã€‚ç„¶åå°†å®ƒä»¬æä¾›ç»™[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)æˆ–[LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)ï¼Œå°†å•è¯å’Œè¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`ã€‚å¯é€‰åœ°ï¼Œå¯ä»¥æä¾›æ•´æ•°`word_labels`ï¼Œè¿™äº›æ ‡ç­¾å°†è½¬æ¢ä¸ºç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚FUNSDã€CORDï¼‰çš„æ ‡è®°çº§åˆ«`labels`ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L69)

```py
( images text: Union = None text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = False max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

æ­¤æ–¹æ³•é¦–å…ˆå°†`images`å‚æ•°è½¬å‘åˆ°[**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚å¦‚æœ[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)åˆå§‹åŒ–æ—¶`apply_ocr`è®¾ç½®ä¸º`True`ï¼Œå®ƒå°†è·å–çš„å•è¯å’Œè¾¹ç•Œæ¡†è¿åŒå…¶ä»–å‚æ•°ä¼ é€’ç»™[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)å¹¶è¿”å›è¾“å‡ºï¼Œä»¥åŠè°ƒæ•´å¤§å°åçš„`images`ã€‚å¦‚æœ[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)åˆå§‹åŒ–æ—¶`apply_ocr`è®¾ç½®ä¸º`False`ï¼Œå®ƒå°†ç”¨æˆ·æŒ‡å®šçš„å•è¯ï¼ˆ`text`/`text_pair`ï¼‰å’Œ`boxes`è¿åŒå…¶ä»–å‚æ•°ä¼ é€’ç»™[__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)å¹¶è¿”å›è¾“å‡ºï¼Œä»¥åŠè°ƒæ•´å¤§å°åçš„`images`ã€‚

è¯·å‚è€ƒä¸Šè¿°ä¸¤ä¸ªæ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## LayoutLMv2Model

### `class transformers.LayoutLMv2Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L688)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„LayoutLMv2æ¨¡å‹å˜æ¢å™¨ï¼Œè¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L802)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `((batch_size, sequence_length), 4)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰æ‹©èŒƒå›´åœ¨`[0, config.max_2d_position_embeddings-1]`å†…ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯(x0, y0, x1, y1)æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1)è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚

+   `image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` æˆ– `detectron.structures.ImageList`ï¼Œå…¶`tensors`çš„å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š

    +   1å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ

    +   1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´åœ¨`[0, config.max_position_embeddings - 1]`å†…ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„ç‰¹å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, LayoutLMv2Model, set_seed
>>> from PIL import Image
>>> import torch
>>> from datasets import load_dataset

>>> set_seed(88)

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
>>> model = LayoutLMv2Model.from_pretrained("microsoft/layoutlmv2-base-uncased")

>>> dataset = load_dataset("hf-internal-testing/fixtures_docvqa")
>>> image_path = dataset["test"][0]["file"]
>>> image = Image.open(image_path).convert("RGB")

>>> encoding = processor(image, return_tensors="pt")

>>> outputs = model(**encoding)
>>> last_hidden_states = outputs.last_hidden_state

>>> last_hidden_states.shape
torch.Size([1, 342, 768])
```

## LayoutLMv2ForSequenceClassification

### `class transformers.LayoutLMv2ForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L944)

```py
( config )
```

å‚æ•°

+   `config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

LayoutLMv2æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€å¹³å‡æ± åŒ–çš„åˆå§‹è§†è§‰åµŒå…¥å’Œå¹³å‡æ± åŒ–çš„æœ€ç»ˆè§†è§‰åµŒå…¥çš„ä¸²è”ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼Œä¾‹å¦‚ç”¨äºæ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå¦‚[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)æ•°æ®é›†ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L967)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_2d_position_embeddings-1]`ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯ (x0, y0, x1, y1) æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­ (x0, y0) å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œè€Œ (x1, y1) è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚

+   `image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` æˆ– `detectron.structures.ImageList`ï¼Œå…¶ `tensors` çš„å½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`) â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚

+   `attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºæœªè¢«é®è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«é®è”½çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`, *optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„ç‰¹å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, LayoutLMv2ForSequenceClassification, set_seed
>>> from PIL import Image
>>> import torch
>>> from datasets import load_dataset

>>> set_seed(88)

>>> dataset = load_dataset("rvl_cdip", split="train", streaming=True)
>>> data = next(iter(dataset))
>>> image = data["image"].convert("RGB")

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
>>> model = LayoutLMv2ForSequenceClassification.from_pretrained(
...     "microsoft/layoutlmv2-base-uncased", num_labels=dataset.info.features["label"].num_classes
... )

>>> encoding = processor(image, return_tensors="pt")
>>> sequence_label = torch.tensor([data["label"]])

>>> outputs = model(**encoding, labels=sequence_label)

>>> loss, logits = outputs.loss, outputs.logits
>>> predicted_idx = logits.argmax(dim=-1).item()
>>> predicted_answer = dataset.info.features["label"].names[4]
>>> predicted_idx, predicted_answer
(4, 'advertisement')
```

## LayoutLMv2ForTokenClassification

### `class transformers.LayoutLMv2ForTokenClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1126)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨LayoutLMv2æ¨¡å‹çš„é¡¶éƒ¨å…·æœ‰æ ‡è®°åˆ†ç±»å¤´éƒ¨ï¼ˆéšè—çŠ¶æ€çš„æ–‡æœ¬éƒ¨åˆ†ä¸Šçš„çº¿æ€§å±‚ï¼‰çš„æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—æ ‡è®°ï¼ˆä¿¡æ¯æå–ï¼‰ä»»åŠ¡çš„[FUNSD](https://guillaumejaume.github.io/FUNSD/)ã€[SROIE](https://rrc.cvc.uab.es/?ch=13)ã€[CORD](https://github.com/clovaai/cord)å’Œ[Kleister-NDA](https://github.com/applicaai/kleister-nda)ã€‚

è¯¥æ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1149)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_2d_position_embeddings-1]`ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯ä¸€ä¸ªè§„èŒƒåŒ–ç‰ˆæœ¬ï¼Œæ ¼å¼ä¸º(x0, y0, x1, y1)ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1)è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`æˆ–`detectron.structures.ImageList`ï¼Œå…¶`tensors`å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” æ‰¹é‡æ–‡æ¡£å›¾åƒã€‚

+   `attention_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   0 å¯¹åº”äº*å¥å­A*çš„æ ‡è®°ã€‚

    +   1 å¯¹åº”äº*å¥å­B*çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ

+   `position_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ

+   `head_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`ä¹‹é—´ã€‚

è¿”å›

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)æˆ–`tuple(torch.FloatTensor)`ã€‚

ä¸€ä¸ª[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`ï¼‰â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºä¹‹ä¸€ï¼Œ+ æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, LayoutLMv2ForTokenClassification, set_seed
>>> from PIL import Image
>>> from datasets import load_dataset

>>> set_seed(88)

>>> datasets = load_dataset("nielsr/funsd", split="test")
>>> labels = datasets.features["ner_tags"].feature.names
>>> id2label = {v: k for v, k in enumerate(labels)}

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")
>>> model = LayoutLMv2ForTokenClassification.from_pretrained(
...     "microsoft/layoutlmv2-base-uncased", num_labels=len(labels)
... )

>>> data = datasets[0]
>>> image = Image.open(data["image_path"]).convert("RGB")
>>> words = data["words"]
>>> boxes = data["bboxes"]  # make sure to normalize your bounding boxes
>>> word_labels = data["ner_tags"]
>>> encoding = processor(
...     image,
...     words,
...     boxes=boxes,
...     word_labels=word_labels,
...     padding="max_length",
...     truncation=True,
...     return_tensors="pt",
... )

>>> outputs = model(**encoding)
>>> logits, loss = outputs.logits, outputs.loss

>>> predicted_token_class_ids = logits.argmax(-1)
>>> predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]
>>> predicted_tokens_classes[:5]
['B-ANSWER', 'B-HEADER', 'B-HEADER', 'B-HEADER', 'B-HEADER']
```

## LayoutLMv2ForQuestionAnswering

### `class transformers.LayoutLMv2ForQuestionAnswering`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1258)

```py
( config has_visual_segment_embedding = True )
```

å‚æ•°

+   `config`ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

LayoutLMv2æ¨¡å‹ï¼Œåœ¨å…¶é¡¶éƒ¨å…·æœ‰ç”¨äºæå–é—®ç­”ä»»åŠ¡çš„è·¨åº¦åˆ†ç±»å¤´ï¼Œä¾‹å¦‚[DocVQA](https://rrc.cvc.uab.es/?ch=17)ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„æ–‡æœ¬éƒ¨åˆ†é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼Œç”¨äºè®¡ç®—`è·¨åº¦èµ·å§‹å¯¹æ•°`å’Œ`è·¨åº¦ç»“æŸå¯¹æ•°`ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1280)

```py
( input_ids: Optional = None bbox: Optional = None image: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`batch_size, sequence_length`çš„`torch.LongTensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `bbox`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, 4)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚åœ¨èŒƒå›´`[0, config.max_2d_position_embeddings-1]`ä¸­é€‰æ‹©ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯(x0, y0, x1, y1)æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œè€Œ(x1, y1)è¡¨ç¤ºè¾¹ç•Œæ¡†å³ä¸‹è§’çš„ä½ç½®ã€‚

+   `image`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`æˆ–`detectron.structures.ImageList`ï¼Œå…¶`tensors`çš„å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼‰ â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`batch_size, sequence_length`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`æ©ç `çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äºè¢«`æ©ç `çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›è’™ç‰ˆï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`batch_size, sequence_length`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   0å¯¹åº”äº*å¥å­A*çš„æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `batch_size, sequence_length`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `start_positions` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦èµ·å§‹ä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ (`sequence_length`)ã€‚è¶…å‡ºåºåˆ—èŒƒå›´çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚

+   `end_positions` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦ç»“æŸä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ (`sequence_length`)ã€‚è¶…å‡ºåºåˆ—èŒƒå›´çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚

è¿”å›

[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `æŸå¤±` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” æ€»è·¨åº¦æŠ½å–æŸå¤±æ˜¯èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚

+   `start_logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€” è·¨åº¦èµ·å§‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `end_logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€” è·¨åº¦ç»“æŸåˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ç»™ LayoutLMv2 æ¨¡å‹ä¸€ä¸ªå›¾åƒï¼ˆåŒ…å«æ–‡æœ¬ï¼‰å¹¶å‘å…¶æé—®ã€‚å®ƒä¼šç»™å‡ºä¸€ä¸ªé¢„æµ‹ï¼Œå³å®ƒè®¤ä¸ºç­”æ¡ˆåœ¨ä»å›¾åƒä¸­è§£æçš„æ–‡æœ¬ä¸­çš„ä½ç½®ã€‚

```py
>>> from transformers import AutoProcessor, LayoutLMv2ForQuestionAnswering, set_seed
>>> import torch
>>> from PIL import Image
>>> from datasets import load_dataset

>>> set_seed(88)
>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
>>> model = LayoutLMv2ForQuestionAnswering.from_pretrained("microsoft/layoutlmv2-base-uncased")

>>> dataset = load_dataset("hf-internal-testing/fixtures_docvqa")
>>> image_path = dataset["test"][0]["file"]
>>> image = Image.open(image_path).convert("RGB")
>>> question = "When is coffee break?"
>>> encoding = processor(image, question, return_tensors="pt")

>>> outputs = model(**encoding)
>>> predicted_start_idx = outputs.start_logits.argmax(-1).item()
>>> predicted_end_idx = outputs.end_logits.argmax(-1).item()
>>> predicted_start_idx, predicted_end_idx
(154, 287)

>>> predicted_answer_tokens = encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1]
>>> predicted_answer = processor.tokenizer.decode(predicted_answer_tokens)
>>> predicted_answer  # results are not very good without further fine-tuning
'council mem - bers conducted by trrf treasurer philip g. kuehn to get answers which the public ...
```

```py
>>> target_start_index = torch.tensor([7])
>>> target_end_index = torch.tensor([14])
>>> outputs = model(**encoding, start_positions=target_start_index, end_positions=target_end_index)
>>> predicted_answer_span_start = outputs.start_logits.argmax(-1).item()
>>> predicted_answer_span_end = outputs.end_logits.argmax(-1).item()
>>> predicted_answer_span_start, predicted_answer_span_end
(154, 287)
```
