["```py\npip install -q transformers\n```", "```py\n>>> from transformers import pipeline\n\n>>> checkpoint = \"vinvino02/glpn-nyu\"\n>>> depth_estimator = pipeline(\"depth-estimation\", model=checkpoint)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&force=true&w=640\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> image\n```", "```py\n>>> predictions = depth_estimator(image)\n```", "```py\n>>> predictions[\"depth\"]\n```", "```py\n>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n\n>>> checkpoint = \"vinvino02/glpn-nyu\"\n\n>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n>>> model = AutoModelForDepthEstimation.from_pretrained(checkpoint)\n```", "```py\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n```", "```py\n>>> import torch\n\n>>> with torch.no_grad():\n...     outputs = model(pixel_values)\n...     predicted_depth = outputs.predicted_depth\n```", "```py\n>>> import numpy as np\n\n>>> # interpolate to original size\n>>> prediction = torch.nn.functional.interpolate(\n...     predicted_depth.unsqueeze(1),\n...     size=image.size[::-1],\n...     mode=\"bicubic\",\n...     align_corners=False,\n... ).squeeze()\n>>> output = prediction.numpy()\n\n>>> formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n>>> depth = Image.fromarray(formatted)\n>>> depth\n```"]