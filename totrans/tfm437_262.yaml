- en: DETR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DETR
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)
    by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
    Kirillov and Sergey Zagoruyko. DETR consists of a convolutional backbone followed
    by an encoder-decoder Transformer which can be trained end-to-end for object detection.
    It greatly simplifies a lot of the complexity of models like Faster-R-CNN and
    Mask-R-CNN, which use things like region proposals, non-maximum suppression procedure
    and anchor generation. Moreover, DETR can also be naturally extended to perform
    panoptic segmentation, by simply adding a mask head on top of the decoder outputs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DETRæ¨¡å‹æ˜¯ç”±Nicolas Carionã€Francisco Massaã€Gabriel Synnaeveã€Nicolas Usunierã€Alexander
    Kirillovå’ŒSergey Zagoruykoåœ¨[ä½¿ç”¨å˜å‹å™¨è¿›è¡Œç«¯åˆ°ç«¯ç›®æ ‡æ£€æµ‹](https://arxiv.org/abs/2005.12872)ä¸­æå‡ºçš„ã€‚DETRç”±ä¸€ä¸ªå·ç§¯ä¸»å¹²åé¢è·Ÿç€ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨å˜å‹å™¨ç»„æˆï¼Œå¯ä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„ç›®æ ‡æ£€æµ‹è®­ç»ƒã€‚å®ƒæå¤§åœ°ç®€åŒ–äº†åƒFaster-R-CNNå’ŒMask-R-CNNè¿™æ ·çš„æ¨¡å‹çš„å¤æ‚æ€§ï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨åŒºåŸŸæè®®ã€éæå¤§å€¼æŠ‘åˆ¶ç¨‹åºå’Œé”šç‚¹ç”Ÿæˆç­‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼ŒDETRè¿˜å¯ä»¥è‡ªç„¶åœ°æ‰©å±•åˆ°æ‰§è¡Œå…¨æ™¯åˆ†å‰²ï¼Œåªéœ€åœ¨è§£ç å™¨è¾“å‡ºä¹‹ä¸Šæ·»åŠ ä¸€ä¸ªè’™ç‰ˆå¤´ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We present a new method that views object detection as a direct set prediction
    problem. Our approach streamlines the detection pipeline, effectively removing
    the need for many hand-designed components like a non-maximum suppression procedure
    or anchor generation that explicitly encode our prior knowledge about the task.
    The main ingredients of the new framework, called DEtection TRansformer or DETR,
    are a set-based global loss that forces unique predictions via bipartite matching,
    and a transformer encoder-decoder architecture. Given a fixed small set of learned
    object queries, DETR reasons about the relations of the objects and the global
    image context to directly output the final set of predictions in parallel. The
    new model is conceptually simple and does not require a specialized library, unlike
    many other modern detectors. DETR demonstrates accuracy and run-time performance
    on par with the well-established and highly-optimized Faster RCNN baseline on
    the challenging COCO object detection dataset. Moreover, DETR can be easily generalized
    to produce panoptic segmentation in a unified manner. We show that it significantly
    outperforms competitive baselines.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ç›®æ ‡æ£€æµ‹è§†ä¸ºç›´æ¥é›†åˆé¢„æµ‹é—®é¢˜çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†æ£€æµ‹æµç¨‹ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†è®¸å¤šæ‰‹å·¥è®¾è®¡çš„ç»„ä»¶ï¼Œå¦‚éæå¤§å€¼æŠ‘åˆ¶ç¨‹åºæˆ–æ˜ç¡®ç¼–ç æˆ‘ä»¬å¯¹ä»»åŠ¡çš„å…ˆéªŒçŸ¥è¯†çš„é”šç‚¹ç”Ÿæˆã€‚æ–°æ¡†æ¶DEtection
    TRansformeræˆ–DETRçš„ä¸»è¦ç»„æˆéƒ¨åˆ†æ˜¯é€šè¿‡äºŒéƒ¨åŒ¹é…å¼ºåˆ¶å”¯ä¸€é¢„æµ‹çš„åŸºäºé›†åˆçš„å…¨å±€æŸå¤±ï¼Œä»¥åŠä¸€ä¸ªå˜å‹å™¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚ç»™å®šä¸€ç»„å›ºå®šçš„å­¦ä¹ ç›®æ ‡æŸ¥è¯¢ï¼ŒDETRæ¨ç†å¯¹è±¡ä¹‹é—´çš„å…³ç³»å’Œå…¨å±€å›¾åƒä¸Šä¸‹æ–‡ï¼Œç›´æ¥å¹¶è¡Œè¾“å‡ºæœ€ç»ˆçš„é¢„æµ‹é›†ã€‚è¿™ä¸ªæ–°æ¨¡å‹åœ¨æ¦‚å¿µä¸Šç®€å•ï¼Œä¸éœ€è¦ä¸“é—¨çš„åº“ï¼Œä¸åƒè®¸å¤šå…¶ä»–ç°ä»£æ£€æµ‹å™¨ã€‚DETRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„COCOç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¸ç»è¿‡å……åˆ†ä¼˜åŒ–çš„Faster
    RCNNåŸºçº¿ç›¸å½“çš„å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDETRå¯ä»¥è½»æ¾æ¨å¹¿ä¸ºä»¥ç»Ÿä¸€æ–¹å¼äº§ç”Ÿå…¨æ™¯åˆ†å‰²ã€‚æˆ‘ä»¬å±•ç¤ºå®ƒæ˜æ˜¾ä¼˜äºç«äº‰åŸºçº¿ã€‚*'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/facebookresearch/detr).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/detr)æ‰¾åˆ°ã€‚
- en: How DETR works
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DETRçš„å·¥ä½œåŸç†
- en: 'Hereâ€™s a TLDR explaining how [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    works:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è§£é‡Š[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)å·¥ä½œåŸç†çš„TLDRï¼š
- en: First, an image is sent through a pre-trained convolutional backbone (in the
    paper, the authors use ResNet-50/ResNet-101). Letâ€™s assume we also add a batch
    dimension. This means that the input to the backbone is a tensor of shape `(batch_size,
    3, height, width)`, assuming the image has 3 color channels (RGB). The CNN backbone
    outputs a new lower-resolution feature map, typically of shape `(batch_size, 2048,
    height/32, width/32)`. This is then projected to match the hidden dimension of
    the Transformer of DETR, which is `256` by default, using a `nn.Conv2D` layer.
    So now, we have a tensor of shape `(batch_size, 256, height/32, width/32).` Next,
    the feature map is flattened and transposed to obtain a tensor of shape `(batch_size,
    seq_len, d_model)` = `(batch_size, width/32*height/32, 256)`. So a difference
    with NLP models is that the sequence length is actually longer than usual, but
    with a smaller `d_model` (which in NLP is typically 768 or higher).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå°†å›¾åƒé€šè¿‡é¢„è®­ç»ƒçš„å·ç§¯ä¸»å¹²ï¼ˆåœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨ResNet-50/ResNet-101ï¼‰ã€‚å‡è®¾æˆ‘ä»¬ä¹Ÿæ·»åŠ äº†ä¸€ä¸ªæ‰¹å¤„ç†ç»´åº¦ã€‚è¿™æ„å‘³ç€ä¸»å¹²çš„è¾“å…¥æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size,
    3, height, width)`çš„å¼ é‡ï¼Œå‡è®¾å›¾åƒæœ‰3ä¸ªé¢œè‰²é€šé“ï¼ˆRGBï¼‰ã€‚CNNä¸»å¹²è¾“å‡ºä¸€ä¸ªæ–°çš„ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ï¼Œé€šå¸¸å½¢çŠ¶ä¸º`(batch_size, 2048,
    height/32, width/32)`ã€‚ç„¶åï¼Œå°†å…¶æŠ•å½±åˆ°DETRå˜å‹å™¨çš„éšè—ç»´åº¦ï¼Œè¯¥ç»´åº¦é»˜è®¤ä¸º`256`ï¼Œä½¿ç”¨`nn.Conv2D`å±‚ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size,
    256, height/32, width/32)`çš„å¼ é‡ã€‚æ¥ä¸‹æ¥ï¼Œç‰¹å¾å›¾è¢«å±•å¹³å¹¶è½¬ç½®ï¼Œä»¥è·å¾—å½¢çŠ¶ä¸º`(batch_size, seq_len, d_model)`
    = `(batch_size, width/32*height/32, 256)`çš„å¼ é‡ã€‚å› æ­¤ï¼Œä¸NLPæ¨¡å‹çš„ä¸€ä¸ªåŒºåˆ«æ˜¯ï¼Œåºåˆ—é•¿åº¦å®é™…ä¸Šæ¯”é€šå¸¸æ›´é•¿ï¼Œä½†`d_model`è¾ƒå°ï¼ˆåœ¨NLPä¸­é€šå¸¸ä¸º768æˆ–æ›´é«˜ï¼‰ã€‚
- en: 'Next, this is sent through the encoder, outputting `encoder_hidden_states`
    of the same shape (you can consider these as image features). Next, so-called
    **object queries** are sent through the decoder. This is a tensor of shape `(batch_size,
    num_queries, d_model)`, with `num_queries` typically set to 100 and initialized
    with zeros. These input embeddings are learnt positional encodings that the authors
    refer to as object queries, and similarly to the encoder, they are added to the
    input of each attention layer. Each object query will look for a particular object
    in the image. The decoder updates these embeddings through multiple self-attention
    and encoder-decoder attention layers to output `decoder_hidden_states` of the
    same shape: `(batch_size, num_queries, d_model)`. Next, two heads are added on
    top for object detection: a linear layer for classifying each object query into
    one of the objects or â€œno objectâ€, and a MLP to predict bounding boxes for each
    query.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè¿™é€šè¿‡ç¼–ç å™¨å‘é€ï¼Œè¾“å‡ºç›¸åŒå½¢çŠ¶çš„`encoder_hidden_states`ï¼ˆæ‚¨å¯ä»¥å°†è¿™äº›è§†ä¸ºå›¾åƒç‰¹å¾ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œæ‰€è°“çš„**å¯¹è±¡æŸ¥è¯¢**é€šè¿‡è§£ç å™¨å‘é€ã€‚è¿™æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size,
    num_queries, d_model)`çš„å¼ é‡ï¼Œå…¶ä¸­`num_queries`é€šå¸¸è®¾ç½®ä¸º100ï¼Œå¹¶ç”¨é›¶åˆå§‹åŒ–ã€‚è¿™äº›è¾“å…¥åµŒå…¥æ˜¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œä½œè€…å°†å…¶ç§°ä¸ºå¯¹è±¡æŸ¥è¯¢ï¼Œç±»ä¼¼äºç¼–ç å™¨ï¼Œå®ƒä»¬è¢«æ·»åŠ åˆ°æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„è¾“å…¥ä¸­ã€‚æ¯ä¸ªå¯¹è±¡æŸ¥è¯¢å°†åœ¨å›¾åƒä¸­å¯»æ‰¾ç‰¹å®šå¯¹è±¡ã€‚è§£ç å™¨é€šè¿‡å¤šä¸ªè‡ªæ³¨æ„åŠ›å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å±‚æ›´æ–°è¿™äº›åµŒå…¥ï¼Œä»¥è¾“å‡ºç›¸åŒå½¢çŠ¶çš„`decoder_hidden_states`ï¼š`(batch_size,
    num_queries, d_model)`ã€‚æ¥ä¸‹æ¥ï¼Œé¡¶éƒ¨æ·»åŠ äº†ä¸¤ä¸ªå¤´ç”¨äºå¯¹è±¡æ£€æµ‹ï¼šä¸€ä¸ªçº¿æ€§å±‚ç”¨äºå°†æ¯ä¸ªå¯¹è±¡æŸ¥è¯¢åˆ†ç±»ä¸ºå¯¹è±¡æˆ–â€œæ— å¯¹è±¡â€ä¹‹ä¸€ï¼Œä»¥åŠä¸€ä¸ªMLPç”¨äºé¢„æµ‹æ¯ä¸ªæŸ¥è¯¢çš„è¾¹ç•Œæ¡†ã€‚
- en: 'The model is trained using a **bipartite matching loss**: so what we actually
    do is compare the predicted classes + bounding boxes of each of the N = 100 object
    queries to the ground truth annotations, padded up to the same length N (so if
    an image only contains 4 objects, 96 annotations will just have a â€œno objectâ€
    as class and â€œno bounding boxâ€ as bounding box). The [Hungarian matching algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm)
    is used to find an optimal one-to-one mapping of each of the N queries to each
    of the N annotations. Next, standard cross-entropy (for the classes) and a linear
    combination of the L1 and [generalized IoU loss](https://giou.stanford.edu/) (for
    the bounding boxes) are used to optimize the parameters of the model.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ä½¿ç”¨**äºŒéƒ¨åŒ¹é…æŸå¤±**è¿›è¡Œè®­ç»ƒï¼šå®é™…ä¸Šæˆ‘ä»¬æ¯”è¾ƒæ¯ä¸ªN = 100ä¸ªå¯¹è±¡æŸ¥è¯¢çš„é¢„æµ‹ç±»åˆ«+è¾¹ç•Œæ¡†ä¸åœ°é¢çœŸå®æ³¨é‡Šï¼Œå¡«å……åˆ°ç›¸åŒé•¿åº¦Nï¼ˆå› æ­¤å¦‚æœå›¾åƒä»…åŒ…å«4ä¸ªå¯¹è±¡ï¼Œåˆ™96ä¸ªæ³¨é‡Šå°†åªæœ‰ä¸€ä¸ªâ€œæ— å¯¹è±¡â€ä½œä¸ºç±»åˆ«å’Œä¸€ä¸ªâ€œæ— è¾¹ç•Œæ¡†â€ä½œä¸ºè¾¹ç•Œæ¡†ï¼‰ã€‚ä½¿ç”¨[åŒˆç‰™åˆ©åŒ¹é…ç®—æ³•](https://en.wikipedia.org/wiki/Hungarian_algorithm)æ‰¾åˆ°æ¯ä¸ªNæŸ¥è¯¢ä¸æ¯ä¸ªNæ³¨é‡Šçš„æœ€ä½³ä¸€å¯¹ä¸€æ˜ å°„ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ ‡å‡†äº¤å‰ç†µï¼ˆç”¨äºç±»åˆ«ï¼‰å’ŒL1çš„çº¿æ€§ç»„åˆä»¥åŠ[å¹¿ä¹‰IoUæŸå¤±](https://giou.stanford.edu/)ï¼ˆç”¨äºè¾¹ç•Œæ¡†ï¼‰æ¥ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚
- en: DETR can be naturally extended to perform panoptic segmentation (which unifies
    semantic segmentation and instance segmentation). [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    adds a segmentation mask head on top of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection).
    The mask head can be trained either jointly, or in a two steps process, where
    one first trains a [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    model to detect bounding boxes around both â€œthingsâ€ (instances) and â€œstuffâ€ (background
    things like trees, roads, sky), then freeze all the weights and train only the
    mask head for 25 epochs. Experimentally, these two approaches give similar results.
    Note that predicting boxes is required for the training to be possible, since
    the Hungarian matching is computed using distances between boxes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DETRå¯ä»¥è‡ªç„¶æ‰©å±•ä»¥æ‰§è¡Œå…¨æ™¯åˆ†å‰²ï¼ˆå°†è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ç»Ÿä¸€èµ·æ¥ï¼‰ã€‚[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)åœ¨[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)çš„é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªåˆ†å‰²æ©ç å¤´ã€‚æ©ç å¤´å¯ä»¥åŒæ—¶è®­ç»ƒï¼Œæˆ–è€…åœ¨ä¸¤ä¸ªæ­¥éª¤çš„è¿‡ç¨‹ä¸­è®­ç»ƒï¼Œé¦–å…ˆè®­ç»ƒä¸€ä¸ª[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)æ¨¡å‹æ¥æ£€æµ‹â€œäº‹ç‰©â€ï¼ˆå®ä¾‹ï¼‰å’Œâ€œç‰©å“â€ï¼ˆèƒŒæ™¯ç‰©å“ï¼Œå¦‚æ ‘æœ¨ã€é“è·¯ã€å¤©ç©ºï¼‰å‘¨å›´çš„è¾¹ç•Œæ¡†ï¼Œç„¶åå†»ç»“æ‰€æœ‰æƒé‡ï¼Œä»…è®­ç»ƒæ©ç å¤´25ä¸ªæ—¶ä»£ã€‚å®éªŒä¸Šï¼Œè¿™ä¸¤ç§æ–¹æ³•ç»™å‡ºäº†ç±»ä¼¼çš„ç»“æœã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†ä½¿è®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œé¢„æµ‹æ¡†æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºåŒˆç‰™åˆ©åŒ¹é…æ˜¯ä½¿ç”¨æ¡†ä¹‹é—´çš„è·ç¦»è®¡ç®—çš„ã€‚
- en: Usage tips
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: DETR uses so-called **object queries** to detect objects in an image. The number
    of queries determines the maximum number of objects that can be detected in a
    single image, and is set to 100 by default (see parameter `num_queries` of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)).
    Note that itâ€™s good to have some slack (in COCO, the authors used 100, while the
    maximum number of objects in a COCO image is ~70).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DETRä½¿ç”¨æ‰€è°“çš„**å¯¹è±¡æŸ¥è¯¢**æ¥æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡ã€‚æŸ¥è¯¢çš„æ•°é‡ç¡®å®šäº†å•ä¸ªå›¾åƒä¸­å¯ä»¥æ£€æµ‹åˆ°çš„å¯¹è±¡çš„æœ€å¤§æ•°é‡ï¼Œé»˜è®¤è®¾ç½®ä¸º100ï¼ˆè¯·å‚é˜…[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)çš„å‚æ•°`num_queries`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæœ€å¥½æœ‰ä¸€äº›ä½™åœ°ï¼ˆåœ¨COCOä¸­ï¼Œä½œè€…ä½¿ç”¨äº†100ï¼Œè€ŒCOCOå›¾åƒä¸­çš„æœ€å¤§å¯¹è±¡æ•°é‡çº¦ä¸º70ï¼‰ã€‚
- en: The decoder of DETR updates the query embeddings in parallel. This is different
    from language models like GPT-2, which use autoregressive decoding instead of
    parallel. Hence, no causal attention mask is used.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DETRçš„è§£ç å™¨å¹¶è¡Œæ›´æ–°æŸ¥è¯¢åµŒå…¥ã€‚è¿™ä¸åƒGPT-2è¿™æ ·ä½¿ç”¨è‡ªå›å½’è§£ç è€Œä¸æ˜¯å¹¶è¡Œçš„è¯­è¨€æ¨¡å‹ä¸åŒã€‚å› æ­¤ï¼Œä¸ä½¿ç”¨å› æœå…³æ³¨æ©ç ã€‚
- en: DETR adds position embeddings to the hidden states at each self-attention and
    cross-attention layer before projecting to queries and keys. For the position
    embeddings of the image, one can choose between fixed sinusoidal or learned absolute
    position embeddings. By default, the parameter `position_embedding_type` of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    is set to `"sine"`.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å°†éšè—çŠ¶æ€æŠ•å½±åˆ°æŸ¥è¯¢å’Œé”®ä¹‹å‰ï¼ŒDETRåœ¨æ¯ä¸ªè‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚ä¸­æ·»åŠ ä½ç½®åµŒå…¥ã€‚å¯¹äºå›¾åƒçš„ä½ç½®åµŒå…¥ï¼Œå¯ä»¥åœ¨å›ºå®šæ­£å¼¦æˆ–å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)çš„å‚æ•°`position_embedding_type`è®¾ç½®ä¸º`"sine"`ã€‚
- en: During training, the authors of DETR did find it helpful to use auxiliary losses
    in the decoder, especially to help the model output the correct number of objects
    of each class. If you set the parameter `auxiliary_loss` of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    to `True`, then prediction feedforward neural networks and Hungarian losses are
    added after each decoder layer (with the FFNs sharing parameters).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæœŸé—´ï¼ŒDETRçš„ä½œè€…ç¡®å®å‘ç°åœ¨è§£ç å™¨ä¸­ä½¿ç”¨è¾…åŠ©æŸå¤±æ˜¯æœ‰å¸®åŠ©çš„ï¼Œç‰¹åˆ«æ˜¯ä¸ºäº†å¸®åŠ©æ¨¡å‹è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ­£ç¡®å¯¹è±¡æ•°é‡ã€‚å¦‚æœå°†[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)çš„å‚æ•°`auxiliary_loss`è®¾ç½®ä¸º`True`ï¼Œåˆ™åœ¨æ¯ä¸ªè§£ç å™¨å±‚ä¹‹åæ·»åŠ é¢„æµ‹å‰é¦ˆç¥ç»ç½‘ç»œå’ŒåŒˆç‰™åˆ©æŸå¤±ï¼ˆFFNå…±äº«å‚æ•°ï¼‰ã€‚
- en: If you want to train the model in a distributed environment across multiple
    nodes, then one should update the *num_boxes* variable in the *DetrLoss* class
    of *modeling_detr.py*. When training on multiple nodes, this should be set to
    the average number of target boxes across all nodes, as can be seen in the original
    implementation [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³åœ¨è·¨å¤šä¸ªèŠ‚ç‚¹çš„åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åº”è¯¥åœ¨*modeling_detr.py*ä¸­çš„*DetrLoss*ç±»ä¸­æ›´æ–°*num_boxes*å˜é‡ã€‚åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè®­ç»ƒæ—¶ï¼Œåº”å°†å…¶è®¾ç½®ä¸ºæ‰€æœ‰èŠ‚ç‚¹ä¸Šç›®æ ‡æ¡†çš„å¹³å‡æ•°é‡ï¼Œå¯ä»¥åœ¨åŸå§‹å®ç°ä¸­çœ‹åˆ°[è¿™é‡Œ](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232)ã€‚
- en: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    and [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    can be initialized with any convolutional backbone available in the [timm library](https://github.com/rwightman/pytorch-image-models).
    Initializing with a MobileNet backbone for example can be done by setting the
    `backbone` attribute of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    to `"tf_mobilenetv3_small_075"`, and then initializing the model with that config.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)å’Œ[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)å¯ä»¥ä½¿ç”¨[timmåº“](https://github.com/rwightman/pytorch-image-models)ä¸­å¯ç”¨çš„ä»»ä½•å·ç§¯éª¨å¹²è¿›è¡Œåˆå§‹åŒ–ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡å°†[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)çš„`backbone`å±æ€§è®¾ç½®ä¸º`"tf_mobilenetv3_small_075"`ï¼Œç„¶åä½¿ç”¨è¯¥é…ç½®åˆå§‹åŒ–æ¨¡å‹æ¥ä½¿ç”¨MobileNetéª¨å¹²ã€‚'
- en: DETR resizes the input images such that the shortest side is at least a certain
    amount of pixels while the longest is at most 1333 pixels. At training time, scale
    augmentation is used such that the shortest side is randomly set to at least 480
    and at most 800 pixels. At inference time, the shortest side is set to 800\. One
    can use [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    to prepare images (and optional annotations in COCO format) for the model. Due
    to this resizing, images in a batch can have different sizes. DETR solves this
    by padding images up to the largest size in a batch, and by creating a pixel mask
    that indicates which pixels are real/which are padding. Alternatively, one can
    also define a custom `collate_fn` in order to batch images together, using `~transformers.DetrImageProcessor.pad_and_create_pixel_mask`.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DETRè°ƒæ•´è¾“å…¥å›¾åƒçš„å¤§å°ï¼Œä½¿æœ€çŸ­è¾¹è‡³å°‘ä¸ºä¸€å®šæ•°é‡çš„åƒç´ ï¼Œè€Œæœ€é•¿è¾¹è‡³å¤šä¸º1333åƒç´ ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œä½¿ç”¨å°ºåº¦å¢å¼ºï¼Œä½¿æœ€çŸ­è¾¹éšæœºè®¾ç½®ä¸ºè‡³å°‘480åƒç´ ï¼Œæœ€å¤š800åƒç´ ã€‚åœ¨æ¨æ–­æ—¶ï¼Œæœ€çŸ­è¾¹è®¾ç½®ä¸º800ã€‚å¯ä»¥ä½¿ç”¨[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒï¼ˆä»¥åŠå¯é€‰çš„ä»¥COCOæ ¼å¼çš„æ³¨é‡Šï¼‰ã€‚ç”±äºè¿™ç§è°ƒæ•´å¤§å°ï¼Œæ‰¹å¤„ç†ä¸­çš„å›¾åƒå¯èƒ½å…·æœ‰ä¸åŒçš„å¤§å°ã€‚DETRé€šè¿‡å°†å›¾åƒå¡«å……åˆ°æ‰¹å¤„ç†ä¸­çš„æœ€å¤§å¤§å°ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåƒç´ æ©ç æ¥æŒ‡ç¤ºå“ªäº›åƒç´ æ˜¯çœŸå®çš„/å“ªäº›æ˜¯å¡«å……æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦å¤–ï¼Œä¹Ÿå¯ä»¥å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„`collate_fn`æ¥æ‰¹å¤„ç†å›¾åƒï¼Œä½¿ç”¨`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`ã€‚
- en: The size of the images will determine the amount of memory being used, and will
    thus determine the `batch_size`. It is advised to use a batch size of 2 per GPU.
    See [this Github thread](https://github.com/facebookresearch/detr/issues/150)
    for more info.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒçš„å¤§å°å°†å†³å®šæ‰€ä½¿ç”¨çš„å†…å­˜é‡ï¼Œä»è€Œç¡®å®š`batch_size`ã€‚å»ºè®®æ¯ä¸ªGPUä½¿ç”¨æ‰¹é‡å¤§å°ä¸º2ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤Githubçº¿ç¨‹](https://github.com/facebookresearch/detr/issues/150)ã€‚
- en: 'There are three ways to instantiate a DETR model (depending on what you prefer):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸‰ç§å®ä¾‹åŒ–DETRæ¨¡å‹çš„æ–¹æ³•ï¼ˆå–å†³äºæ‚¨çš„åå¥½ï¼‰ï¼š
- en: 'Option 1: Instantiate DETR with pre-trained weights for entire model'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰é¡¹1ï¼šä½¿ç”¨æ•´ä¸ªæ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡å®ä¾‹åŒ–DETR
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Option 2: Instantiate DETR with randomly initialized weights for Transformer,
    but pre-trained weights for backbone'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰é¡¹2ï¼šä½¿ç”¨éšæœºåˆå§‹åŒ–çš„Transformeræƒé‡å®ä¾‹åŒ–DETRï¼Œä½†ä½¿ç”¨éª¨å¹²çš„é¢„è®­ç»ƒæƒé‡
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Option 3: Instantiate DETR with randomly initialized weights for backbone +
    Transformer'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰é¡¹3ï¼šä½¿ç”¨éšæœºåˆå§‹åŒ–çš„éª¨å¹²+Transformerå®ä¾‹åŒ–DETR
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a summary, consider the following table:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹è¡¨æ ¼ï¼š
- en: '| Task | Object detection | Instance segmentation | Panoptic segmentation |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| ä»»åŠ¡ | ç›®æ ‡æ£€æµ‹ | å®ä¾‹åˆ†å‰² | å…¨æ™¯åˆ†å‰² |'
- en: '| --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Description** | Predicting bounding boxes and class labels around objects
    in an image | Predicting masks around objects (i.e. instances) in an image | Predicting
    masks around both objects (i.e. instances) as well as â€œstuffâ€ (i.e. background
    things like trees and roads) in an image |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **æè¿°** | é¢„æµ‹å›¾åƒä¸­ç‰©ä½“å‘¨å›´çš„è¾¹ç•Œæ¡†å’Œç±»æ ‡ç­¾ | é¢„æµ‹å›¾åƒä¸­ç‰©ä½“ï¼ˆå³å®ä¾‹ï¼‰å‘¨å›´çš„æ©æ¨¡ | é¢„æµ‹å›¾åƒä¸­ç‰©ä½“ï¼ˆå³å®ä¾‹ï¼‰ä»¥åŠâ€œç‰©è´¨â€ï¼ˆå³èƒŒæ™¯ç‰©å“å¦‚æ ‘æœ¨å’Œé“è·¯ï¼‰å‘¨å›´çš„æ©æ¨¡
    |'
- en: '| **Model** | [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **æ¨¡å‹** | [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    |'
- en: '| **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO
    panoptic |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **ç¤ºä¾‹æ•°æ®é›†** | COCOæ£€æµ‹ | COCOæ£€æµ‹ï¼ŒCOCOå…¨æ™¯ | COCOå…¨æ™¯ |'
- en: '| **Format of annotations to provide to** [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    | {â€˜image_idâ€™: `int`, â€˜annotationsâ€™: `List[Dict]`} each Dict being a COCO object
    annotation | {â€˜image_idâ€™: `int`, â€˜annotationsâ€™: `List[Dict]`} (in case of COCO
    detection) or {â€˜file_nameâ€™: `str`, â€˜image_idâ€™: `int`, â€˜segments_infoâ€™: `List[Dict]`}
    (in case of COCO panoptic) | {â€˜file_nameâ€™: `str`, â€˜image_idâ€™: `int`, â€˜segments_infoâ€™:
    `List[Dict]`} and masks_path (path to directory containing PNG files of the masks)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| æä¾›ç»™[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)çš„æ³¨é‡Šæ ¼å¼
    | {â€˜image_idâ€™: `int`, â€˜annotationsâ€™: `List[Dict]`}ï¼Œæ¯ä¸ªDictæ˜¯ä¸€ä¸ªCOCOå¯¹è±¡æ³¨é‡Š | {â€˜image_idâ€™:
    `int`, â€˜annotationsâ€™: `List[Dict]`}ï¼ˆåœ¨COCOæ£€æµ‹çš„æƒ…å†µä¸‹ï¼‰æˆ–{â€˜file_nameâ€™: `str`, â€˜image_idâ€™:
    `int`, â€˜segments_infoâ€™: `List[Dict]`}ï¼ˆåœ¨COCOå…¨æ™¯çš„æƒ…å†µä¸‹ï¼‰ | {â€˜file_nameâ€™: `str`, â€˜image_idâ€™:
    `int`, â€˜segments_infoâ€™: `List[Dict]`}å’Œmasks_pathï¼ˆåŒ…å«PNGæ–‡ä»¶çš„æ©æ¨¡ç›®å½•çš„è·¯å¾„ï¼‰ |'
- en: '| **Postprocessing** (i.e. converting the output of the model to Pascal VOC
    format) | `post_process()` | `post_process_segmentation()` | `post_process_segmentation()`,
    `post_process_panoptic()` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **åå¤„ç†**ï¼ˆå³å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºPascal VOCæ ¼å¼ï¼‰ | `post_process()` | `post_process_segmentation()`
    | `post_process_segmentation()`, `post_process_panoptic()` |'
- en: '| **evaluators** | `CocoEvaluator` with `iou_types="bbox"` | `CocoEvaluator`
    with `iou_types="bbox"` or `"segm"` | `CocoEvaluator` with `iou_tupes="bbox"`
    or `"segm"`, `PanopticEvaluator` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **è¯„ä¼°å™¨** | `CocoEvaluator` with `iou_types="bbox"` | `CocoEvaluator` with
    `iou_types="bbox"` or `"segm"` | `CocoEvaluator` with `iou_tupes="bbox"` or `"segm"`,
    `PanopticEvaluator` |'
- en: In short, one should prepare the data either in COCO detection or COCO panoptic
    format, then use [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    to create `pixel_values`, `pixel_mask` and optional `labels`, which can then be
    used to train (or fine-tune) a model. For evaluation, one should first convert
    the outputs of the model using one of the postprocessing methods of [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor).
    These can be be provided to either `CocoEvaluator` or `PanopticEvaluator`, which
    allow you to calculate metrics like mean Average Precision (mAP) and Panoptic
    Quality (PQ). The latter objects are implemented in the [original repository](https://github.com/facebookresearch/detr).
    See the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)
    for more info regarding evaluation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œåº”è¯¥å‡†å¤‡æ•°æ®ä»¥COOæ£€æµ‹æˆ–COOå…¨æ™¯æ ¼å¼ï¼Œç„¶åä½¿ç”¨[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)åˆ›å»º`pixel_values`ã€`pixel_mask`å’Œå¯é€‰çš„`labels`ï¼Œç„¶åå¯ä»¥ç”¨äºè®­ç»ƒï¼ˆæˆ–å¾®è°ƒï¼‰æ¨¡å‹ã€‚å¯¹äºè¯„ä¼°ï¼Œåº”è¯¥é¦–å…ˆä½¿ç”¨[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)çš„å…¶ä¸­ä¸€ç§åå¤„ç†æ–¹æ³•è½¬æ¢æ¨¡å‹çš„è¾“å‡ºã€‚è¿™äº›å¯ä»¥æä¾›ç»™`CocoEvaluator`æˆ–`PanopticEvaluator`ï¼Œè¿™äº›è¯„ä¼°å™¨å…è®¸æ‚¨è®¡ç®—åƒå¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰å’Œå…¨æ™¯è´¨é‡ï¼ˆPQï¼‰è¿™æ ·çš„æŒ‡æ ‡ã€‚åè€…å¯¹è±¡åœ¨[åŸå§‹å­˜å‚¨åº“](https://github.com/facebookresearch/detr)ä¸­å®ç°ã€‚æœ‰å…³è¯„ä¼°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[ç¤ºä¾‹ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)ã€‚
- en: Resources
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with DETR.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨DETRã€‚
- en: Object Detection
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹
- en: All example notebooks illustrating fine-tuning [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    and [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    on a custom dataset an be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ç¤ºä¾‹ç¬”è®°æœ¬è¯´æ˜åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¯¹[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)å’Œ[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)è¿›è¡Œå¾®è°ƒçš„ç¤ºä¾‹å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)æ‰¾åˆ°ã€‚
- en: 'See also: [Object detection task guide](../tasks/object_detection)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚è§ï¼š[ç›®æ ‡æ£€æµ‹ä»»åŠ¡æŒ‡å—](../tasks/object_detection)
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: DetrConfig
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrConfig
- en: '### `class transformers.DetrConfig`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L36)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L36)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`use_timm_backbone` (`bool`, *optional*, defaults to `True`) â€” Whether or not
    to use the `timm` library for the backbone. If set to `False`, will use the `AutoBackbone`
    API.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_timm_backbone`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦ä½¿ç”¨`timm`åº“ä½œä¸ºéª¨å¹²ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œå°†ä½¿ç”¨`AutoBackbone`
    APIã€‚'
- en: '`backbone_config` (`PretrainedConfig` or `dict`, *optional*) â€” The configuration
    of the backbone model. Only used in case `use_timm_backbone` is set to `False`
    in which case it will default to `ResNetConfig()`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config`ï¼ˆ`PretrainedConfig`æˆ–`dict`ï¼Œ*å¯é€‰*ï¼‰â€” éª¨å¹²æ¨¡å‹çš„é…ç½®ã€‚ä»…åœ¨`use_timm_backbone`è®¾ç½®ä¸º`False`æ—¶ä½¿ç”¨ï¼Œé»˜è®¤ä¸º`ResNetConfig()`ã€‚'
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰â€” è¾“å…¥é€šé“çš„æ•°é‡ã€‚'
- en: '`num_queries` (`int`, *optional*, defaults to 100) â€” Number of object queries,
    i.e. detection slots. This is the maximal number of objects [DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)
    can detect in a single image. For COCO, we recommend 100 queries.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_queries`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º100ï¼‰â€” å¯¹è±¡æŸ¥è¯¢çš„æ•°é‡ï¼Œå³æ£€æµ‹æ§½çš„æ•°é‡ã€‚è¿™æ˜¯[DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)åœ¨å•ä¸ªå›¾åƒä¸­å¯ä»¥æ£€æµ‹çš„å¯¹è±¡çš„æœ€å¤§æ•°é‡ã€‚å¯¹äºCOCOï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨100ä¸ªæŸ¥è¯¢ã€‚'
- en: '`d_model` (`int`, *optional*, defaults to 256) â€” Dimension of the layers.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º256ï¼‰â€” å±‚çš„ç»´åº¦ã€‚'
- en: '`encoder_layers` (`int`, *optional*, defaults to 6) â€” Number of encoder layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º6ï¼‰â€” ç¼–ç å™¨å±‚æ•°ã€‚'
- en: '`decoder_layers` (`int`, *optional*, defaults to 6) â€” Number of decoder layers.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º6ï¼‰â€” è§£ç å™¨å±‚æ•°ã€‚'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 8) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º8ï¼‰â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 8) â€” Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 8) â€” Transformerè§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 2048) â€” Dimension of the
    â€œintermediateâ€ (often named feed-forward) layer in decoder.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 2048) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 2048) â€” Dimension of the
    â€œintermediateâ€ (often named feed-forward) layer in decoder.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 2048) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    â€” The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.0) â€” å…¨è¿æ¥å±‚å†…æ¿€æ´»çš„dropoutæ¯”ç‡ã€‚'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) â€” The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`init_xavier_std` (`float`, *optional*, defaults to 1) â€” The scaling factor
    used for the Xavier initialization gain in the HM Attention map module.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_xavier_std` (`float`, *optional*, defaults to 1) â€” ç”¨äºHM Attention mapæ¨¡å—ä¸­Xavieråˆå§‹åŒ–å¢ç›Šçš„ç¼©æ”¾å› å­ã€‚'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” The LayerDrop
    probability for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” ç¼–ç å™¨çš„LayerDropæ¦‚ç‡ã€‚æ›´å¤šç»†èŠ‚è¯·å‚é˜…[LayerDrop
    paper](https://arxiv.org/abs/1909.11556)ã€‚'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” The LayerDrop
    probability for the decoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” è§£ç å™¨çš„LayerDropæ¦‚ç‡ã€‚æ›´å¤šç»†èŠ‚è¯·å‚é˜…[LayerDrop
    paper](https://arxiv.org/abs/1909.11556)ã€‚'
- en: '`auxiliary_loss` (`bool`, *optional*, defaults to `False`) â€” Whether auxiliary
    decoding losses (loss at each decoder layer) are to be used.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨è¾…åŠ©è§£ç æŸå¤±ï¼ˆæ¯ä¸ªè§£ç å™¨å±‚çš„æŸå¤±ï¼‰ã€‚'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"sine"`) â€” Type
    of position embeddings to be used on top of the image features. One of `"sine"`
    or `"learned"`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *optional*, defaults to `"sine"`) â€” åœ¨å›¾åƒç‰¹å¾ä¹‹ä¸Šä½¿ç”¨çš„ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚å¯ä»¥æ˜¯`"sine"`æˆ–`"learned"`ä¹‹ä¸€ã€‚'
- en: '`backbone` (`str`, *optional*, defaults to `"resnet50"`) â€” Name of convolutional
    backbone to use in case `use_timm_backbone` = `True`. Supports any convolutional
    backbone from the timm package. For a list of all available models, see [this
    page](https://rwightman.github.io/pytorch-image-models/#load-a-pretrained-model).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone` (`str`, *optional*, defaults to `"resnet50"`) â€” åœ¨`use_timm_backbone`
    = `True`æ—¶è¦ä½¿ç”¨çš„å·ç§¯éª¨å¹²ç½‘ç»œçš„åç§°ã€‚æ”¯æŒtimmåŒ…ä¸­çš„ä»»ä½•å·ç§¯éª¨å¹²ç½‘ç»œã€‚æœ‰å…³æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„åˆ—è¡¨ï¼Œè¯·å‚é˜…[æ­¤é¡µé¢](https://rwightman.github.io/pytorch-image-models/#load-a-pretrained-model)ã€‚'
- en: '`use_pretrained_backbone` (`bool`, *optional*, defaults to `True`) â€” Whether
    to use pretrained weights for the backbone. Only supported when `use_timm_backbone`
    = `True`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_pretrained_backbone` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨éª¨å¹²ç½‘ç»œä¸­ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ã€‚ä»…åœ¨`use_timm_backbone`
    = `True`æ—¶æ”¯æŒã€‚'
- en: '`dilation` (`bool`, *optional*, defaults to `False`) â€” Whether to replace stride
    with dilation in the last convolutional block (DC5). Only supported when `use_timm_backbone`
    = `True`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dilation` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æœ€åçš„å·ç§¯å—ï¼ˆDC5ï¼‰ä¸­ç”¨æ‰©å¼ æ›¿æ¢æ­¥å¹…ã€‚ä»…åœ¨`use_timm_backbone`
    = `True`æ—¶æ”¯æŒã€‚'
- en: '`class_cost` (`float`, *optional*, defaults to 1) â€” Relative weight of the
    classification error in the Hungarian matching cost.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_cost` (`float`, *optional*, defaults to 1) â€” åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­åˆ†ç±»é”™è¯¯çš„ç›¸å¯¹æƒé‡ã€‚'
- en: '`bbox_cost` (`float`, *optional*, defaults to 5) â€” Relative weight of the L1
    error of the bounding box coordinates in the Hungarian matching cost.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_cost` (`float`, *optional*, defaults to 5) â€” ç›¸å¯¹äºåŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­è¾¹ç•Œæ¡†åæ ‡çš„L1è¯¯å·®çš„æƒé‡ã€‚'
- en: '`giou_cost` (`float`, *optional*, defaults to 2) â€” Relative weight of the generalized
    IoU loss of the bounding box in the Hungarian matching cost.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`giou_cost` (`float`, *optional*, defaults to 2) â€” ç›¸å¯¹äºåŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­è¾¹ç•Œæ¡†å¹¿ä¹‰IoUæŸå¤±çš„æƒé‡ã€‚'
- en: '`mask_loss_coefficient` (`float`, *optional*, defaults to 1) â€” Relative weight
    of the Focal loss in the panoptic segmentation loss.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_loss_coefficient` (`float`, *optional*, defaults to 1) â€” æ³›å…‰åˆ†å‰²æŸå¤±ä¸­FocalæŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚'
- en: '`dice_loss_coefficient` (`float`, *optional*, defaults to 1) â€” Relative weight
    of the DICE/F-1 loss in the panoptic segmentation loss.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dice_loss_coefficient` (`float`, *optional*, defaults to 1) â€” æ³›å…‰åˆ†å‰²æŸå¤±ä¸­DICE/F-1æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚'
- en: '`bbox_loss_coefficient` (`float`, *optional*, defaults to 5) â€” Relative weight
    of the L1 bounding box loss in the object detection loss.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_loss_coefficient` (`float`, *optional*, defaults to 5) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­L1è¾¹ç•Œæ¡†æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚'
- en: '`giou_loss_coefficient` (`float`, *optional*, defaults to 2) â€” Relative weight
    of the generalized IoU loss in the object detection loss.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`giou_loss_coefficient` (`float`, *optional*, defaults to 2) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­å¹¿ä¹‰IoUæŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚'
- en: '`eos_coefficient` (`float`, *optional*, defaults to 0.1) â€” Relative classification
    weight of the â€˜no-objectâ€™ class in the object detection loss.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_coefficient` (`float`, *optional*, defaults to 0.1) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­â€œæ— å¯¹è±¡â€ç±»åˆ«çš„ç›¸å¯¹åˆ†ç±»æƒé‡ã€‚'
- en: This is the configuration class to store the configuration of a [DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel).
    It is used to instantiate a DETR model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the DETR [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)
    architecture.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªDETRæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºDETR
    [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Examples:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `from_backbone_config`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_backbone_config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L239)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L239)'
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`backbone_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” The backbone configuration.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” éª¨å¹²é…ç½®ã€‚'
- en: Returns
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)'
- en: An instance of a configuration object
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡çš„å®ä¾‹
- en: Instantiate a [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    (or a derived class) from a pre-trained backbone model configuration.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒçš„éª¨å¹²æ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)ï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚
- en: DetrImageProcessor
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrImageProcessor
- en: '### `class transformers.DetrImageProcessor`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L742)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L742)'
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`format` (`str`, *optional*, defaults to `"coco_detection"`) â€” Data format
    of the annotations. One of â€œcoco_detectionâ€ or â€œcoco_panopticâ€.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format` (`str`, *optional*, é»˜è®¤ä¸º`"coco_detection"`) â€” æ³¨é‡Šçš„æ•°æ®æ ¼å¼ã€‚å¯ä»¥æ˜¯â€œcoco_detectionâ€æˆ–â€œcoco_panopticâ€ä¹‹ä¸€ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Controls whether to
    resize the imageâ€™s `(height, width)` dimensions to the specified `size`. Can be
    overridden by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ§åˆ¶æ˜¯å¦å°†å›¾åƒçš„`(height, width)`ç»´åº¦è°ƒæ•´ä¸ºæŒ‡å®šçš„`size`ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_resize`å‚æ•°è¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 800,
    "longest_edge": 1333}`): Size of the imageâ€™s `(height, width)` dimensions after
    resizing. Can be overridden by the `size` parameter in the `preprocess` method.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *optional*, é»˜è®¤ä¸º `{"shortest_edge" -- 800, "longest_edge":
    1333}`): è°ƒæ•´å¤§å°åçš„å›¾åƒçš„`(height, width)`ç»´åº¦å¤§å°ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`size`å‚æ•°è¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`)
    â€” Resampling filter to use if resizing the image.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, é»˜è®¤ä¸º `PILImageResampling.BILINEAR`)
    â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) â€” Controls whether to
    rescale the image by the specified scale `rescale_factor`. Can be overridden by
    the `do_rescale` parameter in the `preprocess` method.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ§åˆ¶æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹å› å­`rescale_factor`é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_rescale`å‚æ•°è¦†ç›–ã€‚'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) â€” Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method. do_normalize â€” Controls whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` æˆ– `float`, *optional*, é»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°è°ƒæ•´å›¾åƒï¼Œåˆ™è¦ä½¿ç”¨çš„æ¯”ä¾‹å› å­ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`rescale_factor`å‚æ•°è¦†ç›–ã€‚do_normalize
    â€” æ§åˆ¶æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_normalize`å‚æ•°è¦†ç›–ã€‚'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`)
    â€” Mean values to use when normalizing the image. Can be a single value or a list
    of values, one for each channel. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º `IMAGENET_DEFAULT_MEAN`)
    â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„å‡å€¼ã€‚å¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–æ¯ä¸ªé€šé“çš„å€¼åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_mean`å‚æ•°è¦†ç›–ã€‚'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`)
    â€” Standard deviation values to use when normalizing the image. Can be a single
    value or a list of values, one for each channel. Can be overridden by the `image_std`
    parameter in the `preprocess` method.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º `IMAGENET_DEFAULT_STD`)
    â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„æ ‡å‡†å·®å€¼ã€‚å¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–æ¯ä¸ªé€šé“çš„å€¼åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_std`å‚æ•°è¦†ç›–ã€‚'
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) â€” Controls whether to pad
    the image to the largest image in a batch and create a pixel mask. Can be overridden
    by the `do_pad` parameter in the `preprocess` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ§åˆ¶æ˜¯å¦å°†å›¾åƒå¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€å¤§çš„å›¾åƒå¹¶åˆ›å»ºåƒç´ æ©ç ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_pad`å‚æ•°è¦†ç›–ã€‚'
- en: Constructs a Detr image processor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ„é€ ä¸€ä¸ªDetrå›¾åƒå¤„ç†å™¨ã€‚
- en: '#### `preprocess`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1070)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1070)'
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image or batch of images to preprocess. Expects a
    single or batch of images with pixel values ranging from 0 to 255\. If passing
    in images with pixel values between 0 and 1, set `do_rescale=False`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹é‡åƒç´ å€¼èŒƒå›´ä»0åˆ°255çš„å›¾åƒã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚'
- en: '`annotations` (`AnnotationType` or `List[AnnotationType]`, *optional*) â€” List
    of annotations associated with the image or batch of images. If annotation is
    for object detection, the annotations should be a dictionary with the following
    keys:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`annotations` (`AnnotationType` or `List[AnnotationType]`, *optional*) â€” ä¸å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ç›¸å…³è”çš„æ³¨é‡Šåˆ—è¡¨ã€‚å¦‚æœæ³¨é‡Šæ˜¯ç”¨äºå¯¹è±¡æ£€æµ‹çš„ï¼Œåˆ™æ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹é”®çš„å­—å…¸ï¼š'
- en: 'â€œimage_idâ€ (`int`): The image id.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œimage_idâ€ (`int`): å›¾åƒidã€‚'
- en: 'â€œannotationsâ€ (`List[Dict]`): List of annotations for an image. Each annotation
    should be a dictionary. An image can have no annotations, in which case the list
    should be empty. If annotation is for segmentation, the annotations should be
    a dictionary with the following keys:'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œannotationsâ€ (`List[Dict]`): å›¾åƒçš„æ³¨é‡Šåˆ—è¡¨ã€‚æ¯ä¸ªæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ã€‚ä¸€ä¸ªå›¾åƒå¯ä»¥æ²¡æœ‰æ³¨é‡Šï¼Œæ­¤æ—¶åˆ—è¡¨åº”ä¸ºç©ºã€‚å¦‚æœæ³¨é‡Šæ˜¯ç”¨äºåˆ†å‰²çš„ï¼Œæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹é”®çš„å­—å…¸ï¼š'
- en: 'â€œimage_idâ€ (`int`): The image id.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œimage_idâ€ (`int`): å›¾åƒidã€‚'
- en: 'â€œsegments_infoâ€ (`List[Dict]`): List of segments for an image. Each segment
    should be a dictionary. An image can have no segments, in which case the list
    should be empty.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œsegments_infoâ€ (`List[Dict]`): å›¾åƒçš„æ®µåˆ—è¡¨ã€‚æ¯ä¸ªæ®µåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ã€‚ä¸€ä¸ªå›¾åƒå¯ä»¥æ²¡æœ‰æ®µï¼Œæ­¤æ—¶åˆ—è¡¨åº”ä¸ºç©ºã€‚'
- en: 'â€œfile_nameâ€ (`str`): The file name of the image.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œfile_nameâ€ (`str`): å›¾åƒçš„æ–‡ä»¶åã€‚'
- en: '`return_segmentation_masks` (`bool`, *optional*, defaults to self.return_segmentation_masks)
    â€” Whether to return segmentation masks.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_segmentation_masks` (`bool`, *optional*, é»˜è®¤ä¸ºself.return_segmentation_masks)
    â€” æ˜¯å¦è¿”å›åˆ†å‰²è’™ç‰ˆã€‚'
- en: '`masks_path` (`str` or `pathlib.Path`, *optional*) â€” Path to the directory
    containing the segmentation masks.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks_path` (`str` or `pathlib.Path`, *optional*) â€” åŒ…å«åˆ†å‰²è’™ç‰ˆçš„ç›®å½•è·¯å¾„ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to self.do_resize) â€” Whether to resize
    the image.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, é»˜è®¤ä¸ºself.do_resize) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to self.size) â€” Size of the
    image after resizing.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, é»˜è®¤ä¸ºself.size) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to self.resample) â€”
    Resampling filter to use when resizing the image.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, é»˜è®¤ä¸ºself.resample) â€” è°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to self.do_rescale) â€” Whether to
    rescale the image.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, é»˜è®¤ä¸ºself.do_rescale) â€” æ˜¯å¦é‡æ–°ç¼©æ”¾å›¾åƒã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to self.rescale_factor) â€” Rescale
    factor to use when rescaling the image.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, é»˜è®¤ä¸ºself.rescale_factor) â€” è°ƒæ•´å›¾åƒæ—¶ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to self.do_normalize) â€” Whether
    to normalize the image.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, é»˜è®¤ä¸ºself.do_normalize) â€” æ˜¯å¦è§„èŒƒåŒ–å›¾åƒã€‚'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to self.image_mean)
    â€” Mean to use when normalizing the image.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` or `List[float]`, *optional*, é»˜è®¤ä¸ºself.image_mean) â€” åœ¨è§„èŒƒåŒ–å›¾åƒæ—¶ä½¿ç”¨çš„å‡å€¼ã€‚'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to self.image_std)
    â€” Standard deviation to use when normalizing the image.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` or `List[float]`, *optional*, é»˜è®¤ä¸ºself.image_std) â€” åœ¨è§„èŒƒåŒ–å›¾åƒæ—¶ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚'
- en: '`do_pad` (`bool`, *optional*, defaults to self.do_pad) â€” Whether to pad the
    image.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad` (`bool`, *optional*, é»˜è®¤ä¸ºself.do_pad) â€” æ˜¯å¦å¡«å……å›¾åƒã€‚'
- en: '`format` (`str` or `AnnotationFormat`, *optional*, defaults to self.format)
    â€” Format of the annotations.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format` (`str` or `AnnotationFormat`, *optional*, é»˜è®¤ä¸ºself.format) â€” æ³¨é‡Šçš„æ ¼å¼ã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*, defaults to self.return_tensors)
    â€” Type of tensors to return. If `None`, will return the list of images.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` or `TensorType`, *optional*, é»˜è®¤ä¸ºself.return_tensors)
    â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¦‚æœä¸º`None`ï¼Œå°†è¿”å›å›¾åƒåˆ—è¡¨ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` or `str`, *optional*, é»˜è®¤ä¸º`ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: Preprocess an image or a batch of images so that it can be used by the model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥ä½¿ç”¨ã€‚
- en: '#### `post_process_object_detection`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_object_detection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` (`DetrObjectDetectionOutput`) â€” Raw outputs of the model.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` (`DetrObjectDetectionOutput`) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*) â€” Score threshold to keep object detection
    predictions.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *optional*) â€” ä¿ç•™å¯¹è±¡æ£€æµ‹é¢„æµ‹çš„åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`target_sizes` (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*) â€” Tensor
    of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the
    target size `(height, width)` of each image in the batch. If unset, predictions
    will not be resized.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`torch.Tensor`æˆ–`åˆ—è¡¨[å…ƒç»„[int, int]]`, *å¯é€‰*) â€” å½¢çŠ¶ä¸º`(batch_size,
    2)`çš„å¼ é‡æˆ–åŒ…å«æ¯ä¸ªå›¾åƒçš„ç›®æ ‡å¤§å°`(é«˜åº¦ï¼Œå®½åº¦)`çš„å…ƒç»„åˆ—è¡¨(`å…ƒç»„[int, int]`)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Dict]`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`åˆ—è¡¨[å­—å…¸]`'
- en: A list of dictionaries, each dictionary containing the scores, labels and boxes
    for an image in the batch as predicted by the model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹é¢„æµ‹çš„æ‰¹æ¬¡ä¸­å›¾åƒçš„åˆ†æ•°ã€æ ‡ç­¾å’Œæ¡†ã€‚
- en: Converts the raw output of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
    format. Only supports PyTorch.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ€ç»ˆçš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸ºï¼ˆå·¦ä¸Šè§’_xï¼Œå·¦ä¸Šè§’_yï¼Œå³ä¸‹è§’_xï¼Œå³ä¸‹è§’_yï¼‰ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `åå¤„ç†è¯­ä¹‰åˆ†å‰²`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) â€” A list of tuples (`Tuple[int,
    int]`) containing the target size (height, width) of each image in the batch.
    If unset, predictions will not be resized.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`åˆ—è¡¨[å…ƒç»„[int, int]]`, *å¯é€‰*) â€” ä¸€ä¸ªå…ƒç»„åˆ—è¡¨(`å…ƒç»„[int, int]`)ï¼ŒåŒ…å«æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾åƒçš„ç›®æ ‡å¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[torch.Tensor]`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`åˆ—è¡¨[torch.Tensor]`'
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦ä¸º`batch_size`çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(é«˜åº¦ï¼Œå®½åº¦)çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äºç›®æ ‡å¤§å°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†`target_sizes`ï¼‰ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ«idã€‚
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_instance_segmentation`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `åå¤„ç†å®ä¾‹åˆ†å‰²`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) â€” The probability score
    threshold to keep predicted instance masks.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.5) â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©æ¨¡çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) â€” Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.5) â€” å°†é¢„æµ‹çš„æ©æ¨¡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.8) â€” åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°ä¸è¿ç»­éƒ¨åˆ†çš„é‡å æ©æ¨¡åŒºåŸŸé˜ˆå€¼ã€‚'
- en: '`target_sizes` (`List[Tuple]`, *optional*) â€” List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction. If unset, predictions will not be resized.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`åˆ—è¡¨[å…ƒç»„]`, *å¯é€‰*) â€” é•¿åº¦ä¸º(batch_size)çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹(`å…ƒç»„[int, int]]`)å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°(é«˜åº¦ï¼Œå®½åº¦)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: '`return_coco_annotation` (`bool`, *optional*) â€” Defaults to `False`. If set
    to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_coco_annotation` (`bool`, *å¯é€‰*) â€” é»˜è®¤ä¸º`False`ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ä»¥COCOè¿è¡Œé•¿åº¦ç¼–ç ï¼ˆRLEï¼‰æ ¼å¼è¿”å›åˆ†å‰²åœ°å›¾ã€‚'
- en: Returns
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Dict]`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`åˆ—è¡¨[å­—å…¸]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š
- en: '`segmentation` â€” A tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `List[List]` run-length encoding (RLE) of the segmentation map
    if return_coco_annotation is set to `True`. Set to `None` if no mask if found
    above `threshold`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` â€” å½¢çŠ¶ä¸º`(é«˜åº¦ï¼Œå®½åº¦)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨`segment_id`æˆ–`åˆ—è¡¨[åˆ—è¡¨]`çš„è¿è¡Œé•¿åº¦ç¼–ç ï¼ˆRLEï¼‰çš„åˆ†å‰²åœ°å›¾ï¼Œå¦‚æœreturn_coco_annotationè®¾ç½®ä¸º`True`ã€‚å¦‚æœæœªæ‰¾åˆ°é«˜äº`threshold`çš„æ©æ¨¡ï¼Œåˆ™è®¾ç½®ä¸º`None`ã€‚'
- en: '`segments_info` â€” A dictionary that contains additional information on each
    segment.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„é™„åŠ ä¿¡æ¯çš„å­—å…¸ã€‚'
- en: '`id` â€” An integer representing the `segment_id`.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚'
- en: '`label_id` â€” An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«idçš„æ•´æ•°ã€‚'
- en: '`score` â€” Prediction score of segment with `segment_id`.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into instance segmentation predictions. Only supports PyTorch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºå®ä¾‹åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `åå¤„ç†å…¨æ™¯åˆ†å‰²`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” The outputs from [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” æ¥è‡ª[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) â€” The probability score
    threshold to keep predicted instance masks.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.5) â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©æ¨¡çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) â€” Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.5) â€” åœ¨å°†é¢„æµ‹çš„æ©æ¨¡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.8) â€” åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©æ¨¡é¢ç§¯é˜ˆå€¼ã€‚'
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) â€” The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids_to_fuse` (`Set[int]`, *å¯é€‰*) â€” æ­¤çŠ¶æ€ä¸­çš„æ ‡ç­¾å°†ä½¿å…¶æ‰€æœ‰å®ä¾‹è¢«èåˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ä¸€å¼ å›¾åƒä¸­åªèƒ½æœ‰ä¸€ä¸ªå¤©ç©ºï¼Œä½†å¯ä»¥æœ‰å‡ ä¸ªäººï¼Œå› æ­¤å¤©ç©ºçš„æ ‡ç­¾IDå°†åœ¨è¯¥é›†åˆä¸­ï¼Œä½†äººçš„æ ‡ç­¾IDä¸åœ¨å…¶ä¸­ã€‚'
- en: '`target_sizes` (`List[Tuple]`, *optional*) â€” List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If unset, predictions will not be resized.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`, *å¯é€‰*) â€” é•¿åº¦ä¸º(batch_size)çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹(`Tuple[int,
    int]]`)å¯¹åº”äºæ‰¹æ¬¡ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°(é«˜åº¦ã€å®½åº¦)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Dict]`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š
- en: '`segmentation` â€” a tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` â€” å½¢çŠ¶ä¸º`(height, width)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸€ä¸ª`segment_id`ï¼Œå¦‚æœåœ¨`threshold`ä»¥ä¸Šæ‰¾ä¸åˆ°æ©æ¨¡ï¼Œåˆ™è¡¨ç¤ºä¸º`None`ã€‚å¦‚æœæŒ‡å®šäº†`target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„`target_sizes`æ¡ç›®ã€‚'
- en: '`segments_info` â€” A dictionary that contains additional information on each
    segment.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` â€” ä¸€ä¸ªåŒ…å«æ¯ä¸ªæ®µçš„é¢å¤–ä¿¡æ¯çš„å­—å…¸ã€‚'
- en: '`id` â€” an integer representing the `segment_id`.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚'
- en: '`label_id` â€” An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«idçš„æ•´æ•°ã€‚'
- en: '`was_fused` â€” a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ`label_id`åœ¨`label_ids_to_fuse`ä¸­ï¼Œåˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚åŒä¸€ç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ç‹¬çš„`segment_id`ã€‚'
- en: '`score` â€” Prediction score of segment with `segment_id`.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` â€” å¸¦æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into image panoptic segmentation predictions. Only supports PyTorch.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå…¨æ™¯åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: DetrFeatureExtractor
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrFeatureExtractor
- en: '### `class transformers.DetrFeatureExtractor`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/feature_extraction_detr.py#L36)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/feature_extraction_detr.py#L36)'
- en: '[PRE12]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#### `__call__`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Preprocess an image or a batch of images.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: '#### `post_process_object_detection`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_object_detection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` (`DetrObjectDetectionOutput`) â€” Raw outputs of the model.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` (`DetrObjectDetectionOutput`) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*) â€” Score threshold to keep object detection
    predictions.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *å¯é€‰*) â€” ä¿ç•™å¯¹è±¡æ£€æµ‹é¢„æµ‹çš„åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`target_sizes` (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*) â€” Tensor
    of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the
    target size `(height, width)` of each image in the batch. If unset, predictions
    will not be resized.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`torch.Tensor`æˆ–`List[Tuple[int, int]]`, *å¯é€‰*) â€” å½¢çŠ¶ä¸º`(batch_size,
    2)`çš„å¼ é‡æˆ–åŒ…å«æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾åƒçš„ç›®æ ‡å¤§å°`(height, width)`çš„å…ƒç»„åˆ—è¡¨(`Tuple[int, int]`)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Dict]`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: A list of dictionaries, each dictionary containing the scores, labels and boxes
    for an image in the batch as predicted by the model.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹é¢„æµ‹çš„æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾åƒçš„åˆ†æ•°ã€æ ‡ç­¾å’Œæ¡†ã€‚
- en: Converts the raw output of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
    format. Only supports PyTorch.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸º(top_left_x,
    top_left_y, bottom_right_x, bottom_right_y)æ ¼å¼çš„æœ€ç»ˆè¾¹ç•Œæ¡†ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) â€” A list of tuples (`Tuple[int,
    int]`) containing the target size (height, width) of each image in the batch.
    If unset, predictions will not be resized.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆ`List[Tuple[int, int]]`ï¼Œ*å¯é€‰*ï¼‰ - ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼ˆ`Tuple[int, int]`ï¼‰ï¼ŒåŒ…å«æ‰¹å¤„ç†ä¸­æ¯ä¸ªå›¾åƒçš„ç›®æ ‡å¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[torch.Tensor]`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`'
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦ä¸º`batch_size`çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®éƒ½æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº`target_sizes`æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†`target_sizes`ï¼‰ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ«idã€‚
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_instance_segmentation`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_instance_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)ï¼‰
    - æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) â€” The probability score
    threshold to keep predicted instance masks.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰ - ä¿ç•™é¢„æµ‹å®ä¾‹æ©ç çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) â€” Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰ - åœ¨å°†é¢„æµ‹çš„æ©ç è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.8ï¼‰ - åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©ç ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©ç åŒºåŸŸé˜ˆå€¼ã€‚'
- en: '`target_sizes` (`List[Tuple]`, *optional*) â€” List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction. If unset, predictions will not be resized.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆ`List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰ - é•¿åº¦ä¸ºï¼ˆbatch_sizeï¼‰çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ï¼ˆ`Tuple[int,
    int]`ï¼‰å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: '`return_coco_annotation` (`bool`, *optional*) â€” Defaults to `False`. If set
    to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_coco_annotation`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ - é»˜è®¤ä¸º`False`ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ä»¥COCOè¿è¡Œé•¿åº¦ç¼–ç ï¼ˆRLEï¼‰æ ¼å¼è¿”å›åˆ†å‰²åœ°å›¾ã€‚'
- en: Returns
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Dict]`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š
- en: '`segmentation` â€” A tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `List[List]` run-length encoding (RLE) of the segmentation map
    if return_coco_annotation is set to `True`. Set to `None` if no mask if found
    above `threshold`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` - ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤º`segment_id`æˆ–åˆ†å‰²åœ°å›¾çš„`List[List]`è¿è¡Œé•¿åº¦ç¼–ç ï¼ˆRLEï¼‰ï¼Œå¦‚æœ`return_coco_annotation`è®¾ç½®ä¸º`True`ã€‚å¦‚æœæœªæ‰¾åˆ°é«˜äº`threshold`çš„æ©ç ï¼Œåˆ™è®¾ç½®ä¸º`None`ã€‚'
- en: '`segments_info` â€” A dictionary that contains additional information on each
    segment.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` - åŒ…å«æ¯ä¸ªæ®µçš„é™„åŠ ä¿¡æ¯çš„å­—å…¸ã€‚'
- en: '`id` â€” An integer representing the `segment_id`.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` - è¡¨ç¤º`segment_id`çš„æ•´æ•°ã€‚'
- en: '`label_id` â€” An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` - è¡¨ç¤ºä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«idçš„æ•´æ•°ã€‚'
- en: '`score` â€” Prediction score of segment with `segment_id`.'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` - å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into instance segmentation predictions. Only supports PyTorch.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºå®ä¾‹åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_panoptic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    â€” The outputs from [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)ï¼‰
    - æ¥è‡ª[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) â€” The probability score
    threshold to keep predicted instance masks.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰ - ä¿ç•™é¢„æµ‹å®ä¾‹æ©ç çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) â€” Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰ - åœ¨å°†é¢„æµ‹çš„æ©ç è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.8ï¼‰ - åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©ç ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©ç åŒºåŸŸé˜ˆå€¼ã€‚'
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) â€” The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids_to_fuse`ï¼ˆ`Set[int]`ï¼Œ*å¯é€‰*ï¼‰ - æ­¤çŠ¶æ€ä¸­çš„æ ‡ç­¾å°†ä½¿å…¶æ‰€æœ‰å®ä¾‹è¢«èåˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å›¾åƒä¸­åªèƒ½æœ‰ä¸€ä¸ªå¤©ç©ºï¼Œä½†å¯ä»¥æœ‰å‡ ä¸ªäººï¼Œå› æ­¤å¤©ç©ºçš„æ ‡ç­¾IDå°†åœ¨è¯¥é›†åˆä¸­ï¼Œä½†äººçš„æ ‡ç­¾IDä¸åœ¨å…¶ä¸­ã€‚'
- en: '`target_sizes` (`List[Tuple]`, *optional*) â€” List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If unset, predictions will not be resized.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`, *optional*) â€” é•¿åº¦ä¸º`(batch_size)`çš„åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ—è¡¨é¡¹(`Tuple[int,
    int]]`)å¯¹åº”äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°(é«˜åº¦ï¼Œå®½åº¦)ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›å€¼
- en: '`List[Dict]`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®:'
- en: '`segmentation` â€” a tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` â€” å½¢çŠ¶ä¸º`(height, width)`çš„å¼ é‡ï¼Œæ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ª`segment_id`ï¼Œå¦‚æœæ‰¾ä¸åˆ°é®ç½©ï¼Œåˆ™ä¸º`None`ã€‚å¦‚æœæŒ‡å®šäº†`target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„`target_sizes`æ¡ç›®ã€‚'
- en: '`segments_info` â€” A dictionary that contains additional information on each
    segment.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„é™„åŠ ä¿¡æ¯çš„å­—å…¸ã€‚'
- en: '`id` â€” an integer representing the `segment_id`.'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚'
- en: '`label_id` â€” An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«idçš„æ•´æ•°ã€‚'
- en: '`was_fused` â€” a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ`label_id`åœ¨`label_ids_to_fuse`ä¸­ï¼Œåˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚ç›¸åŒç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ä¸€çš„`segment_id`ã€‚'
- en: '`score` â€” Prediction score of segment with `segment_id`.'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into image panoptic segmentation predictions. Only supports PyTorch.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå…¨æ™¯åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: DETR specific outputs
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DETRç‰¹å®šè¾“å‡º
- en: '### `class transformers.models.detr.modeling_detr.DetrModelOutput`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.detr.modeling_detr.DetrModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L94)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L94)'
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ª`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚æ¯å±‚è§£ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ª`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ª`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ª`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚æ¯å±‚ç¼–ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ª`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`intermediate_hidden_states` (`torch.FloatTensor` of shape `(config.decoder_layers,
    batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`)
    â€” Intermediate decoder activations, i.e. the output of each decoder layer, each
    of them gone through a layernorm.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_hidden_states`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(config.decoder_layers,
    batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼Œå½“`config.auxiliary_loss=True`æ—¶è¿”å›ï¼‰â€”
    ä¸­é—´è§£ç å™¨æ¿€æ´»ï¼Œå³æ¯ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œæ¯ä¸ªè¾“å‡ºéƒ½ç»è¿‡äº†layernormã€‚'
- en: Base class for outputs of the DETR encoder-decoder model. This class adds one
    attribute to Seq2SeqModelOutput, namely an optional stack of intermediate decoder
    activations, i.e. the output of each decoder layer, each of them gone through
    a layernorm. This is useful when training the model with auxiliary decoding losses.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: DETRç¼–ç å™¨-è§£ç å™¨æ¨¡å‹è¾“å‡ºçš„åŸºç±»ã€‚è¯¥ç±»åœ¨Seq2SeqModelOutputä¸­æ·»åŠ äº†ä¸€ä¸ªå±æ€§ï¼Œå³ä¸€ä¸ªå¯é€‰çš„ä¸­é—´è§£ç å™¨æ¿€æ´»å †æ ˆï¼Œå³æ¯ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œæ¯ä¸ªè¾“å‡ºéƒ½ç»è¿‡äº†layernormã€‚åœ¨ä½¿ç”¨è¾…åŠ©è§£ç æŸå¤±è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¿™æ˜¯æœ‰ç”¨çš„ã€‚
- en: '### `class transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L134)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L134)'
- en: '[PRE19]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) â€” Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” æ€»æŸå¤±ï¼Œä½œä¸ºç±»é¢„æµ‹çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾¹ç•Œæ¡†æŸå¤±çš„çº¿æ€§ç»„åˆã€‚åè€…å®šä¹‰ä¸ºL1æŸå¤±å’Œå¹¿ä¹‰æ¯”ä¾‹ä¸å˜IoUæŸå¤±çš„çº¿æ€§ç»„åˆã€‚'
- en: '`loss_dict` (`Dict`, *optional*) â€” A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict`ï¼ˆ`Dict`ï¼Œ*å¯é€‰*ï¼‰â€” åŒ…å«å„ä¸ªæŸå¤±çš„å­—å…¸ã€‚ç”¨äºè®°å½•æ—¥å¿—ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) â€” Classification logits (including no-object) for all queries.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes + 1)`çš„`torch.FloatTensor`ï¼‰â€”
    æ‰€æœ‰æŸ¥è¯¢çš„åˆ†ç±»logitsï¼ˆåŒ…æ‹¬æ— å¯¹è±¡ï¼‰ã€‚'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    â€” Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, 4)`çš„`torch.FloatTensor`ï¼‰â€” æ‰€æœ‰æŸ¥è¯¢çš„å½’ä¸€åŒ–æ¡†åæ ‡ï¼Œè¡¨ç¤ºä¸ºï¼ˆä¸­å¿ƒ_xï¼Œä¸­å¿ƒ_yï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰ã€‚è¿™äº›å€¼åœ¨[0,
    1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œç›¸å¯¹äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªå•ç‹¬å›¾åƒçš„å¤§å°ï¼ˆå¿½ç•¥å¯èƒ½çš„å¡«å……ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)æ¥æ£€ç´¢æœªå½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” Optional, only returned when
    auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs`ï¼ˆ`list[Dict]`ï¼Œ*å¯é€‰*ï¼‰â€” ä»…åœ¨æ¿€æ´»è¾…åŠ©æŸå¤±ï¼ˆå³`config.auxiliary_loss`è®¾ç½®ä¸º`True`ï¼‰å¹¶æä¾›æ ‡ç­¾æ—¶è¿”å›ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªä¸Šè¿°é”®ï¼ˆ`logits`å’Œ`pred_boxes`ï¼‰çš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸å¯¹åº”ä¸€ä¸ªè§£ç å™¨å±‚ã€‚'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€”
    æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¯ä¸ªå±‚çš„è§£ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: Output type of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)çš„è¾“å‡ºç±»å‹ã€‚'
- en: '### `class transformers.models.detr.modeling_detr.DetrSegmentationOutput`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.detr.modeling_detr.DetrSegmentationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L197)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L197)'
- en: '[PRE20]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) â€” Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€”
    ä½œä¸ºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾¹ç•Œæ¡†æŸå¤±çš„çº¿æ€§ç»„åˆçš„æ€»æŸå¤±ã€‚åè€…å®šä¹‰ä¸ºL1æŸå¤±å’Œå¹¿ä¹‰å°ºåº¦ä¸å˜IoUæŸå¤±çš„çº¿æ€§ç»„åˆã€‚'
- en: '`loss_dict` (`Dict`, *optional*) â€” A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict` (`Dict`, *optional*) â€” åŒ…å«å„ä¸ªæŸå¤±çš„å­—å…¸ã€‚ç”¨äºè®°å½•æ—¥å¿—ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) â€” Classification logits (including no-object) for all queries.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†ç±»logitsï¼ˆåŒ…æ‹¬æ— å¯¹è±¡ï¼‰ã€‚'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    â€” Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    â€” æ‰€æœ‰æŸ¥è¯¢çš„å½’ä¸€åŒ–æ¡†åæ ‡ï¼Œè¡¨ç¤ºä¸ºï¼ˆä¸­å¿ƒ_xï¼Œä¸­å¿ƒ_yï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰ã€‚è¿™äº›å€¼åœ¨[0, 1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œç›¸å¯¹äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªå•ç‹¬å›¾åƒçš„å¤§å°ï¼ˆå¿½ç•¥å¯èƒ½çš„å¡«å……ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)æ¥æ£€ç´¢æœªå½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4,
    width/4)`) â€” Segmentation masks logits for all queries. See also [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)
    [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)
    to evaluate semantic, instance and panoptic segmentation masks respectively.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4,
    width/4)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†å‰²æ©æ¨¡logitsã€‚å¦è¯·å‚é˜…[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)æˆ–[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)åˆ†åˆ«è¯„ä¼°è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²æ©æ¨¡ã€‚'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” Optional, only returned when
    auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” å¯é€‰ï¼Œä»…åœ¨æ¿€æ´»è¾…åŠ©æŸå¤±ï¼ˆå³`config.auxiliary_loss`è®¾ç½®ä¸º`True`ï¼‰å¹¶æä¾›æ ‡ç­¾æ—¶è¿”å›ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªè§£ç å™¨å±‚çš„ä¸Šè¿°ä¸¤ä¸ªé”®ï¼ˆ`logits`å’Œ`pred_boxes`ï¼‰çš„å­—å…¸åˆ—è¡¨ã€‚'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: Output type of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„è¾“å‡ºç±»å‹ã€‚'
- en: DetrModel
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrModel
- en: '### `class transformers.DetrModel`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1296)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1296)'
- en: '[PRE21]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare DETR Model (consisting of a backbone and encoder-decoder Transformer)
    outputting raw hidden-states without any specific head on top.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„DETRæ¨¡å‹ï¼ˆç”±éª¨å¹²å’Œç¼–ç å™¨-è§£ç å™¨Transformerç»„æˆï¼‰ï¼Œè¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1337)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1337)'
- en: '[PRE22]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Padding will be ignored by default should you provide
    it.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚'
- en: Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„é®ç½©ã€‚é®ç½©å€¼é€‰æ‹©åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸå®çš„åƒç´ ï¼ˆå³`æœªè¢«é®ç½©`ï¼‰ï¼Œ
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¡«å……åƒç´ ï¼ˆå³`è¢«é®ç½©`ï¼‰ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›é®ç½©ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) â€” Not used by default. Can be used to mask object queries.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    é»˜è®¤æƒ…å†µä¸‹ä¸ä½¿ç”¨ã€‚å¯ç”¨äºå±è”½å¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing the flattened feature
    map (output of the backbone + projection layer), you can choose to directly pass
    a flattened representation of an image.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’ä¸€ä¸ªå›¾åƒçš„æ‰å¹³è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’æ‰å¹³ç‰¹å¾å›¾ï¼ˆéª¨å¹²ç½‘ç»œè¾“å‡º+æŠ•å½±å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) â€” Optionally, instead of initializing the queries with
    a tensor of zeros, you can choose to directly pass an embedded representation.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’ä¸€ä¸ªåµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç”¨é›¶å¼ é‡åˆå§‹åŒ–æŸ¥è¯¢ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    and inputs.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€”
    æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—çš„è¾“å‡ºã€‚'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å…ƒç»„åŒ…æ‹¬ï¼ˆæ¯ä¸ªå±‚çš„åµŒå…¥è¾“å‡º+æ¯ä¸ªå±‚çš„è¾“å‡ºçš„`torch.FloatTensor`ï¼‰å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å…ƒç»„åŒ…æ‹¬ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`ã€‚è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”ç¼–ç å™¨çš„éšè—çŠ¶æ€å…ƒç»„ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚æ¯å±‚çš„ç¼–ç å™¨éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡å…ƒç»„ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_heads, sequence_length, sequence_length)`ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚ç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼çš„ç¼–ç å™¨çš„æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ã€‚'
- en: '`intermediate_hidden_states` (`torch.FloatTensor` of shape `(config.decoder_layers,
    batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`)
    â€” Intermediate decoder activations, i.e. the output of each decoder layer, each
    of them gone through a layernorm.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(config.decoder_layers, batch_size, sequence_length,
    hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“`config.auxiliary_loss=True`æ—¶è¿”å›ï¼‰â€”ä¸­é—´è§£ç å™¨æ¿€æ´»ï¼Œå³æ¯ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œæ¯ä¸ªéƒ½ç»è¿‡ä¸€ä¸ªlayernormã€‚'
- en: The [DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)
    forward method, overrides the `__call__` special method.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE23]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: DetrForObjectDetection
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrForObjectDetection
- en: '### `class transformers.DetrForObjectDetection`'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrForObjectDetection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1464)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1464)'
- en: '[PRE24]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)ï¼‰â€”æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: DETR Model (consisting of a backbone and encoder-decoder Transformer) with object
    detection heads on top, for tasks such as COCO detection.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: DETRæ¨¡å‹ï¼ˆç”±éª¨å¹²å’Œç¼–ç å™¨-è§£ç å™¨Transformerç»„æˆï¼‰ï¼Œé¡¶éƒ¨å¸¦æœ‰ç›®æ ‡æ£€æµ‹å¤´ï¼Œç”¨äºè¯¸å¦‚COCOæ£€æµ‹ä¹‹ç±»çš„ä»»åŠ¡ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1497)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1497)'
- en: '[PRE25]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Padding will be ignored by default should you provide
    it.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚'
- en: Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0,
    1]`ä¹‹é—´ï¼š'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºçœŸå®åƒç´ ï¼ˆå³`æœªå±è”½`ï¼‰ï¼Œ
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¡«å……åƒç´ ï¼ˆå³`å±è”½`ï¼‰ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) â€” Not used by default. Can be used to mask object queries.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) â€” é»˜è®¤æƒ…å†µä¸‹ä¸ä½¿ç”¨ã€‚å¯ç”¨äºå±è”½å¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„ç”±(`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`)ç»„æˆï¼Œ`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing the flattened feature
    map (output of the backbone + projection layer), you can choose to directly pass
    a flattened representation of an image.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’å›¾åƒçš„æ‰å¹³åŒ–ç‰¹å¾å›¾ï¼ˆéª¨å¹²ç½‘ç»œ+æŠ•å½±å±‚çš„è¾“å‡ºï¼‰ï¼Œè€Œä¸æ˜¯ä¼ é€’æ‰å¹³åŒ–è¡¨ç¤ºã€‚'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) â€” Optionally, instead of initializing the queries with
    a tensor of zeros, you can choose to directly pass an embedded representation.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç”¨é›¶å¼ é‡åˆå§‹åŒ–æŸ¥è¯¢ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`List[Dict]` of len `(batch_size,)`, *optional*) â€” Labels for computing
    the bipartite matching loss. List of dicts, each dictionary containing at least
    the following 2 keys: â€˜class_labelsâ€™ and â€˜boxesâ€™ (the class labels and bounding
    boxes of an image in the batch respectively). The class labels themselves should
    be a `torch.LongTensor` of len `(number of bounding boxes in the image,)` and
    the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image,
    4)`.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`List[Dict]` of len `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—äºŒéƒ¨åŒ¹é…æŸå¤±çš„æ ‡ç­¾ã€‚å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸è‡³å°‘åŒ…å«ä»¥ä¸‹2ä¸ªé”®ï¼š''class_labels''å’Œ''boxes''ï¼ˆåˆ†åˆ«æ˜¯æ‰¹å¤„ç†ä¸­å›¾åƒçš„ç±»åˆ«æ ‡ç­¾å’Œè¾¹ç•Œæ¡†ï¼‰ã€‚ç±»åˆ«æ ‡ç­¾æœ¬èº«åº”è¯¥æ˜¯é•¿åº¦ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†æ•°é‡,)`çš„`torch.LongTensor`ï¼Œè€Œè¾¹ç•Œæ¡†åº”è¯¥æ˜¯å½¢çŠ¶ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†æ•°é‡,
    4)`çš„`torch.FloatTensor`ã€‚'
- en: Returns
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    and inputs.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆ[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)ï¼‰å’Œè¾“å…¥ã€‚'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) â€” Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) â€” ä½œä¸ºç±»åˆ«é¢„æµ‹çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾¹ç•Œæ¡†æŸå¤±çš„çº¿æ€§ç»„åˆçš„æ€»æŸå¤±ã€‚åè€…è¢«å®šä¹‰ä¸ºL1æŸå¤±å’Œå¹¿ä¹‰å°ºåº¦ä¸å˜IoUæŸå¤±çš„çº¿æ€§ç»„åˆã€‚'
- en: '`loss_dict` (`Dict`, *optional*) â€” A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict` (`Dict`, *optional*) â€” åŒ…å«å„ä¸ªæŸå¤±çš„å­—å…¸ã€‚ç”¨äºè®°å½•æ—¥å¿—ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) â€” Classification logits (including no-object) for all queries.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†ç±»logitsï¼ˆåŒ…æ‹¬æ— å¯¹è±¡ï¼‰ã€‚'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    â€” Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    â€” æ‰€æœ‰æŸ¥è¯¢çš„å½’ä¸€åŒ–æ¡†åæ ‡ï¼Œè¡¨ç¤ºä¸ºï¼ˆä¸­å¿ƒ_xï¼Œä¸­å¿ƒ_yï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰ã€‚è¿™äº›å€¼åœ¨[0, 1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œç›¸å¯¹äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªå•ç‹¬å›¾åƒçš„å¤§å°ï¼ˆå¿½ç•¥å¯èƒ½çš„å¡«å……ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)æ¥æ£€ç´¢æœªå½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” Optional, only returned when
    auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” å¯é€‰ï¼Œä»…åœ¨æ¿€æ´»è¾…åŠ©æŸå¤±ï¼ˆå³`config.auxiliary_loss`è®¾ç½®ä¸º`True`ï¼‰å¹¶æä¾›æ ‡ç­¾æ—¶è¿”å›ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªè§£ç å™¨å±‚çš„ä¸Šè¿°ä¸¤ä¸ªé”®ï¼ˆ`logits`å’Œ`pred_boxes`ï¼‰çš„å­—å…¸åˆ—è¡¨ã€‚'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” è§£ç å™¨çš„éšè—çŠ¶æ€å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ªï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚æ¯å±‚è§£ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ªï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ªï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” ç¼–ç å™¨çš„éšè—çŠ¶æ€å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ªï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚æ¯å±‚ç¼–ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡å…ƒç»„ï¼Œæ¯å±‚ä¸€ä¸ªï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: The [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    forward method, overrides the `__call__` special method.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE26]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: DetrForSegmentation
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrForSegmentation
- en: '### `class transformers.DetrForSegmentation`'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrForSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1637)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1637)'
- en: '[PRE27]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: DETR Model (consisting of a backbone and encoder-decoder Transformer) with a
    segmentation head on top, for tasks such as COCO panoptic.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: DETRæ¨¡å‹ï¼ˆç”±éª¨å¹²å’Œç¼–ç å™¨-è§£ç å™¨Transformerç»„æˆï¼‰ï¼Œé¡¶éƒ¨å¸¦æœ‰åˆ†å‰²å¤´ï¼Œç”¨äºè¯¸å¦‚COCOå…¨æ™¯ç­‰ä»»åŠ¡ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1667)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1667)'
- en: '[PRE28]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Padding will be ignored by default should you provide
    it.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚'
- en: Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸå®çš„åƒç´ ï¼ˆå³`æœªè¢«é®ç½©`ï¼‰ï¼Œ
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¡«å……åƒç´ ï¼ˆå³`é®ç½©`ï¼‰çš„åƒç´ ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›é®ç½©ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) â€” Not used by default. Can be used to mask object queries.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    é»˜è®¤æƒ…å†µä¸‹ä¸ä½¿ç”¨ã€‚å¯ç”¨äºå±è”½å¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*å¯é€‰*æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing the flattened feature
    map (output of the backbone + projection layer), you can choose to directly pass
    a flattened representation of an image.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’å›¾åƒçš„æ‰å¹³ç‰¹å¾å›¾ï¼ˆéª¨å¹²+æŠ•å½±å±‚çš„è¾“å‡ºï¼‰è€Œä¸æ˜¯ä¼ é€’å®ƒã€‚'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) â€” Optionally, instead of initializing the queries with
    a tensor of zeros, you can choose to directly pass an embedded representation.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºæ¥åˆå§‹åŒ–æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é›¶å¼ é‡ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`List[Dict]` of len `(batch_size,)`, *optional*) â€” Labels for computing
    the bipartite matching loss, DICE/F-1 loss and Focal loss. List of dicts, each
    dictionary containing at least the following 3 keys: â€˜class_labelsâ€™, â€˜boxesâ€™ and
    â€˜masksâ€™ (the class labels, bounding boxes and segmentation masks of an image in
    the batch respectively). The class labels themselves should be a `torch.LongTensor`
    of len `(number of bounding boxes in the image,)`, the boxes a `torch.FloatTensor`
    of shape `(number of bounding boxes in the image, 4)` and the masks a `torch.FloatTensor`
    of shape `(number of bounding boxes in the image, height, width)`.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆé•¿åº¦ä¸º`(batch_size,)`çš„`List[Dict]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—äºŒéƒ¨åŒ¹é…æŸå¤±ã€DICE/F-1æŸå¤±å’ŒFocalæŸå¤±çš„æ ‡ç­¾ã€‚å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸è‡³å°‘åŒ…å«ä»¥ä¸‹3ä¸ªé”®ï¼šâ€˜class_labelsâ€™ã€â€˜boxesâ€™å’Œâ€˜masksâ€™ï¼ˆåˆ†åˆ«æ˜¯æ‰¹æ¬¡ä¸­å›¾åƒçš„ç±»æ ‡ç­¾ã€è¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç ï¼‰ã€‚ç±»æ ‡ç­¾æœ¬èº«åº”è¯¥æ˜¯é•¿åº¦ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†çš„æ•°é‡,)`çš„`torch.LongTensor`ï¼Œè¾¹ç•Œæ¡†æ˜¯å½¢çŠ¶ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†çš„æ•°é‡,
    4)`çš„`torch.FloatTensor`ï¼Œæ©ç æ˜¯å½¢çŠ¶ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†çš„æ•°é‡, height, width)`çš„`torch.FloatTensor`ã€‚'
- en: Returns
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    and inputs.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) â€” Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*optional*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” æ€»æŸå¤±ï¼Œä½œä¸ºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾¹ç•Œæ¡†æŸå¤±çš„çº¿æ€§ç»„åˆã€‚åè€…è¢«å®šä¹‰ä¸ºL1æŸå¤±å’Œå¹¿ä¹‰å°ºåº¦ä¸å˜IoUæŸå¤±çš„çº¿æ€§ç»„åˆã€‚'
- en: '`loss_dict` (`Dict`, *optional*) â€” A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict` (`Dict`ï¼Œ*optional*) â€” åŒ…å«å„ä¸ªæŸå¤±çš„å­—å…¸ã€‚ç”¨äºè®°å½•ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) â€” Classification logits (including no-object) for all queries.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes + 1)`ï¼‰
    â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†ç±»logitsï¼ˆåŒ…æ‹¬æ— å¯¹è±¡ï¼‰ã€‚'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    â€” Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, 4)`ï¼‰ â€” æ‰€æœ‰æŸ¥è¯¢çš„å½’ä¸€åŒ–æ¡†åæ ‡ï¼Œè¡¨ç¤ºä¸ºï¼ˆä¸­å¿ƒ_xï¼Œä¸­å¿ƒ_yï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰ã€‚è¿™äº›å€¼åœ¨[0,
    1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œç›¸å¯¹äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªå•ç‹¬å›¾åƒçš„å¤§å°ï¼ˆå¿½ç•¥å¯èƒ½çš„å¡«å……ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)æ¥æ£€ç´¢æœªå½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4,
    width/4)`) â€” Segmentation masks logits for all queries. See also [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)
    [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)
    to evaluate semantic, instance and panoptic segmentation masks respectively.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_masks` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, height/4, width/4)`ï¼‰
    â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†å‰²æ©æ¨¡logitsã€‚å¦è¯·å‚é˜…[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)æˆ–[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)åˆ†åˆ«è¯„ä¼°è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²æ©æ¨¡ã€‚'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” Optional, only returned when
    auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs` (`list[Dict]`, *optional*) â€” å½“è¾…åŠ©æŸå¤±è¢«æ¿€æ´»æ—¶ï¼ˆå³`config.auxiliary_loss`è®¾ç½®ä¸º`True`ï¼‰å¹¶ä¸”æä¾›äº†æ ‡ç­¾æ—¶æ‰è¿”å›ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªè§£ç å™¨å±‚çš„ä¸¤ä¸ªä¸Šè¿°é”®ï¼ˆ`logits`å’Œ`pred_boxes`ï¼‰çš„å­—å…¸åˆ—è¡¨ã€‚'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚è§£ç å™¨åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚ç¼–ç å™¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚'
- en: The [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE29]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
