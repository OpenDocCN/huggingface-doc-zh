- en: DETR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DETR
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)
    by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
    Kirillov and Sergey Zagoruyko. DETR consists of a convolutional backbone followed
    by an encoder-decoder Transformer which can be trained end-to-end for object detection.
    It greatly simplifies a lot of the complexity of models like Faster-R-CNN and
    Mask-R-CNN, which use things like region proposals, non-maximum suppression procedure
    and anchor generation. Moreover, DETR can also be naturally extended to perform
    panoptic segmentation, by simply adding a mask head on top of the decoder outputs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DETR模型是由Nicolas Carion、Francisco Massa、Gabriel Synnaeve、Nicolas Usunier、Alexander
    Kirillov和Sergey Zagoruyko在[使用变压器进行端到端目标检测](https://arxiv.org/abs/2005.12872)中提出的。DETR由一个卷积主干后面跟着一个编码器-解码器变压器组成，可以进行端到端的目标检测训练。它极大地简化了像Faster-R-CNN和Mask-R-CNN这样的模型的复杂性，这些模型使用区域提议、非极大值抑制程序和锚点生成等技术。此外，DETR还可以自然地扩展到执行全景分割，只需在解码器输出之上添加一个蒙版头。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We present a new method that views object detection as a direct set prediction
    problem. Our approach streamlines the detection pipeline, effectively removing
    the need for many hand-designed components like a non-maximum suppression procedure
    or anchor generation that explicitly encode our prior knowledge about the task.
    The main ingredients of the new framework, called DEtection TRansformer or DETR,
    are a set-based global loss that forces unique predictions via bipartite matching,
    and a transformer encoder-decoder architecture. Given a fixed small set of learned
    object queries, DETR reasons about the relations of the objects and the global
    image context to directly output the final set of predictions in parallel. The
    new model is conceptually simple and does not require a specialized library, unlike
    many other modern detectors. DETR demonstrates accuracy and run-time performance
    on par with the well-established and highly-optimized Faster RCNN baseline on
    the challenging COCO object detection dataset. Moreover, DETR can be easily generalized
    to produce panoptic segmentation in a unified manner. We show that it significantly
    outperforms competitive baselines.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们提出了一种将目标检测视为直接集合预测问题的新方法。我们的方法简化了检测流程，有效地消除了许多手工设计的组件，如非极大值抑制程序或明确编码我们对任务的先验知识的锚点生成。新框架DEtection
    TRansformer或DETR的主要组成部分是通过二部匹配强制唯一预测的基于集合的全局损失，以及一个变压器编码器-解码器架构。给定一组固定的学习目标查询，DETR推理对象之间的关系和全局图像上下文，直接并行输出最终的预测集。这个新模型在概念上简单，不需要专门的库，不像许多其他现代检测器。DETR在具有挑战性的COCO目标检测数据集上表现出与经过充分优化的Faster
    RCNN基线相当的准确性和运行时性能。此外，DETR可以轻松推广为以统一方式产生全景分割。我们展示它明显优于竞争基线。*'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/facebookresearch/detr).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。原始代码可以在[这里](https://github.com/facebookresearch/detr)找到。
- en: How DETR works
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DETR的工作原理
- en: 'Here’s a TLDR explaining how [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    works:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解释[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)工作原理的TLDR：
- en: First, an image is sent through a pre-trained convolutional backbone (in the
    paper, the authors use ResNet-50/ResNet-101). Let’s assume we also add a batch
    dimension. This means that the input to the backbone is a tensor of shape `(batch_size,
    3, height, width)`, assuming the image has 3 color channels (RGB). The CNN backbone
    outputs a new lower-resolution feature map, typically of shape `(batch_size, 2048,
    height/32, width/32)`. This is then projected to match the hidden dimension of
    the Transformer of DETR, which is `256` by default, using a `nn.Conv2D` layer.
    So now, we have a tensor of shape `(batch_size, 256, height/32, width/32).` Next,
    the feature map is flattened and transposed to obtain a tensor of shape `(batch_size,
    seq_len, d_model)` = `(batch_size, width/32*height/32, 256)`. So a difference
    with NLP models is that the sequence length is actually longer than usual, but
    with a smaller `d_model` (which in NLP is typically 768 or higher).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将图像通过预训练的卷积主干（在论文中，作者使用ResNet-50/ResNet-101）。假设我们也添加了一个批处理维度。这意味着主干的输入是一个形状为`(batch_size,
    3, height, width)`的张量，假设图像有3个颜色通道（RGB）。CNN主干输出一个新的低分辨率特征图，通常形状为`(batch_size, 2048,
    height/32, width/32)`。然后，将其投影到DETR变压器的隐藏维度，该维度默认为`256`，使用`nn.Conv2D`层。现在，我们有一个形状为`(batch_size,
    256, height/32, width/32)`的张量。接下来，特征图被展平并转置，以获得形状为`(batch_size, seq_len, d_model)`
    = `(batch_size, width/32*height/32, 256)`的张量。因此，与NLP模型的一个区别是，序列长度实际上比通常更长，但`d_model`较小（在NLP中通常为768或更高）。
- en: 'Next, this is sent through the encoder, outputting `encoder_hidden_states`
    of the same shape (you can consider these as image features). Next, so-called
    **object queries** are sent through the decoder. This is a tensor of shape `(batch_size,
    num_queries, d_model)`, with `num_queries` typically set to 100 and initialized
    with zeros. These input embeddings are learnt positional encodings that the authors
    refer to as object queries, and similarly to the encoder, they are added to the
    input of each attention layer. Each object query will look for a particular object
    in the image. The decoder updates these embeddings through multiple self-attention
    and encoder-decoder attention layers to output `decoder_hidden_states` of the
    same shape: `(batch_size, num_queries, d_model)`. Next, two heads are added on
    top for object detection: a linear layer for classifying each object query into
    one of the objects or “no object”, and a MLP to predict bounding boxes for each
    query.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这通过编码器发送，输出相同形状的`encoder_hidden_states`（您可以将这些视为图像特征）。接下来，所谓的**对象查询**通过解码器发送。这是一个形状为`(batch_size,
    num_queries, d_model)`的张量，其中`num_queries`通常设置为100，并用零初始化。这些输入嵌入是学习的位置编码，作者将其称为对象查询，类似于编码器，它们被添加到每个注意力层的输入中。每个对象查询将在图像中寻找特定对象。解码器通过多个自注意力和编码器-解码器注意力层更新这些嵌入，以输出相同形状的`decoder_hidden_states`：`(batch_size,
    num_queries, d_model)`。接下来，顶部添加了两个头用于对象检测：一个线性层用于将每个对象查询分类为对象或“无对象”之一，以及一个MLP用于预测每个查询的边界框。
- en: 'The model is trained using a **bipartite matching loss**: so what we actually
    do is compare the predicted classes + bounding boxes of each of the N = 100 object
    queries to the ground truth annotations, padded up to the same length N (so if
    an image only contains 4 objects, 96 annotations will just have a “no object”
    as class and “no bounding box” as bounding box). The [Hungarian matching algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm)
    is used to find an optimal one-to-one mapping of each of the N queries to each
    of the N annotations. Next, standard cross-entropy (for the classes) and a linear
    combination of the L1 and [generalized IoU loss](https://giou.stanford.edu/) (for
    the bounding boxes) are used to optimize the parameters of the model.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用**二部匹配损失**进行训练：实际上我们比较每个N = 100个对象查询的预测类别+边界框与地面真实注释，填充到相同长度N（因此如果图像仅包含4个对象，则96个注释将只有一个“无对象”作为类别和一个“无边界框”作为边界框）。使用[匈牙利匹配算法](https://en.wikipedia.org/wiki/Hungarian_algorithm)找到每个N查询与每个N注释的最佳一对一映射。接下来，使用标准交叉熵（用于类别）和L1的线性组合以及[广义IoU损失](https://giou.stanford.edu/)（用于边界框）来优化模型的参数。
- en: DETR can be naturally extended to perform panoptic segmentation (which unifies
    semantic segmentation and instance segmentation). [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    adds a segmentation mask head on top of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection).
    The mask head can be trained either jointly, or in a two steps process, where
    one first trains a [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    model to detect bounding boxes around both “things” (instances) and “stuff” (background
    things like trees, roads, sky), then freeze all the weights and train only the
    mask head for 25 epochs. Experimentally, these two approaches give similar results.
    Note that predicting boxes is required for the training to be possible, since
    the Hungarian matching is computed using distances between boxes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DETR可以自然扩展以执行全景分割（将语义分割和实例分割统一起来）。[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)在[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)的顶部添加了一个分割掩码头。掩码头可以同时训练，或者在两个步骤的过程中训练，首先训练一个[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)模型来检测“事物”（实例）和“物品”（背景物品，如树木、道路、天空）周围的边界框，然后冻结所有权重，仅训练掩码头25个时代。实验上，这两种方法给出了类似的结果。请注意，为了使训练成为可能，预测框是必需的，因为匈牙利匹配是使用框之间的距离计算的。
- en: Usage tips
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: DETR uses so-called **object queries** to detect objects in an image. The number
    of queries determines the maximum number of objects that can be detected in a
    single image, and is set to 100 by default (see parameter `num_queries` of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)).
    Note that it’s good to have some slack (in COCO, the authors used 100, while the
    maximum number of objects in a COCO image is ~70).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DETR使用所谓的**对象查询**来检测图像中的对象。查询的数量确定了单个图像中可以检测到的对象的最大数量，默认设置为100（请参阅[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)的参数`num_queries`）。请注意，最好有一些余地（在COCO中，作者使用了100，而COCO图像中的最大对象数量约为70）。
- en: The decoder of DETR updates the query embeddings in parallel. This is different
    from language models like GPT-2, which use autoregressive decoding instead of
    parallel. Hence, no causal attention mask is used.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DETR的解码器并行更新查询嵌入。这与像GPT-2这样使用自回归解码而不是并行的语言模型不同。因此，不使用因果关注掩码。
- en: DETR adds position embeddings to the hidden states at each self-attention and
    cross-attention layer before projecting to queries and keys. For the position
    embeddings of the image, one can choose between fixed sinusoidal or learned absolute
    position embeddings. By default, the parameter `position_embedding_type` of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    is set to `"sine"`.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将隐藏状态投影到查询和键之前，DETR在每个自注意力和交叉注意力层中添加位置嵌入。对于图像的位置嵌入，可以在固定正弦或学习的绝对位置嵌入之间进行选择。默认情况下，[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)的参数`position_embedding_type`设置为`"sine"`。
- en: During training, the authors of DETR did find it helpful to use auxiliary losses
    in the decoder, especially to help the model output the correct number of objects
    of each class. If you set the parameter `auxiliary_loss` of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    to `True`, then prediction feedforward neural networks and Hungarian losses are
    added after each decoder layer (with the FFNs sharing parameters).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间，DETR的作者确实发现在解码器中使用辅助损失是有帮助的，特别是为了帮助模型输出每个类别的正确对象数量。如果将[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)的参数`auxiliary_loss`设置为`True`，则在每个解码器层之后添加预测前馈神经网络和匈牙利损失（FFN共享参数）。
- en: If you want to train the model in a distributed environment across multiple
    nodes, then one should update the *num_boxes* variable in the *DetrLoss* class
    of *modeling_detr.py*. When training on multiple nodes, this should be set to
    the average number of target boxes across all nodes, as can be seen in the original
    implementation [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想在跨多个节点的分布式环境中训练模型，则应该在*modeling_detr.py*中的*DetrLoss*类中更新*num_boxes*变量。在多个节点上训练时，应将其设置为所有节点上目标框的平均数量，可以在原始实现中看到[这里](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232)。
- en: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    and [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    can be initialized with any convolutional backbone available in the [timm library](https://github.com/rwightman/pytorch-image-models).
    Initializing with a MobileNet backbone for example can be done by setting the
    `backbone` attribute of [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    to `"tf_mobilenetv3_small_075"`, and then initializing the model with that config.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)和[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)可以使用[timm库](https://github.com/rwightman/pytorch-image-models)中可用的任何卷积骨干进行初始化。例如，可以通过将[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)的`backbone`属性设置为`"tf_mobilenetv3_small_075"`，然后使用该配置初始化模型来使用MobileNet骨干。'
- en: DETR resizes the input images such that the shortest side is at least a certain
    amount of pixels while the longest is at most 1333 pixels. At training time, scale
    augmentation is used such that the shortest side is randomly set to at least 480
    and at most 800 pixels. At inference time, the shortest side is set to 800\. One
    can use [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    to prepare images (and optional annotations in COCO format) for the model. Due
    to this resizing, images in a batch can have different sizes. DETR solves this
    by padding images up to the largest size in a batch, and by creating a pixel mask
    that indicates which pixels are real/which are padding. Alternatively, one can
    also define a custom `collate_fn` in order to batch images together, using `~transformers.DetrImageProcessor.pad_and_create_pixel_mask`.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DETR调整输入图像的大小，使最短边至少为一定数量的像素，而最长边至多为1333像素。在训练时，使用尺度增强，使最短边随机设置为至少480像素，最多800像素。在推断时，最短边设置为800。可以使用[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)为模型准备图像（以及可选的以COCO格式的注释）。由于这种调整大小，批处理中的图像可能具有不同的大小。DETR通过将图像填充到批处理中的最大大小，并创建一个像素掩码来指示哪些像素是真实的/哪些是填充来解决这个问题。另外，也可以定义一个自定义的`collate_fn`来批处理图像，使用`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`。
- en: The size of the images will determine the amount of memory being used, and will
    thus determine the `batch_size`. It is advised to use a batch size of 2 per GPU.
    See [this Github thread](https://github.com/facebookresearch/detr/issues/150)
    for more info.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的大小将决定所使用的内存量，从而确定`batch_size`。建议每个GPU使用批量大小为2。有关更多信息，请参阅[此Github线程](https://github.com/facebookresearch/detr/issues/150)。
- en: 'There are three ways to instantiate a DETR model (depending on what you prefer):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种实例化DETR模型的方法（取决于您的偏好）：
- en: 'Option 1: Instantiate DETR with pre-trained weights for entire model'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 选项1：使用整个模型的预训练权重实例化DETR
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Option 2: Instantiate DETR with randomly initialized weights for Transformer,
    but pre-trained weights for backbone'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 选项2：使用随机初始化的Transformer权重实例化DETR，但使用骨干的预训练权重
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Option 3: Instantiate DETR with randomly initialized weights for backbone +
    Transformer'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 选项3：使用随机初始化的骨干+Transformer实例化DETR
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a summary, consider the following table:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，请参考以下表格：
- en: '| Task | Object detection | Instance segmentation | Panoptic segmentation |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 目标检测 | 实例分割 | 全景分割 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Description** | Predicting bounding boxes and class labels around objects
    in an image | Predicting masks around objects (i.e. instances) in an image | Predicting
    masks around both objects (i.e. instances) as well as “stuff” (i.e. background
    things like trees and roads) in an image |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **描述** | 预测图像中物体周围的边界框和类标签 | 预测图像中物体（即实例）周围的掩模 | 预测图像中物体（即实例）以及“物质”（即背景物品如树木和道路）周围的掩模
    |'
- en: '| **Model** | [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    | [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    |'
- en: '| **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO
    panoptic |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **示例数据集** | COCO检测 | COCO检测，COCO全景 | COCO全景 |'
- en: '| **Format of annotations to provide to** [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    | {‘image_id’: `int`, ‘annotations’: `List[Dict]`} each Dict being a COCO object
    annotation | {‘image_id’: `int`, ‘annotations’: `List[Dict]`} (in case of COCO
    detection) or {‘file_name’: `str`, ‘image_id’: `int`, ‘segments_info’: `List[Dict]`}
    (in case of COCO panoptic) | {‘file_name’: `str`, ‘image_id’: `int`, ‘segments_info’:
    `List[Dict]`} and masks_path (path to directory containing PNG files of the masks)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 提供给[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)的注释格式
    | {‘image_id’: `int`, ‘annotations’: `List[Dict]`}，每个Dict是一个COCO对象注释 | {‘image_id’:
    `int`, ‘annotations’: `List[Dict]`}（在COCO检测的情况下）或{‘file_name’: `str`, ‘image_id’:
    `int`, ‘segments_info’: `List[Dict]`}（在COCO全景的情况下） | {‘file_name’: `str`, ‘image_id’:
    `int`, ‘segments_info’: `List[Dict]`}和masks_path（包含PNG文件的掩模目录的路径） |'
- en: '| **Postprocessing** (i.e. converting the output of the model to Pascal VOC
    format) | `post_process()` | `post_process_segmentation()` | `post_process_segmentation()`,
    `post_process_panoptic()` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **后处理**（即将模型输出转换为Pascal VOC格式） | `post_process()` | `post_process_segmentation()`
    | `post_process_segmentation()`, `post_process_panoptic()` |'
- en: '| **evaluators** | `CocoEvaluator` with `iou_types="bbox"` | `CocoEvaluator`
    with `iou_types="bbox"` or `"segm"` | `CocoEvaluator` with `iou_tupes="bbox"`
    or `"segm"`, `PanopticEvaluator` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **评估器** | `CocoEvaluator` with `iou_types="bbox"` | `CocoEvaluator` with
    `iou_types="bbox"` or `"segm"` | `CocoEvaluator` with `iou_tupes="bbox"` or `"segm"`,
    `PanopticEvaluator` |'
- en: In short, one should prepare the data either in COCO detection or COCO panoptic
    format, then use [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    to create `pixel_values`, `pixel_mask` and optional `labels`, which can then be
    used to train (or fine-tune) a model. For evaluation, one should first convert
    the outputs of the model using one of the postprocessing methods of [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor).
    These can be be provided to either `CocoEvaluator` or `PanopticEvaluator`, which
    allow you to calculate metrics like mean Average Precision (mAP) and Panoptic
    Quality (PQ). The latter objects are implemented in the [original repository](https://github.com/facebookresearch/detr).
    See the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)
    for more info regarding evaluation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，应该准备数据以COO检测或COO全景格式，然后使用[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)创建`pixel_values`、`pixel_mask`和可选的`labels`，然后可以用于训练（或微调）模型。对于评估，应该首先使用[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)的其中一种后处理方法转换模型的输出。这些可以提供给`CocoEvaluator`或`PanopticEvaluator`，这些评估器允许您计算像平均精度（mAP）和全景质量（PQ）这样的指标。后者对象在[原始存储库](https://github.com/facebookresearch/detr)中实现。有关评估的更多信息，请参见[示例笔记本](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)。
- en: Resources
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with DETR.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用DETR。
- en: Object Detection
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测
- en: All example notebooks illustrating fine-tuning [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    and [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    on a custom dataset an be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有示例笔记本说明在自定义数据集上对[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)和[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)进行微调的示例可以在[此处](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)找到。
- en: 'See also: [Object detection task guide](../tasks/object_detection)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参见：[目标检测任务指南](../tasks/object_detection)
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新东西，而不是重复现有资源。
- en: DetrConfig
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrConfig
- en: '### `class transformers.DetrConfig`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L36)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L36)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`use_timm_backbone` (`bool`, *optional*, defaults to `True`) — Whether or not
    to use the `timm` library for the backbone. If set to `False`, will use the `AutoBackbone`
    API.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_timm_backbone`（`bool`，*可选*，默认为`True`）— 是否使用`timm`库作为骨干。如果设置为`False`，将使用`AutoBackbone`
    API。'
- en: '`backbone_config` (`PretrainedConfig` or `dict`, *optional*) — The configuration
    of the backbone model. Only used in case `use_timm_backbone` is set to `False`
    in which case it will default to `ResNetConfig()`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config`（`PretrainedConfig`或`dict`，*可选*）— 骨干模型的配置。仅在`use_timm_backbone`设置为`False`时使用，默认为`ResNetConfig()`。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels`（`int`，*可选*，默认为3）— 输入通道的数量。'
- en: '`num_queries` (`int`, *optional*, defaults to 100) — Number of object queries,
    i.e. detection slots. This is the maximal number of objects [DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)
    can detect in a single image. For COCO, we recommend 100 queries.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_queries`（`int`，*可选*，默认为100）— 对象查询的数量，即检测槽的数量。这是[DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)在单个图像中可以检测的对象的最大数量。对于COCO，我们建议使用100个查询。'
- en: '`d_model` (`int`, *optional*, defaults to 256) — Dimension of the layers.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model`（`int`，*可选*，默认为256）— 层的维度。'
- en: '`encoder_layers` (`int`, *optional*, defaults to 6) — Number of encoder layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers`（`int`，*可选*，默认为6）— 编码器层数。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 6) — Number of decoder layers.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers`（`int`，*可选*，默认为6）— 解码器层数。'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads`（`int`，*可选*，默认为8）— Transformer编码器中每个注意力层的注意力头数。'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 8) — Transformer解码器中每个注意力层的注意力头数。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 2048) — Dimension of the
    “intermediate” (often named feed-forward) layer in decoder.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 2048) — 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 2048) — Dimension of the
    “intermediate” (often named feed-forward) layer in decoder.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 2048) — 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — 全连接层内激活的dropout比率。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`init_xavier_std` (`float`, *optional*, defaults to 1) — The scaling factor
    used for the Xavier initialization gain in the HM Attention map module.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_xavier_std` (`float`, *optional*, defaults to 1) — 用于HM Attention map模块中Xavier初始化增益的缩放因子。'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop
    probability for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的LayerDrop概率。更多细节请参阅[LayerDrop
    paper](https://arxiv.org/abs/1909.11556)。'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop
    probability for the decoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 解码器的LayerDrop概率。更多细节请参阅[LayerDrop
    paper](https://arxiv.org/abs/1909.11556)。'
- en: '`auxiliary_loss` (`bool`, *optional*, defaults to `False`) — Whether auxiliary
    decoding losses (loss at each decoder layer) are to be used.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss` (`bool`, *optional*, defaults to `False`) — 是否使用辅助解码损失（每个解码器层的损失）。'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"sine"`) — Type
    of position embeddings to be used on top of the image features. One of `"sine"`
    or `"learned"`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *optional*, defaults to `"sine"`) — 在图像特征之上使用的位置嵌入的类型。可以是`"sine"`或`"learned"`之一。'
- en: '`backbone` (`str`, *optional*, defaults to `"resnet50"`) — Name of convolutional
    backbone to use in case `use_timm_backbone` = `True`. Supports any convolutional
    backbone from the timm package. For a list of all available models, see [this
    page](https://rwightman.github.io/pytorch-image-models/#load-a-pretrained-model).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone` (`str`, *optional*, defaults to `"resnet50"`) — 在`use_timm_backbone`
    = `True`时要使用的卷积骨干网络的名称。支持timm包中的任何卷积骨干网络。有关所有可用模型的列表，请参阅[此页面](https://rwightman.github.io/pytorch-image-models/#load-a-pretrained-model)。'
- en: '`use_pretrained_backbone` (`bool`, *optional*, defaults to `True`) — Whether
    to use pretrained weights for the backbone. Only supported when `use_timm_backbone`
    = `True`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_pretrained_backbone` (`bool`, *optional*, defaults to `True`) — 是否在骨干网络中使用预训练权重。仅在`use_timm_backbone`
    = `True`时支持。'
- en: '`dilation` (`bool`, *optional*, defaults to `False`) — Whether to replace stride
    with dilation in the last convolutional block (DC5). Only supported when `use_timm_backbone`
    = `True`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dilation` (`bool`, *optional*, defaults to `False`) — 是否在最后的卷积块（DC5）中用扩张替换步幅。仅在`use_timm_backbone`
    = `True`时支持。'
- en: '`class_cost` (`float`, *optional*, defaults to 1) — Relative weight of the
    classification error in the Hungarian matching cost.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_cost` (`float`, *optional*, defaults to 1) — 匈牙利匹配成本中分类错误的相对权重。'
- en: '`bbox_cost` (`float`, *optional*, defaults to 5) — Relative weight of the L1
    error of the bounding box coordinates in the Hungarian matching cost.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_cost` (`float`, *optional*, defaults to 5) — 相对于匈牙利匹配成本中边界框坐标的L1误差的权重。'
- en: '`giou_cost` (`float`, *optional*, defaults to 2) — Relative weight of the generalized
    IoU loss of the bounding box in the Hungarian matching cost.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`giou_cost` (`float`, *optional*, defaults to 2) — 相对于匈牙利匹配成本中边界框广义IoU损失的权重。'
- en: '`mask_loss_coefficient` (`float`, *optional*, defaults to 1) — Relative weight
    of the Focal loss in the panoptic segmentation loss.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_loss_coefficient` (`float`, *optional*, defaults to 1) — 泛光分割损失中Focal损失的相对权重。'
- en: '`dice_loss_coefficient` (`float`, *optional*, defaults to 1) — Relative weight
    of the DICE/F-1 loss in the panoptic segmentation loss.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dice_loss_coefficient` (`float`, *optional*, defaults to 1) — 泛光分割损失中DICE/F-1损失的相对权重。'
- en: '`bbox_loss_coefficient` (`float`, *optional*, defaults to 5) — Relative weight
    of the L1 bounding box loss in the object detection loss.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_loss_coefficient` (`float`, *optional*, defaults to 5) — 目标检测损失中L1边界框损失的相对权重。'
- en: '`giou_loss_coefficient` (`float`, *optional*, defaults to 2) — Relative weight
    of the generalized IoU loss in the object detection loss.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`giou_loss_coefficient` (`float`, *optional*, defaults to 2) — 目标检测损失中广义IoU损失的相对权重。'
- en: '`eos_coefficient` (`float`, *optional*, defaults to 0.1) — Relative classification
    weight of the ‘no-object’ class in the object detection loss.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_coefficient` (`float`, *optional*, defaults to 0.1) — 目标检测损失中“无对象”类别的相对分类权重。'
- en: This is the configuration class to store the configuration of a [DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel).
    It is used to instantiate a DETR model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the DETR [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)
    architecture.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)的配置。它用于根据指定的参数实例化一个DETR模型，定义模型架构。使用默认值实例化配置将产生类似于DETR
    [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `from_backbone_config`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_backbone_config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L239)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L239)'
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`backbone_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The backbone configuration.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — 骨干配置。'
- en: Returns
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)'
- en: An instance of a configuration object
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的实例
- en: Instantiate a [DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)
    (or a derived class) from a pre-trained backbone model configuration.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的骨干模型配置实例化一个[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)（或派生类）。
- en: DetrImageProcessor
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrImageProcessor
- en: '### `class transformers.DetrImageProcessor`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L742)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L742)'
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`format` (`str`, *optional*, defaults to `"coco_detection"`) — Data format
    of the annotations. One of “coco_detection” or “coco_panoptic”.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format` (`str`, *optional*, 默认为`"coco_detection"`) — 注释的数据格式。可以是“coco_detection”或“coco_panoptic”之一。'
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Controls whether to
    resize the image’s `(height, width)` dimensions to the specified `size`. Can be
    overridden by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为 `True`) — 控制是否将图像的`(height, width)`维度调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 800,
    "longest_edge": 1333}`): Size of the image’s `(height, width)` dimensions after
    resizing. Can be overridden by the `size` parameter in the `preprocess` method.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *optional*, 默认为 `{"shortest_edge" -- 800, "longest_edge":
    1333}`): 调整大小后的图像的`(height, width)`维度大小。可以被`preprocess`方法中的`size`参数覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`)
    — Resampling filter to use if resizing the image.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, 默认为 `PILImageResampling.BILINEAR`)
    — 如果调整图像大小，则要使用的重采样滤波器。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Controls whether to
    rescale the image by the specified scale `rescale_factor`. Can be overridden by
    the `do_rescale` parameter in the `preprocess` method.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `True`) — 控制是否按指定比例因子`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`参数覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method. do_normalize — Controls whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 如果重新调整图像，则要使用的比例因子。可以被`preprocess`方法中的`rescale_factor`参数覆盖。do_normalize
    — 控制是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`)
    — Mean values to use when normalizing the image. Can be a single value or a list
    of values, one for each channel. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_DEFAULT_MEAN`)
    — 在归一化图像时使用的均值。可以是单个值或每个通道的值列表。可以被`preprocess`方法中的`image_mean`参数覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`)
    — Standard deviation values to use when normalizing the image. Can be a single
    value or a list of values, one for each channel. Can be overridden by the `image_std`
    parameter in the `preprocess` method.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_DEFAULT_STD`)
    — 在归一化图像时使用的标准差值。可以是单个值或每个通道的值列表。可以被`preprocess`方法中的`image_std`参数覆盖。'
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) — Controls whether to pad
    the image to the largest image in a batch and create a pixel mask. Can be overridden
    by the `do_pad` parameter in the `preprocess` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad` (`bool`, *optional*, 默认为 `True`) — 控制是否将图像填充到批处理中最大的图像并创建像素掩码。可以被`preprocess`方法中的`do_pad`参数覆盖。'
- en: Constructs a Detr image processor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 构造一个Detr图像处理器。
- en: '#### `preprocess`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1070)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1070)'
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image or batch of images to preprocess. Expects a
    single or batch of images with pixel values ranging from 0 to 255\. If passing
    in images with pixel values between 0 and 1, set `do_rescale=False`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像或图像批次。期望单个或批量像素值范围从0到255的图像。如果传入像素值在0到1之间的图像，请设置`do_rescale=False`。'
- en: '`annotations` (`AnnotationType` or `List[AnnotationType]`, *optional*) — List
    of annotations associated with the image or batch of images. If annotation is
    for object detection, the annotations should be a dictionary with the following
    keys:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`annotations` (`AnnotationType` or `List[AnnotationType]`, *optional*) — 与图像或图像批次相关联的注释列表。如果注释是用于对象检测的，则注释应该是一个带有以下键的字典：'
- en: '“image_id” (`int`): The image id.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“image_id” (`int`): 图像id。'
- en: '“annotations” (`List[Dict]`): List of annotations for an image. Each annotation
    should be a dictionary. An image can have no annotations, in which case the list
    should be empty. If annotation is for segmentation, the annotations should be
    a dictionary with the following keys:'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“annotations” (`List[Dict]`): 图像的注释列表。每个注释应该是一个字典。一个图像可以没有注释，此时列表应为空。如果注释是用于分割的，注释应该是一个带有以下键的字典：'
- en: '“image_id” (`int`): The image id.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“image_id” (`int`): 图像id。'
- en: '“segments_info” (`List[Dict]`): List of segments for an image. Each segment
    should be a dictionary. An image can have no segments, in which case the list
    should be empty.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“segments_info” (`List[Dict]`): 图像的段列表。每个段应该是一个字典。一个图像可以没有段，此时列表应为空。'
- en: '“file_name” (`str`): The file name of the image.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“file_name” (`str`): 图像的文件名。'
- en: '`return_segmentation_masks` (`bool`, *optional*, defaults to self.return_segmentation_masks)
    — Whether to return segmentation masks.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_segmentation_masks` (`bool`, *optional*, 默认为self.return_segmentation_masks)
    — 是否返回分割蒙版。'
- en: '`masks_path` (`str` or `pathlib.Path`, *optional*) — Path to the directory
    containing the segmentation masks.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks_path` (`str` or `pathlib.Path`, *optional*) — 包含分割蒙版的目录路径。'
- en: '`do_resize` (`bool`, *optional*, defaults to self.do_resize) — Whether to resize
    the image.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为self.do_resize) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to self.size) — Size of the
    image after resizing.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, 默认为self.size) — 调整大小后的图像尺寸。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to self.resample) —
    Resampling filter to use when resizing the image.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, 默认为self.resample) — 调整图像大小时使用的重采样滤波器。'
- en: '`do_rescale` (`bool`, *optional*, defaults to self.do_rescale) — Whether to
    rescale the image.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为self.do_rescale) — 是否重新缩放图像。'
- en: '`rescale_factor` (`float`, *optional*, defaults to self.rescale_factor) — Rescale
    factor to use when rescaling the image.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, 默认为self.rescale_factor) — 调整图像时使用的缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to self.do_normalize) — Whether
    to normalize the image.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, 默认为self.do_normalize) — 是否规范化图像。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to self.image_mean)
    — Mean to use when normalizing the image.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` or `List[float]`, *optional*, 默认为self.image_mean) — 在规范化图像时使用的均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to self.image_std)
    — Standard deviation to use when normalizing the image.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` or `List[float]`, *optional*, 默认为self.image_std) — 在规范化图像时使用的标准差。'
- en: '`do_pad` (`bool`, *optional*, defaults to self.do_pad) — Whether to pad the
    image.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad` (`bool`, *optional*, 默认为self.do_pad) — 是否填充图像。'
- en: '`format` (`str` or `AnnotationFormat`, *optional*, defaults to self.format)
    — Format of the annotations.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format` (`str` or `AnnotationFormat`, *optional*, 默认为self.format) — 注释的格式。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*, defaults to self.return_tensors)
    — Type of tensors to return. If `None`, will return the list of images.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` or `TensorType`, *optional*, 默认为self.return_tensors)
    — 要返回的张量类型。如果为`None`，将返回图像列表。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` or `str`, *optional*, 默认为`ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像以（高度，宽度，通道数）格式。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：使用输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像以（高度，宽度，通道数）格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`: 图像以（高度，宽度）格式。'
- en: Preprocess an image or a batch of images so that it can be used by the model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批次，以便模型可以使用。
- en: '#### `post_process_object_detection`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_object_detection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` (`DetrObjectDetectionOutput`) — Raw outputs of the model.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` (`DetrObjectDetectionOutput`) — 模型的原始输出。'
- en: '`threshold` (`float`, *optional*) — Score threshold to keep object detection
    predictions.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *optional*) — 保留对象检测预测的分数阈值。'
- en: '`target_sizes` (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*) — Tensor
    of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the
    target size `(height, width)` of each image in the batch. If unset, predictions
    will not be resized.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`torch.Tensor`或`列表[元组[int, int]]`, *可选*) — 形状为`(batch_size,
    2)`的张量或包含每个图像的目标大小`(高度，宽度)`的元组列表(`元组[int, int]`)。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Dict]`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`列表[字典]`'
- en: A list of dictionaries, each dictionary containing the scores, labels and boxes
    for an image in the batch as predicted by the model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个字典包含模型预测的批次中图像的分数、标签和框。
- en: Converts the raw output of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
    format. Only supports PyTorch.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)的原始输出转换为最终的边界框，格式为（左上角_x，左上角_y，右下角_x，右下角_y）。仅支持PyTorch。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `后处理语义分割`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — Raw outputs of the model.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) — A list of tuples (`Tuple[int,
    int]`) containing the target size (height, width) of each image in the batch.
    If unset, predictions will not be resized.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`列表[元组[int, int]]`, *可选*) — 一个元组列表(`元组[int, int]`)，包含批次中每个图像的目标大小（高度，宽度）。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[torch.Tensor]`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`列表[torch.Tensor]`'
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长度为`batch_size`的列表，其中每个项是一个形状为(高度，宽度)的语义分割地图，对应于目标大小条目（如果指定了`target_sizes`）。每个`torch.Tensor`的每个条目对应于一个语义类别id。
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出转换为语义分割地图。仅支持PyTorch。
- en: '#### `post_process_instance_segmentation`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `后处理实例分割`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — Raw outputs of the model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — 模型的原始输出。'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *可选*, 默认为0.5) — 保留预测实例掩模的概率分数阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *可选*, 默认为0.5) — 将预测的掩模转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *可选*, 默认为0.8) — 合并或丢弃每个二进制实例掩模中的小不连续部分的重叠掩模区域阈值。'
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction. If unset, predictions will not be resized.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`列表[元组]`, *可选*) — 长度为(batch_size)的列表，其中每个列表项(`元组[int, int]]`)对应于每个预测的请求最终大小(高度，宽度)。如果未设置，预测将不会被调整大小。'
- en: '`return_coco_annotation` (`bool`, *optional*) — Defaults to `False`. If set
    to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_coco_annotation` (`bool`, *可选*) — 默认为`False`。如果设置为`True`，则以COCO运行长度编码（RLE）格式返回分割地图。'
- en: Returns
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Dict]`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`列表[字典]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个图像一个字典，每个字典包含两个键：
- en: '`segmentation` — A tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `List[List]` run-length encoding (RLE) of the segmentation map
    if return_coco_annotation is set to `True`. Set to `None` if no mask if found
    above `threshold`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` — 形状为`(高度，宽度)`的张量，其中每个像素代表`segment_id`或`列表[列表]`的运行长度编码（RLE）的分割地图，如果return_coco_annotation设置为`True`。如果未找到高于`threshold`的掩模，则设置为`None`。'
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` — 包含每个段的附加信息的字典。'
- en: '`id` — An integer representing the `segment_id`.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` — 代表`segment_id`的整数。'
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` — 代表与`segment_id`对应的标签/语义类别id的整数。'
- en: '`score` — Prediction score of segment with `segment_id`.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` — 具有`segment_id`的段的预测分数。'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into instance segmentation predictions. Only supports PyTorch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出转换为实例分割预测。仅支持PyTorch。
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `后处理全景分割`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — The outputs from [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — 来自[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出。'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *可选*, 默认为0.5) — 保留预测实例掩模的概率分数阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *可选*, 默认为0.5) — 在将预测的掩模转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *可选*, 默认为0.8) — 合并或丢弃每个二进制实例掩模中的小断开部分的重叠掩模面积阈值。'
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) — The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids_to_fuse` (`Set[int]`, *可选*) — 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说一张图像中只能有一个天空，但可以有几个人，因此天空的标签ID将在该集合中，但人的标签ID不在其中。'
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If unset, predictions will not be resized.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`, *可选*) — 长度为(batch_size)的列表，其中每个列表项(`Tuple[int,
    int]]`)对应于批次中每个预测的请求最终大小(高度、宽度)。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Dict]`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个图像一个字典，每个字典包含两个键：
- en: '`segmentation` — a tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` — 形状为`(height, width)`的张量，其中每个像素表示一个`segment_id`，如果在`threshold`以上找不到掩模，则表示为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。'
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` — 一个包含每个段的额外信息的字典。'
- en: '`id` — an integer representing the `segment_id`.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` — 代表`segment_id`的整数。'
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` — 代表与`segment_id`对应的标签/语义类别id的整数。'
- en: '`was_fused` — a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。同一类别/标签的多个实例被融合并分配一个单独的`segment_id`。'
- en: '`score` — Prediction score of segment with `segment_id`.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` — 带有`segment_id`的段的预测分数。'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into image panoptic segmentation predictions. Only supports PyTorch.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出转换为图像全景分割预测。仅支持PyTorch。
- en: DetrFeatureExtractor
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrFeatureExtractor
- en: '### `class transformers.DetrFeatureExtractor`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/feature_extraction_detr.py#L36)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/feature_extraction_detr.py#L36)'
- en: '[PRE12]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#### `__call__`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Preprocess an image or a batch of images.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理一张图像或一批图像。
- en: '#### `post_process_object_detection`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_object_detection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)'
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` (`DetrObjectDetectionOutput`) — Raw outputs of the model.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` (`DetrObjectDetectionOutput`) — 模型的原始输出。'
- en: '`threshold` (`float`, *optional*) — Score threshold to keep object detection
    predictions.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *可选*) — 保留对象检测预测的分数阈值。'
- en: '`target_sizes` (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*) — Tensor
    of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the
    target size `(height, width)` of each image in the batch. If unset, predictions
    will not be resized.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`torch.Tensor`或`List[Tuple[int, int]]`, *可选*) — 形状为`(batch_size,
    2)`的张量或包含批次中每个图像的目标大小`(height, width)`的元组列表(`Tuple[int, int]`)。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Dict]`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: A list of dictionaries, each dictionary containing the scores, labels and boxes
    for an image in the batch as predicted by the model.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个字典包含模型预测的批次中每个图像的分数、标签和框。
- en: Converts the raw output of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
    format. Only supports PyTorch.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)的原始输出转换为(top_left_x,
    top_left_y, bottom_right_x, bottom_right_y)格式的最终边界框。仅支持PyTorch。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)'
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — Raw outputs of the model.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) — A list of tuples (`Tuple[int,
    int]`) containing the target size (height, width) of each image in the batch.
    If unset, predictions will not be resized.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`（`List[Tuple[int, int]]`，*可选*） - 一个元组列表（`Tuple[int, int]`），包含批处理中每个图像的目标大小（高度，宽度）。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[torch.Tensor]`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`'
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长度为`batch_size`的列表，其中每个项目都是一个形状为（高度，宽度）的语义分割地图，对应于`target_sizes`条目（如果指定了`target_sizes`）。每个`torch.Tensor`的条目对应于一个语义类别id。
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出转换为语义分割地图。仅支持PyTorch。
- en: '#### `post_process_instance_segmentation`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_instance_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)'
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — Raw outputs of the model.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`（[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)）
    - 模型的原始输出。'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`（`float`，*可选*，默认为0.5） - 保留预测实例掩码的概率分数阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`（`float`，*可选*，默认为0.5） - 在将预测的掩码转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold`（`float`，*可选*，默认为0.8） - 合并或丢弃每个二进制实例掩码中的小断开部分的重叠掩码区域阈值。'
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction. If unset, predictions will not be resized.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`（`List[Tuple]`，*可选*） - 长度为（batch_size）的列表，其中每个列表项（`Tuple[int,
    int]`）对应于每个预测的请求最终大小（高度，宽度）。如果未设置，预测将不会被调整大小。'
- en: '`return_coco_annotation` (`bool`, *optional*) — Defaults to `False`. If set
    to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_coco_annotation`（`bool`，*可选*） - 默认为`False`。如果设置为`True`，则以COCO运行长度编码（RLE）格式返回分割地图。'
- en: Returns
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Dict]`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个图像一个字典，每个字典包含两个键：
- en: '`segmentation` — A tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `List[List]` run-length encoding (RLE) of the segmentation map
    if return_coco_annotation is set to `True`. Set to `None` if no mask if found
    above `threshold`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` - 一个形状为（高度，宽度）的张量，其中每个像素表示`segment_id`或分割地图的`List[List]`运行长度编码（RLE），如果`return_coco_annotation`设置为`True`。如果未找到高于`threshold`的掩码，则设置为`None`。'
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` - 包含每个段的附加信息的字典。'
- en: '`id` — An integer representing the `segment_id`.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` - 表示`segment_id`的整数。'
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` - 表示与`segment_id`对应的标签/语义类别id的整数。'
- en: '`score` — Prediction score of segment with `segment_id`.'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` - 具有`segment_id`的段的预测分数。'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into instance segmentation predictions. Only supports PyTorch.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出转换为实例分割预测。仅支持PyTorch。
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_panoptic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)'
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation))
    — The outputs from [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`（[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)）
    - 来自[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出。'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`（`float`，*可选*，默认为0.5） - 保留预测实例掩码的概率分数阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`（`float`，*可选*，默认为0.5） - 在将预测的掩码转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold`（`float`，*可选*，默认为0.8） - 合并或丢弃每个二进制实例掩码中的小断开部分的重叠掩码区域阈值。'
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) — The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids_to_fuse`（`Set[int]`，*可选*） - 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说图像中只能有一个天空，但可以有几个人，因此天空的标签ID将在该集合中，但人的标签ID不在其中。'
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If unset, predictions will not be resized.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`, *optional*) — 长度为`(batch_size)`的列表，每个列表项(`Tuple[int,
    int]]`)对应于批处理中每个预测的请求最终大小(高度，宽度)。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`List[Dict]`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '一个字典列表，每个图像一个字典，每个字典包含两个键:'
- en: '`segmentation` — a tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` — 形状为`(height, width)`的张量，每个像素代表一个`segment_id`，如果找不到遮罩，则为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。'
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` — 包含每个段的附加信息的字典。'
- en: '`id` — an integer representing the `segment_id`.'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` — 代表`segment_id`的整数。'
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` — 代表与`segment_id`对应的标签/语义类别id的整数。'
- en: '`was_fused` — a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。相同类别/标签的多个实例被融合并分配一个单一的`segment_id`。'
- en: '`score` — Prediction score of segment with `segment_id`.'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` — 具有`segment_id`的段的预测分数。'
- en: Converts the output of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    into image panoptic segmentation predictions. Only supports PyTorch.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 将[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出转换为图像全景分割预测。仅支持PyTorch。
- en: DETR specific outputs
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DETR特定输出
- en: '### `class transformers.models.detr.modeling_detr.DetrModelOutput`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.detr.modeling_detr.DetrModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L94)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L94)'
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型解码器最后一层的隐藏状态序列。'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`。每层解码器的隐藏状态加上初始嵌入输出。'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`。每层编码器的隐藏状态加上初始嵌入输出。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: '`intermediate_hidden_states` (`torch.FloatTensor` of shape `(config.decoder_layers,
    batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`)
    — Intermediate decoder activations, i.e. the output of each decoder layer, each
    of them gone through a layernorm.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_hidden_states`（`torch.FloatTensor`，形状为`(config.decoder_layers,
    batch_size, sequence_length, hidden_size)`，*可选*，当`config.auxiliary_loss=True`时返回）—
    中间解码器激活，即每个解码器层的输出，每个输出都经过了layernorm。'
- en: Base class for outputs of the DETR encoder-decoder model. This class adds one
    attribute to Seq2SeqModelOutput, namely an optional stack of intermediate decoder
    activations, i.e. the output of each decoder layer, each of them gone through
    a layernorm. This is useful when training the model with auxiliary decoding losses.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: DETR编码器-解码器模型输出的基类。该类在Seq2SeqModelOutput中添加了一个属性，即一个可选的中间解码器激活堆栈，即每个解码器层的输出，每个输出都经过了layernorm。在使用辅助解码损失训练模型时，这是有用的。
- en: '### `class transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L134)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L134)'
- en: '[PRE19]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) — Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 总损失，作为类预测的负对数似然（交叉熵）和边界框损失的线性组合。后者定义为L1损失和广义比例不变IoU损失的线性组合。'
- en: '`loss_dict` (`Dict`, *optional*) — A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict`（`Dict`，*可选*）— 包含各个损失的字典。用于记录日志。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) — Classification logits (including no-object) for all queries.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_queries, num_classes + 1)`的`torch.FloatTensor`）—
    所有查询的分类logits（包括无对象）。'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    — Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes`（形状为`(batch_size, num_queries, 4)`的`torch.FloatTensor`）— 所有查询的归一化框坐标，表示为（中心_x，中心_y，宽度，高度）。这些值在[0,
    1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)来检索未归一化的边界框。'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) — Optional, only returned when
    auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs`（`list[Dict]`，*可选*）— 仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。它是一个包含两个上述键（`logits`和`pred_boxes`）的字典列表，每个字典对应一个解码器层。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）—
    模型解码器最后一层的隐藏状态序列。'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。每个层的解码器的隐藏状态加上初始嵌入输出。'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*）— 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。编码器在每一层的隐藏状态加上初始嵌入输出。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: Output type of [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)的输出类型。'
- en: '### `class transformers.models.detr.modeling_detr.DetrSegmentationOutput`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.detr.modeling_detr.DetrSegmentationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L197)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L197)'
- en: '[PRE20]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) — Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    作为负对数似然（交叉熵）和边界框损失的线性组合的总损失。后者定义为L1损失和广义尺度不变IoU损失的线性组合。'
- en: '`loss_dict` (`Dict`, *optional*) — A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict` (`Dict`, *optional*) — 包含各个损失的字典。用于记录日志。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) — Classification logits (including no-object) for all queries.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) — 所有查询的分类logits（包括无对象）。'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    — Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    — 所有查询的归一化框坐标，表示为（中心_x，中心_y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)来检索未归一化的边界框。'
- en: '`pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4,
    width/4)`) — Segmentation masks logits for all queries. See also [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)
    [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)
    to evaluate semantic, instance and panoptic segmentation masks respectively.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4,
    width/4)`) — 所有查询的分割掩模logits。另请参阅[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)或[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)分别评估语义、实例和全景分割掩模。'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) — Optional, only returned when
    auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs` (`list[Dict]`, *optional*) — 可选，仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。它是一个包含每个解码器层的上述两个键（`logits`和`pred_boxes`）的字典列表。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型解码器最后一层的隐藏状态序列。'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。解码器在每一层的隐藏状态加上初始嵌入输出。'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。编码器在每层输出的隐藏状态加上初始嵌入输出。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: Output type of [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的输出类型。'
- en: DetrModel
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrModel
- en: '### `class transformers.DetrModel`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1296)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1296)'
- en: '[PRE21]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DETR Model (consisting of a backbone and encoder-decoder Transformer)
    outputting raw hidden-states without any specific head on top.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的DETR模型（由骨干和编码器-解码器Transformer组成），输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1337)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1337)'
- en: '[PRE22]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。默认情况下将忽略填充。'
- en: Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的遮罩。遮罩值选择在`[0,
    1]`中：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于真实的像素（即`未被遮罩`），
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于填充像素（即`被遮罩`）为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力遮罩？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) — Not used by default. Can be used to mask object queries.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, num_queries)`的`torch.FloatTensor`，*可选*）—
    默认情况下不使用。可用于屏蔽对象查询。'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*可选*）是编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing the flattened feature
    map (output of the backbone + projection layer), you can choose to directly pass
    a flattened representation of an image.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递一个图像的扁平表示，而不是传递扁平特征图（骨干网络输出+投影层的输出）。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) — Optionally, instead of initializing the queries with
    a tensor of zeros, you can choose to directly pass an embedded representation.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`（形状为`(batch_size, num_queries, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递一个嵌入表示，而不是用零张量初始化查询。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    and inputs.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.detr.modeling_detr.DetrModelOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）—
    模型解码器最后一层的隐藏状态序列的输出。'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    元组包括（每个层的嵌入输出+每个层的输出的`torch.FloatTensor`）形状为`(batch_size, sequence_length, hidden_size)`。解码器在每一层的输出隐藏状态加上初始嵌入输出。'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    元组包括（每层一个）形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`。解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—编码器的隐藏状态元组，形状为`(batch_size,
    sequence_length, hidden_size)`（每层一个）。每层的编码器隐藏状态加上初始嵌入输出。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—编码器的注意力权重元组，形状为`(batch_size,
    num_heads, sequence_length, sequence_length)`（每层一个）。用于计算自注意力头中的加权平均值的编码器的注意力softmax后的注意力权重。'
- en: '`intermediate_hidden_states` (`torch.FloatTensor` of shape `(config.decoder_layers,
    batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`)
    — Intermediate decoder activations, i.e. the output of each decoder layer, each
    of them gone through a layernorm.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_hidden_states`（形状为`(config.decoder_layers, batch_size, sequence_length,
    hidden_size)`的`torch.FloatTensor`，*可选*，当`config.auxiliary_loss=True`时返回）—中间解码器激活，即每个解码器层的输出，每个都经过一个layernorm。'
- en: The [DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)
    forward method, overrides the `__call__` special method.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrModel](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: DetrForObjectDetection
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrForObjectDetection
- en: '### `class transformers.DetrForObjectDetection`'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrForObjectDetection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1464)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1464)'
- en: '[PRE24]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)）—模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: DETR Model (consisting of a backbone and encoder-decoder Transformer) with object
    detection heads on top, for tasks such as COCO detection.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: DETR模型（由骨干和编码器-解码器Transformer组成），顶部带有目标检测头，用于诸如COCO检测之类的任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1497)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1497)'
- en: '[PRE25]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—像素值。默认情况下，如果提供填充，则将忽略填充。'
- en: Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）—用于避免在填充像素值上执行注意力的掩码。掩码值选在`[0,
    1]`之间：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示真实像素（即`未屏蔽`），
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示填充像素（即`屏蔽`）。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) — Not used by default. Can be used to mask object queries.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) — 默认情况下不使用。可用于屏蔽对象查询。'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组由(`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`)组成，`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing the flattened feature
    map (output of the backbone + projection layer), you can choose to directly pass
    a flattened representation of an image.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递图像的扁平化特征图（骨干网络+投影层的输出），而不是传递扁平化表示。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) — Optionally, instead of initializing the queries with
    a tensor of zeros, you can choose to directly pass an embedded representation.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是用零张量初始化查询。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`List[Dict]` of len `(batch_size,)`, *optional*) — Labels for computing
    the bipartite matching loss. List of dicts, each dictionary containing at least
    the following 2 keys: ‘class_labels’ and ‘boxes’ (the class labels and bounding
    boxes of an image in the batch respectively). The class labels themselves should
    be a `torch.LongTensor` of len `(number of bounding boxes in the image,)` and
    the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image,
    4)`.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`List[Dict]` of len `(batch_size,)`, *optional*) — 用于计算二部匹配损失的标签。字典列表，每个字典至少包含以下2个键：''class_labels''和''boxes''（分别是批处理中图像的类别标签和边界框）。类别标签本身应该是长度为`(图像中边界框数量,)`的`torch.LongTensor`，而边界框应该是形状为`(图像中边界框数量,
    4)`的`torch.FloatTensor`。'
- en: Returns
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    and inputs.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrObjectDetectionOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrObjectDetectionOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)）和输入。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) — Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) — 作为类别预测的负对数似然（交叉熵）和边界框损失的线性组合的总损失。后者被定义为L1损失和广义尺度不变IoU损失的线性组合。'
- en: '`loss_dict` (`Dict`, *optional*) — A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict` (`Dict`, *optional*) — 包含各个损失的字典。用于记录日志。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) — Classification logits (including no-object) for all queries.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) — 所有查询的分类logits（包括无对象）。'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    — Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    — 所有查询的归一化框坐标，表示为（中心_x，中心_y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)来检索未归一化的边界框。'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) — Optional, only returned when
    auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs` (`list[Dict]`, *optional*) — 可选，仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。它是一个包含每个解码器层的上述两个键（`logits`和`pred_boxes`）的字典列表。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型解码器最后一层的隐藏状态序列。'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 解码器的隐藏状态元组，每层一个，形状为`(batch_size, sequence_length, hidden_size)`。每层解码器的隐藏状态加上初始嵌入输出。'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 解码器的注意力权重元组，每层一个，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 解码器的交叉注意力层的注意力权重元组，每层一个，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 编码器的隐藏状态元组，每层一个，形状为`(batch_size, sequence_length, hidden_size)`。每层编码器的隐藏状态加上初始嵌入输出。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 编码器的注意力权重元组，每层一个，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: The [DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)
    forward method, overrides the `__call__` special method.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: DetrForSegmentation
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DetrForSegmentation
- en: '### `class transformers.DetrForSegmentation`'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DetrForSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1637)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1637)'
- en: '[PRE27]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: DETR Model (consisting of a backbone and encoder-decoder Transformer) with a
    segmentation head on top, for tasks such as COCO panoptic.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: DETR模型（由骨干和编码器-解码器Transformer组成），顶部带有分割头，用于诸如COCO全景等任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1667)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1667)'
- en: '[PRE28]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。默认情况下将忽略填充。'
- en: Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[DetrImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的掩码。选择的掩码值在`[0,
    1]`中：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于真实的像素（即`未被遮罩`），
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于填充像素（即`遮罩`）的像素为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力遮罩？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`,
    *optional*) — Not used by default. Can be used to mask object queries.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, num_queries)`的`torch.FloatTensor`，*可选*）—
    默认情况下不使用。可用于屏蔽对象查询。'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*可选*是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing the flattened feature
    map (output of the backbone + projection layer), you can choose to directly pass
    a flattened representation of an image.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递图像的扁平特征图（骨干+投影层的输出）而不是传递它。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries,
    hidden_size)`, *optional*) — Optionally, instead of initializing the queries with
    a tensor of zeros, you can choose to directly pass an embedded representation.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`（形状为`(batch_size, num_queries, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示来初始化查询，而不是使用零张量。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`List[Dict]` of len `(batch_size,)`, *optional*) — Labels for computing
    the bipartite matching loss, DICE/F-1 loss and Focal loss. List of dicts, each
    dictionary containing at least the following 3 keys: ‘class_labels’, ‘boxes’ and
    ‘masks’ (the class labels, bounding boxes and segmentation masks of an image in
    the batch respectively). The class labels themselves should be a `torch.LongTensor`
    of len `(number of bounding boxes in the image,)`, the boxes a `torch.FloatTensor`
    of shape `(number of bounding boxes in the image, 4)` and the masks a `torch.FloatTensor`
    of shape `(number of bounding boxes in the image, height, width)`.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（长度为`(batch_size,)`的`List[Dict]`，*可选*）— 用于计算二部匹配损失、DICE/F-1损失和Focal损失的标签。字典列表，每个字典至少包含以下3个键：‘class_labels’、‘boxes’和‘masks’（分别是批次中图像的类标签、边界框和分割掩码）。类标签本身应该是长度为`(图像中边界框的数量,)`的`torch.LongTensor`，边界框是形状为`(图像中边界框的数量,
    4)`的`torch.FloatTensor`，掩码是形状为`(图像中边界框的数量, height, width)`的`torch.FloatTensor`。'
- en: Returns
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig))
    and inputs.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.detr.modeling_detr.DetrSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.models.detr.modeling_detr.DetrSegmentationOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[DetrConfig](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    are provided)) — Total loss as a linear combination of a negative log-likehood
    (cross-entropy) for class prediction and a bounding box loss. The latter is defined
    as a linear combination of the L1 loss and the generalized scale-invariant IoU
    loss.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回） — 总损失，作为负对数似然（交叉熵）和边界框损失的线性组合。后者被定义为L1损失和广义尺度不变IoU损失的线性组合。'
- en: '`loss_dict` (`Dict`, *optional*) — A dictionary containing the individual losses.
    Useful for logging.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_dict` (`Dict`，*optional*) — 包含各个损失的字典。用于记录。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes
    + 1)`) — Classification logits (including no-object) for all queries.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, num_queries, num_classes + 1)`）
    — 所有查询的分类logits（包括无对象）。'
- en: '`pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`)
    — Normalized boxes coordinates for all queries, represented as (center_x, center_y,
    width, height). These values are normalized in [0, 1], relative to the size of
    each individual image in the batch (disregarding possible padding). You can use
    [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)
    to retrieve the unnormalized bounding boxes.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_boxes` (`torch.FloatTensor`，形状为`(batch_size, num_queries, 4)`） — 所有查询的归一化框坐标，表示为（中心_x，中心_y，宽度，高度）。这些值在[0,
    1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用[post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_object_detection)来检索未归一化的边界框。'
- en: '`pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4,
    width/4)`) — Segmentation masks logits for all queries. See also [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)
    [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)
    to evaluate semantic, instance and panoptic segmentation masks respectively.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_masks` (`torch.FloatTensor`，形状为`(batch_size, num_queries, height/4, width/4)`）
    — 所有查询的分割掩模logits。另请参阅[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_semantic_segmentation)或[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_instance_segmentation)[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrFeatureExtractor.post_process_panoptic_segmentation)分别评估语义、实例和全景分割掩模。'
- en: '`auxiliary_outputs` (`list[Dict]`, *optional*) — Optional, only returned when
    auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
    and labels are provided. It is a list of dictionaries containing the two above
    keys (`logits` and `pred_boxes`) for each decoder layer.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_outputs` (`list[Dict]`, *optional*) — 当辅助损失被激活时（即`config.auxiliary_loss`设置为`True`）并且提供了标签时才返回。这是一个包含每个解码器层的两个上述键（`logits`和`pred_boxes`）的字典列表。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the decoder of the model.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 模型解码器最后一层的隐藏状态序列。'
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。解码器在每个层的输出的隐藏状态加上初始嵌入输出。'
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the decoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。'
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。编码器每一层的隐藏状态加上初始嵌入输出。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`. Attentions weights of the encoder, after the
    attention softmax, used to compute the weighted average in the self-attention
    heads.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。'
- en: The [DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[DetrForSegmentation](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForSegmentation)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
