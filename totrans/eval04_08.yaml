- en: Creating and sharing a new evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/evaluate/creating_and_sharing](https://huggingface.co/docs/evaluate/creating_and_sharing)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/creating_and_sharing.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/CodeBlock-hf-doc-builder.js">
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you can create a new metric make sure you have all the necessary dependencies
    installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Also make sure your Hugging Face token is registered so you can connect to
    the Hugging Face Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Create
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All evaluation modules, be it metrics, comparisons, or measurements live on
    the ðŸ¤— Hub in a [Space](https://huggingface.co/docs/hub/spaces) (see for example
    [Accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)). In principle,
    you could setup a new Space and add a new module following the same structure.
    However, we added a CLI that makes creating a new evaluation module much easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will create a new Space on the ðŸ¤— Hub, clone it locally, and populate it
    with a template. Instructions on how to fill the template will be displayed in
    the terminal, but are also explained here in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Spaces, see the [Spaces documentation](https://huggingface.co/docs/hub/spaces).
  prefs: []
  type: TYPE_NORMAL
- en: Module script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evaluation module script (the file with suffix `*.py`) is the core of the
    new module and includes all the code for computing the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Start by adding some information about your evalution module in `EvaluationModule._info()`.
    The most important attributes you should specify are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo.description` provides a brief description about your
    evalution module.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo.citation` contains a BibTex citation for the evalution
    module.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo.inputs_description` describes the expected inputs and
    outputs. It may also provide an example usage of the evalution module.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo.features` defines the name and type of the predictions
    and references. This has to be either a single `datasets.Features` object or a
    list of `datasets.Features` objects if multiple input types are allowed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we can move on to prepare everything before the actual computation.
  prefs: []
  type: TYPE_NORMAL
- en: Download
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some evaluation modules require some external data such as NLTK that requires
    resources or the BLEURT metric that requires checkpoints. You can implement these
    downloads in `EvaluationModule._download_and_prepare()`, which downloads and caches
    the resources via the `dlmanager`. A simplified example on how BLEURT downloads
    and loads a checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if you need to download the NLTK `"punkt"` resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to define how the computation of the evaluation module works.
  prefs: []
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The computation is performed in the `EvaluationModule._compute()` method. It
    takes the same arguments as `EvaluationModuleInfo.features` and should then return
    the result as a dictionary. Here an example of an exact match metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This method is used when you call `.compute()` later on.
  prefs: []
  type: TYPE_NORMAL
- en: Readme
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you use the `evalute-cli` to setup the evaluation module the Readme structure
    and instructions are automatically created. It should include a general description
    of the metric, information about its input/output format, examples as well as
    information about its limiations or biases and references.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your evaluation modules has additional dependencies (e.g. `sklearn` or `nltk`)
    the `requirements.txt` files is the place to put them. The file follows the `pip`
    format and you can list all dependencies there.
  prefs: []
  type: TYPE_NORMAL
- en: App
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `app.py` is where the Spaces widget lives. In general it looks like the
    following and does not require any changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you want a custom widget you could add your gradio app here.
  prefs: []
  type: TYPE_NORMAL
- en: Push to Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, when you are done with all the above changes it is time to push your
    evaluation module to the hub. To do so navigate to the folder of your module and
    git add/commit/push the changes to the hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tada ðŸŽ‰! Your evaluation module is now on the ðŸ¤— Hub and ready to be used by everybody!
  prefs: []
  type: TYPE_NORMAL
