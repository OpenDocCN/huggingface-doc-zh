- en: Second Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/quiz2](https://huggingface.co/learn/deep-rl-course/unit2/quiz2)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: What is Q-Learning?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Q2: What is a Q-table?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Q3: Why if we have an optimal Q-function Q* we have an optimal policy?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <details data-svelte-h="svelte-u1mjbk"><summary>Solution</summary>
  prefs: []
  type: TYPE_NORMAL
- en: Because if we have an optimal Q-function, we have an optimal policy since we
    know for each state what is the best action to take.
  prefs: []
  type: TYPE_NORMAL
- en: '![link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q4: Can you explain what is Epsilon-Greedy Strategy?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <details data-svelte-h="svelte-gtpca8"><summary>Solution</summary> Epsilon Greedy
    Strategy is a policy that handles the exploration/exploitation trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is that we define epsilon …õ = 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With *probability 1 ‚Äî …õ* : we do exploitation (aka our agent selects the action
    with the highest state-action pair value).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With *probability …õ* : we do exploration (trying random action).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Epsilon Greedy](../Images/30b0aba4490af7f85f0594dc198e9c03.png)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q5: How do we update the Q value of a state, action pair?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Q Update exercise](../Images/010325db58913b84bff90765b74c54c5.png) <details
    data-svelte-h="svelte-1uulm3q"><summary>Solution</summary> ![Q Update exercise](../Images/4129baf3a98c818c8380a3de909730fc.png)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q6: What‚Äôs the difference between on-policy and off-policy'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <details data-svelte-h="svelte-1fmk5x3"><summary>Solution</summary> ![On/off
    policy](../Images/ce691ce98ae89b58669eb975be3f446c.png)</details>
  prefs: []
  type: TYPE_NORMAL
- en: Congrats on finishing this Quiz ü•≥, if you missed some elements, take time to
    read again the chapter to reinforce (üòè) your knowledge.
  prefs: []
  type: TYPE_NORMAL
