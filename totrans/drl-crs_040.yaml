- en: Second Quiz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ¬¡æµ‹éªŒ
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/quiz2](https://huggingface.co/learn/deep-rl-course/unit2/quiz2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/learn/deep-rl-course/unit2/quiz2](https://huggingface.co/learn/deep-rl-course/unit2/quiz2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ å’Œ[é¿å…èƒ½åŠ›å¹»è§‰](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)çš„æœ€ä½³æ–¹æ³•**æ˜¯æµ‹è¯•è‡ªå·±ã€‚**
    è¿™å°†å¸®åŠ©ä½ æ‰¾åˆ°**éœ€è¦åŠ å¼ºçŸ¥è¯†çš„åœ°æ–¹**ã€‚
- en: 'Q1: What is Q-Learning?'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q1: ä»€ä¹ˆæ˜¯Qå­¦ä¹ ï¼Ÿ'
- en: 'Q2: What is a Q-table?'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q2: ä»€ä¹ˆæ˜¯Qè¡¨ï¼Ÿ'
- en: 'Q3: Why if we have an optimal Q-function Q* we have an optimal policy?'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q3: å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€ä¼˜çš„Qå‡½æ•°Q*ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªæœ€ä¼˜çš„ç­–ç•¥ï¼Ÿ'
- en: <details data-svelte-h="svelte-u1mjbk"><summary>Solution</summary>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-u1mjbk"><summary>è§£ç­”</summary>
- en: Because if we have an optimal Q-function, we have an optimal policy since we
    know for each state what is the best action to take.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€ä¼˜çš„Qå‡½æ•°ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€ä¼˜çš„ç­–ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å¯¹äºæ¯ä¸ªçŠ¶æ€ï¼Œé‡‡å–çš„æœ€ä½³è¡ŒåŠ¨æ˜¯ä»€ä¹ˆã€‚
- en: '![link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)</details>'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![é“¾æ¥å€¼ç­–ç•¥](../Images/06e7785cc764e6109bfc6c89005a4d92.png)</details>'
- en: 'Q4: Can you explain what is Epsilon-Greedy Strategy?'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q4: ä½ èƒ½è§£é‡Šä»€ä¹ˆæ˜¯Epsilon-Greedyç­–ç•¥å—ï¼Ÿ'
- en: <details data-svelte-h="svelte-gtpca8"><summary>Solution</summary> Epsilon Greedy
    Strategy is a policy that handles the exploration/exploitation trade-off.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-gtpca8"><summary>è§£ç­”</summary> Epsilon Greedyç­–ç•¥æ˜¯å¤„ç†æ¢ç´¢/åˆ©ç”¨æƒè¡¡çš„ç­–ç•¥ã€‚
- en: 'The idea is that we define epsilon É› = 1.0:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•æ˜¯æˆ‘ä»¬å®šä¹‰epsilonÉ› = 1.0ï¼š
- en: 'With *probability 1 â€” É›* : we do exploitation (aka our agent selects the action
    with the highest state-action pair value).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡1 - É›è¿›è¡Œåˆ©ç”¨ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ä»£ç†é€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œå¯¹å€¼çš„åŠ¨ä½œï¼‰ã€‚
- en: 'With *probability É›* : we do exploration (trying random action).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡É›è¿›è¡Œæ¢ç´¢ï¼ˆå°è¯•éšæœºåŠ¨ä½œï¼‰ã€‚
- en: '![Epsilon Greedy](../Images/30b0aba4490af7f85f0594dc198e9c03.png)</details>'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![Epsilon Greedy](../Images/30b0aba4490af7f85f0594dc198e9c03.png)</details>'
- en: 'Q5: How do we update the Q value of a state, action pair?'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q5: æˆ‘ä»¬å¦‚ä½•æ›´æ–°çŠ¶æ€ã€åŠ¨ä½œå¯¹çš„Qå€¼ï¼Ÿ'
- en: '![Q Update exercise](../Images/010325db58913b84bff90765b74c54c5.png) <details
    data-svelte-h="svelte-1uulm3q"><summary>Solution</summary> ![Q Update exercise](../Images/4129baf3a98c818c8380a3de909730fc.png)</details>'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![Qæ›´æ–°ç»ƒä¹ ](../Images/010325db58913b84bff90765b74c54c5.png) <details data-svelte-h="svelte-1uulm3q"><summary>è§£ç­”</summary>
    ![Qæ›´æ–°ç»ƒä¹ ](../Images/4129baf3a98c818c8380a3de909730fc.png)</details>'
- en: 'Q6: Whatâ€™s the difference between on-policy and off-policy'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q6: åœ¨æ”¿ç­–å’Œç¦»æ”¿ç­–ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«'
- en: <details data-svelte-h="svelte-1fmk5x3"><summary>Solution</summary> ![On/off
    policy](../Images/ce691ce98ae89b58669eb975be3f446c.png)</details>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-1fmk5x3"><summary>è§£ç­”</summary> ![On/off policy](../Images/ce691ce98ae89b58669eb975be3f446c.png)</details>
- en: Congrats on finishing this Quiz ğŸ¥³, if you missed some elements, take time to
    read again the chapter to reinforce (ğŸ˜) your knowledge.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ å®Œæˆäº†è¿™ä¸ªæµ‹éªŒğŸ¥³ï¼Œå¦‚æœä½ é”™è¿‡äº†ä¸€äº›å…ƒç´ ï¼Œè¯·èŠ±æ—¶é—´å†æ¬¡é˜…è¯»ç« èŠ‚ä»¥åŠ å¼ºï¼ˆğŸ˜ï¼‰ä½ çš„çŸ¥è¯†ã€‚
