["```py\npip install -U peft\n```", "```py\nimport torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\ngenerator = torch.manual_seed(42)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\n).images[0]\n```", "```py\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    \"Linaqruf/animagine-xl\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"face focus, cute, masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, night, turtleneck\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\n).images[0]\n```", "```py\nimport torch\nfrom diffusers import AutoPipelineForImage2Image, LCMScheduler\nfrom diffusers.utils import make_image_grid, load_image\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt,\n    image=init_image,\n    num_inference_steps=4,\n    guidance_scale=1,\n    strength=0.6,\n    generator=generator\n).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```", "```py\nimport torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LoRAs\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n\n# Combine LoRAs\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\n\nprompt = \"papercut, a cute fox\"\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\nimage\n```", "```py\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((512, 512))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant=\"fp16\"\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    \"the mona lisa\",\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.8,\n    cross_attention_kwargs={\"scale\": 1},\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```", "```py\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# Prepare image\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1024))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\", \n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5, \n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```", "```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\n# generator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=4,\n    guidance_scale=4, \n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```", "```py\nimport torch\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"diffusers/animatediff-motion-adapter-v1-5\")\npipe = AnimateDiffPipeline.from_pretrained(\n    \"frankjoshua/toonyou_beta6\",\n    motion_adapter=adapter,\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\n\npipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\n\nprompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\ngenerator = torch.manual_seed(0)\nframes = pipe(\n    prompt=prompt,\n    num_inference_steps=5,\n    guidance_scale=1.25,\n    cross_attention_kwargs={\"scale\": 1},\n    num_frames=24,\n    generator=generator\n).frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```"]