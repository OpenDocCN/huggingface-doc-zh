["```py\nfrom transformers import AutoTokenizer\n-from transformers import AutoModelForSequenceClassification\n+from optimum.neuron import NeuronModelForSequenceClassification\n\n# PyTorch checkpoint\n-model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n+model = NeuronModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\",\n+                                                             export=True, **neuron_kwargs)\n```", "```py\n# Save the neuron model\n>>> model.save_pretrained(\"a_local_path_for_compiled_neuron_model\")\n\n# Push the neuron model to HF Hub\n>>> model.push_to_hub(\n...     \"a_local_path_for_compiled_neuron_model\", repository_id=\"my-neuron-repo\", use_auth_token=True\n... )\n```", "```py\n>>> from optimum.neuron import NeuronModelForSequenceClassification\n>>> model = NeuronModelForSequenceClassification.from_pretrained(\"my-neuron-repo\")\n```", "```py\nfrom transformers import AutoTokenizer\n-from transformers import AutoModelForSequenceClassification\n+from optimum.neuron import NeuronModelForSequenceClassification\n\n# PyTorch checkpoint\n-model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n# Compile your model during the first time\n+compiler_args = {\"auto_cast\": \"matmul\", \"auto_cast_type\": \"bf16\"}\n+input_shapes = {\"batch_size\": 1, \"sequence_length\": 64}\n+model = NeuronModelForSequenceClassification.from_pretrained(\n+    \"distilbert-base-uncased-finetuned-sst-2-english\", export=True, **compiler_args, **input_shapes,\n+)\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\ninputs = tokenizer(\"Hamilton is considered to be the best musical of human history.\", return_tensors=\"pt\")\n\nlogits = model(**inputs).logits\nprint(model.config.id2label[logits.argmax().item()])\n# 'POSITIVE'\n```", "```py\n>>> from transformers import AutoModelForSequenceClassification\n>>> from optimum.exporters import TasksManager\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n# Infer the task name if you don't know\n>>> task = TasksManager.infer_task_from_model(model)  # 'text-classification'\n\n>>> neuron_config_constructor = TasksManager.get_exporter_config_constructor(\n...     model=model, exporter=\"neuron\", task='text-classification'\n... )\n>>> print(neuron_config_constructor.func.get_mandatory_axes_for_task(task))\n# ('batch_size', 'sequence_length')\n```", "```py\nfrom transformers import AutoTokenizer\n-from transformers import AutoModelForCausalLM\n+from optimum.neuron import NeuronModelForCausalLM\n\n# Instantiate and convert to Neuron a PyTorch checkpoint\n+compiler_args = {\"num_cores\": 1, \"auto_cast_type\": 'fp32'}\n+input_shapes = {\"batch_size\": 1, \"sequence_length\": 512}\n-model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n+model = NeuronModelForCausalLM.from_pretrained(\"gpt2\", export=True, **compiler_args, **input_shapes)\n```", "```py\nfrom transformers import AutoTokenizer\n-from transformers import AutoModelForCausalLM\n+from optimum.neuron import NeuronModelForCausalLM\n\n# Instantiate and convert to Neuron a PyTorch checkpoint\n-model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n+model = NeuronModelForCausalLM.from_pretrained(\"gpt2\", export=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ntokens = tokenizer(\"I really wish \", return_tensors=\"pt\")\nwith torch.inference_mode():\n    sample_output = model.generate(\n        **tokens,\n        do_sample=True,\n        min_length=128,\n        max_length=256,\n        temperature=0.7,\n    )\n    outputs = [tokenizer.decode(tok) for tok in sample_output]\n    print(outputs)\n```"]