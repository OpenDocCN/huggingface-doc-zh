- en: Add custom Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/guides/custom_dependencies](https://huggingface.co/docs/inference-endpoints/guides/custom_dependencies)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference Endpointsâ€™ base image includes all required libraries to run inference
    on ðŸ¤— Transformers models, but it also supports custom dependencies. This is useful
    if you want to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler)
    and need additional Python dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: run a model which requires special dependencies like the newest or a fixed version
    of a library (for example, `tapas` (`torch-scatter`)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)
    with the Python dependencies you want to install in your model repository on the
    Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference
    Endpoints checks if the model repository contains a `requirements.txt` file and
    installs the dependencies listed within.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Check out the `requirements.txt` files in the following model repositories
    for examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, take a look at how you can create and install dependencies
    when you [use your own custom container](/docs/inference-endpoints/guides/custom_container)
    for inference.
  prefs: []
  type: TYPE_NORMAL
