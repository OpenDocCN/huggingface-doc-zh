["```py\n( vocab_size = 81 hidden_size = 768 encoder_layers = 12 encoder_attention_heads = 12 encoder_ffn_dim = 3072 encoder_layerdrop = 0.1 decoder_layers = 6 decoder_ffn_dim = 3072 decoder_attention_heads = 12 decoder_layerdrop = 0.1 hidden_act = 'gelu' positional_dropout = 0.1 hidden_dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 scale_embedding = False feat_extract_norm = 'group' feat_proj_dropout = 0.0 feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 decoder_start_token_id = 2 num_mel_bins = 80 speech_decoder_prenet_layers = 2 speech_decoder_prenet_units = 256 speech_decoder_prenet_dropout = 0.5 speaker_embedding_dim = 512 speech_decoder_postnet_layers = 5 speech_decoder_postnet_units = 256 speech_decoder_postnet_kernel = 5 speech_decoder_postnet_dropout = 0.5 reduction_factor = 2 max_speech_positions = 4000 max_text_positions = 450 encoder_max_relative_position = 160 use_guided_attention_loss = True guided_attention_loss_num_heads = 2 guided_attention_loss_sigma = 0.4 guided_attention_loss_scale = 10.0 use_cache = True is_encoder_decoder = True **kwargs )\n```", "```py\n>>> from transformers import SpeechT5Model, SpeechT5Config\n\n>>> # Initializing a \"microsoft/speecht5_asr\" style configuration\n>>> configuration = SpeechT5Config()\n\n>>> # Initializing a model (with random weights) from the \"microsoft/speecht5_asr\" style configuration\n>>> model = SpeechT5Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( model_in_dim = 80 sampling_rate = 16000 upsample_initial_channel = 512 upsample_rates = [4, 4, 4, 4] upsample_kernel_sizes = [8, 8, 8, 8] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] initializer_range = 0.01 leaky_relu_slope = 0.1 normalize_before = True **kwargs )\n```", "```py\n>>> from transformers import SpeechT5HifiGan, SpeechT5HifiGanConfig\n\n>>> # Initializing a \"microsoft/speecht5_hifigan\" style configuration\n>>> configuration = SpeechT5HifiGanConfig()\n\n>>> # Initializing a model (with random weights) from the \"microsoft/speecht5_hifigan\" style configuration\n>>> model = SpeechT5HifiGan(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' normalize = False sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( feature_size: int = 1 sampling_rate: int = 16000 padding_value: float = 0.0 do_normalize: bool = False num_mel_bins: int = 80 hop_length: int = 16 win_length: int = 64 win_function: str = 'hann_window' frame_signal_scale: float = 1.0 fmin: float = 80 fmax: float = 7600 mel_floor: float = 1e-10 reduction_factor: int = 2 return_attention_mask: bool = True **kwargs )\n```", "```py\n( audio: Union = None audio_target: Union = None padding: Union = False max_length: Optional = None truncation: bool = False pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None sampling_rate: Optional = None **kwargs )\n```", "```py\n( feature_extractor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n( save_directory push_to_hub: bool = False **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: SpeechT5Config encoder: Optional = None decoder: Optional = None )\n```", "```py\n( input_values: Optional = None attention_mask: Optional = None decoder_input_values: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None speaker_embeddings: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n( config: SpeechT5Config )\n```", "```py\n( input_values: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToText\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\n...     \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\"\n... )  # doctest: +IGNORE_RESULT\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_asr\")\n>>> model = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(audio=dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> predicted_ids = model.generate(**inputs, max_length=100)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n>>> transcription[0]\n'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'\n```", "```py\n>>> inputs[\"labels\"] = processor(text_target=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # compute loss\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n19.68\n```", "```py\n( config: SpeechT5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_values: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None speaker_embeddings: Optional = None labels: Optional = None stop_labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSpectrogramOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, set_seed\n>>> import torch\n\n>>> processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n>>> model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n>>> vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n\n>>> inputs = processor(text=\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> speaker_embeddings = torch.zeros((1, 512))  # or load xvectors from a file\n\n>>> set_seed(555)  # make deterministic\n\n>>> # generate speech\n>>> speech = model.generate(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n>>> speech.shape\ntorch.Size([15872])\n```", "```py\n( input_ids: LongTensor attention_mask: Optional = None speaker_embeddings: Optional = None threshold: float = 0.5 minlenratio: float = 0.0 maxlenratio: float = 20.0 vocoder: Optional = None output_cross_attentions: bool = False return_output_lengths: bool = False **kwargs ) \u2192 export const metadata = 'undefined';tuple(torch.FloatTensor) comprising various elements depending on the inputs\n```", "```py\n( config: SpeechT5Config )\n```", "```py\n( input_values: Optional = None attention_mask: Optional = None decoder_input_values: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None speaker_embeddings: Optional = None labels: Optional = None stop_labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSpectrogramOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan, set_seed\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\n...     \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\"\n... )  # doctest: +IGNORE_RESULT\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_vc\")\n>>> model = SpeechT5ForSpeechToSpeech.from_pretrained(\"microsoft/speecht5_vc\")\n>>> vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(audio=dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n\n>>> speaker_embeddings = torch.zeros((1, 512))  # or load xvectors from a file\n\n>>> set_seed(555)  # make deterministic\n\n>>> # generate speech\n>>> speech = model.generate_speech(inputs[\"input_values\"], speaker_embeddings, vocoder=vocoder)\n>>> speech.shape\ntorch.Size([77824])\n```", "```py\n( input_values: FloatTensor speaker_embeddings: Optional = None attention_mask: Optional = None threshold: float = 0.5 minlenratio: float = 0.0 maxlenratio: float = 20.0 vocoder: Optional = None output_cross_attentions: bool = False return_output_lengths: bool = False ) \u2192 export const metadata = 'undefined';tuple(torch.FloatTensor) comprising various elements depending on the inputs\n```", "```py\n( config: SpeechT5HifiGanConfig )\n```", "```py\n( spectrogram: FloatTensor ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```"]