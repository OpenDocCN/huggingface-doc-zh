- en: Speech Encoder Decoder Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/247.ed9ebbad.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: The [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    can be used to initialize a speech-to-text model with any pretrained speech autoencoding
    model as the encoder (*e.g.* [Wav2Vec2](wav2vec2), [Hubert](hubert)) and any pretrained
    autoregressive model as the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of initializing speech-sequence-to-text-sequence models with
    pretrained checkpoints for speech recognition and speech translation has *e.g.*
    been shown in [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)
    by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.
  prefs: []
  type: TYPE_NORMAL
- en: An example of how to use a [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    for inference can be seen in [Speech2Text2](speech_to_text_2).
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initializing SpeechEncoderDecoderModel from model configurations.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    can be randomly initialized from an encoder and a decoder config. In the following
    example, we show how to do this using the default [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    configuration for the encoder and the default `BertForCausalLM` configuration
    for the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Initialising SpeechEncoderDecoderModel from a pretrained encoder and a pretrained
    decoder.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    can be initialized from a pretrained encoder checkpoint and a pretrained decoder
    checkpoint. Note that any pretrained Transformer-based speech model, *e.g.* [Wav2Vec2](wav2vec2),
    [Hubert](hubert) can serve as the encoder and both pretrained auto-encoding models,
    *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained
    decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used
    as the decoder. Depending on which architecture you choose as the decoder, the
    cross-attention layers might be randomly initialized. Initializing [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned
    on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder
    blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder). To do
    so, the `SpeechEncoderDecoderModel` class provides a [SpeechEncoderDecoderModel.from_encoder_decoder_pretrained()](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained)
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Loading an existing SpeechEncoderDecoderModel checkpoint and perform inference.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To load fine-tuned checkpoints of the `SpeechEncoderDecoderModel` class, [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    provides the `from_pretrained(...)` method just like any other model architecture
    in Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: To perform inference, one uses the `generate` method, which allows to autoregressively
    generate text. This method supports various forms of decoding, such as greedy,
    beam search and multinomial sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the model is created, it can be fine-tuned similar to BART, T5 or any
    other encoder-decoder model on a dataset of (speech, text) pairs. As you can see,
    only 2 inputs are required for the model in order to compute a loss: `input_values`
    (which are the speech inputs) and `labels` (which are the `input_ids` of the encoded
    target sequence).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: SpeechEncoderDecoderConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.SpeechEncoderDecoderConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: ( **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**kwargs** (*optional*) — Dictionary of keyword arguments. Notably:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — An instance of a configuration object that defines the encoder config.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder** ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — An instance of a configuration object that defines the decoder config.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)
    is the configuration class to store the configuration of a [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel).
    It is used to instantiate an Encoder Decoder model according to the specified
    arguments, defining the encoder and decoder configs.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### from_encoder_decoder_configs'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L93)'
  prefs: []
  type: TYPE_NORMAL
- en: '( encoder_config: PretrainedConfig decoder_config: PretrainedConfig **kwargs
    ) → [SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)
    (or a derived class) from a pre-trained encoder model configuration and decoder
    model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechEncoderDecoderModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.SpeechEncoderDecoderModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L172)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: Optional = None encoder: Optional = None decoder: Optional = None
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This class can be used to initialize a speech-sequence-to-text-sequence model
    with any pretrained speech autoencoding model as the encoder and any pretrained
    text autoregressive model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like summarization.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in [Large-Scale Self- and Semi-Supervised Learning for Speech
    Translation](https://arxiv.org/abs/2104.06678) it is shown how leveraging large
    pretrained speech models for speech translation yields a significant performance
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: After such an Speech-Encoder Decoder model has been trained/fine-tuned, it can
    be saved/loaded just like any other models (see the examples for more information).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with one of the base model classes of the library as encoder and another one as
    decoder when created with the :meth*~transformers.AutoModel.from_pretrained* class
    method for the encoder and :meth*~transformers.AutoModelForCausalLM.from_pretrained*
    class method for the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L438)'
  prefs: []
  type: TYPE_NORMAL
- en: '( inputs: Optional = None attention_mask: Optional = None decoder_input_ids:
    Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional
    = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None
    labels: Optional = None use_cache: Optional = None output_attentions: Optional
    = None output_hidden_states: Optional = None input_values: Optional = None input_features:
    Optional = None return_dict: Optional = None **kwargs ) → [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**inputs** (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or
    `(batch_size, sequence_length, feature_dim)`, *optional*) — Float values of input
    raw speech waveform or speech features. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `inputs`, either the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    or [Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_input_ids** (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For training, `decoder_input_ids` are automatically created by the model by
    shifting the `labels` to the right, replacing -100 by the `pad_token_id` and prepending
    them with the `decoder_start_token_id`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attention_mask** (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_outputs** (`tuple(torch.FloatTensor)`, *optional*) — This tuple must
    consist of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) is a tensor of hidden-states at the output of the last layer of
    the encoder. Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `decoder_input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss for the decoder. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_values** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Float values of input raw speech waveform. Values can be obtained
    by loading a *.flac* or *.wav* audio file into an array of type *List[float]*
    or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install soundfile*).
    To prepare the array into *input_values*, the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_features** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    feature_size)`, *optional*) — Float values of fbank features extracted from the
    raw speech waveform. Raw speech waveform can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of type `torch.FloatTensor`. See [**call**()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.Seq2SeqLMOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (*optional*) — Remaining dictionary of keyword arguments. Keyword
    arguments come in two flavors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a prefix which will be input as `**encoder_kwargs` for the encoder forward
    function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With a *decoder_* prefix which will be input as `**decoder_kwargs` for the decoder
    forward function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#### from_encoder_decoder_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L283)'
  prefs: []
  type: TYPE_NORMAL
- en: '( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path:
    str = None *model_args **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**encoder_pretrained_model_name_or_path** (`str`, *optional*) — Information
    necessary to initiate the encoder. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_pretrained_model_name_or_path** (`str`, *optional*, defaults to `None`)
    — Information necessary to initiate the decoder. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model_args** (remaining positional arguments, *optional*) — All remaning
    positional arguments will be passed to the underlying model’s `__init__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (remaining dictionary of keyword arguments, *optional*) — Can be
    used to update the configuration object (after it being loaded) and initiate the
    model (e.g., `output_attentions=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: The model is set in evaluation mode by default using `model.eval()` (Dropout
    modules are deactivated). To train the model, you need to first set it back in
    training mode with `model.train()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: FlaxSpeechEncoderDecoderModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.FlaxSpeechEncoderDecoderModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L328)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: SpeechEncoderDecoderConfig input_shape: Optional = None seed: int
    = 0 dtype: dtype = <class ''jax.numpy.float32''> _do_init: bool = True **kwargs
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dtype** (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`)
    — The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This class can be used to initialize a speech-sequence-to-text-sequence model
    with any pretrained speech autoencoding model as the encoder and any pretrained
    text autoregressive model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like summarization.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in [Large-Scale Self- and Semi-Supervised Learning for Speech
    Translation](https://arxiv.org/abs/2104.06678) it is shown how leveraging large
    pretrained speech models for speech translation yields a significant performance
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: After such an Speech-Encoder Decoder model has been trained/fine-tuned, it can
    be saved/loaded just like any other models (see the examples for more information).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[FlaxSpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with the module (flax.nn.Module) of one of the base model classes of the library
    as encoder module and another one as decoder module when created with the :meth*~transformers.FlaxAutoModel.from_pretrained*
    class method for the encoder and :meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*
    class method for the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L660)'
  prefs: []
  type: TYPE_NORMAL
- en: '( inputs: Array attention_mask: Optional = None decoder_input_ids: Optional
    = None decoder_attention_mask: Optional = None decoder_position_ids: Optional
    = None output_attentions: Optional = None output_hidden_states: Optional = None
    return_dict: Optional = None train: bool = False freeze_feature_encoder: bool
    = False params: dict = None dropout_rng: PRNGKey = None ) → [transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**inputs** (`jnp.ndarray` of shape `(batch_size, sequence_length)` or `(batch_size,
    sequence_length, feature_dim)`, *optional*) — Float values of input raw speech
    waveform or speech features. Values can be obtained by loading a `.flac` or `.wav`
    audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via
    the soundfile library (`pip install soundfile`). To prepare the array into `inputs`,
    either the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    or [Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_input_ids** (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For sequence to sequence training, `decoder_input_ids` should be provided. `decoder_input_ids`
    should be created outside of the model by shifting the `labels` to the right,
    replacing -100 by the `pad_token_id` and prepending them with the `decoder_start_token_id`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attention_mask** (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_position_ids** (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each decoder input sequence tokens in the
    position embeddings. Selected in the range `[0, config.decoder.max_position_embeddings
    - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.FlaxSeq2SeqLMOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**logits** (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(jnp.ndarray))`, *optional*, returned when
    `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(jnp.ndarray)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(jnp.ndarray)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(jnp.ndarray)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [FlaxSpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#### from_encoder_decoder_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L782)'
  prefs: []
  type: TYPE_NORMAL
- en: '( encoder_pretrained_model_name_or_path: Union = None decoder_pretrained_model_name_or_path:
    Union = None *model_args **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**encoder_pretrained_model_name_or_path** (`Union[str, os.PathLike]`, *optional*)
    — Information necessary to initiate the encoder. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_pretrained_model_name_or_path** (`Union[str, os.PathLike]`, *optional*,
    defaults to `None`) — Information necessary to initiate the decoder. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model_args** (remaining positional arguments, *optional*) — All remaning
    positional arguments will be passed to the underlying model’s `__init__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (remaining dictionary of keyword arguments, *optional*) — Can be
    used to update the configuration object (after it being loaded) and initiate the
    model (e.g., `output_attentions=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
