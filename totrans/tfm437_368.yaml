- en: Speech Encoder Decoder Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语音编码器解码器模型
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    can be used to initialize a speech-to-text model with any pretrained speech autoencoding
    model as the encoder (*e.g.* [Wav2Vec2](wav2vec2), [Hubert](hubert)) and any pretrained
    autoregressive model as the decoder.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)可用于使用任何预训练语音自编码模型作为编码器（例如[Wav2Vec2](wav2vec2)，[Hubert](hubert)）和任何预训练自回归模型作为解码器初始化语音到文本模型。'
- en: The effectiveness of initializing speech-sequence-to-text-sequence models with
    pretrained checkpoints for speech recognition and speech translation has *e.g.*
    been shown in [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)
    by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明使用预训练检查点初始化语音序列到文本序列模型，用于语音识别和语音翻译，例如在[Large-Scale Self- and Semi-Supervised
    Learning for Speech Translation](https://arxiv.org/abs/2104.06678)中由Changhan Wang，Anne
    Wu，Juan Pino，Alexei Baevski，Michael Auli，Alexis Conneau展示。
- en: An example of how to use a [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    for inference can be seen in [Speech2Text2](speech_to_text_2).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)进行推理的示例可以在[Speech2Text2](speech_to_text_2)中看到。
- en: Randomly initializing SpeechEncoderDecoderModel from model configurations.
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从模型配置随机初始化SpeechEncoderDecoderModel。
- en: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    can be randomly initialized from an encoder and a decoder config. In the following
    example, we show how to do this using the default [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    configuration for the encoder and the default `BertForCausalLM` configuration
    for the decoder.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)可以从编码器和解码器配置随机初始化。在以下示例中，我们展示了如何使用默认的[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)配置作为编码器和默认的`BertForCausalLM`配置作为解码器。'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Initialising SpeechEncoderDecoderModel from a pretrained encoder and a pretrained
    decoder.
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从预训练的编码器和预训练的解码器初始化SpeechEncoderDecoderModel。
- en: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    can be initialized from a pretrained encoder checkpoint and a pretrained decoder
    checkpoint. Note that any pretrained Transformer-based speech model, *e.g.* [Wav2Vec2](wav2vec2),
    [Hubert](hubert) can serve as the encoder and both pretrained auto-encoding models,
    *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained
    decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used
    as the decoder. Depending on which architecture you choose as the decoder, the
    cross-attention layers might be randomly initialized. Initializing [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned
    on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder
    blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder). To do
    so, the `SpeechEncoderDecoderModel` class provides a [SpeechEncoderDecoderModel.from_encoder_decoder_pretrained()](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained)
    method.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)可以从预训练的编码器检查点和预训练的解码器检查点初始化。请注意，任何预训练的基于Transformer的语音模型，例如[Wav2Vec2](wav2vec2)，[Hubert](hubert)都可以作为编码器，以及预训练的自编码模型，例如BERT，预训练的因果语言模型，例如GPT2，以及序列到序列模型的预训练解码器部分，例如BART的解码器，都可以作为解码器。根据您选择的解码器架构，交叉注意力层可能会被随机初始化。从预训练的编码器和解码器检查点初始化[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)需要对模型进行下游任务微调，正如在*Warm-starting-encoder-decoder
    blog post*中所示。为此，`SpeechEncoderDecoderModel`类提供了一个[SpeechEncoderDecoderModel.from_encoder_decoder_pretrained()](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained)方法。'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading an existing SpeechEncoderDecoderModel checkpoint and perform inference.
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载现有的SpeechEncoderDecoderModel检查点并执行推理。
- en: To load fine-tuned checkpoints of the `SpeechEncoderDecoderModel` class, [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    provides the `from_pretrained(...)` method just like any other model architecture
    in Transformers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载`SpeechEncoderDecoderModel`类的微调检查点，[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)提供了`from_pretrained(...)`方法，就像Transformers中的任何其他模型架构一样。
- en: To perform inference, one uses the `generate` method, which allows to autoregressively
    generate text. This method supports various forms of decoding, such as greedy,
    beam search and multinomial sampling.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行推理，可以使用`generate`方法，该方法允许自回归生成文本。此方法支持各种解码形式，例如贪婪、束搜索和多项式采样。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Training
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Once the model is created, it can be fine-tuned similar to BART, T5 or any
    other encoder-decoder model on a dataset of (speech, text) pairs. As you can see,
    only 2 inputs are required for the model in order to compute a loss: `input_values`
    (which are the speech inputs) and `labels` (which are the `input_ids` of the encoded
    target sequence).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型后，可以像BART、T5或任何其他编码器解码器模型一样对（语音，文本）对数据集进行微调。如您所见，模型只需要2个输入才能计算损失：`input_values`（语音输入）和`labels`（编码目标序列的`input_ids`）。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: SpeechEncoderDecoderConfig
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SpeechEncoderDecoderConfig
- en: '### `class transformers.SpeechEncoderDecoderConfig`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SpeechEncoderDecoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L26)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L26)'
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments. Notably:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（*可选*）— 关键字参数的字典。特别是：'
- en: '`encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — An instance of a configuration object that defines the encoder config.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）—
    定义编码器配置的配置对象的实例。'
- en: '`decoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — An instance of a configuration object that defines the decoder config.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）—
    定义解码器配置的配置对象的实例。'
- en: '[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)
    is the configuration class to store the configuration of a [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel).
    It is used to instantiate an Encoder Decoder model according to the specified
    arguments, defining the encoder and decoder configs.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)是用于存储[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)配置的配置类。根据指定的参数实例化一个编码器解码器模型，定义编码器和解码器配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `from_encoder_decoder_configs`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L93)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L93)'
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Returns
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)'
- en: An instance of a configuration object
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的实例
- en: Instantiate a [SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)
    (or a derived class) from a pre-trained encoder model configuration and decoder
    model configuration.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的编码器模型配置和解码器模型配置实例化一个[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)（或派生类）。
- en: SpeechEncoderDecoderModel
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SpeechEncoderDecoderModel
- en: '### `class transformers.SpeechEncoderDecoderModel`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SpeechEncoderDecoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L172)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L172)'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: This class can be used to initialize a speech-sequence-to-text-sequence model
    with any pretrained speech autoencoding model as the encoder and any pretrained
    text autoregressive model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like summarization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此类可用于初始化一个具有任何预训练语音自编码模型作为编码器和任何预训练文本自回归模型作为解码器的语音序列到文本序列模型。编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载，解码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou,
    Wei Li, Peter J. Liu](https://arxiv.org/abs/1907.12461)的研究中展示了使用预训练检查点初始化序列到序列模型对序列生成任务的有效性。
- en: Additionally, in [Large-Scale Self- and Semi-Supervised Learning for Speech
    Translation](https://arxiv.org/abs/2104.06678) it is shown how leveraging large
    pretrained speech models for speech translation yields a significant performance
    improvement.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)中展示了如何利用大型预训练语音模型进行语音翻译可以显著提高性能。
- en: After such an Speech-Encoder Decoder model has been trained/fine-tuned, it can
    be saved/loaded just like any other models (see the examples for more information).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练/微调了这样一个语音编码器解码器模型之后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库实现的所有模型的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with one of the base model classes of the library as encoder and another one as
    decoder when created with the :meth*~transformers.AutoModel.from_pretrained* class
    method for the encoder and :meth*~transformers.AutoModelForCausalLM.from_pretrained*
    class method for the decoder.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)是一个通用模型类，当使用:meth*~transformers.AutoModel.from_pretrained*类方法为编码器创建一个库的基本模型类，并使用:meth*~transformers.AutoModelForCausalLM.from_pretrained*类方法为解码器创建一个transformer架构时，将被实例化。'
- en: '#### `forward`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L438)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L438)'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size,
    sequence_length, feature_dim)`, *optional*) — Float values of input raw speech
    waveform or speech features. Values can be obtained by loading a `.flac` or `.wav`
    audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via
    the soundfile library (`pip install soundfile`). To prepare the array into `inputs`,
    either the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    or [Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    feature_dim)`的`torch.FloatTensor`，*可选*）— 输入原始语音波形或语音特征的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，*例如*通过soundfile库（`pip
    install soundfile`）。要将数组准备为`inputs`，应使用[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)或[Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于“未屏蔽”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于“屏蔽”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则只需输入最后的`decoder_input_ids`（请参阅`past_key_values`）。
- en: For training, `decoder_input_ids` are automatically created by the model by
    shifting the `labels` to the right, replacing -100 by the `pad_token_id` and prepending
    them with the `decoder_start_token_id`.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于训练，模型会通过将`labels`向右移动，用`pad_token_id`替换-100，并在其前面加上`decoder_start_token_id`来自动创建`decoder_input_ids`。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: '`encoder_outputs` (`tuple(torch.FloatTensor)`, *optional*) — This tuple must
    consist of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) is a tensor of hidden-states at the output of the last layer of
    the encoder. Used in the cross-attention of the decoder.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(torch.FloatTensor)`, *optional*) — 此元组必须包含（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`（形状为`(batch_size,
    sequence_length, hidden_size)`的`torch.FloatTensor`）是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将其过去键值状态提供给此模型的）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `decoder_input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果您想要更多控制权来将`decoder_input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss for the decoder. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算解码器的掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`内（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`中的标记。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Float values of input raw speech waveform. Values can be obtained
    by loading a *.flac* or *.wav* audio file into an array of type *List[float]*
    or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install soundfile*).
    To prepare the array into *input_values*, the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 输入原始语音波形的浮点值。可以通过将*.flac*或*.wav*音频文件加载到*List[float]*或*numpy.ndarray*类型的数组中来获取值，例如通过soundfile库（*pip
    install soundfile*）。要准备数组为*input_values*，应使用[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)进行填充和转换为*torch.FloatTensor*类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    feature_size)`, *optional*) — Float values of fbank features extracted from the
    raw speech waveform. Raw speech waveform can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    feature_size)`, *optional*) — 从原始语音波形中提取的fbank特征的浮点值。原始语音波形可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip
    install soundfile`）。要准备好数组为`input_features`，应使用[Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)来提取fbank特征，填充并转换为`torch.FloatTensor`类型的张量。参见[`call`()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.Seq2SeqLMOutput` instead of a plain tuple.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回一个`~utils.Seq2SeqLMOutput`而不是一个普通元组。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments. Keyword
    arguments come in two flavors:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种类型：'
- en: Without a prefix which will be input as `**encoder_kwargs` for the encoder forward
    function.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编码器前向函数中作为`**encoder_kwargs`输入的前缀。
- en: With a *decoder_* prefix which will be input as `**decoder_kwargs` for the decoder
    forward function.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在解码器前向函数中作为`**decoder_kwargs`输入的*decoder_*前缀。
- en: Returns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    and inputs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads,
    encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个层的解码器隐藏状态加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size,
    num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每个层的输出的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#### `from_encoder_decoder_pretrained`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L283)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L283)'
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_pretrained_model_name_or_path` (`str`, *optional*) — Information necessary
    to initiate the encoder. Can be either:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_pretrained_model_name_or_path` (`str`, *optional*) — 初始化编码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在huggingface.co模型库中的预训练模型的*模型id*。有效的模型id可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存的模型权重，例如，`./my_model_directory/`。
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*tensorflow索引检查点文件*的路径或url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用提供的转换脚本将TensorFlow检查点转换为PyTorch模型并加载PyTorch模型的加载路径比较慢。
- en: '`decoder_pretrained_model_name_or_path` (`str`, *optional*, defaults to `None`)
    — Information necessary to initiate the decoder. Can be either:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_pretrained_model_name_or_path` (`str`, *optional*, 默认为`None`) — 初始化解码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在huggingface.co模型库中的预训练模型的*模型id*。有效的模型id可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存的模型权重，例如，`./my_model_directory/`。
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*tensorflow索引检查点文件*的路径或url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用提供的转换脚本将TensorFlow检查点转换为PyTorch模型并加载PyTorch模型的加载路径比较慢。
- en: '`model_args` (remaining positional arguments, *optional*) — All remaning positional
    arguments will be passed to the underlying model’s `__init__` method.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`（剩余的位置参数，*optional*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。'
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) — Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（剩余的关键字参数字典，*可选*）-可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。'
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新编码器配置，请为每个配置参数使用前缀*encoder_*。
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新解码器配置，请为每个配置参数使用前缀*decoder_*。
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新父模型配置，请不要为每个配置参数使用前缀。
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据是否提供`config`，行为会有所不同或自动加载。
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型检查点的库中实例化一个编码器和一个解码器。
- en: The model is set in evaluation mode by default using `model.eval()` (Dropout
    modules are deactivated). To train the model, you need to first set it back in
    training mode with `model.train()`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用`model.eval()`将模型设置为评估模式（Dropout模块被停用）。要训练模型，首先需要使用`model.train()`将其设置回训练模式。
- en: 'Example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: FlaxSpeechEncoderDecoderModel
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxSpeechEncoderDecoderModel
- en: '### `class transformers.FlaxSpeechEncoderDecoderModel`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxSpeechEncoderDecoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L328)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L328)'
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig））-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）-计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的数据类型执行。
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，这仅指定了计算的数据类型，并不影响模型参数的数据类型。
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改模型参数的数据类型，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。
- en: This class can be used to initialize a speech-sequence-to-text-sequence model
    with any pretrained speech autoencoding model as the encoder and any pretrained
    text autoregressive model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like summarization.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此类可用于使用任何预训练语音自编码模型作为编码器和任何预训练文本自回归模型作为解码器初始化语音序列到文本序列模型。编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载，解码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou,
    Wei Li, Peter J. Liu](https://arxiv.org/abs/1907.12461)的论文中展示了使用预训练检查点初始化序列到序列模型以进行序列生成任务的有效性。
- en: Additionally, in [Large-Scale Self- and Semi-Supervised Learning for Speech
    Translation](https://arxiv.org/abs/2104.06678) it is shown how leveraging large
    pretrained speech models for speech translation yields a significant performance
    improvement.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)中展示了如何利用大型预训练语音模型进行语音翻译，从而实现显著的性能提升。
- en: After such an Speech-Encoder Decoder model has been trained/fine-tuned, it can
    be saved/loaded just like any other models (see the examples for more information).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练/微调了这样一个语音编码器解码器模型之后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。检查超类文档以了解库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个Flax亚麻[flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规的Flax模块，并参考Flax文档以获取有关一般用法和行为的所有相关信息。
- en: '[FlaxSpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with the module (flax.nn.Module) of one of the base model classes of the library
    as encoder module and another one as decoder module when created with the :meth*~transformers.FlaxAutoModel.from_pretrained*
    class method for the encoder and :meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*
    class method for the decoder.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxSpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel)是一个通用的模型类，当使用:meth*~transformers.FlaxAutoModel.from_pretrained*类方法为编码器创建时，将实例化为一个变压器架构，其中模块（flax.nn.Module）是库的一个基本模型类的编码器模块，另一个是解码器模块，为解码器创建时使用:meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*类方法。'
- en: '#### `__call__`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L660)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L660)'
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`jnp.ndarray` of shape `(batch_size, sequence_length)` or `(batch_size,
    sequence_length, feature_dim)`, *optional*) — Float values of input raw speech
    waveform or speech features. Values can be obtained by loading a `.flac` or `.wav`
    audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via
    the soundfile library (`pip install soundfile`). To prepare the array into `inputs`,
    either the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    or [Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    feature_dim)`的`jnp.ndarray`，*可选*）— 输入原始语音波形或语音特征的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，*例如*通过soundfile库（`pip
    install soundfile`）。要将数组准备成`inputs`，应使用[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)或[Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`jnp.ndarray`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。 '
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: For sequence to sequence training, `decoder_input_ids` should be provided. `decoder_input_ids`
    should be created outside of the model by shifting the `labels` to the right,
    replacing -100 by the `pad_token_id` and prepending them with the `decoder_start_token_id`.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于序列到序列训练，应提供`decoder_input_ids`。`decoder_input_ids`应在模型外部创建，方法是将`labels`向右移动，用`pad_token_id`替换-100，并在`decoder_start_token_id`之前添加它们。
- en: '`decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`jnp.ndarray`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: '`decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each decoder input sequence tokens in the
    position embeddings. Selected in the range `[0, config.decoder.max_position_embeddings
    - 1]`.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）—
    每个解码器输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.decoder.max_position_embeddings - 1]`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.FlaxSeq2SeqLMOutput` instead of a plain tuple.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 如果设置为`True`，模型将返回一个`~utils.FlaxSeq2SeqLMOutput`而不是一个普通元组。'
- en: Returns
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig))
    and inputs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig)）和输入。
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — Tuple of `tuple(jnp.ndarray)` of
    length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器每一层输出的隐藏状态加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*) — 模型编码器最后一层输出的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器每一层输出的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [FlaxSpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxSpeechEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。
- en: 'Examples:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#### `from_encoder_decoder_pretrained`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L782)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L782)'
- en: '[PRE15]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *optional*)
    — Information necessary to initiate the encoder. Can be either:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_pretrained_model_name_or_path`（`Union[str, os.PathLike]`，*可选*） — 初始化编码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在huggingface.co模型存储库内的预训练模型的*模型ID*。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)保存的模型权重，例如，`./my_model_directory/`。
- en: '`decoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *optional*,
    defaults to `None`) — Information necessary to initiate the decoder. Can be either:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_pretrained_model_name_or_path`（`Union[str, os.PathLike]`，*可选*，默认为`None`）
    — 初始化解码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在huggingface.co模型存储库内的预训练模型的*模型ID*。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)保存的模型权重，例如，`./my_model_directory/`。
- en: '`model_args` (remaining positional arguments, *optional*) — All remaning positional
    arguments will be passed to the underlying model’s `__init__` method.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`（剩余的位置参数，*可选*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。'
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) — Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（在加载后）并初始化模型（例如，`output_attentions=True`）。'
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新编码器配置，请为每个配置参数使用前缀*encoder_*。
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新解码器配置，请为每个配置参数使用前缀*decoder_*。
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新父模型配置，请不要为每个配置参数使用前缀。
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据是否提供`config`或自动加载而表现不同。
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型检查点的一个或两个库基类实例化编码器和解码器。
- en: 'Example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
