# 如何将🤗 Transformers 模型转换为 TensorFlow？

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/add_tensorflow_model`](https://huggingface.co/docs/transformers/v4.37.2/en/add_tensorflow_model)

在🤗 Transformers 中有多个可用的框架可以使用，这使您在设计应用程序时可以灵活发挥其优势，但这意味着必须根据每个模型添加兼容性。好消息是，将 TensorFlow 兼容性添加到现有模型比从头开始添加新模型更简单！无论您希望更深入地了解大型 TensorFlow 模型，做出重大的开源贡献，还是为您选择的模型启用 TensorFlow，本指南都适合您。

本指南授权您，我们社区的一员，贡献 TensorFlow 模型权重和/或架构，以供在🤗 Transformers 中使用，几乎不需要 Hugging Face 团队的监督。编写一个新模型并不是一件小事，但希望这个指南能让它不再像坐过山车🎢那样，而更像在公园里散步🚶。利用我们的集体经验绝对是使这个过程变得更加容易的关键，因此我们强烈鼓励您对本指南提出改进建议！

在深入研究之前，建议您查看以下资源，如果您对🤗 Transformers 还不熟悉：

+   🤗 Transformers 的概述

+   [拥抱面的 TensorFlow 哲学](https://huggingface.co/blog/tensorflow-philosophy)

在本指南的其余部分，您将学习添加新的 TensorFlow 模型架构所需的内容，将 PyTorch 转换为 TensorFlow 模型权重的过程，以及如何有效地调试跨 ML 框架的不匹配。让我们开始吧！

您是否不确定您想要使用的模型是否已经有相应的 TensorFlow 架构？

检查您选择的模型的`config.json`文件中的`model_type`字段（[示例](https://huggingface.co/bert-base-uncased/blob/main/config.json#L14)）。如果🤗 Transformers 中相应的模型文件夹有一个以“modeling_tf”开头的文件，这意味着它有一个相应的 TensorFlow 架构（[示例](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)）。

## 逐步指南添加 TensorFlow 模型架构代码

设计大型模型架构的方法有很多，实现该设计的方式也有多种。然而，您可能还记得我们在🤗 Transformers 的概述中提到，我们是一个有主见的团队 - 🤗 Transformers 的易用性依赖于一致的设计选择。从经验中，我们可以告诉您一些关于添加 TensorFlow 模型的重要事项：

+   不要重复造轮子！往往至少有两个参考实现您应该检查：您正在实现的模型的 PyTorch 等效版本以及同一类问题的其他 TensorFlow 模型。

+   出色的模型实现经得起时间的考验。这不是因为代码漂亮，而是因为代码清晰，易于调试和构建。如果您通过在 TensorFlow 实现中复制其他 TensorFlow 模型中的相同模式并最小化与 PyTorch 实现的不匹配，使维护者的生活变得轻松，您就确保您的贡献将长期存在。

+   当遇到困难时寻求帮助！🤗 Transformers 团队在这里帮助您，我们可能已经找到了您面临的相同问题的解决方案。

以下是添加 TensorFlow 模型架构所需步骤的概述：

1.  选择您希望转换的模型

1.  准备 transformers 开发环境

1.  （可选）理解理论方面和现有实现

1.  实现模型架构

1.  实现模型测试

1.  提交拉取请求

1.  （可选）构建演示并与世界分享

### 1.-3\. 准备您的模型贡献

**1\.选择要转换的模型**

让我们从基础知识开始：您需要了解要转换的架构。如果您没有特定的架构，向🤗 Transformers 团队寻求建议是最大化影响的好方法 - 我们将指导您选择在 TensorFlow 方面缺失的最突出的架构。如果您想要在 TensorFlow 中使用的特定模型已经在🤗 Transformers 中具有 TensorFlow 架构实现，但缺少权重，请随时直接转到本页的添加 TensorFlow 权重到 hub 部分。

为简单起见，本指南的其余部分假定您已决定使用 TensorFlow 版本的*BrandNewBert*（与指南中添加新模型的示例相同）做出贡献。

在开始 TensorFlow 模型架构的工作之前，请仔细检查是否有正在进行的工作。您可以在[拉取请求 GitHub 页面](https://github.com/huggingface/transformers/pulls?q=is%3Apr)上搜索`BrandNewBert`以确认是否有与 TensorFlow 相关的拉取请求。

**2\.准备 transformers 开发环境**

选择模型架构后，打开一个草稿 PR 以表示您打算进行工作。按照以下说明设置您的环境并打开草稿 PR。

1.  通过单击存储库页面上的“Fork”按钮来分叉[存储库](https://github.com/huggingface/transformers)。这将在您的 GitHub 用户帐户下创建代码副本。

1.  将您的`transformers`分支克隆到本地磁盘，并将基础存储库添加为远程存储库：

```py
git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git
```

1.  建立一个开发环境，例如通过运行以下命令：

```py
python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"
```

根据您的操作系统，由于 Transformers 的可选依赖项数量正在增加，您可能会在此命令中失败。如果是这种情况，请确保安装 TensorFlow，然后执行：

```py
pip install -e ".[quality]"
```

**注意：**您不需要安装 CUDA。使新模型在 CPU 上运行就足够了。

1.  从主分支创建一个具有描述性名称的分支

```py
git checkout -b add_tf_brand_new_bert
```

1.  获取并将当前主分支重新设置为基础

```py
git fetch upstream
git rebase upstream/main
```

1.  在`transformers/src/models/brandnewbert/`中添加一个名为`modeling_tf_brandnewbert.py`的空`.py`文件。这将是您的 TensorFlow 模型文件。

1.  使用以下命令将更改推送到您的帐户：

```py
git add .
git commit -m "initial commit"
git push -u origin add_tf_brand_new_bert
```

1.  一旦您满意了，转到 GitHub 上您的分支的网页。点击“拉取请求”。确保将 Hugging Face 团队的一些成员的 GitHub 句柄添加为审阅者，以便 Hugging Face 团队在未来的更改中收到通知。

1.  通过单击 GitHub 拉取请求网页右侧的“转换为草稿”将 PR 更改为草稿。

现在您已经设置了一个开发环境，可以将*BrandNewBert*移植到🤗 Transformers 中的 TensorFlow 中。

**3\.（可选）了解理论方面和现有实现**

您应该花一些时间阅读*BrandNewBert*的论文，如果存在这样的描述性工作。论文中可能有一些难以理解的大段内容。如果是这种情况，没关系 - 不要担心！目标不是深入理解论文的理论，而是提取在🤗 Transformers 中使用 TensorFlow 有效重新实现模型所需的必要信息。也就是说，您不必花太多时间在理论方面，而是要专注于实践方面，即现有模型文档页面（例如 BERT 的模型文档）。

在掌握了即将实现的模型的基础知识之后，了解现有的实现是很重要的。这是确认工作实现是否符合您对模型的期望，以及预见 TensorFlow 方面的技术挑战的绝佳机会。

你感到不知所措刚刚吸收了大量信息是非常自然的。在这个阶段，你并不需要理解模型的所有方面。尽管如此，我们强烈鼓励你在我们的[论坛](https://discuss.huggingface.co/)中解决任何紧迫的问题。

### 4\. 模型实现

现在是时候开始编码了。我们建议的起点是 PyTorch 文件本身：将`modeling_brand_new_bert.py`的内容复制到`src/transformers/models/brand_new_bert/`中的`modeling_tf_brand_new_bert.py`。本节的目标是修改文件并更新🤗 Transformers 的导入结构，以便你可以成功导入`TFBrandNewBert`和`TFBrandNewBert.from_pretrained(model_repo, from_pt=True)`，从而加载一个可工作的 TensorFlow *BrandNewBert*模型。

遗憾的是，没有将 PyTorch 模型转换为 TensorFlow 的规定。但是，你可以遵循我们的一些提示，使这个过程尽可能顺利：

+   将所有类的名称前加上`TF`（例如，`BrandNewBert`变为`TFBrandNewBert`）。

+   大多数 PyTorch 操作都有直接的 TensorFlow 替代。例如，`torch.nn.Linear`对应于`tf.keras.layers.Dense`，`torch.nn.Dropout`对应于`tf.keras.layers.Dropout`等。如果对特定操作不确定，可以使用[TensorFlow 文档](https://www.tensorflow.org/api_docs/python/tf)或[PyTorch 文档](https://pytorch.org/docs/stable/)。

+   在🤗 Transformers 代码库中寻找模式。如果你遇到某个操作没有直接替代，那么很可能有其他人已经遇到了同样的问题。

+   默认情况下，保持与 PyTorch 相同的变量名称和结构。这将使调试、跟踪问题和添加修复更容易。

+   一些层在每个框架中具有不同的默认值。一个显著的例子是批量归一化层的 epsilon（在[PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)中为`1e-5`，在[TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)中为`1e-3`）。务必仔细检查文档！

+   PyTorch 的`nn.Parameter`变量通常需要在 TF Layer 的`build()`中初始化。参见以下示例：[PyTorch](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_vit_mae.py#L212) / [TensorFlow](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L220)

+   如果 PyTorch 模型在函数顶部有`#copied from ...`，那么你的 TensorFlow 模型很可能也可以从被复制的架构中借用该函数，假设它有一个 TensorFlow 架构。

+   在 TensorFlow 函数中正确设置`name`属性对于进行`from_pt=True`权重交叉加载至关重要。`name`几乎总是 PyTorch 代码中相应变量的名称。如果`name`没有正确设置，加载模型权重时会在错误消息中看到。

+   基础模型类`BrandNewBertModel`的逻辑实际上将驻留在`TFBrandNewBertMainLayer`中，这是一个 Keras 层子类（[示例](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L719)）。`TFBrandNewBertModel`将简单地是这个层的包装器。

+   Keras 模型需要被构建以加载预训练权重。因此，`TFBrandNewBertPreTrainedModel`将需要保存模型的输入示例，即`dummy_inputs`（[示例](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L916)）。

+   如果遇到困难，请寻求帮助 - 我们在这里帮助你！🤗

除了模型文件本身，您还需要添加指向模型类和相关文档页面的指针。您可以完全按照其他 PR 中的模式完成此部分（[示例](https://github.com/huggingface/transformers/pull/18020/files)）。以下是所需手动更改的列表：

+   在 `src/transformers/__init__.py` 中包含 *BrandNewBert* 的所有公共类

+   在 `src/transformers/models/auto/modeling_tf_auto.py` 中将 *BrandNewBert* 类添加到相应的 Auto 类中

+   在 `src/transformers/utils/dummy_tf_objects.py` 中添加与 *BrandNewBert* 相关的延迟加载类

+   更新 `src/transformers/models/brand_new_bert/__init__.py` 中公共类的导入结构

+   在 `docs/source/en/model_doc/brand_new_bert.md` 中为 *BrandNewBert* 的公共方法添加文档指针

+   在 `docs/source/en/model_doc/brand_new_bert.md` 中将自己添加到 *BrandNewBert* 的贡献者列表中

+   最后，在 `docs/source/en/index.md` 中 *BrandNewBert* 的 TensorFlow 列中添加一个绿色勾 ✅

当您对实现感到满意时，请运行以下检查表以确认您的模型架构已准备就绪：

1.  在训练时行为不同的所有层（例如 Dropout）都使用 `training` 参数调用，并且该参数从顶层类一直传播下去

1.  在可能的情况下，您已经使用了 `#copied from ...`

1.  `TFBrandNewBertMainLayer` 和所有使用它的类都将其 `call` 函数装饰为 `@unpack_inputs`

1.  `TFBrandNewBertMainLayer` 被装饰为 `@keras_serializable`

1.  可以使用 `TFBrandNewBert.from_pretrained(model_repo, from_pt=True)` 从 PyTorch 权重加载 TensorFlow 模型

1.  您可以使用预期的输入格式调用 TensorFlow 模型

### 5\. 添加模型测试

万岁，您已经实现了一个 TensorFlow 模型！现在是添加测试以确保您的模型表现如预期的时候了。与前一节一样，我们建议您首先将 `tests/models/brand_new_bert/` 中的 `test_modeling_brand_new_bert.py` 文件复制到 `test_modeling_tf_brand_new_bert.py` 中，然后继续进行必要的 TensorFlow 替换。目前，在所有 `.from_pretrained()` 调用中，您应该使用 `from_pt=True` 标志来加载现有的 PyTorch 权重。

完成后，是真相时刻：运行测试！😬

```py
NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py
```

最有可能的结果是您会看到一堆错误。不要担心，这是正常的！调试 ML 模型是非常困难的，成功的关键因素是耐心（和 `breakpoint()`）。根据我们的经验，最困难的问题来自于 ML 框架之间的微妙不匹配，我们在本指南末尾提供了一些指针。在其他情况下，一般测试可能不直接适用于您的模型，这种情况下，我们建议在模型测试类级别进行覆盖。无论问题是什么，请不要犹豫在您的草稿拉取请求中寻求帮助，如果您遇到困难。

当所有测试通过时，恭喜，您的模型几乎可以添加到 🤗 Transformers 库中了！🎉

### 6.-7\. 确保每个人都可以使用您的模型

**6\. 提交拉取请求**

完成实现和测试后，现在是提交拉取请求的时候了。在推送代码之前，请运行我们的代码格式化工具 `make fixup` 🪄。这将自动修复任何格式问题，否则会导致我们的自动检查失败。

现在是将草稿拉取请求转换为真正拉取请求的时候了。为此，请点击“准备好审查”按钮，并将 Joao (`@gante`) 和 Matt (`@Rocketknight1`) 添加为审阅者。模型拉取请求将需要至少 3 名审阅者，但他们会负责为您的模型找到合适的额外审阅者。

在所有审阅者对您的 PR 的状态满意后，最后一个行动点是在 `.from_pretrained()` 调用中移除 `from_pt=True` 标志。由于没有 TensorFlow 权重，您将需要添加它们！查看下面的部分以获取如何执行此操作的说明。

最后，当 TensorFlow 权重合并时，你至少有 3 个审阅者的批准，并且所有 CI 检查都是绿色的时候，最后再本地再次检查测试

```py
NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py
```

我们将合并你的 PR！祝贺你达到的里程碑🎉

**7\. （可选）构建演示并与世界分享**

开源项目中最困难的部分之一是发现。其他用户如何了解你出色的 TensorFlow 贡献的存在？当然是通过适当的沟通！

有两种主要的方法可以与社区分享你的模型：

+   构建演示。这些包括 Gradio 演示、笔记本和其他有趣的方式来展示你的模型。我们强烈鼓励你向我们的[社区驱动演示](https://huggingface.co/docs/transformers/community)添加一个笔记本。

+   在社交媒体上分享故事，比如 Twitter 和 LinkedIn。你应该为自己的工作感到自豪，并与社区分享你的成就 - 你的模型现在可以被全球数千名工程师和研究人员使用！我们将很乐意转发你的帖子，并帮助你与社区分享你的工作。

## 将 TensorFlow 权重添加到🤗 Hub

假设 TensorFlow 模型架构在🤗 Transformers 中可用，将 PyTorch 权重转换为 TensorFlow 权重将变得轻而易举！

以下是如何做到这一点：

1.  确保你已经在终端中登录到你的 Hugging Face 账户。你可以使用命令`huggingface-cli login`登录（你可以在[这里](https://huggingface.co/settings/tokens)找到你的访问令牌）

1.  运行`transformers-cli pt-to-tf --model-name foo/bar`，其中`foo/bar`是包含你想要转换的 PyTorch 权重的模型存储库的名称

1.  在🤗 Hub PR 中标记`@joaogante`和`@Rocketknight1`，这是上面命令创建的

就是这样！

## 跨 ML 框架调试不匹配🐛

在添加新架构或为现有架构创建 TensorFlow 权重时，你可能会遇到关于 PyTorch 和 TensorFlow 之间不匹配的错误。你甚至可能决定打开两个框架的模型架构代码，并发现它们看起来是相同的。发生了什么？

首先，让我们谈谈为什么理解这些不匹配很重要。许多社区成员将直接使用🤗 Transformers 模型，并相信我们的模型表现如预期。当两个框架之间存在较大的不匹配时，这意味着模型至少在一个框架中没有遵循参考实现。这可能导致悄无声息的失败，即模型运行但性能不佳。这可能比根本无法运行的模型更糟！因此，我们的目标是在模型的所有阶段都有小于`1e-5`的框架不匹配。

就像其他数值问题一样，魔鬼就在细节中。就像任何注重细节的工艺一样，耐心是秘密的关键。以下是我们建议的工作流程，当你遇到这种类型的问题时：

1.  找到不匹配的源头。你要转换的模型可能在某个特定点上有几乎相同的内部变量。在两个框架的架构中放置`breakpoint()`语句，并以自上而下的方式比较数值变量的值，直到找到问题的源头。

1.  现在你已经找到了问题的源头，请与🤗 Transformers 团队联系。可能我们之前见过类似的问题，并且可以迅速提供解决方案。作为备选方案，浏览像 StackOverflow 和 GitHub 问题这样的热门页面。

1.  如果看不到解决方案，这意味着你将不得不深入研究。好消息是你已经找到了问题所在，所以你可以专注于有问题的指令，将模型的其余部分抽象出来！坏消息是你将不得不深入研究该指令的源实现。在某些情况下，你可能会发现参考实现存在问题 - 不要放弃在上游存储库中开启问题。

在某些情况下，在与🤗 Transformers 团队讨论后，我们可能会发现修复不匹配是不可行的。当模型的输出层中不匹配非常小（但在隐藏状态中可能很大）时，我们可能会决定忽略它，以便分发模型。上面提到的`pt-to-tf` CLI 具有一个`--max-error`标志，可以在权重转换时覆盖错误消息。
