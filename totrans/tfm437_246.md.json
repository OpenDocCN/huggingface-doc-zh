["```py\n( activation_dropout: Optional = 0.1 activation_function: Union = 'gelu' vocab_size: Optional = 30522 hidden_size: Optional = 1024 encoder_ffn_dim: Optional = 4096 num_encoder_layers: Optional = 12 num_encoder_attention_heads: Optional = 16 decoder_ffn_dim: Optional = 4096 num_decoder_layers: Optional = 12 num_decoder_attention_heads: Optional = 16 attention_dropout: Optional = 0.1 dropout: Optional = 0.1 max_position_embeddings: Optional = 512 init_std: Optional = 0.02 is_encoder_decoder: Optional = True add_cross_attention: Optional = True decoder_start_token_id: Optional = 0 ngram: Optional = 2 num_buckets: Optional = 32 relative_max_distance: Optional = 128 disable_ngram_loss: Optional = False eps: Optional = 0.0 use_cache: Optional = True pad_token_id: Optional = 0 bos_token_id: Optional = 1 eos_token_id: Optional = 2 **kwargs )\n```", "```py\n( vocab_file bos_token = '[SEP]' eos_token = '[SEP]' sep_token = '[SEP]' unk_token = '[UNK]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: XLMProphetNetConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetSeq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLMProphetNetModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> model = XLMProphetNetModel.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state  # main stream hidden states\n>>> last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states\n```", "```py\n( config: XLMProphetNetConfig word_embeddings: Embedding = None )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLMProphetNetEncoder\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> model = XLMProphetNetEncoder.from_pretrained(\"patrickvonplaten/prophetnet-large-uncased-standalone\")\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: XLMProphetNetConfig word_embeddings: Optional = None )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetDecoderModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLMProphetNetDecoder\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> model = XLMProphetNetDecoder.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\", add_cross_attention=False)\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: XLMProphetNetConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLMProphetNetForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> model = XLMProphetNetForConditionalGeneration.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n\n>>> logits_next_token = outputs.logits  # logits to predict next token as usual\n>>> logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens\n```", "```py\n( config: XLMProphetNetConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetDecoderLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLMProphetNetForCausalLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> model = XLMProphetNetForCausalLM.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n\n>>> # Model can also be used with EncoderDecoder framework\n>>> from transformers import BertTokenizer, EncoderDecoderModel, AutoTokenizer\n>>> import torch\n\n>>> tokenizer_enc = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n>>> tokenizer_dec = AutoTokenizer.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"bert-large-uncased\", \"patrickvonplaten/xprophetnet-large-uncased-standalone\"\n... )\n\n>>> ARTICLE = (\n...     \"the us state department said wednesday it had received no \"\n...     \"formal word from bolivia that it was expelling the us ambassador there \"\n...     \"but said the charges made against him are `` baseless .\"\n... )\n>>> input_ids = tokenizer_enc(ARTICLE, return_tensors=\"pt\").input_ids\n>>> labels = tokenizer_dec(\n...     \"us rejects charges against its ambassador in bolivia\", return_tensors=\"pt\"\n... ).input_ids\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])\n\n>>> loss = outputs.loss\n```"]