["```py\nfrom optimum.neuron import NeuronModelForCausalLM\n\ncompiler_args = {\"num_cores\": 24, \"auto_cast_type\": 'fp16'}\ninput_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\nmodel = NeuronModelForCausalLM.from_pretrained(\n        \"NousResearch/Llama-2-13b-chat-hf\",\n        export=True,\n        **compiler_args,\n        **input_shapes)\n```", "```py\nmodel.save_pretrained(\"llama-2-13b-chat-neuron\")\n```", "```py\nhuggingface-cli login\n```", "```py\nfrom huggingface_hub import whoami\n\norg = whoami()['name']\n\nrepo_id = f\"{org}/llama-2-13b-chat-neuron\"\n\nmodel.push_to_hub(\"llama-2-13b-chat-neuron\", repository_id=repo_id)\n```", "```py\n   memory = bytes per parameter * number of parameters\n```", "```py\nfrom optimum.neuron import NeuronModelForCausalLM\n\ntry:\n    model\nexcept NameError:\n    # Edit this to use another base model\n    model = NeuronModelForCausalLM.from_pretrained('aws-neuron/Llama-2-13b-chat-hf-neuron-latency')\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-13b-chat-hf\")\n```", "```py\ninputs = tokenizer(\"What is deep-learning ?\", return_tensors=\"pt\")\noutputs = model.generate(**inputs,\n                         max_new_tokens=128,\n                         do_sample=True,\n                         temperature=0.9,\n                         top_k=50,\n                         top_p=0.9)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n```", "```py\ndef format_chat_prompt(message, history, max_tokens):\n    \"\"\" Convert a history of messages to a chat prompt\n\n    Args:\n        message(str): the new user message.\n        history (List[str]): the list of user messages and assistant responses.\n        max_tokens (int): the maximum number of input tokens accepted by the model.\n\n    Returns:\n        a `str` prompt.\n    \"\"\"\n    chat = []\n    # Convert all messages in history to chat interactions\n    for interaction in history:\n        chat.append({\"role\": \"user\", \"content\" : interaction[0]})\n        chat.append({\"role\": \"assistant\", \"content\" : interaction[1]})\n    # Add the new message\n    chat.append({\"role\": \"user\", \"content\" : message})\n    # Generate the prompt, verifying that we don't go beyond the maximum number of tokens\n    for i in range(0, len(chat), 2):\n        # Generate candidate prompt with the last n-i entries\n        prompt = tokenizer.apply_chat_template(chat[i:], tokenize=False)\n        # Tokenize to check if we're over the limit\n        tokens = tokenizer(prompt)\n        if len(tokens.input_ids) <= max_tokens:\n            # We're good, stop here\n            return prompt\n    # We shall never reach this line\n    raise SystemError\n```", "```py\nhistory = []\nmax_tokens = 1024\n\ndef chat(message, history, max_tokens):\n    prompt = format_chat_prompt(message, history, max_tokens)\n    # Uncomment the line below to see what the formatted prompt looks like\n    #print(prompt)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs,\n                             max_length=2048,\n                             do_sample=True,\n                             temperature=0.9,\n                             top_k=50,\n                             repetition_penalty=1.2)\n    # Do not include the input tokens\n    outputs = outputs[0, inputs.input_ids.size(-1):]\n    response = tokenizer.decode(outputs, skip_special_tokens=True)\n    history.append([message, response])\n    return response\n```", "```py\nprint(chat(\"My favorite color is blue. My favorite fruit is strawberry.\", history, max_tokens))\nprint(chat(\"Name a fruit that is on my favorite colour.\", history, max_tokens))\nprint(chat(\"What is the colour of my favorite fruit ?\", history, max_tokens))\n```"]