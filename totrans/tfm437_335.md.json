["```py\n>>> from transformers import (\n...     Blip2VisionConfig,\n...     Blip2QFormerConfig,\n...     OPTConfig,\n...     Blip2Config,\n...     Blip2ForConditionalGeneration,\n... )\n\n>>> # Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration\n>>> configuration = Blip2Config()\n\n>>> # Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration\n>>> model = Blip2ForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig\n\n>>> # Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations\n>>> vision_config = Blip2VisionConfig()\n>>> qformer_config = Blip2QFormerConfig()\n>>> text_config = OPTConfig()\n\n>>> config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)\n```", "```py\n>>> from transformers import Blip2VisionConfig, Blip2VisionModel\n\n>>> # Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration\n>>> configuration = Blip2VisionConfig()\n\n>>> # Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration\n>>> model = Blip2VisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import Blip2QFormerConfig, Blip2QFormerModel\n\n>>> # Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration\n>>> configuration = Blip2QFormerConfig()\n\n>>> # Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration\n>>> model = Blip2QFormerModel(configuration)\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Blip2Processor, Blip2Model\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n>>> model.to(device)\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> prompt = \"Question: how many cats are there? Answer:\"\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\n>>> outputs = model(**inputs)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, Blip2Model\n\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> inputs = tokenizer([\"a photo of a cat\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Blip2Model\n\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_outputs = model.get_image_features(**inputs)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Blip2Processor, Blip2Model\n\n>>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> qformer_outputs = model.get_qformer_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Blip2Processor, Blip2ForConditionalGeneration\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\n...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n... )  # doctest: +IGNORE_RESULT\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n```", "```py\n>>> inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n\n>>> generated_ids = model.generate(**inputs)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\ntwo cats laying on a couch\n```", "```py\n>>> prompt = \"Question: how many cats are there? Answer:\"\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n\n>>> generated_ids = model.generate(**inputs)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\ntwo\n```", "```py\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\n...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.bfloat16\n... )  # doctest: +IGNORE_RESULT\n\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n\n>>> generated_ids = model.generate(**inputs)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\ntwo\n```"]