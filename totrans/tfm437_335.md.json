["```py\n( vision_config = None qformer_config = None text_config = None num_query_tokens = 32 **kwargs )\n```", "```py\n>>> from transformers import (\n...     Blip2VisionConfig,\n...     Blip2QFormerConfig,\n...     OPTConfig,\n...     Blip2Config,\n...     Blip2ForConditionalGeneration,\n... )\n\n>>> # Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration\n>>> configuration = Blip2Config()\n\n>>> # Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration\n>>> model = Blip2ForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig\n\n>>> # Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations\n>>> vision_config = Blip2VisionConfig()\n>>> qformer_config = Blip2QFormerConfig()\n>>> text_config = OPTConfig()\n\n>>> config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)\n```", "```py\n( vision_config: Blip2VisionConfig qformer_config: Blip2QFormerConfig text_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';Blip2Config\n```", "```py\n( hidden_size = 1408 intermediate_size = 6144 num_hidden_layers = 39 num_attention_heads = 16 image_size = 224 patch_size = 14 hidden_act = 'gelu' layer_norm_eps = 1e-06 attention_dropout = 0.0 initializer_range = 1e-10 qkv_bias = True **kwargs )\n```", "```py\n>>> from transformers import Blip2VisionConfig, Blip2VisionModel\n\n>>> # Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration\n>>> configuration = Blip2VisionConfig()\n\n>>> # Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration\n>>> model = Blip2VisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' cross_attention_frequency = 2 encoder_hidden_size = 1408 **kwargs )\n```", "```py\n>>> from transformers import Blip2QFormerConfig, Blip2QFormerModel\n\n>>> # Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration\n>>> configuration = Blip2QFormerConfig()\n\n>>> # Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration\n>>> model = Blip2QFormerModel(configuration)\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: Blip2VisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n( config: Blip2QFormerConfig )\n```", "```py\n( query_embeds: FloatTensor attention_mask: Optional = None head_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: Blip2Config )\n```", "```py\n( pixel_values: FloatTensor input_ids: FloatTensor attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Blip2Processor, Blip2Model\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n>>> model.to(device)\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> prompt = \"Question: how many cats are there? Answer:\"\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\n>>> outputs = model(**inputs)\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_outputs (CausalLMOutputWithPast, or tuple(torch.FloatTensor) if return_dict=False)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, Blip2Model\n\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> inputs = tokenizer([\"a photo of a cat\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';vision_outputs (BaseModelOutputWithPooling or tuple of torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Blip2Model\n\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_outputs = model.get_image_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';vision_outputs (BaseModelOutputWithPooling or tuple of torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Blip2Processor, Blip2Model\n\n>>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> qformer_outputs = model.get_qformer_features(**inputs)\n```", "```py\n( config: Blip2Config )\n```", "```py\n( pixel_values: FloatTensor input_ids: FloatTensor attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Blip2Processor, Blip2ForConditionalGeneration\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\n...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n... )  # doctest: +IGNORE_RESULT\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n```", "```py\n>>> inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n\n>>> generated_ids = model.generate(**inputs)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\ntwo cats laying on a couch\n```", "```py\n>>> prompt = \"Question: how many cats are there? Answer:\"\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n\n>>> generated_ids = model.generate(**inputs)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\ntwo\n```", "```py\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\n...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.bfloat16\n... )  # doctest: +IGNORE_RESULT\n\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n\n>>> generated_ids = model.generate(**inputs)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\ntwo\n```", "```py\n( pixel_values: FloatTensor input_ids: Optional = None attention_mask: Optional = None **generate_kwargs ) \u2192 export const metadata = 'undefined';captions (list)\n```"]