# ä¸ Spark ä¸€èµ·ä½¿ç”¨

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/datasets/use_with_spark`](https://huggingface.co/docs/datasets/use_with_spark)

æœ¬æ–‡æ˜¯å…³äºå¦‚ä½•å°† Spark DataFrame åŠ è½½åˆ° Dataset å¯¹è±¡ä¸­çš„ğŸ¤— Datasets çš„å¿«é€Ÿä»‹ç»ã€‚

ä»é‚£é‡Œï¼Œæ‚¨å¯ä»¥å¿«é€Ÿè®¿é—®ä»»ä½•å…ƒç´ ï¼Œå¹¶å°†å…¶ç”¨ä½œæ•°æ®åŠ è½½å™¨æ¥è®­ç»ƒæ¨¡å‹ã€‚

## ä» Spark åŠ è½½

Dataset å¯¹è±¡æ˜¯ Arrow è¡¨çš„åŒ…è£…å™¨ï¼Œå…è®¸ä»æ•°æ®é›†ä¸­çš„æ•°ç»„å¿«é€Ÿè¯»å–åˆ° PyTorchã€TensorFlow å’Œ JAX å¼ é‡ã€‚Arrow è¡¨æ˜¯ä»ç£ç›˜å†…å­˜æ˜ å°„çš„ï¼Œå¯ä»¥åŠ è½½æ¯”æ‚¨å¯ç”¨ RAM æ›´å¤§çš„æ•°æ®é›†ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨`Dataset.from_spark()`ä» Spark DataFrame è·å– Datasetï¼š

```py
>>> from datasets import Dataset
>>> df = spark.createDataFrame(
...     data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
...     columns=["id", "name"],
... )
>>> ds = Dataset.from_spark(df)
```

Spark å·¥ä½œèŠ‚ç‚¹å°†æ•°æ®é›†å†™å…¥ç¼“å­˜ç›®å½•ä¸­çš„ Arrow æ–‡ä»¶ä¸­ï¼Œç„¶åä»é‚£é‡ŒåŠ è½½ Datasetã€‚

æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`IterableDataset.from_spark()`æ¥è·³è¿‡å®ä½“åŒ–ï¼Œå®ƒè¿”å›ä¸€ä¸ª IterableDatasetï¼š

```py
>>> from datasets import IterableDataset
>>> df = spark.createDataFrame(
...     data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
...     columns=["id", "name"],
... )
>>> ds = IterableDataset.from_spark(df)
>>> print(next(iter(ds)))
{"id": 1, "name": "Elia"}
```

### ç¼“å­˜

å½“ä½¿ç”¨`Dataset.from_spark()`æ—¶ï¼Œç”Ÿæˆçš„ Dataset ä¼šè¢«ç¼“å­˜ï¼›å¦‚æœåœ¨ç›¸åŒçš„ DataFrame ä¸Šå¤šæ¬¡è°ƒç”¨`Dataset.from_spark()`ï¼Œå®ƒä¸ä¼šé‡æ–°è¿è¡Œå†™å…¥ Arrow æ–‡ä»¶çš„ Spark ä½œä¸šã€‚

æ‚¨å¯ä»¥é€šè¿‡å°†`cache_dir=`ä¼ é€’ç»™`Dataset.from_spark()`æ¥è®¾ç½®ç¼“å­˜ä½ç½®ã€‚ç¡®ä¿ä½¿ç”¨ä¸€ä¸ªå¯¹æ‚¨çš„å·¥ä½œèŠ‚ç‚¹å’Œå½“å‰æœºå™¨ï¼ˆé©±åŠ¨ç¨‹åºï¼‰éƒ½å¯ç”¨çš„ç£ç›˜ã€‚

åœ¨å¦ä¸€ä¸ªä¼šè¯ä¸­ï¼ŒSpark DataFrame æ²¡æœ‰ç›¸åŒçš„[è¯­ä¹‰å“ˆå¸Œ](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.semanticHash.html)ï¼Œå®ƒå°†é‡æ–°è¿è¡Œä¸€ä¸ª Spark ä½œä¸šå¹¶å°†å…¶å­˜å‚¨åœ¨æ–°çš„ç¼“å­˜ä¸­ã€‚

### ç‰¹å¾ç±»å‹

å¦‚æœæ‚¨çš„æ•°æ®é›†ç”±å›¾åƒã€éŸ³é¢‘æ•°æ®æˆ– N ç»´æ•°ç»„ç»„æˆï¼Œæ‚¨å¯ä»¥åœ¨`Dataset.from_spark()`ï¼ˆæˆ–`IterableDataset.from_spark()`ï¼‰ä¸­æŒ‡å®š`features=`å‚æ•°ï¼š

```py
>>> from datasets import Dataset, Features, Image, Value
>>> data = [(0, open("image.png", "rb").read())]
>>> df = spark.createDataFrame(data, "idx: int, image: binary")
>>> # Also works if you have arrays
>>> # data = [(0, np.zeros(shape=(32, 32, 3), dtype=np.int32).tolist())]
>>> # df = spark.createDataFrame(data, "idx: int, image: array<array<array<int>>>")
>>> features = Features({"idx": Value("int64"), "image": Image()})
>>> dataset = Dataset.from_spark(df, features=features)
>>> dataset[0]
{'idx': 0, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>}
```

æ‚¨å¯ä»¥æŸ¥çœ‹ Features æ–‡æ¡£ï¼Œäº†è§£æ‰€æœ‰å¯ç”¨çš„ç‰¹å¾ç±»å‹ã€‚
