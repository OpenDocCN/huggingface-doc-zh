# 与Spark一起使用

> 原始文本：[https://huggingface.co/docs/datasets/use_with_spark](https://huggingface.co/docs/datasets/use_with_spark)

本文是关于如何将Spark DataFrame加载到[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象中的🤗 Datasets的快速介绍。

从那里，您可以快速访问任何元素，并将其用作数据加载器来训练模型。

## 从Spark加载

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象是Arrow表的包装器，允许从数据集中的数组快速读取到PyTorch、TensorFlow和JAX张量。Arrow表是从磁盘内存映射的，可以加载比您可用RAM更大的数据集。

您可以使用`Dataset.from_spark()`从Spark DataFrame获取[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)：

```py
>>> from datasets import Dataset
>>> df = spark.createDataFrame(
...     data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
...     columns=["id", "name"],
... )
>>> ds = Dataset.from_spark(df)
```

Spark工作节点将数据集写入缓存目录中的Arrow文件中，然后从那里加载[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。

或者，您可以通过使用`IterableDataset.from_spark()`来跳过实体化，它返回一个[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)：

```py
>>> from datasets import IterableDataset
>>> df = spark.createDataFrame(
...     data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
...     columns=["id", "name"],
... )
>>> ds = IterableDataset.from_spark(df)
>>> print(next(iter(ds)))
{"id": 1, "name": "Elia"}
```

### 缓存

当使用`Dataset.from_spark()`时，生成的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)会被缓存；如果在相同的DataFrame上多次调用`Dataset.from_spark()`，它不会重新运行写入Arrow文件的Spark作业。

您可以通过将`cache_dir=`传递给`Dataset.from_spark()`来设置缓存位置。确保使用一个对您的工作节点和当前机器（驱动程序）都可用的磁盘。

在另一个会话中，Spark DataFrame没有相同的[语义哈希](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.semanticHash.html)，它将重新运行一个Spark作业并将其存储在新的缓存中。

### 特征类型

如果您的数据集由图像、音频数据或N维数组组成，您可以在`Dataset.from_spark()`（或`IterableDataset.from_spark()`）中指定`features=`参数：

```py
>>> from datasets import Dataset, Features, Image, Value
>>> data = [(0, open("image.png", "rb").read())]
>>> df = spark.createDataFrame(data, "idx: int, image: binary")
>>> # Also works if you have arrays
>>> # data = [(0, np.zeros(shape=(32, 32, 3), dtype=np.int32).tolist())]
>>> # df = spark.createDataFrame(data, "idx: int, image: array<array<array<int>>>")
>>> features = Features({"idx": Value("int64"), "image": Image()})
>>> dataset = Dataset.from_spark(df, features=features)
>>> dataset[0]
{'idx': 0, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>}
```

您可以查看[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)文档，了解所有可用的特征类型。
