- en: (Optional) What is Curiosity in Deep Reinforcement Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit5/curiosity](https://huggingface.co/learn/deep-rl-course/unit5/curiosity)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an (optional) introduction to Curiosity. If you want to learn more,
    you can read two additional articles where we dive into the mathematical details:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Curiosity-Driven Learning through Next State Prediction](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Network Distillation: a new take on Curiosity-Driven Learning](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two Major Problems in Modern RL
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand what Curiosity is, we first need to understand the two major
    problems with RL:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: First, the *sparse rewards problem:* that is, **most rewards do not contain
    information, and hence are set to zero**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Remember that RL is based on the *reward hypothesis*, which is the idea that
    each goal can be described as the maximization of the rewards. Therefore, rewards
    act as feedback for RL agents; **if they don’t receive any, their knowledge of
    which action is appropriate (or not) cannot change**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Curiosity](../Images/542512c1c81fdf970fe9cb37254df345.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'Source: Thanks to the reward, our agent knows that this action at that state
    was good'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in [Vizdoom](https://vizdoom.cs.put.edu.pl/), a set of environments
    based on the game Doom “DoomMyWayHome,” your agent is only rewarded **if it finds
    the vest**. However, the vest is far away from your starting point, so most of
    your rewards will be zero. Therefore, if our agent does not receive useful feedback
    (dense rewards), it will take much longer to learn an optimal policy, and **it
    can spend time turning around without finding the goal**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Curiosity](../Images/e3753d61bd4ca091ba22d9ba1b36cd3f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: The second big problem is that **the extrinsic reward function is handmade;
    in each environment, a human has to implement a reward function**. But how we
    can scale that in big and complex environments?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: So what is Curiosity?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A solution to these problems is **to develop a reward function intrinsic to
    the agent, i.e., generated by the agent itself**. The agent will act as a self-learner
    since it will be the student and its own feedback master.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**This intrinsic reward mechanism is known as Curiosity** because this reward
    pushes the agent to explore states that are novel/unfamiliar. To achieve that,
    our agent will receive a high reward when exploring new trajectories.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: This reward is inspired by how humans act. **We naturally have an intrinsic
    desire to explore environments and discover new things**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to calculate this intrinsic reward. The classical approach
    (Curiosity through next-state prediction) is to calculate Curiosity **as the error
    of our agent in predicting the next state, given the current state and action
    taken**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Curiosity](../Images/6b030efa6065b6f29760b37ae29749d3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Because the idea of Curiosity is to **encourage our agent to perform actions
    that reduce the uncertainty in the agent’s ability to predict the consequences
    of its actions** (uncertainty will be higher in areas where the agent has spent
    less time or in areas with complex dynamics).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: If the agent spends a lot of time on these states, it will be good at predicting
    the next state (low Curiosity). On the other hand, if it’s in a new, unexplored
    state, it will be hard to predict the following state (high Curiosity).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Curiosity](../Images/513912902e6673ccaf455689bc8325ab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Using Curiosity will push our agent to favor transitions with high prediction
    error (which will be higher in areas where the agent has spent less time, or in
    areas with complex dynamics) and **consequently better explore our environment**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: There’s also **other curiosity calculation methods**. ML-Agents uses a more
    advanced one called Curiosity through random network distillation. This is out
    of the scope of the tutorial but if you’re interested [I wrote an article explaining
    it in detail](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他的好奇心计算方法。ML-Agents使用一种更先进的方法，称为通过随机网络蒸馏实现好奇心。这超出了本教程的范围，但如果你感兴趣，我写了一篇详细解释的文章。
