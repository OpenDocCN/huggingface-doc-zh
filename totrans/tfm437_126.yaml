- en: DeepSpeed Integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/41.5dfe73c6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described
    in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Currently it provides full
    support for:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer state partitioning (ZeRO stage 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient partitioning (ZeRO stage 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameter partitioning (ZeRO stage 3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom mixed precision training handling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A range of fast CUDA-extension-based optimizers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO-Offload to CPU and NVMe
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale
    Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described
    in the paper [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
    Learning](https://arxiv.org/abs/2104.07857).'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-2 is primarily used only for training, as its features are of
    no use to inference.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-3 can be used for inference as well, since it allows huge models
    to be loaded on multiple GPUs, which won’t be possible on a single GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '🤗 Transformers integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    via 2 options:'
  prefs: []
  type: TYPE_NORMAL
- en: Integration of the core DeepSpeed features via [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    This is an everything-done-for-you type of integration - just supply your custom
    config file or use our template and you have nothing else to do. Most of this
    document is focused on this feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t use [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and want to use your own Trainer where you integrated DeepSpeed yourself, core
    functionality functions like `from_pretrained` and `from_config` include integration
    of essential parts of DeepSpeed like `zero.Init` for ZeRO stage 3 and higher.
    To tap into this feature read the docs on [non-Trainer DeepSpeed Integration](#nontrainer-deepspeed-integration).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What is integrated:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training:'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO training supports the full ZeRO stages 1, 2 and 3 with ZeRO-Infinity
    (CPU and NVME offload).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inference:'
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesn’t use an optimizer and a lr scheduler
    and only stage 3 is relevant. For more details see: [zero-inference](#zero-inference).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is also DeepSpeed Inference - this is a totally different technology which
    uses Tensor Parallelism instead of ZeRO (coming soon).
  prefs: []
  type: TYPE_NORMAL
- en: Trainer Deepspeed Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the library via pypi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'or via `transformers`’ `extras`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: or find more details on [the DeepSpeed’s GitHub page](https://github.com/microsoft/deepspeed#installation)
    and [advanced install](https://www.deepspeed.ai/tutorials/advanced-install/).
  prefs: []
  type: TYPE_NORMAL
- en: If you’re still struggling with the build, first make sure to read [CUDA Extension
    Installation Notes](trainer#cuda-extension-installation-notes).
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t prebuild the extensions and rely on them to be built at run time
    and you tried all of the above solutions to no avail, the next thing to try is
    to pre-build the modules before installing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a local build for DeepSpeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you intend to use NVMe offload you will also need to include `DS_BUILD_AIO=1`
    in the instructions above (and also install *libaio-dev* system-wide).
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit `TORCH_CUDA_ARCH_LIST` to insert the code for the architectures of the
    GPU cards you intend to use. Assuming all your cards are the same you can get
    the arch via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So if you get `8, 6`, then use `TORCH_CUDA_ARCH_LIST="8.6"`. If you have multiple
    different cards, you can list all of them like so `TORCH_CUDA_ARCH_LIST="6.1;8.6"`
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to use the same setup on multiple machines, make a binary wheel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: it will generate something like `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`
    which now you can install as `pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`
    locally or on any other machine.
  prefs: []
  type: TYPE_NORMAL
- en: Again, remember to ensure to adjust `TORCH_CUDA_ARCH_LIST` to the target architectures.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete list of NVIDIA GPUs and their corresponding **Compute
    Capabilities** (same as arch in this context) [here](https://developer.nvidia.com/cuda-gpus).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the archs pytorch was built with using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how to find out the arch for one of the installed GPUs. For example,
    for GPU 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: then you know that this card’s arch is `8.6`.
  prefs: []
  type: TYPE_NORMAL
- en: You can also leave `TORCH_CUDA_ARCH_LIST` out completely and then the build
    program will automatically query the architecture of the GPUs the build is made
    on. This may or may not match the GPUs on the target machines, that’s why it’s
    best to specify the desired archs explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: If after trying everything suggested you still encounter build issues, please,
    proceed with the GitHub Issue of [Deepspeed](https://github.com/microsoft/DeepSpeed/issues),
  prefs: []
  type: TYPE_NORMAL
- en: Deployment with multiple GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To deploy the DeepSpeed integration adjust the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments to include a new argument `--deepspeed ds_config.json`,
    where `ds_config.json` is the DeepSpeed configuration file as documented [here](https://www.deepspeed.ai/docs/config-json/).
    The file naming is up to you. It’s recommended to use DeepSpeed’s `add_config_arguments`
    utility to add the necessary command line arguments to your code. For more information
    please see [DeepSpeed’s Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing)
    doc.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use a launcher of your choice here. You can continue using the pytorch
    launcher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'or use the launcher provided by `deepspeed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see the arguments aren’t the same, but for most needs either of them
    works. The full details on how to configure various nodes and GPUs can be found
    [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).
  prefs: []
  type: TYPE_NORMAL
- en: When you use the `deepspeed` launcher and you want to use all available gpus
    you can just omit the `--num_gpus` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of running `run_translation.py` under DeepSpeed deploying
    all available GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the DeepSpeed documentation you are likely to see `--deepspeed
    --deepspeed_config ds_config.json` - i.e. two DeepSpeed-related arguments, but
    for the sake of simplicity, and since there are already so many arguments to deal
    with, we combined the two into a single argument.
  prefs: []
  type: TYPE_NORMAL
- en: For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400).
  prefs: []
  type: TYPE_NORMAL
- en: Deployment with one GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy DeepSpeed with one GPU adjust the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly
    to use just one GPU via `--num_gpus=1`. By default, DeepSpeed deploys all GPUs
    it can see on the given node. If you have only 1 GPU to start with, then you don’t
    need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)
    discusses the launcher options.
  prefs: []
  type: TYPE_NORMAL
- en: Why would you want to use DeepSpeed with just one GPU?
  prefs: []
  type: TYPE_NORMAL
- en: It has a ZeRO-offload feature which can delegate some computations and memory
    to the host’s CPU and RAM, and thus leave more GPU resources for model’s needs
    - e.g. larger batch size, or enabling a fitting of a very big model which normally
    won’t fit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It provides a smart GPU memory management system, that minimizes memory fragmentation,
    which again allows you to fit bigger models and data batches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While we are going to discuss the configuration in details next, the key to
    getting a huge improvement on a single GPU with DeepSpeed is to have at least
    the following configuration in the configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: which enables optimizer offload and some other important features. You may experiment
    with the buffer sizes, you will find more details in the discussion below.
  prefs: []
  type: TYPE_NORMAL
- en: For a practical usage example of this type of deployment, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).
  prefs: []
  type: TYPE_NORMAL
- en: You may also try the ZeRO-3 with CPU and NVMe offload as explained further in
    this document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if you need to run on a specific GPU, which is different from GPU 0, you can’t
    use `CUDA_VISIBLE_DEVICES` to limit the visible scope of available GPUs. Instead,
    you have to use the following syntax:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we tell DeepSpeed to use GPU 1 (second gpu).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deployment with multiple Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The information in this section isn’t not specific to the DeepSpeed integration
    and is applicable to any multi-node program. But DeepSpeed provides a `deepspeed`
    launcher that is easier to use than other launchers unless you are in a SLURM
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: For the duration of this section let’s assume that you have 2 nodes with 8 gpus
    each. And you can reach the first node with `ssh hostname1` and second node with
    `ssh hostname2`, and both must be able to reach each other via ssh locally without
    a password. Of course, you will need to rename these host (node) names to the
    actual host names you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: The torch.distributed.run(torchrun) launcher
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For example, to use `torch.distributed.run`, you could do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You have to ssh to each node and run this same command on each one of them!
    There is no rush, the launcher will wait until both nodes will synchronize.
  prefs: []
  type: TYPE_NORMAL
- en: For more information please see [torchrun](https://pytorch.org/docs/stable/elastic/run.html).
    Incidentally, this is also the launcher that replaced `torch.distributed.launch`
    a few pytorch versions back.
  prefs: []
  type: TYPE_NORMAL
- en: The deepspeed launcher
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To use the `deepspeed` launcher instead, you have to first create a `hostfile`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'and then you can launch it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the `torch.distributed.run` launcher, `deepspeed` will automatically
    launch this command on both nodes!
  prefs: []
  type: TYPE_NORMAL
- en: For more information please see [Resource Configuration (multi-node)](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).
  prefs: []
  type: TYPE_NORMAL
- en: Launching in a SLURM environment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the SLURM environment the following approach can be used. The following is
    a slurm script `launch.slurm` which you will need to adapt it to your specific
    SLURM environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'All is left is to schedule it to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`srun` will take care of launching the program simultaneously on all nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Use of Non-shared filesystem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By default DeepSpeed expects that a multi-node environment uses a shared storage.
    If this is not the case and each node can only see the local filesystem, you need
    to adjust the config file to include a [`checkpoint`_section](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)
    with the following setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can also use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)’s
    `--save_on_each_node` argument, and the above config will be added automatically
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment in Notebooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with running notebook cells as a script is that there is no normal
    `deepspeed` launcher to rely on, so under certain setups we have to emulate it.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using only 1 GPU, here is how you’d have to adjust your training code
    in the notebook to use DeepSpeed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: `...` stands for the normal arguments that you’d pass to the functions.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use more than 1 GPU, you must use a multi-process environment
    for DeepSpeed to work. That is, you have to use the launcher for that purpose
    and this cannot be accomplished by emulating the distributed environment presented
    at the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to create the config file on the fly in the notebook in the current
    directory, you could have a dedicated cell with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If the training script is in a normal file and not in the notebook cells, you
    can launch `deepspeed` normally via shell from a cell. For example, to use `run_translation.py`
    you would launch it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'or with `%%bash` magic, where you can write a multi-line code for the shell
    program to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In such case you don’t need any of the code presented at the beginning of this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: While `%%bash` magic is neat, but currently it buffers the output so
    you won’t see the logs until the process completes.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the complete guide to the DeepSpeed configuration options that can be used
    in its configuration file please refer to the [following documentation](https://www.deepspeed.ai/docs/config-json/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find dozens of DeepSpeed configuration examples that address various
    practical needs in [the DeepSpeedExamples repo](https://github.com/microsoft/DeepSpeedExamples):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuing the code from above, let’s say you’re looking to configure the Lamb
    optimizer. So you can search through the example `.json` files with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Some more examples are to be found in the [main repo](https://github.com/microsoft/DeepSpeed)
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: When using DeepSpeed you always need to supply a DeepSpeed configuration file,
    yet some configuration parameters have to be configured via the command line.
    You will find the nuances in the rest of this guide.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an idea of what DeepSpeed configuration file looks like, here is one
    that activates ZeRO stage 2 features, including optimizer states cpu offload,
    uses `AdamW` optimizer and `WarmupLR` scheduler and will enable mixed precision
    training if `--fp16` is passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: When you execute the program, DeepSpeed will log the configuration it received
    from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    to the console, so you can see exactly what was the final configuration passed
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: Passing Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in this document normally the DeepSpeed configuration is passed
    as a path to a json file, but if you’re not using the command line interface to
    configure the training, and instead instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    via [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    then for the `deepspeed` argument you can pass a nested `dict`. This allows you
    to create the configuration on the fly and doesn’t require you to write it to
    the file system before passing it to [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'or:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Shared Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section is a must-read
  prefs: []
  type: TYPE_NORMAL
- en: Some configuration values are required by both the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and DeepSpeed to function correctly, therefore, to prevent conflicting definitions,
    which could lead to hard to detect errors, we chose to configure those via the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, some configuration values are derived automatically based on the
    model’s configuration, so instead of remembering to manually adjust multiple values,
    it’s the best to let the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    do the majority of configuration for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in the rest of this guide you will find a special configuration
    value: `auto`, which when set will be automatically replaced with the correct
    or most efficient value. Please feel free to choose to ignore this recommendation
    and set the values explicitly, in which case be very careful that your the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    arguments and DeepSpeed configurations agree. For example, are you using the same
    learning rate, or batch size, or gradient accumulation settings? if these mismatch
    the training may fail in very difficult to detect ways. You have been warned.'
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple other values that are specific to DeepSpeed-only and those
    you will have to set manually to suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your own programs, you can also use the following approach if you’d like
    to modify the DeepSpeed config as a master and configure [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    based on that. The steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Create or load the DeepSpeed configuration to be used as a master configuration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object based on these values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do note that some values, such as `scheduler.params.total_num_steps` are calculated
    by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    during `train`, but you can of course do the math yourself.
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)
    is the workhorse of DeepSpeed. It supports 3 different levels (stages) of optimization.
    The first one is not quite interesting for scalability purposes, therefore this
    document focuses on stages 2 and 3\. Stage 3 is further improved by the latest
    addition of ZeRO-Infinity. You will find more indepth information in the DeepSpeed
    documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: The `zero_optimization` section of the configuration file is the most important
    part ([docs](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)),
    since that is where you define which ZeRO stages you want to enable and how to
    configure them. You will find the explanation for each parameter in the DeepSpeed
    docs.
  prefs: []
  type: TYPE_NORMAL
- en: This section has to be configured exclusively via DeepSpeed configuration -
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    provides no equivalent command line arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: currently DeepSpeed doesn’t validate parameter names, so if you misspell
    any, it’ll use the default setting for the parameter that got misspelled. You
    can watch the DeepSpeed engine start up log messages to see what values it is
    going to use.'
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-2 Config
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following is an example of configuration for ZeRO stage 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Performance tuning:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'enabling `offload_optimizer` should reduce GPU RAM usage (it requires `"stage":
    2`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"overlap_comm": true` trades off increased GPU RAM usage to lower all-reduce
    latency. `overlap_comm` uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size`
    values. So if they are set to 5e8, this requires a 9GB footprint (`5e8 x 2Bytes
    x 2 x 4.5`). Therefore, if you have a GPU with 8GB or less RAM, to avoid getting
    OOM-errors you will need to reduce those parameters to about `2e8`, which would
    require 3.6GB. You will want to do the same on larger capacity GPU as well, if
    you’re starting to hit OOM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when reducing these buffers you’re trading communication speed to avail more
    GPU RAM. The smaller the buffer size is, the slower the communication gets, and
    the more GPU RAM will be available to other tasks. So if a bigger batch size is
    important, getting a slightly slower training time could be a good trade.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, `deepspeed==0.4.4` added a new option `round_robin_gradients`
    which you can enable with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This is a stage 2 optimization for CPU offloading that parallelizes gradient
    copying to CPU memory among ranks by fine-grained gradient partitioning. Performance
    benefit grows with gradient accumulation steps (more copying between optimizer
    steps) or GPU count (increased parallelism).
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-3 Config
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following is an example of configuration for ZeRO stage 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are getting OOMs, because your model or activations don’t fit into the
    GPU memory and you have unutilized CPU memory offloading the optimizer states
    and parameters to CPU memory with `"device": "cpu"` may solve this limitation.
    If you don’t want to offload to CPU memory, use `none` instead of `cpu` for the
    `device` entry. Offloading to NVMe is discussed further down.'
  prefs: []
  type: TYPE_NORMAL
- en: Pinned memory is enabled with `pin_memory` set to `true`. This feature can improve
    the throughput at the cost of making less memory available to other processes.
    Pinned memory is set aside to the specific process that requested it and its typically
    accessed much faster than normal CPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance tuning:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`stage3_max_live_parameters`: `1e9`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage3_max_reuse_distance`: `1e9`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If hitting OOM reduce `stage3_max_live_parameters` and `stage3_max_reuse_distance`.
    They should have minimal impact on performance unless you are doing activation
    checkpointing. `1e9` would consume ~2GB. The memory is shared by `stage3_max_live_parameters`
    and `stage3_max_reuse_distance`, so it’s not additive, it’s just 2GB total.
  prefs: []
  type: TYPE_NORMAL
- en: '`stage3_max_live_parameters` is the upper limit on how many full parameters
    you want to keep on the GPU at any given time. “reuse distance” is a metric we
    are using to figure out when will a parameter be used again in the future, and
    we use the `stage3_max_reuse_distance` to decide whether to throw away the parameter
    or to keep it. If a parameter is going to be used again in near future (less than
    `stage3_max_reuse_distance`) then we keep it to reduce communication overhead.
    This is super helpful when you have activation checkpointing enabled, where we
    do a forward recompute and backward passes a single layer granularity and want
    to keep the parameter in the forward recompute till the backward'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following configuration values depend on the model’s hidden size:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reduce_bucket_size`: `hidden_size*hidden_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: therefore set these values to `auto` and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically assign the recommended values. But, of course, feel free to
    set these explicitly as well.
  prefs: []
  type: TYPE_NORMAL
- en: '`stage3_gather_16bit_weights_on_model_save` enables model fp16 weights consolidation
    when model gets saved. With large models and multiple GPUs this is an expensive
    operation both in terms of memory and speed. It’s currently required if you plan
    to resume the training. Watch out for future updates that will remove this limitation
    and make things more flexible.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re migrating from ZeRO-2 configuration note that `allgather_partitions`,
    `allgather_bucket_size` and `reduce_scatter` configuration parameters are not
    used in ZeRO-3\. If you keep these in the config file they will just be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '`sub_group_size`: `1e9`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sub_group_size` controls the granularity in which parameters are updated during
    optimizer steps. Parameters are grouped into buckets of `sub_group_size` and each
    buckets is updated one at a time. When used with NVMe offload in ZeRO-Infinity,
    `sub_group_size` therefore controls the granularity in which model states are
    moved in and out of CPU memory from NVMe during the optimizer step. This prevents
    running out of CPU memory for extremely large models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can leave `sub_group_size` to its default value of *1e9* when not using
    NVMe offload. You may want to change its default value in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running into OOM during optimizer step: Reduce `sub_group_size` to reduce memory
    utilization of temporary buffers'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimizer Step is taking a long time: Increase `sub_group_size` to improve
    bandwidth utilization as a result of the increased data buffers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO-0 Config
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Note that we’re listing Stage 0 and 1 last since they are rarely used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 0 is disabling all types of sharding and just using DeepSpeed as DDP.
    You can turn it on with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This will essentially disable ZeRO without you needing to change anything else.
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-1 Config
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Stage 1 is Stage 2 minus gradient sharding. You can always try it to speed
    things a tiny bit to only shard the optimizer states with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: NVMe Support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ZeRO-Infinity allows for training incredibly large models by extending GPU and
    CPU memory with NVMe memory. Thanks to smart partitioning and tiling algorithms
    each GPU needs to send and receive very small amounts of data during offloading
    so modern NVMe proved to be fit to allow for an even larger total memory pool
    available to your training process. ZeRO-Infinity requires ZeRO-3 enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following configuration example enables NVMe to offload both optimizer
    states and the params:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You can choose to offload both optimizer states and params to NVMe, or just
    one of them or none. For example, if you have copious amounts of CPU memory available,
    by all means offload to CPU memory only as it’d be faster (hint: *“device”: “cpu”*).'
  prefs: []
  type: TYPE_NORMAL
- en: Here is the full documentation for offloading [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)
    and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that your `nvme_path` is actually an NVMe, since it will work with
    the normal hard drive or SSD, but it’ll be much much slower. The fast scalable
    training was designed with modern NVMe transfer speeds in mind (as of this writing
    one can have ~3.5GB/s read, ~3GB/s write peak speeds).
  prefs: []
  type: TYPE_NORMAL
- en: In order to figure out the optimal `aio` configuration block you must run a
    benchmark on your target setup, as [explained here](https://github.com/microsoft/DeepSpeed/issues/998).
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-2 vs ZeRO-3 Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ZeRO-3 is likely to be slower than ZeRO-2 if everything else is configured the
    same because the former has to gather model weights in addition to what ZeRO-2
    does. If ZeRO-2 meets your needs and you don’t need to scale beyond a few GPUs
    then you may choose to stick to it. It’s important to understand that ZeRO-3 enables
    a much higher scalability capacity at a cost of speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s possible to adjust ZeRO-3 configuration to make it perform closer to ZeRO-2:'
  prefs: []
  type: TYPE_NORMAL
- en: set `stage3_param_persistence_threshold` to a very large number - larger than
    the largest parameter, e.g., `6 * hidden_size * hidden_size`. This will keep the
    parameters on the GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: turn off `offload_params` since ZeRO-2 doesn’t have that option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance will likely improve significantly with just `offload_params`
    turned off, even if you don’t change `stage3_param_persistence_threshold`. Of
    course, these changes will impact the size of the model you can train. So these
    help you to trade scalability for speed depending on your needs.
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-2 Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is a full ZeRO-2 auto-configuration file `ds_config_zero2.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here is a full ZeRO-2 all-enabled manually set configuration file. It is here
    mainly for you to see what the typical values look like, but we highly recommend
    using the one with multiple `auto` settings in it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: ZeRO-3 Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is a full ZeRO-3 auto-configuration file `ds_config_zero3.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here is a full ZeRO-3 all-enabled manually set configuration file. It is here
    mainly for you to see what the typical values look like, but we highly recommend
    using the one with multiple `auto` settings in it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How to Choose Which ZeRO Stage and Offloads To Use For Best Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So now you know there are all these different stages. How to decide which of
    them to use? This section will attempt to address this question.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: Speed-wise (left is faster than right)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stage 0 (DDP) > Stage 1 > Stage 2 > Stage 2 + offload > Stage 3 > Stage 3 +
    offloads
  prefs: []
  type: TYPE_NORMAL
- en: GPU Memory usage-wise (right is more GPU memory efficient than left)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stage 0 (DDP) < Stage 1 < Stage 2 < Stage 2 + offload < Stage 3 < Stage 3 +
    offloads
  prefs: []
  type: TYPE_NORMAL
- en: So when you want to get the fastest execution while fitting into minimal number
    of GPUs, here is the process you could follow. We start with the fastest approach
    and if running into GPU OOM we then go to the next slower approach, but which
    will use less GPU memory. And so on and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: First of all set batch size to 1 (you can always use gradient accumulation for
    any desired effective batch size).
  prefs: []
  type: TYPE_NORMAL
- en: Enable `--gradient_checkpointing 1` (HF Trainer) or directly `model.gradient_checkpointing_enable()`
    - if OOM then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try ZeRO stage 2 first. if OOM then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try ZeRO stage 2 + `offload_optimizer` - if OOM then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch to ZeRO stage 3 - if OOM then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable `offload_param` to `cpu` - if OOM then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable `offload_optimizer` to `cpu` - if OOM then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you still can’t fit a batch size of 1 first check various default values
    and lower them if you can. For example, if you use `generate` and you don’t use
    a wide search beam make it narrower as it’d take a lot of memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher
    GPUs and fp16 on older gpu architectures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you still OOM you could add more hardware or enable ZeRO-Infinity - that
    is switch offloads `offload_param` and `offload_optimizer` to `nvme`. You need
    to make sure it’s a very fast nvme. As an anecdote I was able to infer BLOOM-176B
    on a tiny GPU using ZeRO-Infinity except it was extremely slow. But it worked!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can, of course, work through these steps in reverse by starting with the
    most GPU memory efficient config and then going backwards. Or try bi-secting it.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your batch size 1 not leading to OOM, measure your effective throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Next try to increase the batch size to as large as you can, since the higher
    the batch size the more efficient the GPUs are as they perform the best when matrices
    they multiply are huge.
  prefs: []
  type: TYPE_NORMAL
- en: Now the performance optimization game starts. You can turn off some offload
    features or step down in ZeRO stages and increase/decrease batch size and again
    measure your effective throughput. Rinse and repeat until satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t spend forever on it, but if you’re about to start a 3 months training
    - do spend a few days on it to find the most effective throughput-wise setup.
    So that your training cost will be the lowest and you will finish training faster.
    In the current crazy-paced ML world, if it takes you an extra month to train something
    you are likely to miss a golden opportunity. Of course, this is only me sharing
    an observation and in no way I’m trying to rush you. Before beginning to train
    BLOOM-176B I spent 2 days on this process and was able to increase throughput
    from 90 to 150 TFLOPs! This effort saved us more than one month of training time.
  prefs: []
  type: TYPE_NORMAL
- en: These notes were written primarily for the training mode, but they should mostly
    apply for inference as well. For example, during inference Gradient Checkpointing
    is a no-op since it is only useful during training. Additionally, we found out
    that if you are doing a multi-GPU inference and not using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/),
    [Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) should
    provide a superior performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other quick related performance notes:'
  prefs: []
  type: TYPE_NORMAL
- en: if you are training something from scratch always try to have tensors with shapes
    that are divisible by 16 (e.g. hidden size). For batch size try divisible by 2
    at least. There are [wave and tile quanitization](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)
    divisibility that is hardware-specific if you want to squeeze even higher performance
    from your GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation Checkpointing or Gradient Checkpointing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation checkpointing and gradient checkpointing are two distinct terms that
    refer to the same methodology. It’s very confusing but this is how it is.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient checkpointing allows one to trade speed for GPU memory, which either
    allows one to overcome a GPU OOM, or increase their batch size, which often leads
    to a better performance.
  prefs: []
  type: TYPE_NORMAL
- en: HF Transformers models don’t know anything about DeepSpeed’s activation checkpointing,
    so if you try to enable that feature in the DeepSpeed config file, nothing will
    happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore you have two ways to take advantage of this very beneficial feature:'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()`
    or use `--gradient_checkpointing` in the HF Trainer, which will automatically
    enable this for you. `torch.utils.checkpoint` is used there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you write your own model and you want to use DeepSpeed’s activation checkpointing
    you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html).
    You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint`
    with the DeepSpeed’s API. The latter is more flexible since it allows you to offload
    the forward activations to the CPU memory instead of recalculating them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizer and Scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As long as you don’t enable `offload_optimizer` you can mix and match DeepSpeed
    and HuggingFace schedulers and optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to use a non-DeepSpeed optimizer when `offload_optimizer` is
    enabled, as long as it has both CPU and GPU implementation (except LAMB).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DeepSpeed’s main optimizers are Adam, AdamW, OneBitAdam, and Lamb. These have
    been thoroughly tested with ZeRO and are thus recommended to be used. It, however,
    can import other optimizers from `torch`. The full documentation is [here](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t configure the `optimizer` entry in the configuration file, the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to `AdamW` and will use the supplied values or the defaults
    for the following command line arguments: `--learning_rate`, `--adam_beta1`, `--adam_beta2`,
    `--adam_epsilon` and `--weight_decay`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the auto-configured `optimizer` entry for `AdamW`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the command line arguments will set the values in the configuration
    file. This is so that there is one definitive source of the values and to avoid
    hard to find errors when for example, the learning rate is set to different values
    in different places. Command line rules. The values that get overridden are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lr` with the value of `--learning_rate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` with the value of `--adam_beta1 --adam_beta2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` with the value of `--adam_epsilon`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` with the value of `--weight_decay`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore please remember to tune the shared hyperparameters on the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set the values explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use another optimizer which is not listed above, you will have
    to add to the top level configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Similarly to `AdamW`, you can configure other officially supported optimizers.
    Just remember that those may have different config values. e.g. for Adam you will
    want `weight_decay` around `0.01`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, offload works the best when it’s used with Deepspeed’s CPU Adam
    optimizer. If you want to use a different optimizer with offload, since `deepspeed==0.8.3`
    you need to also add:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: to the top level configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DeepSpeed supports `LRRangeTest`, `OneCycle`, `WarmupLR` and `WarmupDecayLR`
    learning rate schedulers. The full documentation is [here](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is where the schedulers overlap between 🤗 Transformers and DeepSpeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WarmupLR` via `--lr_scheduler_type constant_with_warmup`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WarmupDecayLR` via `--lr_scheduler_type linear`. This is also the default
    value for `--lr_scheduler_type`, therefore, if you don’t configure the scheduler
    this is scheduler that will get configured by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don’t configure the `scheduler` entry in the configuration file, the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will use the values of `--lr_scheduler_type`, `--learning_rate` and `--warmup_steps`
    or `--warmup_ratio` to configure a 🤗 Transformers version of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the auto-configured `scheduler` entry for `WarmupLR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Since *“auto”* is used the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    arguments will set the correct values in the configuration file. This is so that
    there is one definitive source of the values and to avoid hard to find errors
    when, for example, the learning rate is set to different values in different places.
    Command line rules. The values that get set are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`warmup_min_lr` with the value of `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`warmup_max_lr` with the value of `--learning_rate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`warmup_num_steps` with the value of `--warmup_steps` if provided. Otherwise
    will use `--warmup_ratio` multiplied by the number of training steps and rounded
    up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_num_steps` with either the value of `--max_steps` or if it is not provided,
    derived automatically at run time based on the environment and the size of the
    dataset and other command line arguments (needed for `WarmupDecayLR`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can, of course, take over any or all of the configuration values and set
    those yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for `WarmupDecayLR`, you can use the following entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: and `total_num_steps`, `warmup_max_lr`, `warmup_num_steps` and `total_num_steps`
    will be set at loading time.
  prefs: []
  type: TYPE_NORMAL
- en: fp32 Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deepspeed supports the full fp32 and the fp16 mixed precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the much reduced memory needs and faster speed one gets with the
    fp16 mixed precision, the only time you will want to not use it is when the model
    you’re using doesn’t behave well under this training mode. Typically this happens
    when the model wasn’t pretrained in the fp16 mixed precision (e.g. often this
    happens with bf16-pretrained models). Such models may overflow or underflow leading
    to `NaN` loss. If this is your case then you will want to use the full fp32 mode,
    by explicitly disabling the otherwise default fp16 mixed precision mode with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: If you’re using the Ampere-architecture based GPU, pytorch version 1.7 and higher
    will automatically switch to using the much more efficient tf32 format for some
    operations, but the results will still be in fp32\. For details and benchmarks,
    please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices).
    The document includes instructions on how to disable this automatic conversion
    if for some reason you prefer not to use it.
  prefs: []
  type: TYPE_NORMAL
- en: With the 🤗 Trainer you can use `--tf32` to enable it, or disable it with `--tf32
    0` or `--no_tf32`. By default the PyTorch default is used.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Mixed Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use automatic mixed precision with either a pytorch-like AMP way or
    the apex-like way:'
  prefs: []
  type: TYPE_NORMAL
- en: fp16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To configure pytorch AMP-like mode with fp16 (float16) set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically enable or disable it based on the value of `args.fp16_backend`.
    The rest of config values are up to you.
  prefs: []
  type: TYPE_NORMAL
- en: This mode gets enabled when `--fp16 --fp16_backend amp` or `--fp16_full_eval`
    command line args are passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also enable/disable this mode explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options).
  prefs: []
  type: TYPE_NORMAL
- en: bf16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If bf16 (bfloat16) is desired instead of fp16 then the following configuration
    section is to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: bf16 has the same dynamic range as fp32 and thus doesn’t require loss scaling.
  prefs: []
  type: TYPE_NORMAL
- en: This mode gets enabled when `--bf16` or `--bf16_full_eval` command line args
    are passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also enable/disable this mode explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As of `deepspeed==0.6.0` the bf16 support is new and experimental.
  prefs: []
  type: TYPE_NORMAL
- en: If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled,
    you need to be aware that it’ll accumulate gradients in bf16, which may not be
    what you want due to this format’s low precision, as it may lead to a lossy accumulation.
  prefs: []
  type: TYPE_NORMAL
- en: A work is being done to fix that and provide an option to use a higher precision
    `dtype` (fp16 or fp32).
  prefs: []
  type: TYPE_NORMAL
- en: NCCL Collectives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is the `dtype` of the training regime and there is a separate `dtype`
    that is used for communication collectives like various reduction and gathering/scattering
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: All gather/scatter ops are performed in the same `dtype` the data is in, so
    if you’re using bf16 training regime it gets gathered in bf16 - gathering is a
    non-lossy operation.
  prefs: []
  type: TYPE_NORMAL
- en: Various reduce operations can be quite lossy, for example when gradients are
    averaged across multiple-gpus, if the communications are done in fp16 or bf16
    the outcome is likely be lossy - since when one ads multiple numbers in low precision
    the result isn’t exact. More so with bf16 as it has a lower precision than fp16\.
    Often fp16 is good enough as the loss is minimal when averaging grads which are
    typically very small. Therefore, by default for half precision training fp16 is
    used as the default for reduction operations. But you have full control over this
    functionality and if you choose you can add a small overhead and ensure that reductions
    will be using fp32 as the accumulation dtype and only when the result is ready
    it’ll get downcast to the half precision `dtype` you’re training in.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to override the default you simply add a new configuration entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The valid values as of this writing are “fp16”, “bfp16”, “fp32”.
  prefs: []
  type: TYPE_NORMAL
- en: 'note: stage zero 3 had a bug with regards to bf16 comm dtype that was fixed
    in `deepspeed==0.8.1`'
  prefs: []
  type: TYPE_NORMAL
- en: apex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To configure apex AMP-like mode set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically configure it based on the values of `args.fp16_backend` and
    `args.fp16_opt_level`.
  prefs: []
  type: TYPE_NORMAL
- en: This mode gets enabled when `--fp16 --fp16_backend apex --fp16_opt_level 01`
    command line args are passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also configure this mode explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options).
  prefs: []
  type: TYPE_NORMAL
- en: Batch Size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To configure batch size, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set `train_micro_batch_size_per_gpu` to the value of `args.per_device_train_batch_size`
    and `train_batch_size` to `args.world_size * args.per_device_train_batch_size
    * args.gradient_accumulation_steps`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set the values explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Accumulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To configure gradient accumulation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to the value of `args.gradient_accumulation_steps`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set the value explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Clipping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To configure gradient gradient clipping set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to the value of `args.max_grad_norm`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set the value explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting The Model Weights Out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As long as you continue training and resuming using DeepSpeed you don’t need
    to worry about anything. DeepSpeed stores fp32 master weights in its custom checkpoint
    optimizer files, which are `global_step*/*optim_states.pt` (this is glob pattern),
    and are saved under the normal checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '**FP16 Weights:**'
  prefs: []
  type: TYPE_NORMAL
- en: When a model is saved under ZeRO-2, you end up having the normal `pytorch_model.bin`
    file with the model weights, but they are only the fp16 version of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under ZeRO-3, things are much more complicated, since the model weights are
    partitioned out over multiple GPUs, therefore `"stage3_gather_16bit_weights_on_model_save":
    true` is required to get the `Trainer` to save the fp16 version of the weights.
    If this setting is `False` `pytorch_model.bin` won’t be created. This is because
    by default DeepSpeed’s `state_dict` contains a placeholder and not the real weights.
    If we were to save this `state_dict` it won’t be possible to load it back.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '**FP32 Weights:**'
  prefs: []
  type: TYPE_NORMAL
- en: While the fp16 weights are fine for resuming training, if you finished finetuning
    your model and want to upload it to the [models hub](https://huggingface.co/models)
    or pass it to someone else you most likely will want to get the fp32 weights.
    This ideally shouldn’t be done during training since this is a process that requires
    a lot of memory, and therefore best to be performed offline after the training
    is complete. But if desired and you have plenty of free CPU memory it can be done
    in the same training script. The following sections will discuss both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '**Live FP32 Weights Recovery:**'
  prefs: []
  type: TYPE_NORMAL
- en: This approach may not work if you model is large and you have little free CPU
    memory left, at the end of the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have saved at least one checkpoint, and you want to use the latest one,
    you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re using the `--load_best_model_at_end` class:*~transformers.TrainingArguments*
    argument (to track the best checkpoint), then you can finish the training by first
    saving the final model explicitly and then do the same as above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Note, that once `load_state_dict_from_zero_checkpoint` was run, the `model`
    will no longer be usable in the DeepSpeed context of the same application. i.e.
    you will need to re-initialize the deepspeed engine, since `model.load_state_dict(state_dict)`
    will remove all the DeepSpeed magic from it. So do this only at the very end of
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you don’t have to use class:*~transformers.Trainer* and you can adjust
    the examples above to your own trainer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If for some reason you want more refinement, you can also extract the fp32
    `state_dict` of the weights and apply these yourself as is shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '**Offline FP32 Weights Recovery:**'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed creates a special conversion script `zero_to_fp32.py` which it places
    in the top-level of the checkpoint folder. Using this script you can extract the
    weights at any point. The script is standalone and you no longer need to have
    the configuration file or a `Trainer` to do the extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say your checkpoint folder looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example there is just one DeepSpeed checkpoint sub-folder *global_step1*.
    Therefore to reconstruct the fp32 weights just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This is it. `pytorch_model.bin` will now contain the full fp32 model weights
    consolidated from multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The script will automatically be able to handle either a ZeRO-2 or ZeRO-3 checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '`python zero_to_fp32.py -h` will give you usage details.'
  prefs: []
  type: TYPE_NORMAL
- en: The script will auto-discover the deepspeed sub-folder using the contents of
    the file `latest`, which in the current example will contain `global_step1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: currently the script requires 2x general RAM of the final fp32 model
    weights.'
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-3 and Infinity Nuances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ZeRO-3 is quite different from ZeRO-2 because of its param sharding feature.
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple other
    speed and scalability improvements.
  prefs: []
  type: TYPE_NORMAL
- en: While all the efforts were made for things to just work without needing any
    special changes to your models, in certain circumstances you may find the following
    information to be needed.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing Massive Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DeepSpeed/ZeRO-3 can handle models with Trillions of parameters which may not
    fit onto the existing RAM. In such cases, but also if you want the initialization
    to happen much faster, initialize the model using *deepspeed.zero.Init()* context
    manager (which is also a function decorator), like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: As you can see this gives you a randomly initialized model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use a pretrained model, `model_class.from_pretrained` will activate
    this feature as long as `is_deepspeed_zero3_enabled()` returns `True`, which currently
    is setup by the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object if the passed DeepSpeed configuration file contains ZeRO-3 config section.
    Thus you must create the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object **before** calling `from_pretrained`. Here is an example of a possible
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: If you’re using the official example scripts and your command line arguments
    include `--deepspeed ds_config.json` with ZeRO-3 config enabled, then everything
    is already done for you, since this is how example scripts are written.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: If the fp16 weights of the model can’t fit onto the memory of a single
    GPU this feature must be used.'
  prefs: []
  type: TYPE_NORMAL
- en: For full details on this method and other related features please refer to [Constructing
    Massive Models](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models).
  prefs: []
  type: TYPE_NORMAL
- en: Also when loading fp16-pretrained models, you will want to tell `from_pretrained`
    to use `torch_dtype=torch.float16`. For details, please, see [from_pretrained-torch-dtype](#from_pretrained-torch-dtype).
  prefs: []
  type: TYPE_NORMAL
- en: Gathering Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Under ZeRO-3 on multiple GPUs no single GPU has all the parameters unless it’s
    the parameters for the currently executing layer. So if you need to access all
    parameters from all layers at once there is a specific method to do it. Most likely
    you won’t need it, but if you do please refer to [Gathering Parameters](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)
  prefs: []
  type: TYPE_NORMAL
- en: We do however use it internally in several places, one such example is when
    loading pretrained model weights in `from_pretrained`. We load one layer at a
    time and immediately partition it to all participating GPUs, as for very large
    models it won’t be possible to load it on one GPU and then spread it out to multiple
    GPUs, due to memory limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also under ZeRO-3, if you write your own code and run into a model parameter
    weight that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: stress on `tensor([1.])`, or if you get an error where it says the parameter
    is of size `1`, instead of some much larger multi-dimensional shape, this means
    that the parameter is partitioned and what you see is a ZeRO-3 placeholder.
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ZeRO Inference uses the same config as ZeRO-3 Training. You just don’t need
    the optimizer and scheduler sections. In fact you can leave these in the config
    file if you want to share the same one with the training. They will just be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise you just need to pass the usual [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    arguments. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The only important thing is that you need to use a ZeRO-3 configuration, since
    ZeRO-2 provides no benefit whatsoever for the inference as only ZeRO-3 performs
    sharding of parameters, whereas ZeRO-1 shards gradients and optimizer states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of running `run_translation.py` under DeepSpeed deploying
    all available GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Since for inference there is no need for additional large memory used by the
    optimizer states and the gradients you should be able to fit much larger batches
    and/or sequence length onto the same hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally DeepSpeed is currently developing a related product called Deepspeed-Inference
    which has no relationship to the ZeRO technology, but instead uses tensor parallelism
    to scale models that can’t fit onto a single GPU. This is a work in progress and
    we will provide the integration once that product is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since Deepspeed ZeRO can offload memory to CPU (and NVMe) the framework provides
    utils that allow one to tell how much CPU and GPU memory will be needed depending
    on the number of GPUs being used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s estimate how much memory is needed to finetune “bigscience/T0_3B” on
    a single GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: So you can fit it on a single 80GB GPU and no CPU offload, or a tiny 8GB GPU
    but then need ~60GB of CPU memory. (Remember this is just the memory for params,
    optimizer states and gradients - you will need a bit more memory for cuda kernels,
    activations and temps.)
  prefs: []
  type: TYPE_NORMAL
- en: Then it’s a tradeoff of cost vs speed. It’ll be cheaper to buy/rent a smaller
    GPU (or less GPUs since you can use multiple GPUs with Deepspeed ZeRO. But then
    it’ll be slower, so even if you don’t care about how fast something will be done,
    the slowdown has a direct impact on the duration of using the GPU and thus bigger
    cost. So experiment and compare which works the best.
  prefs: []
  type: TYPE_NORMAL
- en: If you have enough GPU memory make sure to disable the CPU/NVMe offload as it’ll
    make everything faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s repeat the same for 2 GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: So here you’d want 2x 32GB GPUs or higher without offloading to CPU.
  prefs: []
  type: TYPE_NORMAL
- en: For full information please see [memory estimators](https://deepspeed.readthedocs.io/en/latest/memory.html).
  prefs: []
  type: TYPE_NORMAL
- en: Filing Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here is how to file an issue so that we could quickly get to the bottom of the
    issue and help you to unblock your work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your report please always include:'
  prefs: []
  type: TYPE_NORMAL
- en: the full Deepspeed config file in the report
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: either the command line arguments if you were using the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    or [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    arguments if you were scripting the Trainer setup yourself. Please do not dump
    the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    as it has dozens of entries that are irrelevant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output of:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If possible include a link to a Google Colab notebook that we can reproduce
    the problem with. You can use this [notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)
    as a starting point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unless it’s impossible please always use a standard dataset that we can use
    and not something custom.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If possible try to use one of the existing [examples](https://github.com/huggingface/transformers/tree/main/examples/pytorch)
    to reproduce the problem with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Deepspeed is often not the cause of the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the filed issues proved to be Deepspeed-unrelated. That is once Deepspeed
    was removed from the setup, the problem was still there.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, if it’s not absolutely obvious it’s a DeepSpeed-related problem,
    as in you can see that there is an exception and you can see that DeepSpeed modules
    are involved, first re-test your setup without DeepSpeed in it. And only if the
    problem persists then do mentioned Deepspeed and supply all the required details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If it’s clear to you that the issue is in the DeepSpeed core and not the integration
    part, please file the Issue directly with [Deepspeed](https://github.com/microsoft/DeepSpeed/).
    If you aren’t sure, please do not worry, either Issue tracker will do, we will
    figure it out once you posted it and redirect you to another Issue tracker if
    need be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: the deepspeed process gets killed at startup without a traceback
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the `deepspeed` process gets killed at launch time without a traceback, that
    usually means that the program tried to allocate more CPU memory than your system
    has or your process is allowed to allocate and the OS kernel killed that process.
    This is because your configuration file most likely has either `offload_optimizer`
    or `offload_param` or both configured to offload to `cpu`. If you have NVMe, experiment
    with offloading to NVMe if you’re running under ZeRO-3\. Here is how you can [estimate
    how much memory is needed for a specific model](https://deepspeed.readthedocs.io/en/latest/memory.html).
  prefs: []
  type: TYPE_NORMAL
- en: training and/or eval/predict loss is NaN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This often happens when one takes a model pre-trained in bf16 mixed precision
    mode and tries to use it under fp16 (with or without mixed precision). Most models
    trained on TPU and often the ones released by Google are in this category (e.g.
    almost all t5-based models). Here the solution is to either use fp32 or bf16 if
    your hardware supports it (TPU, Ampere GPUs or newer).
  prefs: []
  type: TYPE_NORMAL
- en: 'The other problem may have to do with using fp16\. When you configure this
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'and you see in your log that Deepspeed reports `OVERFLOW!` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: that means that the Deepspeed loss scaler can’t figure out a scaling co-efficient
    that overcomes loss overflow.
  prefs: []
  type: TYPE_NORMAL
- en: (the log was massaged to be more readable here.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case you usually need to raise the value of `initial_scale_power`.
    Setting it to `"initial_scale_power": 32` will typically resolve the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While DeepSpeed has a pip installable PyPI package, it is highly recommended
    that it gets installed from [source](https://github.com/microsoft/deepspeed#installation)
    to best match your hardware and also if you need to enable certain features, like
    1-bit Adam, which aren’t available in the pypi distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t have to use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    to use DeepSpeed with 🤗 Transformers - you can use any model with your own trainer,
    and you will have to adapt the latter according to [the DeepSpeed integration
    instructions](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Trainer Deepspeed Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    is used to integrate Deepspeed into the 🤗 Transformers core functionality, when
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is not used. The only thing that it does is handling Deepspeed ZeRO-3 param gathering
    and automatically splitting the model onto multiple gpus during `from_pretrained`
    call. Everything else you have to do by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: When using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    everything is automatically taken care of.
  prefs: []
  type: TYPE_NORMAL
- en: When not using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    to efficiently deploy DeepSpeed ZeRO-3, you must instantiate the [HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    object before instantiating the model and keep that object alive.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using Deepspeed ZeRO-1 or ZeRO-2 you don’t need to use `HfDeepSpeedConfig`
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example for a pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'or for non-pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Please note that if you’re not using the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    integration, you’re completely on your own. Basically follow the documentation
    on the [Deepspeed](https://www.deepspeed.ai/) website. Also you have to configure
    explicitly the config file - you can’t use `"auto"` values and you will have to
    put real values instead.
  prefs: []
  type: TYPE_NORMAL
- en: HfDeepSpeedConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.integrations.HfDeepSpeedConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/deepspeed.py#L56)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config_file_or_dict )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config_file_or_dict** (`Union[str, Dict]`) — path to DeepSpeed config file
    or dict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This object contains a DeepSpeed configuration dictionary and can be quickly
    queried for things like zero stage.
  prefs: []
  type: TYPE_NORMAL
- en: A `weakref` of this object is stored in the module’s globals to be able to access
    the config from areas where things like the Trainer object is not available (e.g.
    `from_pretrained` and `_get_resized_embeddings`). Therefore it’s important that
    this object remains alive while the program is still running.
  prefs: []
  type: TYPE_NORMAL
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    uses the `HfTrainerDeepSpeedConfig` subclass instead. That subclass has logic
    to sync the configuration with values of [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    by replacing special placeholder values: `"auto"`. Without this special logic
    the DeepSpeed configuration is not modified in any way.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom DeepSpeed ZeRO Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here is an example of how one could do DeepSpeed ZeRO Inference without using
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    when one can’t fit a model onto a single GPU. The solution includes using additional
    GPUs or/and offloading GPU memory to CPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: The important nuance to understand here is that the way ZeRO is designed you
    can process different inputs on different GPUs in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The example has copious notes and is self-documenting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure to:'
  prefs: []
  type: TYPE_NORMAL
- en: disable CPU offload if you have enough GPU memory (since it slows things down)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: enable bf16 if you own an Ampere or a newer GPU to make things faster. If you
    don’t have that hardware you may enable fp16 as long as you don’t use any model
    that was pre-trained in bf16 mixed precision (such as most t5 models). These usually
    overflow in fp16 and you will see garbage as output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s save it as `t0.py` and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: This was a very basic example and you will want to adapt it to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: generate nuances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using multiple GPUs with ZeRO Stage-3, one has to synchronize the GPUs
    by calling `generate(..., synced_gpus=True)`. If this is not done if one GPU finished
    generating before other GPUs the whole system will hang as the rest of the GPUs
    will not be able to received the shard of weights from the GPU that stopped generating.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from `transformers>=4.28`, if `synced_gpus` isn’t explicitly specified,
    it’ll be set to `True` automatically if these conditions are detected. But you
    can still override the value of `synced_gpus` if need to.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Deepspeed Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you submit a PR that involves DeepSpeed integration please note our CircleCI
    PR CI setup has no GPUs, so we only run tests requiring gpus on a different CI
    nightly. Therefore if you get a green CI report in your PR it doesn’t mean DeepSpeed
    tests pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run DeepSpeed tests, please run at least:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'If you changed any of the modeling or pytorch examples code, then run the model
    zoo tests as well. The following will run all DeepSpeed tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Main DeepSpeed Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Project’s github](https://github.com/microsoft/deepspeed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Usage docs](https://www.deepspeed.ai/getting-started/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[API docs](https://deepspeed.readthedocs.io/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, please, remember that, HuggingFace [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    only integrates DeepSpeed, therefore if you have any problems or questions with
    regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).
  prefs: []
  type: TYPE_NORMAL
