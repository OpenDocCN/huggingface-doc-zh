- en: Using pipelines for a webserver
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Web服务器使用管道
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_webserver](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_webserver)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_webserver](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_webserver)
- en: Creating an inference engine is a complex topic, and the "best" solution will
    most likely depend on your problem space. Are you on CPU or GPU? Do you want the
    lowest latency, the highest throughput, support for many models, or just highly
    optimize 1 specific model? There are many ways to tackle this topic, so what we
    are going to present is a good default to get started which may not necessarily
    be the most optimal solution for you.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 创建推断引擎是一个复杂的主题，“最佳”解决方案很可能取决于您的问题空间。您是在CPU还是GPU上？您想要最低的延迟、最高的吞吐量、对许多模型的支持，还是只是高度优化一个特定模型？解决这个问题的方法有很多，因此我们将提供一个很好的默认值来开始，这可能不一定是最优的解决方案。
- en: The key thing to understand is that we can use an iterator, just like you would
    [on a dataset](pipeline_tutorial#using-pipelines-on-a-dataset), since a webserver
    is basically a system that waits for requests and treats them as they come in.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的理解是，我们可以使用一个迭代器，就像你在[数据集](pipeline_tutorial#using-pipelines-on-a-dataset)上使用的一样，因为Web服务器基本上是一个等待请求并按顺序处理它们的系统。
- en: Usually webservers are multiplexed (multithreaded, async, etc..) to handle various
    requests concurrently. Pipelines on the other hand (and mostly the underlying
    models) are not really great for parallelism; they take up a lot of RAM, so it’s
    best to give them all the available resources when they are running or it’s a
    compute-intensive job.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Web服务器是多路复用的（多线程、异步等），以处理各种请求。另一方面，管道（以及大多数底层模型）并不真正适合并行处理；它们占用大量RAM，因此最好在运行时为它们提供所有可用的资源，或者这是一个计算密集型的工作。
- en: We are going to solve that by having the webserver handle the light load of
    receiving and sending requests, and having a single thread handling the actual
    work. This example is going to use `starlette`. The actual framework is not really
    important, but you might have to tune or change the code if you are using another
    one to achieve the same effect.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过让Web服务器处理接收和发送请求的轻负载，并让单个线程处理实际工作来解决这个问题。这个示例将使用`starlette`。实际的框架并不是很重要，但如果您使用另一个框架来实现相同的效果，可能需要调整或更改代码。
- en: 'Create `server.py`:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`server.py`：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now you can start it with:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以启动它：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And you can query it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查询它：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And there you go, now you have a good idea of how to create a webserver!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解如何创建Web服务器了！
- en: 'What is really important is that we load the model only **once**, so there
    are no copies of the model on the webserver. This way, no unnecessary RAM is being
    used. Then the queuing mechanism allows you to do fancy stuff like maybe accumulating
    a few items before inferring to use dynamic batching:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 真正重要的是，我们只**一次**加载模型，因此在Web服务器上没有模型的副本。这样，就不会使用不必要的RAM。然后，排队机制允许您执行一些花哨的操作，比如可能在推断之前累积一些项目以使用动态批处理：
- en: The code sample below is intentionally written like pseudo-code for readability.
    Do not run this without checking if it makes sense for your system resources!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例故意以伪代码形式编写，以提高可读性。在未检查是否对您的系统资源有意义的情况下，请勿运行此代码！
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Again, the proposed code is optimized for readability, not for being the best
    code. First of all, there’s no batch size limit which is usually not a great idea.
    Next, the timeout is reset on every queue fetch, meaning you could wait much more
    than 1ms before running the inference (delaying the first request by that much).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，建议的代码是为了可读性而优化的，而不是为了成为最佳代码。首先，没有批处理大小限制，这通常不是一个好主意。其次，超时在每次队列获取时重置，这意味着您可能需要等待比1毫秒更长的时间才能运行推断（延迟第一个请求）。
- en: It would be better to have a single 1ms deadline.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最好设置一个单独的1毫秒截止时间。
- en: This will always wait for 1ms even if the queue is empty, which might not be
    the best since you probably want to start doing inference if there’s nothing in
    the queue. But maybe it does make sense if batching is really crucial for your
    use case. Again, there’s really no one best solution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使队列为空，这将始终等待1毫秒，这可能不是最好的，因为如果队列中没有东西，您可能希望开始进行推断。但如果批处理对您的用例非常关键，那么这可能是有意义的。再次强调，没有一个最佳解决方案。
- en: Few things you might want to consider
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你可能想考虑的几件事
- en: Error checking
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误检查
- en: 'There’s a lot that can go wrong in production: out of memory, out of space,
    loading the model might fail, the query might be wrong, the query might be correct
    but still fail to run because of a model misconfiguration, and so on.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中可能会出现很多问题：内存不足、空间不足、加载模型可能失败、查询可能错误、查询可能正确但由于模型配置错误而无法运行，等等。
- en: Generally, it’s good if the server outputs the errors to the user, so adding
    a lot of `try..except` statements to show those errors is a good idea. But keep
    in mind it may also be a security risk to reveal all those errors depending on
    your security context.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果服务器将错误输出给用户，那么添加许多`try..except`语句来显示这些错误是一个好主意。但请记住，根据您的安全上下文，公开所有这些错误也可能是一个安全风险。
- en: Circuit breaking
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 断路处理
- en: Webservers usually look better when they do circuit breaking. It means they
    return proper errors when they’re overloaded instead of just waiting for the query
    indefinitely. Return a 503 error instead of waiting for a super long time or a
    504 after a long time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当Web服务器过载时，通常最好进行断路处理。这意味着它们在过载时返回适当的错误，而不是无限期地等待查询。在等待超长时间后返回503错误，或者在很长时间后返回504错误。
- en: This is relatively easy to implement in the proposed code since there is a single
    queue. Looking at the queue size is a basic way to start returning errors before
    your webserver fails under load.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在建议的代码中实现这个相对容易，因为有一个单一的队列。查看队列大小是在Web服务器在负载下失败之前开始返回错误的基本方法。
- en: Blocking the main thread
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阻塞主线程
- en: Currently PyTorch is not async aware, and computation will block the main thread
    while running. That means it would be better if PyTorch was forced to run on its
    own thread/process. This wasn’t done here because the code is a lot more complex
    (mostly because threads and async and queues don’t play nice together). But ultimately
    it does the same thing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 目前PyTorch不支持异步操作，计算时会阻塞主线程。这意味着最好让PyTorch在自己的线程/进程上运行。这里没有这样做是因为代码更加复杂（主要是因为线程、异步和队列不太兼容）。但最终它做的事情是一样的。
- en: This would be important if the inference of single items were long (> 1s) because
    in this case, it means every query during inference would have to wait for 1s
    before even receiving an error.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单个项目的推断时间很长（> 1秒），这将是很重要的，因为在这种情况下，在推断期间每个查询都必须等待1秒才能收到错误。
- en: Dynamic batching
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态批处理
- en: In general, batching is not necessarily an improvement over passing 1 item at
    a time (see [batching details](./main_classes/pipelines#pipeline-batching) for
    more information). But it can be very effective when used in the correct setting.
    In the API, there is no dynamic batching by default (too much opportunity for
    a slowdown). But for BLOOM inference - which is a very large model - dynamic batching
    is **essential** to provide a decent experience for everyone.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，批处理不一定比逐个传递项目更好（有关更多信息，请参阅[批处理详细信息](./main_classes/pipelines#pipeline-batching)）。但在正确的环境中使用时，它可能非常有效。在API中，默认情况下没有动态批处理（太容易导致减速）。但对于BLOOM推断
    - 这是一个非常庞大的模型 - 动态批处理是**必不可少**的，以为每个人提供良好的体验。
