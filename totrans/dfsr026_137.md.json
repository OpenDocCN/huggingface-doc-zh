["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )\n```", "```py\n( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 5.0 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 1.0 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 original_size: Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None negative_original_size: Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> # !pip install opencv-python transformers accelerate\n>>> from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\n>>> from diffusers.utils import load_image\n>>> import numpy as np\n>>> import torch\n\n>>> import cv2\n>>> from PIL import Image\n\n>>> prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n>>> negative_prompt = \"low quality, bad quality, sketches\"\n\n>>> # download an image\n>>> image = load_image(\n...     \"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n... )\n\n>>> # initialize the models and pipeline\n>>> controlnet_conditioning_scale = 0.5  # recommended for good generalization\n>>> controlnet = ControlNetModel.from_pretrained(\n...     \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n... )\n>>> vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n>>> pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16\n... )\n>>> pipe.enable_model_cpu_offload()\n\n>>> # get canny image\n>>> image = np.array(image)\n>>> image = cv2.Canny(image, 100, 200)\n>>> image = image[:, :, None]\n>>> image = np.concatenate([image, image, image], axis=2)\n>>> canny_image = Image.fromarray(image)\n\n>>> # generate image\n>>> image = pipe(\n...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image\n... ).images[0]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```"]