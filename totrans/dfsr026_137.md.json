["```py\n>>> # !pip install opencv-python transformers accelerate\n>>> from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\n>>> from diffusers.utils import load_image\n>>> import numpy as np\n>>> import torch\n\n>>> import cv2\n>>> from PIL import Image\n\n>>> prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n>>> negative_prompt = \"low quality, bad quality, sketches\"\n\n>>> # download an image\n>>> image = load_image(\n...     \"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n... )\n\n>>> # initialize the models and pipeline\n>>> controlnet_conditioning_scale = 0.5  # recommended for good generalization\n>>> controlnet = ControlNetModel.from_pretrained(\n...     \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n... )\n>>> vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n>>> pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16\n... )\n>>> pipe.enable_model_cpu_offload()\n\n>>> # get canny image\n>>> image = np.array(image)\n>>> image = cv2.Canny(image, 100, 200)\n>>> image = image[:, :, None]\n>>> image = np.concatenate([image, image, image], axis=2)\n>>> canny_image = Image.fromarray(image)\n\n>>> # generate image\n>>> image = pipe(\n...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image\n... ).images[0]\n```"]