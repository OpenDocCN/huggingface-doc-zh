# Megatron-LM

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/usage_guides/megatron_lm](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm)

[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)ä½¿å¾—å¯ä»¥åœ¨è§„æ¨¡ä¸Šè®­ç»ƒå¤§å‹transformerè¯­è¨€æ¨¡å‹ã€‚å®ƒä¸ºé¢„è®­ç»ƒåŸºäºtransformerçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚[GPT](https://arxiv.org/abs/2005.14165)ï¼ˆä»…è§£ç å™¨ï¼‰ã€[BERT](https://arxiv.org/pdf/1810.04805.pdf)ï¼ˆä»…ç¼–ç å™¨ï¼‰å’Œ[T5](https://arxiv.org/abs/1910.10683)ï¼ˆç¼–ç å™¨-è§£ç å™¨ï¼‰ï¼‰æä¾›äº†é«˜æ•ˆçš„å¼ é‡ã€ç®¡é“å’ŒåŸºäºåºåˆ—çš„æ¨¡å‹å¹¶è¡Œæ€§ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ä»¥åŠå¹•åå·¥ä½œåŸç†ï¼Œè¯·å‚è€ƒgithub[repo](https://github.com/NVIDIA/Megatron-LM)ã€‚

## ä»€ä¹ˆæ˜¯é›†æˆçš„ï¼Ÿ

Accelerateé›†æˆäº†Megatron-LMçš„ä»¥ä¸‹åŠŸèƒ½ï¼Œä»¥å®ç°BERTï¼ˆç¼–ç å™¨ï¼‰ã€GPTï¼ˆè§£ç å™¨ï¼‰æˆ–T5æ¨¡å‹ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰çš„å¤§è§„æ¨¡é¢„è®­ç»ƒ/å¾®è°ƒï¼š

a. **å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰**ï¼šå‡å°‘å†…éƒ¨èŠ‚ç‚¹æ’åä¹‹é—´çš„é€šä¿¡ï¼Œå‡å°‘å†…å­˜å ç”¨ã€‚æ¯ä¸ªå¼ é‡è¢«åˆ†å‰²æˆå¤šä¸ªå—ï¼Œæ¯ä¸ªå—éƒ½é©»ç•™åœ¨ä¸åŒçš„GPUä¸Šã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç›¸åŒçš„æ•°æ®å°æ‰¹é‡ç”±æ¯ä¸ªå—ç‹¬ç«‹å¹¶å¹¶è¡Œå¤„ç†ï¼Œç„¶åé€šè¿‡æ‰€æœ‰GPUè¿›è¡ŒåŒæ­¥ï¼ˆ`all-reduce`æ“ä½œï¼‰ã€‚åœ¨ç®€å•çš„transformerå±‚ä¸­ï¼Œè¿™å¯¼è‡´å‰å‘è·¯å¾„ä¸­æœ‰2ä¸ª`all-reduces`ï¼Œåå‘è·¯å¾„ä¸­æœ‰2ä¸ª`all-reduces`ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒç ”ç©¶è®ºæ–‡[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)å’ŒğŸ¤—åšå®¢æ–‡ç« [BLOOM TrainingèƒŒåçš„æŠ€æœ¯](https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism)çš„è¿™ä¸€éƒ¨åˆ†ã€‚

b. **ç®¡é“å¹¶è¡Œï¼ˆPPï¼‰**ï¼šé€šè¿‡èŠ‚ç‚¹é—´å¹¶è¡ŒåŒ–å‡å°‘å†…å­˜å ç”¨å¹¶å®ç°å¤§è§„æ¨¡è®­ç»ƒã€‚é€šè¿‡PipeDream-Flushè°ƒåº¦/1F1Bè°ƒåº¦å’Œäº¤æ›¿1F1Bè°ƒåº¦å‡å°‘äº†å¤©çœŸPPçš„æ°”æ³¡ã€‚å±‚åœ¨PPé˜¶æ®µå‡åŒ€åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰`24`å±‚ï¼Œæˆ‘ä»¬æœ‰`4`ä¸ªGPUç”¨äºç®¡é“å¹¶è¡Œï¼Œæ¯ä¸ªGPUå°†æœ‰`6`å±‚ï¼ˆ24/4ï¼‰ã€‚æœ‰å…³å‡å°‘PPç©ºé—²æ—¶é—´çš„è°ƒåº¦çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒç ”ç©¶è®ºæ–‡[Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)å’ŒğŸ¤—åšå®¢æ–‡ç« [BLOOM TrainingèƒŒåçš„æŠ€æœ¯](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism)çš„è¿™ä¸€éƒ¨åˆ†ã€‚

c. **åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰**ï¼šåœ¨ä¸å¢åŠ ä»»ä½•é¢å¤–é€šä¿¡çš„æƒ…å†µä¸‹å‡å°‘å†…å­˜å ç”¨ã€‚ä»…åœ¨ä½¿ç”¨TPæ—¶é€‚ç”¨ã€‚å®ƒå‡å°‘äº†æ¿€æ´»å†…å­˜çš„éœ€æ±‚ï¼Œå› ä¸ºå®ƒé€šè¿‡åœ¨`all-reduce`åæ›¿æ¢ç›¸åŒçš„å‰¯æœ¬åˆ°å¼ é‡å¹¶è¡Œæ’åä¸Šæ¥é˜²æ­¢è¿™ç§æƒ…å†µï¼Œé€šè¿‡`reduce-scatter`æ›¿æ¢`no-op`æ“ä½œï¼Œ`all-reduce = reduce-scatter + all-gather`ï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå¤§é‡æ¿€æ´»å†…å­˜è€Œä¸å¢åŠ é€šä¿¡æˆæœ¬ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒæ²¿ç€åºåˆ—ç»´åº¦åˆ†ç‰‡æ¯ä¸ªtransformerå±‚çš„è¾“å‡ºï¼Œä¾‹å¦‚ï¼Œå¦‚æœåºåˆ—é•¿åº¦ä¸º`1024`ï¼ŒTPå¤§å°ä¸º`4`ï¼Œæ¯ä¸ªGPUå°†æœ‰`256`ä¸ªæ ‡è®°ï¼ˆ1024/4ï¼‰ç”¨äºæ¯ä¸ªæ ·æœ¬ã€‚è¿™å¢åŠ äº†æ”¯æŒè®­ç»ƒçš„æ‰¹é‡å¤§å°ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒç ”ç©¶è®ºæ–‡[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf)ã€‚

d. **æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰** é€šè¿‡åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ï¼šé€šè¿‡åœ¨ DP æ’åä¹‹é—´åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ¥å‡å°‘å†…å­˜å ç”¨é‡ï¼ˆä¸ä¼ ç»Ÿæ–¹æ³•åœ¨æ•°æ®å¹¶è¡Œæ’åä¹‹é—´å¤åˆ¶ä¼˜åŒ–å™¨çŠ¶æ€ç›¸æ¯”ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒçš„ Adam ä¼˜åŒ–å™¨æ—¶ï¼Œæ¯ä¸ªå‚æ•°å ç”¨ 12 å­—èŠ‚çš„å†…å­˜ã€‚è¿™äº›å†…å­˜å‡åŒ€åˆ†å¸ƒåœ¨ GPU ä¸Šï¼Œå³å¦‚æœæœ‰ 4 ä¸ª GPUï¼Œåˆ™æ¯ä¸ªå‚æ•°å°†å ç”¨ 3 å­—èŠ‚ï¼ˆ12/4ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç ”ç©¶è®ºæ–‡ [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf) å’Œ ğŸ¤— åšå®¢ [BLOOM Training èƒŒåçš„æŠ€æœ¯](https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism) çš„ä»¥ä¸‹éƒ¨åˆ†ã€‚

e. **é€‰æ‹©æ€§æ¿€æ´»é‡è®¡ç®—**ï¼šé€šè¿‡æ™ºèƒ½æ¿€æ´»æ£€æŸ¥ç‚¹å‡å°‘æ¿€æ´»çš„å†…å­˜å ç”¨ã€‚å®ƒä¸ä¼šå­˜å‚¨å ç”¨å¤§é‡å†…å­˜çš„æ¿€æ´»ï¼ŒåŒæ—¶é‡æ–°è®¡ç®—é€Ÿåº¦å¿«ï¼Œä»è€Œåœ¨å†…å­˜å’Œé‡æ–°è®¡ç®—ä¹‹é—´å–å¾—å¾ˆå¥½çš„æŠ˜è¡·ã€‚ä¾‹å¦‚ï¼Œå¯¹äº GPT-3ï¼Œè¿™å¯¼è‡´æ¿€æ´»æ‰€éœ€å†…å­˜å‡å°‘äº† 70%ï¼Œè€Œä»…ä»¥ 2.7% çš„ FLOPs å¼€é”€é‡æ–°è®¡ç®—æ¿€æ´»ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç ”ç©¶è®ºæ–‡ [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf)ã€‚

f. **èåˆå†…æ ¸**ï¼šèåˆ Softmaxã€æ··åˆç²¾åº¦èåˆå±‚å½’ä¸€åŒ–å’Œèåˆæ¢¯åº¦ç´¯ç§¯ä»¥æƒé‡æ¢¯åº¦è®¡ç®—çº¿æ€§å±‚ã€‚PyTorch JIT ç¼–è¯‘çš„èåˆ GeLU å’Œèåˆ Bias+Dropout+Residual additionã€‚

g. **æ”¯æŒç´¢å¼•æ•°æ®é›†**ï¼šç”¨äºå¤§è§„æ¨¡è®­ç»ƒçš„æ•°æ®é›†çš„é«˜æ•ˆäºŒè¿›åˆ¶æ ¼å¼ã€‚æ”¯æŒ `mmap`ã€`cached` ç´¢å¼•æ–‡ä»¶å’Œ `lazy` åŠ è½½å™¨æ ¼å¼ã€‚

h. **æ£€æŸ¥ç‚¹é‡å¡‘å’Œäº’æ“ä½œæ€§**ï¼šç”¨äºå°†å˜é‡å¼ é‡å’Œç®¡é“å¹¶è¡Œå¤§å°çš„ Megatron-LM æ£€æŸ¥ç‚¹é‡å¡‘ä¸ºå¤‡å—å–œçˆ±çš„ ğŸ¤— Transformers åˆ†ç‰‡æ£€æŸ¥ç‚¹çš„å®ç”¨ç¨‹åºï¼Œå› ä¸ºå®ƒä¸ä¼—å¤šå·¥å…·ï¼ˆå¦‚ ğŸ¤— Accelerate Big Model Inferenceã€Megatron-DeepSpeed Inference ç­‰ï¼‰å…·æœ‰è‰¯å¥½çš„æ”¯æŒã€‚è¿˜æ”¯æŒå°† ğŸ¤— Transformers åˆ†ç‰‡æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºå˜é‡å¼ é‡å’Œç®¡é“å¹¶è¡Œå¤§å°çš„ Megatron-LM æ£€æŸ¥ç‚¹ï¼Œç”¨äºå¤§è§„æ¨¡è®­ç»ƒã€‚

## å…ˆå†³æ¡ä»¶

æ‚¨éœ€è¦å®‰è£…æœ€æ–°çš„ pytorchã€cudaã€nccl å’Œ NVIDIA [APEX](https://github.com/NVIDIA/apex#quick-start) ç‰ˆæœ¬ä»¥åŠ nltk åº“ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://github.com/NVIDIA/Megatron-LM#setup)ã€‚å¦ä¸€ç§è®¾ç½®ç¯å¢ƒçš„æ–¹æ³•æ˜¯æ‹‰å–ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰€éœ€å®‰è£…çš„ NVIDIA PyTorch å®¹å™¨ã€‚

ä»¥ä¸‹æ˜¯è®¾ç½® conda ç¯å¢ƒçš„é€æ­¥æ–¹æ³•ï¼š

1.  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```py
conda create --name ml
```

1.  å‡è®¾æœºå™¨å·²å®‰è£… CUDA 11.3ï¼Œå®‰è£…ç›¸åº”çš„ PyTorch GPU ç‰ˆæœ¬

```py
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
```

1.  å®‰è£… Nvidia APEX

```py
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
cd ..
```

1.  å®‰è£… Megatron-LM

```py
pip install git+https://github.com/huggingface/Megatron-LM.git
```

## åŠ é€Ÿ Megatron-LM æ’ä»¶

é‡è¦åŠŸèƒ½é€šè¿‡ `accelerate config` å‘½ä»¤ç›´æ¥æ”¯æŒã€‚ä¸‹é¢æ˜¾ç¤ºäº†ä½¿ç”¨ Megatron-LM åŠŸèƒ½çš„ç›¸åº”é—®é¢˜çš„ç¤ºä¾‹ï¼š

```py
:~$ accelerate config --config_file "megatron_gpt_config.yaml"
In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 2
How many different machines will you use (use more than 1 for multi-node training)? [1]: 
Do you want to use DeepSpeed? [yes/NO]: 
Do you want to use FullyShardedDataParallel? [yes/NO]: 
Do you want to use Megatron-LM ? [yes/NO]: yes
What is the Tensor Parallelism degree/size? [1]:2
Do you want to enable Sequence Parallelism? [YES/no]: 
What is the Pipeline Parallelism degree/size? [1]:2
What is the number of micro-batches? [1]:2
Do you want to enable selective activation recomputation? [YES/no]: 
Do you want to use distributed optimizer which shards optimizer state and gradients across data parallel ranks? [YES/no]: 
What is the gradient clipping value based on global L2 Norm (0 to disable)? [1.0]: 
How many GPU(s) should be used for distributed training? [1]:4
Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: bf16
```

ç»“æœé…ç½®å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
~$ cat megatron_gpt_config.yaml 
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MEGATRON_LM
downcast_bf16: 'no'
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config:
  megatron_lm_gradient_clipping: 1.0
  megatron_lm_num_micro_batches: 2
  megatron_lm_pp_degree: 2
  megatron_lm_recompute_activations: true
  megatron_lm_sequence_parallelism: true
  megatron_lm_tp_degree: 2
  megatron_lm_use_distributed_optimizer: true
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
use_cpu: false
```

æˆ‘ä»¬å°†ä»¥ GPT é¢„è®­ç»ƒä¸ºä¾‹ã€‚è¦ä½¿ç”¨ Megatron-LMï¼Œå¯¹å®˜æ–¹çš„ `run_clm_no_trainer.py` è¿›è¡Œçš„æœ€å°æ›´æ”¹å¦‚ä¸‹ï¼š

1.  ç”±äº Megatron-LM ä½¿ç”¨è‡ªå·±çš„ä¼˜åŒ–å™¨å®ç°ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ä¸ä¹‹å…¼å®¹çš„ç›¸åº”è°ƒåº¦å™¨ã€‚å› æ­¤ï¼Œä»…æ”¯æŒ Megatron-LM çš„è°ƒåº¦å™¨ã€‚ç”¨æˆ·éœ€è¦åˆ›å»º `accelerate.utils.MegatronLMDummyScheduler`ã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š

```py
from accelerate.utils import MegatronLMDummyScheduler

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    lr_scheduler = MegatronLMDummyScheduler(
        optimizer=optimizer,
        total_num_steps=args.max_train_steps,
        warmup_num_steps=args.num_warmup_steps,
    )
else:
    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )
```

1.  ç°åœ¨éœ€è¦äº†è§£æ€»æ‰¹é‡å¤§å°çš„ç»†èŠ‚ï¼Œéœ€è¦è®¤çŸ¥å¼ é‡å’Œç®¡é“å¹¶è¡Œå¤§å°ã€‚è·å–æœ‰æ•ˆæ€»æ‰¹é‡å¤§å°çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š

```py
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size
else:
    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
```

1.  åœ¨ä½¿ç”¨ Megatron-LM æ—¶ï¼ŒæŸå¤±å·²ç»åœ¨æ•°æ®å¹¶è¡Œç»„ä¸­è¿›è¡Œäº†å¹³å‡

```py
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    losses.append(loss)
else:
    losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    losses = torch.tensor(losses)
else:
    losses = torch.cat(losses)
```

1.  å¯¹äº Megatron-LMï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `accelerator.save_state` æ¥ä¿å­˜æ¨¡å‹

```py
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    accelerator.save_state(args.output_dir)
else:
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(
        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
    )
```

å°±è¿™æ ·äº†ï¼æˆ‘ä»¬å‡†å¤‡å¥½å‡ºå‘äº†ğŸš€ã€‚è¯·åœ¨è·¯å¾„`accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py`çš„ç¤ºä¾‹è„šæœ¬ä¸­æ‰¾åˆ°ç¤ºä¾‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨4ä¸ªA100-80GB GPUæ¥è¿è¡Œ`gpt-large`æ¨¡å‹æ¶æ„ã€‚

```py
accelerate launch --config_file megatron_gpt_config.yaml \
examples/by_feature/megatron_lm_gpt_pretraining.py \
--config_name "gpt2-large" \
--tokenizer_name "gpt2-large" \
--dataset_name wikitext \
--dataset_config_name wikitext-2-raw-v1 \
--block_size 1024 \
--learning_rate 5e-5 \
--per_device_train_batch_size 24 \
--per_device_eval_batch_size 24 \
--num_train_epochs 5 \
--with_tracking \
--report_to "wandb" \
--output_dir "awesome_model"
```

ä»¥ä¸‹æ˜¯è¾“å‡ºæ—¥å¿—ä¸­çš„ä¸€äº›é‡è¦æ‘˜å½•ï¼š

```py
Loading extension module fused_dense_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 3.569 seconds
 > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
Building gpt model in the pre-training mode.
The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.
Preparing dataloader
Preparing dataloader
Preparing model
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 210753280
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 209445120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 210753280
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 209445120
Preparing optimizer
Preparing scheduler
> learning rate decay style: linear
10/10/2022 22:57:22 - INFO - __main__ - ***** Running training *****
10/10/2022 22:57:22 - INFO - __main__ -   Num examples = 2318
10/10/2022 22:57:22 - INFO - __main__ -   Num Epochs = 5
10/10/2022 22:57:22 - INFO - __main__ -   Instantaneous batch size per device = 24
10/10/2022 22:57:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 48
10/10/2022 22:57:22 - INFO - __main__ -   Gradient Accumulation steps = 1
10/10/2022 22:57:22 - INFO - __main__ -   Total optimization steps = 245
 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                 | 49/245 [01:04<04:09,  1.27s/it]
 10/10/2022 22:58:29 - INFO - __main__ - epoch 0: perplexity: 1222.1594275215962 eval_loss: 7.10837459564209
 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 98/245 [02:10<03:07,  1.28s/it]
 10/10/2022 22:59:35 - INFO - __main__ - epoch 1: perplexity: 894.5236583794557 eval_loss: 6.796291351318359
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 147/245 [03:16<02:05,  1.28s/it]
 10/10/2022 23:00:40 - INFO - __main__ - epoch 2: perplexity: 702.8458788508042 eval_loss: 6.555137634277344
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 196/245 [04:22<01:02,  1.28s/it]
 10/10/2022 23:01:46 - INFO - __main__ - epoch 3: perplexity: 600.3220028695281 eval_loss: 6.39746618270874
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245/245 [05:27<00:00,  1.28s/it]
```

è¿˜æœ‰è®¸å¤šå…¶ä»–é€‰é¡¹/åŠŸèƒ½å¯ä»¥ä½¿ç”¨`accelerate.utils.MegatronLMPlugin`è®¾ç½®ã€‚

## åˆ©ç”¨ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤å’ŒMegatron-LMç´¢å¼•æ•°æ®é›†çš„é«˜çº§åŠŸèƒ½

è¦åˆ©ç”¨æ›´å¤šåŠŸèƒ½ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ã€‚

1.  ä»¥ä¸‹æ˜¯åœ¨ä½¿ç”¨Megatron-LMæ—¶è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤æ‰€éœ€çš„æ›´æ”¹ç¤ºä¾‹ã€‚æ‚¨å°†å®ç°`accelerate.utils.AbstractTrainStep`æˆ–ç»§æ‰¿è‡ªå…¶ç›¸åº”çš„å­ç±»`accelerate.utils.GPTTrainStep`ã€`accelerate.utils.BertTrainStep`æˆ–`accelerate.utils.T5TrainStep`ã€‚

```py
from accelerate.utils import MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group

# Custom loss function for the Megatron model
class GPTTrainStepWithCustomLoss(GPTTrainStep):
    def __init__(self, megatron_args, **kwargs):
        super().__init__(megatron_args)
        self.kwargs = kwargs

    def get_loss_func(self):
        def loss_func(inputs, loss_mask, output_tensor):
            batch_size, seq_length = output_tensor.shape
            losses = output_tensor.float()
            loss_mask = loss_mask.view(-1).float()
            loss = losses.view(-1) * loss_mask

            # Resize and average loss per sample
            loss_per_sample = loss.view(batch_size, seq_length).sum(axis=1)
            loss_mask_per_sample = loss_mask.view(batch_size, seq_length).sum(axis=1)
            loss_per_sample = loss_per_sample / loss_mask_per_sample

            # Calculate and scale weighting
            weights = torch.stack([(inputs == kt).float() for kt in self.kwargs["keytoken_ids"]]).sum(axis=[0, 2])
            weights = 1.0 + self.kwargs["alpha"] * weights
            # Calculate weighted average
            weighted_loss = (loss_per_sample * weights).mean()

            # Reduce loss across data parallel groups
            averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])

            return weighted_loss, {"lm loss": averaged_loss[0]}

        return loss_func

    def get_forward_step_func(self):
        def forward_step(data_iterator, model):
            """Forward step."""
            # Get the batch.
            tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)
            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)

            return output_tensor, partial(self.loss_func, tokens, loss_mask)

        return forward_step

def main():
    # Custom loss function for the Megatron model
    keytoken_ids = []
    keywords = ["plt", "pd", "sk", "fit", "predict", " plt", " pd", " sk", " fit", " predict"]
    for keyword in keywords:
        ids = tokenizer([keyword]).input_ids[0]
        if len(ids) == 1:
            keytoken_ids.append(ids[0])
    accelerator.print(f"Keytoken ids: {keytoken_ids}")
    accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss
    accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {
        "keytoken_ids": keytoken_ids,
        "alpha": 0.25,
    }
```

1.  è¦ä½¿ç”¨Megatron-LMæ•°æ®é›†ï¼Œéœ€è¦è¿›è¡Œä¸€äº›æ›´æ”¹ã€‚è¿™äº›æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨ä»…åœ¨æ¯ä¸ªå¼ é‡å¹¶è¡Œç»„çš„ç­‰çº§0ä¸Šå¯ç”¨ã€‚å› æ­¤ï¼Œå­˜åœ¨æ•°æ®åŠ è½½å™¨ä¸å¯ç”¨çš„ç­‰çº§ï¼Œè¿™éœ€è¦å¯¹è®­ç»ƒå¾ªç¯è¿›è¡Œè°ƒæ•´ã€‚èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹æ˜¾ç¤ºäº†ğŸ¤— Accelerateæœ‰å¤šä¹ˆçµæ´»å’Œå¯æ‰©å±•ã€‚æ‰€éœ€çš„æ›´æ”¹å¦‚ä¸‹ã€‚

a. å¯¹äºMegatron-LMç´¢å¼•æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`MegatronLMDummyDataLoader`å¹¶å°†æ‰€éœ€çš„æ•°æ®é›†å‚æ•°ä¼ é€’ç»™å®ƒï¼Œä¾‹å¦‚`data_path`ã€`seq_length`ç­‰ã€‚è¯·å‚é˜…[æ­¤å¤„](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L804)ä»¥è·å–å¯ç”¨å‚æ•°çš„åˆ—è¡¨ã€‚

```py
from accelerate.utils import MegatronLMDummyDataLoader

megatron_dataloader_config = {
    "data_path": args.data_path,
    "splits_string": args.splits_string,
    "seq_length": args.block_size,
    "micro_batch_size": args.per_device_train_batch_size,
}
megatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)
accelerator.state.megatron_lm_plugin.megatron_dataset_flag = True
```

b. `megatron_dataloader`é‡å¤3æ¬¡ï¼Œä»¥æ ¹æ®`args.splits_string`æ¯”ä¾‹è·å–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚

```py
model, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(
    model, optimizer, lr_scheduler, megatron_dataloader, megatron_dataloader, megatron_dataloader
)
```

c. ç”±äºæ•°æ®åŠ è½½å™¨ä»…åœ¨å¼ é‡å¹¶è¡Œç­‰çº§0ä¸Šå¯ç”¨ï¼Œå› æ­¤éœ€è¦å¯¹è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯è¿›è¡Œæ›´æ”¹ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦åœ¨æ•°æ®åŠ è½½å™¨ä¸æ˜¯`None`æ—¶è¿­ä»£ï¼Œå¦åˆ™æä¾›ç©ºå­—å…¸ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨`while`å¾ªç¯è¿›è¡Œå¾ªç¯ï¼Œå¹¶åœ¨`completed_steps`ç­‰äº`args.max_train_steps`æ—¶ä¸­æ–­ã€‚è¿™ç±»ä¼¼äºMegatron-LMè®¾ç½®ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨Megaton-LMç´¢å¼•æ•°æ®é›†æ—¶å¿…é¡»æä¾›`max_train_steps`ã€‚è¿™æ˜¾ç¤ºäº†ğŸ¤— Accelerateæœ‰å¤šä¹ˆçµæ´»å’Œå¯æ‰©å±•ã€‚

```py
while completed_steps < args.max_train_steps:
    model.train()
    batch = next(train_dataloader) if train_dataloader is not None else {}
    outputs = model(**batch)
    loss = outputs.loss
    ...

    if completed_steps % eval_interval == 0:
        eval_completed_steps = 0
        losses = []
        while eval_completed_steps < eval_iters:
            model.eval()
            with torch.no_grad():
                batch = next(eval_dataloader) if eval_dataloader is not None else {}
                outputs = model(**batch)
```

## ç”¨äºæ£€æŸ¥ç‚¹é‡å¡‘å’Œäº’æ“ä½œæ€§çš„å®ç”¨ç¨‹åº

1.  è¿™äº›è„šæœ¬ä½äºğŸ¤— Transformersåº“ä¸­çš„ç›¸åº”æ¨¡å‹ä¸‹ã€‚ç›®å‰ï¼Œå®ƒé€‚ç”¨äºGPTæ¨¡å‹[checkpoint_reshaping_and_interoperability.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py)

1.  ä»¥ä¸‹æ˜¯å°†æ£€æŸ¥ç‚¹ä»Megatron-LMè½¬æ¢ä¸ºé€šç”¨ğŸ¤— Transformersåˆ†ç‰‡æ£€æŸ¥ç‚¹çš„ç¤ºä¾‹ã€‚

```py
python checkpoint_reshaping_and_interoperability.py \
--convert_checkpoint_from_megatron_to_transformers \
--load_path "gpt/iter_0005000" \
--save_path "gpt/trfs_checkpoint" \
--max_shard_size "200MB" \
--tokenizer_name "gpt2" \
--print-checkpoint-structure
```

1.  å°†transformersçš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºmegatronï¼Œä½¿ç”¨`tp_size=2`ï¼Œ`pp_size=2`å’Œ`dp_size=2`ã€‚

```py
python checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \
--load_path "gpt/trfs_checkpoint" \
--save_path "gpt/megatron_lm_checkpoint" \
--target_tensor_model_parallel_size 2 \
--target_pipeline_model_parallel_size 2 \
--target_data_parallel_size 2 \
--target_params_dtype "bf16" \
--make_vocab_size_divisible_by 128 \
--use_distributed_optimizer \
--print-checkpoint-structure
```

## Megatron-LM GPTæ¨¡å‹æ”¯æŒè¿”å›å¯¹æ•°å’Œç”¨äºæ–‡æœ¬ç”Ÿæˆçš„megatron_generateå‡½æ•°

1.  è¿”å›å¯¹æ•°éœ€è¦åœ¨MegatronLMPluginä¸­è®¾ç½®`require_logits=True`ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚è¿™äº›å°†åœ¨ç®¡é“çš„æœ€åé˜¶æ®µå¯ç”¨ã€‚

```py
megatron_lm_plugin = MegatronLMPlugin(return_logits=True)
```

1.  Megatron-LM GPTæ¨¡å‹çš„`megatron_generate`æ–¹æ³•ï¼šå½“ä½¿ç”¨è´ªå©ªé‡‡æ ·æˆ–ä¸å¸¦top_k/top_pé‡‡æ ·æ—¶ï¼Œå°†ä½¿ç”¨å¼ é‡å’Œç®¡é“å¹¶è¡Œæ€§æ¥å®Œæˆä¸€æ‰¹è¾“å…¥çš„ç”Ÿæˆï¼Œä»¥åŠå½“ä½¿ç”¨æ³¢æŸæœç´¢è§£ç æ—¶ï¼Œç”¨äºå•ä¸ªæç¤ºè¾“å…¥çš„ç”Ÿæˆã€‚ä»…æ”¯æŒtransformers generateçš„ä¸€éƒ¨åˆ†åŠŸèƒ½ã€‚è¿™å°†æœ‰åŠ©äºé€šè¿‡å¼ é‡å’Œç®¡é“å¹¶è¡Œæ€§ä½¿ç”¨å¤§å‹æ¨¡å‹è¿›è¡Œç”Ÿæˆï¼ˆé»˜è®¤æƒ…å†µä¸‹å·²ç»è¿›è¡Œäº†é”®å€¼ç¼“å­˜å¹¶ä½¿ç”¨èåˆå†…æ ¸ï¼‰ã€‚è¿™éœ€è¦å°†æ•°æ®å¹¶è¡Œå¤§å°è®¾ç½®ä¸º1ï¼Œç¦ç”¨åºåˆ—å¹¶è¡Œæ€§å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ã€‚è¿˜éœ€è¦æŒ‡å®šåˆ†è¯å™¨çš„è¯æ±‡æ–‡ä»¶å’Œåˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºäº†å¦‚ä½•é…ç½®å’Œä½¿ç”¨Megatron-LM GPTæ¨¡å‹çš„`megatron_generate`æ–¹æ³•ã€‚

```py
# specifying tokenizer's vocab and merges file
vocab_file = os.path.join(args.resume_from_checkpoint, "vocab.json")
merge_file = os.path.join(args.resume_from_checkpoint, "merges.txt")
other_megatron_args = {"vocab_file": vocab_file, "merge_file": merge_file}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)

# inference using `megatron_generate` functionality
tokenizer.pad_token = tokenizer.eos_token
max_new_tokens = 64
batch_texts = [
    "Are you human?",
    "The purpose of life is",
    "The arsenal was constructed at the request of",
    "How are you doing these days?",
]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)

# top-p sampling
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"],
    batch_encodings["attention_mask"],
    max_new_tokens=max_new_tokens,
    top_p=0.8,
    top_p_decay=0.5,
    temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# top-k sampling
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"],
    batch_encodings["attention_mask"],
    max_new_tokens=max_new_tokens,
    top_k=50,
    temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# adding `bos` token at the start
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"], batch_encodings["attention_mask"], max_new_tokens=max_new_tokens, add_BOS=True
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# beam search => only takes single prompt
batch_texts = ["The purpose of life is"]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"],
    batch_encodings["attention_mask"],
    max_new_tokens=max_new_tokens,
    num_beams=20,
    length_penalty=1.5,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)
```

1.  æœ‰å…³åœ¨ Megatron-LM GPT æ¨¡å‹ä¸­ä½¿ç”¨ `megatron_generate` æ–¹æ³•çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œè¯·å‚é˜… [megatron_gpt2_generation.py](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/inference/megatron_gpt2_generation.py)ï¼Œé…ç½®æ–‡ä»¶ä¸º [megatron_lm_gpt_generate_config.yaml](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/Configs/megatron_lm_gpt_generate_config.yaml)ã€‚å¸¦æœ‰ accelerate launch å‘½ä»¤çš„ bash è„šæœ¬ä½äº [megatron_lm_gpt_generate.sh](https://github.com/pacman100/accelerate-megatron-test/blob/main/megatron_lm_gpt_generate.sh)ã€‚è„šæœ¬çš„è¾“å‡ºæ—¥å¿—ä½äº [megatron_lm_gpt_generate.log](https://github.com/pacman100/accelerate-megatron-test/blob/main/output_logs/megatron_lm_gpt_generate.log)ã€‚

## æ”¯æŒ ROPE å’Œ ALiBi ä½ç½®åµŒå…¥ä»¥åŠ Multi-Query Attention

1.  å¯¹äº ROPE/ALiBi æ³¨æ„åŠ›ï¼Œå°† `position_embedding_type` ä¸ `("absolute" | "rotary" | "alibi")` ä¼ é€’ç»™ `MegatronLMPlugin`ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```py
other_megatron_args = {"position_embedding_type": "alibi"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
```

1.  å¯¹äº Multi-Query Attentionï¼Œå°† `attention_head_type` ä¸ `("multihead" | "multiquery")` ä¼ é€’ç»™ `MegatronLMPlugin`ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```py
other_megatron_args = {"attention_head_type": "multiquery"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
```

## æ³¨æ„äº‹é¡¹

1.  æ”¯æŒ Transformers GPT2ã€Megatron-BERT å’Œ T5 æ¨¡å‹ã€‚è¿™æ¶µç›–äº†ä»…è§£ç å™¨ã€ä»…ç¼–ç å™¨å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç±»ã€‚

1.  ç”±äºç®¡é“ã€å¼ é‡å’Œæ•°æ®å¹¶è¡ŒèƒŒåå­˜åœ¨ç›¸å½“å¤æ‚çš„ç›¸äº’ä½œç”¨ï¼Œå› æ­¤ä»…ä»æ¨¡å‹å‰å‘ä¼ é€’ä¸­è¿”å›æŸå¤±ã€‚`model(**batch_data)` è°ƒç”¨è¿”å›è·¨æ•°æ®å¹¶è¡Œç­‰çº§å¹³å‡çš„æŸå¤±ã€‚è¿™å¯¹äºå¤§å¤šæ•°æƒ…å†µæ˜¯å¯ä»¥çš„ï¼Œå…¶ä¸­ä½¿ç”¨ Megatron-LM åŠŸèƒ½è¿è¡Œé¢„è®­ç»ƒä½œä¸šï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾ä½¿ç”¨æŸå¤±è®¡ç®— `perplexity`ã€‚å¯¹äº GPT æ¨¡å‹ï¼Œé™¤äº†æŸå¤±ä¹‹å¤–è¿˜æ”¯æŒè¿”å› logitsã€‚è¿™äº› logits ä¸ä¼šåœ¨æ•°æ®å¹¶è¡Œç­‰çº§ä¹‹é—´æ”¶é›†ã€‚ä½¿ç”¨ `accelerator.utils.gather_across_data_parallel_groups` æ¥æ”¶é›†è·¨æ•°æ®å¹¶è¡Œç­‰çº§çš„ logitsã€‚è¿™äº› logits ä»¥åŠæ ‡ç­¾å¯ç”¨äºè®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡ã€‚

1.  ä¸»è¿›ç¨‹æ˜¯æœ€åä¸€ä¸ªç­‰çº§ï¼Œå› ä¸ºæŸå¤±/Logits åœ¨ç®¡é“çš„æœ€åé˜¶æ®µå¯ç”¨ã€‚åœ¨ä½¿ç”¨ Megatron-LM é›†æˆæ—¶ï¼Œ`accelerator.is_main_process` å’Œ `accelerator.is_local_main_process` åœ¨æœ€åä¸€ä¸ªç­‰çº§è¿”å› `True`ã€‚

1.  åœ¨ `accelerator.prepare` è°ƒç”¨ä¸­ï¼Œå°†ä¸ºç»™å®šçš„ Transformers æ¨¡å‹åˆ›å»ºä¸€ä¸ªå…·æœ‰éšæœºæƒé‡çš„ Megatron-LM æ¨¡å‹ã€‚è¯·ä½¿ç”¨ `accelerator.load_state` åŠ è½½å…·æœ‰åŒ¹é… TPã€PP å’Œ DP åˆ†åŒºçš„ Megatron-LM æ£€æŸ¥ç‚¹ã€‚

1.  ç›®å‰ï¼Œæ£€æŸ¥ç‚¹é‡å¡‘å’Œäº’æ“ä½œæ€§æ”¯æŒä»…é€‚ç”¨äº GPTã€‚å¾ˆå¿«å°†æ‰©å±•åˆ° BERT å’Œ T5ã€‚

1.  `gradient_accumulation_steps` éœ€è¦ä¸º 1ã€‚åœ¨ä½¿ç”¨ Megatron-LM æ—¶ï¼Œç®¡é“å¹¶è¡Œè®¾ç½®ä¸­çš„å¾®æ‰¹æ¬¡ç­‰åŒäºæ¢¯åº¦ç´¯ç§¯ã€‚

1.  åœ¨ä½¿ç”¨ Megatron-LM æ—¶ï¼Œè¯·ä½¿ç”¨ `accelerator.save_state` å’Œ `accelerator.load_state` æ¥ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹ã€‚

1.  ä»¥ä¸‹æ˜¯ä» Megatron-LM æ¨¡å‹æ¶æ„åˆ°ç­‰æ•ˆ ğŸ¤— transformers æ¨¡å‹æ¶æ„çš„æ˜ å°„ã€‚ä»…æ”¯æŒè¿™äº› ğŸ¤— transformers æ¨¡å‹æ¶æ„ã€‚

a. Megatron-LM [BertModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py)ï¼šğŸ¤— transformers æ¨¡å‹ä¸­é…ç½®ä¸º `megatron-bert` çš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ [MegatronBERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)

b. Megatron-LM [GPTModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py)ï¼šğŸ¤— transformers æ¨¡å‹ä¸­é…ç½®ä¸º `gpt2` çš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ [OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)

c. Megatron-LM [T5Model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py)ï¼šğŸ¤— transformers æ¨¡å‹ä¸­é…ç½®ä¸º `t5` çš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ [T5](https://huggingface.co/docs/transformers/model_doc/t5) å’Œ [MT5](https://huggingface.co/docs/transformers/model_doc/mt5)
