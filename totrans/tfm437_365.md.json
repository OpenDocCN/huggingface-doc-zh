["```py\n( text_config = None vision_config = None initializer_factor = 1.0 initializer_range = 0.02 is_vqa = False tie_word_embeddings = False is_encoder_decoder = True **kwargs )\n```", "```py\n>>> from transformers import Pix2StructConfig, Pix2StructForConditionalGeneration\n\n>>> # Initializing a Pix2StructConfig with google/pix2struct-base style configuration\n>>> configuration = Pix2StructConfig()\n\n>>> # Initializing a Pix2StructForConditionalGeneration (with random weights) from the google/pix2struct-base style configuration\n>>> model = Pix2StructForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a Pix2StructConfig from a Pix2StructTextConfig and a Pix2StructVisionConfig\n\n>>> # Initializing a Pix2Struct text and Pix2Struct vision configuration\n>>> config_text = Pix2StructTextConfig()\n>>> config_vision = Pix2StructVisionConfig()\n\n>>> config = Pix2StructConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: Pix2StructTextConfig vision_config: Pix2StructVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';Pix2StructConfig\n```", "```py\n( vocab_size = 50244 hidden_size = 768 d_kv = 64 d_ff = 2048 num_layers = 12 num_heads = 12 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 dense_act_fn = 'gelu_new' decoder_start_token_id = 0 use_cache = False pad_token_id = 0 eos_token_id = 1 tie_word_embeddings = False is_decoder = True **kwargs )\n```", "```py\n>>> from transformers import Pix2StructTextConfig, Pix2StructTextModel\n\n>>> # Initializing a Pix2StructTextConfig with google/pix2struct-base style configuration\n>>> configuration = Pix2StructTextConfig()\n\n>>> # Initializing a Pix2StructTextModel (with random weights) from the google/pix2struct-base style configuration\n>>> model = Pix2StructTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 patch_embed_hidden_size = 768 d_ff = 2048 d_kv = 64 num_hidden_layers = 12 num_attention_heads = 12 dense_act_fn = 'gelu_new' layer_norm_eps = 1e-06 dropout_rate = 0.0 attention_dropout = 0.0 initializer_range = 1e-10 initializer_factor = 1.0 seq_len = 4096 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 **kwargs )\n```", "```py\n>>> from transformers import Pix2StructVisionConfig, Pix2StructVisionModel\n\n>>> # Initializing a Pix2StructVisionConfig with google/pix2struct-base style configuration\n>>> configuration = Pix2StructVisionConfig()\n\n>>> # Initializing a Pix2StructVisionModel (with random weights) from the google/pix2struct-base style configuration\n>>> model = Pix2StructVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( do_convert_rgb: bool = True do_normalize: bool = True patch_size: Dict = None max_patches: int = 2048 is_vqa: bool = False **kwargs )\n```", "```py\n( images: Union header_text: Optional = None do_convert_rgb: bool = None do_normalize: Optional = None max_patches: Optional = None patch_size: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None inputs_embeds: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Pix2StructTextModel\n\n>>> processor = AutoProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n>>> model = Pix2StructTextModel.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n>>> inputs = processor(text=\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n```", "```py\n( config: Pix2StructConfig )\n```", "```py\n( flattened_patches: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> import requests\n>>> from PIL import Image\n>>> from transformers import AutoProcessor, Pix2StructVisionModel\n\n>>> image_processor = AutoProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n>>> model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n>>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 2048, 768]\n```", "```py\n( config: Pix2StructConfig )\n```", "```py\n( flattened_patches: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None labels: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n>>> model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n>>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> # autoregressive generation\n>>> generated_ids = model.generate(**inputs, max_new_tokens=50)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\nA stop sign is on a street corner.\n\n>>> # conditional generation\n>>> text = \"A picture of\"\n>>> inputs = processor(text=text, images=image, return_tensors=\"pt\", add_special_tokens=False)\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=50)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\nA picture of a stop sign with a red stop sign\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\n>>> model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-base\")\n\n>>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"A stop sign is on the street corner.\"\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> labels = processor(text=text, return_tensors=\"pt\").input_ids\n\n>>> # forward pass\n>>> outputs = model(**inputs, labels=labels)\n>>> loss = outputs.loss\n>>> print(f\"{loss.item():.5f}\")\n5.94282\n```"]