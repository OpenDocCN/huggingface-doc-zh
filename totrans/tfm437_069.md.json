["```py\ntraining_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n```", "```py\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n)\n```", "```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```", "```py\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```", "```py\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n```", "```py\nTrainingArguments(tf32=True, **default_args)\n```", "```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", **default_args)\n```", "```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adamw_bnb_8bit\", **default_args)\n```", "```py\nimport bitsandbytes as bnb\nfrom torch import nn\nfrom transformers.trainer_pt_utils import get_parameter_names\n\ntraining_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n\ndecay_parameters = get_parameter_names(model, [nn.LayerNorm])\ndecay_parameters = [name for name in decay_parameters if \"bias\" not in name]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n        \"weight_decay\": training_args.weight_decay,\n    },\n    {\n        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n        \"weight_decay\": 0.0,\n    },\n]\n\noptimizer_kwargs = {\n    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n    \"eps\": training_args.adam_epsilon,\n}\noptimizer_kwargs[\"lr\"] = training_args.learning_rate\nadam_bnb_optim = bnb.optim.Adam8bit(\n    optimizer_grouped_parameters,\n    betas=(training_args.adam_beta1, training_args.adam_beta2),\n    eps=training_args.adam_epsilon,\n    lr=training_args.learning_rate,\n)\n```", "```py\ntrainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n```", "```py\ntraining_args = TrainingArguments(torch_compile=True, **default_args)\n```", "```py\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    fp16=True,\n    **default_args,\n)\n```", "```py\nfrom accelerate import Accelerator\nfrom torch.utils.data.dataloader import DataLoader\n\ndataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n\nif training_args.gradient_checkpointing:\n    model.gradient_checkpointing_enable()\n\naccelerator = Accelerator(fp16=training_args.fp16)\nmodel, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n\nmodel.train()\nfor step, batch in enumerate(dataloader, start=1):\n    loss = model(**batch).loss\n    loss = loss / training_args.gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % training_args.gradient_accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n```", "```py\nmodel = model.to_bettertransformer()\n```"]