- en: The intuition behind PPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit8/intuition-behind-ppo](https://huggingface.co/learn/deep-rl-course/unit8/intuition-behind-ppo)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/87.bcd9e846.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea with Proximal Policy Optimization (PPO) is that we want to improve
    the training stability of the policy by limiting the change you make to the policy
    at each training epoch: **we want to avoid having too large of a policy update.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: We know empirically that smaller policy updates during training are **more likely
    to converge to an optimal solution.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A too-big step in a policy update can result in falling “off the cliff” (getting
    a bad policy) **and taking a long time or even having no possibility to recover.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Policy Update cliff](../Images/8fa9c88b0d8ad6450f392bf8060c51c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Taking smaller policy updates to improve the training stability
  prefs: []
  type: TYPE_NORMAL
- en: Modified version from RL — Proximal Policy Optimization (PPO) [Explained by
    Jonathan Hui](https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)
  prefs: []
  type: TYPE_NORMAL
- en: '**So with PPO, we update the policy conservatively**. To do so, we need to
    measure how much the current policy changed compared to the former one using a
    ratio calculation between the current and former policy. And we clip this ratio
    in a range<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">]</mo></mrow>
    <annotation encoding="application/x-tex">[1 - \epsilon, 1 + \epsilon]</annotation></semantics></math>
    [1−ϵ,1+ϵ], meaning that we **remove the incentive for the current policy to go
    too far from the old one (hence the proximal policy term).**'
  prefs: []
  type: TYPE_NORMAL
