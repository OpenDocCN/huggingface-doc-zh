- en: The intuition behind PPO
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO的直觉
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/intuition-behind-ppo](https://huggingface.co/learn/deep-rl-course/unit8/intuition-behind-ppo)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unit8/intuition-behind-ppo](https://huggingface.co/learn/deep-rl-course/unit8/intuition-behind-ppo)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea with Proximal Policy Optimization (PPO) is that we want to improve
    the training stability of the policy by limiting the change you make to the policy
    at each training epoch: **we want to avoid having too large of a policy update.**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Proximal Policy Optimization（PPO）的想法是，我们希望通过限制每个训练时期对策略所做的更改来改善策略的训练稳定性：**我们希望避免进行过大的策略更新。**
- en: 'For two reasons:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个原因：
- en: We know empirically that smaller policy updates during training are **more likely
    to converge to an optimal solution.**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们凭经验知道，在训练过程中较小的策略更新**更有可能收敛到最优解。**
- en: A too-big step in a policy update can result in falling “off the cliff” (getting
    a bad policy) **and taking a long time or even having no possibility to recover.**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在策略更新中迈出太大的一步可能导致“掉下悬崖”（得到一个糟糕的策略）**并且需要很长时间甚至没有恢复的可能性。**
- en: '![Policy Update cliff](../Images/8fa9c88b0d8ad6450f392bf8060c51c3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![策略更新悬崖](../Images/8fa9c88b0d8ad6450f392bf8060c51c3.png)'
- en: Taking smaller policy updates to improve the training stability
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 采取较小的策略更新以改善训练稳定性
- en: Modified version from RL — Proximal Policy Optimization (PPO) [Explained by
    Jonathan Hui](https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自RL - Proximal Policy Optimization（PPO）[由Jonathan Hui解释](https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)
- en: '**So with PPO, we update the policy conservatively**. To do so, we need to
    measure how much the current policy changed compared to the former one using a
    ratio calculation between the current and former policy. And we clip this ratio
    in a range<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">]</mo></mrow>
    <annotation encoding="application/x-tex">[1 - \epsilon, 1 + \epsilon]</annotation></semantics></math>
    [1−ϵ,1+ϵ], meaning that we **remove the incentive for the current policy to go
    too far from the old one (hence the proximal policy term).**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**因此，使用PPO，我们保守地更新策略**。为此，我们需要测量当前策略相对于以前策略的变化量，使用当前和以前策略之间的比率计算。我们将这个比率剪切在一个范围<math><semantics><mrow><mo
    stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 - \epsilon,
    1 + \epsilon]</annotation></semantics></math> [1−ϵ,1+ϵ]，这意味着我们**消除了当前策略远离旧策略的动机（因此有了近端策略术语）。**'
