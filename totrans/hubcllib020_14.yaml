- en: Inference Endpoints
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理端点
- en: 'Original text: [https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints](https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints](https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Inference Endpoints provides a secure production solution to easily deploy any
    `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated
    and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint
    is built from a model from the [Hub](https://huggingface.co/models). In this guide,
    we will learn how to programmatically manage Inference Endpoints with `huggingface_hub`.
    For more information about the Inference Endpoints product itself, check out its
    [official documentation](https://huggingface.co/docs/inference-endpoints/index).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 推理端点提供了一个安全的生产解决方案，可以轻松部署任何`transformers`、`sentence-transformers`和`diffusers`模型，这些模型在由Hugging
    Face管理的专用和自动缩放的基础设施上构建。推理端点是从[Hub](https://huggingface.co/models)中的模型构建的。在本指南中，我们将学习如何使用`huggingface_hub`以编程方式管理推理端点。有关推理端点产品本身的更多信息，请查看其[官方文档](https://huggingface.co/docs/inference-endpoints/index)。
- en: This guide assumes `huggingface_hub` is correctly installed and that your machine
    is logged in. Check out the [Quick Start guide](https://huggingface.co/docs/huggingface_hub/quick-start#quickstart)
    if that’s not the case yet. The minimal version supporting Inference Endpoints
    API is `v0.19.0`.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南假定`huggingface_hub`已正确安装并且您的计算机已登录。如果还没有，请查看[快速入门指南](https://huggingface.co/docs/huggingface_hub/quick-start#quickstart)。支持推理端点API的最低版本是`v0.19.0`。
- en: Create an Inference Endpoint
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建推理端点
- en: 'The first step is to create an Inference Endpoint using [create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用[create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint)创建一个推理端点：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we created a `protected` Inference Endpoint named `"my-endpoint-name"`,
    to serve [gpt2](https://huggingface.co/gpt2) for `text-generation`. A `protected`
    Inference Endpoint means your token is required to access the API. We also need
    to provide additional information to configure the hardware requirements, such
    as vendor, region, accelerator, instance type, and size. You can check out the
    list of available resources [here](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aprovider/list_vendors).
    Alternatively, you can create an Inference Endpoint manually using the [Web interface](https://ui.endpoints.huggingface.co/new)
    for convenience. Refer to this [guide](https://huggingface.co/docs/inference-endpoints/guides/advanced)
    for details on advanced settings and their usage.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个名为`"my-endpoint-name"`的`protected`推理端点，用于为`text-generation`提供[gpt2](https://huggingface.co/gpt2)。`protected`推理端点意味着需要您的令牌才能访问API。我们还需要提供额外的信息来配置硬件要求，如供应商、地区、加速器、实例类型和大小。您可以在这里查看可用资源的列表[here](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aprovider/list_vendors)。或者，您可以使用[Web界面](https://ui.endpoints.huggingface.co/new)方便地手动创建推理端点。有关高级设置及其用法的详细信息，请参考此[指南](https://huggingface.co/docs/inference-endpoints/guides/advanced)。
- en: 'The value returned by [create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint)
    is an [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    object:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint)返回的值是一个[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)对象：'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It’s a dataclass that holds information about the endpoint. You can access important
    attributes such as `name`, `repository`, `status`, `task`, `created_at`, `updated_at`,
    etc. If you need it, you can also access the raw response from the server with
    `endpoint.raw`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数据类，保存有关端点的信息。您可以访问重要属性，如`name`、`repository`、`status`、`task`、`created_at`、`updated_at`等。如果需要，您还可以使用`endpoint.raw`访问服务器的原始响应。
- en: Once your Inference Endpoint is created, you can find it on your [personal dashboard](https://ui.endpoints.huggingface.co/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完推理端点后，您可以在您的[个人仪表板](https://ui.endpoints.huggingface.co/)上找到它。
- en: '![](../Images/96374d759ddf771bc762082295683dbf.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96374d759ddf771bc762082295683dbf.png)'
- en: Using a custom image
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用自定义镜像
- en: 'By default the Inference Endpoint is built from a docker image provided by
    Hugging Face. However, it is possible to specify any docker image using the `custom_image`
    parameter. A common use case is to run LLMs using the [text-generation-inference](https://github.com/huggingface/text-generation-inference)
    framework. This can be done like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，推理端点是由Hugging Face提供的docker镜像构建的。但是，可以使用`custom_image`参数指定任何docker镜像。一个常见的用例是使用[text-generation-inference](https://github.com/huggingface/text-generation-inference)框架运行LLMs。可以这样做：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The value to pass as `custom_image` is a dictionary containing a url to the
    docker container and configuration to run it. For more details about it, checkout
    the [Swagger documentation](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aendpoint/create_endpoint).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为`custom_image`传递的值是一个包含指向docker容器和配置以运行它的url的字典。有关更多详细信息，请查看[Swagger文档](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aendpoint/create_endpoint)。
- en: Get or list existing Inference Endpoints
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取或列出现有的推理端点
- en: In some cases, you might need to manage Inference Endpoints you created previously.
    If you know the name, you can fetch it using [get_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.get_inference_endpoint),
    which returns an [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    object. Alternatively, you can use [list_inference_endpoints()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_inference_endpoints)
    to retrieve a list of all Inference Endpoints. Both methods accept an optional
    `namespace` parameter. You can set the `namespace` to any organization you are
    a part of. Otherwise, it defaults to your username.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能需要管理之前创建的推理端点。如果您知道名称，可以使用[get_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.get_inference_endpoint)来获取它，该方法返回一个[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)对象。或者，您可以使用[list_inference_endpoints()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_inference_endpoints)来检索所有推理端点的列表。这两种方法都接受一个可选的`namespace`参数。您可以将`namespace`设置为您所属的任何组织。否则，默认为您的用户名。
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Check deployment status
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查部署状态
- en: 'In the rest of this guide, we will assume that we have a [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    object called `endpoint`. You might have noticed that the endpoint has a `status`
    attribute of type [InferenceEndpointStatus](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointStatus).
    When the Inference Endpoint is deployed and accessible, the status should be `"running"`
    and the `url` attribute is set:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南的其余部分中，我们将假设我们有一个名为`endpoint`的[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)对象。您可能已经注意到端点具有一个类型为[InferenceEndpointStatus](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointStatus)的`status`属性。当推理端点部署并可访问时，状态应为“运行”，并设置`url`属性：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before reaching a `"running"` state, the Inference Endpoint typically goes
    through an `"initializing"` or `"pending"` phase. You can fetch the new state
    of the endpoint by running [fetch()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.fetch).
    Like every other method from [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    that makes a request to the server, the internal attributes of `endpoint` are
    mutated in place:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在达到“运行”状态之前，推理端点通常会经历“初始化”或“挂起”阶段。您可以通过运行[fetch()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.fetch)来获取端点的新状态。与[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)中的其他向服务器发出请求的方法一样，`endpoint`的内部属性会就地发生变化：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Instead of fetching the Inference Endpoint status while waiting for it to run,
    you can directly call [wait()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.wait).
    This helper takes as input a `timeout` and a `fetch_every` parameter (in seconds)
    and will block the thread until the Inference Endpoint is deployed. Default values
    are respectively `None` (no timeout) and `5` seconds.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在等待其运行时，您可以直接调用[wait()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.wait)而不是获取推理端点状态。此辅助程序接受`timeout`和`fetch_every`参数（以秒为单位），并将阻塞线程直到推理端点部署。默认值分别为`None`（无超时）和`5`秒。
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If `timeout` is set and the Inference Endpoint takes too much time to load,
    a `InferenceEndpointTimeoutError` timeout error is raised.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置了`timeout`并且推理端点加载时间过长，则会引发`InferenceEndpointTimeoutError`超时错误。
- en: Run inference
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行推理
- en: Once your Inference Endpoint is up and running, you can finally run inference
    on it!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的推理端点正常运行，您最终可以在其上运行推理！
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    has two properties `client` and `async_client` returning respectively an [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    and an [AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
    objects.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)有两个属性`client`和`async_client`，分别返回一个[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)和一个[AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)对象。'
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If the Inference Endpoint is not running, an [InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    exception is raised:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果推理端点未运行，则会引发[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)异常：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For more details about how to use the [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient),
    check out the [Inference guide](../guides/inference).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)的更多详细信息，请查看[推理指南](../guides/inference)。
- en: Manage lifecycle
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理生命周期
- en: Now that we saw how to create an Inference Endpoint and run inference on it,
    let’s see how to manage its lifecycle.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到如何创建推理端点并在其上运行推理，让我们看看如何管理其生命周期。
- en: 'In this section, we will see methods like [pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause),
    [resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume),
    [scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero),
    [update()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.update)
    and [delete()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.delete).
    All of those methods are aliases added to [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    for convenience. If you prefer, you can also use the generic methods defined in
    `HfApi`: [pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint),
    [resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint),
    [scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint),
    [update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint),
    and [delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到像[pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)、[resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume)、[scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero)、[update()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.update)和[delete()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.delete)等方法。所有这些方法都是为了方便而添加到[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)中的别名。如果您愿意，您也可以使用`HfApi`中定义的通用方法：[pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint)、[resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint)、[scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint)、[update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint)和[delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint)。
- en: Pause or scale to zero
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 暂停或缩减为零
- en: To reduce costs when your Inference Endpoint is not in use, you can choose to
    either pause it using [pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)
    or scale it to zero using [scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的推理端点不在使用时，您可以选择使用[pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)暂停它，或者使用[scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero)将其缩减为零以减少成本。
- en: An Inference Endpoint that is *paused* or *scaled to zero* doesn’t cost anything.
    The difference between those two is that a *paused* endpoint needs to be explicitly
    *resumed* using [resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume).
    On the contrary, a *scaled to zero* endpoint will automatically start if an inference
    call is made to it, with an additional cold start delay. An Inference Endpoint
    can also be configured to scale to zero automatically after a certain period of
    inactivity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*暂停*或*缩减为零*的推理端点不会产生任何费用。这两者之间的区别在于*暂停*的端点需要使用[resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume)显式地*恢复*。相反，*缩减为零*的端点将在进行推理调用时自动启动，并附加冷启动延迟。推理端点还可以配置为在一段时间内自动缩减为零。'
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Update model or hardware requirements
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新模型或硬件要求
- en: 'In some cases, you might also want to update your Inference Endpoint without
    creating a new one. You can either update the hosted model or the hardware requirements
    to run the model. You can do this using [update()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.update):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能还想更新您的推理端点而不创建新的。您可以更新托管模型或运行模型所需的硬件要求。您可以使用[update()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.update)来实现：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Delete the endpoint
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除端点
- en: Finally if you won’t use the Inference Endpoint anymore, you can simply call
    `~InferenceEndpoint.delete()`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您不再使用推理端点，您可以简单地调用`~InferenceEndpoint.delete()`。
- en: This is a non-revertible action that will completely remove the endpoint, including
    its configuration, logs and usage metrics. You cannot restore a deleted Inference
    Endpoint.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不可逆转的操作，将完全删除端点，包括其配置、日志和使用指标。您无法恢复已删除的推理端点。
- en: An end-to-end example
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端示例
- en: 'A typical use case of Inference Endpoints is to process a batch of jobs at
    once to limit the infrastructure costs. You can automate this process using what
    we saw in this guide:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 推理端点的典型用例是一次处理一批作业以限制基础设施成本。您可以使用本指南中所见的方法自动化此过程：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Or if your Inference Endpoint already exists and is paused:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您的推理端点已经存在并且已暂停：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
