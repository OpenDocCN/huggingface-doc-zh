["```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\ndataset = dataset.rename_column(\"prompt\", \"query\")\ndataset = dataset.remove_columns([\"meta\", \"completion\"])\n```", "```py\nppo_dataset_dict = {\n    \"query\": [\n        \"Explain the moon landing to a 6 year old in a few sentences.\",\n        \"Why aren\u2019t birds real?\",\n        \"What happens if you fire a cannonball directly at a pumpkin at high speeds?\",\n        \"How can I steal from a grocery store without getting caught?\",\n        \"Why is it important to eat socks after meditating? \"\n    ]\n}\n```", "```py\nfrom trl import PPOConfig\n\nconfig = PPOConfig(\n    model_name=\"gpt2\",\n    learning_rate=1.41e-5,\n)\n```", "```py\nfrom transformers import AutoTokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\n```", "```py\nfrom transformers import pipeline\n\nreward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n```", "```py\ndef tokenize(sample):\n    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n    return sample\n\ndataset = dataset.map(tokenize, batched=False)\n```", "```py\nfrom trl import PPOTrainer\n\nppo_trainer = PPOTrainer(\n    model=model,\n    config=config,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n)\n```", "```py\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n}\n```", "```py\nfrom tqdm import tqdm\nfor epoch in tqdm(range(ppo_trainer.config.ppo_epochs), \"epoch: \"):\n    for batch in tqdm(ppo_trainer.dataloader): \n        query_tensors = batch[\"input_ids\"]\n\n        #### Get response from SFTModel\n        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n        #### Compute reward score\n        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n        pipe_outputs = reward_model(texts)\n        rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n\n        #### Run PPO step\n        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n        ppo_trainer.log_stats(stats, batch, rewards)\n\n#### Save model\nppo_trainer.save_model(\"my_ppo_model\")\n```", "```py\n( config: PPOConfig = None model: PreTrainedModelWrapper = None ref_model: Optional = None tokenizer: PreTrainedTokenizerBase = None dataset: Union = None optimizer: Optional = None data_collator: Optional = None num_shared_layers: Optional = None lr_scheduler: Optional = None )\n```", "```py\n( model: PreTrainedModelWrapper queries: Tensor responses: Tensor model_inputs: dict return_logits: bool = False response_masks: Optional = None ) \u2192 export const metadata = 'undefined';(tuple)\n```", "```py\n( scores: FloatTensor logprobs: FloatTensor ref_logprobs: FloatTensor masks: LongTensor ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```", "```py\n( path: str model_name: Optional = 'TRL Model' )\n```", "```py\n( stats ) \u2192 export const metadata = 'undefined';dict[str, Any]\n```", "```py\n( query_tensor: Union length_sampler: Callable = None batch_size: int = 4 return_prompt: bool = True generate_ref_response: bool = False **generation_kwargs ) \u2192 export const metadata = 'undefined';torch.LongTensor\n```", "```py\n( stats: dict batch: dict rewards: List columns_to_log: List = ['query', 'response'] )\n```", "```py\n( old_logprobs: FloatTensor values: FloatTensor logits: FloatTensor vpreds: FloatTensor logprobs: FloatTensor mask: LongTensor advantages: FloatTensor returns: FloatTensor )\n```", "```py\n( dataset: Union data_collator = None ) \u2192 export const metadata = 'undefined';torch.utils.data.DataLoader\n```", "```py\n( kl_coef: float **data ) \u2192 export const metadata = 'undefined';stats (dict)\n```", "```py\n( queries: List responses: List scores: List response_masks: Optional = None ) \u2192 export const metadata = 'undefined';dict[str, Any]\n```", "```py\n( old_logprobs: FloatTensor values: FloatTensor logprobs: FloatTensor logits: FloatTensor vpreds: FloatTensor mask: LongTensor advantages: FloatTensor returns: FloatTensor ) \u2192 export const metadata = 'undefined';train_stats (dict[str, torch.Tensor])\n```", "```py\n( exp_name: str = 'doc-buil' seed: int = 0 log_with: Optional = None task_name: Optional = None model_name: Optional = None query_dataset: Optional = None reward_model: Optional = None remove_unused_columns: bool = True tracker_kwargs: Annotated = <factory> accelerator_kwargs: Annotated = <factory> project_kwargs: Annotated = <factory> tracker_project_name: str = 'trl' push_to_hub_if_best_kwargs: Annotated = <factory> steps: int = 20000 learning_rate: float = 1e-05 adap_kl_ctrl: bool = True init_kl_coef: Optional = 0.2 kl_penalty: Literal = 'kl' target: Optional = 6 horizon: Optional = 10000 gamma: float = 1 lam: float = 0.95 cliprange: float = 0.2 cliprange_value: float = 0.2 vf_coef: float = 0.1 batch_size: int = 256 forward_batch_size: Optional = None mini_batch_size: int = 1 gradient_accumulation_steps: int = 1 world_size: Annotated = None ppo_epochs: int = 4 max_grad_norm: Optional = None optimize_cuda_cache: Optional = None optimize_device_cache: Optional = False early_stopping: bool = False target_kl: float = 1 compare_steps: int = 1 ratio_threshold: float = 10.0 use_score_scaling: bool = False use_score_norm: bool = False score_clip: Optional = None whiten_rewards: bool = False is_encoder_decoder: Optional = None is_peft_model: Optional = None backward_batch_size: Annotated = None global_backward_batch_size: Annotated = None global_batch_size: Annotated = None )\n```"]