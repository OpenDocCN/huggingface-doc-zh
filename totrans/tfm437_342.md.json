["```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True classifier_dropout = None **kwargs )\n```", "```py\n>>> from transformers import Data2VecTextConfig, Data2VecTextModel\n\n>>> # Initializing a Data2VecText facebook/data2vec-text-base style configuration\n>>> configuration = Data2VecTextConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/data2vec-text-base style configuration\n>>> model = Data2VecTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_size = 32 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embedding_groups = 16 conv_pos_kernel_size = 19 num_conv_pos_embeddings = 5 mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 ctc_loss_reduction = 'sum' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None **kwargs )\n```", "```py\n>>> from transformers import Data2VecAudioConfig, Data2VecAudioModel\n\n>>> # Initializing a Data2VecAudio facebook/data2vec-audio-base-960h style configuration\n>>> configuration = Data2VecAudioConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/data2vec-audio-base-960h style configuration\n>>> model = Data2VecAudioModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 use_mask_token = False use_absolute_position_embeddings = False use_relative_position_bias = False use_shared_relative_position_bias = False layer_scale_init_value = 0.1 drop_path_rate = 0.1 use_mean_pooling = True out_indices = [3, 5, 7, 11] pool_scales = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input = False semantic_loss_ignore_index = 255 **kwargs )\n```", "```py\n>>> from transformers import Data2VecVisionConfig, Data2VecVisionModel\n\n>>> # Initializing a Data2VecVision data2vec_vision-base-patch16-224-in22k style configuration\n>>> configuration = Data2VecVisionConfig()\n\n>>> # Initializing a model (with random weights) from the data2vec_vision-base-patch16-224-in22k style configuration\n>>> model = Data2VecVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: Data2VecAudioConfig )\n```", "```py\n( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Data2VecAudioModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n>>> model = Data2VecAudioModel.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 292, 768]\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Data2VecAudioForAudioFrameClassification\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n>>> model = Data2VecAudioForAudioFrameClassification.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sampling_rate)\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> probabilities = torch.sigmoid(logits[0])\n>>> # labels is a one-hot array of shape (num_frames, num_speakers)\n>>> labels = (probabilities > 0.5).long()\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Data2VecAudioForCTC\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n>>> model = Data2VecAudioForCTC.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n\n>>> inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # compute loss\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n66.95\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```"]