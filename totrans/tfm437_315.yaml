- en: SEW-D
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SEW-D
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sew-d](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sew-d)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sew-d](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sew-d)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: SEW-D (Squeezed and Efficient Wav2Vec with Disentangled attention) was proposed
    in [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech
    Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing
    Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: SEW-D（具有解耦注意力的压缩和高效Wav2Vec）由Felix Wu、Kwangyoun Kim、Jing Pan、Kyu Han、Kilian Q.
    Weinberger、Yoav Artzi在[无监督预训练语音识别中的性能效率权衡](https://arxiv.org/abs/2109.06870)中提出。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*This paper is a study of performance-efficiency trade-offs in pre-trained
    models for automatic speech recognition (ASR). We focus on wav2vec 2.0, and formalize
    several architecture designs that influence both the model performance and its
    efficiency. Putting together all our observations, we introduce SEW (Squeezed
    and Efficient Wav2vec), a pre-trained model architecture with significant improvements
    along both performance and efficiency dimensions across a variety of training
    setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech,
    SEW achieves a 1.9x inference speedup compared to wav2vec 2.0, with a 13.5% relative
    reduction in word error rate. With a similar inference time, SEW reduces word
    error rate by 25-50% across different model sizes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文研究了预训练模型在自动语音识别（ASR）中的性能效率权衡。我们关注wav2vec 2.0，并形式化了几种影响模型性能和效率的架构设计。综合我们所有的观察，我们引入了SEW（压缩和高效Wav2vec），这是一个预训练模型架构，在各种训练设置中在性能和效率两个维度上都有显著改进。例如，在LibriSpeech的100h-960h半监督设置下，SEW相对于wav2vec
    2.0实现了1.9倍的推理加速，词错误率相对减少了13.5%。在类似的推理时间下，SEW在不同模型大小下将词错误率降低了25-50%。*'
- en: This model was contributed by [anton-l](https://huggingface.co/anton-l).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[anton-l](https://huggingface.co/anton-l)贡献。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: SEW-D is a speech model that accepts a float array corresponding to the raw
    waveform of the speech signal.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SEW-D是一个接受与语音信号的原始波形对应的浮点数组的语音模型。
- en: SEWDForCTC is fine-tuned using connectionist temporal classification (CTC) so
    the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SEWDForCTC经过连接主义时间分类（CTC）微调，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。
- en: Resources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[音频分类任务指南](../tasks/audio_classification)'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动语音识别任务指南](../tasks/asr)'
- en: SEWDConfig
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SEWDConfig
- en: '### `class transformers.SEWDConfig`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SEWDConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/configuration_sew_d.py#L32)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/configuration_sew_d.py#L32)'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32) — Vocabulary size of the SEW-D
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling `SEWD`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 32) — SEW-D模型的词汇量。定义了在调用`SEWD`时可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`squeeze_factor` (`int`, *optional*, defaults to 2) — Sequence length downsampling
    factor after the encoder and upsampling factor after the transformer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`squeeze_factor` (`int`, *optional*, defaults to 2) — 编码器后的序列长度下采样因子和变压器后的上采样因子。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`position_buckets` (`int`, *optional*, defaults to 256) — The maximum size
    of relative position embeddings.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_buckets` (`int`, *optional*, defaults to 256) — 相对位置嵌入的最大大小。'
- en: '`share_att_key` (`bool`, *optional*, defaults to `True`) — Whether to share
    attention key with c2p and p2c.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`share_att_key` (`bool`, *optional*, defaults to `True`) — 是否与c2p和p2c共享注意力键。'
- en: '`relative_attention` (`bool`, *optional*, defaults to `True`) — Whether to
    use relative position encoding.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention` (`bool`, *optional*, defaults to `True`) — 是否使用相对位置编码。'
- en: '`pos_att_type` (`Tuple[str]`, *optional*, defaults to `("p2c", "c2p")`) — The
    type of relative position attention, it can be a combination of `("p2c", "c2p")`,
    e.g. `("p2c")`, `("p2c", "c2p")`, `("p2c", "c2p")`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos_att_type` (`Tuple[str]`, *optional*, defaults to `("p2c", "c2p")`) — 相对位置注意力的类型，可以是`("p2c",
    "c2p")`的组合，例如`("p2c")`，`("p2c", "c2p")`，`("p2c", "c2p")`。'
- en: '`norm_rel_ebd` (`str`, *optional*, defaults to `"layer_norm"`) — Whether to
    use layer norm in relative embedding (`"layer_norm"` if yes)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_rel_ebd` (`str`, *optional*, defaults to `"layer_norm"`) — 是否在相对嵌入中使用层归一化（如果是，则为`"layer_norm"`）'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu_python"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"`, `"gelu_python"` and `"gelu_new"` are
    supported.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu_python"`)
    — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`、`"gelu_python"`和`"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — Deprecated. Not used
    by the model and will be removed in a future version.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — 已弃用。模型不使用，将在未来版本中移除。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the final projection layer of [SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) — [SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC)最终投影层的dropout概率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-7) — The epsilon used
    by the layer normalization layers in the transformer encoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-7) — 变压器编码器中层归一化层使用的epsilon。'
- en: '`feature_layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — The epsilon
    used by the layer normalization after the feature encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 特征编码器后的层归一化使用的epsilon。'
- en: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — The norm to
    be applied to 1D convolutional layers in feature encoder. One of `"group"` for
    group normalization of only the first 1D convolutional layer or `"layer"` for
    layer normalization of all 1D convolutional layers.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 应用于特征编码器中1D卷积层的规范化。`"group"`表示仅对第一个1D卷积层进行组归一化，`"layer"`表示对所有1D卷积层进行层归一化。'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for output of the feature encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的dropout概率。'
- en: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` “gelu”`,` “relu”`,` “selu”`and`“gelu_new”` are
    supported.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`“gelu”`、`“relu”`、`“selu”`和`“gelu_new”`。'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(64, 128,
    128, 128, 128, 256, 256, 256, 256, 512, 512, 512, 512)`) — A tuple of integers
    defining the number of input and output channels of each 1D convolutional layer
    in the feature encoder. The length of *conv_dim* defines the number of 1D convolutional
    layers.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(64, 128,
    128, 128, 128, 256, 256, 256, 256, 512, 512, 512, 512)`) — 定义特征编码器中每个1D卷积层的输入和输出通道数的整数元组。*conv_dim*的长度定义了1D卷积层的数量。'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1)`) — A tuple of integers defining the stride of
    each 1D convolutional layer in the feature encoder. The length of *conv_stride*
    defines the number of convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1)`) — 定义特征编码器中每个1D卷积层的步幅的整数元组。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    1, 3, 1, 3, 1, 3, 1, 2, 1, 2, 1)`) — A tuple of integers defining the kernel size
    of each 1D convolutional layer in the feature encoder. The length of *conv_kernel*
    defines the number of convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    1, 3, 1, 3, 1, 3, 1, 2, 1, 2, 1)`) — 定义特征编码器中每个1D卷积层的卷积核大小的整数元组。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) — Whether the 1D convolutional
    layers have a bias.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否具有偏置。'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的卷积核大小。'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。'
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — 是否对特征编码器的输出应用*SpecAugment*数据增强。参考[SpecAugment:
    A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)。'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates ”mask_time_prob*len(time_axis)/mask_time_length” independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *可选*, 默认为0.05) — 沿时间轴的所有特征向量的百分比（介于0和1之间），将被掩盖。掩盖过程在该轴上生成”mask_time_prob*len(time_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为要掩盖的向量跨度的起始的概率推理，*
    mask_time_prob *应为`prob_vector_start*mask_time_length`。请注意，重叠可能会降低掩盖向量的实际百分比。只有在`apply_spec_augment`为True时才相关。'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *可选*, 默认为10) — 沿时间轴的向量跨度长度。'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *可选*, 默认为2) — 沿时间轴生成的长度为`mask_feature_length`的掩码的最小数量，每个时间步，与`mask_feature_prob`无关。只有在”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”时才相关。'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates ”mask_feature_prob*len(feature_axis)/mask_time_length”
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *可选*, 默认为0.0) — 沿特征轴的所有特征向量的百分比（介于0和1之间），将被掩盖。掩盖过程在该轴上生成”mask_feature_prob*len(feature_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为要掩盖的向量跨度的起始的概率推理，*
    mask_feature_prob *应为`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低掩盖向量的实际百分比。只有在`apply_spec_augment`为True时才相关。'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *可选*, 默认为10) — 沿特征轴的向量跨度长度。'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), — The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *可选*, 默认为0) — 沿特征轴生成的长度为`mask_feature_length`的掩码的最小数量，每个时间步，与`mask_feature_prob`无关。只有在”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”时才相关。'
- en: '`diversity_loss_weight` (`int`, *optional*, defaults to 0.1) — The weight of
    the codebook diversity loss component.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss_weight` (`int`, *可选*, 默认为0.1) — 代码本多样性损失组件的权重。'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) — Specifies the
    reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training
    an instance of [SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *可选*, 默认为`"sum"`) — 指定应用于`torch.nn.CTCLoss`输出的减少方式。只有在训练[SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC)实例时才相关。'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *可选*, 默认为`False`) — 是否将`torch.nn.CTCLoss`的无限损失和相关梯度置零。当输入太短无法与目标对齐时，主要会发生无限损失。只有在训练[SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC)实例时才相关。'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *可选*, 默认为`False`) — 是否使用具有学习权重的层输出的加权平均。只有在使用[Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)实例时才相关。'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) — Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *可选*, 默认为256) — 分类前的投影维度，用于标记均值池化。'
- en: This is the configuration class to store the configuration of a [SEWDModel](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDModel).
    It is used to instantiate a SEW-D model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the SEW-D [asapp/sew-d-tiny-100k](https://huggingface.co/asapp/sew-d-tiny-100k)
    architecture.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[SEWDModel](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDModel)配置的配置类。它用于根据指定的参数实例化一个SEW-D模型，定义模型架构。使用默认值实例化配置将产生类似于SEW-D
    [asapp/sew-d-tiny-100k](https://huggingface.co/asapp/sew-d-tiny-100k)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `to_dict`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/configuration_sew_d.py#L292)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/configuration_sew_d.py#L292)'
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Serializes this instance to a Python dictionary.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将此实例序列化为Python字典。
- en: SEWDModel
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SEWDModel
- en: '### `class transformers.SEWDModel`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SEWDModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1355)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1355)'
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([SEWDConfig](/docs/transformers/v4.37.2/zh/model_doc/sew-d#transformers.SEWDConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare SEW-D Model transformer outputting raw hidden-states without any specific
    head on top. SEW-D was proposed in [Performance-Efficiency Trade-offs in Unsupervised
    Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix
    Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 裸SEW-D模型变压器输出原始隐藏状态，没有特定的顶部头。SEW-D是由Felix Wu、Kwangyoun Kim、Jing Pan、Kyu Han、Kilian
    Q. Weinberger、Yoav Artzi在[Performance-Efficiency Trade-offs in Unsupervised Pre-training
    for Speech Recognition](https://arxiv.org/abs/2109.06870)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1427)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1427)'
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，*例如*通过soundfile库（`pip
    install soundfile`）。要将数组准备为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/zh/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参见[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为`1`，
- en: 0 for tokens that are `masked`.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的标记为`0`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig))
    and inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置([SEWDConfig](/docs/transformers/v4.37.2/zh/model_doc/sew-d#transformers.SEWDConfig))和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [SEWDModel](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDModel)
    forward method, overrides the `__call__` special method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[SEWDModel](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: SEWDForCTC
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SEWDForCTC
- en: '### `class transformers.SEWDForCTC`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SEWDForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1484)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1484)'
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig)）-
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: SEW-D Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC). SEW-D was proposed in [Performance-Efficiency Trade-offs
    in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870)
    by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 带有顶部`语言建模`头部的SEW-D模型，用于连接主义时间分类（CTC）。SEW-D是由Felix Wu、Kwangyoun Kim、Jing Pan、Kyu
    Han、Kilian Q. Weinberger、Yoav Artzi在[无监督预训练在语音识别中的性能效率权衡](https://arxiv.org/abs/2109.06870)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `前进`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1561)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1561)'
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip
    install soundfile`）。要将数组准备成`input_values`，应该使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    用于避免在填充令牌索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`掩码`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`掩码`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, target_length)`，*optional*) —
    连接主义时间分类的标签。注意`target_length`必须小于或等于输出logits的序列长度。索引在`[-100, 0, ..., config.vocab_size
    - 1]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0, ..., config.vocab_size - 1]`中的标签。'
- en: Returns
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig))
    and inputs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回） — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[SEWDForCTC](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForCTC)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: SEWDForSequenceClassification
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SEWDForSequenceClassification
- en: '### `class transformers.SEWDForSequenceClassification`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SEWDForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1641)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1641)'
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: SEWD Model with a sequence classification head on top (a linear layer over the
    pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部具有序列分类头（池化输出上的线性层）的SEWD模型，用于类似SUPERB关键词识别的任务。
- en: SEW-D was proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training
    for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun
    Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SEW-D是由Felix Wu、Kwangyoun Kim、Jing Pan、Kyu Han、Kilian Q. Weinberger、Yoav Artzi在[Performance-Efficiency
    Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1694)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sew_d/modeling_sew_d.py#L1694)'
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库(`pip
    install soundfile`)。要将数组准备成`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于避免在填充标记索引上执行卷积和注意力的遮罩。遮罩值选择在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被遮蔽的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被遮蔽的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力遮罩？](../glossary#attention-mask)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig))
    and inputs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，具体取决于配置（[SEWDConfig](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 分类（或如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（或如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [SEWDForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[SEWDForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/sew-d#transformers.SEWDForSequenceClassification)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
