# 快速入门

> 原文链接：[https://huggingface.co/docs/diffusers/quicktour](https://huggingface.co/docs/diffusers/quicktour)

扩散模型被训练为逐步去噪随机高斯噪声，以生成感兴趣的样本，如图像或音频。这引发了对生成式人工智能的极大兴趣，您可能在互联网上看到过扩散生成的图像示例。🧨 Diffusers 是一个旨在使扩散模型普遍可访问的库。

无论您是开发人员还是日常用户，这个快速入门将向您介绍 🧨 Diffusers 并帮助您快速上手生成！了解该库的三个主要组件：

+   [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 是一个高级端到端类，旨在快速从预训练的扩散模型中生成样本以进行推断。

+   流行的预训练 [模型](./api/models) 架构和模块可用作创建扩散系统的构建块。

+   许多不同的 [调度程序](./api/schedulers/overview) - 控制训练时如何添加噪声以及推断时如何生成去噪图像的算法。

快速入门将向您展示如何使用 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 进行推断，然后指导您如何组合模型和调度程序以复制 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 内部发生的情况。

快速入门是简化的 🧨 Diffusers 入门笔记本，帮助您快速入门。如果您想了解更多关于 🧨 Diffusers 的目标、设计理念以及有关其核心 API 的其他详细信息，请查看笔记本！

在开始之前，请确保已安装所有必要的库：

```py
# uncomment to install the necessary libraries in Colab
#!pip install --upgrade diffusers accelerate transformers
```

+   [🤗 Accelerate](https://huggingface.co/docs/accelerate/index) 可加快模型加载的速度，用于推断和训练。

+   要运行最受欢迎的扩散模型，如 [稳定扩散](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)，需要 [🤗 Transformers](https://huggingface.co/docs/transformers/index)。

## DiffusionPipeline

[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 是使用预训练扩散系统进行推断的最简单方式。它是一个包含模型和调度程序的端到端系统。您可以直接使用 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 来执行许多任务。查看下面的表格以了解一些支持的任务，以及完整支持任务列表，请查看 [🧨 Diffusers 概要](./api/pipelines/overview#diffusers-summary) 表。

| **任务** | **描述** | **管道** |
| --- | --- | --- |
| 无条件图像生成 | 从高斯噪声生成图像 | [无条件图像生成](./using-diffusers/unconditional_image_generation) |
| 文本引导的图像生成 | 根据文本提示生成图像 | [有条件图像生成](./using-diffusers/conditional_image_generation) |
| 文本引导的图像到图像转换 | 根据文本提示调整图像 | [img2img](./using-diffusers/img2img) |
| 文本引导的图像修复 | 根据图像、蒙版和文本提示填充图像的遮罩部分 | [inpaint](./using-diffusers/inpaint) |
| 文本引导的深度到图像转换 | 在保留结构的同时，通过深度估计调整图像的部分，由文本提示引导 | [depth2img](./using-diffusers/depth2img) |

首先创建一个[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)的实例，并指定您想要下载的pipeline检查点。您可以在Hugging Face Hub上存储的任何[检查点](https://huggingface.co/models?library=diffusers&sort=downloads)上使用[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。在这个快速浏览中，您将加载[`stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)检查点进行文本到图像生成。

对于[Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion)模型，请在运行模型之前仔细阅读[许可证](https://huggingface.co/spaces/CompVis/stable-diffusion-license)。🧨 Diffusers实现了一个[`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py)来防止冒犯性或有害内容，但模型的改进图像生成能力仍可能产生潜在有害内容。

使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)方法加载模型：

```py
>>> from diffusers import DiffusionPipeline

>>> pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", use_safetensors=True)
```

[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)下载并缓存所有建模、标记化和调度组件。您将看到稳定扩散pipeline由[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)和[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)等组件组成：

```py
>>> pipeline
StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.21.4",
  ...,
  "scheduler": [
    "diffusers",
    "PNDMScheduler"
  ],
  ...,
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
}
```

我们强烈建议在GPU上运行pipeline，因为该模型大约由14亿个参数组成。您可以将生成器对象移动到GPU上，就像在PyTorch中一样：

```py
>>> pipeline.to("cuda")
```

现在您可以将文本提示传递给`pipeline`以生成图像，然后访问去噪后的图像。默认情况下，图像输出被包装在[`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=image#the-image-class)对象中。

```py
>>> image = pipeline("An image of a squirrel in Picasso style").images[0]
>>> image
```

![](../Images/020c32038fcd5d69660e0f7072208150.png)

通过调用`save`保存图像：

```py
>>> image.save("image_of_squirrel_painting.png")
```

### 本地pipeline

您也可以在本地使用pipeline。唯一的区别是您需要先下载权重：

```py
!git lfs install
!git clone https://huggingface.co/runwayml/stable-diffusion-v1-5
```

然后将保存的权重加载到pipeline中：

```py
>>> pipeline = DiffusionPipeline.from_pretrained("./stable-diffusion-v1-5", use_safetensors=True)
```

现在，您可以像上面的部分一样运行pipeline。

### 交换调度程序

不同的调度程序具有不同的去噪速度和质量权衡。找出哪种最适合您的最佳方法是尝试它们！🧨 Diffusers的主要功能之一是允许您轻松切换调度程序。例如，要用[EulerDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler#diffusers.EulerDiscreteScheduler)替换默认的[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)，请使用[from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)方法加载：

```py
>>> from diffusers import EulerDiscreteScheduler

>>> pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", use_safetensors=True)
>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)
```

尝试使用新的调度程序生成图像，看看是否有区别！

在下一节中，您将更仔细地查看组件 - 模型和调度程序 - 这些组件构成了[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)，并学习如何使用这些组件生成一张猫的图像。

## 模型

大多数模型接受嘈杂样本，并在每个时间步预测*噪声残差*（其他模型学习直接预测先前样本或速度或[`v-prediction`](https://github.com/huggingface/diffusers/blob/5e5ce13e2f89ac45a0066cb3f369462a3cf1d9ef/src/diffusers/schedulers/scheduling_ddim.py#L110)之间的差异，较少嘈杂的图像和输入图像之间的差异。您可以混合和匹配模型以创建其他扩散系统。

模型是使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)方法初始化的，该方法还会本地缓存模型权重，因此下次加载模型时会更快。对于快速入门，您将加载[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)，这是一个基本的无条件图像生成模型，带有在猫图像上训练的检查点：

```py
>>> from diffusers import UNet2DModel

>>> repo_id = "google/ddpm-cat-256"
>>> model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)
```

要访问模型参数，请调用`model.config`：

```py
>>> model.config
```

模型配置是一个🧊冻结🧊字典，这意味着在创建模型后无法更改这些参数。这是有意为之的，可以确保在开始定义模型架构时使用的参数保持不变，而其他参数仍然可以在推断过程中进行调整。

一些最重要的参数是：

+   `sample_size`：输入样本的高度和宽度维度。

+   `in_channels`：输入样本的输入通道数。

+   `down_block_types`和`up_block_types`：用于创建UNet架构的下采样和上采样块的类型。

+   `block_out_channels`：下采样块的输出通道数；也以相反顺序用于上采样块的输入通道数。

+   `layers_per_block`：每个UNet块中存在的ResNet块的数量。

要将模型用于推断，请使用随机高斯噪声创建图像形状。它应该有一个`batch`轴，因为模型可以接收多个随机噪声，一个`channel`轴对应于输入通道的数量，以及一个`sample_size`轴用于图像的高度和宽度：

```py
>>> import torch

>>> torch.manual_seed(0)

>>> noisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)
>>> noisy_sample.shape
torch.Size([1, 3, 256, 256])
```

对于推断，将嘈杂图像和一个`timestep`传递给模型。`timestep`指示输入图像有多嘈杂，开始时更多噪声，结束时更少噪声。这有助于模型确定其在扩散过程中的位置，无论是靠近开始还是结束。使用`sample`方法获取模型输出：

```py
>>> with torch.no_grad():
...     noisy_residual = model(sample=noisy_sample, timestep=2).sample
```

但要生成实际示例，您需要一个调度器来指导去噪过程。在下一节中，您将学习如何将模型与调度器配对。

## 调度器

调度器管理从嘈杂的样本到较少嘈杂的样本的转变，给定模型输出 - 在这种情况下，它是`noisy_residual`。

🧨 Diffusers是一个构建扩散系统的工具箱。虽然[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)是一个方便的方式来开始使用预构建的扩散系统，但您也可以选择自己的模型和调度器组件分别构建自定义扩散系统。

对于快速入门，您将使用其[from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)方法实例化[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)：

```py
>>> from diffusers import DDPMScheduler

>>> scheduler = DDPMScheduler.from_pretrained(repo_id)
>>> scheduler
DDPMScheduler {
  "_class_name": "DDPMScheduler",
  "_diffusers_version": "0.21.4",
  "beta_end": 0.02,
  "beta_schedule": "linear",
  "beta_start": 0.0001,
  "clip_sample": true,
  "clip_sample_range": 1.0,
  "dynamic_thresholding_ratio": 0.995,
  "num_train_timesteps": 1000,
  "prediction_type": "epsilon",
  "sample_max_value": 1.0,
  "steps_offset": 0,
  "thresholding": false,
  "timestep_spacing": "leading",
  "trained_betas": null,
  "variance_type": "fixed_small"
}
```

💡 与模型不同，调度器没有可训练的权重，也没有参数！

一些最重要的参数是：

+   `num_train_timesteps`：去噪过程的长度，或者换句话说，处理随机高斯噪声成为数据样本所需的时间步数。

+   `beta_schedule`：用于推断和训练的噪声计划类型。

+   `beta_start`和`beta_end`：噪声计划的起始和结束噪声值。

要预测一个稍微不那么嘈杂的图像，请将以下内容传递给调度器的[step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step)方法：模型输出，`timestep`和当前`sample`。

```py
>>> less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample).prev_sample
>>> less_noisy_sample.shape
torch.Size([1, 3, 256, 256])
```

`less_noisy_sample`可以传递到下一个`timestep`，在那里它会变得更少嘈杂！现在让我们把所有内容汇总起来，可视化整个去噪过程。

首先，创建一个后处理和显示去噪图像的函数作为`PIL.Image`：

```py
>>> import PIL.Image
>>> import numpy as np

>>> def display_sample(sample, i):
...     image_processed = sample.cpu().permute(0, 2, 3, 1)
...     image_processed = (image_processed + 1.0) * 127.5
...     image_processed = image_processed.numpy().astype(np.uint8)

...     image_pil = PIL.Image.fromarray(image_processed[0])
...     display(f"Image at step {i}")
...     display(image_pil)
```

为加快去噪过程，请将输入和模型移至GPU：

```py
>>> model.to("cuda")
>>> noisy_sample = noisy_sample.to("cuda")
```

现在创建一个去噪循环，预测较少噪音样本的残差，并使用调度器计算较少噪音的样本：

```py
>>> import tqdm

>>> sample = noisy_sample

>>> for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):
...     # 1\. predict noise residual
...     with torch.no_grad():
...         residual = model(sample, t).sample

...     # 2\. compute less noisy image and set x_t -> x_t-1
...     sample = scheduler.step(residual, t, sample).prev_sample

...     # 3\. optionally look at image
...     if (i + 1) % 50 == 0:
...         display_sample(sample, i + 1)
```

坐下来看着一只猫从一无所有的噪音中生成！😻

![](../Images/e6bad1468d511bff5946306330accda8.png)

## 下一步

希望您在这次快速浏览中使用🧨 Diffusers 生成了一些酷炫的图像！接下来，您可以：

+   在[训练](./tutorials/basic_training)教程中训练或微调模型以生成您自己的图像。

+   查看官方和社区的[训练或微调脚本示例](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples)，适用于各种用例。

+   在[使用不同调度器](./using-diffusers/schedulers)指南中了解更多关于加载、访问、更改和比较调度器的信息。

+   探索快速工程、速度和内存优化，以及使用[稳定扩散](./stable_diffusion)指南生成更高质量图像的技巧和窍门。

+   深入了解如何通过[优化PyTorch在GPU上](./optimization/fp16)的指南以及在Apple Silicon（M1/M2）上运行[稳定扩散](./optimization/mps)和[ONNX Runtime](./optimization/onnx)的推理指南来加快🧨 Diffusers的速度。
