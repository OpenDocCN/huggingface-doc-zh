- en: Wav2Vec2Phoneme
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2_phoneme](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2_phoneme)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual
    Phoneme Recognition (Xu et al., 2021](https://arxiv.org/abs/2109.11680) by Qiantong
    Xu, Alexei Baevski, Michael Auli.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent progress in self-training, self-supervised pretraining and unsupervised
    learning enabled well performing speech recognition systems without any labeled
    data. However, in many cases there is labeled data available for related languages
    which is not utilized by these methods. This paper extends previous work on zero-shot
    cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec
    2.0 model to transcribe unseen languages. This is done by mapping phonemes of
    the training languages to the target language using articulatory features. Experiments
    show that this simple method significantly outperforms prior work which introduced
    task-specific architectures and used only part of a monolingually pretrained model.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Relevant checkpoints can be found under [https://huggingface.co/models?other=phoneme-recognition](https://huggingface.co/models?other=phoneme-recognition).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme uses the exact same architecture as Wav2Vec2
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme is a speech model that accepts a float array corresponding to
    the raw waveform of the speech signal.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme model was trained using connectionist temporal classification
    (CTC) so the model output has to be decoded using [Wav2Vec2PhonemeCTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme can be fine-tuned on multiple language at once and decode unseen
    languages in a single forward pass to a sequence of phonemes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the model outputs a sequence of phonemes. In order to transform
    the phonemes to a sequence of words one should make use of a dictionary and language
    model.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme’s architecture is based on the Wav2Vec2 model, for API reference,
    check out [`Wav2Vec2`](wav2vec2)’s documentation page except for the tokenizer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Wav2Vec2PhonemeCTCTokenizer
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Wav2Vec2PhonemeCTCTokenizer`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L94)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sentence
    token.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sentence
    token.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_phonemize` (`bool`, *optional*, defaults to `True`) — Whether the tokenizer
    should phonetize the input or not. Only if a sequence of phonemes is passed to
    the tokenizer, `do_phonemize` should be set to `False`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`phonemizer_lang` (`str`, *optional*, defaults to `"en-us"`) — The language
    of the phoneme set to which the tokenizer should phonetize the input text to.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`phonemizer_backend` (`str`, *optional*. defaults to `"espeak"`) — The backend
    phonetization library that shall be used by the phonemizer library. Defaults to
    `espeak-ng`. See the [phonemizer package](https://github.com/bootphon/phonemizer#readme).
    for more information.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs — Additional keyword arguments passed along to [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs — 传递给[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的额外关键字参数'
- en: Constructs a Wav2Vec2PhonemeCTC tokenizer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Wav2Vec2PhonemeCTC分词器。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains some of the main methods. Users should refer to the superclass
    for more information regarding such methods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含一些主要方法。用户应参考超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）: 不填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 截断到指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。这将逐标记截断，如果提供了一对序列（或一批对序列），则从该对中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到指定的最大长度，由参数 `max_length` 指定，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对输入
    ID 序列（或一批对），并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出标记。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, defaults to 0) — 如果设置为一个数字，并且与 `max_length` 一起使用，当
    `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *optional*, 默认为 `False`) — 输入是否已经预分词（例如，已经分成单词）。如果设置为
    `True`，分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于 NER 或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。需要激活 `padding`。这对于启用
    NVIDIA 硬件上的 Tensor Cores 特别有用，计算能力 `>= 7.5`（Volta）。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *optional*) — 是否返回标记类型 ID。如果保持默认设置，将根据特定分词器的默认值返回标记类型
    ID，由 `return_outputs` 属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *optional`) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由
    `return_outputs` 属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *optional*, 默认为 `False`) — 是否返回溢出标记序列。如果提供了一对输入
    ID 序列（或一批对），并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出标记。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *optional*, 默认为 `False`) — 是否返回特殊标记掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *optional*, 默认为 `False`) — 是否返回每个标记的 `(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅适用于继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    的快速分词器，如果使用 Python 的分词器，此方法将引发 `NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *optional*, 默认为 `False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *optional*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给 `self.tokenize()`
    方法'
- en: Returns
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的标记id列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要提供给模型的标记类型id列表（当`return_token_type_ids=True`或*“token_type_ids”*在`self.model_input_names`中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些标记应由模型关注的索引列表（当`return_attention_mask=True`或*“attention_mask”*在`self.model_input_names`中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出的标记序列列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 截断的标记数（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当`return_length=True`时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个或多个序列或一个或多个序列对进行标记化和准备模型的主要方法。
- en: '#### `batch_decode`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L527)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L527)'
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`）
    — 标记化输入id的列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`） — 是否在解码中删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*） — 是否清除标记化空格。'
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_char_offsets`（`bool`，*可选*，默认为`False`） — 是否输出字符偏移。字符偏移可以与采样率和模型下采样率结合使用，计算转录字符的时间戳。'
- en: Please take a look at the Example of `~models.wav2vec2.tokenization_wav2vec2.decode`
    to better understand how to make use of `output_word_offsets`. `~model.wav2vec2_phoneme.tokenization_wav2vec2_phoneme.batch_decode`
    works analogous with phonemes and batched output.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看`~models.wav2vec2.tokenization_wav2vec2.decode`的示例，以更好地理解如何使用`output_word_offsets`。`~model.wav2vec2_phoneme.tokenization_wav2vec2_phoneme.batch_decode`与音素和批处理输出的处理方式类似。
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（其他关键字参数，*可选*） — 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]` or `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`或`~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`'
- en: The decoded sentence. Will be a `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`
    when `output_char_offsets == True`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子。当`output_char_offsets == True`时，将是`~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`。
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用解码将标记id的列表列表转换为字符串列表。
- en: '#### `decode`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L471)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L471)'
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`（`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`） —
    标记化输入id的列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`） — 是否在解码中删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*） — 是否清除标记化空格。'
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_char_offsets`（`bool`，*可选*，默认为`False`） — 是否输出字符偏移。字符偏移可以与采样率和模型下采样率结合使用，计算转录字符的时间戳。'
- en: Please take a look at the Example of `~models.wav2vec2.tokenization_wav2vec2.decode`
    to better understand how to make use of `output_word_offsets`. `~model.wav2vec2_phoneme.tokenization_wav2vec2_phoneme.batch_decode`
    works the same way with phonemes.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看`~models.wav2vec2.tokenization_wav2vec2.decode`的示例，以更好地理解如何使用`output_word_offsets`。`~model.wav2vec2_phoneme.tokenization_wav2vec2_phoneme.batch_decode`与音素的处理方式相同。
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（额外的关键字参数，*可选*）- 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`str` or `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`str` 或 `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`'
- en: The decoded sentence. Will be a `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`
    when `output_char_offsets == True`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子。当 `output_char_offsets == True` 时，将是一个 `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`。
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列id转换为字符串，使用分词器和词汇表，可以选择删除特殊标记并清理分词空格。
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于执行 `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。
- en: '#### `phonemize`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`phonemize`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L269)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L269)'
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
