- en: Wav2Vec2Phoneme
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2_phoneme](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2_phoneme)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual
    Phoneme Recognition (Xu et al., 2021](https://arxiv.org/abs/2109.11680) by Qiantong
    Xu, Alexei Baevski, Michael Auli.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent progress in self-training, self-supervised pretraining and unsupervised
    learning enabled well performing speech recognition systems without any labeled
    data. However, in many cases there is labeled data available for related languages
    which is not utilized by these methods. This paper extends previous work on zero-shot
    cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec
    2.0 model to transcribe unseen languages. This is done by mapping phonemes of
    the training languages to the target language using articulatory features. Experiments
    show that this simple method significantly outperforms prior work which introduced
    task-specific architectures and used only part of a monolingually pretrained model.*'
  prefs: []
  type: TYPE_NORMAL
- en: Relevant checkpoints can be found under [https://huggingface.co/models?other=phoneme-recognition](https://huggingface.co/models?other=phoneme-recognition).
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten)
  prefs: []
  type: TYPE_NORMAL
- en: The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme uses the exact same architecture as Wav2Vec2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme is a speech model that accepts a float array corresponding to
    the raw waveform of the speech signal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme model was trained using connectionist temporal classification
    (CTC) so the model output has to be decoded using [Wav2Vec2PhonemeCTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme can be fine-tuned on multiple language at once and decode unseen
    languages in a single forward pass to a sequence of phonemes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the model outputs a sequence of phonemes. In order to transform
    the phonemes to a sequence of words one should make use of a dictionary and language
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Phoneme’s architecture is based on the Wav2Vec2 model, for API reference,
    check out [`Wav2Vec2`](wav2vec2)’s documentation page except for the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Wav2Vec2PhonemeCTCTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Wav2Vec2PhonemeCTCTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L94)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sentence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sentence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_phonemize` (`bool`, *optional*, defaults to `True`) — Whether the tokenizer
    should phonetize the input or not. Only if a sequence of phonemes is passed to
    the tokenizer, `do_phonemize` should be set to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`phonemizer_lang` (`str`, *optional*, defaults to `"en-us"`) — The language
    of the phoneme set to which the tokenizer should phonetize the input text to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`phonemizer_backend` (`str`, *optional*. defaults to `"espeak"`) — The backend
    phonetization library that shall be used by the phonemizer library. Defaults to
    `espeak-ng`. See the [phonemizer package](https://github.com/bootphon/phonemizer#readme).
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs — Additional keyword arguments passed along to [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Constructs a Wav2Vec2PhonemeCTC tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains some of the main methods. Users should refer to the superclass
    for more information regarding such methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` — List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` — The length of the inputs (when `return_length=True`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L527)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please take a look at the Example of `~models.wav2vec2.tokenization_wav2vec2.decode`
    to better understand how to make use of `output_word_offsets`. `~model.wav2vec2_phoneme.tokenization_wav2vec2_phoneme.batch_decode`
    works analogous with phonemes and batched output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[str]` or `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: The decoded sentence. Will be a `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`
    when `output_char_offsets == True`.
  prefs: []
  type: TYPE_NORMAL
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L471)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please take a look at the Example of `~models.wav2vec2.tokenization_wav2vec2.decode`
    to better understand how to make use of `output_word_offsets`. `~model.wav2vec2_phoneme.tokenization_wav2vec2_phoneme.batch_decode`
    works the same way with phonemes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str` or `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: The decoded sentence. Will be a `~models.wav2vec2.tokenization_wav2vec2_phoneme.Wav2Vec2PhonemeCTCTokenizerOutput`
    when `output_char_offsets == True`.
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `phonemize`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py#L269)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
