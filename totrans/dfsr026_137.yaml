- en: ControlNet with Stable Diffusion XL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/41.a8dcb501.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/ExampleCodeBlock.658f5cd6.js">
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet was introduced in [Adding Conditional Control to Text-to-Image Diffusion
    Models](https://huggingface.co/papers/2302.05543) by Lvmin Zhang, Anyi Rao, and
    Maneesh Agrawala.
  prefs: []
  type: TYPE_NORMAL
- en: With a ControlNet model, you can provide an additional control image to condition
    and control Stable Diffusion generation. For example, if you provide a depth map,
    the ControlNet model generates an image that‚Äôll preserve the spatial information
    from the depth map. It is a more flexible and accurate way to control the image
    generation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We present ControlNet, a neural network architecture to add spatial conditioning
    controls to large, pretrained text-to-image diffusion models. ControlNet locks
    the production-ready large diffusion models, and reuses their deep and robust
    encoding layers pretrained with billions of images as a strong backbone to learn
    a diverse set of conditional controls. The neural architecture is connected with
    ‚Äúzero convolutions‚Äù (zero-initialized convolution layers) that progressively grow
    the parameters from zero and ensure that no harmful noise could affect the finetuning.
    We test various conditioning controls, eg, edges, depth, segmentation, human pose,
    etc, with Stable Diffusion, using single or multiple conditions, with or without
    prompts. We show that the training of ControlNets is robust with small (<50k)
    and large (>1m) datasets. Extensive results show that ControlNet may facilitate
    wider applications to control image diffusion models.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can find additional smaller Stable Diffusion XL (SDXL) ControlNet checkpoints
    from the ü§ó [Diffusers](https://huggingface.co/diffusers) Hub organization, and
    browse [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
    checkpoints on the Hub.
  prefs: []
  type: TYPE_NORMAL
- en: üß™ Many of the SDXL ControlNet checkpoints are experimental, and there is a lot
    of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose)
    and leave us feedback on how we can improve!
  prefs: []
  type: TYPE_NORMAL
- en: If you don‚Äôt see a checkpoint you‚Äôre interested in, you can train your own SDXL
    ControlNet with our [training script](../../../../../examples/controlnet/README_sdxl).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionXLControlNetPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.StableDiffusionXLControlNetPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L117)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection
    tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel
    controlnet: Union scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt:
    bool = True add_watermarker: Optional = None feature_extractor: CLIPImageProcessor
    = None image_encoder: CLIPVisionModelWithProjection = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vae** ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder_2** ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    ‚Äî Second frozen text-encoder ([laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer_2** ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet** ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    ‚Äî A `UNet2DConditionModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**controlnet** ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)
    or `List[ControlNetModel]`) ‚Äî Provides additional conditioning to the `unet` during
    the denoising process. If you set multiple ControlNets as a list, the outputs
    from each ControlNet are added together to create one combined additional conditioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler** ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**force_zeros_for_empty_prompt** (`bool`, *optional*, defaults to `"True"`)
    ‚Äî Whether the negative prompt embeddings should always be set to 0\. Also see
    the config of `stabilityai/stable-diffusion-xl-base-1-0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_watermarker** (`bool`, *optional*) ‚Äî Whether to use the [invisible_watermark](https://github.com/ShieldMnt/invisible-watermark/)
    library to watermark output images. If not defined, it defaults to `True` if the
    package is installed; otherwise no watermarker is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-image generation using Stable Diffusion XL with ControlNet
    guidance.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_lora_weights()` for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L943)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional
    = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float
    = 5.0 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt:
    Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None
    prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds:
    Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image:
    Union = None output_type: Optional = ''pil'' return_dict: bool = True cross_attention_kwargs:
    Optional = None controlnet_conditioning_scale: Union = 1.0 guess_mode: bool =
    False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 original_size:
    Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None negative_original_size:
    Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size:
    Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None
    callback_on_step_end_tensor_inputs: List = [''latents''] **kwargs ) ‚Üí [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to
    be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used
    in both text-encoders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image** (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, `List[np.ndarray]`, ‚Äî `List[List[torch.FloatTensor]]`,
    `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`): The ControlNet input
    condition to provide guidance to the `unet` for generation. If the type is specified
    as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can
    also be accepted as an image. The dimensions of the output image defaults to `image`‚Äôs
    dimensions. If height and/or width are passed, `image` is resized accordingly.
    If multiple ControlNets are specified in `init`, images must be passed as a list
    such that each element of the list can be correctly batched for input to a single
    ControlNet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*, defaults to `self.unet.config.sample_size *
    self.vae_scale_factor`) ‚Äî The height in pixels of the generated image. Anything
    below 512 pixels won‚Äôt work well for [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
    and checkpoints that are not specifically fine-tuned on low resolutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The width in pixels of the generated image. Anything below 512 pixels won‚Äôt
    work well for [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
    and checkpoints that are not specifically fine-tuned on low resolutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 50) ‚Äî The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 5.0) ‚Äî A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    to guide what to not include in image generation. This is sent to `tokenizer_2`
    and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) ‚Äî The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eta** (`float`, *optional*, defaults to 0.0) ‚Äî Corresponds to parameter eta
    (Œ∑) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) ‚Äî
    A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    pooled text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, pooled text embeddings are generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs (prompt
    weighting). If not provided, pooled `negative_prompt_embeds` are generated from
    `negative_prompt` input argument. ip_adapter_image ‚Äî (`PipelineImageInput`, *optional*):
    Optional image input to work with IP Adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) ‚Äî The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_kwargs** (`dict`, *optional*) ‚Äî A kwargs dictionary that
    if specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**controlnet_conditioning_scale** (`float` or `List[float]`, *optional*, defaults
    to 1.0) ‚Äî The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale`
    before they are added to the residual in the original `unet`. If multiple ControlNets
    are specified in `init`, you can set the corresponding scale as a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guess_mode** (`bool`, *optional*, defaults to `False`) ‚Äî The ControlNet encoder
    tries to recognize the content of the input image even if you remove all prompts.
    A `guidance_scale` value between 3.0 and 5.0 is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**control_guidance_start** (`float` or `List[float]`, *optional*, defaults
    to 0.0) ‚Äî The percentage of total steps at which the ControlNet starts applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**control_guidance_end** (`float` or `List[float]`, *optional*, defaults to
    1.0) ‚Äî The percentage of total steps at which the ControlNet stops applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**original_size** (`Tuple[int]`, *optional*, defaults to (1024, 1024)) ‚Äî If
    `original_size` is not the same as `target_size` the image will appear to be down-
    or upsampled. `original_size` defaults to `(height, width)` if not specified.
    Part of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**crops_coords_top_left** (`Tuple[int]`, *optional*, defaults to (0, 0)) ‚Äî
    `crops_coords_top_left` can be used to generate an image that appears to be ‚Äúcropped‚Äù
    from the position `crops_coords_top_left` downwards. Favorable, well-centered
    images are usually achieved by setting `crops_coords_top_left` to (0, 0). Part
    of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**target_size** (`Tuple[int]`, *optional*, defaults to (1024, 1024)) ‚Äî For
    most cases, `target_size` should be set to the desired height and width of the
    generated image. If not specified it will default to `(height, width)`. Part of
    SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_original_size** (`Tuple[int]`, *optional*, defaults to (1024, 1024))
    ‚Äî To negatively condition the generation process based on a specific image resolution.
    Part of SDXL‚Äôs micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_crops_coords_top_left** (`Tuple[int]`, *optional*, defaults to (0,
    0)) ‚Äî To negatively condition the generation process based on a specific crop
    coordinates. Part of SDXL‚Äôs micro-conditioning as explained in section 2.2 of
    [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_target_size** (`Tuple[int]`, *optional*, defaults to (1024, 1024))
    ‚Äî To negatively condition the generation process based on a target image resolution.
    It should be as same as the `target_size` for most cases. Part of SDXL‚Äôs micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clip_skip** (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_on_step_end** (`Callable`, *optional*) ‚Äî A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_on_step_end_tensor_inputs** (`List`, *optional*) ‚Äî The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned containing the output images.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### disable_freeu'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L887)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_vae_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L234)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_vae_tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L251)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_freeu'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L864)'
  prefs: []
  type: TYPE_NORMAL
- en: '( s1: float s2: float b1: float b2: float )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**s1** (`float`) ‚Äî Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**s2** (`float`) ‚Äî Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b1** (`float`) ‚Äî Scaling factor for stage 1 to amplify the contributions
    of backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b2** (`float`) ‚Äî Scaling factor for stage 2 to amplify the contributions
    of backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_vae_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L226)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_vae_tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L242)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### encode_prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L259)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt:
    int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None
    negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds:
    Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds:
    Optional = None lora_scale: Optional = None clip_skip: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) ‚Äî prompt to be encoded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to
    be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
    used in both text-encoders device ‚Äî (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`) ‚Äî number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_classifier_free_guidance** (`bool`) ‚Äî whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_2** (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, pooled text embeddings will be generated from `prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_pooled_prompt_embeds** (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lora_scale** (`float`, *optional*) ‚Äî A lora scale that will be applied to
    all LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clip_skip** (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
