- en: ControlNet with Stable Diffusion XL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¸¦æœ‰ Stable Diffusion XL çš„ ControlNet
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet was introduced in [Adding Conditional Control to Text-to-Image Diffusion
    Models](https://huggingface.co/papers/2302.05543) by Lvmin Zhang, Anyi Rao, and
    Maneesh Agrawala.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet åœ¨ [å‘æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ·»åŠ æ¡ä»¶æ§åˆ¶](https://huggingface.co/papers/2302.05543) ä¸­ç”±
    Lvmin Zhangã€Anyi Rao å’Œ Maneesh Agrawala æå‡ºã€‚
- en: With a ControlNet model, you can provide an additional control image to condition
    and control Stable Diffusion generation. For example, if you provide a depth map,
    the ControlNet model generates an image thatâ€™ll preserve the spatial information
    from the depth map. It is a more flexible and accurate way to control the image
    generation process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ControlNet æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æä¾›é¢å¤–çš„æ§åˆ¶å›¾åƒæ¥è°ƒèŠ‚å’Œæ§åˆ¶ Stable Diffusion ç”Ÿæˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æä¾›æ·±åº¦å›¾ï¼ŒControlNet
    æ¨¡å‹å°†ç”Ÿæˆä¸€ä¸ªä¿ç•™æ·±åº¦å›¾ä¸­ç©ºé—´ä¿¡æ¯çš„å›¾åƒã€‚è¿™æ˜¯ä¸€ç§æ›´çµæ´»å’Œå‡†ç¡®çš„æ§åˆ¶å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ–¹å¼ã€‚
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡çš„æ‘˜è¦ä¸ºï¼š
- en: '*We present ControlNet, a neural network architecture to add spatial conditioning
    controls to large, pretrained text-to-image diffusion models. ControlNet locks
    the production-ready large diffusion models, and reuses their deep and robust
    encoding layers pretrained with billions of images as a strong backbone to learn
    a diverse set of conditional controls. The neural architecture is connected with
    â€œzero convolutionsâ€ (zero-initialized convolution layers) that progressively grow
    the parameters from zero and ensure that no harmful noise could affect the finetuning.
    We test various conditioning controls, eg, edges, depth, segmentation, human pose,
    etc, with Stable Diffusion, using single or multiple conditions, with or without
    prompts. We show that the training of ControlNets is robust with small (<50k)
    and large (>1m) datasets. Extensive results show that ControlNet may facilitate
    wider applications to control image diffusion models.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬æå‡ºäº† ControlNetï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºå‘å¤§å‹ã€é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ·»åŠ ç©ºé—´è°ƒèŠ‚æ§åˆ¶ã€‚ControlNet é”å®šäº†ç”Ÿäº§å°±ç»ªçš„å¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡å¤ä½¿ç”¨å®ƒä»¬çš„æ·±åº¦å’Œç¨³å¥çš„ç¼–ç å±‚ï¼Œè¿™äº›ç¼–ç å±‚ç»è¿‡æ•°åäº¿å¼ å›¾åƒçš„é¢„è®­ç»ƒï¼Œä½œä¸ºå­¦ä¹ å„ç§æ¡ä»¶æ§åˆ¶çš„å¼ºå¤§æ”¯æ’‘ã€‚ç¥ç»æ¶æ„è¿æ¥äº†â€œé›¶å·ç§¯â€ï¼ˆä»é›¶åˆå§‹åŒ–çš„å·ç§¯å±‚ï¼‰ï¼Œé€æ¸å¢åŠ å‚æ•°ï¼Œç¡®ä¿æ²¡æœ‰æœ‰å®³å™ªéŸ³ä¼šå½±å“å¾®è°ƒã€‚æˆ‘ä»¬æµ‹è¯•äº†å„ç§è°ƒèŠ‚æ§åˆ¶ï¼Œä¾‹å¦‚è¾¹ç¼˜ã€æ·±åº¦ã€åˆ†å‰²ã€äººä½“å§¿åŠ¿ç­‰ï¼Œä½¿ç”¨å•ä¸ªæˆ–å¤šä¸ªæ¡ä»¶ï¼Œæœ‰æˆ–æ²¡æœ‰æç¤ºï¼Œä¸
    Stable Diffusion ç»“åˆã€‚æˆ‘ä»¬å±•ç¤ºäº† ControlNet çš„è®­ç»ƒå¯¹äºå°å‹ï¼ˆ<50kï¼‰å’Œå¤§å‹ï¼ˆ>1mï¼‰æ•°æ®é›†æ˜¯ç¨³å¥çš„ã€‚å¹¿æ³›çš„ç»“æœè¡¨æ˜ï¼ŒControlNet
    å¯èƒ½ä¿ƒè¿›æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œä»¥æ§åˆ¶å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚*'
- en: You can find additional smaller Stable Diffusion XL (SDXL) ControlNet checkpoints
    from the ğŸ¤— [Diffusers](https://huggingface.co/diffusers) Hub organization, and
    browse [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
    checkpoints on the Hub.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ğŸ¤— [Diffusers](https://huggingface.co/diffusers) Hub ç»„ç»‡ä¸­æ‰¾åˆ°é¢å¤–çš„è¾ƒå°çš„ Stable Diffusion
    XLï¼ˆSDXLï¼‰ControlNet æ£€æŸ¥ç‚¹ï¼Œå¹¶åœ¨ Hub ä¸Šæµè§ˆ[ç¤¾åŒºè®­ç»ƒçš„](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)æ£€æŸ¥ç‚¹ã€‚
- en: ğŸ§ª Many of the SDXL ControlNet checkpoints are experimental, and there is a lot
    of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose)
    and leave us feedback on how we can improve!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ§ª è®¸å¤š SDXL ControlNet æ£€æŸ¥ç‚¹æ˜¯å®éªŒæ€§çš„ï¼Œæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ¬¢è¿æ‰“å¼€ä¸€ä¸ª[Issue](https://github.com/huggingface/diffusers/issues/new/choose)ï¼Œå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•æ”¹è¿›ï¼
- en: If you donâ€™t see a checkpoint youâ€™re interested in, you can train your own SDXL
    ControlNet with our [training script](../../../../../examples/controlnet/README_sdxl).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ²¡æœ‰çœ‹åˆ°æ‚¨æ„Ÿå…´è¶£çš„æ£€æŸ¥ç‚¹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„[è®­ç»ƒè„šæœ¬](../../../../../examples/controlnet/README_sdxl)è®­ç»ƒè‡ªå·±çš„SDXL
    ControlNetã€‚
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æŸ¥çœ‹è°ƒåº¦å™¨[æŒ‡å—](../../using-diffusers/schedulers)ä»¥äº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŸ¥çœ‹[è·¨ç®¡é“é‡ç”¨ç»„ä»¶](../../using-diffusers/loading#reuse-components-across-pipelines)éƒ¨åˆ†ï¼Œä»¥äº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚
- en: StableDiffusionXLControlNetPipeline
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StableDiffusionXLControlNetPipeline
- en: '### `class diffusers.StableDiffusionXLControlNetPipeline`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.StableDiffusionXLControlNetPipeline` ç±»'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L117)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L117)'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    â€” Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºå½¢å¼ã€‚'
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    â€” Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚'
- en: '`text_encoder_2` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” Second frozen text-encoder ([laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_2` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” ç¬¬äºŒä¸ªå†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)ï¼‰ã€‚'
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” A `CLIPTokenizer` to tokenize text.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚'
- en: '`tokenizer_2` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” A `CLIPTokenizer` to tokenize text.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_2` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” A `UNet2DConditionModel` to denoise the encoded image latents.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„ `UNet2DConditionModel`ã€‚'
- en: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)
    or `List[ControlNetModel]`) â€” Provides additional conditioning to the `unet` during
    the denoising process. If you set multiple ControlNets as a list, the outputs
    from each ControlNet are added together to create one combined additional conditioning.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)
    æˆ– `List[ControlNetModel]`) â€” åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¸º `unet` æä¾›é¢å¤–çš„æ¡ä»¶ã€‚å¦‚æœå°†å¤šä¸ª ControlNet è®¾ç½®ä¸ºåˆ—è¡¨ï¼Œåˆ™æ¯ä¸ª
    ControlNet çš„è¾“å‡ºå°†ç›¸åŠ ï¼Œä»¥åˆ›å»ºä¸€ä¸ªç»„åˆçš„é¢å¤–æ¡ä»¶ã€‚'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    â€” A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    â€” ç”¨äºä¸ `unet` ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„è°ƒåº¦ç¨‹åºã€‚å¯ä»¥æ˜¯ [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ã€[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)
    æˆ– [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)
    ä¸­çš„ä¸€ä¸ªã€‚'
- en: '`force_zeros_for_empty_prompt` (`bool`, *optional*, defaults to `"True"`) â€”
    Whether the negative prompt embeddings should always be set to 0\. Also see the
    config of `stabilityai/stable-diffusion-xl-base-1-0`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_zeros_for_empty_prompt` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `"True"`) â€” æ˜¯å¦å§‹ç»ˆå°†è´Ÿæç¤ºåµŒå…¥è®¾ç½®ä¸º
    0ã€‚ä¹Ÿè¯·å‚é˜… `stabilityai/stable-diffusion-xl-base-1-0` çš„é…ç½®ã€‚'
- en: '`add_watermarker` (`bool`, *optional*) â€” Whether to use the [invisible_watermark](https://github.com/ShieldMnt/invisible-watermark/)
    library to watermark output images. If not defined, it defaults to `True` if the
    package is installed; otherwise no watermarker is used.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_watermarker` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä½¿ç”¨ [invisible_watermark](https://github.com/ShieldMnt/invisible-watermark/)
    åº“å¯¹è¾“å‡ºå›¾åƒè¿›è¡Œæ°´å°å¤„ç†ã€‚å¦‚æœæœªå®šä¹‰ï¼Œä¸”å®‰è£…äº†è¯¥è½¯ä»¶åŒ…ï¼Œåˆ™é»˜è®¤ä¸º `True`ï¼›å¦åˆ™ä¸ä½¿ç”¨æ°´å°å¤„ç†ã€‚'
- en: Pipeline for text-to-image generation using Stable Diffusion XL with ControlNet
    guidance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Stable Diffusion XL å’Œ ControlNet æŒ‡å¯¼çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæµæ°´çº¿ã€‚
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æµæ°´çº¿å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯¥æµæ°´çº¿è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•:'
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥'
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    ç”¨äºåŠ è½½ LoRA æƒé‡'
- en: '`save_lora_weights()` for saving LoRA weights'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_lora_weights()` ç”¨äºä¿å­˜ LoRA æƒé‡'
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    ç”¨äºåŠ è½½ `.ckpt` æ–‡ä»¶'
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    ç”¨äºåŠ è½½ IP é€‚é…å™¨'
- en: '#### `__call__`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L943)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L943)'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’ `prompt_embeds`ã€‚'
- en: '`prompt_2` (`str` or `List[str]`, *optional*) â€” The prompt or prompts to be
    sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used in
    both text-encoders.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” å‘é€åˆ° `tokenizer_2` å’Œ `text_encoder_2`
    çš„æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™åœ¨ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨ä¸­ä½¿ç”¨ `prompt`ã€‚'
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, `List[np.ndarray]`, â€” `List[List[torch.FloatTensor]]`,
    `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`): The ControlNet input
    condition to provide guidance to the `unet` for generation. If the type is specified
    as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can
    also be accepted as an image. The dimensions of the output image defaults to `image`â€™s
    dimensions. If height and/or width are passed, `image` is resized accordingly.
    If multiple ControlNets are specified in `init`, images must be passed as a list
    such that each element of the list can be correctly batched for input to a single
    ControlNet.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, `List[np.ndarray]`, â€” `List[List[torch.FloatTensor]]`,
    `List[List[np.ndarray]]` æˆ– `List[List[PIL.Image.Image]]`): ç”¨äºä¸º`unet`æä¾›æŒ‡å¯¼çš„ControlNetè¾“å…¥æ¡ä»¶ã€‚å¦‚æœæŒ‡å®šç±»å‹ä¸º`torch.FloatTensor`ï¼Œåˆ™æŒ‰åŸæ ·ä¼ é€’ç»™ControlNetã€‚`PIL.Image.Image`ä¹Ÿå¯ä»¥ä½œä¸ºå›¾åƒæ¥å—ã€‚è¾“å‡ºå›¾åƒçš„å°ºå¯¸é»˜è®¤ä¸º`image`çš„å°ºå¯¸ã€‚å¦‚æœä¼ é€’äº†é«˜åº¦å’Œ/æˆ–å®½åº¦ï¼Œ`image`å°†ç›¸åº”è°ƒæ•´å¤§å°ã€‚å¦‚æœåœ¨`init`ä¸­æŒ‡å®šäº†å¤šä¸ªControlNetsï¼Œåˆ™å¿…é¡»å°†å›¾åƒä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼Œä»¥ä¾¿åˆ—è¡¨çš„æ¯ä¸ªå…ƒç´ å¯ä»¥æ­£ç¡®æ‰¹å¤„ç†ä¸ºå•ä¸ªControlNetçš„è¾“å…¥ã€‚'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    â€” The height in pixels of the generated image. Anything below 512 pixels wonâ€™t
    work well for [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
    and checkpoints that are not specifically fine-tuned on low resolutions.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`)
    â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚ä½äº512åƒç´ çš„ä»»ä½•å†…å®¹éƒ½ä¸é€‚ç”¨äº[stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)å’Œæœªç»ä¸“é—¨è°ƒæ•´ä»¥é€‚åº”ä½åˆ†è¾¨ç‡çš„æ£€æŸ¥ç‚¹ã€‚'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    â€” The width in pixels of the generated image. Anything below 512 pixels wonâ€™t
    work well for [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
    and checkpoints that are not specifically fine-tuned on low resolutions.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`)
    â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚ä½äº512åƒç´ çš„ä»»ä½•å†…å®¹éƒ½ä¸é€‚ç”¨äº[stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)å’Œæœªç»ä¸“é—¨è°ƒæ•´ä»¥é€‚åº”ä½åˆ†è¾¨ç‡çš„æ£€æŸ¥ç‚¹ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º50) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 5.0) â€” A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º5.0) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale
    > 1`æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆ`guidance_scale
    < 1`ï¼‰å°†è¢«å¿½ç•¥ã€‚'
- en: '`negative_prompt_2` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    to guide what to not include in image generation. This is sent to `tokenizer_2`
    and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚è¿™å°†å‘é€åˆ°`tokenizer_2`å’Œ`text_encoder_2`ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™åœ¨ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨ä¸­éƒ½ä½¿ç”¨`negative_prompt`ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`eta` (`float`, *optional*, defaults to 0.0) â€” Corresponds to parameter eta
    (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” å¯¹åº”äº[DDIM](https://arxiv.org/abs/2010.02502)è®ºæ–‡ä¸­çš„å‚æ•°eta
    (Î·)ã€‚ä»…é€‚ç”¨äº[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­å°†è¢«å¿½ç•¥ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆ`negative_prompt_embeds`ã€‚'
- en: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated pooled
    text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
    not provided, pooled text embeddings are generated from `prompt` input argument.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ± åŒ–æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œæ± åŒ–æ–‡æœ¬åµŒå…¥å°†ä»
    `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆã€‚'
- en: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs (prompt
    weighting). If not provided, pooled `negative_prompt_embeds` are generated from
    `negative_prompt` input argument. ip_adapter_image â€” (`PipelineImageInput`, *optional*):
    Optional image input to work with IP Adapters.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿé¢æ± åŒ–æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œè´Ÿé¢æ± åŒ–æ–‡æœ¬åµŒå…¥å°†ä»
    `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆã€‚ip_adapter_image â€” (`PipelineImageInput`, *optional*):
    å¯é€‰çš„å›¾åƒè¾“å…¥ï¼Œç”¨äºä¸ IP é€‚é…å™¨ä¸€èµ·ä½¿ç”¨ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ–
    `np.array` ä¹‹é—´ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`cross_attention_kwargs` (`dict`, *optional*) â€” A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) â€” å¦‚æœæŒ‡å®šï¼Œå°†ä¼ é€’ç»™ `AttentionProcessor`
    çš„ kwargs å­—å…¸ï¼Œå¦‚ [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    ä¸­å®šä¹‰ã€‚'
- en: '`controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 1.0) â€” The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale`
    before they are added to the residual in the original `unet`. If multiple ControlNets
    are specified in `init`, you can set the corresponding scale as a list.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`controlnet_conditioning_scale` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º 1.0)
    â€” åœ¨å°† ControlNet çš„è¾“å‡ºæ·»åŠ åˆ°åŸå§‹ `unet` ä¸­çš„æ®‹å·®ä¹‹å‰ï¼Œå°†å…¶ä¹˜ä»¥ `controlnet_conditioning_scale`ã€‚å¦‚æœåœ¨
    `init` ä¸­æŒ‡å®šäº†å¤šä¸ª ControlNetsï¼Œå¯ä»¥å°†ç›¸åº”çš„æ¯”ä¾‹è®¾ç½®ä¸ºåˆ—è¡¨ã€‚'
- en: '`guess_mode` (`bool`, *optional*, defaults to `False`) â€” The ControlNet encoder
    tries to recognize the content of the input image even if you remove all prompts.
    A `guidance_scale` value between 3.0 and 5.0 is recommended.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guess_mode` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ§åˆ¶ç½‘ç»œç¼–ç å™¨å°è¯•è¯†åˆ«è¾“å…¥å›¾åƒçš„å†…å®¹ï¼Œå³ä½¿æ‚¨åˆ é™¤äº†æ‰€æœ‰æç¤ºã€‚å»ºè®®è®¾ç½®
    `guidance_scale` å€¼åœ¨ 3.0 åˆ° 5.0 ä¹‹é—´ã€‚'
- en: '`control_guidance_start` (`float` or `List[float]`, *optional*, defaults to
    0.0) â€” The percentage of total steps at which the ControlNet starts applying.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control_guidance_start` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º 0.0) â€” æ§åˆ¶ç½‘ç»œå¼€å§‹åº”ç”¨çš„æ€»æ­¥éª¤ç™¾åˆ†æ¯”ã€‚'
- en: '`control_guidance_end` (`float` or `List[float]`, *optional*, defaults to 1.0)
    â€” The percentage of total steps at which the ControlNet stops applying.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control_guidance_end` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º 1.0) â€” æ§åˆ¶ç½‘ç»œåœæ­¢åº”ç”¨çš„æ€»æ­¥éª¤ç™¾åˆ†æ¯”ã€‚'
- en: '`original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) â€” If `original_size`
    is not the same as `target_size` the image will appear to be down- or upsampled.
    `original_size` defaults to `(height, width)` if not specified. Part of SDXLâ€™s
    micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_size` (`Tuple[int]`, *optional*, é»˜è®¤ä¸º (1024, 1024)) â€” å¦‚æœ `original_size`
    ä¸ `target_size` ä¸åŒï¼Œå›¾åƒå°†å‘ˆç°ä¸ºç¼©å°æˆ–æ”¾å¤§ã€‚å¦‚æœæœªæŒ‡å®šï¼Œ`original_size` é»˜è®¤ä¸º `(height, width)`ã€‚ä½œä¸º
    SDXL çš„å¾®è°ƒæ¡ä»¶çš„ä¸€éƒ¨åˆ†ï¼Œè¯¦è§ [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    ç¬¬2.2èŠ‚ã€‚'
- en: '`crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0, 0)) â€” `crops_coords_top_left`
    can be used to generate an image that appears to be â€œcroppedâ€ from the position
    `crops_coords_top_left` downwards. Favorable, well-centered images are usually
    achieved by setting `crops_coords_top_left` to (0, 0). Part of SDXLâ€™s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_coords_top_left` (`Tuple[int]`, *optional*, é»˜è®¤ä¸º (0, 0)) â€” `crops_coords_top_left`
    å¯ç”¨äºç”Ÿæˆä¸€ä¸ªçœ‹èµ·æ¥ä»ä½ç½® `crops_coords_top_left` å‘ä¸‹â€œè£å‰ªâ€çš„å›¾åƒã€‚é€šå¸¸é€šè¿‡å°† `crops_coords_top_left`
    è®¾ç½®ä¸º (0, 0) æ¥å®ç°æœ‰åˆ©çš„ã€å±…ä¸­çš„å›¾åƒã€‚ä½œä¸º SDXL çš„å¾®è°ƒæ¡ä»¶çš„ä¸€éƒ¨åˆ†ï¼Œè¯¦è§ [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    ç¬¬2.2èŠ‚ã€‚'
- en: '`target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) â€” For most
    cases, `target_size` should be set to the desired height and width of the generated
    image. If not specified it will default to `(height, width)`. Part of SDXLâ€™s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size` (`Tuple[int]`, *optional*, é»˜è®¤ä¸º (1024, 1024)) â€” å¯¹äºå¤§å¤šæ•°æƒ…å†µï¼Œ`target_size`
    åº”è®¾ç½®ä¸ºç”Ÿæˆå›¾åƒçš„æœŸæœ›é«˜åº¦å’Œå®½åº¦ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†é»˜è®¤ä¸º `(height, width)`ã€‚ä½œä¸º SDXL çš„å¾®è°ƒæ¡ä»¶çš„ä¸€éƒ¨åˆ†ï¼Œè¯¦è§ [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    ç¬¬2.2èŠ‚ã€‚'
- en: '`negative_original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024))
    â€” To negatively condition the generation process based on a specific image resolution.
    Part of SDXLâ€™s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_original_size` (`Tuple[int]`, *optional*, é»˜è®¤ä¸º (1024, 1024)) â€” åŸºäºç‰¹å®šå›¾åƒåˆ†è¾¨ç‡å¦å®šåœ°è°ƒæ•´ç”Ÿæˆè¿‡ç¨‹ã€‚ä½œä¸º
    SDXL çš„å¾®è°ƒæ¡ä»¶çš„ä¸€éƒ¨åˆ†ï¼Œè¯¦è§ [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    ç¬¬2.2èŠ‚ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤é—®é¢˜çº¿ç¨‹ï¼š[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)ã€‚'
- en: '`negative_crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0,
    0)) â€” To negatively condition the generation process based on a specific crop
    coordinates. Part of SDXLâ€™s micro-conditioning as explained in section 2.2 of
    [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_crops_coords_top_left`ï¼ˆ`Tuple[int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º(0, 0)ï¼‰â€”æ ¹æ®ç‰¹å®šè£å‰ªåæ ‡å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œè´Ÿæ¡ä»¶åŒ–ã€‚ä½œä¸ºSDXLçš„å¾®æ¡ä»¶åŒ–çš„ä¸€éƒ¨åˆ†ï¼Œå¦‚[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)ç¬¬2.2èŠ‚ä¸­æ‰€è¿°ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤é—®é¢˜çº¿ç¨‹ï¼š[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)ã€‚'
- en: '`negative_target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024))
    â€” To negatively condition the generation process based on a target image resolution.
    It should be as same as the `target_size` for most cases. Part of SDXLâ€™s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_target_size`ï¼ˆ`Tuple[int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º(1024, 1024)ï¼‰â€”æ ¹æ®ç›®æ ‡å›¾åƒåˆ†è¾¨ç‡å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œè´Ÿæ¡ä»¶åŒ–ã€‚å¯¹äºå¤§å¤šæ•°æƒ…å†µï¼Œå®ƒåº”ä¸`target_size`ç›¸åŒã€‚ä½œä¸ºSDXLçš„å¾®æ¡ä»¶åŒ–çš„ä¸€éƒ¨åˆ†ï¼Œå¦‚[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)ç¬¬2.2èŠ‚ä¸­æ‰€è¿°ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤é—®é¢˜çº¿ç¨‹ï¼š[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)ã€‚'
- en: '`clip_skip` (`int`, *optional*) â€” Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦è·³è¿‡çš„CLIPå±‚æ•°ã€‚å€¼ä¸º1æ„å‘³ç€å°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚'
- en: '`callback_on_step_end` (`Callable`, *optional*) â€” A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨æ¨æ–­æœŸé—´æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…æ‹¬ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs`ï¼ˆ`List`ï¼Œ*å¯é€‰*ï¼‰â€”`callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚'
- en: Returns
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)æˆ–`tuple`'
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned containing the output images.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªåŒ…å«è¾“å‡ºå›¾åƒçš„`tuple`ã€‚
- en: The call function to the pipeline for generation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `disable_freeu`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L887)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L887)'
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¯ç”¨ï¼Œç¦ç”¨FreeUæœºåˆ¶ã€‚
- en: '#### `disable_vae_slicing`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L234)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L234)'
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚
- en: '#### `disable_vae_tiling`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L251)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L251)'
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦ç”¨å¹³é“ºVAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚
- en: '#### `enable_freeu`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L864)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L864)'
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`s1` (`float`) â€” Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate â€œoversmoothing effectâ€ in the enhanced
    denoising process.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1`ï¼ˆ`float`ï¼‰â€”ç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾è´¡çŒ®çš„ç¬¬1é˜¶æ®µçš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚'
- en: '`s2` (`float`) â€” Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate â€œoversmoothing effectâ€ in the enhanced
    denoising process.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2`ï¼ˆ`float`ï¼‰â€”ç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾è´¡çŒ®çš„ç¬¬2é˜¶æ®µçš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚'
- en: '`b1` (`float`) â€” Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1`ï¼ˆ`float`ï¼‰â€”ç”¨äºæ”¾å¤§ç¬¬1é˜¶æ®µçš„éª¨å¹²ç‰¹å¾è´¡çŒ®çš„ç¼©æ”¾å› å­ã€‚'
- en: '`b2` (`float`) â€” Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2`ï¼ˆ`float`ï¼‰â€”ç”¨äºæ”¾å¤§ç¬¬2é˜¶æ®µçš„éª¨å¹²ç‰¹å¾è´¡çŒ®çš„ç¼©æ”¾å› å­ã€‚'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨FreeUæœºåˆ¶ï¼Œå¦‚[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)ã€‚
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºåº”ç”¨å®ƒä»¬çš„é˜¶æ®µã€‚
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ï¼Œäº†è§£å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚Stable Diffusion
    v1ã€v2å’ŒStable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚
- en: '#### `enable_vae_slicing`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L226)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L226)'
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†åœ¨å‡ ä¸ªæ­¥éª¤ä¸­å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆåˆ‡ç‰‡ä»¥è¿›è¡Œè§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°éå¸¸æœ‰ç”¨ã€‚
- en: '#### `enable_vae_tiling`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L242)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L242)'
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨å¹³é“ºVAEè§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†æˆç“¦ç‰‡ä»¥åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è¿›è¡Œè§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚
- en: '#### `encode_prompt`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L259)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L259)'
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`, *optional*) â€” prompt to be encoded'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *optional*) â€” è¦ç¼–ç çš„æç¤º'
- en: '`prompt_2` (`str` or `List[str]`, *optional*) â€” The prompt or prompts to be
    sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used
    in both text-encoders device â€” (`torch.device`): torch device'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2` (`str` or `List[str]`, *optional*) â€” è¦å‘é€åˆ°`tokenizer_2`å’Œ`text_encoder_2`çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™åœ¨ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨è®¾å¤‡ä¸­ä½¿ç”¨`prompt`
    â€” (`torch.device`): torchè®¾å¤‡'
- en: '`num_images_per_prompt` (`int`) â€” number of images that should be generated
    per prompt'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡'
- en: '`do_classifier_free_guidance` (`bool`) â€” whether to use classifier free guidance
    or not'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” ä¸ç”¨æ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™è¢«å¿½ç•¥ï¼‰ã€‚'
- en: '`negative_prompt_2` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`str` or `List[str]`, *optional*) â€” ä¸ç”¨æ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºï¼Œè¦å‘é€åˆ°`tokenizer_2`å’Œ`text_encoder_2`ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™åœ¨ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨ä¸­ä½¿ç”¨`negative_prompt`'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆè´Ÿnegative_prompt_embedsã€‚'
- en: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated pooled
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, pooled text embeddings will be generated from `prompt` input
    argument.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ± åŒ–æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ± åŒ–çš„æ–‡æœ¬åµŒå…¥ã€‚'
- en: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ± åŒ–æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ± åŒ–çš„negative_prompt_embedsã€‚'
- en: '`lora_scale` (`float`, *optional*) â€” A lora scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *optional*) â€” å¦‚æœåŠ è½½äº†LoRAå±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰LoRAå±‚çš„loraæ¯”ä¾‹ã€‚'
- en: '`clip_skip` (`int`, *optional*) â€” Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) â€” è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä»CLIPä¸­è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º1è¡¨ç¤ºå°†ä½¿ç”¨é¢„ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚
