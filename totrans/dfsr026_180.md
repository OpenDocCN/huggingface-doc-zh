# UniDiffuser

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser](https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser)

UniDiffuser模型是由Fan Bao、Shen Nie、Kaiwen Xue、Chongxuan Li、Shi Pu、Yaole Wang、Gang Yue、Yue Cao、Hang Su、Jun Zhu在[One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](https://huggingface.co/papers/2303.06555)中提出的。

论文摘要如下：

*本文提出了一个统一的扩散框架（称为UniDiffuser），以适应一个模型中与一组多模态数据相关的所有分布。我们的关键见解是——学习边缘、条件和联合分布的扩散模型可以统一为预测扰动数据中的噪声，其中扰动水平（即时间步长）可以针对不同的模态而不同。受到统一视角的启发，UniDiffuser通过最小的修改原始扩散模型——扰动所有模态的数据而不是单个模态，输入不同模态中的单个时间步长，并预测所有模态的噪声而不是单个模态。UniDiffuser由一个用于处理不同模态输入类型的扩散模型的变压器参数化。在大规模配对的图像-文本数据上实现，UniDiffuser能够通过设置适当的时间步长执行图像、文本、文本到图像、图像到文本和图像-文本对生成，而无需额外的开销。特别是，UniDiffuser能够在所有任务中生成感知上逼真的样本，其定量结果（例如FID和CLIP分数）不仅优于现有的通用模型，而且在代表性任务（例如文本到图像生成）中也与定制模型（例如稳定扩散和DALL-E 2）可比。*

您可以在[thu-ml/unidiffuser](https://github.com/thu-ml/unidiffuser)找到原始代码库，并在[thu-ml](https://huggingface.co/thu-ml)找到额外的检查点。

PyTorch 1.X目前存在一个问题，即输出图像全部为黑色或像素值变为`NaNs`。可以通过切换到PyTorch 2.X来缓解这个问题。

此管道由[dg845](https://github.com/dg845)贡献。❤️

## 使用示例

由于UniDiffuser模型经过训练，可以对（图像、文本）对的联合分布进行建模，因此它能够执行各种生成任务：

### 无条件图像和文本生成

从[UniDiffuserPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline)生成的无条件生成（从标准高斯先验中采样的潜变量开始）将产生一个（图像、文本）对：

```py
import torch

from diffusers import UniDiffuserPipeline

device = "cuda"
model_id_or_path = "thu-ml/unidiffuser-v1"
pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
pipe.to(device)

# Unconditional image and text generation. The generation task is automatically inferred.
sample = pipe(num_inference_steps=20, guidance_scale=8.0)
image = sample.images[0]
text = sample.text[0]
image.save("unidiffuser_joint_sample_image.png")
print(text)
```

这在UniDiffuser论文中也被称为“联合”生成，因为我们是从联合图像-文本分布中进行采样。

请注意，生成任务是从调用管道时使用的输入中推断出来的。也可以使用[UniDiffuserPipeline.set_joint_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_joint_mode)手动指定无条件生成任务（“模式”）：

```py
# Equivalent to the above.
pipe.set_joint_mode()
sample = pipe(num_inference_steps=20, guidance_scale=8.0)
```

当手动设置模式时，后续对管道的调用将使用设置的模式，而不会尝试推断模式。您可以使用[UniDiffuserPipeline.reset_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.reset_mode)重置模式，之后管道将再次推断模式。

您还可以仅生成图像或仅文本（UniDiffuser论文称为“边缘”生成，因为我们分别从图像和文本的边缘分布中进行采样）：

```py
# Unlike other generation tasks, image-only and text-only generation don't use classifier-free guidance
# Image-only generation
pipe.set_image_mode()
sample_image = pipe(num_inference_steps=20).images[0]
# Text-only generation
pipe.set_text_mode()
sample_text = pipe(num_inference_steps=20).text[0]
```

### 文本到图像生成

UniDiffuser还能够从条件分布中进行采样；即，基于文本提示的图像分布或基于图像的文本分布。以下是从条件图像分布（文本到图像生成或文本条件图像生成）中进行采样的示例：

```py
import torch

from diffusers import UniDiffuserPipeline

device = "cuda"
model_id_or_path = "thu-ml/unidiffuser-v1"
pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
pipe.to(device)

# Text-to-image generation
prompt = "an elephant under the sea"

sample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)
t2i_image = sample.images[0]
t2i_image
```

`text2img`模式要求提供输入的`prompt`或`prompt_embeds`。您可以使用[UniDiffuserPipeline.set_text_to_image_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_text_to_image_mode)手动设置`text2img`模式。

### 图像到文本生成

同样，UniDiffuser也可以在给定图像的情况下生成文本样本（图像到文本或图像条件文本生成）：

```py
import torch

from diffusers import UniDiffuserPipeline
from diffusers.utils import load_image

device = "cuda"
model_id_or_path = "thu-ml/unidiffuser-v1"
pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
pipe.to(device)

# Image-to-text generation
image_url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg"
init_image = load_image(image_url).resize((512, 512))

sample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)
i2t_text = sample.text[0]
print(i2t_text)
```

`img2text`模式要求提供输入的`image`。您可以使用[UniDiffuserPipeline.set_image_to_text_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_image_to_text_mode)手动设置`img2text`模式。

### 图像变化

UniDiffuser的作者建议通过“往返”生成方法执行图像变化，即给定一个输入图像，首先执行图像到文本的生成，然后在第一代的输出上执行文本到图像的生成。这会产生一个与输入图像在语义上相似的新图像：

```py
import torch

from diffusers import UniDiffuserPipeline
from diffusers.utils import load_image

device = "cuda"
model_id_or_path = "thu-ml/unidiffuser-v1"
pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
pipe.to(device)

# Image variation can be performed with an image-to-text generation followed by a text-to-image generation:
# 1\. Image-to-text generation
image_url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg"
init_image = load_image(image_url).resize((512, 512))

sample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)
i2t_text = sample.text[0]
print(i2t_text)

# 2\. Text-to-image generation
sample = pipe(prompt=i2t_text, num_inference_steps=20, guidance_scale=8.0)
final_image = sample.images[0]
final_image.save("unidiffuser_image_variation_sample.png")
```

### 文本变化

类似地，可以通过文本到图像生成，然后通过图像到文本生成，在输入提示上执行文本变化：

```py
import torch

from diffusers import UniDiffuserPipeline

device = "cuda"
model_id_or_path = "thu-ml/unidiffuser-v1"
pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
pipe.to(device)

# Text variation can be performed with a text-to-image generation followed by a image-to-text generation:
# 1\. Text-to-image generation
prompt = "an elephant under the sea"

sample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)
t2i_image = sample.images[0]
t2i_image.save("unidiffuser_text2img_sample_image.png")

# 2\. Image-to-text generation
sample = pipe(image=t2i_image, num_inference_steps=20, guidance_scale=8.0)
final_prompt = sample.text[0]
print(final_prompt)
```

请务必查看调度器指南，了解如何在调度器速度和质量之间进行权衡，并查看重复使用组件跨管道部分，以了解如何有效地将相同组件加载到多个管道中。

## UniDiffuserPipeline

### `class diffusers.UniDiffuserPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L51)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel image_encoder: CLIPVisionModelWithProjection clip_image_processor: CLIPImageProcessor clip_tokenizer: CLIPTokenizer text_decoder: UniDiffuserTextDecoder text_tokenizer: GPT2Tokenizer unet: UniDiffuserModel scheduler: KarrasDiffusionSchedulers )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)） — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。这是UniDiffuser图像表示的一部分，与CLIP视觉编码一起。

+   `text_encoder` (`CLIPTextModel`) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `image_encoder` (`CLIPVisionModel`) — 一个[CLIPVisionModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)，用于将图像编码为其图像表示的一部分，以及VAE潜在表示。

+   `image_processor` (`CLIPImageProcessor`) — [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)，用于在使用`image_encoder`对图像进行编码之前预处理图像。

+   `clip_tokenizer` (`CLIPTokenizer`) — 一个[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，用于在使用`text_encoder`对提示进行编码之前对其进行标记化。

+   `text_decoder` (`UniDiffuserTextDecoder`) — 冻结的文本解码器。这是一个类似GPT的模型，用于从UniDiffuser嵌入生成文本。

+   `text_tokenizer` (`GPT2Tokenizer`) — 一个[GPT2Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Tokenizer)，用于解码文本以进行文本生成；与`text_decoder`一起使用。

+   `unet` (`UniDiffuserModel`) — 一个[U-ViT](https://github.com/baofff/U-ViT)模型，具有在变压器层之间的UNNet风格跳过连接，用于去噪编码图像潜在表示。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与 `unet` 结合使用以去噪编码图像和/或文本潜在空间的调度器。原始 UniDiffuser 论文使用 [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler) 调度器。

用于支持无条件文本和图像生成、文本条件图像生成、图像条件文本生成以及联合图像文本生成的双模态图像文本模型的流水线。

该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L1079)

```py
( prompt: Union = None image: Union = None height: Optional = None width: Optional = None data_type: Optional = 1 num_inference_steps: int = 50 guidance_scale: float = 8.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 num_prompts_per_image: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_latents: Optional = None vae_latents: Optional = None clip_latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 ) → export const metadata = 'undefined';ImageTextPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。文本条件图像生成（`text2img`）模式需要。

+   `image` (`torch.FloatTensor` or `PIL.Image.Image`, *optional*) — 代表图像批次的 `Image` 或张量。图像条件文本生成（`img2text`）模式需要。

+   `height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `data_type` (`int`, *optional*, defaults to 1) — 数据类型（0 或 1）。仅在加载支持数据类型嵌入的检查点时使用；这是为了与 [UniDiffuser-v1](https://huggingface.co/thu-ml/unidiffuser-v1) 检查点兼容而添加的。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, defaults to 8.0) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成中不包括的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用引导时被忽略（`guidance_scale < 1`）。用于文本条件图像生成（`text2img`）模式。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。用于 `text2img`（文本条件图像生成）和 `img` 模式。如果模式是联合的，并且同时提供了 `num_images_per_prompt` 和 `num_prompts_per_image`，则生成 `min(num_images_per_prompt, num_prompts_per_image)` 个样本。

+   `num_prompts_per_image` (`int`, *optional*, defaults to 1) — 每个图像生成的提示数量。用于 `img2text`（图像条件文本生成）和 `text` 模式。如果模式是联合的，并且同时提供了 `num_images_per_prompt` 和 `num_prompts_per_image`，则生成 `min(num_images_per_prompt, num_prompts_per_image)` 个样本。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作联合图像文本生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。假设提供了完整的VAE、CLIP和文本潜变量，如果提供，则覆盖`prompt_latents`、`vae_latents`和`clip_latents`的值。

+   `prompt_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作文本生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `vae_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `clip_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。用于文本条件图像生成（`text2img`）模式。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。用于文本条件图像生成（`text2img`）模式。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)而不是普通的tuple。

+   `callback` (`Callable`, *可选*) — 在推断过程中每`callback_steps`步调用一次的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调函数。

返回

[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是生成的文本列表。

用于生成的管道的调用函数。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L147)

```py
( )
```

禁用切片式VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L164)

```py
( )
```

禁用平铺式VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L139)

```py
( )
```

启用切片VAE解码。当启用此选项时，VAE将在几个步骤中将输入张量分割成片段以计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L155)

```py
( )
```

启用平铺VAE解码。当启用此选项时，VAE将将输入张量分成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像很有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L382)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str`或`List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由引导

+   `negative_prompt` (`str`或`List[str]`, *可选*) — 不用于引导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用引导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。

+   `lora_scale` (`float`, *可选*) — 将应用于文本编码器的所有LoRA层的LoRA比例，如果加载了LoRA层。 

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的层数。值为1意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `reset_mode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L268)

```py
( )
```

移除手动设置的模式；调用此方法后，管道将从输入中推断模式。

#### `set_image_mode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L252)

```py
( )
```

手动设置生成模式为无条件（“边际”）图像生成。

#### `set_image_to_text_mode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L260)

```py
( )
```

手动设置生成模式为图像条件的文本生成。

#### `set_joint_mode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L264)

```py
( )
```

手动设置生成模式为无条件联合图像文本生成。

#### `set_text_mode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L248)

```py
( )
```

手动设置生成模式为无条件（“边际”）文本生成。

#### `set_text_to_image_mode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L256)

```py
( )
```

手动设置生成模式为文本条件的图像生成。

## ImageTextPipelineOutput

### `class diffusers.ImageTextPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L33)

```py
( images: Union text: Union )
```

参数

+   `images` (`List[PIL.Image.Image]` or `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

+   `text` (`List[str]` or `List[List[str]]`) — 长度为 `batch_size` 的生成文本字符串列表或外部列表长度为 `batch_size` 的字符串列表的列表。

联合图像文本管道的输出类。
