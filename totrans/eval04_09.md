# 使用评估器

> 原始文本：[https://huggingface.co/docs/evaluate/base_evaluator](https://huggingface.co/docs/evaluate/base_evaluator)

`Evaluator`类允许评估模型、数据集和度量的三元组。包装在管道中的模型负责处理所有预处理和后处理，并且`Evaluator`支持支持任务的transformers管道，但可以传递自定义管道，如在[使用自定义管道与评估器](custom_evaluator)部分中展示的那样。

当前支持的任务有：

+   "text-classification"：将使用[TextClassificationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator)。

+   "token-classification"：将使用[TokenClassificationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TokenClassificationEvaluator)。

+   "question-answering"：将使用[QuestionAnsweringEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator)。

+   "image-classification"：将使用[ImageClassificationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator)。

+   "text-generation"：将使用[TextGenerationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TextGenerationEvaluator)。

+   "text2text-generation"：将使用[Text2TextGenerationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.Text2TextGenerationEvaluator)。

+   "summarization"：将使用[SummarizationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.SummarizationEvaluator)。

+   "translation"：将使用[TranslationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TranslationEvaluator)。

+   "automatic-speech-recognition"：将使用[AutomaticSpeechRecognitionEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.AutomaticSpeechRecognitionEvaluator)。

要在单个调用中运行具有多个任务的`Evaluator`，请使用[EvaluationSuite](evaluation_suite)，它在一组`SubTask`上运行评估。

每个任务都有自己的数据集格式和管道输出的要求，确保为您的自定义用例检查它们。让我们看看其中一些，并了解如何使用评估器同时评估单个或多个模型、数据集和度量。

## 文本分类

文本分类评估器可用于评估文本模型在分类数据集（如IMDb）上的性能。除了模型、数据和度量输入外，它还接受以下可选输入：

+   "input_column="text""：使用此参数可以指定管道数据的列。

+   "label_column="label""：使用此参数可以指定用于评估的标签列。

+   "label_mapping=None"：标签映射将使管道输出中的标签与评估所需的标签对齐。例如，标签列中的标签可以是整数（`0`/`1`），而管道可以生成标签名称，如`"positive"`/`"negative"`。通过该字典，管道输出被映射到标签。

默认情况下计算"accuracy"度量。

### 在Hub上评估模型

有几种方法可以将模型传递给评估器：您可以传递Hub上模型的名称，可以加载`transformers`模型并将其传递给评估器，也可以传递已初始化的`transformers.Pipeline`。或者，您可以传递任何行为类似于任何框架中任务的`pipeline`调用的可调用函数。

因此以下任何一种都可以：

```py
from datasets import load_dataset
from evaluate import evaluator
from transformers import AutoModelForSequenceClassification, pipeline

data = load_dataset("imdb", split="test").shuffle(seed=42).select(range(1000))
task_evaluator = evaluator("text-classification")

# 1\. Pass a model name or path
eval_results = task_evaluator.compute(
    model_or_pipeline="lvwerra/distilbert-imdb",
    data=data,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)

# 2\. Pass an instantiated model
model = AutoModelForSequenceClassification.from_pretrained("lvwerra/distilbert-imdb")

eval_results = task_evaluator.compute(
    model_or_pipeline=model,
    data=data,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)

# 3\. Pass an instantiated pipeline
pipe = pipeline("text-classification", model="lvwerra/distilbert-imdb")

eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)
print(eval_results)
```

如果不指定设备，默认情况下，模型推理将使用机器上的第一个GPU（如果有的话），否则使用CPU。如果要使用特定设备，可以将`device`传递给`compute`，其中-1将使用GPU，正整数（从0开始）将使用相关的CUDA设备。

结果如下所示：

```py
{
    'accuracy': 0.918,
    'latency_in_seconds': 0.013,
    'samples_per_second': 78.887,
    'total_time_in_seconds': 12.676
}
```

请注意，评估结果包括请求的指标以及通过管道获取预测所花费的时间的信息。

时间性能可以为推理速度提供有用的指示，但应该谨慎对待：它们包括管道中进行的所有处理。这可能包括标记化、后处理，这可能因模型而异。此外，它在您运行评估的硬件上有很大的依赖性，您可以通过优化诸如批处理大小之类的内容来提高性能。

### 评估多个指标

使用[combine()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.combine)函数，可以将几个指标捆绑到一个行为像单个指标的对象中。我们可以使用这个来一次性评估几个指标：

```py
import evaluate

eval_results = task_evaluator.compute(
    model_or_pipeline="lvwerra/distilbert-imdb",
    data=data,
    metric=evaluate.combine(["accuracy", "recall", "precision", "f1"]),
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)
print(eval_results)

```

结果如下所示：

```py
{
    'accuracy': 0.918,
    'f1': 0.916,
    'precision': 0.9147,
    'recall': 0.9187,
    'latency_in_seconds': 0.013,
    'samples_per_second': 78.887,
    'total_time_in_seconds': 12.676
}
```

接下来让我们看一下标记分类。

## 标记分类

使用标记分类评估器，可以评估诸如NER或POS标记等任务的模型。它具有以下特定参数：

+   `input_column="text"`: 使用此参数可以指定用于管道的数据列。

+   `label_column="label"`: 使用此参数可以指定用于评估的标签列。

+   `label_mapping=None`: 标签映射将管道输出中的标签与评估所需的标签对齐。例如，`label_column`中的标签可以是整数（`0`/`1`），而管道可以生成标签名称，如`"positive"`/`"negative"`。通过该字典，管道输出被映射到标签。

+   `join_by=" "`: 虽然大多数数据集已经被标记，但管道期望一个字符串。因此，在传递给管道之前，需要将标记连接起来。默认情况下，它们用空格连接。

让我们看看如何使用评估器来对比几个模型。

### 对多个模型进行基准测试

以下是一个示例，几行代码就可以比较几个模型，这要归功于`evaluator`，它抽象了预处理、推理、后处理、度量计算：

```py
import pandas as pd
from datasets import load_dataset
from evaluate import evaluator
from transformers import pipeline

models = [
    "xlm-roberta-large-finetuned-conll03-english",
    "dbmdz/bert-large-cased-finetuned-conll03-english",
    "elastic/distilbert-base-uncased-finetuned-conll03-english",
    "dbmdz/electra-large-discriminator-finetuned-conll03-english",
    "gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner",
    "philschmid/distilroberta-base-ner-conll2003",
    "Jorgeutd/albert-base-v2-finetuned-ner",
]

data = load_dataset("conll2003", split="validation").shuffle().select(1000)
task_evaluator = evaluator("token-classification")

results = []
for model in models:
    results.append(
        task_evaluator.compute(
            model_or_pipeline=model, data=data, metric="seqeval"
            )
        )

df = pd.DataFrame(results, index=models)
df[["overall_f1", "overall_accuracy", "total_time_in_seconds", "samples_per_second", "latency_in_seconds"]]
```

结果是一个看起来像这样的表格：

| model | overall_f1 | overall_accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds |
| :-- | --: | --: | --: | --: | --: |
| Jorgeutd/albert-base-v2-finetuned-ner | 0.941 | 0.989 | 4.515 | 221.468 | 0.005 |
| dbmdz/bert-large-cased-finetuned-conll03-english | 0.962 | 0.881 | 11.648 | 85.850 | 0.012 |
| dbmdz/electra-large-discriminator-finetuned-conll03-english | 0.965 | 0.881 | 11.456 | 87.292 | 0.011 |
| elastic/distilbert-base-uncased-finetuned-conll03-english | 0.940 | 0.989 | 2.318 | 431.378 | 0.002 |
| gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner | 0.947 | 0.991 | 2.376 | 420.873 | 0.002 |
| philschmid/distilroberta-base-ner-conll2003 | 0.961 | 0.994 | 2.436 | 410.579 | 0.002 |
| xlm-roberta-large-finetuned-conll03-english | 0.969 | 0.882 | 11.996 | 83.359 | 0.012 |

### 可视化结果

您可以将上面的`results`列表输入到`plot_radar()`函数中，以可视化它们的性能的不同方面，并选择最适合的模型，具体取决于与您的用例相关的指标。

```py
import evaluate
from evaluate.visualization import radar_plot

>>> plot = radar_plot(data=results, model_names=models, invert_range=["latency_in_seconds"])
>>> plot.show()
```

![](../Images/c3518aca067a8cd6a7da9de34fdc0dcb.png)

不要忘记为那些较小更好的指标（比如秒级的延迟）指定`invert_range`。

如果要将图保存在本地，可以使用`plot.savefig()`函数，并使用选项`bbox_inches='tight'`，以确保图像的任何部分都不会被裁剪掉。

## 问答

使用问答评估器，可以评估QA模型，而无需担心这些模型所需的复杂预处理和后处理。它具有以下特定参数：

+   `question_column="question"`：数据集中包含问题的列的名称

+   `context_column="context"`：包含上下文的列的名称

+   `id_column="id"`：包含问题和答案对的标识字段的列的名称

+   `label_column="answers"`：包含答案的列的名称

+   `squad_v2_format=None`：数据集是否遵循squad_v2数据集的格式，其中问题可能在上下文中没有答案。如果未提供此参数，则将自动推断格式。

让我们看看如何同时评估QA模型并计算置信区间。

### 置信区间

每个评估器都提供使用[bootstrapping](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html)计算置信区间的选项。只需传递`strategy="bootstrap"`并设置重新采样的次数为`n_resamples`。

```py
from datasets import load_dataset
from evaluate import evaluator

task_evaluator = evaluator("question-answering")

data = load_dataset("squad", split="validation[:1000]")
eval_results = task_evaluator.compute(
    model_or_pipeline="distilbert-base-uncased-distilled-squad",
    data=data,
    metric="squad",
    strategy="bootstrap",
    n_resamples=30
)
```

结果包括置信区间以及以下的误差估计：

```py
{
    'exact_match':
    {
        'confidence_interval': (79.67, 84.54),
        'score': 82.30,
        'standard_error': 1.28
    },
    'f1':
    {
        'confidence_interval': (85.30, 88.88),
        'score': 87.23,
        'standard_error': 0.97
    },
    'latency_in_seconds': 0.0085,
    'samples_per_second': 117.31,
    'total_time_in_seconds': 8.52
 }
```

## 图像分类

使用图像分类评估器，我们可以评估任何图像分类器。它使用与文本分类器相同的关键字参数：

+   `input_column="image"`：包含图像的列的名称为PIL ImageFile

+   `label_column="label"`：包含标签的列的名称

+   `label_mapping=None`：我们希望将流水线中模型定义的类标签映射到与`label_column`中定义的一致的值

让我们来看看如何在大型数据集上评估图像分类模型。

### 处理大型数据集

评估器可以用于大型数据集！下面，一个示例展示了如何在ImageNet-1k上进行图像分类评估。请注意，此示例将需要下载约150 GB。

```py
data = load_dataset("imagenet-1k", split="validation", use_auth_token=True)

pipe = pipeline(
    task="image-classification",
    model="facebook/deit-small-distilled-patch16-224"
)

task_evaluator = evaluator("image-classification")
eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="accuracy",
    label_mapping=pipe.model.config.label2id
)
```

由于我们使用`datasets`来存储数据，我们利用一种称为内存映射的技术。这意味着数据集永远不会完全加载到内存中，从而节省了大量RAM。运行上述代码仅使用大约1.5 GB的RAM，而验证拆分的大小超过30 GB。
