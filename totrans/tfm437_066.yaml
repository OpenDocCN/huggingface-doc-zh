- en: Performance and Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/performance](https://huggingface.co/docs/transformers/v4.37.2/en/performance)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/327.a6b1abfa.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Training large transformer models and deploying them to production present various
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model may require more GPU memory than available or exhibit
    slow training speed. In the deployment phase, the model can struggle to handle
    the required throughput in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: This documentation aims to assist you in overcoming these challenges and finding
    the optimal setting for your use-case. The guides are divided into training and
    inference sections, as each comes with different challenges and solutions. Within
    each section you’ll find separate guides for different hardware configurations,
    such as single GPU vs. multi-GPU for training or CPU vs. GPU for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Use this document as your starting point to navigate further to the methods
    that match your scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training large transformer models efficiently requires an accelerator such as
    a GPU or TPU. The most common case is where you have a single GPU. The methods
    that you can apply to improve training efficiency on a single GPU extend to other
    setups such as multiple GPU. However, there are also techniques that are specific
    to multi-GPU or CPU training. We cover them in separate sections.
  prefs: []
  type: TYPE_NORMAL
- en: '[Methods and tools for efficient training on a single GPU](perf_train_gpu_one):
    start here to learn common approaches that can help optimize GPU memory utilization,
    speed up the training, or both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-GPU training section](perf_train_gpu_many): explore this section to
    learn about further optimization methods that apply to a multi-GPU settings, such
    as data, tensor, and pipeline parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CPU training section](perf_train_cpu): learn about mixed precision training
    on CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Efficient Training on Multiple CPUs](perf_train_cpu_many): learn about distributed
    CPU training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Training on TPU with TensorFlow](perf_train_tpu_tf): if you are new to TPUs,
    refer to this section for an opinionated introduction to training on TPUs and
    using XLA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Custom hardware for training](perf_hardware): find tips and tricks when building
    your own deep learning rig.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperparameter Search using Trainer API](hpo_train)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Efficient inference with large models in a production environment can be as
    challenging as training them. In the following sections we go through the steps
    to run inference on CPU and single/multi-GPU setups.
  prefs: []
  type: TYPE_NORMAL
- en: '[Inference on a single CPU](perf_infer_cpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inference on a single GPU](perf_infer_gpu_one)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-GPU inference](perf_infer_gpu_one)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XLA Integration for TensorFlow Models](tf_xla)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here you’ll find techniques, tips and tricks that apply whether you are training
    a model, or running inference with it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Instantiating a big model](big_models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Troubleshooting performance issues](debugging)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contribute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This document is far from being complete and a lot more needs to be added, so
    if you have additions or corrections to make please don’t hesitate to open a PR
    or if you aren’t sure start an Issue and we can discuss the details there.
  prefs: []
  type: TYPE_NORMAL
- en: When making contributions that A is better than B, please try to include a reproducible
    benchmark and/or a link to the source of that information (unless it comes directly
    from you).
  prefs: []
  type: TYPE_NORMAL
