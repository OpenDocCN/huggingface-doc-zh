- en: tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers.js/api/tokenizers](https://huggingface.co/docs/transformers.js/api/tokenizers)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizers are used to prepare textual inputs for a model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** Create an `AutoTokenizer` and use it to tokenize a sentence. This
    will automatically detect the tokenizer type based on the tokenizer class defined
    in `tokenizer.json`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[tokenizers](#module_tokenizers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.TokenizerModel](#module_tokenizers.TokenizerModel) ⇐ `Callable`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new TokenizerModel(config)`](#new_module_tokenizers.TokenizerModel_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.vocab`](#module_tokenizers.TokenizerModel+vocab) : `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers.TokenizerModel+tokens_to_ids) : `Map.<string,
    number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fuse_unk`](#module_tokenizers.TokenizerModel+fuse_unk) : `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens)`](#module_tokenizers.TokenizerModel+_call) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers.TokenizerModel+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_ids(tokens)`](#module_tokenizers.TokenizerModel+convert_tokens_to_ids)
    ⇒ `Array.<number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_ids_to_tokens(ids)`](#module_tokenizers.TokenizerModel+convert_ids_to_tokens)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config, ...args)`](#module_tokenizers.TokenizerModel.fromConfig)
    ⇒ `TokenizerModel`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.PreTrainedTokenizer](#module_tokenizers.PreTrainedTokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.PreTrainedTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.added_tokens`](#module_tokenizers.PreTrainedTokenizer+added_tokens) : `Array.<AddedToken>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.remove_space`](#module_tokenizers.PreTrainedTokenizer+remove_space) : `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.padding_side`](#module_tokenizers.PreTrainedTokenizer+padding_side) : `’right’`
    | `’left’`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.getToken(...keys)`](#module_tokenizers.PreTrainedTokenizer+getToken) ⇒ `string`
    | `null`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(text, options)`](#module_tokenizers.PreTrainedTokenizer+_call) ⇒ `BatchEncoding`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._encode_text(text)`](#module_tokenizers.PreTrainedTokenizer+_encode_text)
    ⇒ `Array<string>` | `null`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(text, text_pair, options)`](#module_tokenizers.PreTrainedTokenizer+encode)
    ⇒ `Array.<number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.batch_decode(batch, decode_args)`](#module_tokenizers.PreTrainedTokenizer+batch_decode)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode(token_ids, [decode_args])`](#module_tokenizers.PreTrainedTokenizer+decode)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_single(token_ids, decode_args)`](#module_tokenizers.PreTrainedTokenizer+decode_single)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.apply_chat_template(conversation, options)`](#module_tokenizers.PreTrainedTokenizer+apply_chat_template)
    ⇒ `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.PreTrainedTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.BertTokenizer](#module_tokenizers.BertTokenizer) ⇐ `PreTrainedTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.AlbertTokenizer](#module_tokenizers.AlbertTokenizer) ⇐ `PreTrainedTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.NllbTokenizer](#module_tokenizers.NllbTokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)`](#module_tokenizers.NllbTokenizer+_build_translation_inputs)
    ⇒ `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.M2M100Tokenizer](#module_tokenizers.M2M100Tokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)`](#module_tokenizers.M2M100Tokenizer+_build_translation_inputs)
    ⇒ `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.WhisperTokenizer](#module_tokenizers.WhisperTokenizer) ⇐ `PreTrainedTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._decode_asr(sequences, options)`](#module_tokenizers.WhisperTokenizer+_decode_asr)
    ⇒ `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode()`](#module_tokenizers.WhisperTokenizer+decode) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.get_decoder_prompt_ids(options)`](#module_tokenizers.WhisperTokenizer+get_decoder_prompt_ids)
    ⇒ `Array.<Array<number>>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.MarianTokenizer](#module_tokenizers.MarianTokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MarianTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.MarianTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._encode_text(text)`](#module_tokenizers.MarianTokenizer+_encode_text) ⇒
    `Array`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.AutoTokenizer](#module_tokenizers.AutoTokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.AutoTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*inner*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~AddedToken](#module_tokenizers..AddedToken)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new AddedToken(config)`](#new_module_tokenizers..AddedToken_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~WordPieceTokenizer](#module_tokenizers..WordPieceTokenizer) ⇐ `TokenizerModel`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WordPieceTokenizer(config)`](#new_module_tokenizers..WordPieceTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..WordPieceTokenizer+tokens_to_ids) :
    `Map.<string, number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.unk_token_id`](#module_tokenizers..WordPieceTokenizer+unk_token_id) : `number`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.unk_token`](#module_tokenizers..WordPieceTokenizer+unk_token) : `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.max_input_chars_per_word`](#module_tokenizers..WordPieceTokenizer+max_input_chars_per_word)
    : `number`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.vocab`](#module_tokenizers..WordPieceTokenizer+vocab) : `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..WordPieceTokenizer+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~Unigram](#module_tokenizers..Unigram) ⇐ `TokenizerModel`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Unigram(config, moreConfig)`](#new_module_tokenizers..Unigram_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.populateNodes(lattice)`](#module_tokenizers..Unigram+populateNodes)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokenize(normalized)`](#module_tokenizers..Unigram+tokenize) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..Unigram+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~BPE](#module_tokenizers..BPE) ⇐ `TokenizerModel`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BPE(config)`](#new_module_tokenizers..BPE_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..BPE+tokens_to_ids) : `Map.<string, number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.cache`](#module_tokenizers..BPE+cache) : `Map.<string, Array<string>>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.bpe(token)`](#module_tokenizers..BPE+bpe) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..BPE+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~LegacyTokenizerModel](#module_tokenizers..LegacyTokenizerModel)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new LegacyTokenizerModel(config, moreConfig)`](#new_module_tokenizers..LegacyTokenizerModel_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..LegacyTokenizerModel+tokens_to_ids)
    : `Map.<string, number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[~Normalizer](#module_tokenizers..Normalizer)*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`new Normalizer(config)`](#new_module_tokenizers..Normalizer_new)*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[`.normalize(text)`](#module_tokenizers..Normalizer+normalize) ⇒ `string`**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`._call(text)`](#module_tokenizers..Normalizer+_call) ⇒ `string`*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`.fromConfig(config)`](#module_tokenizers..Normalizer.fromConfig) ⇒ `Normalizer`*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~Replace](#module_tokenizers..Replace) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..Replace+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~NFC](#module_tokenizers..NFC) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..NFC+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~NFKC](#module_tokenizers..NFKC) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..NFKC+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~NFKD](#module_tokenizers..NFKD) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..NFKD+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~StripNormalizer](#module_tokenizers..StripNormalizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..StripNormalizer+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~StripAccents](#module_tokenizers..StripAccents) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..StripAccents+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~Lowercase](#module_tokenizers..Lowercase) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..Lowercase+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~Prepend](#module_tokenizers..Prepend) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..Prepend+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~NormalizerSequence](#module_tokenizers..NormalizerSequence) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new NormalizerSequence(config)`](#new_module_tokenizers..NormalizerSequence_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..NormalizerSequence+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~BertNormalizer](#module_tokenizers..BertNormalizer) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._tokenize_chinese_chars(text)`](#module_tokenizers..BertNormalizer+_tokenize_chinese_chars)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._is_chinese_char(cp)`](#module_tokenizers..BertNormalizer+_is_chinese_char)
    ⇒ `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.stripAccents(text)`](#module_tokenizers..BertNormalizer+stripAccents) ⇒
    `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..BertNormalizer+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~PreTokenizer](#module_tokenizers..PreTokenizer) ⇐ `Callable`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(text, [options])`](#module_tokenizers..PreTokenizer+_call) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..PreTokenizer.fromConfig) ⇒ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~BertPreTokenizer](#module_tokenizers..BertPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BertPreTokenizer(config)`](#new_module_tokenizers..BertPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..BertPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~ByteLevelPreTokenizer](#module_tokenizers..ByteLevelPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ByteLevelPreTokenizer(config)`](#new_module_tokenizers..ByteLevelPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.add_prefix_space`](#module_tokenizers..ByteLevelPreTokenizer+add_prefix_space)
    : `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.trim_offsets`](#module_tokenizers..ByteLevelPreTokenizer+trim_offsets) :
    `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.use_regex`](#module_tokenizers..ByteLevelPreTokenizer+use_regex) : `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ByteLevelPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~SplitPreTokenizer](#module_tokenizers..SplitPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new SplitPreTokenizer(config)`](#new_module_tokenizers..SplitPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..SplitPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~PunctuationPreTokenizer](#module_tokenizers..PunctuationPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PunctuationPreTokenizer(config)`](#new_module_tokenizers..PunctuationPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PunctuationPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~DigitsPreTokenizer](#module_tokenizers..DigitsPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new DigitsPreTokenizer(config)`](#new_module_tokenizers..DigitsPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..DigitsPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~PostProcessor](#module_tokenizers..PostProcessor) ⇐ `Callable`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PostProcessor(config)`](#new_module_tokenizers..PostProcessor_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, ...args)`](#module_tokenizers..PostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens, ...args)`](#module_tokenizers..PostProcessor+_call) ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..PostProcessor.fromConfig) ⇒ `PostProcessor`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~BertProcessing](#module_tokenizers..BertProcessing)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BertProcessing(config)`](#new_module_tokenizers..BertProcessing_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..BertProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~TemplateProcessing](#module_tokenizers..TemplateProcessing) ⇐ `PostProcessor`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new TemplateProcessing(config)`](#new_module_tokenizers..TemplateProcessing_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..TemplateProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~ByteLevelPostProcessor](#module_tokenizers..ByteLevelPostProcessor) ⇐ `PostProcessor`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..ByteLevelPostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~Decoder](#module_tokenizers..Decoder) ⇐ `Callable`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Decoder(config)`](#new_module_tokenizers..Decoder_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.added_tokens`](#module_tokenizers..Decoder+added_tokens) : `Array.<AddedToken>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens)`](#module_tokenizers..Decoder+_call) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode(tokens)`](#module_tokenizers..Decoder+decode) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain(tokens)`](#module_tokenizers..Decoder+decode_chain) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..Decoder.fromConfig) ⇒ `Decoder`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~FuseDecoder](#module_tokenizers..FuseDecoder)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..FuseDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~WordPieceDecoder](#module_tokenizers..WordPieceDecoder) ⇐ `Decoder`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WordPieceDecoder(config)`](#new_module_tokenizers..WordPieceDecoder_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..WordPieceDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~ByteLevelDecoder](#module_tokenizers..ByteLevelDecoder) ⇐ `Decoder`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ByteLevelDecoder(config)`](#new_module_tokenizers..ByteLevelDecoder_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..ByteLevelDecoder+convert_tokens_to_string)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..ByteLevelDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~CTCDecoder](#module_tokenizers..CTCDecoder)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..CTCDecoder+convert_tokens_to_string)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..CTCDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~DecoderSequence](#module_tokenizers..DecoderSequence) ⇐ `Decoder`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new DecoderSequence(config)`](#new_module_tokenizers..DecoderSequence_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..DecoderSequence+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~MetaspacePreTokenizer](#module_tokenizers..MetaspacePreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MetaspacePreTokenizer(config)`](#new_module_tokenizers..MetaspacePreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..MetaspacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~MetaspaceDecoder](#module_tokenizers..MetaspaceDecoder) ⇐ `Decoder`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MetaspaceDecoder(config)`](#new_module_tokenizers..MetaspaceDecoder_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..MetaspaceDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~Precompiled](#module_tokenizers..Precompiled) ⇐ `Normalizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Precompiled(config)`](#new_module_tokenizers..Precompiled_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..Precompiled+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~PreTokenizerSequence](#module_tokenizers..PreTokenizerSequence) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PreTokenizerSequence(config)`](#new_module_tokenizers..PreTokenizerSequence_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizerSequence+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~WhitespacePreTokenizer](#module_tokenizers..WhitespacePreTokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WhitespacePreTokenizer(config)`](#new_module_tokenizers..WhitespacePreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~WhitespaceSplit](#module_tokenizers..WhitespaceSplit) ⇐ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WhitespaceSplit(config)`](#new_module_tokenizers..WhitespaceSplit_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespaceSplit+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[~ReplacePreTokenizer](#module_tokenizers..ReplacePreTokenizer)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ReplacePreTokenizer(config)`](#new_module_tokenizers..ReplacePreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ReplacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~BYTES_TO_UNICODE`](#module_tokenizers..BYTES_TO_UNICODE) ⇒ `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~loadTokenizer(pretrained_model_name_or_path, options)`](#module_tokenizers..loadTokenizer)
    ⇒ `Promise.<Array<any>>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~regexSplit(text, regex)`](#module_tokenizers..regexSplit) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~createPattern(pattern, invert)`](#module_tokenizers..createPattern) ⇒ `RegExp`
    | `null`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~objectToMap(obj)`](#module_tokenizers..objectToMap) ⇒ `Map.<string, any>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~prepareTensorForDecode(tensor)`](#module_tokenizers..prepareTensorForDecode)
    ⇒ `Array.<number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~clean_up_tokenization(text)`](#module_tokenizers..clean_up_tokenization)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~remove_accents(text)`](#module_tokenizers..remove_accents) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~lowercase_and_remove_accent(text)`](#module_tokenizers..lowercase_and_remove_accent)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~fuse(arr, value, mapping)`](#module_tokenizers..fuse)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~whitespace_split(text)`](#module_tokenizers..whitespace_split) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~PretrainedTokenizerOptions`](#module_tokenizers..PretrainedTokenizerOptions)
    : `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~BPENode`](#module_tokenizers..BPENode) : `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~SplitDelimiterBehavior`](#module_tokenizers..SplitDelimiterBehavior) : `’removed’`
    | `’isolated’` | `’mergedWithPrevious’` | `’mergedWithNext’` | `’contiguous’`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~PostProcessedOutput`](#module_tokenizers..PostProcessedOutput) : `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~EncodingSingle`](#module_tokenizers..EncodingSingle) : `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~BatchEncoding`](#module_tokenizers..BatchEncoding) : `Array<number>` | `Array<Array<number>>`
    | `Tensor`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`~Message`](#module_tokenizers..Message) : `Object`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.TokenizerModel ⇐ <code> Callable </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abstract base class for tokenizer models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[.TokenizerModel](#module_tokenizers.TokenizerModel) ⇐ `Callable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new TokenizerModel(config)`](#new_module_tokenizers.TokenizerModel_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.vocab`](#module_tokenizers.TokenizerModel+vocab) : `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers.TokenizerModel+tokens_to_ids) : `Map.<string,
    number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fuse_unk`](#module_tokenizers.TokenizerModel+fuse_unk) : `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens)`](#module_tokenizers.TokenizerModel+_call) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers.TokenizerModel+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_ids(tokens)`](#module_tokenizers.TokenizerModel+convert_tokens_to_ids)
    ⇒ `Array.<number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_ids_to_tokens(ids)`](#module_tokenizers.TokenizerModel+convert_ids_to_tokens)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config, ...args)`](#module_tokenizers.TokenizerModel.fromConfig)
    ⇒ `TokenizerModel`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new TokenizerModel(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of TokenizerModel.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the TokenizerModel. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizerModel.vocab : <code> Array. < string > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizerModel.tokens_to_ids : <code> Map. < string, number > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A mapping of tokens to ids.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizerModel.fuse_unk : <code> boolean </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether to fuse unknown tokens when encoding. Defaults to false.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizerModel._call(tokens) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Internal function to call the TokenizerModel instance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The encoded token IDs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizerModel.encode(tokens) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes a list of tokens into a list of token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The encoded tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: Will throw an error if not implemented in a subclass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizerModel.convert_tokens_to_ids(tokens) ⇒ <code> Array. < number > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converts a list of tokens into a list of token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<number>` - The converted token IDs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The tokens to convert. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizerModel.convert_ids_to_tokens(ids) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converts a list of token IDs into a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The converted tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ids | `Array.<number>` | The token IDs to convert. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: TokenizerModel.fromConfig(config, ...args) ⇒ <code> TokenizerModel </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instantiates a new TokenizerModel instance based on the configuration object
    provided.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `TokenizerModel` - A new instance of a TokenizerModel.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: Will throw an error if the TokenizerModel type in the config is not recognized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the TokenizerModel. |'
  prefs: []
  type: TYPE_TB
- en: '| ...args | `*` | Optional arguments to pass to the specific TokenizerModel
    constructor. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.PreTrainedTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.PreTrainedTokenizer](#module_tokenizers.PreTrainedTokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.PreTrainedTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.added_tokens`](#module_tokenizers.PreTrainedTokenizer+added_tokens) : `Array.<AddedToken>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.remove_space`](#module_tokenizers.PreTrainedTokenizer+remove_space) : `boolean`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.padding_side`](#module_tokenizers.PreTrainedTokenizer+padding_side) : `’right’`
    | `’left’`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.getToken(...keys)`](#module_tokenizers.PreTrainedTokenizer+getToken) ⇒ `string`
    | `null`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(text, options)`](#module_tokenizers.PreTrainedTokenizer+_call) ⇒ `BatchEncoding`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._encode_text(text)`](#module_tokenizers.PreTrainedTokenizer+_encode_text)
    ⇒ `Array<string>` | `null`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(text, text_pair, options)`](#module_tokenizers.PreTrainedTokenizer+encode)
    ⇒ `Array.<number>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.batch_decode(batch, decode_args)`](#module_tokenizers.PreTrainedTokenizer+batch_decode)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode(token_ids, [decode_args])`](#module_tokenizers.PreTrainedTokenizer+decode)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_single(token_ids, decode_args)`](#module_tokenizers.PreTrainedTokenizer+decode_single)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.apply_chat_template(conversation, options)`](#module_tokenizers.PreTrainedTokenizer+apply_chat_template)
    ⇒ `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.PreTrainedTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new PreTrainedTokenizer instance.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokenizerJSON | `Object` | The JSON of the tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '| tokenizerConfig | `Object` | The config of the tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'preTrainedTokenizer.added_tokens : <code> Array. < AddedToken > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'preTrainedTokenizer.remove_space : <code> boolean </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether or not to strip the text when tokenizing (removing excess spaces before
    and after the string).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'preTrainedTokenizer.padding_side : <code> ’ right ’ </code> | <code> ’ left
    ’ </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer.getToken(...keys) ⇒ <code> string </code> | <code> null
    </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Returns the value of the first matching key in the tokenizer config object.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` | `null` - The value associated with the first matching
    key, or null if no match is found.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If an object is found for a matching key and its __type property is
    not "AddedToken".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ...keys | `string` | One or more keys to search for in the tokenizer config
    object. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer._call(text, options) ⇒ <code> BatchEncoding </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encode/tokenize the given text(s).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `BatchEncoding` - Object to be passed to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `Array<string>` |  | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| options | `Object` |  | An optional object containing the following properties:
    |'
  prefs: []
  type: TYPE_TB
- en: '| [options.text_pair] | `string` &#124; `Array<string>` | `null` | Optional
    second sequence to be encoded. If set, must be the same type as text. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.padding] | `boolean` &#124; `''max_length''` | `false` | Whether
    to pad the input sequences. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.add_special_tokens] | `boolean` | `true` | Whether or not to add
    the special tokens associated with the corresponding model. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.truncation] | `boolean` |  | Whether to truncate the input sequences.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [options.max_length] | `number` |  | Maximum length of the returned list
    and optionally padding length. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.return_tensor] | `boolean` | `true` | Whether to return the results
    as Tensors or arrays. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer._encode_text(text) ⇒ <code> Array < string > </code> | <code>
    null </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes a single text using the preprocessor pipeline of the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array<string>` | `null` - The encoded tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `null` | The text to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer.encode(text, text_pair, options) ⇒ <code> Array. < number
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes a single text or a pair of texts using the model’s tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<number>` - An array of token IDs representing the encoded
    text(s).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` |  | The text to encode. |'
  prefs: []
  type: TYPE_TB
- en: '| text_pair | `string` &#124; `null` | `null` | The optional second text to
    encode. |'
  prefs: []
  type: TYPE_TB
- en: '| options | `Object` |  | An optional object containing the following properties:
    |'
  prefs: []
  type: TYPE_TB
- en: '| [options.add_special_tokens] | `boolean` | `true` | Whether or not to add
    the special tokens associated with the corresponding model. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer.batch_decode(batch, decode_args) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decode a batch of tokenized sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - List of decoded sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| batch | `Array<Array<number>>` &#124; `Tensor` | List/Tensor of tokenized
    input sequences. |'
  prefs: []
  type: TYPE_TB
- en: '| decode_args | `Object` | (Optional) Object with decoding arguments. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer.decode(token_ids, [decode_args]) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decodes a sequence of token IDs back to a string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If `token_ids` is not a non-empty array of integers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| token_ids | `Array<number>` &#124; `Tensor` |  | List/Tensor of token IDs
    to decode. |'
  prefs: []
  type: TYPE_TB
- en: '| [decode_args] | `Object` | `{}` |  |'
  prefs: []
  type: TYPE_TB
- en: '| [decode_args.skip_special_tokens] | `boolean` | `false` | If true, special
    tokens are removed from the output string. |'
  prefs: []
  type: TYPE_TB
- en: '| [decode_args.clean_up_tokenization_spaces] | `boolean` | `true` | If true,
    spaces before punctuations and abbreviated forms are removed. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer.decode_single(token_ids, decode_args) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decode a single list of token ids to a string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| token_ids | `Array.<number>` |  | List of token ids to decode |'
  prefs: []
  type: TYPE_TB
- en: '| decode_args | `Object` |  | Optional arguments for decoding |'
  prefs: []
  type: TYPE_TB
- en: '| [decode_args.skip_special_tokens] | `boolean` | `false` | Whether to skip
    special tokens during decoding |'
  prefs: []
  type: TYPE_TB
- en: '| [decode_args.clean_up_tokenization_spaces] | `boolean` |  | Whether to clean
    up tokenization spaces during decoding. If null, the value is set to `this.decoder.cleanup`
    if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists,
    falling back to `true`. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTrainedTokenizer.apply_chat_template(conversation, options) ⇒ <code> string
    </code> | <code> Tensor </code> | <code> Array < number > </code> | <code> Array
    < Array < number > > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converts a list of message objects with `"role"` and `"content"` keys to a list
    of token ids. This method is intended for use with chat models, and will read
    the tokenizer’s chat_template attribute to determine the format and control tokens
    to use when converting. When chat_template is None, it will fall back to the default_chat_template
    specified at the class level.
  prefs: []
  type: TYPE_NORMAL
- en: See [here](https://huggingface.co/docs/transformers/chat_templating) for more
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** Applying a chat template to a conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`
    - The tokenized output.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| conversation | `Array.<Message>` |  | A list of message objects with `"role"`
    and `"content"` keys. |'
  prefs: []
  type: TYPE_TB
- en: '| options | `Object` |  | An optional object containing the following properties:
    |'
  prefs: []
  type: TYPE_TB
- en: '| [options.chat_template] | `string` | `null` | A Jinja template to use for
    this conversion. If this is not passed, the model''s default chat template will
    be used instead. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.add_generation_prompt] | `boolean` | `false` | Whether to end the
    prompt with the token(s) that indicate the start of an assistant message. This
    is useful when you want to generate a response from the model. Note that this
    argument will be passed to the chat template, and so it must be supported in the
    template for this argument to have any effect. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.tokenize] | `boolean` | `true` | Whether to tokenize the output.
    If false, the output will be a string. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.padding] | `boolean` | `false` | Whether to pad sequences to the
    maximum length. Has no effect if tokenize is false. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.truncation] | `boolean` | `false` | Whether to truncate sequences
    to the maximum length. Has no effect if tokenize is false. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.max_length] | `number` |  | Maximum length (in tokens) to use for
    padding or truncation. Has no effect if tokenize is false. If not specified, the
    tokenizer''s `max_length` attribute will be used as a default. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.return_tensor] | `boolean` | `true` | Whether to return the output
    as a Tensor or an Array. Has no effect if tokenize is false. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: PreTrainedTokenizer.from_pretrained(pretrained_model_name_or_path, options)
    ⇒ <code> Promise. < PreTrainedTokenizer > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Promise.<PreTrainedTokenizer>` - A new instance of the `PreTrainedTokenizer`
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` Throws an error if the tokenizer.json or tokenizer_config.json files
    are not found in the `pretrained_model_name_or_path`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pretrained_model_name_or_path | `string` | The path to the pre-trained tokenizer.
    |'
  prefs: []
  type: TYPE_TB
- en: '| options | `PretrainedTokenizerOptions` | Additional options for loading the
    tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.BertTokenizer ⇐ <code> PreTrainedTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BertTokenizer is a class used to tokenize text for BERT models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTrainedTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.AlbertTokenizer ⇐ <code> PreTrainedTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Albert tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTrainedTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.NllbTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NllbTokenizer class is used to tokenize text for NLLB (“No Language Left
    Behind”) models.
  prefs: []
  type: TYPE_NORMAL
- en: No Language Left Behind (NLLB) is a first-of-its-kind, AI breakthrough project
    that open-sources models capable of delivering high-quality translations directly
    between any pair of 200+ languages — including low-resource languages like Asturian,
    Luganda, Urdu and more. It aims to help people communicate with anyone, anywhere,
    regardless of their language preferences. For more information, check out their
    [paper](https://arxiv.org/abs/2207.04672).
  prefs: []
  type: TYPE_NORMAL
- en: For a list of supported languages (along with their language codes),
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**See**: [https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: nllbTokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)
    ⇒ <code> Object </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Helper function to build translation inputs for an `NllbTokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NllbTokenizer`](#module_tokenizers.NllbTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Object` - Object to be passed to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| raw_inputs | `string` &#124; `Array<string>` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| tokenizer_options | `Object` | Options to be sent to the tokenizer |'
  prefs: []
  type: TYPE_TB
- en: '| generate_kwargs | `Object` | Generation options. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.M2M100Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The M2M100Tokenizer class is used to tokenize text for M2M100 (“Many-to-Many”)
    models.
  prefs: []
  type: TYPE_NORMAL
- en: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many
    multilingual translation. It was introduced in this [paper](https://arxiv.org/abs/2010.11125)
    and first released in [this](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100)
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: For a list of supported languages (along with their language codes),
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**See**: [https://huggingface.co/facebook/m2m100_418M#languages-covered](https://huggingface.co/facebook/m2m100_418M#languages-covered)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: m2M100Tokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)
    ⇒ <code> Object </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Helper function to build translation inputs for an `M2M100Tokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`M2M100Tokenizer`](#module_tokenizers.M2M100Tokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Object` - Object to be passed to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| raw_inputs | `string` &#124; `Array<string>` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| tokenizer_options | `Object` | Options to be sent to the tokenizer |'
  prefs: []
  type: TYPE_TB
- en: '| generate_kwargs | `Object` | Generation options. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.WhisperTokenizer ⇐ <code> PreTrainedTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WhisperTokenizer tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTrainedTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[.WhisperTokenizer](#module_tokenizers.WhisperTokenizer) ⇐ `PreTrainedTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._decode_asr(sequences, options)`](#module_tokenizers.WhisperTokenizer+_decode_asr)
    ⇒ `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode()`](#module_tokenizers.WhisperTokenizer+decode) : `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.get_decoder_prompt_ids(options)`](#module_tokenizers.WhisperTokenizer+get_decoder_prompt_ids)
    ⇒ `Array.<Array<number>>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: whisperTokenizer._decode_asr(sequences, options) ⇒ <code> * </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decodes automatic speech recognition (ASR) sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `*` - The decoded sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| sequences | `*` | The sequences to decode. |'
  prefs: []
  type: TYPE_TB
- en: '| options | `Object` | The options to use for decoding. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'whisperTokenizer.decode() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: whisperTokenizer.get_decoder_prompt_ids(options) ⇒ <code> Array. < Array < number
    > > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Helper function to build translation inputs for a `WhisperTokenizer`, depending
    on the language, task, and whether to predict timestamp tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Used to override the prefix tokens appended to the start of the label sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example: Get ids for a language**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Kind**: instance method of [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<Array<number>>` - The decoder prompt ids.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| options | `Object` | Options to generate the decoder prompt. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.language] | `string` | The language of the transcription text. The
    corresponding language id token is appended to the start of the sequence for multilingual
    speech recognition and speech translation tasks, e.g. for "Spanish" the token
    "<&#124;es&#124;>" is appended to the start of sequence. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.task] | `string` | Task identifier to append at the start of sequence
    (if any). This should be used for mulitlingual fine-tuning, with "transcribe"
    for speech recognition and "translate" for speech translation. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.no_timestamps] | `boolean` | Whether to add the <&#124;notimestamps&#124;>
    token at the start of the sequence. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.MarianTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Todo**'
  prefs: []
  type: TYPE_NORMAL
- en: This model is not yet supported by Hugging Face’s “fast” tokenizers library
    ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).
    Therefore, this implementation (which is based on fast tokenizers) may produce
    slightly inaccurate results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[.MarianTokenizer](#module_tokenizers.MarianTokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MarianTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.MarianTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._encode_text(text)`](#module_tokenizers.MarianTokenizer+_encode_text) ⇒
    `Array`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new MarianTokenizer(tokenizerJSON, tokenizerConfig)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new MarianTokenizer instance.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokenizerJSON | `Object` | The JSON of the tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '| tokenizerConfig | `Object` | The config of the tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: marianTokenizer._encode_text(text) ⇒ <code> Array </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes a single text. Overriding this method is necessary since the language
    codes must be removed before encoding with sentencepiece model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`MarianTokenizer`](#module_tokenizers.MarianTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array` - The encoded tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '**See**: [https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213](https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `null` | The text to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers.AutoTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper class which is used to instantiate pretrained tokenizers with the `from_pretrained`
    function. The chosen tokenizer class is determined by the type specified in the
    tokenizer config.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: AutoTokenizer.from_pretrained(pretrained_model_name_or_path, options) ⇒ <code>
    Promise. < PreTrainedTokenizer > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instantiate one of the tokenizer classes of the library from a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer class to instantiate is selected based on the `tokenizer_class`
    property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path`
    if possible)
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`AutoTokenizer`](#module_tokenizers.AutoTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Promise.<PreTrainedTokenizer>` - A new instance of the PreTrainedTokenizer
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pretrained_model_name_or_path | `string` | The name or path of the pretrained
    model. Can be either:'
  prefs: []
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained tokenizer hosted inside a model repo
    on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| options | `PretrainedTokenizerOptions` | Additional options for loading the
    tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~AddedToken
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Represent a token added by the user on top of the existing Model vocabulary.
    AddedToken can be configured to specify the behavior they should have in various
    situations like:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether they should only match single words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to include any whitespace on its left or right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new AddedToken(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of AddedToken.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` |  | Added token configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.content | `string` |  | The content of the added token. |'
  prefs: []
  type: TYPE_TB
- en: '| config.id | `number` |  | The id of the added token. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.single_word] | `boolean` | `false` | Whether this token must be a
    single word or can break words. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.lstrip] | `boolean` | `false` | Whether this token should strip whitespaces
    on its left. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.rstrip] | `boolean` | `false` | Whether this token should strip whitespaces
    on its right. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.normalized] | `boolean` | `false` | Whether this token should be
    normalized. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.special] | `boolean` | `false` | Whether this token is special. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WordPieceTokenizer ⇐ <code> TokenizerModel </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A subclass of TokenizerModel that uses WordPiece encoding to encode tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `TokenizerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~WordPieceTokenizer](#module_tokenizers..WordPieceTokenizer) ⇐ `TokenizerModel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WordPieceTokenizer(config)`](#new_module_tokenizers..WordPieceTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..WordPieceTokenizer+tokens_to_ids) :
    `Map.<string, number>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.unk_token_id`](#module_tokenizers..WordPieceTokenizer+unk_token_id) : `number`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.unk_token`](#module_tokenizers..WordPieceTokenizer+unk_token) : `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.max_input_chars_per_word`](#module_tokenizers..WordPieceTokenizer+max_input_chars_per_word)
    : `number`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.vocab`](#module_tokenizers..WordPieceTokenizer+vocab) : `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..WordPieceTokenizer+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new WordPieceTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` |  | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.vocab | `Object` |  | A mapping of tokens to ids. |'
  prefs: []
  type: TYPE_TB
- en: '| config.unk_token | `string` |  | The unknown token string. |'
  prefs: []
  type: TYPE_TB
- en: '| config.continuing_subword_prefix | `string` |  | The prefix to use for continuing
    subwords. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.max_input_chars_per_word] | `number` | `100` | The maximum number
    of characters per word. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.tokens_to_ids : <code> Map. < string, number > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A mapping of tokens to ids.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.unk_token_id : <code> number </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The id of the unknown token.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.unk_token : <code> string </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The unknown token string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.max_input_chars_per_word : <code> number </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The maximum number of characters allowed per word.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.vocab : <code> Array. < string > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An array of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: wordPieceTokenizer.encode(tokens) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes an array of tokens using WordPiece encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of encoded tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Unigram ⇐ <code> TokenizerModel </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Class representing a Unigram tokenizer model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `TokenizerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~Unigram](#module_tokenizers..Unigram) ⇐ `TokenizerModel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Unigram(config, moreConfig)`](#new_module_tokenizers..Unigram_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.populateNodes(lattice)`](#module_tokenizers..Unigram+populateNodes)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokenize(normalized)`](#module_tokenizers..Unigram+tokenize) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..Unigram+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new Unigram(config, moreConfig)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new Unigram tokenizer model.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the Unigram model. |'
  prefs: []
  type: TYPE_TB
- en: '| config.unk_id | `number` | The ID of the unknown token |'
  prefs: []
  type: TYPE_TB
- en: '| config.vocab | `Array.<Array<any>>` | A 2D array representing a mapping of
    tokens to scores. |'
  prefs: []
  type: TYPE_TB
- en: '| moreConfig | `Object` | Additional configuration object for the Unigram model.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: unigram.populateNodes(lattice)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Populates lattice nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Unigram`](#module_tokenizers..Unigram)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| lattice | `TokenLattice` | The token lattice to populate with nodes. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: unigram.tokenize(normalized) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes an array of tokens into an array of subtokens using the unigram model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Unigram`](#module_tokenizers..Unigram)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of subtokens obtained by encoding
    the input tokens using the unigram model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| normalized | `string` | The normalized string. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: unigram.encode(tokens) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes an array of tokens using Unigram encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Unigram`](#module_tokenizers..Unigram)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of encoded tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BPE ⇐ <code> TokenizerModel </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BPE class for encoding text into Byte-Pair-Encoding (BPE) tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `TokenizerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~BPE](#module_tokenizers..BPE) ⇐ `TokenizerModel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BPE(config)`](#new_module_tokenizers..BPE_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..BPE+tokens_to_ids) : `Map.<string, number>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.cache`](#module_tokenizers..BPE+cache) : `Map.<string, Array<string>>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.bpe(token)`](#module_tokenizers..BPE+bpe) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..BPE+encode) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new BPE(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a BPE instance.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for BPE. |'
  prefs: []
  type: TYPE_TB
- en: '| config.vocab | `Object` | A mapping of tokens to ids. |'
  prefs: []
  type: TYPE_TB
- en: '| config.unk_token | `string` | The unknown token used for out of vocabulary
    words. |'
  prefs: []
  type: TYPE_TB
- en: '| config.end_of_word_suffix | `string` | The suffix to place at the end of
    each word. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.continuing_subword_suffix] | `string` | The suffix to insert between
    words. |'
  prefs: []
  type: TYPE_TB
- en: '| config.merges | `Array` | An array of BPE merges as strings. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'bpE.tokens_to_ids : <code> Map. < string, number > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`BPE`](#module_tokenizers..BPE)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'bpE.cache : <code> Map. < string, Array < string > > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`BPE`](#module_tokenizers..BPE)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bpE.bpe(token) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apply Byte-Pair-Encoding (BPE) to a given token. Efficient heap-based priority
    queue implementation adapted from [https://github.com/belladoreai/llama-tokenizer-js](https://github.com/belladoreai/llama-tokenizer-js).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BPE`](#module_tokenizers..BPE)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The BPE encoded tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| token | `string` | The token to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bpE.encode(tokens) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encodes the input sequence of tokens using the BPE algorithm and returns the
    resulting subword tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BPE`](#module_tokenizers..BPE)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The resulting subword tokens after applying
    the BPE algorithm to the input sequence of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The input sequence of tokens to encode. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~LegacyTokenizerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Legacy tokenizer class for tokenizers with only a vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[~LegacyTokenizerModel](#module_tokenizers..LegacyTokenizerModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new LegacyTokenizerModel(config, moreConfig)`](#new_module_tokenizers..LegacyTokenizerModel_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..LegacyTokenizerModel+tokens_to_ids)
    : `Map.<string, number>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new LegacyTokenizerModel(config, moreConfig)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a LegacyTokenizerModel instance.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for LegacyTokenizerModel. |'
  prefs: []
  type: TYPE_TB
- en: '| config.vocab | `Object` | A (possibly nested) mapping of tokens to ids. |'
  prefs: []
  type: TYPE_TB
- en: '| moreConfig | `Object` | Additional configuration object for the LegacyTokenizerModel
    model. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'legacyTokenizerModel.tokens_to_ids : <code> Map. < string, number > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`LegacyTokenizerModel`](#module_tokenizers..LegacyTokenizerModel)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Normalizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A base class for text normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner abstract class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '*[~Normalizer](#module_tokenizers..Normalizer)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`new Normalizer(config)`](#new_module_tokenizers..Normalizer_new)*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[`.normalize(text)`](#module_tokenizers..Normalizer+normalize) ⇒ `string`**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`._call(text)`](#module_tokenizers..Normalizer+_call) ⇒ `string`*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`.fromConfig(config)`](#module_tokenizers..Normalizer.fromConfig) ⇒ `Normalizer`*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new Normalizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the normalizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: normalizer.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance abstract method of [`Normalizer`](#module_tokenizers..Normalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If this method is not implemented in a subclass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: normalizer._call(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alias for [Normalizer#normalize](Normalizer#normalize).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Normalizer`](#module_tokenizers..Normalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizer.fromConfig(config) ⇒ <code> Normalizer </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factory method for creating normalizers from config objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`Normalizer`](#module_tokenizers..Normalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Normalizer` - A Normalizer object.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If an unknown Normalizer type is specified in the config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the normalizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Replace ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Replace normalizer that replaces occurrences of a pattern with a given string
    or regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: replace.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize the input text by replacing the pattern with the content.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Replace`](#module_tokenizers..Replace)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text after replacing the pattern with
    the content.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text to be normalized. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NFC ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A normalizer that applies Unicode normalization form C (NFC) to the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: nfC.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize the input text by applying Unicode normalization form C (NFC).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NFC`](#module_tokenizers..NFC)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text to be normalized. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NFKC ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NFKC Normalizer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: nfkC.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize text using NFKC normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NFKC`](#module_tokenizers..NFKC)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be normalized. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NFKD ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NFKD Normalizer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: nfkD.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize text using NFKD normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NFKD`](#module_tokenizers..NFKD)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be normalized. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~StripNormalizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A normalizer that strips leading and/or trailing whitespace from the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: stripNormalizer.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Strip leading and/or trailing whitespace from the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`StripNormalizer`](#module_tokenizers..StripNormalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~StripAccents ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: StripAccents normalizer removes all accents from the text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: stripAccents.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remove all accents from the text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`StripAccents`](#module_tokenizers..StripAccents)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text without accents.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Lowercase ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Normalizer that lowercases the input string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: lowercase.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lowercases the input string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Lowercase`](#module_tokenizers..Lowercase)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Prepend ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Normalizer that prepends a string to the input string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: prepend.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prepends the input string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Prepend`](#module_tokenizers..Prepend)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NormalizerSequence ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Normalizer that applies a sequence of Normalizers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~NormalizerSequence](#module_tokenizers..NormalizerSequence) ⇐ `Normalizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new NormalizerSequence(config)`](#new_module_tokenizers..NormalizerSequence_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..NormalizerSequence+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new NormalizerSequence(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new instance of NormalizerSequence.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.normalizers | `Array.<Object>` | An array of Normalizer configuration
    objects. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: normalizerSequence.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apply a sequence of Normalizers to the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NormalizerSequence`](#module_tokenizers..NormalizerSequence)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BertNormalizer ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A class representing a normalizer used in BERT tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~BertNormalizer](#module_tokenizers..BertNormalizer) ⇐ `Normalizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._tokenize_chinese_chars(text)`](#module_tokenizers..BertNormalizer+_tokenize_chinese_chars)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._is_chinese_char(cp)`](#module_tokenizers..BertNormalizer+_is_chinese_char)
    ⇒ `boolean`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.stripAccents(text)`](#module_tokenizers..BertNormalizer+stripAccents) ⇒
    `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..BertNormalizer+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer._tokenize_chinese_chars(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the
    input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The tokenized text with whitespace added around CJK
    characters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer._is_chinese_char(cp) ⇒ <code> boolean </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Checks whether the given Unicode codepoint represents a CJK (Chinese, Japanese,
    or Korean) character.
  prefs: []
  type: TYPE_NORMAL
- en: 'A “chinese character” is defined as anything in the CJK Unicode block: [https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)](https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block))'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the CJK Unicode block is NOT all Japanese and Korean characters, despite
    its name. The modern Korean Hangul alphabet is a different block, as is Japanese
    Hiragana and Katakana. Those alphabets are used to write space-separated words,
    so they are not treated specially and are handled like all other languages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `boolean` - True if the codepoint represents a CJK character,
    false otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| cp | `number` | The Unicode codepoint to check. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer.stripAccents(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Strips accents from the given text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The text with accents removed.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to strip accents from. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalizes the given text based on the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PreTokenizer ⇐ <code> Callable </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A callable class representing a pre-tokenizer used in tokenization. Subclasses
    should implement the `pre_tokenize_text` method to define the specific pre-tokenization
    logic.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~PreTokenizer](#module_tokenizers..PreTokenizer) ⇐ `Callable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(text, [options])`](#module_tokenizers..PreTokenizer+_call) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..PreTokenizer.fromConfig) ⇒ `PreTokenizer`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Method that should be implemented by subclasses to define the specific pre-tokenization
    logic.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance abstract method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The pre-tokenized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the method is not implemented in the subclass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to pre-tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizer.pre_tokenize(text, [options]) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes the given text into pre-tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of pre-tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `Array<string>` | The text or array of texts to pre-tokenize.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizer._call(text, [options]) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alias for [PreTokenizer#pre_tokenize](PreTokenizer#pre_tokenize).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of pre-tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `Array<string>` | The text or array of texts to pre-tokenize.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: PreTokenizer.fromConfig(config) ⇒ <code> PreTokenizer </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factory method that returns an instance of a subclass of `PreTokenizer` based
    on the provided configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PreTokenizer` - An instance of a subclass of `PreTokenizer`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the provided configuration object does not correspond to any known
    pre-tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | A configuration object for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BertPreTokenizer ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~BertPreTokenizer](#module_tokenizers..BertPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BertPreTokenizer(config)`](#new_module_tokenizers..BertPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..BertPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new BertPreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A PreTokenizer that splits text into wordpieces using a basic tokenization scheme
    similar to that used in the original implementation of BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bertPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes a single text using the BERT pre-tokenization scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertPreTokenizer`](#module_tokenizers..BertPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ByteLevelPreTokenizer ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pre-tokenizer that splits text into Byte-Pair-Encoding (BPE) subwords.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~ByteLevelPreTokenizer](#module_tokenizers..ByteLevelPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ByteLevelPreTokenizer(config)`](#new_module_tokenizers..ByteLevelPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.add_prefix_space`](#module_tokenizers..ByteLevelPreTokenizer+add_prefix_space)
    : `boolean`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.trim_offsets`](#module_tokenizers..ByteLevelPreTokenizer+trim_offsets) :
    `boolean`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.use_regex`](#module_tokenizers..ByteLevelPreTokenizer+use_regex) : `boolean`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ByteLevelPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new ByteLevelPreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of the `ByteLevelPreTokenizer` class.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelPreTokenizer.add_prefix_space : <code> boolean </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether to add a leading space to the first word.This allows to treat the leading
    word just as any other word.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelPreTokenizer.trim_offsets : <code> boolean </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether the post processing step should trim offsetsto avoid including whitespaces.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Todo**'
  prefs: []
  type: TYPE_NORMAL
- en: Use this in the pretokenization step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelPreTokenizer.use_regex : <code> boolean </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether to use the standard GPT2 regex for whitespace splitting.Set it to False
    if you want to use your own splitting. Defaults to true.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: byteLevelPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes a single piece of text using byte-level tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~SplitPreTokenizer ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text using a given pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~SplitPreTokenizer](#module_tokenizers..SplitPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new SplitPreTokenizer(config)`](#new_module_tokenizers..SplitPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..SplitPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new SplitPreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '| config.pattern | `Object` | The pattern used to split the text. Can be a
    string or a regex object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.pattern.String | `string` &#124; `undefined` | The string to use for
    splitting. Only defined if the pattern is a string. |'
  prefs: []
  type: TYPE_TB
- en: '| config.pattern.Regex | `string` &#124; `undefined` | The regex to use for
    splitting. Only defined if the pattern is a regex. |'
  prefs: []
  type: TYPE_TB
- en: '| config.behavior | `SplitDelimiterBehavior` | The behavior to use when splitting.
    |'
  prefs: []
  type: TYPE_TB
- en: '| config.invert | `boolean` | Whether to split (invert=false) or match (invert=true)
    the pattern. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: splitPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes text by splitting it using the given pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`SplitPreTokenizer`](#module_tokenizers..SplitPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PunctuationPreTokenizer ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text based on punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~PunctuationPreTokenizer](#module_tokenizers..PunctuationPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PunctuationPreTokenizer(config)`](#new_module_tokenizers..PunctuationPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PunctuationPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new PunctuationPreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '| config.behavior | `SplitDelimiterBehavior` | The behavior to use when splitting.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: punctuationPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. <
    string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes text by splitting it using the given pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PunctuationPreTokenizer`](#module_tokenizers..PunctuationPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~DigitsPreTokenizer ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text based on digits.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~DigitsPreTokenizer](#module_tokenizers..DigitsPreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new DigitsPreTokenizer(config)`](#new_module_tokenizers..DigitsPreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..DigitsPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new DigitsPreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '| config.individual_digits | `boolean` | Whether to split on individual digits.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: digitsPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes text by splitting it using the given pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`DigitsPreTokenizer`](#module_tokenizers..DigitsPreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PostProcessor ⇐ <code> Callable </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~PostProcessor](#module_tokenizers..PostProcessor) ⇐ `Callable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PostProcessor(config)`](#new_module_tokenizers..PostProcessor_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, ...args)`](#module_tokenizers..PostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens, ...args)`](#module_tokenizers..PostProcessor+_call) ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..PostProcessor.fromConfig) ⇒ `PostProcessor`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new PostProcessor(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration for the post-processor. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: postProcessor.post_process(tokens, ...args) ⇒ <code> PostProcessedOutput </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Method to be implemented in subclass to apply post-processing on the given tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PostProcessor`](#module_tokenizers..PostProcessor)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - The post-processed tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the method is not implemented in subclass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array` | The input tokens to be post-processed. |'
  prefs: []
  type: TYPE_TB
- en: '| ...args | `*` | Additional arguments required by the post-processing logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: postProcessor._call(tokens, ...args) ⇒ <code> PostProcessedOutput </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alias for [PostProcessor#post_process](PostProcessor#post_process).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PostProcessor`](#module_tokenizers..PostProcessor)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - The post-processed tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array` | The text or array of texts to post-process. |'
  prefs: []
  type: TYPE_TB
- en: '| ...args | `*` | Additional arguments required by the post-processing logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: PostProcessor.fromConfig(config) ⇒ <code> PostProcessor </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factory method to create a PostProcessor object from a configuration object.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`PostProcessor`](#module_tokenizers..PostProcessor)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessor` - A PostProcessor object created from the given
    configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If an unknown PostProcessor type is encountered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | Configuration object representing a PostProcessor. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BertProcessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A post-processor that adds special tokens to the beginning and end of the input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[~BertProcessing](#module_tokenizers..BertProcessing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BertProcessing(config)`](#new_module_tokenizers..BertProcessing_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..BertProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new BertProcessing(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration for the post-processor. |'
  prefs: []
  type: TYPE_TB
- en: '| config.cls | `Array.<string>` | The special tokens to add to the beginning
    of the input. |'
  prefs: []
  type: TYPE_TB
- en: '| config.sep | `Array.<string>` | The special tokens to add to the end of the
    input. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bertProcessing.post_process(tokens, [tokens_pair]) ⇒ <code> PostProcessedOutput
    </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adds the special tokens to the beginning and end of the input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertProcessing`](#module_tokenizers..BertProcessing)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - The post-processed tokens with the special
    tokens added to the beginning and end.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` |  | The input tokens. |'
  prefs: []
  type: TYPE_TB
- en: '| [tokens_pair] | `Array.<string>` |  | An optional second set of input tokens.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~TemplateProcessing ⇐ <code> PostProcessor </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post processor that replaces special tokens in a template with actual tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PostProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~TemplateProcessing](#module_tokenizers..TemplateProcessing) ⇐ `PostProcessor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new TemplateProcessing(config)`](#new_module_tokenizers..TemplateProcessing_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..TemplateProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new TemplateProcessing(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of `TemplateProcessing`.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the post processor. |'
  prefs: []
  type: TYPE_TB
- en: '| config.single | `Array` | The template for a single sequence of tokens. |'
  prefs: []
  type: TYPE_TB
- en: '| config.pair | `Array` | The template for a pair of sequences of tokens. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: templateProcessing.post_process(tokens, [tokens_pair]) ⇒ <code> PostProcessedOutput
    </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Replaces special tokens in the template with actual tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`TemplateProcessing`](#module_tokenizers..TemplateProcessing)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - An object containing the list of tokens
    with the special tokens replaced with actual tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` |  | The list of tokens for the first sequence.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [tokens_pair] | `Array.<string>` |  | The list of tokens for the second sequence
    (optional). |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ByteLevelPostProcessor ⇐ <code> PostProcessor </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A PostProcessor that returns the given tokens as is.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PostProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: byteLevelPostProcessor.post_process(tokens, [tokens_pair]) ⇒ <code> PostProcessedOutput
    </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Post process the given tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelPostProcessor`](#module_tokenizers..ByteLevelPostProcessor)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - An object containing the post-processed
    tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` |  | The list of tokens for the first sequence.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [tokens_pair] | `Array.<string>` |  | The list of tokens for the second sequence
    (optional). |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Decoder ⇐ <code> Callable </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class for token decoders.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~Decoder](#module_tokenizers..Decoder) ⇐ `Callable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Decoder(config)`](#new_module_tokenizers..Decoder_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.added_tokens`](#module_tokenizers..Decoder+added_tokens) : `Array.<AddedToken>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens)`](#module_tokenizers..Decoder+_call) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode(tokens)`](#module_tokenizers..Decoder+decode) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain(tokens)`](#module_tokenizers..Decoder+decode_chain) ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..Decoder.fromConfig) ⇒ `Decoder`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new Decoder(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of `Decoder`.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'decoder.added_tokens : <code> Array. < AddedToken > </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`Decoder`](#module_tokenizers..Decoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: decoder._call(tokens) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Calls the `decode` method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Decoder`](#module_tokenizers..Decoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The list of tokens. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: decoder.decode(tokens) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decodes a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Decoder`](#module_tokenizers..Decoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The list of tokens. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: decoder.decode_chain(tokens) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apply the decoder to a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Decoder`](#module_tokenizers..Decoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The decoded list of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the `decode_chain` method is not implemented in the subclass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The list of tokens. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Decoder.fromConfig(config) ⇒ <code> Decoder </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a decoder instance based on the provided configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`Decoder`](#module_tokenizers..Decoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Decoder` - A decoder instance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If an unknown decoder type is provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~FuseDecoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fuse simply fuses all tokens into one big string. It’s usually the last decoding
    step anyway, but this decoder exists incase some decoders need to happen after
    that step
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'fuseDecoder.decode_chain() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`FuseDecoder`](#module_tokenizers..FuseDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WordPieceDecoder ⇐ <code> Decoder </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A decoder that decodes a list of WordPiece tokens into a single string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~WordPieceDecoder](#module_tokenizers..WordPieceDecoder) ⇐ `Decoder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WordPieceDecoder(config)`](#new_module_tokenizers..WordPieceDecoder_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..WordPieceDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new WordPieceDecoder(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of WordPieceDecoder.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.prefix | `string` | The prefix used for WordPiece encoding. |'
  prefs: []
  type: TYPE_TB
- en: '| config.cleanup | `boolean` | Whether to cleanup the decoded string. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceDecoder.decode_chain() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WordPieceDecoder`](#module_tokenizers..WordPieceDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ByteLevelDecoder ⇐ <code> Decoder </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Byte-level decoder for tokenization output. Inherits from the `Decoder` class.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~ByteLevelDecoder](#module_tokenizers..ByteLevelDecoder) ⇐ `Decoder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ByteLevelDecoder(config)`](#new_module_tokenizers..ByteLevelDecoder_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..ByteLevelDecoder+convert_tokens_to_string)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..ByteLevelDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new ByteLevelDecoder(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a `ByteLevelDecoder` object.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | Configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: byteLevelDecoder.convert_tokens_to_string(tokens) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convert an array of tokens to string by decoding each byte.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelDecoder`](#module_tokenizers..ByteLevelDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | Array of tokens to be decoded. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelDecoder.decode_chain() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelDecoder`](#module_tokenizers..ByteLevelDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~CTCDecoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CTC (Connectionist Temporal Classification) decoder. See [https://github.com/huggingface/tokenizers/blob/bb38f390a61883fc2f29d659af696f428d1cda6b/tokenizers/src/decoders/ctc.rs](https://github.com/huggingface/tokenizers/blob/bb38f390a61883fc2f29d659af696f428d1cda6b/tokenizers/src/decoders/ctc.rs)
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[~CTCDecoder](#module_tokenizers..CTCDecoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..CTCDecoder+convert_tokens_to_string)
    ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..CTCDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ctcDecoder.convert_tokens_to_string(tokens) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converts a connectionist-temporal-classification (CTC) output tokens into a
    single string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`CTCDecoder`](#module_tokenizers..CTCDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | Array of tokens to be decoded. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'ctcDecoder.decode_chain() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`CTCDecoder`](#module_tokenizers..CTCDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~DecoderSequence ⇐ <code> Decoder </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apply a sequence of decoders.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~DecoderSequence](#module_tokenizers..DecoderSequence) ⇐ `Decoder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new DecoderSequence(config)`](#new_module_tokenizers..DecoderSequence_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..DecoderSequence+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new DecoderSequence(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of DecoderSequence.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.decoders | `Array.<Decoder>` | The list of decoders to apply. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'decoderSequence.decode_chain() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`DecoderSequence`](#module_tokenizers..DecoderSequence)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~MetaspacePreTokenizer ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This PreTokenizer replaces spaces with the given replacement character, adds
    a prefix space if requested, and returns a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~MetaspacePreTokenizer](#module_tokenizers..MetaspacePreTokenizer) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MetaspacePreTokenizer(config)`](#new_module_tokenizers..MetaspacePreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..MetaspacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new MetaspacePreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` |  | The configuration object for the MetaspacePreTokenizer.
    |'
  prefs: []
  type: TYPE_TB
- en: '| config.add_prefix_space | `boolean` |  | Whether to add a prefix space to
    the first token. |'
  prefs: []
  type: TYPE_TB
- en: '| config.replacement | `string` |  | The character to replace spaces with.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [config.str_rep] | `string` | `"config.replacement"` | An optional string
    representation of the replacement character. |'
  prefs: []
  type: TYPE_TB
- en: '| [config.prepend_scheme] | `''first''` &#124; `''never''` &#124; `''always''`
    | `''always''` | The metaspace prepending scheme. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: metaspacePreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method takes a string, replaces spaces with the replacement character,
    adds a prefix space if requested, and returns a new list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`MetaspacePreTokenizer`](#module_tokenizers..MetaspacePreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - A new list of pre-tokenized tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to pre-tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | The options for the pre-tokenization. |'
  prefs: []
  type: TYPE_TB
- en: '| [options.section_index] | `number` | The index of the section to pre-tokenize.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~MetaspaceDecoder ⇐ <code> Decoder </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MetaspaceDecoder class extends the Decoder class and decodes Metaspace tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~MetaspaceDecoder](#module_tokenizers..MetaspaceDecoder) ⇐ `Decoder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MetaspaceDecoder(config)`](#new_module_tokenizers..MetaspaceDecoder_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..MetaspaceDecoder+decode_chain) : `*`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new MetaspaceDecoder(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constructs a new MetaspaceDecoder object.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the MetaspaceDecoder. |'
  prefs: []
  type: TYPE_TB
- en: '| config.add_prefix_space | `boolean` | Whether to add a prefix space to the
    decoded string. |'
  prefs: []
  type: TYPE_TB
- en: '| config.replacement | `string` | The string to replace spaces with. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'metaspaceDecoder.decode_chain() : <code> * </code>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`MetaspaceDecoder`](#module_tokenizers..MetaspaceDecoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Precompiled ⇐ <code> Normalizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A normalizer that applies a precompiled charsmap. This is useful for applying
    complex normalizations in C++ and exposing them to JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~Precompiled](#module_tokenizers..Precompiled) ⇐ `Normalizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Precompiled(config)`](#new_module_tokenizers..Precompiled_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..Precompiled+normalize) ⇒ `string`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new Precompiled(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new instance of Precompiled normalizer.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the Precompiled normalizer.
    |'
  prefs: []
  type: TYPE_TB
- en: '| config.precompiled_charsmap | `Object` | The precompiled charsmap object.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: precompiled.normalize(text) ⇒ <code> string </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalizes the given text by applying the precompiled charsmap.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Precompiled`](#module_tokenizers..Precompiled)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PreTokenizerSequence ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pre-tokenizer that applies a sequence of pre-tokenizers to the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~PreTokenizerSequence](#module_tokenizers..PreTokenizerSequence) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PreTokenizerSequence(config)`](#new_module_tokenizers..PreTokenizerSequence_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizerSequence+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new PreTokenizerSequence(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of PreTokenizerSequence.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the pre-tokenizer sequence.
    |'
  prefs: []
  type: TYPE_TB
- en: '| config.pretokenizers | `Array.<Object>` | An array of pre-tokenizer configurations.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizerSequence.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applies each pre-tokenizer in the sequence to the input text in turn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTokenizerSequence`](#module_tokenizers..PreTokenizerSequence)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The pre-tokenized text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to pre-tokenize. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WhitespacePreTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Splits on word boundaries (using the following regular expression: `\w+|[^\w\s]+`).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[~WhitespacePreTokenizer](#module_tokenizers..WhitespacePreTokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WhitespacePreTokenizer(config)`](#new_module_tokenizers..WhitespacePreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new WhitespacePreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of WhitespacePreTokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: whitespacePreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. <
    string > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-tokenizes the input text by splitting it on word boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WhitespacePreTokenizer`](#module_tokenizers..WhitespacePreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens produced by splitting the
    input text on whitespace.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be pre-tokenized. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WhitespaceSplit ⇐ <code> PreTokenizer </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits a string of text by whitespace characters into individual tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[~WhitespaceSplit](#module_tokenizers..WhitespaceSplit) ⇐ `PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WhitespaceSplit(config)`](#new_module_tokenizers..WhitespaceSplit_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespaceSplit+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new WhitespaceSplit(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of WhitespaceSplit.
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: whitespaceSplit.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-tokenizes the input text by splitting it on whitespace characters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WhitespaceSplit`](#module_tokenizers..WhitespaceSplit)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens produced by splitting the
    input text on whitespace.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be pre-tokenized. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ReplacePreTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[~ReplacePreTokenizer](#module_tokenizers..ReplacePreTokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ReplacePreTokenizer(config)`](#new_module_tokenizers..ReplacePreTokenizer_new)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ReplacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: new ReplacePreTokenizer(config)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '| config.pattern | `Object` | The pattern used to split the text. Can be a
    string or a regex object. |'
  prefs: []
  type: TYPE_TB
- en: '| config.content | `string` | What to replace the pattern with. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: replacePreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-tokenizes the input text by replacing certain characters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ReplacePreTokenizer`](#module_tokenizers..ReplacePreTokenizer)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens produced by replacing certain
    characters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be pre-tokenized. |'
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BYTES_TO_UNICODE ⇒ <code> Object </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Returns list of utf-8 byte and a mapping to unicode strings. Specifically avoids
    mapping to whitespace/control characters the BPE code barfs on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner constant of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Object` - Object with utf-8 byte keys and unicode string values.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~loadTokenizer(pretrained_model_name_or_path, options) ⇒ <code> Promise.
    < Array < any > > </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loads a tokenizer from the specified path.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Promise.<Array<any>>` - A promise that resolves with information
    about the loaded tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pretrained_model_name_or_path | `string` | The path to the tokenizer directory.
    |'
  prefs: []
  type: TYPE_TB
- en: '| options | `PretrainedTokenizerOptions` | Additional options for loading the
    tokenizer. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~regexSplit(text, regex) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to split a string on a regex, but keep the delimiters. This
    is required, because the JavaScript `.split()` method does not keep the delimiters,
    and wrapping in a capturing group causes issues with existing capturing groups
    (due to nesting).
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The split string.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to split. |'
  prefs: []
  type: TYPE_TB
- en: '| regex | `RegExp` | The regex to split on. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~createPattern(pattern, invert) ⇒ <code> RegExp </code> | <code> null
    </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper method to construct a pattern from a config object.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `RegExp` | `null` - The compiled pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pattern | `Object` |  | The pattern object. |'
  prefs: []
  type: TYPE_TB
- en: '| invert | `boolean` | `true` | Whether to invert the pattern. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~objectToMap(obj) ⇒ <code> Map. < string, any > </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to convert an Object to a Map
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Map.<string, any>` - The map.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| obj | `Object` | The object to convert. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~prepareTensorForDecode(tensor) ⇒ <code> Array. < number > </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to convert a tensor to a list before decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<number>` - The tensor as a list.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tensor | `Tensor` | The tensor to convert. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~clean_up_tokenization(text) ⇒ <code> string </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clean up a list of simple English tokenization artifacts like spaces before
    punctuations and abbreviated forms
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The cleaned up text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to clean up. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~remove_accents(text) ⇒ <code> string </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to remove accents from a string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The text with accents removed.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to remove accents from. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~lowercase_and_remove_accent(text) ⇒ <code> string </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to lowercase a string and remove accents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The lowercased text with accents removed.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to lowercase and remove accents from. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~fuse(arr, value, mapping)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to fuse consecutive values in an array equal to the specified
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| arr | `Array.<string>` | The input array |'
  prefs: []
  type: TYPE_TB
- en: '| value | `any` | The value to fuse on. |'
  prefs: []
  type: TYPE_TB
- en: '| mapping | `Map.<string, any>` | The mapping from input domain to value. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~whitespace_split(text) ⇒ <code> Array. < string > </code>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Split a string on whitespace.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The split string.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to split. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~PretrainedTokenizerOptions : <code> Object </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additional tokenizer-specific properties.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [legacy] | `boolean` | `false` | Whether or not the `legacy` behavior of
    the tokenizer should be used. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~BPENode : <code> Object </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| token | `string` | The token associated with the node |'
  prefs: []
  type: TYPE_TB
- en: '| bias | `number` | A positional bias for the node. |'
  prefs: []
  type: TYPE_TB
- en: '| [score] | `number` | The score of the node. |'
  prefs: []
  type: TYPE_TB
- en: '| [prev] | `BPENode` | The previous node in the linked list. |'
  prefs: []
  type: TYPE_TB
- en: '| [next] | `BPENode` | The next node in the linked list. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~SplitDelimiterBehavior : <code> ’ removed ’ </code> | <code> ’ isolated
    ’ </code> | <code> ’ mergedWithPrevious ’ </code> | <code> ’ mergedWithNext ’
    </code> | <code> ’ contiguous ’ </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~PostProcessedOutput : <code> Object </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | List of token produced by the post-processor.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [token_type_ids] | `Array.<number>` | List of token type ids produced by
    the post-processor. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~EncodingSingle : <code> Object </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| input_ids | `Array.<number>` | List of token ids to be fed to a model. |'
  prefs: []
  type: TYPE_TB
- en: '| attention_mask | `Array.<number>` | List of token type ids to be fed to a
    model |'
  prefs: []
  type: TYPE_TB
- en: '| [token_type_ids] | `Array.<number>` | List of indices specifying which tokens
    should be attended to by the model |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~BatchEncoding : <code> Array < number > </code> | <code> Array <
    Array < number > > </code> | <code> Tensor </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Holds the output of the tokenizer’s call function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| input_ids | `BatchEncodingItem` | List of token ids to be fed to a model.
    |'
  prefs: []
  type: TYPE_TB
- en: '| attention_mask | `BatchEncodingItem` | List of indices specifying which tokens
    should be attended to by the model. |'
  prefs: []
  type: TYPE_TB
- en: '| [token_type_ids] | `BatchEncodingItem` | List of token type ids to be fed
    to a model. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~Message : <code> Object </code>'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| role | `string` | The role of the message (e.g., "user" or "assistant" or
    "system"). |'
  prefs: []
  type: TYPE_TB
- en: '| content | `string` | The content of the message. |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
