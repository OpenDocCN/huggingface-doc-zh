- en: tokenizers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tokenizers
- en: 'Original text: [https://huggingface.co/docs/transformers.js/api/tokenizers](https://huggingface.co/docs/transformers.js/api/tokenizers)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原始文本: [https://huggingface.co/docs/transformers.js/api/tokenizers](https://huggingface.co/docs/transformers.js/api/tokenizers)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizers are used to prepare textual inputs for a model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器用于为模型准备文本输入。
- en: '**Example:** Create an `AutoTokenizer` and use it to tokenize a sentence. This
    will automatically detect the tokenizer type based on the tokenizer class defined
    in `tokenizer.json`.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** 创建一个`AutoTokenizer`并使用它对句子进行分词。这将根据`tokenizer.json`中定义的分词器类自动检测分词器类型。'
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[tokenizers](#module_tokenizers)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tokenizers](#module_tokenizers)'
- en: '*static*'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '[.TokenizerModel](#module_tokenizers.TokenizerModel) ⇐ `Callable`'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.TokenizerModel](#module_tokenizers.TokenizerModel) ⇐ `Callable`'
- en: '[`new TokenizerModel(config)`](#new_module_tokenizers.TokenizerModel_new)'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new TokenizerModel(config)`](#new_module_tokenizers.TokenizerModel_new)'
- en: '*instance*'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instance*'
- en: '[`.vocab`](#module_tokenizers.TokenizerModel+vocab) : `Array.<string>`'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.vocab`](#module_tokenizers.TokenizerModel+vocab) : `Array.<string>`'
- en: '[`.tokens_to_ids`](#module_tokenizers.TokenizerModel+tokens_to_ids) : `Map.<string,
    number>`'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers.TokenizerModel+tokens_to_ids) : `Map.<string,
    number>`'
- en: '[`.fuse_unk`](#module_tokenizers.TokenizerModel+fuse_unk) : `boolean`'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fuse_unk`](#module_tokenizers.TokenizerModel+fuse_unk) : `boolean`'
- en: '[`._call(tokens)`](#module_tokenizers.TokenizerModel+_call) ⇒ `Array.<string>`'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(tokens)`](#module_tokenizers.TokenizerModel+_call) ⇒ `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers.TokenizerModel+encode) ⇒ `Array.<string>`'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers.TokenizerModel+encode) ⇒ `Array.<string>`'
- en: '[`.convert_tokens_to_ids(tokens)`](#module_tokenizers.TokenizerModel+convert_tokens_to_ids)
    ⇒ `Array.<number>`'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.convert_tokens_to_ids(tokens)`](#module_tokenizers.TokenizerModel+convert_tokens_to_ids)
    ⇒ `Array.<number>`'
- en: '[`.convert_ids_to_tokens(ids)`](#module_tokenizers.TokenizerModel+convert_ids_to_tokens)
    ⇒ `Array.<string>`'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.convert_ids_to_tokens(ids)`](#module_tokenizers.TokenizerModel+convert_ids_to_tokens)
    ⇒ `Array.<string>`'
- en: '*static*'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '[`.fromConfig(config, ...args)`](#module_tokenizers.TokenizerModel.fromConfig)
    ⇒ `TokenizerModel`'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fromConfig(config, ...args)`](#module_tokenizers.TokenizerModel.fromConfig)
    ⇒ `TokenizerModel`'
- en: '[.PreTrainedTokenizer](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.PreTrainedTokenizer](#module_tokenizers.PreTrainedTokenizer)'
- en: '[`new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.PreTrainedTokenizer_new)'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.PreTrainedTokenizer_new)'
- en: '*instance*'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instance*'
- en: '[`.added_tokens`](#module_tokenizers.PreTrainedTokenizer+added_tokens) : `Array.<AddedToken>`'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.added_tokens`](#module_tokenizers.PreTrainedTokenizer+added_tokens) : `Array.<AddedToken>`'
- en: '[`.remove_space`](#module_tokenizers.PreTrainedTokenizer+remove_space) : `boolean`'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.remove_space`](#module_tokenizers.PreTrainedTokenizer+remove_space) : `boolean`'
- en: '[`.padding_side`](#module_tokenizers.PreTrainedTokenizer+padding_side) : `’right’`
    | `’left’`'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.padding_side`](#module_tokenizers.PreTrainedTokenizer+padding_side) : `’right’`
    | `’left’`'
- en: '[`.getToken(...keys)`](#module_tokenizers.PreTrainedTokenizer+getToken) ⇒ `string`
    | `null`'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.getToken(...keys)`](#module_tokenizers.PreTrainedTokenizer+getToken) ⇒ `string`
    | `null`'
- en: '[`._call(text, options)`](#module_tokenizers.PreTrainedTokenizer+_call) ⇒ `BatchEncoding`'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(text, options)`](#module_tokenizers.PreTrainedTokenizer+_call) ⇒ `BatchEncoding`'
- en: '[`._encode_text(text)`](#module_tokenizers.PreTrainedTokenizer+_encode_text)
    ⇒ `Array<string>` | `null`'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._encode_text(text)`](#module_tokenizers.PreTrainedTokenizer+_encode_text)
    ⇒ `Array<string>` | `null`'
- en: '[`.encode(text, text_pair, options)`](#module_tokenizers.PreTrainedTokenizer+encode)
    ⇒ `Array.<number>`'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(text, text_pair, options)`](#module_tokenizers.PreTrainedTokenizer+encode)
    ⇒ `Array.<number>`'
- en: '[`.batch_decode(batch, decode_args)`](#module_tokenizers.PreTrainedTokenizer+batch_decode)
    ⇒ `Array.<string>`'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.batch_decode(batch, decode_args)`](#module_tokenizers.PreTrainedTokenizer+batch_decode)
    ⇒ `Array.<string>`'
- en: '[`.decode(token_ids, [decode_args])`](#module_tokenizers.PreTrainedTokenizer+decode)
    ⇒ `string`'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode(token_ids, [decode_args])`](#module_tokenizers.PreTrainedTokenizer+decode)
    ⇒ `string`'
- en: '[`.decode_single(token_ids, decode_args)`](#module_tokenizers.PreTrainedTokenizer+decode_single)
    ⇒ `string`'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_single(token_ids, decode_args)`](#module_tokenizers.PreTrainedTokenizer+decode_single)
    ⇒ `string`'
- en: '[`.apply_chat_template(conversation, options)`](#module_tokenizers.PreTrainedTokenizer+apply_chat_template)
    ⇒ `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.apply_chat_template(conversation, options)`](#module_tokenizers.PreTrainedTokenizer+apply_chat_template)
    ⇒ `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`'
- en: '*static*'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.PreTrainedTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.PreTrainedTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
- en: '[.BertTokenizer](#module_tokenizers.BertTokenizer) ⇐ `PreTrainedTokenizer`'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.BertTokenizer](#module_tokenizers.BertTokenizer) ⇐ `PreTrainedTokenizer`'
- en: '[.AlbertTokenizer](#module_tokenizers.AlbertTokenizer) ⇐ `PreTrainedTokenizer`'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.AlbertTokenizer](#module_tokenizers.AlbertTokenizer) ⇐ `PreTrainedTokenizer`'
- en: '[.NllbTokenizer](#module_tokenizers.NllbTokenizer)'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.NllbTokenizer](#module_tokenizers.NllbTokenizer)'
- en: '[`._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)`](#module_tokenizers.NllbTokenizer+_build_translation_inputs)
    ⇒ `Object`'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)`](#module_tokenizers.NllbTokenizer+_build_translation_inputs)
    ⇒ `Object`'
- en: '[.M2M100Tokenizer](#module_tokenizers.M2M100Tokenizer)'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.M2M100Tokenizer](#module_tokenizers.M2M100Tokenizer)'
- en: '[`._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)`](#module_tokenizers.M2M100Tokenizer+_build_translation_inputs)
    ⇒ `Object`'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)`](#module_tokenizers.M2M100Tokenizer+_build_translation_inputs)
    ⇒ `Object`'
- en: '[.WhisperTokenizer](#module_tokenizers.WhisperTokenizer) ⇐ `PreTrainedTokenizer`'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.WhisperTokenizer](#module_tokenizers.WhisperTokenizer) ⇐ `PreTrainedTokenizer`'
- en: '[`._decode_asr(sequences, options)`](#module_tokenizers.WhisperTokenizer+_decode_asr)
    ⇒ `*`'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._decode_asr(sequences, options)`](#module_tokenizers.WhisperTokenizer+_decode_asr)
    ⇒ `*`'
- en: '[`.decode()`](#module_tokenizers.WhisperTokenizer+decode) : `*`'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode()`](#module_tokenizers.WhisperTokenizer+decode) : `*`'
- en: '[`.get_decoder_prompt_ids(options)`](#module_tokenizers.WhisperTokenizer+get_decoder_prompt_ids)
    ⇒ `Array.<Array<number>>`'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.get_decoder_prompt_ids(options)`](#module_tokenizers.WhisperTokenizer+get_decoder_prompt_ids)
    ⇒ `Array.<Array<number>>`'
- en: '[.MarianTokenizer](#module_tokenizers.MarianTokenizer)'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.MarianTokenizer](#module_tokenizers.MarianTokenizer)'
- en: '[`new MarianTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.MarianTokenizer_new)'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new MarianTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.MarianTokenizer_new)'
- en: '[`._encode_text(text)`](#module_tokenizers.MarianTokenizer+_encode_text) ⇒
    `Array`'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._encode_text(text)`](#module_tokenizers.MarianTokenizer+_encode_text) ⇒
    `Array`'
- en: '[.AutoTokenizer](#module_tokenizers.AutoTokenizer)'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.AutoTokenizer](#module_tokenizers.AutoTokenizer)'
- en: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.AutoTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.AutoTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
- en: '*inner*'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*inner*'
- en: '[~AddedToken](#module_tokenizers..AddedToken)'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~AddedToken](#module_tokenizers..AddedToken)'
- en: '[`new AddedToken(config)`](#new_module_tokenizers..AddedToken_new)'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new AddedToken(config)`](#new_module_tokenizers..AddedToken_new)'
- en: '[~WordPieceTokenizer](#module_tokenizers..WordPieceTokenizer) ⇐ `TokenizerModel`'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~WordPieceTokenizer](#module_tokenizers..WordPieceTokenizer) ⇐ `TokenizerModel`'
- en: '[`new WordPieceTokenizer(config)`](#new_module_tokenizers..WordPieceTokenizer_new)'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new WordPieceTokenizer(config)`](#new_module_tokenizers..WordPieceTokenizer_new)'
- en: '[`.tokens_to_ids`](#module_tokenizers..WordPieceTokenizer+tokens_to_ids) :
    `Map.<string, number>`'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers..WordPieceTokenizer+tokens_to_ids) :
    `Map.<string, number>`'
- en: '[`.unk_token_id`](#module_tokenizers..WordPieceTokenizer+unk_token_id) : `number`'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.unk_token_id`](#module_tokenizers..WordPieceTokenizer+unk_token_id) : `number`'
- en: '[`.unk_token`](#module_tokenizers..WordPieceTokenizer+unk_token) : `string`'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.unk_token`](#module_tokenizers..WordPieceTokenizer+unk_token) : `string`'
- en: '[`.max_input_chars_per_word`](#module_tokenizers..WordPieceTokenizer+max_input_chars_per_word)
    : `number`'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.max_input_chars_per_word`](#module_tokenizers..WordPieceTokenizer+max_input_chars_per_word)
    : `number`'
- en: '[`.vocab`](#module_tokenizers..WordPieceTokenizer+vocab) : `Array.<string>`'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.vocab`](#module_tokenizers..WordPieceTokenizer+vocab) : `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers..WordPieceTokenizer+encode) ⇒ `Array.<string>`'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers..WordPieceTokenizer+encode) ⇒ `Array.<string>`'
- en: '[~Unigram](#module_tokenizers..Unigram) ⇐ `TokenizerModel`'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~Unigram](#module_tokenizers..Unigram) ⇐ `TokenizerModel`'
- en: '[`new Unigram(config, moreConfig)`](#new_module_tokenizers..Unigram_new)'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new Unigram(config, moreConfig)`](#new_module_tokenizers..Unigram_new)'
- en: '[`.populateNodes(lattice)`](#module_tokenizers..Unigram+populateNodes)'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.populateNodes(lattice)`](#module_tokenizers..Unigram+populateNodes)'
- en: '[`.tokenize(normalized)`](#module_tokenizers..Unigram+tokenize) ⇒ `Array.<string>`'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokenize(normalized)`](#module_tokenizers..Unigram+tokenize) ⇒ `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers..Unigram+encode) ⇒ `Array.<string>`'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers..Unigram+encode) ⇒ `Array.<string>`'
- en: '[~BPE](#module_tokenizers..BPE) ⇐ `TokenizerModel`'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~BPE](#module_tokenizers..BPE) ⇐ `TokenizerModel`'
- en: '[`new BPE(config)`](#new_module_tokenizers..BPE_new)'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new BPE(config)`](#new_module_tokenizers..BPE_new)'
- en: '[`.tokens_to_ids`](#module_tokenizers..BPE+tokens_to_ids) : `Map.<string, number>`'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers..BPE+tokens_to_ids) : `Map.<string, number>`'
- en: '[`.cache`](#module_tokenizers..BPE+cache) : `Map.<string, Array<string>>`'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.cache`](#module_tokenizers..BPE+cache) : `Map.<string, Array<string>>`'
- en: '[`.bpe(token)`](#module_tokenizers..BPE+bpe) ⇒ `Array.<string>`'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.bpe(token)`](#module_tokenizers..BPE+bpe) ⇒ `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers..BPE+encode) ⇒ `Array.<string>`'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers..BPE+encode) ⇒ `Array.<string>`'
- en: '[~LegacyTokenizerModel](#module_tokenizers..LegacyTokenizerModel)'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~LegacyTokenizerModel](#module_tokenizers..LegacyTokenizerModel)'
- en: '[`new LegacyTokenizerModel(config, moreConfig)`](#new_module_tokenizers..LegacyTokenizerModel_new)'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new LegacyTokenizerModel(config, moreConfig)`](#new_module_tokenizers..LegacyTokenizerModel_new)'
- en: '[`.tokens_to_ids`](#module_tokenizers..LegacyTokenizerModel+tokens_to_ids)
    : `Map.<string, number>`'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers..LegacyTokenizerModel+tokens_to_ids)
    : `Map.<string, number>`'
- en: '*[~Normalizer](#module_tokenizers..Normalizer)*'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[~Normalizer](#module_tokenizers..Normalizer)*'
- en: '*[`new Normalizer(config)`](#new_module_tokenizers..Normalizer_new)*'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`new Normalizer(config)`](#new_module_tokenizers..Normalizer_new)*'
- en: '*instance*'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instance*'
- en: '**[`.normalize(text)`](#module_tokenizers..Normalizer+normalize) ⇒ `string`**'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[`.normalize(text)`](#module_tokenizers..Normalizer+normalize) ⇒ `string`**'
- en: '*[`._call(text)`](#module_tokenizers..Normalizer+_call) ⇒ `string`*'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`._call(text)`](#module_tokenizers..Normalizer+_call) ⇒ `string`*'
- en: '*static*'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '*[`.fromConfig(config)`](#module_tokenizers..Normalizer.fromConfig) ⇒ `Normalizer`*'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`.fromConfig(config)`](#module_tokenizers..Normalizer.fromConfig) ⇒ `Normalizer`*'
- en: '[~Replace](#module_tokenizers..Replace) ⇐ `Normalizer`'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~Replace](#module_tokenizers..Replace) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..Replace+normalize) ⇒ `string`'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..Replace+normalize) ⇒ `string`'
- en: '[~NFC](#module_tokenizers..NFC) ⇐ `Normalizer`'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~NFC](#module_tokenizers..NFC) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..NFC+normalize) ⇒ `string`'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..NFC+normalize) ⇒ `string`'
- en: '[~NFKC](#module_tokenizers..NFKC) ⇐ `Normalizer`'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~NFKC](#module_tokenizers..NFKC) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..NFKC+normalize) ⇒ `string`'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..NFKC+normalize) ⇒ `string`'
- en: '[~NFKD](#module_tokenizers..NFKD) ⇐ `Normalizer`'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~NFKD](#module_tokenizers..NFKD) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..NFKD+normalize) ⇒ `string`'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..NFKD+normalize) ⇒ `string`'
- en: '[~StripNormalizer](#module_tokenizers..StripNormalizer)'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~StripNormalizer](#module_tokenizers..StripNormalizer)'
- en: '[`.normalize(text)`](#module_tokenizers..StripNormalizer+normalize) ⇒ `string`'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..StripNormalizer+normalize) ⇒ `string`'
- en: '[~StripAccents](#module_tokenizers..StripAccents) ⇐ `Normalizer`'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~StripAccents](#module_tokenizers..StripAccents) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..StripAccents+normalize) ⇒ `string`'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..StripAccents+normalize) ⇒ `string`'
- en: '[~Lowercase](#module_tokenizers..Lowercase) ⇐ `Normalizer`'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~Lowercase](#module_tokenizers..Lowercase) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..Lowercase+normalize) ⇒ `string`'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..Lowercase+normalize) ⇒ `string`'
- en: '[~Prepend](#module_tokenizers..Prepend) ⇐ `Normalizer`'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~Prepend](#module_tokenizers..Prepend) ⇐ `Normalizer`'
- en: '[`.normalize(text)`](#module_tokenizers..Prepend+normalize) ⇒ `string`'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..Prepend+normalize) ⇒ `string`'
- en: '[~NormalizerSequence](#module_tokenizers..NormalizerSequence) ⇐ `Normalizer`'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~NormalizerSequence](#module_tokenizers..NormalizerSequence) ⇐ `Normalizer`'
- en: '[`new NormalizerSequence(config)`](#new_module_tokenizers..NormalizerSequence_new)'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new NormalizerSequence(config)`](#new_module_tokenizers..NormalizerSequence_new)'
- en: '[`.normalize(text)`](#module_tokenizers..NormalizerSequence+normalize) ⇒ `string`'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..NormalizerSequence+normalize) ⇒ `string`'
- en: '[~BertNormalizer](#module_tokenizers..BertNormalizer) ⇐ `Normalizer`'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~BertNormalizer](#module_tokenizers..BertNormalizer) ⇐ `Normalizer`'
- en: '[`._tokenize_chinese_chars(text)`](#module_tokenizers..BertNormalizer+_tokenize_chinese_chars)
    ⇒ `string`'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._tokenize_chinese_chars(text)`](#module_tokenizers..BertNormalizer+_tokenize_chinese_chars)
    ⇒ `string`'
- en: '[`._is_chinese_char(cp)`](#module_tokenizers..BertNormalizer+_is_chinese_char)
    ⇒ `boolean`'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._is_chinese_char(cp)`](#module_tokenizers..BertNormalizer+_is_chinese_char)
    ⇒ `boolean`'
- en: '[`.stripAccents(text)`](#module_tokenizers..BertNormalizer+stripAccents) ⇒
    `string`'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.stripAccents(text)`](#module_tokenizers..BertNormalizer+stripAccents) ⇒
    `string`'
- en: '[`.normalize(text)`](#module_tokenizers..BertNormalizer+normalize) ⇒ `string`'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..BertNormalizer+normalize) ⇒ `string`'
- en: '[~PreTokenizer](#module_tokenizers..PreTokenizer) ⇐ `Callable`'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~PreTokenizer](#module_tokenizers..PreTokenizer) ⇐ `Callable`'
- en: '*instance*'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instance*'
- en: '*[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`*'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`*'
- en: '[`.pre_tokenize(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize)
    ⇒ `Array.<string>`'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize)
    ⇒ `Array.<string>`'
- en: '[`._call(text, [options])`](#module_tokenizers..PreTokenizer+_call) ⇒ `Array.<string>`'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(text, [options])`](#module_tokenizers..PreTokenizer+_call) ⇒ `Array.<string>`'
- en: '*static*'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '[`.fromConfig(config)`](#module_tokenizers..PreTokenizer.fromConfig) ⇒ `PreTokenizer`'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fromConfig(config)`](#module_tokenizers..PreTokenizer.fromConfig) ⇒ `PreTokenizer`'
- en: '[~BertPreTokenizer](#module_tokenizers..BertPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~BertPreTokenizer](#module_tokenizers..BertPreTokenizer) ⇐ `PreTokenizer`'
- en: '[`new BertPreTokenizer(config)`](#new_module_tokenizers..BertPreTokenizer_new)'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new BertPreTokenizer(config)`](#new_module_tokenizers..BertPreTokenizer_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..BertPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..BertPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~ByteLevelPreTokenizer](#module_tokenizers..ByteLevelPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~ByteLevelPreTokenizer](#module_tokenizers..ByteLevelPreTokenizer) ⇐ `PreTokenizer`'
- en: '[`new ByteLevelPreTokenizer(config)`](#new_module_tokenizers..ByteLevelPreTokenizer_new)'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new ByteLevelPreTokenizer(config)`](#new_module_tokenizers..ByteLevelPreTokenizer_new)'
- en: '[`.add_prefix_space`](#module_tokenizers..ByteLevelPreTokenizer+add_prefix_space)
    : `boolean`'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.add_prefix_space`](#module_tokenizers..ByteLevelPreTokenizer+add_prefix_space)
    : `boolean`'
- en: '[`.trim_offsets`](#module_tokenizers..ByteLevelPreTokenizer+trim_offsets) :
    `boolean`'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.trim_offsets`](#module_tokenizers..ByteLevelPreTokenizer+trim_offsets) :
    `boolean`'
- en: '[`.use_regex`](#module_tokenizers..ByteLevelPreTokenizer+use_regex) : `boolean`'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.use_regex`](#module_tokenizers..ByteLevelPreTokenizer+use_regex) : `boolean`'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ByteLevelPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ByteLevelPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~SplitPreTokenizer](#module_tokenizers..SplitPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~SplitPreTokenizer](#module_tokenizers..SplitPreTokenizer) ⇐ `PreTokenizer`'
- en: '[`new SplitPreTokenizer(config)`](#new_module_tokenizers..SplitPreTokenizer_new)'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new SplitPreTokenizer(config)`](#new_module_tokenizers..SplitPreTokenizer_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..SplitPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..SplitPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~PunctuationPreTokenizer](#module_tokenizers..PunctuationPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~PunctuationPreTokenizer](#module_tokenizers..PunctuationPreTokenizer) ⇐ `PreTokenizer`'
- en: '[`new PunctuationPreTokenizer(config)`](#new_module_tokenizers..PunctuationPreTokenizer_new)'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new PunctuationPreTokenizer(config)`](#new_module_tokenizers..PunctuationPreTokenizer_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PunctuationPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PunctuationPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~DigitsPreTokenizer](#module_tokenizers..DigitsPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~DigitsPreTokenizer](#module_tokenizers..DigitsPreTokenizer) ⇐ `PreTokenizer`'
- en: '[`new DigitsPreTokenizer(config)`](#new_module_tokenizers..DigitsPreTokenizer_new)'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new DigitsPreTokenizer(config)`](#new_module_tokenizers..DigitsPreTokenizer_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..DigitsPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..DigitsPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~PostProcessor](#module_tokenizers..PostProcessor) ⇐ `Callable`'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~PostProcessor](#module_tokenizers..PostProcessor) ⇐ `Callable`'
- en: '[`new PostProcessor(config)`](#new_module_tokenizers..PostProcessor_new)'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new PostProcessor(config)`](#new_module_tokenizers..PostProcessor_new)'
- en: '*instance*'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instance*'
- en: '[`.post_process(tokens, ...args)`](#module_tokenizers..PostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.post_process(tokens, ...args)`](#module_tokenizers..PostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
- en: '[`._call(tokens, ...args)`](#module_tokenizers..PostProcessor+_call) ⇒ `PostProcessedOutput`'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(tokens, ...args)`](#module_tokenizers..PostProcessor+_call) ⇒ `PostProcessedOutput`'
- en: '*static*'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '[`.fromConfig(config)`](#module_tokenizers..PostProcessor.fromConfig) ⇒ `PostProcessor`'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fromConfig(config)`](#module_tokenizers..PostProcessor.fromConfig) ⇒ `PostProcessor`'
- en: '[~BertProcessing](#module_tokenizers..BertProcessing)'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~BertProcessing](#module_tokenizers..BertProcessing)'
- en: '[`new BertProcessing(config)`](#new_module_tokenizers..BertProcessing_new)'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new BertProcessing(config)`](#new_module_tokenizers..BertProcessing_new)'
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..BertProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..BertProcessing+post_process)
    ⇒ `PostProcessedOutput`'
- en: '[~TemplateProcessing](#module_tokenizers..TemplateProcessing) ⇐ `PostProcessor`'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~TemplateProcessing](#module_tokenizers..TemplateProcessing) ⇐ `PostProcessor`'
- en: '[`new TemplateProcessing(config)`](#new_module_tokenizers..TemplateProcessing_new)'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new TemplateProcessing(config)`](#new_module_tokenizers..TemplateProcessing_new)'
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..TemplateProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..TemplateProcessing+post_process)
    ⇒ `PostProcessedOutput`'
- en: '[~ByteLevelPostProcessor](#module_tokenizers..ByteLevelPostProcessor) ⇐ `PostProcessor`'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~ByteLevelPostProcessor](#module_tokenizers..ByteLevelPostProcessor) ⇐ `PostProcessor`'
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..ByteLevelPostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..ByteLevelPostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
- en: '[~Decoder](#module_tokenizers..Decoder) ⇐ `Callable`'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~Decoder](#module_tokenizers..Decoder) ⇐ `Callable`'
- en: '[`new Decoder(config)`](#new_module_tokenizers..Decoder_new)'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new Decoder(config)`](#new_module_tokenizers..Decoder_new)'
- en: '*instance*'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*instance*'
- en: '[`.added_tokens`](#module_tokenizers..Decoder+added_tokens) : `Array.<AddedToken>`'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.added_tokens`](#module_tokenizers..Decoder+added_tokens) : `Array.<AddedToken>`'
- en: '[`._call(tokens)`](#module_tokenizers..Decoder+_call) ⇒ `string`'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(tokens)`](#module_tokenizers..Decoder+_call) ⇒ `string`'
- en: '[`.decode(tokens)`](#module_tokenizers..Decoder+decode) ⇒ `string`'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode(tokens)`](#module_tokenizers..Decoder+decode) ⇒ `string`'
- en: '[`.decode_chain(tokens)`](#module_tokenizers..Decoder+decode_chain) ⇒ `Array.<string>`'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain(tokens)`](#module_tokenizers..Decoder+decode_chain) ⇒ `Array.<string>`'
- en: '*static*'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*static*'
- en: '[`.fromConfig(config)`](#module_tokenizers..Decoder.fromConfig) ⇒ `Decoder`'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fromConfig(config)`](#module_tokenizers..Decoder.fromConfig) ⇒ `Decoder`'
- en: '[~FuseDecoder](#module_tokenizers..FuseDecoder)'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~FuseDecoder](#module_tokenizers..FuseDecoder)'
- en: '[`.decode_chain()`](#module_tokenizers..FuseDecoder+decode_chain) : `*`'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain()`](#module_tokenizers..FuseDecoder+decode_chain) : `*`'
- en: '[~WordPieceDecoder](#module_tokenizers..WordPieceDecoder) ⇐ `Decoder`'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~WordPieceDecoder](#module_tokenizers..WordPieceDecoder) ⇐ `Decoder`'
- en: '[`new WordPieceDecoder(config)`](#new_module_tokenizers..WordPieceDecoder_new)'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new WordPieceDecoder(config)`](#new_module_tokenizers..WordPieceDecoder_new)'
- en: '[`.decode_chain()`](#module_tokenizers..WordPieceDecoder+decode_chain) : `*`'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain()`](#module_tokenizers..WordPieceDecoder+decode_chain) : `*`'
- en: '[~ByteLevelDecoder](#module_tokenizers..ByteLevelDecoder) ⇐ `Decoder`'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~ByteLevelDecoder](#module_tokenizers..ByteLevelDecoder) ⇐ `Decoder`'
- en: '[`new ByteLevelDecoder(config)`](#new_module_tokenizers..ByteLevelDecoder_new)'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new ByteLevelDecoder(config)`](#new_module_tokenizers..ByteLevelDecoder_new)'
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..ByteLevelDecoder+convert_tokens_to_string)
    ⇒ `string`'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..ByteLevelDecoder+convert_tokens_to_string)
    ⇒ `string`'
- en: '[`.decode_chain()`](#module_tokenizers..ByteLevelDecoder+decode_chain) : `*`'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain()`](#module_tokenizers..ByteLevelDecoder+decode_chain) : `*`'
- en: '[~CTCDecoder](#module_tokenizers..CTCDecoder)'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~CTCDecoder](#module_tokenizers..CTCDecoder)'
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..CTCDecoder+convert_tokens_to_string)
    ⇒ `string`'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..CTCDecoder+convert_tokens_to_string)
    ⇒ `string`'
- en: '[`.decode_chain()`](#module_tokenizers..CTCDecoder+decode_chain) : `*`'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain()`](#module_tokenizers..CTCDecoder+decode_chain) : `*`'
- en: '[~DecoderSequence](#module_tokenizers..DecoderSequence) ⇐ `Decoder`'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~DecoderSequence](#module_tokenizers..DecoderSequence) ⇐ `Decoder`'
- en: '[`new DecoderSequence(config)`](#new_module_tokenizers..DecoderSequence_new)'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new DecoderSequence(config)`](#new_module_tokenizers..DecoderSequence_new)'
- en: '[`.decode_chain()`](#module_tokenizers..DecoderSequence+decode_chain) : `*`'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain()`](#module_tokenizers..DecoderSequence+decode_chain) : `*`'
- en: '[~MetaspacePreTokenizer](#module_tokenizers..MetaspacePreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~MetaspacePreTokenizer](#module_tokenizers..MetaspacePreTokenizer) ⇐ `PreTokenizer`'
- en: '[`new MetaspacePreTokenizer(config)`](#new_module_tokenizers..MetaspacePreTokenizer_new)'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new MetaspacePreTokenizer(config)`](#new_module_tokenizers..MetaspacePreTokenizer_new)
    : `Object`'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..MetaspacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..MetaspacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~MetaspaceDecoder](#module_tokenizers..MetaspaceDecoder) ⇐ `Decoder`'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~MetaspaceDecoder](#module_tokenizers..MetaspaceDecoder) ⇐ `Decoder`'
- en: '[`new MetaspaceDecoder(config)`](#new_module_tokenizers..MetaspaceDecoder_new)'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new MetaspaceDecoder(config)`](#new_module_tokenizers..MetaspaceDecoder_new)'
- en: '[`.decode_chain()`](#module_tokenizers..MetaspaceDecoder+decode_chain) : `*`'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_chain()`](#module_tokenizers..MetaspaceDecoder+decode_chain) : `*`'
- en: '[~Precompiled](#module_tokenizers..Precompiled) ⇐ `Normalizer`'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~Precompiled](#module_tokenizers..Precompiled) ⇐ `Normalizer`'
- en: '[`new Precompiled(config)`](#new_module_tokenizers..Precompiled_new)'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new Precompiled(config)`](#new_module_tokenizers..Precompiled_new)'
- en: '[`.normalize(text)`](#module_tokenizers..Precompiled+normalize) ⇒ `string`'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.normalize(text)`](#module_tokenizers..Precompiled+normalize) ⇒ `string`'
- en: '[~PreTokenizerSequence](#module_tokenizers..PreTokenizerSequence) ⇐ `PreTokenizer`'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~PreTokenizerSequence](#module_tokenizers..PreTokenizerSequence) ⇐ `PreTokenizer`'
- en: '[`new PreTokenizerSequence(config)`](#new_module_tokenizers..PreTokenizerSequence_new)'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new PreTokenizerSequence(config)`](#new_module_tokenizers..PreTokenizerSequence_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizerSequence+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizerSequence+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~WhitespacePreTokenizer](#module_tokenizers..WhitespacePreTokenizer)'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~WhitespacePreTokenizer](#module_tokenizers..WhitespacePreTokenizer)'
- en: '[`new WhitespacePreTokenizer(config)`](#new_module_tokenizers..WhitespacePreTokenizer_new)'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new WhitespacePreTokenizer(config)`](#new_module_tokenizers..WhitespacePreTokenizer_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~WhitespaceSplit](#module_tokenizers..WhitespaceSplit) ⇐ `PreTokenizer`'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~WhitespaceSplit](#module_tokenizers..WhitespaceSplit) ⇐ `PreTokenizer`'
- en: '[`new WhitespaceSplit(config)`](#new_module_tokenizers..WhitespaceSplit_new)'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new WhitespaceSplit(config)`](#new_module_tokenizers..WhitespaceSplit_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespaceSplit+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespaceSplit+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[~ReplacePreTokenizer](#module_tokenizers..ReplacePreTokenizer)'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~ReplacePreTokenizer](#module_tokenizers..ReplacePreTokenizer)'
- en: '[`new ReplacePreTokenizer(config)`](#new_module_tokenizers..ReplacePreTokenizer_new)'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new ReplacePreTokenizer(config)`](#new_module_tokenizers..ReplacePreTokenizer_new)'
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ReplacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ReplacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
- en: '[`~BYTES_TO_UNICODE`](#module_tokenizers..BYTES_TO_UNICODE) ⇒ `Object`'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~BYTES_TO_UNICODE`](#module_tokenizers..BYTES_TO_UNICODE) ⇒ `Object`'
- en: '[`~loadTokenizer(pretrained_model_name_or_path, options)`](#module_tokenizers..loadTokenizer)
    ⇒ `Promise.<Array<any>>`'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~loadTokenizer(pretrained_model_name_or_path, options)`](#module_tokenizers..loadTokenizer)
    ⇒ `Promise.<Array<any>>`'
- en: '[`~regexSplit(text, regex)`](#module_tokenizers..regexSplit) ⇒ `Array.<string>`'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~regexSplit(text, regex)`](#module_tokenizers..regexSplit) ⇒ `Array.<string>`'
- en: '[`~createPattern(pattern, invert)`](#module_tokenizers..createPattern) ⇒ `RegExp`
    | `null`'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~createPattern(pattern, invert)`](#module_tokenizers..createPattern) ⇒ `RegExp`
    | `null`'
- en: '[`~objectToMap(obj)`](#module_tokenizers..objectToMap) ⇒ `Map.<string, any>`'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~objectToMap(obj)`](#module_tokenizers..objectToMap) ⇒ `Map.<string, any>`'
- en: '[`~prepareTensorForDecode(tensor)`](#module_tokenizers..prepareTensorForDecode)
    ⇒ `Array.<number>`'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~prepareTensorForDecode(tensor)`](#module_tokenizers..prepareTensorForDecode)
    ⇒ `Array.<number>`'
- en: '[`~clean_up_tokenization(text)`](#module_tokenizers..clean_up_tokenization)
    ⇒ `string`'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~clean_up_tokenization(text)`](#module_tokenizers..clean_up_tokenization)
    ⇒ `string`'
- en: '[`~remove_accents(text)`](#module_tokenizers..remove_accents) ⇒ `string`'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~remove_accents(text)`](#module_tokenizers..remove_accents) ⇒ `string`'
- en: '[`~lowercase_and_remove_accent(text)`](#module_tokenizers..lowercase_and_remove_accent)
    ⇒ `string`'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~lowercase_and_remove_accent(text)`](#module_tokenizers..lowercase_and_remove_accent)
    ⇒ `string`'
- en: '[`~fuse(arr, value, mapping)`](#module_tokenizers..fuse)'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~fuse(arr, value, mapping)`](#module_tokenizers..fuse)'
- en: '[`~whitespace_split(text)`](#module_tokenizers..whitespace_split) ⇒ `Array.<string>`'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~whitespace_split(text)`](#module_tokenizers..whitespace_split) ⇒ `Array.<string>`'
- en: '[`~PretrainedTokenizerOptions`](#module_tokenizers..PretrainedTokenizerOptions)
    : `Object`'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~PretrainedTokenizerOptions`](#module_tokenizers..PretrainedTokenizerOptions)
    : `Object`'
- en: '[`~BPENode`](#module_tokenizers..BPENode) : `Object`'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~BPENode`](#module_tokenizers..BPENode) : `Object`'
- en: '[`~SplitDelimiterBehavior`](#module_tokenizers..SplitDelimiterBehavior) : `’removed’`
    | `’isolated’` | `’mergedWithPrevious’` | `’mergedWithNext’` | `’contiguous’`'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~SplitDelimiterBehavior`](#module_tokenizers..SplitDelimiterBehavior)：`’removed’`
    | `’isolated’` | `’mergedWithPrevious’` | `’mergedWithNext’` | `’contiguous’`'
- en: '[`~PostProcessedOutput`](#module_tokenizers..PostProcessedOutput) : `Object`'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~PostProcessedOutput`](#module_tokenizers..PostProcessedOutput)：`Object`'
- en: '[`~EncodingSingle`](#module_tokenizers..EncodingSingle) : `Object`'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~EncodingSingle`](#module_tokenizers..EncodingSingle)：`Object`'
- en: '[`~BatchEncoding`](#module_tokenizers..BatchEncoding) : `Array<number>` | `Array<Array<number>>`
    | `Tensor`'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~BatchEncoding`](#module_tokenizers..BatchEncoding)：`Array<number>` | `Array<Array<number>>`
    | `Tensor`'
- en: '[`~Message`](#module_tokenizers..Message) : `Object`'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~Message`](#module_tokenizers..Message)：`Object`'
- en: '* * *'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.TokenizerModel ⇐ <code> Callable </code>
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.TokenizerModel ⇐ <code> Callable </code>
- en: Abstract base class for tokenizer models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: tokenizer models的抽象基类。
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`tokenizers`](#module_tokenizers)的静态类'
- en: '**Extends**: `Callable`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**：`Callable`'
- en: '[.TokenizerModel](#module_tokenizers.TokenizerModel) ⇐ `Callable`'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.TokenizerModel](#module_tokenizers.TokenizerModel) ⇐ `Callable`'
- en: '[`new TokenizerModel(config)`](#new_module_tokenizers.TokenizerModel_new)'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new TokenizerModel(config)`](#new_module_tokenizers.TokenizerModel_new)'
- en: '*instance*'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例*'
- en: '[`.vocab`](#module_tokenizers.TokenizerModel+vocab) : `Array.<string>`'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.vocab`](#module_tokenizers.TokenizerModel+vocab)：`Array.<string>`'
- en: '[`.tokens_to_ids`](#module_tokenizers.TokenizerModel+tokens_to_ids) : `Map.<string,
    number>`'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers.TokenizerModel+tokens_to_ids)：`Map.<string,
    number>`'
- en: '[`.fuse_unk`](#module_tokenizers.TokenizerModel+fuse_unk) : `boolean`'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fuse_unk`](#module_tokenizers.TokenizerModel+fuse_unk)：`boolean`'
- en: '[`._call(tokens)`](#module_tokenizers.TokenizerModel+_call) ⇒ `Array.<string>`'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(tokens)`](#module_tokenizers.TokenizerModel+_call) ⇒ `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers.TokenizerModel+encode) ⇒ `Array.<string>`'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers.TokenizerModel+encode) ⇒ `Array.<string>`'
- en: '[`.convert_tokens_to_ids(tokens)`](#module_tokenizers.TokenizerModel+convert_tokens_to_ids)
    ⇒ `Array.<number>`'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.convert_tokens_to_ids(tokens)`](#module_tokenizers.TokenizerModel+convert_tokens_to_ids)
    ⇒ `Array.<number>`'
- en: '[`.convert_ids_to_tokens(ids)`](#module_tokenizers.TokenizerModel+convert_ids_to_tokens)
    ⇒ `Array.<string>`'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.convert_ids_to_tokens(ids)`](#module_tokenizers.TokenizerModel+convert_ids_to_tokens)
    ⇒ `Array.<string>`'
- en: '*static*'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*静态*'
- en: '[`.fromConfig(config, ...args)`](#module_tokenizers.TokenizerModel.fromConfig)
    ⇒ `TokenizerModel`'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.fromConfig(config, ...args)`](#module_tokenizers.TokenizerModel.fromConfig)
    ⇒ `TokenizerModel`'
- en: '* * *'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new TokenizerModel(config)
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: new TokenizerModel(config)
- en: Creates a new instance of TokenizerModel.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的TokenizerModel实例。
- en: '| Param | Type | Description |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for the TokenizerModel. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| config | `Object` | TokenizerModel的配置对象。|'
- en: '* * *'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'tokenizerModel.vocab : <code> Array. < string > </code>'
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel.vocab：<code> Array. < string > </code>
- en: '**Kind**: instance property of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例属性'
- en: '* * *'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'tokenizerModel.tokens_to_ids : <code> Map. < string, number > </code>'
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel.tokens_to_ids：<code> Map. < string, number > </code>
- en: A mapping of tokens to ids.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 将tokens映射到ids。
- en: '**Kind**: instance property of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例属性'
- en: '* * *'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'tokenizerModel.fuse_unk : <code> boolean </code>'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel.fuse_unk：<code> boolean </code>
- en: Whether to fuse unknown tokens when encoding. Defaults to false.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码时是否融合未知tokens。默认为false。
- en: '**Kind**: instance property of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例属性'
- en: '* * *'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizerModel._call(tokens) ⇒ <code> Array. < string > </code>
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel._call(tokens) ⇒ <code> Array. < string > </code>
- en: Internal function to call the TokenizerModel instance.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 调用TokenizerModel实例的内部函数。
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例方法'
- en: '**Returns**: `Array.<string>` - The encoded token IDs.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Array.<string>` - 编码后的token IDs。'
- en: '| Param | Type | Description |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| tokens | `Array.<string>` | 要编码的tokens。|'
- en: '* * *'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizerModel.encode(tokens) ⇒ <code> Array. < string > </code>
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel.encode(tokens) ⇒ <code> Array. < string > </code>
- en: Encodes a list of tokens into a list of token IDs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 将一组tokens编码为一组token IDs。
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例方法'
- en: '**Returns**: `Array.<string>` - The encoded tokens.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - 编码后的tokens。'
- en: '**Throws**:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**：'
- en: Will throw an error if not implemented in a subclass.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在子类中未实现，将会抛出错误。
- en: '| Param | Type | Description |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| tokens | `Array.<string>` | 要编码的tokens。|'
- en: '* * *'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizerModel.convert_tokens_to_ids(tokens) ⇒ <code> Array. < number > </code>
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel.convert_tokens_to_ids(tokens) ⇒ <code> Array. < number > </code>
- en: Converts a list of tokens into a list of token IDs.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 将一组tokens转换为一组token IDs。
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例方法'
- en: '**Returns**: `Array.<number>` - The converted token IDs.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<number>` - 转换后的token IDs。'
- en: '| Param | Type | Description |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokens | `Array.<string>` | The tokens to convert. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| tokens | `Array.<string>` | 要转换的tokens。|'
- en: '* * *'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizerModel.convert_ids_to_tokens(ids) ⇒ <code> Array. < string > </code>
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tokenizerModel.convert_ids_to_tokens(ids) ⇒ <code> Array. < string > </code>
- en: Converts a list of token IDs into a list of tokens.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 将一组token IDs转换为一组tokens。
- en: '**Kind**: instance method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的实例方法'
- en: '**Returns**: `Array.<string>` - The converted tokens.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - 转换后的tokens。'
- en: '| Param | Type | Description |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ids | `Array.<number>` | The token IDs to convert. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ids | `Array.<number>` | 要转换的token IDs。|'
- en: '* * *'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: TokenizerModel.fromConfig(config, ...args) ⇒ <code> TokenizerModel </code>
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TokenizerModel.fromConfig(config, ...args) ⇒ <code> TokenizerModel </code>
- en: Instantiates a new TokenizerModel instance based on the configuration object
    provided.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的配置对象实例化一个新的TokenizerModel实例。
- en: '**Kind**: static method of [`TokenizerModel`](#module_tokenizers.TokenizerModel)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`TokenizerModel`](#module_tokenizers.TokenizerModel)的静态方法'
- en: '**Returns**: `TokenizerModel` - A new instance of a TokenizerModel.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`TokenizerModel` - 一个TokenizerModel的新实例。'
- en: '**Throws**:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**：'
- en: Will throw an error if the TokenizerModel type in the config is not recognized.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果配置中的TokenizerModel类型未被识别，则会抛出错误。
- en: '| Param | Type | Description |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for the TokenizerModel. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| config | `Object` | TokenizerModel的配置对象。 |'
- en: '| ...args | `*` | Optional arguments to pass to the specific TokenizerModel
    constructor. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ...args | `*` | 传递给特定TokenizerModel构造函数的可选参数。 |'
- en: '* * *'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.PreTrainedTokenizer
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.PreTrainedTokenizer
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`tokenizers`](#module_tokenizers)的静态类'
- en: '[.PreTrainedTokenizer](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.PreTrainedTokenizer](#module_tokenizers.PreTrainedTokenizer)'
- en: '[`new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.PreTrainedTokenizer_new)'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.PreTrainedTokenizer_new)'
- en: '*instance*'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例*'
- en: '[`.added_tokens`](#module_tokenizers.PreTrainedTokenizer+added_tokens) : `Array.<AddedToken>`'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.added_tokens`](#module_tokenizers.PreTrainedTokenizer+added_tokens)：`Array.<AddedToken>`'
- en: '[`.remove_space`](#module_tokenizers.PreTrainedTokenizer+remove_space) : `boolean`'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.remove_space`](#module_tokenizers.PreTrainedTokenizer+remove_space)：`boolean`'
- en: '[`.padding_side`](#module_tokenizers.PreTrainedTokenizer+padding_side) : `’right’`
    | `’left’`'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.padding_side`](#module_tokenizers.PreTrainedTokenizer+padding_side)：`’right’`
    | `’left’`'
- en: '[`.getToken(...keys)`](#module_tokenizers.PreTrainedTokenizer+getToken) ⇒ `string`
    | `null`'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.getToken(...keys)`](#module_tokenizers.PreTrainedTokenizer+getToken)⇒`string`
    | `null`'
- en: '[`._call(text, options)`](#module_tokenizers.PreTrainedTokenizer+_call) ⇒ `BatchEncoding`'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._call(text, options)`](#module_tokenizers.PreTrainedTokenizer+_call)⇒`BatchEncoding`'
- en: '[`._encode_text(text)`](#module_tokenizers.PreTrainedTokenizer+_encode_text)
    ⇒ `Array<string>` | `null`'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._encode_text(text)`](#module_tokenizers.PreTrainedTokenizer+_encode_text)⇒`Array<string>`
    | `null`'
- en: '[`.encode(text, text_pair, options)`](#module_tokenizers.PreTrainedTokenizer+encode)
    ⇒ `Array.<number>`'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(text, text_pair, options)`](#module_tokenizers.PreTrainedTokenizer+encode)⇒`Array.<number>`'
- en: '[`.batch_decode(batch, decode_args)`](#module_tokenizers.PreTrainedTokenizer+batch_decode)
    ⇒ `Array.<string>`'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.batch_decode(batch, decode_args)`](#module_tokenizers.PreTrainedTokenizer+batch_decode)⇒`Array.<string>`'
- en: '[`.decode(token_ids, [decode_args])`](#module_tokenizers.PreTrainedTokenizer+decode)
    ⇒ `string`'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode(token_ids, [decode_args])`](#module_tokenizers.PreTrainedTokenizer+decode)⇒`string`'
- en: '[`.decode_single(token_ids, decode_args)`](#module_tokenizers.PreTrainedTokenizer+decode_single)
    ⇒ `string`'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode_single(token_ids, decode_args)`](#module_tokenizers.PreTrainedTokenizer+decode_single)⇒`string`'
- en: '[`.apply_chat_template(conversation, options)`](#module_tokenizers.PreTrainedTokenizer+apply_chat_template)
    ⇒ `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.apply_chat_template(conversation, options)`](#module_tokenizers.PreTrainedTokenizer+apply_chat_template)⇒`string`
    | `Tensor` | `Array<number>` | `Array<Array<number>>`'
- en: '*static*'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*静态*'
- en: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.PreTrainedTokenizer.from_pretrained)
    ⇒ `Promise.<PreTrainedTokenizer>`'
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.from_pretrained(pretrained_model_name_or_path, options)`](#module_tokenizers.PreTrainedTokenizer.from_pretrained)⇒`Promise.<PreTrainedTokenizer>`'
- en: '* * *'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)
- en: Create a new PreTrainedTokenizer instance.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的PreTrainedTokenizer实例。
- en: '| Param | Type | Description |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokenizerJSON | `Object` | The JSON of the tokenizer. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| tokenizerJSON | `Object` | tokenizer的JSON。 |'
- en: '| tokenizerConfig | `Object` | The config of the tokenizer. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| tokenizerConfig | `Object` | tokenizer的配置。 |'
- en: '* * *'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'preTrainedTokenizer.added_tokens : <code> Array. < AddedToken > </code>'
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.added_tokens：<code>Array.<AddedToken></code>
- en: '**Kind**: instance property of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例属性'
- en: '* * *'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'preTrainedTokenizer.remove_space : <code> boolean </code>'
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.remove_space：<code>boolean</code>
- en: Whether or not to strip the text when tokenizing (removing excess spaces before
    and after the string).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化时是否去除文本（删除字符串前后的多余空格）。
- en: '**Kind**: instance property of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例属性'
- en: '* * *'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'preTrainedTokenizer.padding_side : <code> ’ right ’ </code> | <code> ’ left
    ’ </code>'
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'preTrainedTokenizer.padding_side : <code>’right’</code> | <code>’left’</code>'
- en: '**Kind**: instance property of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例属性'
- en: '* * *'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer.getToken(...keys) ⇒ <code> string </code> | <code> null
    </code>
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.getToken(...keys)⇒<code>string</code> | <code>null</code>
- en: Returns the value of the first matching key in the tokenizer config object.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 返回tokenizer配置对象中第一个匹配键的值。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `string` | `null` - The value associated with the first matching
    key, or null if no match is found.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`string` | `null` - 与第一个匹配键关联的值，如果找不到匹配项则返回null。'
- en: '**Throws**:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**：'
- en: '`Error` If an object is found for a matching key and its __type property is
    not "AddedToken".'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Error` 如果找到匹配键的对象且其__type属性不是"AddedToken"。'
- en: '| Param | Type | Description |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ...keys | `string` | One or more keys to search for in the tokenizer config
    object. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| ...keys | `string` | 要在tokenizer配置对象中搜索的一个或多个键。 |'
- en: '* * *'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer._call(text, options) ⇒ <code> BatchEncoding </code>
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer._call(text, options)⇒<code>BatchEncoding</code>
- en: Encode/tokenize the given text(s).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定的文本进行编码/标记化。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `BatchEncoding` - Object to be passed to the model.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`BatchEncoding` - 传递给模型的对象。'
- en: '| Param | Type | Default | Description |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| text | `string` &#124; `Array<string>` |  | The text to tokenize. |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| text | `string` &#124; `Array<string>` |  | 要进行标记化的文本。 |'
- en: '| options | `Object` |  | An optional object containing the following properties:
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| options | `Object` |  | 包含以下属性的可选对象： |'
- en: '| [options.text_pair] | `string` &#124; `Array<string>` | `null` | Optional
    second sequence to be encoded. If set, must be the same type as text. |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| [options.text_pair] | `string` &#124; `Array<string>` | `null` | 要编码的可选第二个序列。如果设置，必须与文本相同类型。
    |'
- en: '| [options.padding] | `boolean` &#124; `''max_length''` | `false` | Whether
    to pad the input sequences. |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| [options.padding] | `boolean` &#124; `''max_length''` | `false` | 是否填充输入序列。
    |'
- en: '| [options.add_special_tokens] | `boolean` | `true` | Whether or not to add
    the special tokens associated with the corresponding model. |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| [options.add_special_tokens] | `boolean` | `true` | 是否添加与相应模型相关的特殊标记。 |'
- en: '| [options.truncation] | `boolean` |  | Whether to truncate the input sequences.
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| [options.truncation] | `boolean` |  | 是否截断输入序列。 |'
- en: '| [options.max_length] | `number` |  | Maximum length of the returned list
    and optionally padding length. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| [options.max_length] | `number` |  | 返回列表的最大长度和可选填充长度。 |'
- en: '| [options.return_tensor] | `boolean` | `true` | Whether to return the results
    as Tensors or arrays. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| [options.return_tensor] | `boolean` | `true` | 是否将结果返回为张量或数组。 |'
- en: '* * *'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer._encode_text(text) ⇒ <code> Array < string > </code> | <code>
    null </code>
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer._encode_text(text) ⇒ <code> Array < string > </code> | <code>
    null </code>
- en: Encodes a single text using the preprocessor pipeline of the tokenizer.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分词器的预处理流程对单个文本进行编码。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `Array<string>` | `null` - The encoded tokens.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Array<string>` | `null` - 编码的标记。'
- en: '| Param | Type | Description |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| text | `string` &#124; `null` | The text to encode. |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| text | `string` &#124; `null` | 要编码的文本。 |'
- en: '* * *'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer.encode(text, text_pair, options) ⇒ <code> Array. < number
    > </code>
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.encode(text, text_pair, options) ⇒ <code> Array. < number
    > </code>
- en: Encodes a single text or a pair of texts using the model’s tokenizer.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型的分词器对单个文本或一对文本进行编码。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `Array.<number>` - An array of token IDs representing the encoded
    text(s).'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Array.<number>` - 代表编码文本的标记ID的数组。'
- en: '| Param | Type | Default | Description |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| text | `string` |  | The text to encode. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| text | `string` |  | 要编码的文本。 |'
- en: '| text_pair | `string` &#124; `null` | `null` | The optional second text to
    encode. |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| text_pair | `string` &#124; `null` | `null` | 要编码的可选第二个文本。 |'
- en: '| options | `Object` |  | An optional object containing the following properties:
    |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| options | `Object` |  | 包含以下属性的可选对象： |'
- en: '| [options.add_special_tokens] | `boolean` | `true` | Whether or not to add
    the special tokens associated with the corresponding model. |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| [options.add_special_tokens] | `boolean` | `true` | 是否添加与相应模型相关的特殊标记。 |'
- en: '* * *'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer.batch_decode(batch, decode_args) ⇒ <code> Array. < string
    > </code>
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.batch_decode(batch, decode_args) ⇒ <code> Array. < string
    > </code>
- en: Decode a batch of tokenized sequences.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 解码一批标记化序列。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `Array.<string>` - List of decoded sequences.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Array.<string>` - 解码序列的列表。'
- en: '| Param | Type | Description |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| batch | `Array<Array<number>>` &#124; `Tensor` | List/Tensor of tokenized
    input sequences. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| batch | `Array<Array<number>>` &#124; `Tensor` | 标记化输入序列的列表/张量。 |'
- en: '| decode_args | `Object` | (Optional) Object with decoding arguments. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| decode_args | `Object` | (可选) 解码参数的对象。 |'
- en: '* * *'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer.decode(token_ids, [decode_args]) ⇒ <code> string </code>
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.decode(token_ids, [decode_args]) ⇒ <code> string </code>
- en: Decodes a sequence of token IDs back to a string.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记ID解码回字符串。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `string` - The decoded string.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `string` - 解码后的字符串。'
- en: '**Throws**:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**:'
- en: '`Error` If `token_ids` is not a non-empty array of integers.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`错误` 如果`token_ids`不是非空整数数组。'
- en: '| Param | Type | Default | Description |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| token_ids | `Array<number>` &#124; `Tensor` |  | List/Tensor of token IDs
    to decode. |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| token_ids | `Array<number>` &#124; `Tensor` |  | 要解码的标记ID列表/张量。 |'
- en: '| [decode_args] | `Object` | `{}` |  |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| [decode_args] | `Object` | `{}` |  |'
- en: '| [decode_args.skip_special_tokens] | `boolean` | `false` | If true, special
    tokens are removed from the output string. |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| [decode_args.skip_special_tokens] | `boolean` | `false` | 如果为true，则从输出字符串中删除特殊标记。
    |'
- en: '| [decode_args.clean_up_tokenization_spaces] | `boolean` | `true` | If true,
    spaces before punctuations and abbreviated forms are removed. |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| [decode_args.clean_up_tokenization_spaces] | `boolean` | `true` | 如果为true，则删除标点符号和缩写形式之前的空格。
    |'
- en: '* * *'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer.decode_single(token_ids, decode_args) ⇒ <code> string </code>
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.decode_single(token_ids, decode_args) ⇒ <code> string </code>
- en: Decode a single list of token ids to a string.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 将单个标记id列表解码为字符串。
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `string` - The decoded string'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `string` - 解码后的字符串'
- en: '| Param | Type | Default | Description |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| token_ids | `Array.<number>` |  | List of token ids to decode |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| token_ids | `Array.<number>` |  | 要解码的标记id列表 |'
- en: '| decode_args | `Object` |  | Optional arguments for decoding |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| decode_args | `Object` |  | 解码的可选参数 |'
- en: '| [decode_args.skip_special_tokens] | `boolean` | `false` | Whether to skip
    special tokens during decoding |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| [decode_args.skip_special_tokens] | `boolean` | `false` | 解码过程中是否跳过特殊标记 |'
- en: '| [decode_args.clean_up_tokenization_spaces] | `boolean` |  | Whether to clean
    up tokenization spaces during decoding. If null, the value is set to `this.decoder.cleanup`
    if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists,
    falling back to `true`. |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| [decode_args.clean_up_tokenization_spaces] | `boolean` |  | 是否在解码期间清除标记化空格。如果为null，则值将设置为`this.decoder.cleanup`（如果存在），否则将回退到`this.clean_up_tokenization_spaces`（如果存在），最后回退到`true`。
    |'
- en: '* * *'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: preTrainedTokenizer.apply_chat_template(conversation, options) ⇒ <code> string
    </code> | <code> Tensor </code> | <code> Array < number > </code> | <code> Array
    < Array < number > > </code>
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: preTrainedTokenizer.apply_chat_template(conversation, options) ⇒ <code> string
    </code> | <code> Tensor </code> | <code> Array < number > </code> | <code> Array
    < Array < number > > </code>
- en: Converts a list of message objects with `"role"` and `"content"` keys to a list
    of token ids. This method is intended for use with chat models, and will read
    the tokenizer’s chat_template attribute to determine the format and control tokens
    to use when converting. When chat_template is None, it will fall back to the default_chat_template
    specified at the class level.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 将具有`“role”`和`“content”`键的消息对象列表转换为标记ID列表。此方法旨在与聊天模型一起使用，并将读取分词器的`chat_template`属性以确定在转换时要使用的格式和控制标记。当`chat_template`为`None`时，将退回到在类级别指定的`default_chat_template`。
- en: See [here](https://huggingface.co/docs/transformers/chat_templating) for more
    information.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多信息，请访问[这里](https://huggingface.co/docs/transformers/chat_templating)。
- en: '**Example:** Applying a chat template to a conversation.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** 应用聊天模板到对话。'
- en: '[PRE1]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Kind**: instance method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的实例方法'
- en: '**Returns**: `string` | `Tensor` | `Array<number>` | `Array<Array<number>>`
    - The tokenized output.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `string` | `Tensor` | `Array<number>` | `Array<Array<number>>` - 标记化的输出。'
- en: '| Param | Type | Default | Description |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| conversation | `Array.<Message>` |  | A list of message objects with `"role"`
    and `"content"` keys. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| conversation | `Array.<Message>` |  | 具有`“role”`和`“content”`键的消息对象列表。 |'
- en: '| options | `Object` |  | An optional object containing the following properties:
    |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| options | `Object` |  | 包含以下属性的可选对象: |'
- en: '| [options.chat_template] | `string` | `null` | A Jinja template to use for
    this conversion. If this is not passed, the model''s default chat template will
    be used instead. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| [options.chat_template] | `string` | `null` | 用于此转换的Jinja模板。如果未传递此参数，则将使用模型的默认聊天模板。
    |'
- en: '| [options.add_generation_prompt] | `boolean` | `false` | Whether to end the
    prompt with the token(s) that indicate the start of an assistant message. This
    is useful when you want to generate a response from the model. Note that this
    argument will be passed to the chat template, and so it must be supported in the
    template for this argument to have any effect. |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| [options.add_generation_prompt] | `boolean` | `false` | 是否以指示助手消息开始的标记结束提示。当您想要从模型生成响应时，这很有用。请注意，此参数将传递给聊天模板，因此模板必须支持此参数才能产生任何效果。
    |'
- en: '| [options.tokenize] | `boolean` | `true` | Whether to tokenize the output.
    If false, the output will be a string. |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| [options.tokenize] | `boolean` | `true` | 是否对输出进行标记化。如果为false，则输出将是一个字符串。
    |'
- en: '| [options.padding] | `boolean` | `false` | Whether to pad sequences to the
    maximum length. Has no effect if tokenize is false. |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| [options.padding] | `boolean` | `false` | 是否将序列填充到最大长度。如果tokenize为false，则无效。
    |'
- en: '| [options.truncation] | `boolean` | `false` | Whether to truncate sequences
    to the maximum length. Has no effect if tokenize is false. |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| [options.truncation] | `boolean` | `false` | 是否将序列截断到最大长度。如果tokenize为false，则无效。
    |'
- en: '| [options.max_length] | `number` |  | Maximum length (in tokens) to use for
    padding or truncation. Has no effect if tokenize is false. If not specified, the
    tokenizer''s `max_length` attribute will be used as a default. |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| [options.max_length] | `number` |  | 用于填充或截断的最大长度（以标记为单位）。如果未指定，则将使用分词器的`max_length`属性作为默认值。
    |'
- en: '| [options.return_tensor] | `boolean` | `true` | Whether to return the output
    as a Tensor or an Array. Has no effect if tokenize is false. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| [options.return_tensor] | `boolean` | `true` | 是否将输出作为张量或数组返回。如果tokenize为false，则无效。
    |'
- en: '* * *'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: PreTrainedTokenizer.from_pretrained(pretrained_model_name_or_path, options)
    ⇒ <code> Promise. < PreTrainedTokenizer > </code>
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PreTrainedTokenizer.from_pretrained(pretrained_model_name_or_path, options)
    ⇒ <code> Promise. < PreTrainedTokenizer > </code>
- en: Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定的`pretrained_model_name_or_path`加载预训练的分词器。
- en: '**Kind**: static method of [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`PreTrainedTokenizer`](#module_tokenizers.PreTrainedTokenizer)的静态方法'
- en: '**Returns**: `Promise.<PreTrainedTokenizer>` - A new instance of the `PreTrainedTokenizer`
    class.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Promise.<PreTrainedTokenizer>` - `PreTrainedTokenizer`类的新实例。'
- en: '**Throws**:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**:'
- en: '`Error` Throws an error if the tokenizer.json or tokenizer_config.json files
    are not found in the `pretrained_model_name_or_path`.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在`pretrained_model_name_or_path`中找不到`tokenizer.json`或`tokenizer_config.json`文件，则抛出`Error`。
- en: '| Param | Type | Description |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| pretrained_model_name_or_path | `string` | The path to the pre-trained tokenizer.
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| pretrained_model_name_or_path | `string` | 预训练分词器的路径。 |'
- en: '| options | `PretrainedTokenizerOptions` | Additional options for loading the
    tokenizer. |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| options | `PretrainedTokenizerOptions` | 加载分词器的附加选项。 |'
- en: '* * *'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.BertTokenizer ⇐ <code> PreTrainedTokenizer </code>
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.BertTokenizer ⇐ <code> PreTrainedTokenizer </code>
- en: BertTokenizer is a class used to tokenize text for BERT models.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: BertTokenizer是用于为BERT模型标记化文本的类。
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`tokenizers`](#module_tokenizers)的静态类'
- en: '**Extends**: `PreTrainedTokenizer`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**: `PreTrainedTokenizer`'
- en: '* * *'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.AlbertTokenizer ⇐ <code> PreTrainedTokenizer </code>
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.AlbertTokenizer ⇐ <code> PreTrainedTokenizer </code>
- en: Albert tokenizer
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: Albert分词器
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`tokenizers`](#module_tokenizers)的静态类'
- en: '**Extends**: `PreTrainedTokenizer`'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**: `PreTrainedTokenizer`'
- en: '* * *'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.NllbTokenizer
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.NllbTokenizer
- en: The NllbTokenizer class is used to tokenize text for NLLB (“No Language Left
    Behind”) models.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: NllbTokenizer类用于为NLLB（“没有语言被遗忘”）模型对文本进行标记。
- en: No Language Left Behind (NLLB) is a first-of-its-kind, AI breakthrough project
    that open-sources models capable of delivering high-quality translations directly
    between any pair of 200+ languages — including low-resource languages like Asturian,
    Luganda, Urdu and more. It aims to help people communicate with anyone, anywhere,
    regardless of their language preferences. For more information, check out their
    [paper](https://arxiv.org/abs/2207.04672).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 没有语言被遗忘（NLLB）是一项首创性的人工智能项目，开源模型能够直接在200多种语言之间提供高质量的翻译 — 包括Asturian、Luganda、Urdu等低资源语言。它旨在帮助人们与任何人在任何地方进行交流，无论他们的语言偏好如何。有关更多信息，请查看他们的[论文](https://arxiv.org/abs/2207.04672)。
- en: For a list of supported languages (along with their language codes),
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 有关支持的语言列表（以及它们的语言代码），
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`tokenizers`](#module_tokenizers)的静态类'
- en: '**See**: [https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '**查看**：[https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)'
- en: '* * *'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: nllbTokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)
    ⇒ <code> Object </code>
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: nllbTokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)
    ⇒ <code> Object </code>
- en: Helper function to build translation inputs for an `NllbTokenizer`.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 用于为`NllbTokenizer`构建翻译输入的辅助函数。
- en: '**Kind**: instance method of [`NllbTokenizer`](#module_tokenizers.NllbTokenizer)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`NllbTokenizer`](#module_tokenizers.NllbTokenizer)的实例方法'
- en: '**Returns**: `Object` - Object to be passed to the model.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Object` - 传递给模型的对象。'
- en: '| Param | Type | Description |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| Param | Type | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| raw_inputs | `string` &#124; `Array<string>` | The text to tokenize. |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| raw_inputs | `string` &#124; `Array<string>` | 要标记的文本。 |'
- en: '| tokenizer_options | `Object` | Options to be sent to the tokenizer |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| tokenizer_options | `Object` | 发送到标记器的选项 |'
- en: '| generate_kwargs | `Object` | Generation options. |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| generate_kwargs | `Object` | 生成选项。 |'
- en: '* * *'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.M2M100Tokenizer
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.M2M100Tokenizer
- en: The M2M100Tokenizer class is used to tokenize text for M2M100 (“Many-to-Many”)
    models.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: M2M100Tokenizer类用于为M2M100（“多对多”）模型对文本进行标记。
- en: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many
    multilingual translation. It was introduced in this [paper](https://arxiv.org/abs/2010.11125)
    and first released in [this](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100)
    repository.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: M2M100是一个多语言编码器-解码器（seq-to-seq）模型，用于训练多对多多语言翻译。它在这篇[论文](https://arxiv.org/abs/2010.11125)中介绍，并首次在[此](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100)存储库中发布。
- en: For a list of supported languages (along with their language codes),
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 有关支持的语言列表（以及它们的语言代码），
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`tokenizers`](#module_tokenizers)的静态类'
- en: '**See**: [https://huggingface.co/facebook/m2m100_418M#languages-covered](https://huggingface.co/facebook/m2m100_418M#languages-covered)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '**查看**：[https://huggingface.co/facebook/m2m100_418M#languages-covered](https://huggingface.co/facebook/m2m100_418M#languages-covered)'
- en: '* * *'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: m2M100Tokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)
    ⇒ <code> Object </code>
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: m2M100Tokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)
    ⇒ <code> Object </code>
- en: Helper function to build translation inputs for an `M2M100Tokenizer`.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 用于为`M2M100Tokenizer`构建翻译输入的辅助函数。
- en: '**Kind**: instance method of [`M2M100Tokenizer`](#module_tokenizers.M2M100Tokenizer)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`M2M100Tokenizer`](#module_tokenizers.M2M100Tokenizer)的实例方法'
- en: '**Returns**: `Object` - Object to be passed to the model.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Object` - 传递给模型的对象。'
- en: '| Param | Type | Description |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| Param | Type | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| raw_inputs | `string` &#124; `Array<string>` | The text to tokenize. |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| raw_inputs | `string` &#124; `Array<string>` | 要标记的文本。 |'
- en: '| tokenizer_options | `Object` | Options to be sent to the tokenizer |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| tokenizer_options | `Object` | 发送到标记器的选项 |'
- en: '| generate_kwargs | `Object` | Generation options. |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| generate_kwargs | `Object` | 生成选项。 |'
- en: '* * *'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.WhisperTokenizer ⇐ <code> PreTrainedTokenizer </code>
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.WhisperTokenizer ⇐ <code> PreTrainedTokenizer </code>
- en: WhisperTokenizer tokenizer
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: WhisperTokenizer标记器
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`tokenizers`](#module_tokenizers)的静态类'
- en: '**Extends**: `PreTrainedTokenizer`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**：`PreTrainedTokenizer`'
- en: '[.WhisperTokenizer](#module_tokenizers.WhisperTokenizer) ⇐ `PreTrainedTokenizer`'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[.WhisperTokenizer](#module_tokenizers.WhisperTokenizer) ⇐ `PreTrainedTokenizer`'
- en: '[`._decode_asr(sequences, options)`](#module_tokenizers.WhisperTokenizer+_decode_asr)
    ⇒ `*`'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._decode_asr(sequences, options)`](#module_tokenizers.WhisperTokenizer+_decode_asr)
    ⇒ `*`'
- en: '[`.decode()`](#module_tokenizers.WhisperTokenizer+decode) : `*`'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.decode()`](#module_tokenizers.WhisperTokenizer+decode) : `*`'
- en: '[`.get_decoder_prompt_ids(options)`](#module_tokenizers.WhisperTokenizer+get_decoder_prompt_ids)
    ⇒ `Array.<Array<number>>`'
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.get_decoder_prompt_ids(options)`](#module_tokenizers.WhisperTokenizer+get_decoder_prompt_ids)
    ⇒ `Array.<Array<number>>`'
- en: '* * *'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: whisperTokenizer._decode_asr(sequences, options) ⇒ <code> * </code>
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: whisperTokenizer._decode_asr(sequences, options) ⇒ <code> * </code>
- en: Decodes automatic speech recognition (ASR) sequences.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 解码自动语音识别（ASR）序列。
- en: '**Kind**: instance method of [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)的实例方法'
- en: '**Returns**: `*` - The decoded sequences.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`*` - 解码后的序列。'
- en: '| Param | Type | Description |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| Param | Type | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| sequences | `*` | The sequences to decode. |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| sequences | `*` | 要解码的序列。 |'
- en: '| options | `Object` | The options to use for decoding. |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| options | `Object` | 用于解码的选项。 |'
- en: '* * *'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'whisperTokenizer.decode() : <code> * </code>'
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'whisperTokenizer.decode() : <code> * </code>'
- en: '**Kind**: instance method of [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`WhisperTokenizer`](#module_tokenizers)的实例方法'
- en: '* * *'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: whisperTokenizer.get_decoder_prompt_ids(options) ⇒ <code> Array. < Array < number
    > > </code>
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: whisperTokenizer.get_decoder_prompt_ids(options) ⇒ <code> Array. < Array < number
    > > </code>
- en: Helper function to build translation inputs for a `WhisperTokenizer`, depending
    on the language, task, and whether to predict timestamp tokens.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 构建`WhisperTokenizer`的翻译输入的辅助函数，取决于语言、任务以及是否要预测时间戳标记。
- en: Used to override the prefix tokens appended to the start of the label sequence.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 用于覆盖附加到标签序列开头的前缀标记。
- en: '**Example: Get ids for a language**'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：获取语言的id**'
- en: '[PRE2]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Kind**: instance method of [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`WhisperTokenizer`](#module_tokenizers.WhisperTokenizer)的实例方法'
- en: '**Returns**: `Array.<Array<number>>` - The decoder prompt ids.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Array.<Array<number>>` - 解码器提示的id。'
- en: '| Param | Type | Description |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| options | `Object` | Options to generate the decoder prompt. |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | `Object` | 生成解码器提示的选项。 |'
- en: '| [options.language] | `string` | The language of the transcription text. The
    corresponding language id token is appended to the start of the sequence for multilingual
    speech recognition and speech translation tasks, e.g. for "Spanish" the token
    "<&#124;es&#124;>" is appended to the start of sequence. |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| [options.language] | `string` | 转录文本的语言。对于多语言语音识别和语音翻译任务，相应的语言id标记会附加到序列的开头，例如对于"西班牙语"，标记"<|es|>"会附加到序列的开头。
    |'
- en: '| [options.task] | `string` | Task identifier to append at the start of sequence
    (if any). This should be used for mulitlingual fine-tuning, with "transcribe"
    for speech recognition and "translate" for speech translation. |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| [options.task] | `string` | 要附加到序列开头的任务标识符（如果有）。这应该用于多语言微调，使用"transcribe"进行语音识别和"translate"进行语音翻译。
    |'
- en: '| [options.no_timestamps] | `boolean` | Whether to add the <&#124;notimestamps&#124;>
    token at the start of the sequence. |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| [options.no_timestamps] | `boolean` | 是否在序列开头添加<|notimestamps|>标记。 |'
- en: '* * *'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.MarianTokenizer
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.MarianTokenizer
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`tokenizers`](#module_tokenizers)的静态类'
- en: '**Todo**'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**待办事项**'
- en: This model is not yet supported by Hugging Face’s “fast” tokenizers library
    ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).
    Therefore, this implementation (which is based on fast tokenizers) may produce
    slightly inaccurate results.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face的“快速”分词器库尚不支持此模型（[https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)）。因此，这个基于快速分词器的实现可能会产生略微不准确的结果。
- en: '[.MarianTokenizer](#module_tokenizers.MarianTokenizer)'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MarianTokenizer](#module_tokenizers.MarianTokenizer)'
- en: '[`new MarianTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.MarianTokenizer_new)'
  id: totrans-517
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new MarianTokenizer(tokenizerJSON, tokenizerConfig)`](#new_module_tokenizers.MarianTokenizer_new)'
- en: '[`._encode_text(text)`](#module_tokenizers.MarianTokenizer+_encode_text) ⇒
    `Array`'
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`._encode_text(text)`](#module_tokenizers.MarianTokenizer+_encode_text) ⇒
    `Array`'
- en: '* * *'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new MarianTokenizer(tokenizerJSON, tokenizerConfig)
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: new MarianTokenizer(tokenizerJSON, tokenizerConfig)
- en: Create a new MarianTokenizer instance.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的MarianTokenizer实例。
- en: '| Param | Type | Description |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokenizerJSON | `Object` | The JSON of the tokenizer. |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| tokenizerJSON | `Object` | 分词器的JSON。 |'
- en: '| tokenizerConfig | `Object` | The config of the tokenizer. |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| tokenizerConfig | `Object` | 分词器的配置。 |'
- en: '* * *'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: marianTokenizer._encode_text(text) ⇒ <code> Array </code>
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: marianTokenizer._encode_text(text) ⇒ <code> Array </code>
- en: Encodes a single text. Overriding this method is necessary since the language
    codes must be removed before encoding with sentencepiece model.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 编码单个文本。必须重写此方法，因为在使用sentencepiece模型进行编码之前必须删除语言代码。
- en: '**Kind**: instance method of [`MarianTokenizer`](#module_tokenizers.MarianTokenizer)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`MarianTokenizer`](#module_tokenizers.MarianTokenizer)的实例方法'
- en: '**Returns**: `Array` - The encoded tokens.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Array` - 编码后的标记。'
- en: '**See**: [https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213](https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213)'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '**查看**: [https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213](https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213)'
- en: '| Param | Type | Description |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| text | `string` &#124; `null` | The text to encode. |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| text | `string` &#124; `null` | 要编码的文本。 |'
- en: '* * *'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers.AutoTokenizer
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers.AutoTokenizer
- en: Helper class which is used to instantiate pretrained tokenizers with the `from_pretrained`
    function. The chosen tokenizer class is determined by the type specified in the
    tokenizer config.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用`from_pretrained`函数实例化预训练分词器的辅助类。所选的分词器类由分词器配置中指定的类型确定。
- en: '**Kind**: static class of [`tokenizers`](#module_tokenizers)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`tokenizers`](#module_tokenizers)的静态类'
- en: '* * *'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: AutoTokenizer.from_pretrained(pretrained_model_name_or_path, options) ⇒ <code>
    Promise. < PreTrainedTokenizer > </code>
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoTokenizer.from_pretrained(pretrained_model_name_or_path, options) ⇒ <code>
    Promise. < PreTrainedTokenizer > </code>
- en: Instantiate one of the tokenizer classes of the library from a pretrained model.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型实例化库中的一个分词器类。
- en: The tokenizer class to instantiate is selected based on the `tokenizer_class`
    property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path`
    if possible)
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 要实例化的分词器类是根据配置对象的`tokenizer_class`属性选择的（如果可能，作为参数传递或从`pretrained_model_name_or_path`加载）
- en: '**Kind**: static method of [`AutoTokenizer`](#module_tokenizers.AutoTokenizer)'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**: [`AutoTokenizer`](#module_tokenizers.AutoTokenizer)的静态方法'
- en: '**Returns**: `Promise.<PreTrainedTokenizer>` - A new instance of the PreTrainedTokenizer
    class.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**: `Promise.<PreTrainedTokenizer>` - PreTrainedTokenizer类的新实例。'
- en: '| Param | Type | Description |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| pretrained_model_name_or_path | `string` | The name or path of the pretrained
    model. Can be either:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '| pretrained_model_name_or_path | `string` | 预训练模型的名称或路径。可以是：'
- en: A string, the *model id* of a pretrained tokenizer hosted inside a model repo
    on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '| options | `PretrainedTokenizerOptions` | Additional options for loading the
    tokenizer. |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~AddedToken
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Represent a token added by the user on top of the existing Model vocabulary.
    AddedToken can be configured to specify the behavior they should have in various
    situations like:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Whether they should only match single words
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to include any whitespace on its left or right
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: new AddedToken(config)
  id: totrans-559
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of AddedToken.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` |  | Added token configuration object. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '| config.content | `string` |  | The content of the added token. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| config.id | `number` |  | The id of the added token. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| [config.single_word] | `boolean` | `false` | Whether this token must be a
    single word or can break words. |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| [config.lstrip] | `boolean` | `false` | Whether this token should strip whitespaces
    on its left. |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '| [config.rstrip] | `boolean` | `false` | Whether this token should strip whitespaces
    on its right. |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '| [config.normalized] | `boolean` | `false` | Whether this token should be
    normalized. |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| [config.special] | `boolean` | `false` | Whether this token is special. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WordPieceTokenizer ⇐ <code> TokenizerModel </code>
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A subclass of TokenizerModel that uses WordPiece encoding to encode tokens.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `TokenizerModel`'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '[~WordPieceTokenizer](#module_tokenizers..WordPieceTokenizer) ⇐ `TokenizerModel`'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WordPieceTokenizer(config)`](#new_module_tokenizers..WordPieceTokenizer_new)'
  id: totrans-577
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.tokens_to_ids`](#module_tokenizers..WordPieceTokenizer+tokens_to_ids) :
    `Map.<string, number>`'
  id: totrans-578
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.unk_token_id`](#module_tokenizers..WordPieceTokenizer+unk_token_id) : `number`'
  id: totrans-579
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.unk_token`](#module_tokenizers..WordPieceTokenizer+unk_token) : `string`'
  id: totrans-580
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.max_input_chars_per_word`](#module_tokenizers..WordPieceTokenizer+max_input_chars_per_word)
    : `number`'
  id: totrans-581
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.vocab`](#module_tokenizers..WordPieceTokenizer+vocab) : `Array.<string>`'
  id: totrans-582
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.encode(tokens)`](#module_tokenizers..WordPieceTokenizer+encode) ⇒ `Array.<string>`'
  id: totrans-583
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: new WordPieceTokenizer(config)
  id: totrans-585
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` |  | The configuration object. |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
- en: '| config.vocab | `Object` |  | A mapping of tokens to ids. |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
- en: '| config.unk_token | `string` |  | The unknown token string. |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
- en: '| config.continuing_subword_prefix | `string` |  | The prefix to use for continuing
    subwords. |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '| [config.max_input_chars_per_word] | `number` | `100` | The maximum number
    of characters per word. |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.tokens_to_ids : <code> Map. < string, number > </code>'
  id: totrans-594
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A mapping of tokens to ids.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.unk_token_id : <code> number </code>'
  id: totrans-598
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The id of the unknown token.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.unk_token : <code> string </code>'
  id: totrans-602
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The unknown token string.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.max_input_chars_per_word : <code> number </code>'
  id: totrans-606
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The maximum number of characters allowed per word.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceTokenizer.vocab : <code> Array. < string > </code>'
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An array of tokens.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: wordPieceTokenizer.encode(tokens) ⇒ <code> Array. < string > </code>
  id: totrans-614
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: wordPieceTokenizer.encode(tokens) ⇒ <code> Array. < string > </code>
- en: Encodes an array of tokens using WordPiece encoding.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 WordPiece 编码对标记数组进行编码。
- en: '**Kind**: instance method of [`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer)'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`WordPieceTokenizer`](#module_tokenizers..WordPieceTokenizer) 的实例方法'
- en: '**Returns**: `Array.<string>` - An array of encoded tokens.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - 编码的标记数组。'
- en: '| Param | Type | Description |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| tokens | `Array.<string>` | 要编码的标记。 |'
- en: '* * *'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers~Unigram ⇐ <code> TokenizerModel </code>
  id: totrans-622
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers~Unigram ⇐ <code> TokenizerModel </code>
- en: Class representing a Unigram tokenizer model.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 表示 Unigram 分词器模型的类。
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`tokenizers`](#module_tokenizers) 的内部类'
- en: '**Extends**: `TokenizerModel`'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**：`TokenizerModel`'
- en: '[~Unigram](#module_tokenizers..Unigram) ⇐ `TokenizerModel`'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 【~Unigram】(#module_tokenizers..Unigram) ⇐ `TokenizerModel`
- en: '[`new Unigram(config, moreConfig)`](#new_module_tokenizers..Unigram_new)'
  id: totrans-627
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new Unigram(config, moreConfig)`](#new_module_tokenizers..Unigram_new)'
- en: '[`.populateNodes(lattice)`](#module_tokenizers..Unigram+populateNodes)'
  id: totrans-628
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.populateNodes(lattice)`](#module_tokenizers..Unigram+populateNodes)'
- en: '[`.tokenize(normalized)`](#module_tokenizers..Unigram+tokenize) ⇒ `Array.<string>`'
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokenize(normalized)`](#module_tokenizers..Unigram+tokenize) ⇒ `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers..Unigram+encode) ⇒ `Array.<string>`'
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers..Unigram+encode) ⇒ `Array.<string>`'
- en: '* * *'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new Unigram(config, moreConfig)
  id: totrans-632
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: new Unigram(config, moreConfig)
- en: Create a new Unigram tokenizer model.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的 Unigram 分词器模型。
- en: '| Param | Type | Description |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for the Unigram model. |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | `Object` | Unigram 模型的配置对象。 |'
- en: '| config.unk_id | `number` | The ID of the unknown token |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| config.unk_id | `number` | 未知标记的 ID |'
- en: '| config.vocab | `Array.<Array<any>>` | A 2D array representing a mapping of
    tokens to scores. |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| config.vocab | `Array.<Array<any>>` | 表示标记到分数的映射的二维数组。 |'
- en: '| moreConfig | `Object` | Additional configuration object for the Unigram model.
    |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| moreConfig | `Object` | Unigram 模型的附加配置对象。 |'
- en: '* * *'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: unigram.populateNodes(lattice)
  id: totrans-641
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: unigram.populateNodes(lattice)
- en: Populates lattice nodes.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 填充格节点。
- en: '**Kind**: instance method of [`Unigram`](#module_tokenizers..Unigram)'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`Unigram`](#module_tokenizers..Unigram) 的实例方法'
- en: '| Param | Type | Description |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| lattice | `TokenLattice` | The token lattice to populate with nodes. |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| lattice | `TokenLattice` | 要填充节点的标记格。 |'
- en: '* * *'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: unigram.tokenize(normalized) ⇒ <code> Array. < string > </code>
  id: totrans-648
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: unigram.tokenize(normalized) ⇒ <code> Array. < string > </code>
- en: Encodes an array of tokens into an array of subtokens using the unigram model.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 unigram 模型将标记数组编码为子标记数组。
- en: '**Kind**: instance method of [`Unigram`](#module_tokenizers..Unigram)'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`Unigram`](#module_tokenizers..Unigram) 的实例方法'
- en: '**Returns**: `Array.<string>` - An array of subtokens obtained by encoding
    the input tokens using the unigram model.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - 通过使用 unigram 模型对输入标记进行编码获得的子标记数组。'
- en: '| Param | Type | Description |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| normalized | `string` | The normalized string. |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| normalized | `string` | 规范化的字符串。 |'
- en: '* * *'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: unigram.encode(tokens) ⇒ <code> Array. < string > </code>
  id: totrans-656
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: unigram.encode(tokens) ⇒ <code> Array. < string > </code>
- en: Encodes an array of tokens using Unigram encoding.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Unigram 编码对标记数组进行编码。
- en: '**Kind**: instance method of [`Unigram`](#module_tokenizers..Unigram)'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`Unigram`](#module_tokenizers..Unigram) 的实例方法'
- en: '**Returns**: `Array.<string>` - An array of encoded tokens.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - 编码的标记数组。'
- en: '| Param | Type | Description |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokens | `Array.<string>` | The tokens to encode. |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| tokens | `Array.<string>` | 要编码的标记。 |'
- en: '* * *'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers~BPE ⇐ <code> TokenizerModel </code>
  id: totrans-664
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers~BPE ⇐ <code> TokenizerModel </code>
- en: BPE class for encoding text into Byte-Pair-Encoding (BPE) tokens.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将文本编码为字节对编码（BPE）标记的 BPE 类。
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`tokenizers`](#module_tokenizers) 的内部类'
- en: '**Extends**: `TokenizerModel`'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**：`TokenizerModel`'
- en: '[~BPE](#module_tokenizers..BPE) ⇐ `TokenizerModel`'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 【~BPE】(#module_tokenizers..BPE) ⇐ `TokenizerModel`
- en: '[`new BPE(config)`](#new_module_tokenizers..BPE_new)'
  id: totrans-669
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new BPE(config)`](#new_module_tokenizers..BPE_new)'
- en: '[`.tokens_to_ids`](#module_tokenizers..BPE+tokens_to_ids) : `Map.<string, number>`'
  id: totrans-670
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers..BPE+tokens_to_ids) : `Map.<string, number>`'
- en: '[`.cache`](#module_tokenizers..BPE+cache) : `Map.<string, Array<string>>`'
  id: totrans-671
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.cache`](#module_tokenizers..BPE+cache) : `Map.<string, Array<string>>`'
- en: '[`.bpe(token)`](#module_tokenizers..BPE+bpe) ⇒ `Array.<string>`'
  id: totrans-672
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.bpe(token)`](#module_tokenizers..BPE+bpe) ⇒ `Array.<string>`'
- en: '[`.encode(tokens)`](#module_tokenizers..BPE+encode) ⇒ `Array.<string>`'
  id: totrans-673
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.encode(tokens)`](#module_tokenizers..BPE+encode) ⇒ `Array.<string>`'
- en: '* * *'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new BPE(config)
  id: totrans-675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: new BPE(config)
- en: Create a BPE instance.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 BPE 实例。
- en: '| Param | Type | Description |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for BPE. |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | `Object` | BPE 的配置对象。 |'
- en: '| config.vocab | `Object` | A mapping of tokens to ids. |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| config.vocab | `Object` | 将标记映射到 ID 的映射。 |'
- en: '| config.unk_token | `string` | The unknown token used for out of vocabulary
    words. |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| config.unk_token | `string` | 用于词汇表外单词的未知标记。 |'
- en: '| config.end_of_word_suffix | `string` | The suffix to place at the end of
    each word. |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| config.end_of_word_suffix | `string` | 放在每个单词末尾的后缀。 |'
- en: '| [config.continuing_subword_suffix] | `string` | The suffix to insert between
    words. |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| [config.continuing_subword_suffix] | `string` | 插入在单词之间的后缀。 |'
- en: '| config.merges | `Array` | An array of BPE merges as strings. |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| config.merges | `Array` | 作为字符串的 BPE 合并数组。 |'
- en: '* * *'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'bpE.tokens_to_ids : <code> Map. < string, number > </code>'
  id: totrans-686
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'bpE.tokens_to_ids : <code> Map. < string, number > </code>'
- en: '**Kind**: instance property of [`BPE`](#module_tokenizers..BPE)'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`BPE`](#module_tokenizers..BPE) 的实例属性'
- en: '* * *'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'bpE.cache : <code> Map. < string, Array < string > > </code>'
  id: totrans-689
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'bpE.cache : <code> Map. < string, Array < string > > </code>'
- en: '**Kind**: instance property of [`BPE`](#module_tokenizers..BPE)'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：[`BPE`](#module_tokenizers..BPE) 的实例属性'
- en: '* * *'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bpE.bpe(token) ⇒ <code> Array. < string > </code>
  id: totrans-692
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bpE.bpe(token) ⇒ <code> Array. < string > </code>
- en: Apply Byte-Pair-Encoding (BPE) to a given token. Efficient heap-based priority
    queue implementation adapted from [https://github.com/belladoreai/llama-tokenizer-js](https://github.com/belladoreai/llama-tokenizer-js).
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 将字节对编码（BPE）应用于给定的标记。高效的基于堆的优先级队列实现，改编自[https://github.com/belladoreai/llama-tokenizer-js](https://github.com/belladoreai/llama-tokenizer-js)。
- en: '**Kind**: instance method of [`BPE`](#module_tokenizers..BPE)'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`BPE`](#module_tokenizers..BPE)的实例方法'
- en: '**Returns**: `Array.<string>` - The BPE encoded tokens.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - BPE编码的标记。'
- en: '| Param | Type | Description |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| token | `string` | The token to encode. |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| token | `string` | 要编码的标记。 |'
- en: '* * *'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bpE.encode(tokens) ⇒ <code> Array. < string > </code>
  id: totrans-700
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bpE.encode(tokens) ⇒ <code> Array. < string > </code>
- en: Encodes the input sequence of tokens using the BPE algorithm and returns the
    resulting subword tokens.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入的标记序列使用BPE算法进行编码，并返回生成的子词标记。
- en: '**Kind**: instance method of [`BPE`](#module_tokenizers..BPE)'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`BPE`](#module_tokenizers..BPE)的实例方法'
- en: '**Returns**: `Array.<string>` - The resulting subword tokens after applying
    the BPE algorithm to the input sequence of tokens.'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Array.<string>` - 对输入的标记序列应用BPE算法后得到的子词标记。'
- en: '| Param | Type | Description |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| tokens | `Array.<string>` | The input sequence of tokens to encode. |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| tokens | `Array.<string>` | 要编码的输入标记序列。 |'
- en: '* * *'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers~LegacyTokenizerModel
  id: totrans-708
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers~LegacyTokenizerModel
- en: Legacy tokenizer class for tokenizers with only a vocabulary.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 用于仅具有词汇表的标记器的传统标记器类。
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`tokenizers`](#module_tokenizers)的内部类'
- en: '[~LegacyTokenizerModel](#module_tokenizers..LegacyTokenizerModel)'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[~LegacyTokenizerModel](#module_tokenizers..LegacyTokenizerModel)'
- en: '[`new LegacyTokenizerModel(config, moreConfig)`](#new_module_tokenizers..LegacyTokenizerModel_new)'
  id: totrans-712
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`new LegacyTokenizerModel(config, moreConfig)`](#new_module_tokenizers..LegacyTokenizerModel_new)'
- en: '[`.tokens_to_ids`](#module_tokenizers..LegacyTokenizerModel+tokens_to_ids)
    : `Map.<string, number>`'
  id: totrans-713
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`.tokens_to_ids`](#module_tokenizers..LegacyTokenizerModel+tokens_to_ids)
    : `Map.<string, number>`'
- en: '* * *'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new LegacyTokenizerModel(config, moreConfig)
  id: totrans-715
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新的LegacyTokenizerModel(config, moreConfig)
- en: Create a LegacyTokenizerModel instance.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个LegacyTokenizerModel实例。
- en: '| Param | Type | Description |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for LegacyTokenizerModel. |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| config | `Object` | LegacyTokenizerModel的配置对象。 |'
- en: '| config.vocab | `Object` | A (possibly nested) mapping of tokens to ids. |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| config.vocab | `Object` | 一个（可能是嵌套的）将标记映射到ID的映射。 |'
- en: '| moreConfig | `Object` | Additional configuration object for the LegacyTokenizerModel
    model. |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '| moreConfig | `Object` | 为LegacyTokenizerModel模型提供额外的配置对象。 |'
- en: '* * *'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'legacyTokenizerModel.tokens_to_ids : <code> Map. < string, number > </code>'
  id: totrans-723
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'legacyTokenizerModel.tokens_to_ids : <code> Map. < string, number > </code>'
- en: '**Kind**: instance property of [`LegacyTokenizerModel`](#module_tokenizers..LegacyTokenizerModel)'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`LegacyTokenizerModel`](#module_tokenizers..LegacyTokenizerModel)的实例属性'
- en: '* * *'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers~Normalizer
  id: totrans-726
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers~Normalizer
- en: A base class for text normalization.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 文本规范化的基类。
- en: '**Kind**: inner abstract class of [`tokenizers`](#module_tokenizers)'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`tokenizers`](#module_tokenizers)的内部抽象类'
- en: '*[~Normalizer](#module_tokenizers..Normalizer)*'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[~Normalizer](#module_tokenizers..Normalizer)*'
- en: '*[`new Normalizer(config)`](#new_module_tokenizers..Normalizer_new)*'
  id: totrans-730
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`new Normalizer(config)`](#new_module_tokenizers..Normalizer_new)*'
- en: '*instance*'
  id: totrans-731
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例*'
- en: '**[`.normalize(text)`](#module_tokenizers..Normalizer+normalize) ⇒ `string`**'
  id: totrans-732
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[`.normalize(text)`](#module_tokenizers..Normalizer+normalize) ⇒ `string`**'
- en: '*[`._call(text)`](#module_tokenizers..Normalizer+_call) ⇒ `string`*'
  id: totrans-733
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`._call(text)`](#module_tokenizers..Normalizer+_call) ⇒ `string`*'
- en: '*static*'
  id: totrans-734
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*静态*'
- en: '*[`.fromConfig(config)`](#module_tokenizers..Normalizer.fromConfig) ⇒ `Normalizer`*'
  id: totrans-735
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[`.fromConfig(config)`](#module_tokenizers..Normalizer.fromConfig) ⇒ `Normalizer`*'
- en: '* * *'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: new Normalizer(config)
  id: totrans-737
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新的Normalizer(config)
- en: '| Param | Type | Description |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for the normalizer. |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '| config | `Object` | 规范化器的配置对象。 |'
- en: '* * *'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: normalizer.normalize(text) ⇒ <code> string </code>
  id: totrans-742
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: normalizer.normalize(text) ⇒ <code> string </code>
- en: Normalize the input text.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化输入文本。
- en: '**Kind**: instance abstract method of [`Normalizer`](#module_tokenizers..Normalizer)'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`Normalizer`](#module_tokenizers..Normalizer)的实例抽象方法'
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`string` - 规范化后的文本。'
- en: '**Throws**:'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**：'
- en: '`Error` If this method is not implemented in a subclass.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`错误` 如果在子类中未实现此方法。'
- en: '| Param | Type | Description |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| text | `string` | The text to normalize. |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| text | `string` | 要规范化的文本。 |'
- en: '* * *'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: normalizer._call(text) ⇒ <code> string </code>
  id: totrans-752
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: normalizer._call(text) ⇒ <code> string </code>
- en: Alias for [Normalizer#normalize](Normalizer#normalize).
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 别名为[Normalizer#normalize](Normalizer#normalize)。
- en: '**Kind**: instance method of [`Normalizer`](#module_tokenizers..Normalizer)'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`Normalizer`](#module_tokenizers..Normalizer)的实例方法'
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`string` - 规范化后的文本。'
- en: '| Param | Type | Description |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| text | `string` | The text to normalize. |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| text | `string` | 要规范化的文本。 |'
- en: '* * *'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Normalizer.fromConfig(config) ⇒ <code> Normalizer </code>
  id: totrans-760
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Normalizer.fromConfig(config) ⇒ <code> Normalizer </code>
- en: Factory method for creating normalizers from config objects.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 从配置对象创建规范化器的工厂方法。
- en: '**Kind**: static method of [`Normalizer`](#module_tokenizers..Normalizer)'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`Normalizer`](#module_tokenizers..Normalizer)的静态方法'
- en: '**Returns**: `Normalizer` - A Normalizer object.'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：`Normalizer` - 一个Normalizer对象。'
- en: '**Throws**:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '**抛出**：'
- en: '`Error` If an unknown Normalizer type is specified in the config.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`错误` 如果在配置中指定了未知的规范化器类型。'
- en: '| Param | Type | Description |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| config | `Object` | The configuration object for the normalizer. |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| config | `Object` | 规范化器的配置对象。 |'
- en: '* * *'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: tokenizers~Replace ⇐ <code> Normalizer </code>
  id: totrans-770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tokenizers~Replace ⇐ <code> Normalizer </code>
- en: Replace normalizer that replaces occurrences of a pattern with a given string
    or regular expression.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 替换规范化器，用给定的字符串或正则表达式替换模式的出现。
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: '**种类**：[`tokenizers`](#module_tokenizers)的内部类'
- en: '**Extends**: `Normalizer`'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展**：`Normalizer`'
- en: '* * *'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: replace.normalize(text) ⇒ <code> string </code>
  id: totrans-775
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: replace.normalize(text) ⇒ <code> string </code>
- en: Normalize the input text by replacing the pattern with the content.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Replace`](#module_tokenizers..Replace)'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text after replacing the pattern with
    the content.'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text to be normalized. |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NFC ⇐ <code> Normalizer </code>
  id: totrans-783
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A normalizer that applies Unicode normalization form C (NFC) to the input text.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: nfC.normalize(text) ⇒ <code> string </code>
  id: totrans-788
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize the input text by applying Unicode normalization form C (NFC).
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NFC`](#module_tokenizers..NFC)'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text to be normalized. |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NFKC ⇐ <code> Normalizer </code>
  id: totrans-796
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NFKC Normalizer.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: nfkC.normalize(text) ⇒ <code> string </code>
  id: totrans-801
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize text using NFKC normalization.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NFKC`](#module_tokenizers..NFKC)'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-805
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be normalized. |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NFKD ⇐ <code> Normalizer </code>
  id: totrans-809
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NFKD Normalizer.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: nfkD.normalize(text) ⇒ <code> string </code>
  id: totrans-814
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalize text using NFKD normalization.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NFKD`](#module_tokenizers..NFKD)'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be normalized. |'
  id: totrans-820
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~StripNormalizer
  id: totrans-822
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A normalizer that strips leading and/or trailing whitespace from the input text.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: stripNormalizer.normalize(text) ⇒ <code> string </code>
  id: totrans-826
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Strip leading and/or trailing whitespace from the input text.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`StripNormalizer`](#module_tokenizers..StripNormalizer)'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text. |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~StripAccents ⇐ <code> Normalizer </code>
  id: totrans-834
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: StripAccents normalizer removes all accents from the text.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: stripAccents.normalize(text) ⇒ <code> string </code>
  id: totrans-839
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remove all accents from the text.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`StripAccents`](#module_tokenizers..StripAccents)'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text without accents.'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text. |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Lowercase ⇐ <code> Normalizer </code>
  id: totrans-847
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Normalizer that lowercases the input string.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: lowercase.normalize(text) ⇒ <code> string </code>
  id: totrans-852
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lowercases the input string.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Lowercase`](#module_tokenizers..Lowercase)'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  id: totrans-858
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Prepend ⇐ <code> Normalizer </code>
  id: totrans-860
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Normalizer that prepends a string to the input string.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: prepend.normalize(text) ⇒ <code> string </code>
  id: totrans-865
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prepends the input string.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Prepend`](#module_tokenizers..Prepend)'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  id: totrans-871
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~NormalizerSequence ⇐ <code> Normalizer </code>
  id: totrans-873
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Normalizer that applies a sequence of Normalizers.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: '[~NormalizerSequence](#module_tokenizers..NormalizerSequence) ⇐ `Normalizer`'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new NormalizerSequence(config)`](#new_module_tokenizers..NormalizerSequence_new)'
  id: totrans-878
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..NormalizerSequence+normalize) ⇒ `string`'
  id: totrans-879
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: new NormalizerSequence(config)
  id: totrans-881
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new instance of NormalizerSequence.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
- en: '| config.normalizers | `Array.<Object>` | An array of Normalizer configuration
    objects. |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
- en: normalizerSequence.normalize(text) ⇒ <code> string </code>
  id: totrans-888
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apply a sequence of Normalizers to the input text.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`NormalizerSequence`](#module_tokenizers..NormalizerSequence)'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BertNormalizer ⇐ <code> Normalizer </code>
  id: totrans-896
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A class representing a normalizer used in BERT tokenization.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: '[~BertNormalizer](#module_tokenizers..BertNormalizer) ⇐ `Normalizer`'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._tokenize_chinese_chars(text)`](#module_tokenizers..BertNormalizer+_tokenize_chinese_chars)
    ⇒ `string`'
  id: totrans-901
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._is_chinese_char(cp)`](#module_tokenizers..BertNormalizer+_is_chinese_char)
    ⇒ `boolean`'
  id: totrans-902
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.stripAccents(text)`](#module_tokenizers..BertNormalizer+stripAccents) ⇒
    `string`'
  id: totrans-903
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..BertNormalizer+normalize) ⇒ `string`'
  id: totrans-904
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer._tokenize_chinese_chars(text) ⇒ <code> string </code>
  id: totrans-906
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the
    input text.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The tokenized text with whitespace added around CJK
    characters.'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-910
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-911
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The input text to tokenize. |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer._is_chinese_char(cp) ⇒ <code> boolean </code>
  id: totrans-914
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Checks whether the given Unicode codepoint represents a CJK (Chinese, Japanese,
    or Korean) character.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: 'A “chinese character” is defined as anything in the CJK Unicode block: [https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)](https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block))'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: Note that the CJK Unicode block is NOT all Japanese and Korean characters, despite
    its name. The modern Korean Hangul alphabet is a different block, as is Japanese
    Hiragana and Katakana. Those alphabets are used to write space-separated words,
    so they are not treated specially and are handled like all other languages.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `boolean` - True if the codepoint represents a CJK character,
    false otherwise.'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-920
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-921
  prefs: []
  type: TYPE_TB
- en: '| cp | `number` | The Unicode codepoint to check. |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer.stripAccents(text) ⇒ <code> string </code>
  id: totrans-924
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Strips accents from the given text.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The text with accents removed.'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to strip accents from. |'
  id: totrans-930
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: bertNormalizer.normalize(text) ⇒ <code> string </code>
  id: totrans-932
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalizes the given text based on the configuration.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertNormalizer`](#module_tokenizers..BertNormalizer)'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PreTokenizer ⇐ <code> Callable </code>
  id: totrans-940
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A callable class representing a pre-tokenizer used in tokenization. Subclasses
    should implement the `pre_tokenize_text` method to define the specific pre-tokenization
    logic.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
- en: '[~PreTokenizer](#module_tokenizers..PreTokenizer) ⇐ `Callable`'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  id: totrans-945
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`*'
  id: totrans-946
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize(text, [options])`](#module_tokenizers..PreTokenizer+pre_tokenize)
    ⇒ `Array.<string>`'
  id: totrans-947
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(text, [options])`](#module_tokenizers..PreTokenizer+_call) ⇒ `Array.<string>`'
  id: totrans-948
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  id: totrans-949
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..PreTokenizer.fromConfig) ⇒ `PreTokenizer`'
  id: totrans-950
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string > </code>
  id: totrans-952
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Method that should be implemented by subclasses to define the specific pre-tokenization
    logic.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance abstract method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The pre-tokenized text.'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the method is not implemented in the subclass.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to pre-tokenize. |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-961
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizer.pre_tokenize(text, [options]) ⇒ <code> Array. < string > </code>
  id: totrans-963
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes the given text into pre-tokens.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of pre-tokens.'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `Array<string>` | The text or array of texts to pre-tokenize.
    |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizer._call(text, [options]) ⇒ <code> Array. < string > </code>
  id: totrans-972
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alias for [PreTokenizer#pre_tokenize](PreTokenizer#pre_tokenize).
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of pre-tokens.'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
- en: '| text | `string` &#124; `Array<string>` | The text or array of texts to pre-tokenize.
    |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-979
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: PreTokenizer.fromConfig(config) ⇒ <code> PreTokenizer </code>
  id: totrans-981
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factory method that returns an instance of a subclass of `PreTokenizer` based
    on the provided configuration.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`PreTokenizer`](#module_tokenizers..PreTokenizer)'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PreTokenizer` - An instance of a subclass of `PreTokenizer`.'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the provided configuration object does not correspond to any known
    pre-tokenizer.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | A configuration object for the pre-tokenizer. |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BertPreTokenizer ⇐ <code> PreTokenizer </code>
  id: totrans-991
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: '[~BertPreTokenizer](#module_tokenizers..BertPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BertPreTokenizer(config)`](#new_module_tokenizers..BertPreTokenizer_new)'
  id: totrans-995
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..BertPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-996
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: new BertPreTokenizer(config)
  id: totrans-998
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A PreTokenizer that splits text into wordpieces using a basic tokenization scheme
    similar to that used in the original implementation of BERT.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: bertPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1004
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes a single text using the BERT pre-tokenization scheme.
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertPreTokenizer`](#module_tokenizers..BertPreTokenizer)'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ByteLevelPreTokenizer ⇐ <code> PreTokenizer </code>
  id: totrans-1013
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pre-tokenizer that splits text into Byte-Pair-Encoding (BPE) subwords.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: '[~ByteLevelPreTokenizer](#module_tokenizers..ByteLevelPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ByteLevelPreTokenizer(config)`](#new_module_tokenizers..ByteLevelPreTokenizer_new)'
  id: totrans-1018
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.add_prefix_space`](#module_tokenizers..ByteLevelPreTokenizer+add_prefix_space)
    : `boolean`'
  id: totrans-1019
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.trim_offsets`](#module_tokenizers..ByteLevelPreTokenizer+trim_offsets) :
    `boolean`'
  id: totrans-1020
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.use_regex`](#module_tokenizers..ByteLevelPreTokenizer+use_regex) : `boolean`'
  id: totrans-1021
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ByteLevelPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1022
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: new ByteLevelPreTokenizer(config)
  id: totrans-1024
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of the `ByteLevelPreTokenizer` class.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelPreTokenizer.add_prefix_space : <code> boolean </code>'
  id: totrans-1030
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether to add a leading space to the first word.This allows to treat the leading
    word just as any other word.
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelPreTokenizer.trim_offsets : <code> boolean </code>'
  id: totrans-1034
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether the post processing step should trim offsetsto avoid including whitespaces.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
- en: '**Todo**'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: Use this in the pretokenization step.
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelPreTokenizer.use_regex : <code> boolean </code>'
  id: totrans-1040
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether to use the standard GPT2 regex for whitespace splitting.Set it to False
    if you want to use your own splitting. Defaults to true.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
- en: byteLevelPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1044
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes a single piece of text using byte-level tokenization.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelPreTokenizer`](#module_tokenizers..ByteLevelPreTokenizer)'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1051
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~SplitPreTokenizer ⇐ <code> PreTokenizer </code>
  id: totrans-1053
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text using a given pattern.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
- en: '[~SplitPreTokenizer](#module_tokenizers..SplitPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new SplitPreTokenizer(config)`](#new_module_tokenizers..SplitPreTokenizer_new)'
  id: totrans-1058
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..SplitPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1059
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
- en: new SplitPreTokenizer(config)
  id: totrans-1061
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
- en: '| config.pattern | `Object` | The pattern used to split the text. Can be a
    string or a regex object. |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
- en: '| config.pattern.String | `string` &#124; `undefined` | The string to use for
    splitting. Only defined if the pattern is a string. |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
- en: '| config.pattern.Regex | `string` &#124; `undefined` | The regex to use for
    splitting. Only defined if the pattern is a regex. |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
- en: '| config.behavior | `SplitDelimiterBehavior` | The behavior to use when splitting.
    |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
- en: '| config.invert | `boolean` | Whether to split (invert=false) or match (invert=true)
    the pattern. |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: splitPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1071
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes text by splitting it using the given pattern.
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`SplitPreTokenizer`](#module_tokenizers..SplitPreTokenizer)'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PunctuationPreTokenizer ⇐ <code> PreTokenizer </code>
  id: totrans-1080
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text based on punctuation.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: '[~PunctuationPreTokenizer](#module_tokenizers..PunctuationPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PunctuationPreTokenizer(config)`](#new_module_tokenizers..PunctuationPreTokenizer_new)'
  id: totrans-1085
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PunctuationPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1086
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: new PunctuationPreTokenizer(config)
  id: totrans-1088
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1090
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  id: totrans-1091
  prefs: []
  type: TYPE_TB
- en: '| config.behavior | `SplitDelimiterBehavior` | The behavior to use when splitting.
    |'
  id: totrans-1092
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
- en: punctuationPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. <
    string > </code>
  id: totrans-1094
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes text by splitting it using the given pattern.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PunctuationPreTokenizer`](#module_tokenizers..PunctuationPreTokenizer)'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1098
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~DigitsPreTokenizer ⇐ <code> PreTokenizer </code>
  id: totrans-1103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text based on digits.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
- en: '[~DigitsPreTokenizer](#module_tokenizers..DigitsPreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new DigitsPreTokenizer(config)`](#new_module_tokenizers..DigitsPreTokenizer_new)'
  id: totrans-1108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..DigitsPreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: new DigitsPreTokenizer(config)
  id: totrans-1111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
- en: '| config.individual_digits | `boolean` | Whether to split on individual digits.
    |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: digitsPreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokenizes text by splitting it using the given pattern.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`DigitsPreTokenizer`](#module_tokenizers..DigitsPreTokenizer)'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens.'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to tokenize. |'
  id: totrans-1123
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PostProcessor ⇐ <code> Callable </code>
  id: totrans-1126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
- en: '[~PostProcessor](#module_tokenizers..PostProcessor) ⇐ `Callable`'
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PostProcessor(config)`](#new_module_tokenizers..PostProcessor_new)'
  id: totrans-1130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  id: totrans-1131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, ...args)`](#module_tokenizers..PostProcessor+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-1132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens, ...args)`](#module_tokenizers..PostProcessor+_call) ⇒ `PostProcessedOutput`'
  id: totrans-1133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  id: totrans-1134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..PostProcessor.fromConfig) ⇒ `PostProcessor`'
  id: totrans-1135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: new PostProcessor(config)
  id: totrans-1137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration for the post-processor. |'
  id: totrans-1140
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
- en: postProcessor.post_process(tokens, ...args) ⇒ <code> PostProcessedOutput </code>
  id: totrans-1142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Method to be implemented in subclass to apply post-processing on the given tokens.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PostProcessor`](#module_tokenizers..PostProcessor)'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - The post-processed tokens.'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the method is not implemented in subclass.'
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1148
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1149
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array` | The input tokens to be post-processed. |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
- en: '| ...args | `*` | Additional arguments required by the post-processing logic.
    |'
  id: totrans-1151
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: postProcessor._call(tokens, ...args) ⇒ <code> PostProcessedOutput </code>
  id: totrans-1153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alias for [PostProcessor#post_process](PostProcessor#post_process).
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PostProcessor`](#module_tokenizers..PostProcessor)'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - The post-processed tokens.'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array` | The text or array of texts to post-process. |'
  id: totrans-1159
  prefs: []
  type: TYPE_TB
- en: '| ...args | `*` | Additional arguments required by the post-processing logic.
    |'
  id: totrans-1160
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: PostProcessor.fromConfig(config) ⇒ <code> PostProcessor </code>
  id: totrans-1162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factory method to create a PostProcessor object from a configuration object.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`PostProcessor`](#module_tokenizers..PostProcessor)'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessor` - A PostProcessor object created from the given
    configuration.'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If an unknown PostProcessor type is encountered.'
  id: totrans-1167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1168
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1169
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | Configuration object representing a PostProcessor. |'
  id: totrans-1170
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BertProcessing
  id: totrans-1172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A post-processor that adds special tokens to the beginning and end of the input.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: '[~BertProcessing](#module_tokenizers..BertProcessing)'
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new BertProcessing(config)`](#new_module_tokenizers..BertProcessing_new)'
  id: totrans-1176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..BertProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-1177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: new BertProcessing(config)
  id: totrans-1179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1180
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1181
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration for the post-processor. |'
  id: totrans-1182
  prefs: []
  type: TYPE_TB
- en: '| config.cls | `Array.<string>` | The special tokens to add to the beginning
    of the input. |'
  id: totrans-1183
  prefs: []
  type: TYPE_TB
- en: '| config.sep | `Array.<string>` | The special tokens to add to the end of the
    input. |'
  id: totrans-1184
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: bertProcessing.post_process(tokens, [tokens_pair]) ⇒ <code> PostProcessedOutput
    </code>
  id: totrans-1186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adds the special tokens to the beginning and end of the input.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`BertProcessing`](#module_tokenizers..BertProcessing)'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - The post-processed tokens with the special
    tokens added to the beginning and end.'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-1190
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` |  | The input tokens. |'
  id: totrans-1192
  prefs: []
  type: TYPE_TB
- en: '| [tokens_pair] | `Array.<string>` |  | An optional second set of input tokens.
    |'
  id: totrans-1193
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~TemplateProcessing ⇐ <code> PostProcessor </code>
  id: totrans-1195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post processor that replaces special tokens in a template with actual tokens.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PostProcessor`'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
- en: '[~TemplateProcessing](#module_tokenizers..TemplateProcessing) ⇐ `PostProcessor`'
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new TemplateProcessing(config)`](#new_module_tokenizers..TemplateProcessing_new)'
  id: totrans-1200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.post_process(tokens, [tokens_pair])`](#module_tokenizers..TemplateProcessing+post_process)
    ⇒ `PostProcessedOutput`'
  id: totrans-1201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
- en: new TemplateProcessing(config)
  id: totrans-1203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of `TemplateProcessing`.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1205
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1206
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the post processor. |'
  id: totrans-1207
  prefs: []
  type: TYPE_TB
- en: '| config.single | `Array` | The template for a single sequence of tokens. |'
  id: totrans-1208
  prefs: []
  type: TYPE_TB
- en: '| config.pair | `Array` | The template for a pair of sequences of tokens. |'
  id: totrans-1209
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
- en: templateProcessing.post_process(tokens, [tokens_pair]) ⇒ <code> PostProcessedOutput
    </code>
  id: totrans-1211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Replaces special tokens in the template with actual tokens.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`TemplateProcessing`](#module_tokenizers..TemplateProcessing)'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - An object containing the list of tokens
    with the special tokens replaced with actual tokens.'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-1216
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` |  | The list of tokens for the first sequence.
    |'
  id: totrans-1217
  prefs: []
  type: TYPE_TB
- en: '| [tokens_pair] | `Array.<string>` |  | The list of tokens for the second sequence
    (optional). |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ByteLevelPostProcessor ⇐ <code> PostProcessor </code>
  id: totrans-1220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A PostProcessor that returns the given tokens as is.
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PostProcessor`'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
- en: byteLevelPostProcessor.post_process(tokens, [tokens_pair]) ⇒ <code> PostProcessedOutput
    </code>
  id: totrans-1225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Post process the given tokens.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelPostProcessor`](#module_tokenizers..ByteLevelPostProcessor)'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `PostProcessedOutput` - An object containing the post-processed
    tokens.'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-1229
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-1230
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` |  | The list of tokens for the first sequence.
    |'
  id: totrans-1231
  prefs: []
  type: TYPE_TB
- en: '| [tokens_pair] | `Array.<string>` |  | The list of tokens for the second sequence
    (optional). |'
  id: totrans-1232
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Decoder ⇐ <code> Callable </code>
  id: totrans-1234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class for token decoders.
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Callable`'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
- en: '[~Decoder](#module_tokenizers..Decoder) ⇐ `Callable`'
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Decoder(config)`](#new_module_tokenizers..Decoder_new)'
  id: totrans-1239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*instance*'
  id: totrans-1240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.added_tokens`](#module_tokenizers..Decoder+added_tokens) : `Array.<AddedToken>`'
  id: totrans-1241
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`._call(tokens)`](#module_tokenizers..Decoder+_call) ⇒ `string`'
  id: totrans-1242
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode(tokens)`](#module_tokenizers..Decoder+decode) ⇒ `string`'
  id: totrans-1243
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain(tokens)`](#module_tokenizers..Decoder+decode_chain) ⇒ `Array.<string>`'
  id: totrans-1244
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*static*'
  id: totrans-1245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.fromConfig(config)`](#module_tokenizers..Decoder.fromConfig) ⇒ `Decoder`'
  id: totrans-1246
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
- en: new Decoder(config)
  id: totrans-1248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of `Decoder`.
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1250
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1251
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-1252
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
- en: 'decoder.added_tokens : <code> Array. < AddedToken > </code>'
  id: totrans-1254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance property of [`Decoder`](#module_tokenizers..Decoder)'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: decoder._call(tokens) ⇒ <code> string </code>
  id: totrans-1257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Calls the `decode` method.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Decoder`](#module_tokenizers..Decoder)'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1261
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1262
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The list of tokens. |'
  id: totrans-1263
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
- en: decoder.decode(tokens) ⇒ <code> string </code>
  id: totrans-1265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decodes a list of tokens.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Decoder`](#module_tokenizers..Decoder)'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1269
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1270
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The list of tokens. |'
  id: totrans-1271
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
- en: decoder.decode_chain(tokens) ⇒ <code> Array. < string > </code>
  id: totrans-1273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apply the decoder to a list of tokens.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Decoder`](#module_tokenizers..Decoder)'
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The decoded list of tokens.'
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If the `decode_chain` method is not implemented in the subclass.'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1279
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1280
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | The list of tokens. |'
  id: totrans-1281
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: Decoder.fromConfig(config) ⇒ <code> Decoder </code>
  id: totrans-1283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a decoder instance based on the provided configuration.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: static method of [`Decoder`](#module_tokenizers..Decoder)'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Decoder` - A decoder instance.'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
- en: '**Throws**:'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: '`Error` If an unknown decoder type is provided.'
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1289
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1290
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-1291
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~FuseDecoder
  id: totrans-1293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fuse simply fuses all tokens into one big string. It’s usually the last decoding
    step anyway, but this decoder exists incase some decoders need to happen after
    that step
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
- en: 'fuseDecoder.decode_chain() : <code> * </code>'
  id: totrans-1297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`FuseDecoder`](#module_tokenizers..FuseDecoder)'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WordPieceDecoder ⇐ <code> Decoder </code>
  id: totrans-1300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A decoder that decodes a list of WordPiece tokens into a single string.
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: '[~WordPieceDecoder](#module_tokenizers..WordPieceDecoder) ⇐ `Decoder`'
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WordPieceDecoder(config)`](#new_module_tokenizers..WordPieceDecoder_new)'
  id: totrans-1305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..WordPieceDecoder+decode_chain) : `*`'
  id: totrans-1306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
- en: new WordPieceDecoder(config)
  id: totrans-1308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of WordPieceDecoder.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1310
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1311
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-1312
  prefs: []
  type: TYPE_TB
- en: '| config.prefix | `string` | The prefix used for WordPiece encoding. |'
  id: totrans-1313
  prefs: []
  type: TYPE_TB
- en: '| config.cleanup | `boolean` | Whether to cleanup the decoded string. |'
  id: totrans-1314
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
- en: 'wordPieceDecoder.decode_chain() : <code> * </code>'
  id: totrans-1316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WordPieceDecoder`](#module_tokenizers..WordPieceDecoder)'
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ByteLevelDecoder ⇐ <code> Decoder </code>
  id: totrans-1319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Byte-level decoder for tokenization output. Inherits from the `Decoder` class.
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
- en: '[~ByteLevelDecoder](#module_tokenizers..ByteLevelDecoder) ⇐ `Decoder`'
  id: totrans-1323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ByteLevelDecoder(config)`](#new_module_tokenizers..ByteLevelDecoder_new)'
  id: totrans-1324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..ByteLevelDecoder+convert_tokens_to_string)
    ⇒ `string`'
  id: totrans-1325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..ByteLevelDecoder+decode_chain) : `*`'
  id: totrans-1326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
- en: new ByteLevelDecoder(config)
  id: totrans-1328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a `ByteLevelDecoder` object.
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1331
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | Configuration object. |'
  id: totrans-1332
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
- en: byteLevelDecoder.convert_tokens_to_string(tokens) ⇒ <code> string </code>
  id: totrans-1334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convert an array of tokens to string by decoding each byte.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelDecoder`](#module_tokenizers..ByteLevelDecoder)'
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1338
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1339
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | Array of tokens to be decoded. |'
  id: totrans-1340
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
- en: 'byteLevelDecoder.decode_chain() : <code> * </code>'
  id: totrans-1342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ByteLevelDecoder`](#module_tokenizers..ByteLevelDecoder)'
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~CTCDecoder
  id: totrans-1345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CTC (Connectionist Temporal Classification) decoder. See [https://github.com/huggingface/tokenizers/blob/bb38f390a61883fc2f29d659af696f428d1cda6b/tokenizers/src/decoders/ctc.rs](https://github.com/huggingface/tokenizers/blob/bb38f390a61883fc2f29d659af696f428d1cda6b/tokenizers/src/decoders/ctc.rs)
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
- en: '[~CTCDecoder](#module_tokenizers..CTCDecoder)'
  id: totrans-1348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.convert_tokens_to_string(tokens)`](#module_tokenizers..CTCDecoder+convert_tokens_to_string)
    ⇒ `string`'
  id: totrans-1349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..CTCDecoder+decode_chain) : `*`'
  id: totrans-1350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
- en: ctcDecoder.convert_tokens_to_string(tokens) ⇒ <code> string </code>
  id: totrans-1352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converts a connectionist-temporal-classification (CTC) output tokens into a
    single string.
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`CTCDecoder`](#module_tokenizers..CTCDecoder)'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The decoded string.'
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1356
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1357
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | Array of tokens to be decoded. |'
  id: totrans-1358
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
- en: 'ctcDecoder.decode_chain() : <code> * </code>'
  id: totrans-1360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`CTCDecoder`](#module_tokenizers..CTCDecoder)'
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~DecoderSequence ⇐ <code> Decoder </code>
  id: totrans-1363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apply a sequence of decoders.
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
- en: '[~DecoderSequence](#module_tokenizers..DecoderSequence) ⇐ `Decoder`'
  id: totrans-1367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new DecoderSequence(config)`](#new_module_tokenizers..DecoderSequence_new)'
  id: totrans-1368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..DecoderSequence+decode_chain) : `*`'
  id: totrans-1369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
- en: new DecoderSequence(config)
  id: totrans-1371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates a new instance of DecoderSequence.
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1373
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1374
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object. |'
  id: totrans-1375
  prefs: []
  type: TYPE_TB
- en: '| config.decoders | `Array.<Decoder>` | The list of decoders to apply. |'
  id: totrans-1376
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
- en: 'decoderSequence.decode_chain() : <code> * </code>'
  id: totrans-1378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`DecoderSequence`](#module_tokenizers..DecoderSequence)'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~MetaspacePreTokenizer ⇐ <code> PreTokenizer </code>
  id: totrans-1381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This PreTokenizer replaces spaces with the given replacement character, adds
    a prefix space if requested, and returns a list of tokens.
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
- en: '[~MetaspacePreTokenizer](#module_tokenizers..MetaspacePreTokenizer) ⇐ `PreTokenizer`'
  id: totrans-1385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MetaspacePreTokenizer(config)`](#new_module_tokenizers..MetaspacePreTokenizer_new)'
  id: totrans-1386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..MetaspacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
- en: new MetaspacePreTokenizer(config)
  id: totrans-1389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-1390
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-1391
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` |  | The configuration object for the MetaspacePreTokenizer.
    |'
  id: totrans-1392
  prefs: []
  type: TYPE_TB
- en: '| config.add_prefix_space | `boolean` |  | Whether to add a prefix space to
    the first token. |'
  id: totrans-1393
  prefs: []
  type: TYPE_TB
- en: '| config.replacement | `string` |  | The character to replace spaces with.
    |'
  id: totrans-1394
  prefs: []
  type: TYPE_TB
- en: '| [config.str_rep] | `string` | `"config.replacement"` | An optional string
    representation of the replacement character. |'
  id: totrans-1395
  prefs: []
  type: TYPE_TB
- en: '| [config.prepend_scheme] | `''first''` &#124; `''never''` &#124; `''always''`
    | `''always''` | The metaspace prepending scheme. |'
  id: totrans-1396
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
- en: metaspacePreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method takes a string, replaces spaces with the replacement character,
    adds a prefix space if requested, and returns a new list of tokens.
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`MetaspacePreTokenizer`](#module_tokenizers..MetaspacePreTokenizer)'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - A new list of pre-tokenized tokens.'
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1402
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1403
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to pre-tokenize. |'
  id: totrans-1404
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | The options for the pre-tokenization. |'
  id: totrans-1405
  prefs: []
  type: TYPE_TB
- en: '| [options.section_index] | `number` | The index of the section to pre-tokenize.
    |'
  id: totrans-1406
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~MetaspaceDecoder ⇐ <code> Decoder </code>
  id: totrans-1408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MetaspaceDecoder class extends the Decoder class and decodes Metaspace tokenization.
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Decoder`'
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
- en: '[~MetaspaceDecoder](#module_tokenizers..MetaspaceDecoder) ⇐ `Decoder`'
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new MetaspaceDecoder(config)`](#new_module_tokenizers..MetaspaceDecoder_new)'
  id: totrans-1413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.decode_chain()`](#module_tokenizers..MetaspaceDecoder+decode_chain) : `*`'
  id: totrans-1414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
- en: new MetaspaceDecoder(config)
  id: totrans-1416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constructs a new MetaspaceDecoder object.
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1418
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1419
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the MetaspaceDecoder. |'
  id: totrans-1420
  prefs: []
  type: TYPE_TB
- en: '| config.add_prefix_space | `boolean` | Whether to add a prefix space to the
    decoded string. |'
  id: totrans-1421
  prefs: []
  type: TYPE_TB
- en: '| config.replacement | `string` | The string to replace spaces with. |'
  id: totrans-1422
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
- en: 'metaspaceDecoder.decode_chain() : <code> * </code>'
  id: totrans-1424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`MetaspaceDecoder`](#module_tokenizers..MetaspaceDecoder)'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~Precompiled ⇐ <code> Normalizer </code>
  id: totrans-1427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A normalizer that applies a precompiled charsmap. This is useful for applying
    complex normalizations in C++ and exposing them to JavaScript.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `Normalizer`'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
- en: '[~Precompiled](#module_tokenizers..Precompiled) ⇐ `Normalizer`'
  id: totrans-1431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new Precompiled(config)`](#new_module_tokenizers..Precompiled_new)'
  id: totrans-1432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.normalize(text)`](#module_tokenizers..Precompiled+normalize) ⇒ `string`'
  id: totrans-1433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
- en: new Precompiled(config)
  id: totrans-1435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a new instance of Precompiled normalizer.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1437
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1438
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the Precompiled normalizer.
    |'
  id: totrans-1439
  prefs: []
  type: TYPE_TB
- en: '| config.precompiled_charsmap | `Object` | The precompiled charsmap object.
    |'
  id: totrans-1440
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
- en: precompiled.normalize(text) ⇒ <code> string </code>
  id: totrans-1442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalizes the given text by applying the precompiled charsmap.
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`Precompiled`](#module_tokenizers..Precompiled)'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The normalized text.'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1446
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1447
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to normalize. |'
  id: totrans-1448
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~PreTokenizerSequence ⇐ <code> PreTokenizer </code>
  id: totrans-1450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pre-tokenizer that applies a sequence of pre-tokenizers to the input text.
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
- en: '[~PreTokenizerSequence](#module_tokenizers..PreTokenizerSequence) ⇐ `PreTokenizer`'
  id: totrans-1454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new PreTokenizerSequence(config)`](#new_module_tokenizers..PreTokenizerSequence_new)'
  id: totrans-1455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..PreTokenizerSequence+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
- en: new PreTokenizerSequence(config)
  id: totrans-1458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of PreTokenizerSequence.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1460
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1461
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the pre-tokenizer sequence.
    |'
  id: totrans-1462
  prefs: []
  type: TYPE_TB
- en: '| config.pretokenizers | `Array.<Object>` | An array of pre-tokenizer configurations.
    |'
  id: totrans-1463
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
- en: preTokenizerSequence.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applies each pre-tokenizer in the sequence to the input text in turn.
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`PreTokenizerSequence`](#module_tokenizers..PreTokenizerSequence)'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The pre-tokenized text.'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1469
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1470
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to pre-tokenize. |'
  id: totrans-1471
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1472
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WhitespacePreTokenizer
  id: totrans-1474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Splits on word boundaries (using the following regular expression: `\w+|[^\w\s]+`).'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
- en: '[~WhitespacePreTokenizer](#module_tokenizers..WhitespacePreTokenizer)'
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WhitespacePreTokenizer(config)`](#new_module_tokenizers..WhitespacePreTokenizer_new)'
  id: totrans-1478
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1479
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
- en: new WhitespacePreTokenizer(config)
  id: totrans-1481
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of WhitespacePreTokenizer.
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1483
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1484
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the pre-tokenizer. |'
  id: totrans-1485
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
- en: whitespacePreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. <
    string > </code>
  id: totrans-1487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-tokenizes the input text by splitting it on word boundaries.
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WhitespacePreTokenizer`](#module_tokenizers..WhitespacePreTokenizer)'
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens produced by splitting the
    input text on whitespace.'
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1491
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1492
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be pre-tokenized. |'
  id: totrans-1493
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1494
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~WhitespaceSplit ⇐ <code> PreTokenizer </code>
  id: totrans-1496
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits a string of text by whitespace characters into individual tokens.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
- en: '**Extends**: `PreTokenizer`'
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
- en: '[~WhitespaceSplit](#module_tokenizers..WhitespaceSplit) ⇐ `PreTokenizer`'
  id: totrans-1500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new WhitespaceSplit(config)`](#new_module_tokenizers..WhitespaceSplit_new)'
  id: totrans-1501
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..WhitespaceSplit+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1502
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
- en: new WhitespaceSplit(config)
  id: totrans-1504
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creates an instance of WhitespaceSplit.
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1506
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1507
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration object for the pre-tokenizer. |'
  id: totrans-1508
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
- en: whitespaceSplit.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-tokenizes the input text by splitting it on whitespace characters.
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`WhitespaceSplit`](#module_tokenizers..WhitespaceSplit)'
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens produced by splitting the
    input text on whitespace.'
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1514
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1515
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be pre-tokenized. |'
  id: totrans-1516
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1517
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~ReplacePreTokenizer
  id: totrans-1519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner class of [`tokenizers`](#module_tokenizers)'
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
- en: '[~ReplacePreTokenizer](#module_tokenizers..ReplacePreTokenizer)'
  id: totrans-1521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`new ReplacePreTokenizer(config)`](#new_module_tokenizers..ReplacePreTokenizer_new)'
  id: totrans-1522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`.pre_tokenize_text(text, [options])`](#module_tokenizers..ReplacePreTokenizer+pre_tokenize_text)
    ⇒ `Array.<string>`'
  id: totrans-1523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
- en: new ReplacePreTokenizer(config)
  id: totrans-1525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1526
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1527
  prefs: []
  type: TYPE_TB
- en: '| config | `Object` | The configuration options for the pre-tokenizer. |'
  id: totrans-1528
  prefs: []
  type: TYPE_TB
- en: '| config.pattern | `Object` | The pattern used to split the text. Can be a
    string or a regex object. |'
  id: totrans-1529
  prefs: []
  type: TYPE_TB
- en: '| config.content | `string` | What to replace the pattern with. |'
  id: totrans-1530
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
- en: replacePreTokenizer.pre_tokenize_text(text, [options]) ⇒ <code> Array. < string
    > </code>
  id: totrans-1532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-tokenizes the input text by replacing certain characters.
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: instance method of [`ReplacePreTokenizer`](#module_tokenizers..ReplacePreTokenizer)'
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - An array of tokens produced by replacing certain
    characters.'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1536
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1537
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to be pre-tokenized. |'
  id: totrans-1538
  prefs: []
  type: TYPE_TB
- en: '| [options] | `Object` | Additional options for the pre-tokenization logic.
    |'
  id: totrans-1539
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~BYTES_TO_UNICODE ⇒ <code> Object </code>
  id: totrans-1541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Returns list of utf-8 byte and a mapping to unicode strings. Specifically avoids
    mapping to whitespace/control characters the BPE code barfs on.
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner constant of [`tokenizers`](#module_tokenizers)'
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Object` - Object with utf-8 byte keys and unicode string values.'
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~loadTokenizer(pretrained_model_name_or_path, options) ⇒ <code> Promise.
    < Array < any > > </code>
  id: totrans-1546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loads a tokenizer from the specified path.
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Promise.<Array<any>>` - A promise that resolves with information
    about the loaded tokenizer.'
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1550
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1551
  prefs: []
  type: TYPE_TB
- en: '| pretrained_model_name_or_path | `string` | The path to the tokenizer directory.
    |'
  id: totrans-1552
  prefs: []
  type: TYPE_TB
- en: '| options | `PretrainedTokenizerOptions` | Additional options for loading the
    tokenizer. |'
  id: totrans-1553
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~regexSplit(text, regex) ⇒ <code> Array. < string > </code>
  id: totrans-1555
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to split a string on a regex, but keep the delimiters. This
    is required, because the JavaScript `.split()` method does not keep the delimiters,
    and wrapping in a capturing group causes issues with existing capturing groups
    (due to nesting).
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The split string.'
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1559
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1560
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to split. |'
  id: totrans-1561
  prefs: []
  type: TYPE_TB
- en: '| regex | `RegExp` | The regex to split on. |'
  id: totrans-1562
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~createPattern(pattern, invert) ⇒ <code> RegExp </code> | <code> null
    </code>
  id: totrans-1564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper method to construct a pattern from a config object.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `RegExp` | `null` - The compiled pattern.'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Default | Description |'
  id: totrans-1568
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-1569
  prefs: []
  type: TYPE_TB
- en: '| pattern | `Object` |  | The pattern object. |'
  id: totrans-1570
  prefs: []
  type: TYPE_TB
- en: '| invert | `boolean` | `true` | Whether to invert the pattern. |'
  id: totrans-1571
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~objectToMap(obj) ⇒ <code> Map. < string, any > </code>
  id: totrans-1573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to convert an Object to a Map
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Map.<string, any>` - The map.'
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1577
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1578
  prefs: []
  type: TYPE_TB
- en: '| obj | `Object` | The object to convert. |'
  id: totrans-1579
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~prepareTensorForDecode(tensor) ⇒ <code> Array. < number > </code>
  id: totrans-1581
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to convert a tensor to a list before decoding.
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<number>` - The tensor as a list.'
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1585
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1586
  prefs: []
  type: TYPE_TB
- en: '| tensor | `Tensor` | The tensor to convert. |'
  id: totrans-1587
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~clean_up_tokenization(text) ⇒ <code> string </code>
  id: totrans-1589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clean up a list of simple English tokenization artifacts like spaces before
    punctuations and abbreviated forms
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The cleaned up text.'
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1593
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1594
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to clean up. |'
  id: totrans-1595
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~remove_accents(text) ⇒ <code> string </code>
  id: totrans-1597
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to remove accents from a string.
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The text with accents removed.'
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1601
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1602
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to remove accents from. |'
  id: totrans-1603
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~lowercase_and_remove_accent(text) ⇒ <code> string </code>
  id: totrans-1605
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to lowercase a string and remove accents.
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `string` - The lowercased text with accents removed.'
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1609
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1610
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to lowercase and remove accents from. |'
  id: totrans-1611
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~fuse(arr, value, mapping)
  id: totrans-1613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helper function to fuse consecutive values in an array equal to the specified
    value.
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1616
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1617
  prefs: []
  type: TYPE_TB
- en: '| arr | `Array.<string>` | The input array |'
  id: totrans-1618
  prefs: []
  type: TYPE_TB
- en: '| value | `any` | The value to fuse on. |'
  id: totrans-1619
  prefs: []
  type: TYPE_TB
- en: '| mapping | `Map.<string, any>` | The mapping from input domain to value. |'
  id: totrans-1620
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
- en: tokenizers~whitespace_split(text) ⇒ <code> Array. < string > </code>
  id: totrans-1622
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Split a string on whitespace.
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner method of [`tokenizers`](#module_tokenizers)'
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
- en: '**Returns**: `Array.<string>` - The split string.'
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
- en: '| Param | Type | Description |'
  id: totrans-1626
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1627
  prefs: []
  type: TYPE_TB
- en: '| text | `string` | The text to split. |'
  id: totrans-1628
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~PretrainedTokenizerOptions : <code> Object </code>'
  id: totrans-1630
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additional tokenizer-specific properties.
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Default | Description |'
  id: totrans-1634
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-1635
  prefs: []
  type: TYPE_TB
- en: '| [legacy] | `boolean` | `false` | Whether or not the `legacy` behavior of
    the tokenizer should be used. |'
  id: totrans-1636
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~BPENode : <code> Object </code>'
  id: totrans-1638
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  id: totrans-1641
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1642
  prefs: []
  type: TYPE_TB
- en: '| token | `string` | The token associated with the node |'
  id: totrans-1643
  prefs: []
  type: TYPE_TB
- en: '| bias | `number` | A positional bias for the node. |'
  id: totrans-1644
  prefs: []
  type: TYPE_TB
- en: '| [score] | `number` | The score of the node. |'
  id: totrans-1645
  prefs: []
  type: TYPE_TB
- en: '| [prev] | `BPENode` | The previous node in the linked list. |'
  id: totrans-1646
  prefs: []
  type: TYPE_TB
- en: '| [next] | `BPENode` | The next node in the linked list. |'
  id: totrans-1647
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~SplitDelimiterBehavior : <code> ’ removed ’ </code> | <code> ’ isolated
    ’ </code> | <code> ’ mergedWithPrevious ’ </code> | <code> ’ mergedWithNext ’
    </code> | <code> ’ contiguous ’ </code>'
  id: totrans-1649
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~PostProcessedOutput : <code> Object </code>'
  id: totrans-1652
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  id: totrans-1655
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1656
  prefs: []
  type: TYPE_TB
- en: '| tokens | `Array.<string>` | List of token produced by the post-processor.
    |'
  id: totrans-1657
  prefs: []
  type: TYPE_TB
- en: '| [token_type_ids] | `Array.<number>` | List of token type ids produced by
    the post-processor. |'
  id: totrans-1658
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~EncodingSingle : <code> Object </code>'
  id: totrans-1660
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  id: totrans-1663
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1664
  prefs: []
  type: TYPE_TB
- en: '| input_ids | `Array.<number>` | List of token ids to be fed to a model. |'
  id: totrans-1665
  prefs: []
  type: TYPE_TB
- en: '| attention_mask | `Array.<number>` | List of token type ids to be fed to a
    model |'
  id: totrans-1666
  prefs: []
  type: TYPE_TB
- en: '| [token_type_ids] | `Array.<number>` | List of indices specifying which tokens
    should be attended to by the model |'
  id: totrans-1667
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~BatchEncoding : <code> Array < number > </code> | <code> Array <
    Array < number > > </code> | <code> Tensor </code>'
  id: totrans-1669
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Holds the output of the tokenizer’s call function.
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  id: totrans-1673
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1674
  prefs: []
  type: TYPE_TB
- en: '| input_ids | `BatchEncodingItem` | List of token ids to be fed to a model.
    |'
  id: totrans-1675
  prefs: []
  type: TYPE_TB
- en: '| attention_mask | `BatchEncodingItem` | List of indices specifying which tokens
    should be attended to by the model. |'
  id: totrans-1676
  prefs: []
  type: TYPE_TB
- en: '| [token_type_ids] | `BatchEncodingItem` | List of token type ids to be fed
    to a model. |'
  id: totrans-1677
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
- en: 'tokenizers~Message : <code> Object </code>'
  id: totrans-1679
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kind**: inner typedef of [`tokenizers`](#module_tokenizers)'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties**'
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Type | Description |'
  id: totrans-1682
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1683
  prefs: []
  type: TYPE_TB
- en: '| role | `string` | The role of the message (e.g., "user" or "assistant" or
    "system"). |'
  id: totrans-1684
  prefs: []
  type: TYPE_TB
- en: '| content | `string` | The content of the message. |'
  id: totrans-1685
  prefs: []
  type: TYPE_TB
- en: '* * *'
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
