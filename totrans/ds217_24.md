# 云存储

> 原文链接：[`huggingface.co/docs/datasets/filesystems`](https://huggingface.co/docs/datasets/filesystems)

🤗 数据集通过`fsspec`文件系统实现支持访问云存储提供商。您可以以 Pythonic 方式从任何云存储中保存和加载数据集。查看以下表格，了解一些受支持的云存储提供商的示例：

| 存储提供商 | 文件系统实现 |
| --- | --- |
| 亚马逊 S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |
| Google 云存储 | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |
| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |
| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs) |
| Google 云盘 | [gdrivefs](https://github.com/intake/gdrivefs) |
| Oracle 云存储 | [ocifs](https://ocifs.readthedocs.io/en/latest/) |

本指南将向您展示如何使用任何云存储保存和加载数据集。以下是 S3、Google 云存储、Azure Blob 存储和 Oracle 云对象存储的示例。

## 设置您的云存储文件系统

### 亚马逊 S3

1.  安装 S3 文件系统实现：

```py
>>> pip install s3fs
```

1.  定义您的凭据

要使用匿名连接，请使用`anon=True`。否则，在与私有 S3 存储桶交互时，请包括您的`aws_access_key_id`和`aws_secret_access_key`。

```py
>>> storage_options = {"anon": True}  # for anonymous connection
# or use your credentials
>>> storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}  # for private buckets
# or use a botocore session
>>> import aiobotocore.session
>>> s3_session = aiobotocore.session.AioSession(profile="my_profile_name")
>>> storage_options = {"session": s3_session}
```

1.  创建您的文件系统实例

```py
>>> import s3fs
>>> fs = s3fs.S3FileSystem(**storage_options)
```

### Google 云存储

1.  安装 Google 云存储实现：

```py
>>> conda install -c conda-forge gcsfs
# or install with pip
>>> pip install gcsfs
```

1.  定义您的凭据

```py
>>> storage_options={"token": "anon"}  # for anonymous connection
# or use your credentials of your default gcloud credentials or from the google metadata service
>>> storage_options={"project": "my-google-project"}
# or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/
>>> storage_options={"project": "my-google-project", "token": TOKEN}
```

1.  创建您的文件系统实例

```py
>>> import gcsfs
>>> fs = gcsfs.GCSFileSystem(**storage_options)
```

### Azure Blob 存储

1.  安装 Azure Blob 存储实现：

```py
>>> conda install -c conda-forge adlfs
# or install with pip
>>> pip install adlfs
```

1.  定义您的凭据

```py
>>> storage_options = {"anon": True}  # for anonymous connection
# or use your credentials
>>> storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY}  # gen 2 filesystem
# or use your credentials with the gen 1 filesystem
>>> storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}
```

1.  创建您的文件系统实例

```py
>>> import adlfs
>>> fs = adlfs.AzureBlobFileSystem(**storage_options)
```

### Oracle 云对象存储

1.  安装 OCI 文件系统实现：

```py
>>> pip install ocifs
```

1.  定义您的凭据

```py
>>> storage_options = {"config": "~/.oci/config", "region": "us-ashburn-1"} 
```

1.  创建您的文件系统实例

```py
>>> import ocifs
>>> fs = ocifs.OCIFileSystem(**storage_options)
```

## 使用云存储文件系统加载和保存数据集

### 下载并准备数据集到云存储

您可以通过在`download_and_prepare`中指定远程`output_dir`来下载并准备数据集到您的云存储。不要忘记使用先前定义的包含您的凭据的`storage_options`来写入私有云存储。

`download_and_prepare`方法分两步进行：

1.  首先，它会下载原始数据文件（如果有）到您的本地缓存中。您可以通过将`cache_dir`传递给 load_dataset_builder()来设置缓存目录

1.  然后，通过迭代原始数据文件，在您的云存储中生成 Arrow 或 Parquet 格式的数据集。

从 Hugging Face Hub 加载数据集构建器（参见如何从 Hugging Face Hub 加载）：

```py
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("imdb")
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

使用加载脚本加载数据集构建器（参见如何加载本地加载脚本）:

```py
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("path/to/local/loading_script/loading_script.py")
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

使用您自己的数据文件（参见如何加载本地和远程文件）：

```py
>>> data_files = {"train": ["path/to/train.csv"]}
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("csv", data_files=data_files)
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

强烈建议将文件保存为压缩的 Parquet 文件，以通过指定`file_format="parquet"`来优化 I/O。否则，数据集将保存为未压缩的 Arrow 文件。

您还可以使用`max_shard_size`指定碎片的大小（默认为 500MB）：

```py
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet", max_shard_size="1GB")
```

#### Dask

Dask 是一个并行计算库，它具有类似于 pandas 的 API，用于并行处理大于内存的 Parquet 数据集。Dask 可以在单台机器上使用多个线程或进程，或者在集群中使用多台机器并行处理数据。Dask 支持本地数据，也支持来自云存储的数据。

因此，您可以在 Dask 中加载保存为分片 Parquet 文件的数据集

```py
import dask.dataframe as dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

# or if your dataset is split into train/valid/test
df_train = dd.read_parquet(output_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(output_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)
```

您可以在它们的[文档](https://docs.dask.org/en/stable/dataframe.html)中找到更多关于 dask 数据框的信息。

## 保存序列化数据集

处理完数据集后，您可以使用 Dataset.save_to_disk()将其保存到云存储中：

```py
# saves encoded_dataset to amazon s3
>>> encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", storage_options=storage_options)
# saves encoded_dataset to google cloud storage
>>> encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", storage_options=storage_options)
# saves encoded_dataset to microsoft azure blob/datalake
>>> encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", storage_options=storage_options)
```

请记得在与私有云存储交互时，在您的 FileSystem 实例 `fs`中定义您的凭据。

## 列出序列化数据集

使用您的 FileSystem 实例`fs`从云存储中列出文件，使用`fs.ls`：

```py
>>> fs.ls("my-private-datasets/imdb/train", detail=False)
["dataset_info.json.json","dataset.arrow","state.json"]
```

### 加载序列化数据集

当您准备再次使用数据集时，请使用 Dataset.load_from_disk()重新加载：

```py
>>> from datasets import load_from_disk
# load encoded_dataset from cloud storage
>>> dataset = load_from_disk("s3://a-public-datasets/imdb/train", storage_options=storage_options)  
>>> print(len(dataset))
25000
```
