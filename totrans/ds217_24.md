# äº‘å­˜å‚¨

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/datasets/filesystems`](https://huggingface.co/docs/datasets/filesystems)

ğŸ¤— æ•°æ®é›†é€šè¿‡`fsspec`æ–‡ä»¶ç³»ç»Ÿå®ç°æ”¯æŒè®¿é—®äº‘å­˜å‚¨æä¾›å•†ã€‚æ‚¨å¯ä»¥ä»¥ Pythonic æ–¹å¼ä»ä»»ä½•äº‘å­˜å‚¨ä¸­ä¿å­˜å’ŒåŠ è½½æ•°æ®é›†ã€‚æŸ¥çœ‹ä»¥ä¸‹è¡¨æ ¼ï¼Œäº†è§£ä¸€äº›å—æ”¯æŒçš„äº‘å­˜å‚¨æä¾›å•†çš„ç¤ºä¾‹ï¼š

| å­˜å‚¨æä¾›å•† | æ–‡ä»¶ç³»ç»Ÿå®ç° |
| --- | --- |
| äºšé©¬é€Š S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |
| Google äº‘å­˜å‚¨ | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |
| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |
| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs) |
| Google äº‘ç›˜ | [gdrivefs](https://github.com/intake/gdrivefs) |
| Oracle äº‘å­˜å‚¨ | [ocifs](https://ocifs.readthedocs.io/en/latest/) |

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä»»ä½•äº‘å­˜å‚¨ä¿å­˜å’ŒåŠ è½½æ•°æ®é›†ã€‚ä»¥ä¸‹æ˜¯ S3ã€Google äº‘å­˜å‚¨ã€Azure Blob å­˜å‚¨å’Œ Oracle äº‘å¯¹è±¡å­˜å‚¨çš„ç¤ºä¾‹ã€‚

## è®¾ç½®æ‚¨çš„äº‘å­˜å‚¨æ–‡ä»¶ç³»ç»Ÿ

### äºšé©¬é€Š S3

1.  å®‰è£… S3 æ–‡ä»¶ç³»ç»Ÿå®ç°ï¼š

```py
>>> pip install s3fs
```

1.  å®šä¹‰æ‚¨çš„å‡­æ®

è¦ä½¿ç”¨åŒ¿åè¿æ¥ï¼Œè¯·ä½¿ç”¨`anon=True`ã€‚å¦åˆ™ï¼Œåœ¨ä¸ç§æœ‰ S3 å­˜å‚¨æ¡¶äº¤äº’æ—¶ï¼Œè¯·åŒ…æ‹¬æ‚¨çš„`aws_access_key_id`å’Œ`aws_secret_access_key`ã€‚

```py
>>> storage_options = {"anon": True}  # for anonymous connection
# or use your credentials
>>> storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}  # for private buckets
# or use a botocore session
>>> import aiobotocore.session
>>> s3_session = aiobotocore.session.AioSession(profile="my_profile_name")
>>> storage_options = {"session": s3_session}
```

1.  åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹

```py
>>> import s3fs
>>> fs = s3fs.S3FileSystem(**storage_options)
```

### Google äº‘å­˜å‚¨

1.  å®‰è£… Google äº‘å­˜å‚¨å®ç°ï¼š

```py
>>> conda install -c conda-forge gcsfs
# or install with pip
>>> pip install gcsfs
```

1.  å®šä¹‰æ‚¨çš„å‡­æ®

```py
>>> storage_options={"token": "anon"}  # for anonymous connection
# or use your credentials of your default gcloud credentials or from the google metadata service
>>> storage_options={"project": "my-google-project"}
# or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/
>>> storage_options={"project": "my-google-project", "token": TOKEN}
```

1.  åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹

```py
>>> import gcsfs
>>> fs = gcsfs.GCSFileSystem(**storage_options)
```

### Azure Blob å­˜å‚¨

1.  å®‰è£… Azure Blob å­˜å‚¨å®ç°ï¼š

```py
>>> conda install -c conda-forge adlfs
# or install with pip
>>> pip install adlfs
```

1.  å®šä¹‰æ‚¨çš„å‡­æ®

```py
>>> storage_options = {"anon": True}  # for anonymous connection
# or use your credentials
>>> storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY}  # gen 2 filesystem
# or use your credentials with the gen 1 filesystem
>>> storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}
```

1.  åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹

```py
>>> import adlfs
>>> fs = adlfs.AzureBlobFileSystem(**storage_options)
```

### Oracle äº‘å¯¹è±¡å­˜å‚¨

1.  å®‰è£… OCI æ–‡ä»¶ç³»ç»Ÿå®ç°ï¼š

```py
>>> pip install ocifs
```

1.  å®šä¹‰æ‚¨çš„å‡­æ®

```py
>>> storage_options = {"config": "~/.oci/config", "region": "us-ashburn-1"} 
```

1.  åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹

```py
>>> import ocifs
>>> fs = ocifs.OCIFileSystem(**storage_options)
```

## ä½¿ç”¨äº‘å­˜å‚¨æ–‡ä»¶ç³»ç»ŸåŠ è½½å’Œä¿å­˜æ•°æ®é›†

### ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†åˆ°äº‘å­˜å‚¨

æ‚¨å¯ä»¥é€šè¿‡åœ¨`download_and_prepare`ä¸­æŒ‡å®šè¿œç¨‹`output_dir`æ¥ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†åˆ°æ‚¨çš„äº‘å­˜å‚¨ã€‚ä¸è¦å¿˜è®°ä½¿ç”¨å…ˆå‰å®šä¹‰çš„åŒ…å«æ‚¨çš„å‡­æ®çš„`storage_options`æ¥å†™å…¥ç§æœ‰äº‘å­˜å‚¨ã€‚

`download_and_prepare`æ–¹æ³•åˆ†ä¸¤æ­¥è¿›è¡Œï¼š

1.  é¦–å…ˆï¼Œå®ƒä¼šä¸‹è½½åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰åˆ°æ‚¨çš„æœ¬åœ°ç¼“å­˜ä¸­ã€‚æ‚¨å¯ä»¥é€šè¿‡å°†`cache_dir`ä¼ é€’ç»™ load_dataset_builder()æ¥è®¾ç½®ç¼“å­˜ç›®å½•

1.  ç„¶åï¼Œé€šè¿‡è¿­ä»£åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œåœ¨æ‚¨çš„äº‘å­˜å‚¨ä¸­ç”Ÿæˆ Arrow æˆ– Parquet æ ¼å¼çš„æ•°æ®é›†ã€‚

ä» Hugging Face Hub åŠ è½½æ•°æ®é›†æ„å»ºå™¨ï¼ˆå‚è§å¦‚ä½•ä» Hugging Face Hub åŠ è½½ï¼‰ï¼š

```py
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("imdb")
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

ä½¿ç”¨åŠ è½½è„šæœ¬åŠ è½½æ•°æ®é›†æ„å»ºå™¨ï¼ˆå‚è§å¦‚ä½•åŠ è½½æœ¬åœ°åŠ è½½è„šæœ¬ï¼‰:

```py
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("path/to/local/loading_script/loading_script.py")
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®æ–‡ä»¶ï¼ˆå‚è§å¦‚ä½•åŠ è½½æœ¬åœ°å’Œè¿œç¨‹æ–‡ä»¶ï¼‰ï¼š

```py
>>> data_files = {"train": ["path/to/train.csv"]}
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("csv", data_files=data_files)
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

å¼ºçƒˆå»ºè®®å°†æ–‡ä»¶ä¿å­˜ä¸ºå‹ç¼©çš„ Parquet æ–‡ä»¶ï¼Œä»¥é€šè¿‡æŒ‡å®š`file_format="parquet"`æ¥ä¼˜åŒ– I/Oã€‚å¦åˆ™ï¼Œæ•°æ®é›†å°†ä¿å­˜ä¸ºæœªå‹ç¼©çš„ Arrow æ–‡ä»¶ã€‚

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`max_shard_size`æŒ‡å®šç¢ç‰‡çš„å¤§å°ï¼ˆé»˜è®¤ä¸º 500MBï¼‰ï¼š

```py
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet", max_shard_size="1GB")
```

#### Dask

Dask æ˜¯ä¸€ä¸ªå¹¶è¡Œè®¡ç®—åº“ï¼Œå®ƒå…·æœ‰ç±»ä¼¼äº pandas çš„ APIï¼Œç”¨äºå¹¶è¡Œå¤„ç†å¤§äºå†…å­˜çš„ Parquet æ•°æ®é›†ã€‚Dask å¯ä»¥åœ¨å•å°æœºå™¨ä¸Šä½¿ç”¨å¤šä¸ªçº¿ç¨‹æˆ–è¿›ç¨‹ï¼Œæˆ–è€…åœ¨é›†ç¾¤ä¸­ä½¿ç”¨å¤šå°æœºå™¨å¹¶è¡Œå¤„ç†æ•°æ®ã€‚Dask æ”¯æŒæœ¬åœ°æ•°æ®ï¼Œä¹Ÿæ”¯æŒæ¥è‡ªäº‘å­˜å‚¨çš„æ•°æ®ã€‚

å› æ­¤ï¼Œæ‚¨å¯ä»¥åœ¨ Dask ä¸­åŠ è½½ä¿å­˜ä¸ºåˆ†ç‰‡ Parquet æ–‡ä»¶çš„æ•°æ®é›†

```py
import dask.dataframe as dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

# or if your dataset is split into train/valid/test
df_train = dd.read_parquet(output_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(output_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)
```

æ‚¨å¯ä»¥åœ¨å®ƒä»¬çš„[æ–‡æ¡£](https://docs.dask.org/en/stable/dataframe.html)ä¸­æ‰¾åˆ°æ›´å¤šå…³äº dask æ•°æ®æ¡†çš„ä¿¡æ¯ã€‚

## ä¿å­˜åºåˆ—åŒ–æ•°æ®é›†

å¤„ç†å®Œæ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Dataset.save_to_disk()å°†å…¶ä¿å­˜åˆ°äº‘å­˜å‚¨ä¸­ï¼š

```py
# saves encoded_dataset to amazon s3
>>> encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", storage_options=storage_options)
# saves encoded_dataset to google cloud storage
>>> encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", storage_options=storage_options)
# saves encoded_dataset to microsoft azure blob/datalake
>>> encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", storage_options=storage_options)
```

è¯·è®°å¾—åœ¨ä¸ç§æœ‰äº‘å­˜å‚¨äº¤äº’æ—¶ï¼Œåœ¨æ‚¨çš„ FileSystem å®ä¾‹ `fs`ä¸­å®šä¹‰æ‚¨çš„å‡­æ®ã€‚

## åˆ—å‡ºåºåˆ—åŒ–æ•°æ®é›†

ä½¿ç”¨æ‚¨çš„ FileSystem å®ä¾‹`fs`ä»äº‘å­˜å‚¨ä¸­åˆ—å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨`fs.ls`ï¼š

```py
>>> fs.ls("my-private-datasets/imdb/train", detail=False)
["dataset_info.json.json","dataset.arrow","state.json"]
```

### åŠ è½½åºåˆ—åŒ–æ•°æ®é›†

å½“æ‚¨å‡†å¤‡å†æ¬¡ä½¿ç”¨æ•°æ®é›†æ—¶ï¼Œè¯·ä½¿ç”¨ Dataset.load_from_disk()é‡æ–°åŠ è½½ï¼š

```py
>>> from datasets import load_from_disk
# load encoded_dataset from cloud storage
>>> dataset = load_from_disk("s3://a-public-datasets/imdb/train", storage_options=storage_options)  
>>> print(len(dataset))
25000
```
