# BARThez

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/barthez`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/barthez)

## 概述

BARThez 模型是由 Moussa Kamal Eddine、Antoine J.-P. Tixier 和 Michalis Vazirgiannis 于 2020 年 10 月 23 日提出的[BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321)。

论文摘要：

*归纳传递学习，通过自监督学习实现，已经席卷了整个自然语言处理（NLP）领域，像 BERT 和 BART 这样的模型在无数自然语言理解任务上取得了新的最先进水平。尽管有一些显著的例外，但大多数可用的模型和研究都是针对英语进行的。在这项工作中，我们介绍了 BARThez，这是法语语言的第一个 BART 模型（据我们所知）。BARThez 在过去研究中从一个非常大的单语法语语料库上进行了预训练，我们对其进行了调整以适应 BART 的扰动方案。与已经存在的基于 BERT 的法语语言模型（如 CamemBERT 和 FlauBERT）不同，BARThez 特别适用于生成任务，因为它的编码器和解码器都经过了预训练。除了 FLUE 基准测试中的判别任务，我们还在一个新的摘要数据集 OrangeSum 上评估 BARThez，我们在本文中发布了这个数据集。我们还继续在 BARThez 的语料库上对已经预训练的多语言 BART 进行预训练，并展示了由此产生的模型，我们称之为 mBARTHez，比普通的 BARThez 提供了显著的提升，并且与 CamemBERT 和 FlauBERT 相媲美或者表现更好。*

这个模型是由[moussakam](https://huggingface.co/moussakam)贡献的。作者的代码可以在[这里](https://github.com/moussaKam/BARThez)找到。

BARThez 的实现与 BART 相同，除了标记化。有关配置类及其参数的信息，请参考 BART 文档。BARThez 特定的标记器如下所述。

## 资源

+   BARThez 可以像 BART 一样在序列到序列任务上进行微调，查看：[examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md)。

## BarthezTokenizer

### `class transformers.BarthezTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez.py#L53)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' sp_model_kwargs: Optional = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含实例化标记器所需词汇表的[SentencePiece](https://github.com/google/sentencepiece)文件（通常具有*.spm*扩展名）。

+   `bos_token` (`str`, *可选*，默认为`"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    在构建使用特殊标记的序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *可选*，默认为`"</s>"`) — 序列结束标记。

    在构建使用特殊标记的序列时，这不是用于序列结尾的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *可选*，默认为`"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *可选*，默认为`"<s>"`) — 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *可选*，默认为`"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *optional*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时。

+   `mask_token` (`str`, *optional*, 默认为 `"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `sp_model_kwargs` (`dict`, *optional*) — 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece 的 Python 包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`: 启用子词正则化。

    +   `nbest_size`: unigram 采样参数。对于 BPE-Dropout 无效。

        +   `nbest_size = {0,1}`: 不执行采样。

        +   `nbest_size > 1`: 从 nbest_size 结果中进行采样。

        +   `nbest_size < 0`: 假设 nbest_size 为无限，并使用前向过滤和后向采样算法从所有假设（格）中进行采样。

    +   `alpha`: 用于 unigram 采样的平滑参数，以及 BPE-dropout 的合并操作的丢弃概率。

+   `sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和 ID）的*SentencePiece*处理器。

改编自 CamembertTokenizer 和 BartTokenizer。构建一个 BARThez 标记器。基于[SentencePiece](https://github.com/google/sentencepiece)。

此标记器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez.py#L159)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

具有适当特殊标记的 input IDs 列表。

通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。BARThez 序列具有以下格式：

+   单个序列: `<s> X </s>`

+   一对序列: `<s> A </s></s> B </s>`

#### `convert_tokens_to_string`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez.py#L255)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez.py#L212)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez.py#L185)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — 标记列表是否已经使用特殊标记格式化为模型。

返回

`List[int]`

一个整数列表，范围为[0, 1]：特殊标记为 1，序列标记为 0。

从没有添加特殊标记的标记列表中检索序列 ID。在使用标记器的`prepare_for_model`方法添加特殊标记时调用此方法。

## BarthezTokenizerFast

### `class transformers.BarthezTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez_fast.py#L62)

```py
( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含实例化分词器所需词汇表的[SentencePiece](https://github.com/google/sentencepiece)文件（通常具有*.spm*扩展名）。

+   `bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开头标记。可用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开头的标记。用于开头的标记是`cls_token`。

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结尾的标记。用于结尾的标记是`sep_token`。

+   `sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *optional*, defaults to `"<s>"`) — 在进行序列分类（整个序列的分类而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `additional_special_tokens` (`List[str]`, *optional*, defaults to `["<s>NOTUSED", "</s>NOTUSED"]`) — 分词器使用的其他特殊标记。

改编自 CamembertTokenizer 和 BartTokenizer。构建一个“快速”BARThez 分词器。基于[SentencePiece](https://github.com/google/sentencepiece)。

此分词器继承自 PreTrainedTokenizerFast，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez_fast.py#L154)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

带有适当特殊标记的输入 ID 列表。

通过连接和添加特殊标记构建用于序列分类任务的序列或序列对的模型输入。BARThez 序列具有以下格式：

+   单个序列：`<s> X </s>`

+   序列对：`<s> A </s></s> B </s>`

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/barthez/tokenization_barthez_fast.py#L180)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。
