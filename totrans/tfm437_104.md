# 模型训练解剖学

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy`](https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy)

为了了解可以应用的性能优化技术，以提高模型训练速度和内存利用效率，有助于熟悉在训练期间 GPU 的使用情况，以及根据执行的操作而变化的计算强度。

让我们从探索 GPU 利用率和模型训练运行的一个激励示例开始。为了演示，我们需要安装一些库：

```py
pip install transformers datasets accelerate nvidia-ml-py3
```

`nvidia-ml-py3`库允许我们从 Python 内部监视模型的内存使用情况。您可能熟悉终端中的`nvidia-smi`命令-这个库允许直接在 Python 中访问相同的信息。

然后，我们创建一些虚拟数据：100 到 30000 之间的随机标记 ID 和用于分类器的二进制标签。总共，我们得到 512 个长度为 512 的序列，并将它们存储在 PyTorch 格式的[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)中。

```py
>>> import numpy as np
>>> from datasets import Dataset

>>> seq_len, dataset_size = 512, 512
>>> dummy_data = {
...     "input_ids": np.random.randint(100, 30000, (dataset_size, seq_len)),
...     "labels": np.random.randint(0, 1, (dataset_size)),
... }
>>> ds = Dataset.from_dict(dummy_data)
>>> ds.set_format("pt")
```

为了打印 GPU 利用率和使用 Trainer 进行训练的摘要统计信息，我们定义了两个辅助函数：

```py
>>> from pynvml import *

>>> def print_gpu_utilization():
...     nvmlInit()
...     handle = nvmlDeviceGetHandleByIndex(0)
...     info = nvmlDeviceGetMemoryInfo(handle)
...     print(f"GPU memory occupied: {info.used//1024**2} MB.")

>>> def print_summary(result):
...     print(f"Time: {result.metrics['train_runtime']:.2f}")
...     print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
...     print_gpu_utilization()
```

让我们验证一下我们从空闲 GPU 内存开始：

```py
>>> print_gpu_utilization()
GPU memory occupied: 0 MB.
```

看起来不错：GPU 内存没有被占用，正如我们在加载任何模型之前所期望的那样。如果在您的计算机上不是这种情况，请确保停止使用 GPU 内存的所有进程。然而，并非所有空闲 GPU 内存都可以被用户使用。当模型加载到 GPU 时，内核也会被加载，这可能占用 1-2GB 的内存。为了查看有多少内存被占用，我们将一个微小的张量加载到 GPU 中，这将触发内核的加载。

```py
>>> import torch

>>> torch.ones((1, 1)).to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 1343 MB.
```

我们看到内核单独占用了 1.3GB 的 GPU 内存。现在让我们看看模型使用了多少空间。

## 加载模型

首先，我们加载`bert-large-uncased`模型。我们直接将模型权重加载到 GPU，以便我们可以检查仅权重使用了多少空间。

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 2631 MB.
```

我们可以看到模型权重单独占用了 1.3GB 的 GPU 内存。确切的数字取决于您使用的具体 GPU。请注意，在较新的 GPU 上，模型有时可能占用更多空间，因为权重以优化的方式加载，加快了模型的使用速度。现在我们还可以快速检查是否与`nvidia-smi` CLI 得到相同的结果：

```py
nvidia-smi
```

```py
Tue Jan 11 08:58:05 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |
+-----------------------------------------------------------------------------+
```

我们得到了与之前相同的数字，您还可以看到我们正在使用一块具有 16GB 内存的 V100 GPU。现在我们可以开始训练模型并查看 GPU 内存消耗的变化。首先，我们设置一些标准的训练参数：

```py
default_args = {
    "output_dir": "tmp",
    "evaluation_strategy": "steps",
    "num_train_epochs": 1,
    "log_level": "error",
    "report_to": "none",
}
```

如果您计划运行多个实验，为了在实验之间正确清除内存，请在实验之间重新启动 Python 内核。

## 在普通训练中的内存利用率

让我们使用 Trainer 并在不使用任何 GPU 性能优化技术的情况下训练模型，批量大小为 4：

```py
>>> from transformers import TrainingArguments, Trainer, logging

>>> logging.set_verbosity_error()

>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)
>>> result = trainer.train()
>>> print_summary(result)
```

```py
Time: 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.
```

我们看到即使是一个相对较小的批量大小几乎填满了我们 GPU 的整个内存。然而，较大的批量大小通常会导致模型更快地收敛或获得更好的最终性能。因此，理想情况下，我们希望根据模型的需求而不是 GPU 的限制来调整批量大小。有趣的是，我们使用的内存比模型的大小要多得多。为了更好地理解为什么会这样，让我们看一下模型的操作和内存需求。

## 模型操作解剖学

Transformers 架构包括 3 个主要的操作组，根据计算强度分组如下。

1.  **张量收缩**

    线性层和多头注意力组件都进行批量**矩阵-矩阵乘法**。这些操作是训练 transformer 最计算密集的部分。

1.  **统计归一化**

    Softmax 和层归一化的计算密度低于张量收缩，并涉及一个或多个**减少操作**，其结果然后通过映射应用。

1.  **逐元素运算符**

    这些是剩余的运算符：**偏置、丢弃、激活和残差连接**。这些是计算密集度最低的操作。

这些知识在分析性能瓶颈时可能会有所帮助。

这个总结来源于[数据移动就是你需要的一切：优化 Transformer 的案例研究 2020](https://arxiv.org/abs/2007.00072)

## 模型内存的解剖

我们已经看到，训练模型使用的内存比仅将模型放在 GPU 上要多得多。这是因为在训练期间有许多组件使用 GPU 内存。在 GPU 内存中的组件包括以下内容：

1.  模型权重

1.  优化器状态

1.  梯度

1.  为梯度计算保存的前向激活

1.  临时缓冲区

1.  功能特定的内存

使用 AdamW 进行混合精度训练的典型模型每个模型参数需要 18 字节加上激活内存。对于推断，没有优化器状态和梯度，因此我们可以减去这些。因此，对于混合精度推断，每个模型参数需要 6 字节，加上激活内存。

让我们看看细节。

**模型权重：**

+   每个参数的 4 字节，用于 fp32 训练

+   每个参数的 6 字节，用于混合精度训练（在内存中维护一个 fp32 模型和一个 fp16 模型）

**优化器状态：**

+   每个参数的 8 字节，用于普通的 AdamW（维护 2 个状态）

+   每个参数的 8 位 AdamW 优化器需要 2 字节，例如[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

+   每个参数的 4 字节，用于像带有动量的 SGD 这样的优化器（仅维护 1 个状态）

**梯度**

+   每个参数的 4 字节，用于 fp32 或混合精度训练（梯度始终保持在 fp32 中）

**前向激活**

+   大小取决于许多因素，关键因素是序列长度、隐藏大小和批量大小。

这里有通过前向和后向函数传递和返回的输入和输出，以及为梯度计算保存的前向激活。

**临时内存**

此外，还有各种临时变量，一旦计算完成就会释放，但在某些时刻这些变量可能需要额外的内存并可能导致 OOM。因此，在编码时，战略性地考虑这些临时变量并有时明确释放那些不再需要的变量是至关重要的。

**功能特定的内存**

然后，您的软件可能有特殊的内存需求。例如，使用波束搜索生成文本时，软件需要维护多个输入和输出的副本。

**`前向`与`后向`执行速度**

对于卷积和线性层，与前向相比，后向中的 flops 是前向的 2 倍，这通常会导致大约 2 倍的速度变慢（有时更多，因为后向中的大小往往更加尴尬）。激活通常受带宽限制，一个激活在后向中通常需要读取比前向更多的数据（例如，激活前向读取一次，写入一次，激活后向读取两次，gradOutput 和前向的输出，然后写入一次，gradInput）。

正如您所看到的，有一些地方我们可以节省 GPU 内存或加快操作速度。现在您已经了解了影响 GPU 利用率和计算速度的因素，请参考单个 GPU 上高效训练的方法和工具文档页面，了解性能优化技术。
