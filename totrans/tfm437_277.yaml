- en: MobileViT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MobileViT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The MobileViT model was proposed in [MobileViT: Light-weight, General-purpose,
    and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin
    Mehta and Mohammad Rastegari. MobileViT introduces a new layer that replaces local
    processing in convolutions with global processing using transformers.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'MobileViT模型是由Sachin Mehta和Mohammad Rastegari在[MobileViT: Light-weight, General-purpose,
    and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)中提出的。MobileViT引入了一种新的层，用transformers进行全局处理来替代卷积中的局部处理。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Light-weight convolutional neural networks (CNNs) are the de-facto for mobile
    vision tasks. Their spatial inductive biases allow them to learn representations
    with fewer parameters across different vision tasks. However, these networks are
    spatially local. To learn global representations, self-attention-based vision
    trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In
    this paper, we ask the following question: is it possible to combine the strengths
    of CNNs and ViTs to build a light-weight and low latency network for mobile vision
    tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose
    vision transformer for mobile devices. MobileViT presents a different perspective
    for the global processing of information with transformers, i.e., transformers
    as convolutions. Our results show that MobileViT significantly outperforms CNN-
    and ViT-based networks across different tasks and datasets. On the ImageNet-1k
    dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters,
    which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based)
    for a similar number of parameters. On the MS-COCO object detection task, MobileViT
    is 5.7% more accurate than MobileNetv3 for a similar number of parameters.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*轻量级卷积神经网络（CNNs）已成为移动视觉任务的事实标准。它们的空间归纳偏差使它们能够学习跨不同视觉任务的表示，并且参数更少。然而，这些网络是空间局部的。为了学习全局表示，基于自注意力的视觉transformers（ViTs）已被采用。与CNNs不同，ViTs是重量级的。在本文中，我们提出了以下问题：是否可能结合CNNs和ViTs的优势构建一个轻量级和低延迟的网络用于移动视觉任务？为此，我们引入了MobileViT，一个轻量级和通用的移动设备视觉transformer。MobileViT提出了一种不同的视角，即使用transformers作为卷积进行信息的全局处理。我们的结果表明，MobileViT在不同任务和数据集上明显优于基于CNN和ViT的网络。在ImageNet-1k数据集上，MobileViT实现了78.4%的top-1准确率，参数约为600万，比MobileNetv3（基于CNN）和DeIT（基于ViT）准确率高出3.2%和6.2%。在MS-COCO目标检测任务中，MobileViT比MobileNetv3准确率高出5.7%，参数相似。*'
- en: This model was contributed by [matthijs](https://huggingface.co/Matthijs). The
    TensorFlow version of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul).
    The original code and weights can be found [here](https://github.com/apple/ml-cvnets).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[matthijs](https://huggingface.co/Matthijs)贡献。模型的TensorFlow版本由[sayakpaul](https://huggingface.co/sayakpaul)贡献。原始代码和权重可以在[这里](https://github.com/apple/ml-cvnets)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: MobileViT is more like a CNN than a Transformer model. It does not work on sequence
    data but on batches of images. Unlike ViT, there are no embeddings. The backbone
    model outputs a feature map. You can follow [this tutorial](https://keras.io/examples/vision/mobilevit)
    for a lightweight introduction.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileViT更像是CNN而不是Transformer模型。它不适用于序列数据，而是适用于图像批次。与ViT不同，没有嵌入。骨干模型输出特征图。您可以参考[此教程](https://keras.io/examples/vision/mobilevit)进行轻量级介绍。
- en: One can use [MobileViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor)
    to prepare images for the model. Note that if you do your own preprocessing, the
    pretrained checkpoints expect images to be in BGR pixel order (not RGB).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用[MobileViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor)来为模型准备图像。请注意，如果您自己进行预处理，预训练的检查点期望图像按BGR像素顺序排列（而不是RGB）。
- en: The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的图像分类检查点在[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)上进行了预训练（也称为ILSVRC
    2012，包含130万张图像和1000个类别）。
- en: The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587)
    head. The available semantic segmentation checkpoints are pre-trained on [PASCAL
    VOC](http://host.robots.ox.ac.uk/pascal/VOC/).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割模型使用[DeepLabV3](https://arxiv.org/abs/1706.05587)头部。可用的语义分割检查点在[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)上进行了预训练。
- en: As the name suggests MobileViT was designed to be performant and efficient on
    mobile phones. The TensorFlow versions of the MobileViT models are fully compatible
    with [TensorFlow Lite](https://www.tensorflow.org/lite).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如其名称所示，MobileViT旨在在手机上表现出色并高效。MobileViT模型的TensorFlow版本与[TensorFlow Lite](https://www.tensorflow.org/lite)完全兼容。
- en: 'You can use the following code to convert a MobileViT checkpoint (be it image
    classification or semantic segmentation) to generate a TensorFlow Lite model:'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以使用以下代码将MobileViT检查点（无论是图像分类还是语义分割）转换为生成TensorFlow Lite模型：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The resulting model will be just **about an MB** making it a good fit for mobile
    applications where resources and network bandwidth can be constrained.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的模型大小约为**1MB**，非常适合资源和网络带宽受限的移动应用程序。
- en: Resources
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with MobileViT.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 官方Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用MobileViT。
- en: Image Classification
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: '**Semantic segmentation**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义分割**'
- en: '[Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义分割任务指南](../tasks/semantic_segmentation)'
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将对其进行审查！资源应该尽可能展示一些新内容，而不是重复现有资源。
- en: MobileViTConfig
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTConfig
- en: '### `class transformers.MobileViTConfig`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/configuration_mobilevit.py#L46)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/configuration_mobilevit.py#L46)'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。'
- en: '`image_size` (`int`, *optional*, defaults to 256) — The size (resolution) of
    each image.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 256) — 每个图像的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 2) — The size (resolution) of
    each patch.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 2) — 每个补丁的大小（分辨率）。'
- en: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[144, 192, 240]`) — Dimensionality
    (hidden size) of the Transformer encoders at each stage.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[144, 192, 240]`) — 每个阶段Transformer编码器的维度（隐藏大小）。'
- en: '`neck_hidden_sizes` (`List[int]`, *optional*, defaults to `[16, 32, 64, 96,
    128, 160, 640]`) — The number of channels for the feature maps of the backbone.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neck_hidden_sizes` (`List[int]`, *optional*, defaults to `[16, 32, 64, 96,
    128, 160, 640]`) — 主干特征图的通道数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 4) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 4) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`mlp_ratio` (`float`, *optional*, defaults to 2.0) — The ratio of the number
    of channels in the output of the MLP to the number of channels in the input.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratio` (`float`, *optional*, defaults to 2.0) — MLP输出通道数与输入通道数的比率。'
- en: '`expand_ratio` (`float`, *optional*, defaults to 4.0) — Expansion factor for
    the MobileNetv2 layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expand_ratio` (`float`, *optional*, defaults to 4.0) — MobileNetv2层的扩展因子。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) — The
    non-linear activation function (function or string) in the Transformer encoder
    and convolution layers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) — Transformer编码器和卷积层中的非线性激活函数（函数或字符串）。'
- en: '`conv_kernel_size` (`int`, *optional*, defaults to 3) — The size of the convolutional
    kernel in the MobileViT layer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel_size` (`int`, *optional*, defaults to 3) — MobileViT层中卷积核的大小。'
- en: '`output_stride` (`int`, *optional*, defaults to 32) — The ratio of the spatial
    resolution of the output to the resolution of the input image.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_stride` (`int`, *optional*, defaults to 32) — 输出的空间分辨率与输入图像分辨率的比率。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probabilitiy for all fully connected layers in the Transformer encoder.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — Transformer编码器中所有全连接层的dropout概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。'
- en: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    ratio for attached classifiers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) — 附加分类器的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。'
- en: '`aspp_out_channels` (`int`, *optional*, defaults to 256) — Number of output
    channels used in the ASPP layer for semantic segmentation.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aspp_out_channels` (`int`, *optional*, defaults to 256) — 语义分割中ASPP层使用的输出通道数。'
- en: '`atrous_rates` (`List[int]`, *optional*, defaults to `[6, 12, 18]`) — Dilation
    (atrous) factors used in the ASPP layer for semantic segmentation.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`atrous_rates` (`List[int]`, *optional*, defaults to `[6, 12, 18]`) — 语义分割中ASPP层使用的扩张（atrous）因子。'
- en: '`aspp_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the ASPP layer for semantic segmentation.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aspp_dropout_prob` (`float`, *optional*, defaults to 0.1) — 语义分割中ASPP层的dropout比率。'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) — The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) — 语义分割模型损失函数中被忽略的索引。'
- en: This is the configuration class to store the configuration of a [MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel).
    It is used to instantiate a MobileViT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MobileViT [apple/mobilevit-small](https://huggingface.co/apple/mobilevit-small)
    architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 [MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel)
    配置的配置类。它用于根据指定的参数实例化一个 MobileViT 模型，定义模型架构。使用默认值实例化配置将产生类似于 MobileViT [apple/mobilevit-small](https://huggingface.co/apple/mobilevit-small)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型的输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: MobileViTFeatureExtractor
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTFeatureExtractor
- en: '### `class transformers.MobileViTFeatureExtractor`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/feature_extraction_mobilevit.py#L26)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/feature_extraction_mobilevit.py#L26)'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#### `__call__`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L176)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L176)'
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Preprocesses a batch of images and optionally segmentation maps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对一批图像和可选的分割地图进行预处理。
- en: Overrides the `__call__` method of the `Preprocessor` class so that both images
    and segmentation maps can be passed in as positional arguments.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 重写 `Preprocessor` 类的 `__call__` 方法，以便将图像和分割地图作为位置参数传入。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    — Raw outputs of the model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`，长度为 `batch_size`，*可选*) — 与每个预测的请求最终大小（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: semantic_segmentation
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`，长度为 `batch_size`，其中每个项目是形状为（高度，宽度）的语义分割地图，对应于 `target_sizes`
    条目（如果指定了 `target_sizes`）。每个 `torch.Tensor` 的每个条目对应于一个语义类别 id。'
- en: Converts the output of [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    的输出转换为语义分割地图。仅支持 PyTorch。
- en: MobileViTImageProcessor
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTImageProcessor
- en: '### `class transformers.MobileViTImageProcessor`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L46)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L46)'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为 `True`) — 是否将图像的（高度，宽度）维度调整为指定的 `size`。可以被 `preprocess`
    方法中的 `do_resize` 参数覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`):
    Controls the size of the output image after resizing. Can be overridden by the
    `size` parameter in the `preprocess` method.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *可选*, 默认为 `{"shortest_edge" -- 224}`): 控制调整大小后输出图像的大小。可以被
    `preprocess` 方法中的 `size` 参数覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Defines the resampling filter to use if resizing the image. Can be overridden
    by the `resample` parameter in the `preprocess` method.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *可选*, 默认为 `Resampling.BILINEAR`) — 定义在调整图像大小时要使用的重采样滤波器。可以被
    `preprocess` 方法中的 `resample` 参数覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *可选*, 默认为 `True`) — 是否按指定的比例因子 `rescale_factor` 重新缩放图像。可以被
    `preprocess` 方法中的 `do_rescale` 参数覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新调整图像，则使用的比例因子。可以被
    `preprocess` 方法中的 `rescale_factor` 参数覆盖。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to crop
    the input at the center. If the input size is smaller than `crop_size` along any
    edge, the image is padded with 0’s and then center cropped. Can be overridden
    by the `do_center_crop` parameter in the `preprocess` method.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`，*可选*，默认为 `True`) — 是否在中心裁剪输入。如果沿任何边缘的输入尺寸小于 `crop_size`，则图像将填充为
    0，然后进行中心裁剪。可以被 `preprocess` 方法中的 `do_center_crop` 参数覆盖。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 256, "width":
    256}`): Desired output size `(size["height"], size["width"])` when applying center-cropping.
    Can be overridden by the `crop_size` parameter in the `preprocess` method.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`，*可选*，默认为 `{"height" -- 256, "width": 256}`)：应用中心裁剪时期望的输出尺寸
    `(size["height"], size["width"])`。可以被 `preprocess` 方法中的 `crop_size` 参数覆盖。'
- en: '`do_flip_channel_order` (`bool`, *optional*, defaults to `True`) — Whether
    to flip the color channels from RGB to BGR. Can be overridden by the `do_flip_channel_order`
    parameter in the `preprocess` method.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_flip_channel_order` (`bool`，*可选*，默认为 `True`) — 是否将颜色通道从 RGB 翻转为 BGR。可以被
    `preprocess` 方法中的 `do_flip_channel_order` 参数覆盖。'
- en: Constructs a MobileViT image processor.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 MobileViT 图像处理器。
- en: '#### `preprocess`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `预处理`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L292)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L292)'
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个图像或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1
    之间的图像，请设置 `do_rescale=False`。'
- en: '`segmentation_maps` (`ImageInput`, *optional*) — Segmentation map to preprocess.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation_maps` (`ImageInput`，*可选*) — 要预处理的分割地图。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`，*可选*，默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`，*可选*，默认为 `self.size`) — 调整大小后的图像尺寸。'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`，*可选*，默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是枚举 `PILImageResampling`
    中的一个。仅在 `do_resize` 设置为 `True` 时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image by rescale factor.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`，*可选*，默认为 `self.do_rescale`) — 是否按照重新缩放因子重新缩放图像。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`，*可选*，默认为 `self.rescale_factor`) — 如果 `do_rescale`
    设置为 `True`，则用于重新缩放图像的重新缩放因子。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`，*可选*，默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop if `do_center_crop` is set to `True`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`，*可选*，默认为 `self.crop_size`) — 如果 `do_center_crop`
    设置为 `True`，则为中心裁剪的尺寸。'
- en: '`do_flip_channel_order` (`bool`, *optional*, defaults to `self.do_flip_channel_order`)
    — Whether to flip the channel order of the image.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_flip_channel_order` (`bool`，*可选*，默认为 `self.do_flip_channel_order`) — 是否翻转图像的通道顺序。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`，*可选*) — 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个 `np.ndarray` 列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`：返回一个 `tf.Tensor` 类型的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`：返回一个 `torch.Tensor` 类型的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`：返回一个 `np.ndarray` 类型的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`：返回一个 `jax.numpy.ndarray` 类型的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`：图像以 (num_channels, height, width) 格式。'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`：图像以 (height, width, num_channels) 格式。'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` 或 `str`，*可选*）— 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (num_channels, height, width)
    格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`：图像以 (height, width, num_channels)
    格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`：图像以 (height, width) 格式。'
- en: Preprocess an image or batch of images.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或批量图像。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `后处理语义分割`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    — Raw outputs of the model.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`（长度为`batch_size`的`List[Tuple]`，*可选*）— 每个预测的请求最终大小（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: semantic_segmentation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为`batch_size`的`List[torch.Tensor]`，其中每个项目都是形状为（高度，宽度）的语义分割地图，对应于`target_sizes`条目（如果指定了`target_sizes`）。每个`torch.Tensor`的每个条目对应于一个语义类别id。
- en: Converts the output of [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将[MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)的输出转换为语义分割地图。仅支持PyTorch。
- en: PytorchHide Pytorch content
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: MobileViTModel
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTModel
- en: '### `class transformers.MobileViTModel`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L693)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L693)'
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare MobileViT model outputting raw hidden-states without any specific head
    on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的MobileViT模型输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L734)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L734)'
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获得。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    `tuple(torch.FloatTensor)`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    模型最后一层的隐藏状态的序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 在空间维度上进行池化操作后的最后一层隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出的一个+每一层的输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: The [MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: MobileViTForImageClassification
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTForImageClassification
- en: '### `class transformers.MobileViTForImageClassification`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L784)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L784)'
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MobileViT model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViT模型，顶部带有图像分类头（在池化特征的顶部是一个线性层），例如用于ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L807)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L807)'
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss). If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失）。如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）— 分类（如果`config.num_labels==1`则为回归）分数（在SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组。模型在每个阶段的输出的隐藏状态（也称为特征图）。'
- en: The [MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: MobileViTForSemanticSegmentation
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTForSemanticSegmentation
- en: '### `class transformers.MobileViTForSemanticSegmentation`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L980)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L980)'
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MobileViT model with a semantic segmentation head on top, e.g. for Pascal VOC.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViT模型，顶部带有语义分割头，例如用于Pascal VOC。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L997)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L997)'
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-
    像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）- 用于计算损失的地面真实语义分割地图。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）- 分类（或回归，如果`config.num_labels==1`）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) — Classification scores for each pixel.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels, logits_height, logits_width)`的`torch.FloatTensor`）-
    每个像素的分类分数。'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">返回的logits不一定与作为输入传递的`pixel_values`具有相同的大小。这是为了避免进行两次插值并在用户需要将logits调整为原始图像大小时丢失一些质量。您应该始终检查您的logits形状并根据需要调整大小。</tip>
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size,
    patch_size, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每个层的输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出处的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size,
    num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例，而不是调用此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: TensorFlowHide TensorFlow content
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: TFMobileViTModel
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFMobileViTModel
- en: '### `class transformers.TFMobileViTModel`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMobileViTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L986)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L986)'
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig））-模型配置类，具有模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare MobileViT model outputting raw hidden-states without any specific head
    on top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的MobileViT模型输出原始的隐藏状态，没有特定的顶部头。此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以“正常工作”-只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果要在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个张量，其中仅包含`pixel_values`，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照文档字符串中给定的顺序，使用长度不同的列表包含一个或多个输入张量：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些，因为您可以像对待任何其他
    Python 函数一样传递输入！
- en: '#### `call`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L998)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L998)'
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    或 `Dict[str, np.ndarray]`，每个示例的形状必须为 `(batch_size, num_channels, height, width)`)
    — 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为
    True。'
- en: Returns
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    或 `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    或一个 `tf.Tensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，取决于配置（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）和输入。'
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor`，形状为 `(batch_size, sequence_length, hidden_size)`)
    — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`tf.Tensor`，形状为 `(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和双曲正切激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个输出通常*不*是输入语义内容的好摘要，您通常最好对整个输入序列的隐藏状态进行平均或池化。
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或当
    `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)`
    的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递 `output_attentions=True` 或当 `config.output_attentions=True`
    时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor`
    元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFMobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFMobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTModel)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数中定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TFMobileViTForImageClassification
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFMobileViTForImageClassification
- en: '### `class transformers.TFMobileViTForImageClassification`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMobileViTForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1026)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1026)'
- en: '[PRE21]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MobileViT model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViT模型在顶部具有图像分类头（在池化特征的顶部有一个线性层），例如用于ImageNet。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有事项。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典的第一个位置参数。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该可以“正常工作”-只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅具有`pixel_values`的单个张量，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1047)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1047)'
- en: '[PRE22]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）-像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）-是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。这个参数可以在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for computing
    the image classification/regression loss. Indices should be in `[0, ..., config.num_labels
    - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square
    loss). If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`tf.Tensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0, ...,
    config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失）。如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`或`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`或一个`tf.Tensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含各种元素，这取决于配置（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）和输入。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`tf.Tensor`，*可选*，当提供`labels`时返回）— 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`tf.Tensor`）— 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    模型在每个阶段输出的`tf.Tensor`元组（如果模型有嵌入层，则为嵌入的输出+每个阶段的输出）的形状为`(batch_size, num_channels,
    height, width)`。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: The [TFMobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFMobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForImageClassification)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TFMobileViTForSemanticSegmentation
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFMobileViTForSemanticSegmentation
- en: '### `class transformers.TFMobileViTForSemanticSegmentation`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMobileViTForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1258)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1258)'
- en: '[PRE24]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MobileViT model with a semantic segmentation head on top, e.g. for Pascal VOC.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViT模型在顶部带有语义分割头，例如用于Pascal VOC。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入都作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入作为第一个位置参数的列表、元组或字典。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该“只需工作”
    - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras `Functional`
    API 创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有`pixel_values`的单个张量，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1292)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1292)'
- en: '[PRE25]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例必须具有形状`(batch_size, num_channels, height, width)`）- 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数仅在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, height, width)`, *optional*) —
    Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, height, width)`的`tf.Tensor`，*可选*）- 用于计算损失的地面真实语义分割地图。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`
    or `tuple(tf.Tensor)`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`或`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`tf.Tensor`，*可选*，在提供`labels`时返回）- 分类（或回归，如果`config.num_labels==1`）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) — Classification scores for each pixel.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels, logits_height, logits_width)`的`tf.Tensor`）-
    每个像素的分类分数。'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">返回的logits不一定与作为输入传递的`pixel_values`具有相同的大小。这是为了避免进行两次插值并在用户需要将logits调整为原始图像大小时丢失一些质量。您应始终检查您的logits形状并根据需要调整大小。</tip>
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, patch_size, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: The [TFMobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFMobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForSemanticSegmentation)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Examples:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
