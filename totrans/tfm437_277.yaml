- en: MobileViT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MobileViT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevit)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The MobileViT model was proposed in [MobileViT: Light-weight, General-purpose,
    and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin
    Mehta and Mohammad Rastegari. MobileViT introduces a new layer that replaces local
    processing in convolutions with global processing using transformers.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'MobileViTæ¨¡å‹æ˜¯ç”±Sachin Mehtaå’ŒMohammad Rastegariåœ¨[MobileViT: Light-weight, General-purpose,
    and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)ä¸­æå‡ºçš„ã€‚MobileViTå¼•å…¥äº†ä¸€ç§æ–°çš„å±‚ï¼Œç”¨transformersè¿›è¡Œå…¨å±€å¤„ç†æ¥æ›¿ä»£å·ç§¯ä¸­çš„å±€éƒ¨å¤„ç†ã€‚'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Light-weight convolutional neural networks (CNNs) are the de-facto for mobile
    vision tasks. Their spatial inductive biases allow them to learn representations
    with fewer parameters across different vision tasks. However, these networks are
    spatially local. To learn global representations, self-attention-based vision
    trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In
    this paper, we ask the following question: is it possible to combine the strengths
    of CNNs and ViTs to build a light-weight and low latency network for mobile vision
    tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose
    vision transformer for mobile devices. MobileViT presents a different perspective
    for the global processing of information with transformers, i.e., transformers
    as convolutions. Our results show that MobileViT significantly outperforms CNN-
    and ViT-based networks across different tasks and datasets. On the ImageNet-1k
    dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters,
    which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based)
    for a similar number of parameters. On the MS-COCO object detection task, MobileViT
    is 5.7% more accurate than MobileNetv3 for a similar number of parameters.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å·²æˆä¸ºç§»åŠ¨è§†è§‰ä»»åŠ¡çš„äº‹å®æ ‡å‡†ã€‚å®ƒä»¬çš„ç©ºé—´å½’çº³åå·®ä½¿å®ƒä»¬èƒ½å¤Ÿå­¦ä¹ è·¨ä¸åŒè§†è§‰ä»»åŠ¡çš„è¡¨ç¤ºï¼Œå¹¶ä¸”å‚æ•°æ›´å°‘ã€‚ç„¶è€Œï¼Œè¿™äº›ç½‘ç»œæ˜¯ç©ºé—´å±€éƒ¨çš„ã€‚ä¸ºäº†å­¦ä¹ å…¨å±€è¡¨ç¤ºï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›çš„è§†è§‰transformersï¼ˆViTsï¼‰å·²è¢«é‡‡ç”¨ã€‚ä¸CNNsä¸åŒï¼ŒViTsæ˜¯é‡é‡çº§çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼šæ˜¯å¦å¯èƒ½ç»“åˆCNNså’ŒViTsçš„ä¼˜åŠ¿æ„å»ºä¸€ä¸ªè½»é‡çº§å’Œä½å»¶è¿Ÿçš„ç½‘ç»œç”¨äºç§»åŠ¨è§†è§‰ä»»åŠ¡ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MobileViTï¼Œä¸€ä¸ªè½»é‡çº§å’Œé€šç”¨çš„ç§»åŠ¨è®¾å¤‡è§†è§‰transformerã€‚MobileViTæå‡ºäº†ä¸€ç§ä¸åŒçš„è§†è§’ï¼Œå³ä½¿ç”¨transformersä½œä¸ºå·ç§¯è¿›è¡Œä¿¡æ¯çš„å…¨å±€å¤„ç†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMobileViTåœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸Šæ˜æ˜¾ä¼˜äºåŸºäºCNNå’ŒViTçš„ç½‘ç»œã€‚åœ¨ImageNet-1kæ•°æ®é›†ä¸Šï¼ŒMobileViTå®ç°äº†78.4%çš„top-1å‡†ç¡®ç‡ï¼Œå‚æ•°çº¦ä¸º600ä¸‡ï¼Œæ¯”MobileNetv3ï¼ˆåŸºäºCNNï¼‰å’ŒDeITï¼ˆåŸºäºViTï¼‰å‡†ç¡®ç‡é«˜å‡º3.2%å’Œ6.2%ã€‚åœ¨MS-COCOç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒMobileViTæ¯”MobileNetv3å‡†ç¡®ç‡é«˜å‡º5.7%ï¼Œå‚æ•°ç›¸ä¼¼ã€‚*'
- en: This model was contributed by [matthijs](https://huggingface.co/Matthijs). The
    TensorFlow version of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul).
    The original code and weights can be found [here](https://github.com/apple/ml-cvnets).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç”±[matthijs](https://huggingface.co/Matthijs)è´¡çŒ®ã€‚æ¨¡å‹çš„TensorFlowç‰ˆæœ¬ç”±[sayakpaul](https://huggingface.co/sayakpaul)è´¡çŒ®ã€‚åŸå§‹ä»£ç å’Œæƒé‡å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/apple/ml-cvnets)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: MobileViT is more like a CNN than a Transformer model. It does not work on sequence
    data but on batches of images. Unlike ViT, there are no embeddings. The backbone
    model outputs a feature map. You can follow [this tutorial](https://keras.io/examples/vision/mobilevit)
    for a lightweight introduction.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileViTæ›´åƒæ˜¯CNNè€Œä¸æ˜¯Transformeræ¨¡å‹ã€‚å®ƒä¸é€‚ç”¨äºåºåˆ—æ•°æ®ï¼Œè€Œæ˜¯é€‚ç”¨äºå›¾åƒæ‰¹æ¬¡ã€‚ä¸ViTä¸åŒï¼Œæ²¡æœ‰åµŒå…¥ã€‚éª¨å¹²æ¨¡å‹è¾“å‡ºç‰¹å¾å›¾ã€‚æ‚¨å¯ä»¥å‚è€ƒ[æ­¤æ•™ç¨‹](https://keras.io/examples/vision/mobilevit)è¿›è¡Œè½»é‡çº§ä»‹ç»ã€‚
- en: One can use [MobileViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor)
    to prepare images for the model. Note that if you do your own preprocessing, the
    pretrained checkpoints expect images to be in BGR pixel order (not RGB).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[MobileViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor)æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨è‡ªå·±è¿›è¡Œé¢„å¤„ç†ï¼Œé¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹æœŸæœ›å›¾åƒæŒ‰BGRåƒç´ é¡ºåºæ’åˆ—ï¼ˆè€Œä¸æ˜¯RGBï¼‰ã€‚
- en: The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ç”¨çš„å›¾åƒåˆ†ç±»æ£€æŸ¥ç‚¹åœ¨[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ˆä¹Ÿç§°ä¸ºILSVRC
    2012ï¼ŒåŒ…å«130ä¸‡å¼ å›¾åƒå’Œ1000ä¸ªç±»åˆ«ï¼‰ã€‚
- en: The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587)
    head. The available semantic segmentation checkpoints are pre-trained on [PASCAL
    VOC](http://host.robots.ox.ac.uk/pascal/VOC/).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†å‰²æ¨¡å‹ä½¿ç”¨[DeepLabV3](https://arxiv.org/abs/1706.05587)å¤´éƒ¨ã€‚å¯ç”¨çš„è¯­ä¹‰åˆ†å‰²æ£€æŸ¥ç‚¹åœ¨[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚
- en: As the name suggests MobileViT was designed to be performant and efficient on
    mobile phones. The TensorFlow versions of the MobileViT models are fully compatible
    with [TensorFlow Lite](https://www.tensorflow.org/lite).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼ŒMobileViTæ—¨åœ¨åœ¨æ‰‹æœºä¸Šè¡¨ç°å‡ºè‰²å¹¶é«˜æ•ˆã€‚MobileViTæ¨¡å‹çš„TensorFlowç‰ˆæœ¬ä¸[TensorFlow Lite](https://www.tensorflow.org/lite)å®Œå…¨å…¼å®¹ã€‚
- en: 'You can use the following code to convert a MobileViT checkpoint (be it image
    classification or semantic segmentation) to generate a TensorFlow Lite model:'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç å°†MobileViTæ£€æŸ¥ç‚¹ï¼ˆæ— è®ºæ˜¯å›¾åƒåˆ†ç±»è¿˜æ˜¯è¯­ä¹‰åˆ†å‰²ï¼‰è½¬æ¢ä¸ºç”ŸæˆTensorFlow Liteæ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The resulting model will be just **about an MB** making it a good fit for mobile
    applications where resources and network bandwidth can be constrained.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„æ¨¡å‹å¤§å°çº¦ä¸º**1MB**ï¼Œéå¸¸é€‚åˆèµ„æºå’Œç½‘ç»œå¸¦å®½å—é™çš„ç§»åŠ¨åº”ç”¨ç¨‹åºã€‚
- en: Resources
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with MobileViT.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨MobileViTã€‚
- en: Image Classification
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: '[MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
- en: '**Semantic segmentation**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯­ä¹‰åˆ†å‰²**'
- en: '[Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—](../tasks/semantic_segmentation)'
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å°½å¯èƒ½å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: MobileViTConfig
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTConfig
- en: '### `class transformers.MobileViTConfig`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/configuration_mobilevit.py#L46)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/configuration_mobilevit.py#L46)'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“çš„æ•°é‡ã€‚'
- en: '`image_size` (`int`, *optional*, defaults to 256) â€” The size (resolution) of
    each image.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 256) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`patch_size` (`int`, *optional*, defaults to 2) â€” The size (resolution) of
    each patch.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 2) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[144, 192, 240]`) â€” Dimensionality
    (hidden size) of the Transformer encoders at each stage.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[144, 192, 240]`) â€” æ¯ä¸ªé˜¶æ®µTransformerç¼–ç å™¨çš„ç»´åº¦ï¼ˆéšè—å¤§å°ï¼‰ã€‚'
- en: '`neck_hidden_sizes` (`List[int]`, *optional*, defaults to `[16, 32, 64, 96,
    128, 160, 640]`) â€” The number of channels for the feature maps of the backbone.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neck_hidden_sizes` (`List[int]`, *optional*, defaults to `[16, 32, 64, 96,
    128, 160, 640]`) â€” ä¸»å¹²ç‰¹å¾å›¾çš„é€šé“æ•°ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 4) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 4) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`mlp_ratio` (`float`, *optional*, defaults to 2.0) â€” The ratio of the number
    of channels in the output of the MLP to the number of channels in the input.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratio` (`float`, *optional*, defaults to 2.0) â€” MLPè¾“å‡ºé€šé“æ•°ä¸è¾“å…¥é€šé“æ•°çš„æ¯”ç‡ã€‚'
- en: '`expand_ratio` (`float`, *optional*, defaults to 4.0) â€” Expansion factor for
    the MobileNetv2 layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expand_ratio` (`float`, *optional*, defaults to 4.0) â€” MobileNetv2å±‚çš„æ‰©å±•å› å­ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) â€” The
    non-linear activation function (function or string) in the Transformer encoder
    and convolution layers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) â€” Transformerç¼–ç å™¨å’Œå·ç§¯å±‚ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`conv_kernel_size` (`int`, *optional*, defaults to 3) â€” The size of the convolutional
    kernel in the MobileViT layer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel_size` (`int`, *optional*, defaults to 3) â€” MobileViTå±‚ä¸­å·ç§¯æ ¸çš„å¤§å°ã€‚'
- en: '`output_stride` (`int`, *optional*, defaults to 32) â€” The ratio of the spatial
    resolution of the output to the resolution of the input image.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_stride` (`int`, *optional*, defaults to 32) â€” è¾“å‡ºçš„ç©ºé—´åˆ†è¾¨ç‡ä¸è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„æ¯”ç‡ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout
    probabilitiy for all fully connected layers in the Transformer encoder.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” Transformerç¼–ç å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout
    ratio for attached classifiers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” é™„åŠ åˆ†ç±»å™¨çš„dropoutæ¯”ç‡ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” Whether to add a bias
    to the queries, keys and values.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åç½®ã€‚'
- en: '`aspp_out_channels` (`int`, *optional*, defaults to 256) â€” Number of output
    channels used in the ASPP layer for semantic segmentation.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aspp_out_channels` (`int`, *optional*, defaults to 256) â€” è¯­ä¹‰åˆ†å‰²ä¸­ASPPå±‚ä½¿ç”¨çš„è¾“å‡ºé€šé“æ•°ã€‚'
- en: '`atrous_rates` (`List[int]`, *optional*, defaults to `[6, 12, 18]`) â€” Dilation
    (atrous) factors used in the ASPP layer for semantic segmentation.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`atrous_rates` (`List[int]`, *optional*, defaults to `[6, 12, 18]`) â€” è¯­ä¹‰åˆ†å‰²ä¸­ASPPå±‚ä½¿ç”¨çš„æ‰©å¼ ï¼ˆatrousï¼‰å› å­ã€‚'
- en: '`aspp_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio
    for the ASPP layer for semantic segmentation.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aspp_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” è¯­ä¹‰åˆ†å‰²ä¸­ASPPå±‚çš„dropoutæ¯”ç‡ã€‚'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚'
- en: This is the configuration class to store the configuration of a [MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel).
    It is used to instantiate a MobileViT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MobileViT [apple/mobilevit-small](https://huggingface.co/apple/mobilevit-small)
    architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ [MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel)
    é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª MobileViT æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº MobileViT [apple/mobilevit-small](https://huggingface.co/apple/mobilevit-small)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹çš„è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: MobileViTFeatureExtractor
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTFeatureExtractor
- en: '### `class transformers.MobileViTFeatureExtractor`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/feature_extraction_mobilevit.py#L26)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/feature_extraction_mobilevit.py#L26)'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#### `__call__`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L176)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L176)'
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Preprocesses a batch of images and optionally segmentation maps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸€æ‰¹å›¾åƒå’Œå¯é€‰çš„åˆ†å‰²åœ°å›¾è¿›è¡Œé¢„å¤„ç†ã€‚
- en: Overrides the `__call__` method of the `Preprocessor` class so that both images
    and segmentation maps can be passed in as positional arguments.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å†™ `Preprocessor` ç±»çš„ `__call__` æ–¹æ³•ï¼Œä»¥ä¾¿å°†å›¾åƒå’Œåˆ†å‰²åœ°å›¾ä½œä¸ºä½ç½®å‚æ•°ä¼ å…¥ã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`ï¼Œé•¿åº¦ä¸º `batch_size`ï¼Œ*å¯é€‰*) â€” ä¸æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: semantic_segmentation
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`ï¼Œé•¿åº¦ä¸º `batch_size`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº `target_sizes`
    æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚'
- en: Converts the output of [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å°† [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚
- en: MobileViTImageProcessor
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTImageProcessor
- en: '### `class transformers.MobileViTImageProcessor`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L46)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L46)'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    imageâ€™s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ç»´åº¦è°ƒæ•´ä¸ºæŒ‡å®šçš„ `size`ã€‚å¯ä»¥è¢« `preprocess`
    æ–¹æ³•ä¸­çš„ `do_resize` å‚æ•°è¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`):
    Controls the size of the output image after resizing. Can be overridden by the
    `size` parameter in the `preprocess` method.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *å¯é€‰*, é»˜è®¤ä¸º `{"shortest_edge" -- 224}`): æ§åˆ¶è°ƒæ•´å¤§å°åè¾“å‡ºå›¾åƒçš„å¤§å°ã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `size` å‚æ•°è¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    â€” Defines the resampling filter to use if resizing the image. Can be overridden
    by the `resample` parameter in the `preprocess` method.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º `Resampling.BILINEAR`) â€” å®šä¹‰åœ¨è°ƒæ•´å›¾åƒå¤§å°æ—¶è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¦†ç›–ã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) â€” Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šçš„æ¯”ä¾‹å› å­ `rescale_factor` é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¦†ç›–ã€‚'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) â€” Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` æˆ– `float`, *å¯é€‰*, é»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°è°ƒæ•´å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ¯”ä¾‹å› å­ã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¦†ç›–ã€‚'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) â€” Whether to crop
    the input at the center. If the input size is smaller than `crop_size` along any
    edge, the image is padded with 0â€™s and then center cropped. Can be overridden
    by the `do_center_crop` parameter in the `preprocess` method.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨ä¸­å¿ƒè£å‰ªè¾“å…¥ã€‚å¦‚æœæ²¿ä»»ä½•è¾¹ç¼˜çš„è¾“å…¥å°ºå¯¸å°äº `crop_size`ï¼Œåˆ™å›¾åƒå°†å¡«å……ä¸º
    0ï¼Œç„¶åè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_center_crop` å‚æ•°è¦†ç›–ã€‚'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 256, "width":
    256}`): Desired output size `(size["height"], size["width"])` when applying center-cropping.
    Can be overridden by the `crop_size` parameter in the `preprocess` method.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `{"height" -- 256, "width": 256}`)ï¼šåº”ç”¨ä¸­å¿ƒè£å‰ªæ—¶æœŸæœ›çš„è¾“å‡ºå°ºå¯¸
    `(size["height"], size["width"])`ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `crop_size` å‚æ•°è¦†ç›–ã€‚'
- en: '`do_flip_channel_order` (`bool`, *optional*, defaults to `True`) â€” Whether
    to flip the color channels from RGB to BGR. Can be overridden by the `do_flip_channel_order`
    parameter in the `preprocess` method.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_flip_channel_order` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†é¢œè‰²é€šé“ä» RGB ç¿»è½¬ä¸º BGRã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `do_flip_channel_order` å‚æ•°è¦†ç›–ã€‚'
- en: Constructs a MobileViT image processor.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª MobileViT å›¾åƒå¤„ç†å™¨ã€‚
- en: '#### `preprocess`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `é¢„å¤„ç†`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L292)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L292)'
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1
    ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_rescale=False`ã€‚'
- en: '`segmentation_maps` (`ImageInput`, *optional*) â€” Segmentation map to preprocess.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation_maps` (`ImageInput`ï¼Œ*å¯é€‰*) â€” è¦é¢„å¤„ç†çš„åˆ†å‰²åœ°å›¾ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) â€” Whether to
    resize the image.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” Size of the
    image after resizing.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) â€” Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚è¿™å¯ä»¥æ˜¯æšä¸¾ `PILImageResampling`
    ä¸­çš„ä¸€ä¸ªã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) â€” Whether
    to rescale the image by rescale factor.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_rescale`) â€” æ˜¯å¦æŒ‰ç…§é‡æ–°ç¼©æ”¾å› å­é‡æ–°ç¼©æ”¾å›¾åƒã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) â€”
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.rescale_factor`) â€” å¦‚æœ `do_rescale`
    è®¾ç½®ä¸º `True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) â€”
    Whether to center crop the image.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_center_crop`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) â€”
    Size of the center crop if `do_center_crop` is set to `True`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.crop_size`) â€” å¦‚æœ `do_center_crop`
    è®¾ç½®ä¸º `True`ï¼Œåˆ™ä¸ºä¸­å¿ƒè£å‰ªçš„å°ºå¯¸ã€‚'
- en: '`do_flip_channel_order` (`bool`, *optional*, defaults to `self.do_flip_channel_order`)
    â€” Whether to flip the channel order of the image.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_flip_channel_order` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_flip_channel_order`) â€” æ˜¯å¦ç¿»è½¬å›¾åƒçš„é€šé“é¡ºåºã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) â€” The type of tensors
    to return. Can be one of:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` æˆ– `''tf''`ï¼šè¿”å›ä¸€ä¸ª `tf.Tensor` ç±»å‹çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` æˆ– `''pt''`ï¼šè¿”å›ä¸€ä¸ª `torch.Tensor` ç±»å‹çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` æˆ– `''np''`ï¼šè¿”å›ä¸€ä¸ª `np.ndarray` ç±»å‹çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` æˆ– `''jax''`ï¼šè¿”å›ä¸€ä¸ª `jax.numpy.ndarray` ç±»å‹çš„æ‰¹å¤„ç†ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ (num_channels, height, width) æ ¼å¼ã€‚'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ (height, width, num_channels) æ ¼å¼ã€‚'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ (num_channels, height, width)
    æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ (height, width, num_channels)
    æ ¼å¼ã€‚'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` æˆ– `ChannelDimension.NONE`ï¼šå›¾åƒä»¥ (height, width) æ ¼å¼ã€‚'
- en: Preprocess an image or batch of images.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–æ‰¹é‡å›¾åƒã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `åå¤„ç†è¯­ä¹‰åˆ†å‰²`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/image_processing_mobilevit.py#L429)'
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation))
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆé•¿åº¦ä¸º`batch_size`çš„`List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: semantic_segmentation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º`batch_size`çš„`List[torch.Tensor]`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®éƒ½æ˜¯å½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº`target_sizes`æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†`target_sizes`ï¼‰ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ«idã€‚
- en: Converts the output of [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: PytorchHide Pytorch content
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: MobileViTModel
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTModel
- en: '### `class transformers.MobileViTModel`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L693)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L693)'
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare MobileViT model outputting raw hidden-states without any specific head
    on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„MobileViTæ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L734)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L734)'
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    `tuple(torch.FloatTensor)`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€çš„åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Last layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” åœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºçš„ä¸€ä¸ª+æ¯ä¸€å±‚çš„è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: The [MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: MobileViTForImageClassification
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTForImageClassification
- en: '### `class transformers.MobileViTForImageClassification`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L784)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L784)'
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: MobileViT model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViTæ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨æ± åŒ–ç‰¹å¾çš„é¡¶éƒ¨æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L807)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L807)'
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss). If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ã€‚å¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆåœ¨SoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The [MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: MobileViTForSemanticSegmentation
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileViTForSemanticSegmentation
- en: '### `class transformers.MobileViTForSemanticSegmentation`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileViTForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L980)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L980)'
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: MobileViT model with a semantic segmentation head on top, e.g. for Pascal VOC.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViTæ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­ä¹‰åˆ†å‰²å¤´ï¼Œä¾‹å¦‚ç”¨äºPascal VOCã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L997)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_mobilevit.py#L997)'
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-
    åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰- åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ`config.num_labels==1`ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” Classification scores for each pixel.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height, logits_width)`çš„`torch.FloatTensor`ï¼‰-
    æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">è¿”å›çš„logitsä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°†logitsè°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”è¯¥å§‹ç»ˆæ£€æŸ¥æ‚¨çš„logitså½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-å½¢çŠ¶ä¸º`(batch_size,
    patch_size, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ª+æ¯ä¸ªå±‚çš„è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-å½¢çŠ¶ä¸º`(batch_size,
    num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: TensorFlowHide TensorFlow content
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlowå†…å®¹
- en: TFMobileViTModel
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFMobileViTModel
- en: '### `class transformers.TFMobileViTModel`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMobileViTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L986)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L986)'
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfigï¼‰ï¼‰-æ¨¡å‹é…ç½®ç±»ï¼Œå…·æœ‰æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare MobileViT model outputting raw hidden-states without any specific head
    on top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„MobileViTæ¨¡å‹è¾“å‡ºåŸå§‹çš„éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–è€…
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€-åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœè¦åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras`Functional`APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªæœ‰ä¸€ä¸ªå¼ é‡ï¼Œå…¶ä¸­ä»…åŒ…å«`pixel_values`ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼Œä½¿ç”¨é•¿åº¦ä¸åŒçš„åˆ—è¡¨åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([pixel_values, attention_mask])`æˆ–`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»–
    Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L998)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L998)'
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) â€” Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    æˆ– `Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º `(batch_size, num_channels, height, width)`)
    â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º
    Trueã€‚'
- en: Returns
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    æˆ– `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    æˆ–ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰å’Œè¾“å…¥ã€‚'
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`)
    â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) â€” Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, hidden_size)`) â€” åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œç»è¿‡çº¿æ€§å±‚å’ŒåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°è¿›ä¸€æ­¥å¤„ç†ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚'
- en: This output is usually *not* a good summary of the semantic content of the input,
    youâ€™re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¾“å‡ºé€šå¸¸*ä¸*æ˜¯è¾“å…¥è¯­ä¹‰å†…å®¹çš„å¥½æ‘˜è¦ï¼Œæ‚¨é€šå¸¸æœ€å¥½å¯¹æ•´ä¸ªè¾“å…¥åºåˆ—çš„éšè—çŠ¶æ€è¿›è¡Œå¹³å‡æˆ–æ± åŒ–ã€‚
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“
    `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`
    çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True`
    æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor`
    å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFMobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFMobileViTModel](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTModel)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°ä¸­å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TFMobileViTForImageClassification
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFMobileViTForImageClassification
- en: '### `class transformers.TFMobileViTForImageClassification`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMobileViTForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1026)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1026)'
- en: '[PRE21]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰-æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: MobileViT model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViTæ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨æ± åŒ–ç‰¹å¾çš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–è€…
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸çš„ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKerasæ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰æ­¤æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€-åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras`Functional`APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»…å…·æœ‰`pixel_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([pixel_values, attention_mask])`æˆ–`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1047)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1047)'
- en: '[PRE22]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) â€” Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str,
    np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`ï¼‰-åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰-æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) â€” Labels for computing
    the image classification/regression loss. Indices should be in `[0, ..., config.num_labels
    - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square
    loss). If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ...,
    config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ã€‚å¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`æˆ–`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`tf.Tensor`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_channels,
    height, width)`ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The [TFMobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFMobileViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForImageClassification)çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE23]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TFMobileViTForSemanticSegmentation
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFMobileViTForSemanticSegmentation
- en: '### `class transformers.TFMobileViTForSemanticSegmentation`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMobileViTForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1258)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1258)'
- en: '[PRE24]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: MobileViT model with a semantic segmentation head on top, e.g. for Pascal VOC.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: MobileViTæ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰è¯­ä¹‰åˆ†å‰²å¤´ï¼Œä¾‹å¦‚ç”¨äºPascal VOCã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¾“å…¥éƒ½ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–è€…
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¾“å…¥ä½œä¸ºç¬¬ä¸€ä¸ªä½ç½®å‚æ•°çš„åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥â€œåªéœ€å·¥ä½œâ€
    - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨Keras `Functional`
    API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªæœ‰`pixel_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šé¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([pixel_values, attention_mask])`æˆ–`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨ä¸éœ€è¦æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1292)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py#L1292)'
- en: '[PRE25]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) â€” Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)
    for details.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str,
    np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹å¿…é¡»å…·æœ‰å½¢çŠ¶`(batch_size, num_channels, height, width)`ï¼‰- åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, height, width)`, *optional*) â€”
    Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`
    or `tuple(tf.Tensor)`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`æˆ–`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig))
    and inputs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_tf_outputs.TFSemanticSegmenterOutputWithNoAttention`æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[MobileViTConfig](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰- åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ`config.num_labels==1`ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” Classification scores for each pixel.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height, logits_width)`çš„`tf.Tensor`ï¼‰-
    æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">è¿”å›çš„logitsä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°†logitsè°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”å§‹ç»ˆæ£€æŸ¥æ‚¨çš„logitså½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, patch_size, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: The [TFMobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFMobileViTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.TFMobileViTForSemanticSegmentation)çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
