- en: How to add a model to ü§ó Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/add_new_model](https://huggingface.co/docs/transformers/v4.37.2/en/add_new_model)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/3.0553ebe2.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: The ü§ó Transformers library is often able to offer new models thanks to community
    contributors. But this can be a challenging project and requires an in-depth knowledge
    of the ü§ó Transformers library and the model to implement. At Hugging Face, we‚Äôre
    trying to empower more of the community to actively add models and we‚Äôve put together
    this guide to walk you through the process of adding a PyTorch model (make sure
    you have [PyTorch installed](https://pytorch.org/get-started/locally/)).
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre interested in implementing a TensorFlow model, take a look at the
    [How to convert a ü§ó Transformers model to TensorFlow](add_tensorflow_model) guide!
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the way, you‚Äôll:'
  prefs: []
  type: TYPE_NORMAL
- en: get insights into open-source best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: understand the design principles behind one of the most popular deep learning
    libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learn how to efficiently test large models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learn how to integrate Python utilities like `black`, `ruff`, and `make fix-copies`
    to ensure clean and readable code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Hugging Face team member will be available to help you along the way so you‚Äôll
    never be alone. ü§ó ‚ù§Ô∏è
  prefs: []
  type: TYPE_NORMAL
- en: To get started, open a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml)
    issue for the model you want to see in ü§ó Transformers. If you‚Äôre not especially
    picky about contributing a specific model, you can filter by the [New model label](https://github.com/huggingface/transformers/labels/New%20model)
    to see if there are any unclaimed model requests and work on it.
  prefs: []
  type: TYPE_NORMAL
- en: Once you‚Äôve opened a new model request, the first step is to get familiar with
    ü§ó Transformers if you aren‚Äôt already!
  prefs: []
  type: TYPE_NORMAL
- en: General overview of ü§ó Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you should get a general overview of ü§ó Transformers. ü§ó Transformers is
    a very opinionated library, so there is a chance that you don‚Äôt agree with some
    of the library‚Äôs philosophies or design choices. From our experience, however,
    we found that the fundamental design choices and philosophies of the library are
    crucial to efficiently scale ü§ó Transformers while keeping maintenance costs at
    a reasonable level.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good first starting point to better understand the library is to read the
    [documentation of our philosophy](philosophy). As a result of our way of working,
    there are some choices that we try to apply to all models:'
  prefs: []
  type: TYPE_NORMAL
- en: Composition is generally favored over-abstraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicating code is not always bad if it strongly improves the readability or
    accessibility of a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model files are as self-contained as possible so that when you read the code
    of a specific model, you ideally only have to look into the respective `modeling_....py`
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our opinion, the library‚Äôs code is not just a means to provide a product,
    *e.g.* the ability to use BERT for inference, but also as the very product that
    we want to improve. Hence, when adding a model, the user is not only the person
    who will use your model, but also everybody who will read, try to understand,
    and possibly tweak your code.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let‚Äôs go a bit deeper into the general library design.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To successfully add a model, it is important to understand the interaction between
    your model and its config, [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    and [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).
    For exemplary purposes, we will call the model to be added to ü§ó Transformers `BrandNewBert`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea4f05530b7bf13a323b2cf6fd4f78ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we do make use of inheritance in ü§ó Transformers, but we keep
    the level of abstraction to an absolute minimum. There are never more than two
    levels of abstraction for any model in the library. `BrandNewBertModel` inherits
    from `BrandNewBertPreTrainedModel` which in turn inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and that‚Äôs it. As a general rule, we want to make sure that a new model only depends
    on [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    The important functionalities that are automatically provided to every new model
    are [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    which are used for serialization and deserialization. All of the other important
    functionalities, such as `BrandNewBertModel.forward` should be completely defined
    in the new `modeling_brand_new_bert.py` script. Next, we want to make sure that
    a model with a specific head layer, such as `BrandNewBertForMaskedLM` does not
    inherit from `BrandNewBertModel`, but rather uses `BrandNewBertModel` as a component
    that can be called in its forward pass to keep the level of abstraction low. Every
    new model requires a configuration class, called `BrandNewBertConfig`. This configuration
    is always stored as an attribute in [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    and thus can be accessed via the `config` attribute for all classes inheriting
    from `BrandNewBertPreTrainedModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the model, the configuration inherits basic serialization and deserialization
    functionalities from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).
    Note that the configuration and the model are always serialized into two different
    formats - the model to a *pytorch_model.bin* file and the configuration to a *config.json*
    file. Calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    will automatically call [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained),
    so that both model and configuration are saved.
  prefs: []
  type: TYPE_NORMAL
- en: Code style
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When coding your new model, keep in mind that Transformers is an opinionated
    library and we have a few quirks of our own regarding how code should be written
    :-)
  prefs: []
  type: TYPE_NORMAL
- en: The forward pass of your model should be fully written in the modeling file
    while being fully independent of other models in the library. If you want to reuse
    a block from another model, copy the code and paste it with a `# Copied from`
    comment on top (see [here](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)
    for a good example and [there](pr_checks#check-copies) for more documentation
    on Copied from).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code should be fully understandable, even by a non-native English speaker.
    This means you should pick descriptive variable names and avoid abbreviations.
    As an example, `activation` is preferred to `act`. One-letter variable names are
    strongly discouraged unless it‚Äôs an index in a for loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More generally we prefer longer explicit code to short magical one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid subclassing `nn.Sequential` in PyTorch but subclass `nn.Module` and write
    the forward pass, so that anyone using your code can quickly debug it by adding
    print statements or breaking points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your function signature should be type-annotated. For the rest, good variable
    names are way more readable and understandable than type annotations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overview of tokenizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not quite ready yet :-( This section will be added soon!
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step recipe to add a model to ü§ó Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Everyone has different preferences of how to port a model so it can be very
    helpful for you to take a look at summaries of how other contributors ported models
    to Hugging Face. Here is a list of community blog posts on how to port a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Porting GPT2 Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)
    by [Thomas](https://huggingface.co/thomwolf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt) by [Stas](https://huggingface.co/stas)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From experience, we can tell you that the most important things to keep in
    mind when adding a model are:'
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt reinvent the wheel! Most parts of the code you will add for the new ü§ó
    Transformers model already exist somewhere in ü§ó Transformers. Take some time to
    find similar, already existing models and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/)
    and [rg](https://github.com/BurntSushi/ripgrep) are your friends. Note that it
    might very well happen that your model‚Äôs tokenizer is based on one model implementation,
    and your model‚Äôs modeling code on another one. *E.g.* FSMT‚Äôs modeling code is
    based on BART, while FSMT‚Äôs tokenizer code is based on XLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It‚Äôs more of an engineering challenge than a scientific challenge. You should
    spend more time creating an efficient debugging environment rather than trying
    to understand all theoretical aspects of the model in the paper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask for help, when you‚Äôre stuck! Models are the core component of ü§ó Transformers
    so we at Hugging Face are more than happy to help you at every step to add your
    model. Don‚Äôt hesitate to ask if you notice you are not making progress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following, we try to give you a general recipe that we found most useful
    when porting a model to ü§ó Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list is a summary of everything that has to be done to add a
    model and can be used by you as a To-Do List:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê (Optional) Understood the model‚Äôs theoretical aspects
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Prepared ü§ó Transformers dev environment
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Set up debugging environment of the original repository
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Created script that successfully runs the `forward()` pass using the original
    repository and checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Successfully added the model skeleton to ü§ó Transformers
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Successfully converted original checkpoint to ü§ó Transformers checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Successfully ran `forward()` pass in ü§ó Transformers that gives identical output
    to original checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Finished model tests in ü§ó Transformers
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Successfully added tokenizer in ü§ó Transformers
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Run end-to-end integration tests
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Finished docs
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Uploaded model weights to the Hub
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê Submitted the pull request
  prefs: []
  type: TYPE_NORMAL
- en: ‚òê (Optional) Added a demo notebook
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we usually recommend starting by getting a good theoretical understanding
    of `BrandNewBert`. However, if you prefer to understand the theoretical aspects
    of the model *on-the-job*, then it is totally fine to directly dive into the `BrandNewBert`‚Äôs
    code-base. This option might suit you better if your engineering skills are better
    than your theoretical skill, if you have trouble understanding `BrandNewBert`‚Äôs
    paper, or if you just enjoy programming much more than reading scientific papers.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. (Optional) Theoretical aspects of BrandNewBert
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You should take some time to read *BrandNewBert‚Äôs* paper, if such descriptive
    work exists. There might be large sections of the paper that are difficult to
    understand. If this is the case, this is fine - don‚Äôt worry! The goal is not to
    get a deep theoretical understanding of the paper, but to extract the necessary
    information required to effectively re-implement the model in ü§ó Transformers.
    That being said, you don‚Äôt have to spend too much time on the theoretical aspects,
    but rather focus on the practical ones, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like
    decoder-only model? BART-like encoder-decoder model? Look at the [model_summary](model_summary)
    if you‚Äôre not familiar with the differences between those.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the applications of *brand_new_bert*? Text classification? Text generation?
    Seq2Seq tasks, *e.g.,* summarization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the novel feature of the model that makes it different from BERT/GPT-2/BART?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which of the already existing [ü§ó Transformers models](https://huggingface.co/transformers/#contents)
    is most similar to *brand_new_bert*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer?
    Is it the same tokenizer as used for BERT or BART?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you feel like you have gotten a good overview of the architecture of the
    model, you might want to write to the Hugging Face team with any questions you
    might have. This might include questions regarding the model‚Äôs architecture, its
    attention layer, etc. We will be more than happy to help you.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Next prepare your environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fork the [repository](https://github.com/huggingface/transformers) by clicking
    on the ‚ÄòFork‚Äô button on the repository‚Äôs page. This creates a copy of the code
    under your GitHub user account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone your `transformers` fork to your local disk, and add the base repository
    as a remote:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up a development environment, for instance by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on your OS, and since the number of optional dependencies of Transformers
    is growing, you might get a failure with this command. If that‚Äôs the case make
    sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow
    and/or Flax) then do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: which should be enough for most use cases. You can then return to the parent
    directory
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We recommend adding the PyTorch version of *brand_new_bert* to Transformers.
    To install PyTorch, please follow the instructions on [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note:** You don‚Äôt need to have CUDA installed. Making the new model work
    on CPU is sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To port *brand_new_bert*, you will also need access to its original repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now you have set up a development environment to port *brand_new_bert* to ü§ó
    Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.-4\. Run a pretrained checkpoint using the original repository
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At first, you will work on the original *brand_new_bert* repository. Often,
    the original implementation is very ‚Äúresearchy‚Äù. Meaning that documentation might
    be lacking and the code can be difficult to understand. But this should be exactly
    your motivation to reimplement *brand_new_bert*. At Hugging Face, one of our main
    goals is to *make people stand on the shoulders of giants* which translates here
    very well into taking a working model and rewriting it to make it as **accessible,
    user-friendly, and beautiful** as possible. This is the number-one motivation
    to re-implement models into ü§ó Transformers - trying to make complex new NLP technology
    accessible to **everybody**.
  prefs: []
  type: TYPE_NORMAL
- en: You should start thereby by diving into the original repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Successfully running the official pretrained model in the original repository
    is often **the most difficult** step. From our experience, it is very important
    to spend some time getting familiar with the original code-base. You need to figure
    out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Where to find the pretrained weights?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to load the pretrained weights into the corresponding model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to run the tokenizer independently from the model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trace one forward pass so that you know which classes and functions are required
    for a simple forward pass. Usually, you only have to reimplement those functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Be able to locate the important components of the model: Where is the model‚Äôs
    class? Are there model sub-classes, *e.g.* EncoderModel, DecoderModel? Where is
    the self-attention layer? Are there multiple different attention layers, *e.g.*
    *self-attention*, *cross-attention*‚Ä¶?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you debug the model in the original environment of the repo? Do you
    have to add *print* statements, can you work with an interactive debugger like
    *ipdb*, or should you use an efficient IDE to debug the model, like PyCharm?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very important that before you start the porting process, you can **efficiently**
    debug code in the original repository! Also, remember that you are working with
    an open-source library, so do not hesitate to open an issue, or even a pull request
    in the original repository. The maintainers of this repository are most likely
    very happy about someone looking into their code!
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is really up to you which debugging environment and strategy
    you prefer to use to debug the original model. We strongly advise against setting
    up a costly GPU environment, but simply work on a CPU both when starting to dive
    into the original repository and also when starting to write the ü§ó Transformers
    implementation of the model. Only at the very end, when the model has already
    been successfully ported to ü§ó Transformers, one should verify that the model also
    works as expected on GPU.
  prefs: []
  type: TYPE_NORMAL
- en: In general, there are two possible debugging environments for running the original
    model
  prefs: []
  type: TYPE_NORMAL
- en: '[Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local python scripts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter notebooks have the advantage that they allow for cell-by-cell execution
    which can be helpful to better split logical components from one another and to
    have faster debugging cycles as intermediate results can be stored. Also, notebooks
    are often easier to share with other contributors, which might be very helpful
    if you want to ask the Hugging Face team for help. If you are familiar with Jupyter
    notebooks, we strongly recommend you work with them.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious disadvantage of Jupyter notebooks is that if you are not used to
    working with them you will have to spend some time adjusting to the new programming
    environment and you might not be able to use your known debugging tools anymore,
    like `ipdb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each code-base, a good first step is always to load a **small** pretrained
    checkpoint and to be able to reproduce a single forward pass using a dummy integer
    vector of input IDs as an input. Such a script could look like this (in pseudocode):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, regarding the debugging strategy, there are generally a few from which
    to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: Decompose the original model into many small testable components and run a forward
    pass on each of those for verification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose the original model only into the original *tokenizer* and the original
    *model*, run a forward pass on those, and use intermediate print statements or
    breakpoints for verification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, it is up to you which strategy to choose. Often, one or the other is
    advantageous depending on the original code base.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the original code-base allows you to decompose the model into smaller sub-components,
    *e.g.* if the original code-base can easily be run in eager mode, it is usually
    worth the effort to do so. There are some important advantages to taking the more
    difficult road in the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: at a later stage when comparing the original model to the Hugging Face implementation,
    you can verify automatically for each component individually that the corresponding
    component of the ü§ó Transformers implementation matches instead of relying on visual
    comparison via print statements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it can give you some rope to decompose the big problem of porting a model into
    smaller problems of just porting individual components and thus structure your
    work better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: separating the model into logical meaningful components will help you to get
    a better overview of the model‚Äôs design and thus to better understand the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: at a later stage those component-by-component tests help you to ensure that
    no regression occurs as you continue changing your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lysandre‚Äôs](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed)
    integration checks for ELECTRA gives a nice example of how this can be done.'
  prefs: []
  type: TYPE_NORMAL
- en: However, if the original code-base is very complex or only allows intermediate
    components to be run in a compiled mode, it might be too time-consuming or even
    impossible to separate the model into smaller testable sub-components. A good
    example is [T5‚Äôs MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow)
    library which is very complex and does not offer a simple way to decompose the
    model into its sub-components. For such libraries, one often relies on verifying
    print statements.
  prefs: []
  type: TYPE_NORMAL
- en: No matter which strategy you choose, the recommended procedure is often the
    same that you should start to debug the starting layers first and the ending layers
    last.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is recommended that you retrieve the output, either by print statements
    or sub-component functions, of the following layers in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the input IDs passed to the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the word embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the input of the first Transformer layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the output of the first Transformer layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the output of the following n - 1 Transformer layers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the output of the whole BrandNewBert Model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input IDs should thereby consists of an array of integers, *e.g.* `input_ids
    = [0, 4, 4, 3, 2, 4, 1, 7, 19]`
  prefs: []
  type: TYPE_NORMAL
- en: 'The outputs of the following layers often consist of multi-dimensional float
    arrays and can look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We expect that every model added to ü§ó Transformers passes a couple of integration
    tests, meaning that the original model and the reimplemented version in ü§ó Transformers
    have to give the exact same output up to a precision of 0.001! Since it is normal
    that the exact same model written in different libraries can give a slightly different
    output depending on the library framework, we accept an error tolerance of 1e-3
    (0.001). It is not enough if the model gives nearly the same output, they have
    to be almost identical. Therefore, you will certainly compare the intermediate
    outputs of the ü§ó Transformers version multiple times against the intermediate
    outputs of the original implementation of *brand_new_bert* in which case an **efficient**
    debugging environment of the original repository is absolutely important. Here
    is some advice to make your debugging environment as efficient as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Find the best way of debugging intermediate results. Is the original repository
    written in PyTorch? Then you should probably take the time to write a longer script
    that decomposes the original model into smaller sub-components to retrieve intermediate
    values. Is the original repository written in Tensorflow 1? Then you might have
    to rely on TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print)
    to output intermediate values. Is the original repository written in Jax? Then
    make sure that the model is **not jitted** when running the forward pass, *e.g.*
    check-out [this link](https://github.com/google/jax/issues/196).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the smallest pretrained checkpoint you can find. The smaller the checkpoint,
    the faster your debug cycle becomes. It is not efficient if your pretrained model
    is so big that your forward pass takes more than 10 seconds. In case only very
    large checkpoints are available, it might make more sense to create a dummy model
    in the new environment with randomly initialized weights and save those weights
    for comparison with the ü§ó Transformers version of your model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure you are using the easiest way of calling a forward pass in the original
    repository. Ideally, you want to find the function in the original repository
    that **only** calls a single forward pass, *i.e.* that is often called `predict`,
    `evaluate`, `forward` or `__call__`. You don‚Äôt want to debug a function that calls
    `forward` multiple times, *e.g.* to generate text, like `autoregressive_sample`,
    `generate`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to separate the tokenization from the model‚Äôs *forward* pass. If the original
    repository shows examples where you have to input a string, then try to find out
    where in the forward call the string input is changed to input ids and start from
    this point. This might mean that you have to possibly write a small script yourself
    or change the original code so that you can directly input the ids instead of
    an input string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the model in your debugging setup is **not** in training mode,
    which often causes the model to yield random outputs due to multiple dropout layers
    in the model. Make sure that the forward pass in your debugging environment is
    **deterministic** so that the dropout layers are not used. Or use *transformers.utils.set_seed*
    if the old and new implementations are in the same framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section gives you more specific details/tips on how you can do
    this for *brand_new_bert*.
  prefs: []
  type: TYPE_NORMAL
- en: 5.-14\. Port BrandNewBert to ü§ó Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, you can finally start adding new code to ü§ó Transformers. Go into the
    clone of your ü§ó Transformers‚Äô fork:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the special case that you are adding a model whose architecture exactly matches
    the model architecture of an existing model you only have to add a conversion
    script as described in [this section](#write-a-conversion-script). In this case,
    you can just re-use the whole model architecture of the already existing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, let‚Äôs start generating a new model. You have two choices here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers-cli add-new-model-like` to add a new model like an existing one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers-cli add-new-model` to add a new model from our template (will
    look like BERT or Bart depending on the type of model you select)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, you will be prompted with a questionnaire to fill in the basic
    information of your model. The second command requires to install `cookiecutter`,
    you can find more information on it [here](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model).
  prefs: []
  type: TYPE_NORMAL
- en: '**Open a Pull Request on the main huggingface/transformers repo**'
  prefs: []
  type: TYPE_NORMAL
- en: Before starting to adapt the automatically generated code, now is the time to
    open a ‚ÄúWork in progress (WIP)‚Äù pull request, *e.g.* ‚Äú[WIP] Add *brand_new_bert*‚Äù,
    in ü§ó Transformers so that you and the Hugging Face team can work side-by-side
    on integrating the model into ü§ó Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a branch with a descriptive name from your main branch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Commit the automatically generated code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Fetch and rebase to current main
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Push the changes to your account using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once you are satisfied, go to the webpage of your fork on GitHub. Click on ‚ÄúPull
    request‚Äù. Make sure to add the GitHub handle of some members of the Hugging Face
    team as reviewers, so that the Hugging Face team gets notified for future changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the PR into a draft by clicking on ‚ÄúConvert to draft‚Äù on the right of
    the GitHub pull request web page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following, whenever you have made some progress, don‚Äôt forget to commit
    your work and push it to your account so that it shows in the pull request. Additionally,
    you should make sure to update your work with the current main from time to time
    by doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In general, all questions you might have regarding the model or your implementation
    should be asked in your PR and discussed/solved in the PR. This way, the Hugging
    Face team will always be notified when you are committing new code or if you have
    a question. It is often very helpful to point the Hugging Face team to your added
    code so that the Hugging Face team can efficiently understand your problem or
    question.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, you can go to the ‚ÄúFiles changed‚Äù tab where you see all of your changes,
    go to a line regarding which you want to ask a question, and click on the ‚Äú+‚Äù
    symbol to add a comment. Whenever a question or problem has been solved, you can
    click on the ‚ÄúResolve‚Äù button of the created comment.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the Hugging Face team will open comments when reviewing your
    code. We recommend asking most questions on GitHub on your PR. For some very general
    questions that are not very useful for the public, feel free to ping the Hugging
    Face team by Slack or email.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Adapt the generated models code for brand_new_bert**'
  prefs: []
  type: TYPE_NORMAL
- en: At first, we will focus only on the model itself and not care about the tokenizer.
    All the relevant code should be found in the generated files `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    and `src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can finally start coding :). The generated code in `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    will either have the same architecture as BERT if it‚Äôs an encoder-only model or
    BART if it‚Äôs an encoder-decoder model. At this point, you should remind yourself
    what you‚Äôve learned in the beginning about the theoretical aspects of the model:
    *How is the model different from BERT or BART?*‚Äù. Implement those changes which
    often means changing the *self-attention* layer, the order of the normalization
    layer, etc‚Ä¶ Again, it is often useful to look at the similar architecture of already
    existing models in Transformers to get a better feeling of how your model should
    be implemented.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note** that at this point, you don‚Äôt have to be very sure that your code
    is fully correct or clean. Rather, it is advised to add a first *unclean*, copy-pasted
    version of the original code to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    until you feel like all the necessary code is added. From our experience, it is
    much more efficient to quickly add a first version of the required code and improve/correct
    the code iteratively with the conversion script as described in the next section.
    The only thing that has to work at this point is that you can instantiate the
    ü§ó Transformers implementation of *brand_new_bert*, *i.e.* the following command
    should work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The above command will create a model according to the default parameters as
    defined in `BrandNewBertConfig()` with random weights, thus making sure that the
    `init()` methods of all components works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that all random initialization should happen in the `_init_weights` method
    of your `BrandnewBertPreTrainedModel` class. It should initialize all leaf modules
    depending on the variables of the config. Here is an example with the BERT `_init_weights`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can have some more custom schemes if you need a special initialization
    for some modules. For instance, in `Wav2Vec2ForPreTraining`, the last two linear
    layers need to have the initialization of the regular PyTorch `nn.Linear` but
    all the other ones should use an initialization as above. This is coded like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `_is_hf_initialized` flag is internally used to make sure we only initialize
    a submodule once. By setting it to `True` for `module.project_q` and `module.project_hid`,
    we make sure the custom initialization we did is not overridden later on, the
    `_init_weights` function won‚Äôt be applied to them.
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Write a conversion script**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, you should write a conversion script that lets you convert the checkpoint
    you used to debug *brand_new_bert* in the original repository to a checkpoint
    compatible with your just created ü§ó Transformers implementation of *brand_new_bert*.
    It is not advised to write the conversion script from scratch, but rather to look
    through already existing conversion scripts in ü§ó Transformers for one that has
    been used to convert a similar model that was written in the same framework as
    *brand_new_bert*. Usually, it is enough to copy an already existing conversion
    script and slightly adapt it for your use case. Don‚Äôt hesitate to ask the Hugging
    Face team to point you to a similar already existing conversion script for your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: If you are porting a model from TensorFlow to PyTorch, a good starting point
    might be BERT‚Äôs conversion script [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are porting a model from PyTorch to PyTorch, a good starting point might
    be BART‚Äôs conversion script [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following, we‚Äôll quickly explain how PyTorch models store layer weights
    and define layer names. In PyTorch, the name of a layer is defined by the name
    of the class attribute you give the layer. Let‚Äôs define a dummy model in PyTorch,
    called `SimpleModel` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create an instance of this model definition which will fill all
    weights: `dense`, `intermediate`, `layer_norm` with random weights. We can print
    the model to see its architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the layer names are defined by the name of the class attribute
    in PyTorch. You can print out the weight values of a specific layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: to see that the weights were randomly initialized
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the conversion script, you should fill those randomly initialized weights
    with the exact weights of the corresponding layer in the checkpoint. *E.g.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'While doing so, you must verify that each randomly initialized weight of your
    PyTorch model and its corresponding pretrained checkpoint weight exactly match
    in both **shape and name**. To do so, it is **necessary** to add assert statements
    for the shape and print out the names of the checkpoints weights. E.g. you should
    add statements like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Besides, you should also print out the names of both weights to make sure they
    match, *e.g.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If either the shape or the name doesn‚Äôt match, you probably assigned the wrong
    checkpoint weight to a randomly initialized layer of the ü§ó Transformers implementation.
  prefs: []
  type: TYPE_NORMAL
- en: An incorrect shape is most likely due to an incorrect setting of the config
    parameters in `BrandNewBertConfig()` that do not exactly match those that were
    used for the checkpoint you want to convert. However, it could also be that PyTorch‚Äôs
    implementation of a layer requires the weight to be transposed beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should also check that **all** required weights are initialized
    and print out all checkpoint weights that were not used for initialization to
    make sure the model is correctly converted. It is completely normal, that the
    conversion trials fail with either a wrong shape statement or a wrong name assignment.
    This is most likely because either you used incorrect parameters in `BrandNewBertConfig()`,
    have a wrong architecture in the ü§ó Transformers implementation, you have a bug
    in the `init()` functions of one of the components of the ü§ó Transformers implementation
    or you need to transpose one of the checkpoint weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'This step should be iterated with the previous step until all weights of the
    checkpoint are correctly loaded in the Transformers model. Having correctly loaded
    the checkpoint into the ü§ó Transformers implementation, you can then save the model
    under a folder of your choice `/path/to/converted/checkpoint/folder` that should
    then contain both a `pytorch_model.bin` file and a `config.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**7\. Implement the forward pass**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having managed to correctly load the pretrained weights into the ü§ó Transformers
    implementation, you should now make sure that the forward pass is correctly implemented.
    In [Get familiar with the original repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),
    you have already created a script that runs a forward pass of the model using
    the original repository. Now you should write an analogous script using the ü§ó
    Transformers implementation instead of the original one. It should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It is very likely that the ü§ó Transformers implementation and the original model
    implementation don‚Äôt give the exact same output the very first time or that the
    forward pass throws an error. Don‚Äôt be disappointed - it‚Äôs expected! First, you
    should make sure that the forward pass doesn‚Äôt throw any errors. It often happens
    that the wrong dimensions are used leading to a *Dimensionality mismatch* error
    or that the wrong data type object is used, *e.g.* `torch.long` instead of `torch.float32`.
    Don‚Äôt hesitate to ask the Hugging Face team for help, if you don‚Äôt manage to solve
    certain errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final part to make sure the ü§ó Transformers implementation works correctly
    is to ensure that the outputs are equivalent to a precision of `1e-3`. First,
    you should ensure that the output shapes are identical, *i.e.* `outputs.shape`
    should yield the same value for the script of the ü§ó Transformers implementation
    and the original implementation. Next, you should make sure that the output values
    are identical as well. This one of the most difficult parts of adding a new model.
    Common mistakes why the outputs are not identical are:'
  prefs: []
  type: TYPE_NORMAL
- en: Some layers were not added, *i.e.* an *activation* layer was not added, or the
    residual connection was forgotten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word embedding matrix was not tied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The wrong positional embeddings are used because the original implementation
    uses on offset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout is applied during the forward pass. To fix this make sure *model.training
    is False* and that no dropout layer is falsely activated during the forward pass,
    *i.e.* pass *self.training* to [PyTorch‚Äôs functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best way to fix the problem is usually to look at the forward pass of the
    original implementation and the ü§ó Transformers implementation side-by-side and
    check if there are any differences. Ideally, you should debug/print out intermediate
    outputs of both implementations of the forward pass to find the exact position
    in the network where the ü§ó Transformers implementation shows a different output
    than the original implementation. First, make sure that the hard-coded `input_ids`
    in both scripts are identical. Next, verify that the outputs of the first transformation
    of the `input_ids` (usually the word embeddings) are identical. And then work
    your way up to the very last layer of the network. At some point, you will notice
    a difference between the two implementations, which should point you to the bug
    in the ü§ó Transformers implementation. From our experience, a simple and efficient
    way is to add many print statements in both the original implementation and ü§ó
    Transformers implementation, at the same positions in the network respectively,
    and to successively remove print statements showing the same values for intermediate
    presentations.
  prefs: []
  type: TYPE_NORMAL
- en: When you‚Äôre confident that both implementations yield the same output, verify
    the outputs with `torch.allclose(original_output, output, atol=1e-3)`, you‚Äôre
    done with the most difficult part! Congratulations - the work left to be done
    should be a cakewalk üòä.
  prefs: []
  type: TYPE_NORMAL
- en: '**8\. Adding all necessary model tests**'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you have successfully added a new model. However, it is very
    much possible that the model does not yet fully comply with the required design.
    To make sure, the implementation is fully compatible with ü§ó Transformers, all
    common tests should pass. The Cookiecutter should have automatically added a test
    file for your model, probably under the same `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`.
    Run this test file to verify that all common tests pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Having fixed all common tests, it is now crucial to ensure that all the nice
    work you have done is well tested, so that
  prefs: []
  type: TYPE_NORMAL
- en: a) The community can easily understand your work by looking at specific tests
    of *brand_new_bert*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b) Future changes to your model will not break any important feature of the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At first, integration tests should be added. Those integration tests essentially
    do the same as the debugging scripts you used earlier to implement the model to
    ü§ó Transformers. A template of those model tests has already added by the Cookiecutter,
    called `BrandNewBertModelIntegrationTests` and only has to be filled out by you.
    To ensure that those tests are passing, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, all features that are special to *brand_new_bert* should be tested
    additionally in a separate test under `BrandNewBertModelTester`/``BrandNewBertModelTest`.
    This part is often forgotten but is extremely useful in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps to transfer the knowledge you have acquired during the model addition
    to the community by showing how the special features of *brand_new_bert* should
    work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future contributors can quickly test changes to the model by running those special
    tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**9\. Implement the tokenizer**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we should add the tokenizer of *brand_new_bert*. Usually, the tokenizer
    is equivalent to or very similar to an already existing tokenizer of ü§ó Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to find/extract the original tokenizer file and to manage
    to load this file into the ü§ó Transformers‚Äô implementation of the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that the tokenizer works correctly, it is recommended to first create
    a script in the original repository that inputs a string and returns the `input_ids‚Äú.
    It could look similar to this (in pseudo-code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You might have to take a deeper look again into the original repository to
    find the correct tokenizer function or you might even have to do changes to your
    clone of the original repository to only output the `input_ids`. Having written
    a functional tokenization script that uses the original repository, an analogous
    script for ü§ó Transformers should be created. It should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When both `input_ids` yield the same values, as a final step a tokenizer test
    file should also be added.
  prefs: []
  type: TYPE_NORMAL
- en: Analogous to the modeling test files of *brand_new_bert*, the tokenization test
    files of *brand_new_bert* should contain a couple of hard-coded integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**10\. Run End-to-end integration tests**'
  prefs: []
  type: TYPE_NORMAL
- en: Having added the tokenizer, you should also add a couple of end-to-end integration
    tests using both the model and the tokenizer to `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`
    in ü§ó Transformers. Such a test should show on a meaningful text-to-text sample
    that the ü§ó Transformers implementation works as expected. A meaningful text-to-text
    sample can include *e.g.* a source-to-target-translation pair, an article-to-summary
    pair, a question-to-answer pair, etc‚Ä¶ If none of the ported checkpoints has been
    fine-tuned on a downstream task it is enough to simply rely on the model tests.
    In a final step to ensure that the model is fully functional, it is advised that
    you also run all tests on GPU. It can happen that you forgot to add some `.to(self.device)`
    statements to internal tensors of the model, which in such a test would show in
    an error. In case you have no access to a GPU, the Hugging Face team can take
    care of running those tests for you.
  prefs: []
  type: TYPE_NORMAL
- en: '**11\. Add Docstring**'
  prefs: []
  type: TYPE_NORMAL
- en: Now, all the necessary functionality for *brand_new_bert* is added - you‚Äôre
    almost done! The only thing left to add is a nice docstring and a doc page. The
    Cookiecutter should have added a template file called `docs/source/model_doc/brand_new_bert.md`
    that you should fill out. Users of your model will usually first look at this
    page before using your model. Hence, the documentation must be understandable
    and concise. It is very useful for the community to add some *Tips* to show how
    the model should be used. Don‚Äôt hesitate to ping the Hugging Face team regarding
    the docstrings.
  prefs: []
  type: TYPE_NORMAL
- en: Next, make sure that the docstring added to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    is correct and included all necessary inputs and outputs. We have a detailed guide
    about writing documentation and our docstring format [here](writing-documentation).
    It is always to good to remind oneself that documentation should be treated at
    least as carefully as the code in ü§ó Transformers since the documentation is usually
    the first contact point of the community with the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code refactor**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Great, now you have added all the necessary code for *brand_new_bert*. At this
    point, you should correct some potential incorrect code style by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'and verify that your coding style passes the quality check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: There are a couple of other very strict design tests in ü§ó Transformers that
    might still be failing, which shows up in the tests of your pull request. This
    is often because of some missing information in the docstring or some incorrect
    naming. The Hugging Face team will surely help you if you‚Äôre stuck here.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it is always a good idea to refactor one‚Äôs code after having ensured
    that the code works correctly. With all tests passing, now it‚Äôs a good time to
    go over the added code again and do some refactoring.
  prefs: []
  type: TYPE_NORMAL
- en: You have now finished the coding part, congratulation! üéâ You are Awesome! üòé
  prefs: []
  type: TYPE_NORMAL
- en: '**12\. Upload the models to the model hub**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this final part, you should convert and upload all checkpoints to the model
    hub and add a model card for each uploaded model checkpoint. You can get familiar
    with the hub functionalities by reading our [Model sharing and uploading Page](model_sharing).
    You should work alongside the Hugging Face team here to decide on a fitting name
    for each checkpoint and to get the required access rights to be able to upload
    the model under the author‚Äôs organization of *brand_new_bert*. The `push_to_hub`
    method, present in all models in `transformers`, is a quick and efficient way
    to push your checkpoint to the hub. A little snippet is pasted below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: It is worth spending some time to create fitting model cards for each checkpoint.
    The model cards should highlight the specific characteristics of this particular
    checkpoint, *e.g.* On which dataset was the checkpoint pretrained/fine-tuned on?
    On what down-stream task should the model be used? And also include some code
    on how to correctly use the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**13\. (Optional) Add notebook**'
  prefs: []
  type: TYPE_NORMAL
- en: It is very helpful to add a notebook that showcases in-detail how *brand_new_bert*
    can be used for inference and/or fine-tuned on a downstream task. This is not
    mandatory to merge your PR, but very useful for the community.
  prefs: []
  type: TYPE_NORMAL
- en: '**14\. Submit your finished PR**'
  prefs: []
  type: TYPE_NORMAL
- en: You‚Äôre done programming now and can move to the last step, which is getting
    your PR merged into main. Usually, the Hugging Face team should have helped you
    already at this point, but it is worth taking some time to give your finished
    PR a nice description and eventually add comments to your code, if you want to
    point out certain design choices to your reviewer.
  prefs: []
  type: TYPE_NORMAL
- en: Share your work!!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, it‚Äôs time to get some credit from the community for your work! Having completed
    a model addition is a major contribution to Transformers and the whole NLP community.
    Your code and the ported pre-trained models will certainly be used by hundreds
    and possibly even thousands of developers and researchers. You should be proud
    of your work and share your achievements with the community.
  prefs: []
  type: TYPE_NORMAL
- en: '**You have made another model that is super easy to access for everyone in
    the community! ü§Ø**'
  prefs: []
  type: TYPE_NORMAL
