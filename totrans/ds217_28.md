# 故障排除

> 原始文本：[https://huggingface.co/docs/datasets/troubleshoot](https://huggingface.co/docs/datasets/troubleshoot)

本指南旨在为您提供导航一些常见问题所需的工具和知识。如果本指南中列出的建议未涵盖您的情况，请参考[寻求帮助](#asking-for-help)部分，了解在哪里找到有关您特定问题的帮助。

## 使用push_to_hub上传数据集时出现的问题

### 身份验证问题

如果在使用[Dataset.push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.push_to_hub)和Hugging Face访问令牌在🤗 Hub上共享数据集时遇到身份验证问题：

+   确保您用于身份验证的Hugging Face令牌具有**写入**权限。

+   在OSX上，可能有助于清理您的钥匙串访问中所有的huggingface.co密码，以及在使用`huggingface-cli login`之前重新配置`git config --global credential.helper osxkeychain`。

或者，您可以使用SSH密钥进行身份验证-在[🤗 Hub文档](https://huggingface.co/docs/hub/security-git-ssh)中阅读更多信息。

### 大型数据集上传时断开连接

当向Hub上传大型数据集时，如果数据集分片数量较多，可能会在短时间内为Hub创建过多的提交。这将导致连接错误。连接错误也可能是由Hub内部使用的AWS S3存储桶返回的HTTP 500错误引起的。在任何情况下，您可以重新运行[Dataset.push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.push_to_hub)以继续进行数据集上传。Hub将检查已上传分片的SHAs，以避免重新上传它们。我们正在努力使上传过程更具鲁棒性以应对瞬态错误，因此始终更新到最新的库版本是一个好主意。

### 请求过多

通过`push_to_hub()`上传大型数据集可能会导致错误：

```py
HfHubHTTPError: 429 Client Error: Too Many Requests for url: ...
You have exceeded our hourly quotas for action: commit. We invite you to retry later.
```

如果遇到此问题，您需要将`datasets`库升级到最新版本（或至少为`2.15.0`）。

## 从自定义数据创建数据集时出现的问题

### 从文件夹加载图像和音频

从文件夹创建数据集时，最常见的问题之一是文件结构不符合预期格式，或者元数据文件存在问题。

在相应的文档页面中了解所需的文件夹结构：

+   [AudioFolder](https://huggingface.co/docs/datasets/audio_dataset#audiofolder)

+   [ImageFolder](https://huggingface.co/docs/datasets/image_dataset#imagefolder)

### 腌制问题

#### 在使用Dataset.from_generator时出现的腌制问题

创建数据集时，[IterableDataset.from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.from_generator)和[Dataset.from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_generator)需要一个“可腌制”的生成器函数。这是必要的，以使用[`pickle`](https://docs.python.org/3/library/pickle.html)对函数进行哈希处理，以便能够将数据集缓存到磁盘上。

虽然生成器函数通常是“可腌制”的，但请注意生成器对象不是。因此，如果您使用生成器对象，您将遇到类似于这样的`TypeError`：

```py
TypeError: cannot pickle 'generator' object
```

当使用一个使用全局对象的生成器函数时，该全局对象不是“可腌制”的，例如数据库连接，这种错误也可能发生。如果是这种情况，您可以直接在生成器函数内初始化这样的对象，以避免此错误。

#### 使用Dataset.map时的腌制问题

在多进程Dataset.map()中也可能发生腌制错误-对象被腌制以传递给子进程。如果转换中使用的对象不可腌制，则无法缓存`map`的结果，这将导致错误被引发。

以下是解决此问题的一些方法：

+   解决pickle问题的通用解决方案是确保对象（或生成器类）通过手动实现`__getstate__` / `__setstate__` / `__reduce__`可被pickle。

+   您还可以在`map`中使用`new_fingerprint`参数提供自己的唯一哈希值。

+   您还可以通过调用`datasets.disable_caching()`来禁用缓存，但这是不可取的- [了解更多关于缓存重要性的信息](cache)

## 寻求帮助

如果上述故障排除建议无法帮助您解决问题，请向社区和团队寻求帮助。

### 论坛

在Hugging Face论坛上寻求帮助-在[🤗数据集类别](https://discuss.huggingface.co/c/datasets/10)中发布您的问题。请确保写一个描述性的帖子，提供有关您的设置和可重现代码的相关上下文，以最大程度地增加解决问题的可能性！

### Discord

在[Discord](http://hf.co/join/discord)上发布问题，让团队和社区帮助您。

### 在🤗 Hub上的社区讨论

如果您在Hub上使用脚本创建自定义数据集时遇到问题，可以通过在数据集的社区选项卡中打开讨论并发送以下消息向Hugging Face团队寻求帮助：

```py
# Dataset rewiew request for <Dataset name>

## Description

<brief description of the dataset>

## Files to review

- file1
- file2
- ...

cc @lhoestq @polinaeterna @mariosasko @albertvillanova
```

### GitHub问题

最后，如果您怀疑发现与库本身相关的错误，请在🤗数据集[GitHub存储库](https://github.com/huggingface/datasets/issues)上创建一个问题。包括有关错误的上下文：用于重现的代码片段，有关您的环境和数据的详细信息等，以帮助我们找出问题所在以及如何解决它。
