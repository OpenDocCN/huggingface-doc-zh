# 视频视觉Transformer（ViViT）

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vivit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vivit)

## 概述

ViViT模型是由Anurag Arnab、Mostafa Dehghani、Georg Heigold、Chen Sun、Mario Lučić、Cordelia Schmid提出的，论文标题为[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)。该论文提出了一组基于纯Transformer的成功视频理解模型。

论文摘要如下：

*我们提出了基于纯Transformer的视频分类模型，借鉴了这些模型在图像分类中的最近成功。我们的模型从输入视频中提取时空标记，然后通过一系列Transformer层对其进行编码。为了处理视频中遇到的长序列标记，我们提出了我们模型的几种高效变体，这些变体因子化了输入的空间和时间维度。尽管已知基于Transformer的模型只有在有大型训练数据集时才有效，但我们展示了如何在训练过程中有效地正则化模型，并利用预训练的图像模型能够在相对较小的数据集上进行训练。我们进行了彻底的消融研究，并在多个视频分类基准测试中取得了最先进的结果，包括Kinetics 400和600、Epic Kitchens、Something-Something v2和Moments in Time，优于基于深度3D卷积网络的先前方法。*

该模型由[jegormeister](https://huggingface.co/jegormeister)贡献。原始代码（使用JAX编写）可在[此处](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)找到。

## VivitConfig

### `class transformers.VivitConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/configuration_vivit.py#L31)

```py
( image_size = 224 num_frames = 32 tubelet_size = [2, 16, 16] num_channels = 3 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu_fast' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-06 qkv_bias = True **kwargs )
```

参数

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `num_frames` (`int`, *optional*, defaults to 32) — 每个视频中的帧数。

+   `tubelet_size` (`List[int]`, *optional*, defaults to `[2, 16, 16]`) — 每个tubelet的大小（分辨率）。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu_fast"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`、`"gelu_fast"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — 层归一化层使用的epsilon。

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。

这是用于存储[VivitModel](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitModel)配置的配置类。它用于根据指定的参数实例化一个ViViT模型，定义模型架构。使用默认值实例化配置将产生类似于ViViT [google/vivit-b-16x2-kinetics400](https://huggingface.co/google/vivit-b-16x2-kinetics400)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例:

```py
>>> from transformers import VivitConfig, VivitModel

>>> # Initializing a ViViT google/vivit-b-16x2-kinetics400 style configuration
>>> configuration = VivitConfig()

>>> # Initializing a model (with random weights) from the google/vivit-b-16x2-kinetics400 style configuration
>>> model = VivitModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## VivitImageProcessor

### `class transformers.VivitImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/image_processing_vivit.py#L64)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00784313725490196 offset: bool = True do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

参数

+   `do_resize` (`bool`, *可选*, 默认为`True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。

+   `size` (`Dict[str, int]` *可选*, 默认为`{"shortest_edge" -- 256}`): 调整大小后的输出图像大小。图像的最短边将被调整为`size["shortest_edge"]`，同时保持原始图像的纵横比。可以被`preprocess`方法中的`size`覆盖。

+   `resample` (`PILImageResampling`, *可选*, 默认为`Resampling.BILINEAR`) — 如果调整图像大小，要使用的重采样滤波器。可以被`preprocess`方法中的`resample`参数覆盖。

+   `do_center_crop` (`bool`, *可选*, 默认为`True`) — 是否将图像中心裁剪到指定的`crop_size`。可以被`preprocess`方法中的`do_center_crop`参数覆盖。

+   `crop_size` (`Dict[str, int]`, *可选*, 默认为`{"height" -- 224, "width": 224}`): 应用中心裁剪后的图像大小。可以被`preprocess`方法中的`crop_size`参数覆盖。

+   `do_rescale` (`bool`, *可选*, 默认为`True`) — 是否按照指定的比例`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`参数覆盖。

+   `rescale_factor` (`int` or `float`, *可选*, 默认为`1/127.5`) — 如果重新缩放图像，定义要使用的比例因子。可以被`preprocess`方法中的`rescale_factor`参数覆盖。

+   `offset` (`bool`, *可选*, 默认为`True`) — 是否在负方向和正方向上缩放图像。可以被`preprocess`方法中的`offset`参数覆盖。

+   `do_normalize` (`bool`, *可选*, 默认为`True`) — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。

+   `image_mean` (`float` or `List[float]`, *可选*, 默认为`IMAGENET_STANDARD_MEAN`) — 如果对图像进行归一化，要使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` or `List[float]`, *可选*, 默认为`IMAGENET_STANDARD_STD`) — 如果对图像进行归一化，要使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_std`参数覆盖。

构建一个Vivit图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/image_processing_vivit.py#L285)

```py
( videos: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None offset: bool = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `videos` (`ImageInput`) — 预处理的视频帧。期望单个或批量的视频帧，像素值范围从0到255。如果传入像素值在0到1之间的帧，请设置`do_rescale=False`。

+   `do_resize` (`bool`，*可选*，默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`，*可选*，默认为 `self.size`) — 应用调整大小后的图像大小。

+   `resample` (`PILImageResampling`，*可选*，默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。可以是枚举 `PILImageResampling` 中的一个，仅在 `do_resize` 设置为 `True` 时有效。

+   `do_center_crop` (`bool`，*可选*，默认为 `self.do_centre_crop`) — 是否对图像进行中心裁剪。

+   `crop_size` (`Dict[str, int]`，*可选*，默认为 `self.crop_size`) — 应用中心裁剪后的图像大小。

+   `do_rescale` (`bool`，*可选*，默认为 `self.do_rescale`) — 如果 `offset` 为 `True`，是否在 `[-1 - 1]` 之间重新缩放图像值，否则在 `[0, 1]` 之间。

+   `rescale_factor` (`float`，*可选*，默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则重新缩放图像的重新缩放因子。

+   `offset` (`bool`，*可选*，默认为 `self.offset`) — 是否在负方向和正方向上缩放图像。

+   `do_normalize` (`bool`，*可选*，默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`，*可选*，默认为 `self.image_mean`) — 图像均值。

+   `image_std` (`float` 或 `List[float]`，*可选*，默认为 `self.image_std`) — 图像标准差。

+   `return_tensors` (`str` 或 `TensorType`，*可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回类型为 `tf.Tensor` 的批处理。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回类型为 `torch.Tensor` 的批处理。

    +   `TensorType.NUMPY` 或 `'np'`：返回类型为 `np.ndarray` 的批处理。

    +   `TensorType.JAX` 或 `'jax'`：返回类型为 `jax.numpy.ndarray` 的批处理。

+   `data_format` (`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`：图像以 (通道数, 高度, 宽度) 格式。

    +   `ChannelDimension.LAST`：图像以 (高度, 宽度, 通道数) 格式。

    +   未设置：使用推断的输入图像通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`，*可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`：图像以 (高度, 宽度) 格式。

预处理图像或图像批处理。

## VivitModel

### `class transformers.VivitModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/modeling_vivit.py#L446)

```py
( config add_pooling_layer = True )
```

参数

+   `config` ([VivitConfig](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

ViViT Transformer 模型的基本输出，输出原始隐藏状态，没有特定的头部。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/modeling_vivit.py#L478)

```py
( pixel_values: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_frames, num_channels, height, width)`) — 像素值。可以使用[VivitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitImageProcessor)获取像素值。详情请参阅[VivitImageProcessor.preprocess()](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitImageProcessor.preprocess)。

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间。

    +   1表示头部为`未屏蔽`,

    +   0表示头部为`屏蔽`。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回值

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[VivitConfig](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitConfig)）和输入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后的序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入层的输出+每层的输出）。

    每层模型的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[VivitModel](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import av
>>> import numpy as np

>>> from transformers import VivitImageProcessor, VivitModel
>>> from huggingface_hub import hf_hub_download

>>> np.random.seed(0)

>>> def read_video_pyav(container, indices):
...     '''
...     Decode the video with PyAV decoder.
...     Args:
...         container (`av.container.input.InputContainer`): PyAV container.
...         indices (`List[int]`): List of frame indices to decode.
...     Returns:
...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
...     '''
...     frames = []
...     container.seek(0)
...     start_index = indices[0]
...     end_index = indices[-1]
...     for i, frame in enumerate(container.decode(video=0)):
...         if i > end_index:
...             break
...         if i >= start_index and i in indices:
...             frames.append(frame)
...     return np.stack([x.to_ndarray(format="rgb24") for x in frames])

>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
...     '''
...     Sample a given number of frame indices from the video.
...     Args:
...         clip_len (`int`): Total number of frames to sample.
...         frame_sample_rate (`int`): Sample every n-th frame.
...         seg_len (`int`): Maximum allowed index of sample's last frame.
...     Returns:
...         indices (`List[int]`): List of sampled frame indices
...     '''
...     converted_len = int(clip_len * frame_sample_rate)
...     end_idx = np.random.randint(converted_len, seg_len)
...     start_idx = end_idx - converted_len
...     indices = np.linspace(start_idx, end_idx, num=clip_len)
...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
...     return indices

>>> # video clip consists of 300 frames (10 seconds at 30 FPS)
>>> file_path = hf_hub_download(
...     repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
... )
>>> container = av.open(file_path)

>>> # sample 32 frames
>>> indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=container.streams.video[0].frames)
>>> video = read_video_pyav(container=container, indices=indices)

>>> image_processor = VivitImageProcessor.from_pretrained("google/vivit-b-16x2-kinetics400")
>>> model = VivitModel.from_pretrained("google/vivit-b-16x2-kinetics400")

>>> # prepare video for the model
>>> inputs = image_processor(list(video), return_tensors="pt")

>>> # forward pass
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 3137, 768]
```

## VivitForVideoClassification

### `class transformers.VivitForVideoClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/modeling_vivit.py#L599)

```py
( config )
```

参数

+   `config` ([VivitConfig](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有视频分类头部的ViViT Transformer模型（在[CLS]标记的最终隐藏状态之上的线性层），例如用于Kinetics-400。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vivit/modeling_vivit.py#L617)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_frames, num_channels, height, width)`) — 像素值。像素值可以使用[VivitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitImageProcessor)获取。有关详细信息，请参阅[VivitImageProcessor.preprocess()](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitImageProcessor.preprocess)。

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels==1`，则计算回归损失（均方损失），如果`config.num_labels>1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或`config.return_dict=False`），包括根据配置（[VivitConfig](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitConfig)）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回） — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — 模型在每个阶段输出的隐藏状态（也称为特征图）的元组，形状为`(batch_size, sequence_length, hidden_size)`。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[VivitForVideoClassification](/docs/transformers/v4.37.2/en/model_doc/vivit#transformers.VivitForVideoClassification) 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个函数，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import av
>>> import numpy as np
>>> import torch

>>> from transformers import VivitImageProcessor, VivitForVideoClassification
>>> from huggingface_hub import hf_hub_download

>>> np.random.seed(0)

>>> def read_video_pyav(container, indices):
...     '''
...     Decode the video with PyAV decoder.
...     Args:
...         container (`av.container.input.InputContainer`): PyAV container.
...         indices (`List[int]`): List of frame indices to decode.
...     Returns:
...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
...     '''
...     frames = []
...     container.seek(0)
...     start_index = indices[0]
...     end_index = indices[-1]
...     for i, frame in enumerate(container.decode(video=0)):
...         if i > end_index:
...             break
...         if i >= start_index and i in indices:
...             frames.append(frame)
...     return np.stack([x.to_ndarray(format="rgb24") for x in frames])

>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
...     '''
...     Sample a given number of frame indices from the video.
...     Args:
...         clip_len (`int`): Total number of frames to sample.
...         frame_sample_rate (`int`): Sample every n-th frame.
...         seg_len (`int`): Maximum allowed index of sample's last frame.
...     Returns:
...         indices (`List[int]`): List of sampled frame indices
...     '''
...     converted_len = int(clip_len * frame_sample_rate)
...     end_idx = np.random.randint(converted_len, seg_len)
...     start_idx = end_idx - converted_len
...     indices = np.linspace(start_idx, end_idx, num=clip_len)
...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
...     return indices

>>> # video clip consists of 300 frames (10 seconds at 30 FPS)
>>> file_path = hf_hub_download(
...     repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
... )
>>> container = av.open(file_path)

>>> # sample 32 frames
>>> indices = sample_frame_indices(clip_len=32, frame_sample_rate=4, seg_len=container.streams.video[0].frames)
>>> video = read_video_pyav(container=container, indices=indices)

>>> image_processor = VivitImageProcessor.from_pretrained("google/vivit-b-16x2-kinetics400")
>>> model = VivitForVideoClassification.from_pretrained("google/vivit-b-16x2-kinetics400")

>>> inputs = image_processor(list(video), return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     logits = outputs.logits

>>> # model predicts one of the 400 Kinetics-400 classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
LABEL_116
```
