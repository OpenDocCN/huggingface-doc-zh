- en: Training from memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/training_from_memory](https://huggingface.co/docs/tokenizers/training_from_memory)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: In the [Quicktour](quicktour), we saw how to build and train a tokenizer using
    text files, but we can actually use any Python Iterator. In this section we‚Äôll
    see a few different ways of training our tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For all the examples listed below, we‚Äôll use the same [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    and `Trainer`, built as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This tokenizer is based on the [Unigram](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Unigram)
    model. It takes care of normalizing the input using the NFKC Unicode normalization
    method, and uses a [ByteLevel](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)
    pre-tokenizer with the corresponding decoder.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the components used here, you can check [here](components).
  prefs: []
  type: TYPE_NORMAL
- en: The most basic way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you probably guessed already, the easiest way to train our tokenizer is
    by using a `List`{.interpreted-text role=‚Äúobj‚Äù}:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Easy, right? You can use anything working as an iterator here, be it a `List`{.interpreted-text
    role=‚Äúobj‚Äù}, `Tuple`{.interpreted-text role=‚Äúobj‚Äù}, or a `np.Array`{.interpreted-text
    role=‚Äúobj‚Äù}. Anything works as long as it provides strings.
  prefs: []
  type: TYPE_NORMAL
- en: Using the ü§ó Datasets library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An awesome way to access one of the many datasets that exist out there is by
    using the ü§ó Datasets library. For more information about it, you should check
    [the official documentation here](https://huggingface.co/docs/datasets/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs start by loading our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build an iterator over this dataset. The easiest way to
    do this is probably by using a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see here, for improved efficiency we can actually provide a batch
    of examples used to train, instead of iterating over them one by one. By doing
    so, we can expect performances very similar to those we got while training directly
    from files.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our iterator ready, we just need to launch the training. In order to improve
    the look of our progress bars, we can specify the total length of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And that‚Äôs it!
  prefs: []
  type: TYPE_NORMAL
- en: Using gzip files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since gzip files in Python can be used as iterators, it is extremely simple
    to train on such files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we wanted to train from multiple gzip files, it wouldn‚Äôt be much harder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: And voil√†!
  prefs: []
  type: TYPE_NORMAL
