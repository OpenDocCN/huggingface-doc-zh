- en: Hands-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit8/hands-on-cleanrl](https://huggingface.co/learn/deep-rl-course/unit8/hands-on-cleanrl)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/83.03c23455.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/CodeBlock.a6d3f852.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/CourseFloatingBanner.36c274d0.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">[![Ask
    a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we studied the theory behind PPO, the best way to understand how it
    works **is to implement it from scratch.**
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an architecture from scratch is the best way to understand it,
    and it’s a good habit. We have already done it for a value-based method with Q-Learning
    and a Policy-based method with Reinforce.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to be able to code it, we’re going to use two resources:'
  prefs: []
  type: TYPE_NORMAL
- en: A tutorial made by [Costa Huang](https://github.com/vwxyzjn). Costa is behind
    [CleanRL](https://github.com/vwxyzjn/cleanrl), a Deep Reinforcement Learning library
    that provides high-quality single-file implementation with research-friendly features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the tutorial, to go deeper, you can read the 13 core implementation
    details: [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, to test its robustness, we’re going to train it in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[LunarLander-v2](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <assets/63_deep_rl_intro/lunarlander.mp4>
  prefs: []
  type: TYPE_NORMAL
- en: And finally, we will push the trained model to the Hub to evaluate and visualize
    your agent playing.
  prefs: []
  type: TYPE_NORMAL
- en: LunarLander-v2 is the first environment you used when you started this course.
    At that time, you didn’t know how it worked, and now you can code it from scratch
    and train it. **How incredible is that 🤩.**
  prefs: []
  type: TYPE_NORMAL
- en: '[https://giphy.com/embed/pynZagVcYxVUk](https://giphy.com/embed/pynZagVcYxVUk)'
  prefs: []
  type: TYPE_NORMAL
- en: '[via GIPHY](https://giphy.com/gifs/the-office-michael-heartbreak-pynZagVcYxVUk)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started! 🚀
  prefs: []
  type: TYPE_NORMAL
- en: 'The colab notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit8/unit8_part1.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 8: Proximal Policy Gradient (PPO) with PyTorch 🤖'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
  prefs: []
  type: TYPE_IMG
- en: In this notebook, you’ll learn to **code your PPO agent from scratch with PyTorch
    using CleanRL implementation as model**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test its robustness, we’re going to train it in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[LunarLander-v2 🚀](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Be able to **code your PPO agent from scratch using PyTorch**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score 🔥.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites 🏗️
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: 🔲 📚 Study [PPO by reading Unit 8](https://huggingface.co/deep-rl-course/unit8/introduction)
    🤗
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push one model, we don’t ask for a minimal result but we **advise
    you to try different hyperparameters settings to get better results**.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t find your model, **go to the bottom of the page and click on the
    refresh button**
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: Set the GPU 💪
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  prefs: []
  type: TYPE_IMG
- en: Create a virtual display 🔽
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the notebook, we’ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install the librairies and create and run a virtual
    screen 🖥
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Install dependencies 🔽
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, we use `gym==0.21` because the video was recorded with Gym.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s code PPO from scratch with Costa Huang’s tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the core implementation of PPO we’re going to use the excellent [Costa Huang](https://costa.sh/)
    tutorial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to the tutorial, to go deeper you can read the 37 core implementation
    details: [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '👉 The video tutorial: [https://youtu.be/MEt6rrxH8W4](https://youtu.be/MEt6rrxH8W4)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Add the Hugging Face Integration 🤗
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to push our model to the Hub, we need to define a function `package_to_hub`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add dependencies we need to push our model to the Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Add new argument in `parse_args()` function to define the repo-id where we want
    to push the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we add the methods needed to push the model to the Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methods will:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_evalutate_agent()`: evaluate the agent.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_generate_model_card()`: generate the model card of your agent.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_record_video()`: record a video of your agent.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we call this function at the end of the PPO training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what the final ppo.py file looks like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Sign in and get your authentication token from the Hugging Face website.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and paste the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to use Google Colab or a Jupyter Notebook, you need to use
    this command instead: `huggingface-cli login`'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start the training 🔥
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ⚠️ ⚠️ ⚠️ Don’t use **the same repo id with the one you used for the Unit 1**
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve coded PPO from scratch and added the Hugging Face Integration,
    we’re ready to start the training 🔥
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, you need to copy all your code to a file you create called `ppo.py`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![PPO](../Images/c6a57155d0c4da38fd607c740b13277e.png) ![PPO](../Images/2fd5c967c9514b78a4b2bbbefa476afd.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we just need to run this python script using `python <name-of-python-script>.py`
    with the additional parameters we defined using `argparse`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should modify more hyperparameters otherwise the training will not be super
    stable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Some additional challenges 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to learn **is to try things on your own**! Why not try another
    environment? Or why not trying to modify the implementation to work with Gymnasium?
  prefs: []
  type: TYPE_NORMAL
- en: See you in Unit 8, part 2 where we’re going to train agents to play Doom 🔥
  prefs: []
  type: TYPE_NORMAL
- en: Keep learning, stay awesome 🤗
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
