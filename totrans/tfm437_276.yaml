- en: MobileNet V2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MobileNet V2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear
    Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard,
    Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'MobileNet模型是由Mark Sandler、Andrew Howard、Menglong Zhu、Andrey Zhmoginov、Liang-Chieh
    Chen在[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*In this paper we describe a new mobile architecture, MobileNetV2, that improves
    the state of the art performance of mobile models on multiple tasks and benchmarks
    as well as across a spectrum of different model sizes. We also describe efficient
    ways of applying these mobile models to object detection in a novel framework
    we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation
    models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我们描述了一种新的移动架构MobileNetV2，它提高了移动模型在多个任务和基准上的性能，并跨不同模型大小的光谱。我们还描述了将这些移动模型应用于对象检测的高效方法，这是我们称之为SSDLite的新颖框架。此外，我们演示了如何通过我们称之为Mobile
    DeepLabV3的DeepLabv3的简化形式构建移动语义分割模型。*'
- en: '*The MobileNetV2 architecture is based on an inverted residual structure where
    the input and output of the residual block are thin bottleneck layers opposite
    to traditional residual models which use expanded representations in the input
    an MobileNetV2 uses lightweight depthwise convolutions to filter features in the
    intermediate expansion layer. Additionally, we find that it is important to remove
    non-linearities in the narrow layers in order to maintain representational power.
    We demonstrate that this improves performance and provide an intuition that led
    to this design. Finally, our approach allows decoupling of the input/output domains
    from the expressiveness of the transformation, which provides a convenient framework
    for further analysis. We measure our performance on Imagenet classification, COCO
    object detection, VOC image segmentation. We evaluate the trade-offs between accuracy,
    and number of operations measured by multiply-adds (MAdd), as well as the number
    of parameters.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*MobileNetV2架构基于倒置残差结构，其中残差块的输入和输出是薄瓶颈层，与传统的残差模型相反，传统模型在输入中使用扩展表示，MobileNetV2使用轻量级深度卷积来过滤中间扩展层中的特征。此外，我们发现在窄层中去除非线性对于保持表征能力是重要的。我们证明这可以提高性能，并提供导致这种设计的直觉。最后，我们的方法允许将输入/输出域与变换的表现力分离，为进一步分析提供了便利的框架。我们在Imagenet分类、COCO目标检测、VOC图像分割上衡量我们的性能。我们评估了准确性和操作数量之间的权衡，操作数量由乘加操作（MAdd）和参数数量来衡量。*'
- en: This model was contributed by [matthijs](https://huggingface.co/Matthijs). The
    original code and weights can be found [here for the main model](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
    and [here for DeepLabV3+](https://github.com/tensorflow/models/tree/master/research/deeplab).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[matthijs](https://huggingface.co/Matthijs)贡献。原始代码和权重可以在[主模型这里找到](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)，在[DeepLabV3+这里找到](https://github.com/tensorflow/models/tree/master/research/deeplab)。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: The checkpoints are named **mobilenet_v2_*depth*_*size***, for example **mobilenet_v2_1.0_224**,
    where **1.0** is the depth multiplier (sometimes also referred to as “alpha” or
    the width multiplier) and **224** is the resolution of the input images the model
    was trained on.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点命名为**mobilenet_v2_*depth*_*size***，例如**mobilenet_v2_1.0_224**，其中**1.0**是深度乘数（有时也称为“alpha”或宽度乘数），**224**是模型训练的输入图像的分辨率。
- en: Even though the checkpoint is trained on images of specific size, the model
    will work on images of any size. The smallest supported image size is 32x32.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管检查点是在特定大小的图像上训练的，但模型可以在任何大小的图像上运行。支持的最小图像大小为32x32。
- en: One can use [MobileNetV2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessor)
    to prepare images for the model.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用[MobileNetV2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessor)来为模型准备图像。
- en: 'The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet
    plus an extra “background” class (index 0).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的图像分类检查点是在[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)上预训练的（也称为ILSVRC
    2012，包含130万张图像和1000个类）。然而，模型预测1001个类别：来自ImageNet的1000个类别加上额外的“背景”类（索引0）。
- en: The segmentation model uses a [DeepLabV3+](https://arxiv.org/abs/1802.02611)
    head. The available semantic segmentation checkpoints are pre-trained on [PASCAL
    VOC](http://host.robots.ox.ac.uk/pascal/VOC/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割模型使用[DeepLabV3+](https://arxiv.org/abs/1802.02611)头部。可用的语义分割检查点是在[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)上预训练的。
- en: The original TensorFlow checkpoints use different padding rules than PyTorch,
    requiring the model to determine the padding amount at inference time, since this
    depends on the input image size. To use native PyTorch padding behavior, create
    a [MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config)
    with `tf_padding = False`.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的TensorFlow检查点使用不同的填充规则，需要模型在推断时确定填充量，因为这取决于输入图像的大小。要使用原生PyTorch填充行为，请创建一个[MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config)，其中`tf_padding
    = False`。
- en: 'Unsupported features:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不支持的功能：
- en: The [MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model)
    outputs a globally pooled version of the last hidden state. In the original model
    it is possible to use an average pooling layer with a fixed 7x7 window and stride
    1 instead of global pooling. For inputs that are larger than the recommended image
    size, this gives a pooled output that is larger than 1x1\. The Hugging Face implementation
    does not support this.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model)输出最后一个隐藏状态的全局池化版本。在原始模型中，可以使用固定7x7窗口和步幅1的平均池化层代替全局池化。对于大于推荐图像尺寸的输入，这将产生一个大于1x1的池化输出。Hugging
    Face的实现不支持这一点。'
- en: The original TensorFlow checkpoints include quantized models. We do not support
    these models as they include additional “FakeQuantization” operations to unquantize
    the weights.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的TensorFlow检查点包括量化模型。我们不支持这些模型，因为它们包括额外的“FakeQuantization”操作来取消量化权重。
- en: It’s common to extract the output from the expansion layers at indices 10 and
    13, as well as the output from the final 1x1 convolution layer, for downstream
    purposes. Using `output_hidden_states=True` returns the output from all intermediate
    layers. There is currently no way to limit this to specific layers.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常会提取扩展层的输出，索引为10和13，以及最终1x1卷积层的输出，用于下游目的。使用`output_hidden_states=True`返回所有中间层的输出。目前无法将其限制为特定层。
- en: The DeepLabV3+ segmentation head does not use the final convolution layer from
    the backbone, but this layer gets computed anyway. There is currently no way to
    tell [MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model)
    up to which layer it should run.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLabV3+分割头部不使用骨干网络的最终卷积层，但是这一层仍然会被计算。目前无法告诉[MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model)应该运行到哪一层。
- en: Resources
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with MobileNetV2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用MobileNetV2。
- en: Image Classification
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[MobileNetV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MobileNetV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: '**Semantic segmentation**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义分割**'
- en: '[Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义分割任务指南](../tasks/semantic_segmentation)'
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。
- en: MobileNetV2Config
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNetV2Config
- en: '### `class transformers.MobileNetV2Config`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileNetV2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/configuration_mobilenet_v2.py#L38)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/configuration_mobilenet_v2.py#L38)'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。'
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。'
- en: '`depth_multiplier` (`float`, *optional*, defaults to 1.0) — Shrinks or expands
    the number of channels in each layer. Default is 1.0, which starts the network
    with 32 channels. This is sometimes also called “alpha” or “width multiplier”.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth_multiplier` (`float`, *optional*, defaults to 1.0) — 收缩或扩展每一层的通道数。默认为1.0，网络从32个通道开始。有时也称为“alpha”或“宽度乘数”。'
- en: '`depth_divisible_by` (`int`, *optional*, defaults to 8) — The number of channels
    in each layer will always be a multiple of this number.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth_divisible_by` (`int`, *optional*, defaults to 8) — 每一层的通道数将始终是这个数字的倍数。'
- en: '`min_depth` (`int`, *optional*, defaults to 8) — All layers will have at least
    this many channels.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_depth` (`int`, *optional*, defaults to 8) — 所有层至少具有这么多通道。'
- en: '`expand_ratio` (`float`, *optional*, defaults to 6.0) — The number of output
    channels of the first layer in each block is input channels times expansion ratio.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expand_ratio` (`float`, *optional*, defaults to 6.0) — 每个块中第一层的输出通道数是输入通道数乘以扩展比率。'
- en: '`output_stride` (`int`, *optional*, defaults to 32) — The ratio between the
    spatial resolution of the input and output feature maps. By default the model
    reduces the input dimensions by a factor of 32\. If `output_stride` is 8 or 16,
    the model uses dilated convolutions on the depthwise layers instead of regular
    convolutions, so that the feature maps never become more than 8x or 16x smaller
    than the input image.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_stride` (`int`, *optional*, defaults to 32) — 输入和输出特征图的空间分辨率之间的比率。默认情况下，模型将输入维度减小32倍。如果`output_stride`为8或16，则模型在深度层上使用扩张卷积而不是常规卷积，以确保特征图永远不会比输入图像小8倍或16倍。'
- en: '`first_layer_is_expansion` (`bool`, *optional*, defaults to `True`) — True
    if the very first convolution layer is also the expansion layer for the first
    expansion block.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first_layer_is_expansion` (`bool`, *optional*, defaults to `True`) — 如果第一个卷积层也是第一个扩展块的扩展层，则为True。'
- en: '`finegrained_output` (`bool`, *optional*, defaults to `True`) — If true, the
    number of output channels in the final convolution layer will stay large (1280)
    even if `depth_multiplier` is less than 1.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`finegrained_output` (`bool`, *optional*, defaults to `True`) — 如果为真，则最终卷积层中的输出通道数将保持较大（1280），即使`depth_multiplier`小于1。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu6"`) — The
    non-linear activation function (function or string) in the Transformer encoder
    and convolution layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` 或 `function`，*可选*，默认为 `"relu6"`) — Transformer 编码器和卷积层中的非线性激活函数（函数或字符串）。'
- en: '`tf_padding` (`bool`, *optional*, defaults to `True`) — Whether to use TensorFlow
    padding rules on the convolution layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf_padding` (`bool`，*可选*，默认为 `True`) — 是否在卷积层上使用 TensorFlow 填充规则。'
- en: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.8) — The dropout
    ratio for attached classifiers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout_prob` (`float`，*可选*，默认为 0.8) — 附加分类器的丢失比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`，*可选*，默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 0.001) — The epsilon used
    by the layer normalization layers.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`，*可选*，默认为 0.001) — 层归一化层使用的 epsilon。'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) — The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`，*可选*，默认为 255) — 语义分割模型的损失函数中被忽略的索引。'
- en: This is the configuration class to store the configuration of a [MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model).
    It is used to instantiate a MobileNetV2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MobileNetV2 [google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224)
    architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储 [MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model)
    的配置。根据指定的参数实例化一个 MobileNetV2 模型，定义模型架构。使用默认值实例化配置将产生类似于 MobileNetV2 [google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: MobileNetV2FeatureExtractor
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNetV2FeatureExtractor
- en: '### `class transformers.MobileNetV2FeatureExtractor`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileNetV2FeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/feature_extraction_mobilenet_v2.py#L26)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/feature_extraction_mobilenet_v2.py#L26)'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `preprocess`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L170)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L170)'
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个图像或批处理图像，像素值范围为 0 到 255。如果传入像素值在 0 到
    1 之间的图像，请设置 `do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`，*可选*，默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing. Shortest edge of the image is resized to size[“shortest_edge”],
    with the longest edge resized to keep the input aspect ratio.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`，*可选*，默认为 `self.size`) — 调整大小后的图像尺寸。图像的最短边被调整为 size[“shortest_edge”]，最长边被调整以保持输入的纵横比。'
- en: '`resample` (`PILImageResampling` filter, *optional*, defaults to `self.resample`)
    — `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling` 过滤器，*可选*，默认为 `self.resample`) — 如果调整图像大小，则使用的
    `PILImageResampling` 过滤器，例如 `PILImageResampling.BILINEAR`。仅在 `do_resize` 设置为 `True`
    时有效。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`，*可选*，默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`，*可选*，默认为 `self.crop_size`) — 中心裁剪的尺寸。仅在 `do_center_crop`
    设置为 `True` 时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`，*可选*，默认为 `self.do_rescale`) — 是否重新缩放图像值在 [0 - 1] 之间。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`，*可选*，默认为 `self.rescale_factor`) — 如果 `do_rescale`
    设置为 `True`，则用于重新缩放图像的重新缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`，*可选*，默认为 `self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to use if `do_normalize` is set to `True`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`，*可选*，默认为 `self.image_mean`) — 如果 `do_normalize`
    设置为 `True`，则使用的图像均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to use if `do_normalize` is set to `True`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`，*可选*，默认为 `self.image_std`) — 如果 `do_normalize`
    设置为 `True`，则使用的图像标准差。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`，*可选*) — 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个 `np.ndarray` 列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`：返回类型为 `tf.Tensor` 的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`: 返回类型为 `torch.Tensor` 的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`: 返回类型为 `np.ndarray` 的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`: 返回类型为 `jax.numpy.ndarray` 的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`，*可选*, 默认为 `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：使用输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` 或 `str`，*可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`: 图像格式为 (高度, 宽度)。'
- en: Preprocess an image or batch of images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批处理。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L313)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L313)'
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation))
    — Raw outputs of the model.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`，长度为 `batch_size`，*可选*) — 每个预测的请求最终尺寸 (高度, 宽度)
    对应的元组列表。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: semantic_segmentation
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`，长度为 `batch_size`，其中每个项目是形状为 (高度, 宽度) 的语义分割地图，对应于 `target_sizes`
    条目（如果指定了 `target_sizes`）。每个 `torch.Tensor` 的每个条目对应一个语义类别 id。'
- en: Converts the output of [MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation)
    的输出转换为语义分割地图。仅支持 PyTorch。
- en: MobileNetV2ImageProcessor
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNetV2ImageProcessor
- en: '### `class transformers.MobileNetV2ImageProcessor`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileNetV2ImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L49)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L49)'
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by `do_resize` in the `preprocess` method.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为 `True`) — 是否将图像的 (高度, 宽度) 尺寸调整为指定的 `size`。可以被
    `preprocess` 方法中的 `do_resize` 覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 256}`):
    Size of the image after resizing. The shortest edge of the image is resized to
    size[“shortest_edge”], with the longest edge resized to keep the input aspect
    ratio. Can be overridden by `size` in the `preprocess` method.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *可选*, 默认为 `{"shortest_edge" -- 256}`): 调整大小后的图像尺寸。图像的最短边被调整为
    size[“shortest_edge”]，最长边被调整以保持输入的长宽比。可以被 `preprocess` 方法中的 `size` 覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`，*可选*, 默认为 `PILImageResampling.BILINEAR`) —
    调整图像大小时要使用的重采样滤波器。可以被 `preprocess` 方法中的 `resample` 参数覆盖。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image. If the input size is smaller than `crop_size` along any edge,
    the image is padded with 0’s and then center cropped. Can be overridden by the
    `do_center_crop` parameter in the `preprocess` method.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行中心裁剪。如果输入尺寸小于任何边上的 `crop_size`，则图像将填充为
    0，然后进行中心裁剪。可以被 `preprocess` 方法中的 `do_center_crop` 参数覆盖。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 224, "width":
    224}`): Desired output size when applying center-cropping. Only has an effect
    if `do_center_crop` is set to `True`. Can be overridden by the `crop_size` parameter
    in the `preprocess` method.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`，*可选*, 默认为 `{"height" -- 224, "width": 224}`):
    应用中心裁剪时的期望输出尺寸。仅在 `do_center_crop` 设置为 `True` 时有效。可以被 `preprocess` 方法中的 `crop_size`
    参数覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *可选*, 默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以被
    `preprocess` 方法中的 `do_rescale` 参数覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method. do_normalize — Whether to normalize the
    image. Can be overridden by the `do_normalize` parameter in the `preprocess` method.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新缩放图像，则使用的缩放因子。可以被
    `preprocess` 方法中的 `rescale_factor` 参数覆盖。是否归一化图像。可以被 `preprocess` 方法中的 `do_normalize`
    参数覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_MEAN`)
    — 如果归一化图像，则使用的均值。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean`
    参数覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_STD`) —
    如果归一化图像，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。'
- en: Constructs a MobileNetV2 image processor.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 MobileNetV2 图像处理器。
- en: '#### `preprocess`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L170)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L170)'
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置
    `do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing. Shortest edge of the image is resized to size[“shortest_edge”],
    with the longest edge resized to keep the input aspect ratio.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 调整大小后的图像大小。图像的最短边被调整为 size[“shortest_edge”]，最长边被调整以保持输入的长宽比。'
- en: '`resample` (`PILImageResampling` filter, *optional*, defaults to `self.resample`)
    — `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling` 过滤器, *可选*, 默认为 `self.resample`) — 如果调整图像大小，要使用的
    `PILImageResampling` 过滤器，例如 `PILImageResampling.BILINEAR`。仅在 `do_resize` 设置为 `True`
    时有效。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *可选*, 默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *可选*, 默认为 `self.crop_size`) — 中心裁剪的大小。仅在 `do_center_crop`
    设置为 `True` 时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放到 [0 - 1] 之间。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果 `do_rescale`
    设置为 `True`，则按照此因子重新缩放图像。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to use if `do_normalize` is set to `True`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 如果 `do_normalize`
    设置为 `True`，则使用的图像均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to use if `do_normalize` is set to `True`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_std`) — 如果 `do_normalize`
    设置为 `True`，则使用的图像标准差。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一:'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '未设置: 返回一个 `np.ndarray` 列表。'
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`: 返回类型为 `tf.Tensor` 的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`: 返回类型为 `torch.Tensor` 的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`: 返回类型为 `np.ndarray` 的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`: 返回类型为 `jax.numpy.ndarray` 的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一:'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (num_channels, height,
    width)。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (height, width, num_channels)。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '未设置: 使用输入图像的通道维度格式。'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一:'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (num_channels, height,
    width)。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。'
- en: Preprocess an image or batch of images.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理一张图片或一批图片。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L313)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L313)'
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation))
    — Raw outputs of the model.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — 与每个预测的请求最终大小（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: semantic_segmentation
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`，长度为`batch_size`，每个项目是一个形状为（高度，宽度）的语义分割地图，对应于目标大小条目（如果指定了`target_sizes`）。每个`torch.Tensor`的每个条目对应于一个语义类别ID。'
- en: Converts the output of [MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将[MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation)的输出转换为语义分割地图。仅支持PyTorch。
- en: MobileNetV2Model
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNetV2Model
- en: '### `class transformers.MobileNetV2Model`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileNetV2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L498)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L498)'
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare MobileNetV2 model outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的MobileNetV2模型输出原始的隐藏状态，没有特定的头部。这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L566)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L566)'
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileNetV2ImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[MobileNetV2ImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    `tuple(torch.FloatTensor)`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    and inputs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — 空间维度上进行池化操作后的最后一层隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。
- en: The [MobileNetV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV2Model的forward方法覆盖了`__call__`特殊方法。
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: MobileNetV2ForImageClassification
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNetV2ForImageClassification
- en: '### `class transformers.MobileNetV2ForImageClassification`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileNetV2ForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L615)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L615)'
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MobileNetV2 model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV2模型，顶部带有图像分类头（池化特征之上的线性层），例如用于ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch的一个[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L638)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L638)'
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileNetV2ImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileNetV2ImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss). If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失）。如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    and inputs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，具体取决于配置（[MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: The [MobileNetV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileNetV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: MobileNetV2ForSemanticSegmentation
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNetV2ForSemanticSegmentation
- en: '### `class transformers.MobileNetV2ForSemanticSegmentation`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MobileNetV2ForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L775)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L775)'
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MobileNetV2 model with a semantic segmentation head on top, e.g. for Pascal
    VOC.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV2模型，顶部带有语义分割头，例如用于Pascal VOC。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L792)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L792)'
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MobileNetV2ImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[MobileNetV2ImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, height, width)`，*可选*) — 用于计算损失的地面真实语义分割地图。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config))
    and inputs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[MobileNetV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) — Classification scores for each pixel.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels, logits_height, logits_width)`的`torch.FloatTensor`）
    — 每个像素的分类分数。'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">返回的logits不一定与作为输入传递的`pixel_values`具有相同的大小。这是为了避免进行两次插值并在用户需要将logits调整为原始图像大小时丢失一些质量。您应始终检查您的logits形状并根据需要调整大小。</tip>
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, patch_size, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileNetV2ForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
