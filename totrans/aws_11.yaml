- en: Fine-tune and Test Llama 2 7B on AWS Trainium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_llama_7b](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_llama_7b)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a notebook version of that tutorial [here](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-generation/llama2-7b-fine-tuning.ipynb)*.'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will teach you how to fine-tune open LLMs like [Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    on AWS Trainium. In our example, we are going to leverage Hugging Face [https://huggingface.co/docs/optimum-neuron/index](https://huggingface.co/docs/optimum-neuron/index),
    [Transformers](https://huggingface.co/docs/transformers/index) and [https://huggingface.co/docs/datasets/index](https://huggingface.co/docs/datasets/index).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Setup AWS environment](#1-setup-aws-environment)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Load and process the dataset](#2-load-and-prepare-the-dataset)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Fine-tune Llama on AWS Trainium using the `NeuronTrainer`](#3-fine-tune-llama-on-aws-trainium-using-the-neurontrainer)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Evaluate and test fine-tuned Llama model](#4-evaluate-and-test-fine-tuned-llama-model)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quick intro: AWS Trainium'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is
    a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the
    successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls)
    focused on high-performance training workloads. Trainium has been optimized for
    training natural language processing, computer vision, and recommender models.
    The accelerator supports a wide range of data types, including FP32, TF32, BF16,
    FP16, UINT8, and configurable FP8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of
    memory, making it easy to fine-tune ~10B parameter models on a single instance.
    Below you will find an overview of the available instance types. More details
    [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):'
  prefs: []
  type: TYPE_NORMAL
- en: '| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price
    per hour |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| trn1.2xlarge | 1 | 32 | 8 | 32 | \$1.34 |'
  prefs: []
  type: TYPE_TB
- en: '| trn1.32xlarge | 16 | 512 | 128 | 512 | \$21.50 |'
  prefs: []
  type: TYPE_TB
- en: '| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | \$24.78 |'
  prefs: []
  type: TYPE_TB
- en: '*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.*'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Setup AWS environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator,
    including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2).
    The Hugging Face AMI comes with all important libraries, like Transformers, Datasets,
    Optimum and Neuron packages pre-installed. This makes it super easy to get started,
    since there is no need for environment management.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial doesn‚Äôt cover how to create the instance in detail. You can check
    out the dedicated tutorial about [‚ÄúSetting up AWS Trainium for Hugging Face Transformers‚Äù](https://huggingface.co/docs/optimum-neuron/guides/setup_aws_instance),
    which includes a step-by-step guide on setting up the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Once the instance is up and running, we can ssh into it. But instead of developing
    inside a terminal we want to use a `Jupyter` environment, which we can use for
    preparing our dataset and launching the training. For this, we need to add a port
    for forwarding in the `ssh` command, which will tunnel our localhost traffic to
    the Trainium instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs now pull the optimum repository with the [example notebook and scripts](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next we can change our directory to `notbooks/text-generation` and launch the
    `jupyter` environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You should see a familiar **`jupyter`** output with a URL to the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**'
  prefs: []
  type: TYPE_NORMAL
- en: We can click on it, and a **`jupyter`** environment opens in our local browser.
    Open the notebook **`llama2-7b-fine-tuning.ipynb`** and lets get started.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: We are going to use the Jupyter environment only for preparing the dataset
    and then `torchrun` for launching our training script for distributed training.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are going to use official Llama 2 checkpoint you need to login into
    our hugging face account, which has access to the model, to use your token for
    accessing the gated repository. We can do this by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: We also provide an ungated checkpoint.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Load and prepare the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    an open source dataset of instruction-following records on categories outlined
    in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming,
    classification, closed QA, generation, information extraction, open QA, and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To load the `dolly` dataset, we use the `load_dataset()` method from the ü§ó Datasets
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To instruct tune our model we need to convert our structured examples into a
    collection of tasks described via instructions. We define a `formatting_function`
    that takes a sample and returns a string with our format instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: let‚Äôs test our formatting function on a random example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In addition, to formatting our samples we also want to pack multiple samples
    to one sequence to have a more efficient training. This means that we are stacking
    multiple samples to one sequence and split them with an EOS Token. This makes
    the training more efficient. Packing/stacking samples can be done during training
    or before. We will do it before training to save time. We created a utility method
    [pack_dataset](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation/scripts/utils/pack_dataset.py)
    that takes a dataset and a packing function and returns a packed dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To pack/stack our dataset we need to first tokenize it and then we can pack
    it with the `pack_dataset` method. To prepare our dataset we will now:'
  prefs: []
  type: TYPE_NORMAL
- en: Format our samples using the template method and add an EOS token at the end
    of each sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize our dataset to convert it from text to tokens
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pack our dataset to 2048 tokens
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: After we processed the datasets we are going save it to disk. You could also
    save it to S3 or the Hugging Face Hub for later use.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Packing and preprocessing your dataset can be run outside of the Trainium
    instance.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Fine-tune Llama on AWS Trainium using the NeuronTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally you would use the **[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)**
    and **[TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)**
    to fine-tune PyTorch-based transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: But together with AWS, we have developed a `NeuronTrainer` to improve performance,
    robustness, and safety when training on Trainium instances. The `NeuronTrainer`
    is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement
    for the `Trainer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to distributed training on AWS Trainium there are a few things
    we need to take care of. Since Llama is a big model it might not fit on a single
    accelerator, thats why we added support for different distributed training strategies
    to the `NeuronTrainer` including:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html):
    shards the optimizer state over multiple devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html):
    shards the model parameters along a given dimension on multiple devices, defined
    with `tensor_parallel_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf) shards the activations
    on the sequence axis outside of the tensor parallel regions. It is useful because
    it saves memory by sharding the activations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html):
    *coming soon*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We prepared a [run_clm.py](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-generation/scripts/run_clm.py),
    which implements those distributed training strategies for you already. If you
    want to know more about the details you can take a look at the [documentation](https://huggingface.co/docs/optimum-neuron/guides/distributed_training).
    When training models on AWS Accelerators we first need to compile our model with
    our training arguments.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this we added a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system),
    which allows us to use precompiled models and configuration from Hugging Face
    Hub to skip the compilation step. But every change in the config, will lead to
    a new compilation, which could result in some cache misses.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: If your configuration is not cached please open an issue on [Github](https://github.com/huggingface/optimum-neuron/issues),
    we are happy to include it.*'
  prefs: []
  type: TYPE_NORMAL
- en: We pre-compiled the config for our training already meaning you can either skip
    the cell below or rerun it will only take a few minutes since it reuses the cached
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: Compiling without a cache can take ~40 minutes. It will also create
    dummy files in the `dolly_llama_sharded` during compilation you we have to remove
    them afterwards. We also need to add `MALLOC_ARENA_MAX=64` to limit the CPU allocation
    to avoid potential crashes, don‚Äôt remove it for now.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After the compilation is done we can start our training with a similar command,
    we just need to remove the `neuron_parallel_compile`. We will use `torchrun` to
    launch our training script. `torchrun` is a tool that automatically distributes
    a PyTorch model across multiple accelerators. We can pass the number of accelerators
    as `nproc_per_node` arguments alongside our hyperparameters. The difference to
    the compilation command is that we changed from `max_steps=10` to `num_train_epochs=3`.
  prefs: []
  type: TYPE_NORMAL
- en: Launch the training, with the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Thats it, we successfully trained Llama 7B on AWS Trainium. The training took
    for 3 epochs on dolly (15k samples) took 43:24 minutes where the raw training
    time was only 31:46 minutes. This leads to a cost of ~$15.5 for the e2e training
    on the trn1.32xlarge instance. Not Bad!
  prefs: []
  type: TYPE_NORMAL
- en: But before we can share and test our model we need to consolidate our model.
    Since we used Tensor Parallelism during training, we need to consolidate the model
    weights before we can use it. Tensor Parallelism shards the model weights accross
    different workers, only sharded checkpoints will be saved during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Optimum CLI provides a way of doing that very easily via the `optimum neuron
    consolidate‚Äú command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Lets remove our ‚Äúsharded‚Äù checkpoints as we have consolidated them already to
    safetensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Evaluate and test fine-tuned Llama model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to training to be able to run inferece on AWS Trainium or AWS Inferentia2
    we need to compile our model for the correct use. We will use our Trainium instance
    for the inference test, but we recommend customer to switch to Inferentia2 for
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Optimum Neuron implements similar to Transformers AutoModel classes for easy
    inference use. We will use the `NeuronModelForCausalLM` class to load our vanilla
    transformers checkpoint and convert it to neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: Inference compilation can take ~25minutes. Luckily, you need to only
    run this onces. Since you can save the model afterwards. If you are going to run
    on Inferentia2 you need to recompile again. The compilation is parameter and hardware
    specific.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can now test inference, but have to make sure we format our input to our
    prompt format we used for fine-tuning. Therefore we created a helper method, which
    accepts a `dict` with our `instruction` and optionally a `context`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Lets test inference. First we test without a context.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Inference is not expected to be super fast on AWS Trainium using 2 cores.
    For Inference we recommend using Inferentia2.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: AWS stands for Amazon Web Services. AWS is a suite of remote computing services
    offered by Amazon. The most widely used of these include Amazon Elastic Compute
    Cloud (Amazon EC2), which provides resizable compute capacity in the cloud; Amazon
    Simple Storage Service (Amazon S3), which is an object storage service; and Amazon
    Elastic Block Store (Amazon EBS), which is designed to provide high performance,
    durable block storage volumes for use with AWS instances. AWS also provides other
    services, such as AWS Identity and Access Management (IAM), a service that enables
    organizations to control access to their AWS resources, and AWS Key Management
    Service (AWS KMS), which helps customers create and control the use of encryption
    keys.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That looks correct. Now, lets add some context, e.g. as you would do for RAG
    applications
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You can use the Optimum Neuron interface to train models on AWS Trainium.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Awesome, our model also correctly uses the provided context. We are done. Congrats
    on fine-tuning Llama on AWS Trainium.
  prefs: []
  type: TYPE_NORMAL
