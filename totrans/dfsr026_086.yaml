- en: OpenVINO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/optimization/open_vino](https://huggingface.co/docs/diffusers/optimization/open_vino)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/127.d4cf7dbc.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion
    pipelines compatible with OpenVINO to perform inference on a variety of Intel
    processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html)
    of supported devices).
  prefs: []
  type: TYPE_NORMAL
- en: 'Youâ€™ll need to install ðŸ¤— Optimum Intel with the `--upgrade-strategy eager`
    option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel)
    is using the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This guide will show you how to use the Stable Diffusion and Stable Diffusion
    XL (SDXL) pipelines with OpenVINO.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load and run inference, use the `OVStableDiffusionPipeline`. If you want
    to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set
    `export=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To further speed-up inference, statically reshape the model. If you change any
    parameters such as the outputs height or width, youâ€™ll need to statically reshape
    your model again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c891b691562157d71fc3b1359bbb0a05.png)'
  prefs: []
  type: TYPE_IMG
- en: You can find more examples in the ðŸ¤— Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion),
    and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion XL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load and run inference with SDXL, use the `OVStableDiffusionXLPipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To further speed-up inference, [statically reshape](#stable-diffusion) the model
    as shown in the Stable Diffusion section.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more examples in the ðŸ¤— Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl),
    and running SDXL in OpenVINO is supported for text-to-image and image-to-image.
  prefs: []
  type: TYPE_NORMAL
