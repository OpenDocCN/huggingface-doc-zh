# 量化

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)

量化技术专注于用更少的信息表示数据，同时也试图不丢失太多准确性。这通常意味着将数据类型转换为用更少的位表示相同信息。例如，如果您的模型权重存储为32位浮点数，并且它们被量化为16位浮点数，这将使模型大小减半，使其更容易存储并减少内存使用。较低的精度也可以加快推理速度，因为使用更少的位进行计算需要更少的时间。

Transformers支持几种量化方案，帮助您在大型语言模型（LLMs）上运行推理和在量化模型上微调适配器。本指南将向您展示如何使用激活感知权重量化（AWQ）、AutoGPTQ和bitsandbytes。

## AWQ

尝试使用这个[notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)进行AWQ量化！

[激活感知权重量化（AWQ）](https://hf.co/papers/2306.00978)不会量化模型中的所有权重，而是保留对LLM性能重要的一小部分权重。这显著减少了量化损失，使您可以在不经历任何性能降级的情况下以4位精度运行模型。

有几个库可用于使用AWQ算法量化模型，例如[llm-awq](https://github.com/mit-han-lab/llm-awq)、[autoawq](https://github.com/casper-hansen/AutoAWQ)或[optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)。 Transformers支持加载使用llm-awq和autoawq库量化的模型。本指南将向您展示如何加载使用autoawq量化的模型，但对于使用llm-awq量化的模型，过程类似。

确保您已安装autoawq：

```py
pip install autoawq
```

可以通过检查模型的[config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)文件中的`quantization_config`属性来识别AWQ量化模型：

```py
{
  "_name_or_path": "/workspace/process/huggingfaceh4_zephyr-7b-alpha/source",
  "architectures": [
    "MistralForCausalLM"
  ],
  ...
  ...
  ...
  "quantization_config": {
    "quant_method": "awq",
    "zero_point": true,
    "group_size": 128,
    "bits": 4,
    "version": "gemm"
  }
}
```

使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法加载量化模型。如果您在CPU上加载了模型，请确保首先将其移动到GPU设备上。使用`device_map`参数指定模型放置的位置：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

加载AWQ量化模型会自动将其他权重默认设置为fp16以提高性能。如果您想以不同格式加载这些其他权重，请使用`torch_dtype`参数：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)
```

AWQ量化也可以与[FlashAttention-2](perf_infer_gpu_one#flashattention-2)结合，以进一步加速推理：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

### 融合模块

融合模块提供了改进的准确性和性能，并且对于[Llama](https://huggingface.co/meta-llama)和[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)架构的AWQ模块支持开箱即用，但您也可以为不支持的架构融合AWQ模块。

融合模块不能与FlashAttention-2等其他优化技术结合使用。

支持的架构不支持的架构

要为支持的架构启用融合模块，请创建一个[AwqConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.AwqConfig)并设置参数`fuse_max_seq_len`和`do_fuse=True`。`fuse_max_seq_len`参数是总序列长度，应包括上下文长度和预期生成长度。您可以将其设置为较大的值以确保安全。

例如，要融合[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)模型的AWQ模块。

```py
import torch
from transformers import AwqConfig, AutoModelForCausalLM

model_id = "TheBloke/Mistral-7B-OpenOrca-AWQ"

quantization_config = AwqConfig(
    bits=4,
    fuse_max_seq_len=512,
    do_fuse=True,
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)
```

## AutoGPTQ

在这个[notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)中尝试使用PEFT进行GPTQ量化，并在这篇[博客文章](https://huggingface.co/blog/gptq-integration)中了解更多细节！

[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)库实现了GPTQ算法，这是一种后训练量化技术，其中权重矩阵的每一行都独立量化，以找到最小化误差的权重版本。这些权重被量化为int4，但在推理过程中会动态恢复为fp16。这可以通过4倍减少内存使用，因为int4权重在融合内核中而不是GPU的全局内存中被解量化，您还可以期望推理速度提升，因为使用较低的位宽需要更少的通信时间。

开始之前，请确保安装了以下库：

```py
pip install auto-gptq
pip install git+https://github.com/huggingface/optimum.git
pip install git+https://github.com/huggingface/transformers.git
pip install --upgrade accelerate
```

要量化一个模型（目前仅支持文本模型），您需要创建一个[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)类，并设置要量化的位数、用于校准权重的数据集以及准备数据集的分词器。

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset="c4", tokenizer=tokenizer)
```

您还可以将自己的数据集作为字符串列表传递，但强烈建议使用GPTQ论文中相同的数据集。

```py
dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]
gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)
```

加载一个要量化的模型，并将`gptq_config`传递给[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法。设置`device_map="auto"`以自动将模型卸载到CPU，以帮助将模型适配到内存中，并允许模型模块在CPU和GPU之间移动以进行量化。

```py
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```

如果由于数据集过大而导致内存不足，不支持磁盘卸载。如果是这种情况，请尝试传递`max_memory`参数来分配设备（GPU和CPU）上要使用的内存量：

```py
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", max_memory={0: "30GiB", 1: "46GiB", "cpu": "30GiB"}, quantization_config=gptq_config)
```

根据您的硬件，从头开始量化一个模型可能需要一些时间。在免费的Google Colab GPU上，量化faceboook/opt-350m模型可能需要约5分钟，但在NVIDIA A100上，量化一个175B参数模型可能需要约4小时。在量化模型之前，最好先检查Hub，看看模型的GPTQ量化版本是否已经存在。

一旦您的模型被量化，您可以将模型和分词器推送到Hub，这样可以轻松共享和访问。使用[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法保存[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)：

```py
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

您还可以使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)方法将您的量化模型保存在本地。如果模型是使用`device_map`参数量化的，请确保在保存之前将整个模型移动到GPU或CPU。例如，要在CPU上保存模型：

```py
quantized_model.save_pretrained("opt-125m-gptq")
tokenizer.save_pretrained("opt-125m-gptq")

# if quantized with device_map set
quantized_model.to("cpu")
quantized_model.save_pretrained("opt-125m-gptq")
```

使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法重新加载一个量化模型，并设置`device_map="auto"`以自动将模型分布在所有可用的GPU上，以便更快地加载模型而不使用比所需更多的内存。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

### ExLlama

[ExLlama](https://github.com/turboderp/exllama)是[Llama](model_doc/llama)模型的Python/C++/CUDA实现，旨在通过4位GPTQ权重实现更快的推理（查看这些[基准测试](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)）。当您创建一个[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)对象时，默认情况下会激活ExLlama内核。为了进一步提高推理速度，请配置`exllama_config`参数使用[ExLlamaV2](https://github.com/turboderp/exllamav2)内核：

```py
import torch
from transformers import AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)
```

仅支持4位模型，并且我们建议在对量化模型进行微调时停用ExLlama内核。

只有当整个模型在GPU上时才支持ExLlama内核。如果您在CPU上使用AutoGPTQ（版本>0.4.2）进行推理，则需要禁用ExLlama内核。这将覆盖config.json文件中与ExLlama内核相关的属性。

```py
import torch
from transformers import AutoModelForCausalLM, GPTQConfig
gptq_config = GPTQConfig(bits=4, use_exllama=False)
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="cpu", quantization_config=gptq_config)
```

## bitsandbytes

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)是将模型量化为8位和4位的最简单选择。8位量化将fp16中的异常值与int8中的非异常值相乘，将非异常值转换回fp16，然后将它们相加以返回fp16中的权重。这减少了异常值对模型性能的降级影响。4位量化进一步压缩模型，通常与[QLoRA](https://hf.co/papers/2305.14314)一起用于微调量化的LLM。

要使用bitsandbytes，请确保已安装以下库：

8位4位

```py
pip install transformers accelerate bitsandbytes>0.37.0
```

现在您可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法中的`load_in_8bit`或`load_in_4bit`参数来量化模型。只要模型支持使用Accelerate加载并包含`torch.nn.Linear`层，这对任何模态的模型都适用。

8位4位

将模型量化为8位可以减半内存使用量，对于大型模型，设置`device_map="auto"`以有效地使用可用的GPU：

```py
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b7", device_map="auto", load_in_8bit=True)
```

默认情况下，所有其他模块（如`torch.nn.LayerNorm`）都会转换为`torch.float16`。如果需要，您可以使用`torch_dtype`参数更改这些模块的数据类型：

```py
import torch
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True, torch_dtype=torch.float32)
model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

一旦模型被量化为8位，除非您使用最新版本的Transformers和bitsandbytes，否则无法将量化的权重推送到Hub。如果您有最新版本，则可以使用[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法将8位模型推送到Hub。首先推送量化的config.json文件，然后是量化的模型权重。

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

仅支持使用8位和4位权重进行训练*额外*参数。

您可以使用`get_memory_footprint`方法检查内存占用情况：

```py
print(model.get_memory_footprint())
```

可以从[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法中加载量化模型，无需指定`load_in_8bit`或`load_in_4bit`参数：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

### 8位

在这篇[博客文章](https://huggingface.co/blog/hf-bitsandbytes-integration)中了解更多关于8位量化的细节！

本节探讨了8位模型的一些特定功能，如转移、异常值阈值、跳过模块转换和微调。

#### 转移

8位模型可以在CPU和GPU之间转移权重，以支持将非常大的模型适配到内存中。发送到CPU的权重实际上是以**float32**存储的，并且不会转换为8位。例如，要为[bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)模型启用转移，请首先创建一个[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)：

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

设计一个自定义设备映射，将所有内容都适配到GPU上，除了`lm_head`，这部分将发送到CPU：

```py
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

现在使用自定义的`device_map`和`quantization_config`加载您的模型：

```py
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

#### 异常值阈值

“异常值”是大于某个阈值的隐藏状态值，这些值是在fp16中计算的。虽然这些值通常是正态分布的（[-3.5, 3.5]），但对于大型模型（[-60, 6]或[6, 60]），这种分布可能会有很大不同。8位量化适用于值约为5，但超过这个值，会有显著的性能损失。一个很好的默认阈值是6，但对于更不稳定的模型（小模型或微调），可能需要更低的阈值。

为了找到您的模型的最佳阈值，我们建议尝试在[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中使用`llm_int8_threshold`参数进行实验：

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
```

#### 跳过模块转换

对于一些模型，如[Jukebox](model_doc/jukebox)，您不需要将每个模块量化为8位，这实际上可能会导致不稳定性。对于Jukebox，有几个`lm_head`模块应该使用[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中的`llm_int8_skip_modules`参数跳过：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=quantization_config,
)
```

#### 微调

使用[PEFT](https://github.com/huggingface/peft)库，您可以使用8位量化微调大型模型，如[flan-t5-large](https://huggingface.co/google/flan-t5-large)和[facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)。在训练时不需要传递`device_map`参数，因为它会自动将您的模型加载到GPU上。但是，如果您想要，仍然可以使用`device_map`参数自定义设备映射（`device_map="auto"`仅应用于推断）。

### 4位

在这个[notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)中尝试4位量化，并在这篇[博客文章](https://huggingface.co/blog/4bit-transformers-bitsandbytes)中了解更多细节。

本节探讨了4位模型的一些特定功能，如更改计算数据类型、使用Normal Float 4 (NF4)数据类型和使用嵌套量化。

#### 计算数据类型

为了加速计算，您可以将数据类型从float32（默认值）更改为bf16，使用[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中的`bnb_4bit_compute_dtype`参数：

```py
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

#### Normal Float 4 (NF4)

NF4是来自[QLoRA](https://hf.co/papers/2305.14314)论文的4位数据类型，适用于从正态分布初始化的权重。您应该使用NF4来训练4位基础模型。这可以通过[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中的`bnb_4bit_quant_type`参数进行配置：

```py
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

对于推断，`bnb_4bit_quant_type`对性能没有太大影响。但是，为了保持与模型权重一致，您应该使用`bnb_4bit_compute_dtype`和`torch_dtype`值。

#### 嵌套量化

嵌套量化是一种技术，可以在不增加性能成本的情况下节省额外的内存。此功能对已经量化的权重执行第二次量化，以节省额外的0.4位/参数。例如，使用嵌套量化，您可以在16GB的NVIDIA T4 GPU上微调[Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)模型，序列长度为1024，批量大小为1，并启用梯度累积4步。

```py
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b", quantization_config=double_quant_config)
```

## Optimum

[Optimum](https://huggingface.co/docs/optimum/index)库支持Intel、Furiosa、ONNX Runtime、GPTQ和较低级别的PyTorch量化功能。如果您正在使用像Intel CPU、Furiosa NPU或像ONNX Runtime这样的模型加速器这样的特定和优化的硬件，请考虑使用Optimum进行量化。

## 基准测试

要比较每种量化方案的速度、吞吐量和延迟，请查看从[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)库获得的以下基准测试。该基准测试在NVIDIA A1000上运行，用于[TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)和[TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)模型。这些还与bitsandbytes量化方法以及本机fp16模型进行了测试。

![每批前向峰值内存](../Images/7f7a45cc2b5704faa291fb7c3ca255d6.png)

前向峰值内存/批处理大小

![每批前向峰值内存](../Images/c53e9d1c99d4b8ce87793021dc7f7393.png)

每批生成峰值内存/批处理大小

![每批生成吞吐量](../Images/40d57cb1a4c4282ec140a27de593156a.png)

每批生成吞吐量/批处理大小

![每批前向延迟](../Images/67c3f5ce2fa384e77579473e9efb77fa.png)

前向延迟/批处理大小

基准测试表明，AWQ量化在推理、文本生成方面是最快的，并且在文本生成方面具有最低的峰值内存。然而，AWQ在每个批处理大小上具有最大的前向延迟。要了解每种量化方法的优缺点的更详细讨论，请阅读[🤗 Transformers中本地支持的量化方案概述](https://huggingface.co/blog/overview-quantization-transformers)博客文章。

### 融合AWQ模块

[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)模型在`batch_size=1`下进行了基准测试，有无融合模块。

未融合模块

| 批处理大小 | 预填充长度 | 解码长度 | 预填充标记/秒 | 解码标记/秒 | 内存（VRAM） |
| --: | --: | --: | --: | --: | :-- |
| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |
| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |
| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |
| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |
| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |
| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |
| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |

融合模块

| 批处理大小 | 预填充长度 | 解码长度 | 预填充标记/秒 | 解码标记/秒 | 内存（VRAM） |
| --: | --: | --: | --: | --: | :-- |
| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |
| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |
| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |
| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |
| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |
| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |
| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |

融合和未融合模块的速度和吞吐量也经过了[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)库的测试。

![每批生成吞吐量](../Images/c73fad074a31449cad262be148000a7b.png)

前向峰值内存/批处理大小

![每批前向延迟](../Images/a6ca88db796d9aca9bc439edc51f861d.png)

每批生成吞吐量/批处理大小
