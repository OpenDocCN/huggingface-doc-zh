# 量化

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/quantization`](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)

量化技术专注于用更少的信息表示数据，同时也试图不丢失太多准确性。这通常意味着将数据类型转换为用更少的位表示相同信息。例如，如果您的模型权重存储为 32 位浮点数，并且它们被量化为 16 位浮点数，这将使模型大小减半，使其更容易存储并减少内存使用。较低的精度也可以加快推理速度，因为使用更少的位进行计算需要更少的时间。

Transformers 支持几种量化方案，帮助您在大型语言模型（LLMs）上运行推理和在量化模型上微调适配器。本指南将向您展示如何使用激活感知权重量化（AWQ）、AutoGPTQ 和 bitsandbytes。

## AWQ

尝试使用这个[notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)进行 AWQ 量化！

[激活感知权重量化（AWQ）](https://hf.co/papers/2306.00978)不会量化模型中的所有权重，而是保留对 LLM 性能重要的一小部分权重。这显著减少了量化损失，使您可以在不经历任何性能降级的情况下以 4 位精度运行模型。

有几个库可用于使用 AWQ 算法量化模型，例如[llm-awq](https://github.com/mit-han-lab/llm-awq)、[autoawq](https://github.com/casper-hansen/AutoAWQ)或[optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)。 Transformers 支持加载使用 llm-awq 和 autoawq 库量化的模型。本指南将向您展示如何加载使用 autoawq 量化的模型，但对于使用 llm-awq 量化的模型，过程类似。

确保您已安装 autoawq：

```py
pip install autoawq
```

可以通过检查模型的[config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)文件中的`quantization_config`属性来识别 AWQ 量化模型：

```py
{
  "_name_or_path": "/workspace/process/huggingfaceh4_zephyr-7b-alpha/source",
  "architectures": [
    "MistralForCausalLM"
  ],
  ...
  ...
  ...
  "quantization_config": {
    "quant_method": "awq",
    "zero_point": true,
    "group_size": 128,
    "bits": 4,
    "version": "gemm"
  }
}
```

使用 from_pretrained()方法加载量化模型。如果您在 CPU 上加载了模型，请确保首先将其移动到 GPU 设备上。使用`device_map`参数指定模型放置的位置：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

加载 AWQ 量化模型会自动将其他权重默认设置为 fp16 以提高性能。如果您想以不同格式加载这些其他权重，请使用`torch_dtype`参数：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)
```

AWQ 量化也可以与 FlashAttention-2 结合，以进一步加速推理：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

### 融合模块

融合模块提供了改进的准确性和性能，并且对于[Llama](https://huggingface.co/meta-llama)和[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)架构的 AWQ 模块支持开箱即用，但您也可以为不支持的架构融合 AWQ 模块。

融合模块不能与 FlashAttention-2 等其他优化技术结合使用。

支持的架构不支持的架构

要为支持的架构启用融合模块，请创建一个 AwqConfig 并设置参数`fuse_max_seq_len`和`do_fuse=True`。`fuse_max_seq_len`参数是总序列长度，应包括上下文长度和预期生成长度。您可以将其设置为较大的值以确保安全。

例如，要融合[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)模型的 AWQ 模块。

```py
import torch
from transformers import AwqConfig, AutoModelForCausalLM

model_id = "TheBloke/Mistral-7B-OpenOrca-AWQ"

quantization_config = AwqConfig(
    bits=4,
    fuse_max_seq_len=512,
    do_fuse=True,
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)
```

## AutoGPTQ

在这个[notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)中尝试使用 PEFT 进行 GPTQ 量化，并在这篇[博客文章](https://huggingface.co/blog/gptq-integration)中了解更多细节！

[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)库实现了 GPTQ 算法，这是一种后训练量化技术，其中权重矩阵的每一行都独立量化，以找到最小化误差的权重版本。这些权重被量化为 int4，但在推理过程中会动态恢复为 fp16。这可以通过 4 倍减少内存使用，因为 int4 权重在融合内核中而不是 GPU 的全局内存中被解量化，您还可以期望推理速度提升，因为使用较低的位宽需要更少的通信时间。

开始之前，请确保安装了以下库：

```py
pip install auto-gptq
pip install git+https://github.com/huggingface/optimum.git
pip install git+https://github.com/huggingface/transformers.git
pip install --upgrade accelerate
```

要量化一个模型（目前仅支持文本模型），您需要创建一个 GPTQConfig 类，并设置要量化的位数、用于校准权重的数据集以及准备数据集的分词器。

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset="c4", tokenizer=tokenizer)
```

您还可以将自己的数据集作为字符串列表传递，但强烈建议使用 GPTQ 论文中相同的数据集。

```py
dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]
gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)
```

加载一个要量化的模型，并将`gptq_config`传递给 from_pretrained()方法。设置`device_map="auto"`以自动将模型卸载到 CPU，以帮助将模型适配到内存中，并允许模型模块在 CPU 和 GPU 之间移动以进行量化。

```py
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```

如果由于数据集过大而导致内存不足，不支持磁盘卸载。如果是这种情况，请尝试传递`max_memory`参数来分配设备（GPU 和 CPU）上要使用的内存量：

```py
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", max_memory={0: "30GiB", 1: "46GiB", "cpu": "30GiB"}, quantization_config=gptq_config)
```

根据您的硬件，从头开始量化一个模型可能需要一些时间。在免费的 Google Colab GPU 上，量化 faceboook/opt-350m 模型可能需要约 5 分钟，但在 NVIDIA A100 上，量化一个 175B 参数模型可能需要约 4 小时。在量化模型之前，最好先检查 Hub，看看模型的 GPTQ 量化版本是否已经存在。

一旦您的模型被量化，您可以将模型和分词器推送到 Hub，这样可以轻松共享和访问。使用 push_to_hub()方法保存 GPTQConfig：

```py
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

您还可以使用 save_pretrained()方法将您的量化模型保存在本地。如果模型是使用`device_map`参数量化的，请确保在保存之前将整个模型移动到 GPU 或 CPU。例如，要在 CPU 上保存模型：

```py
quantized_model.save_pretrained("opt-125m-gptq")
tokenizer.save_pretrained("opt-125m-gptq")

# if quantized with device_map set
quantized_model.to("cpu")
quantized_model.save_pretrained("opt-125m-gptq")
```

使用 from_pretrained()方法重新加载一个量化模型，并设置`device_map="auto"`以自动将模型分布在所有可用的 GPU 上，以便更快地加载模型而不使用比所需更多的内存。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

### ExLlama

[ExLlama](https://github.com/turboderp/exllama)是 Llama 模型的 Python/C++/CUDA 实现，旨在通过 4 位 GPTQ 权重实现更快的推理（查看这些[基准测试](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)）。当您创建一个 GPTQConfig 对象时，默认情况下会激活 ExLlama 内核。为了进一步提高推理速度，请配置`exllama_config`参数使用[ExLlamaV2](https://github.com/turboderp/exllamav2)内核：

```py
import torch
from transformers import AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)
```

仅支持 4 位模型，并且我们建议在对量化模型进行微调时停用 ExLlama 内核。

只有当整个模型在 GPU 上时才支持 ExLlama 内核。如果您在 CPU 上使用 AutoGPTQ（版本>0.4.2）进行推理，则需要禁用 ExLlama 内核。这将覆盖 config.json 文件中与 ExLlama 内核相关的属性。

```py
import torch
from transformers import AutoModelForCausalLM, GPTQConfig
gptq_config = GPTQConfig(bits=4, use_exllama=False)
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="cpu", quantization_config=gptq_config)
```

## bitsandbytes

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)是将模型量化为 8 位和 4 位的最简单选择。8 位量化将 fp16 中的异常值与 int8 中的非异常值相乘，将非异常值转换回 fp16，然后将它们相加以返回 fp16 中的权重。这减少了异常值对模型性能的降级影响。4 位量化进一步压缩模型，通常与[QLoRA](https://hf.co/papers/2305.14314)一起用于微调量化的 LLM。

要使用 bitsandbytes，请确保已安装以下库：

8 位 4 位

```py
pip install transformers accelerate bitsandbytes>0.37.0
```

现在您可以使用 from_pretrained()方法中的`load_in_8bit`或`load_in_4bit`参数来量化模型。只要模型支持使用 Accelerate 加载并包含`torch.nn.Linear`层，这对任何模态的模型都适用。

8 位 4 位

将模型量化为 8 位可以减半内存使用量，对于大型模型，设置`device_map="auto"`以有效地使用可用的 GPU：

```py
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b7", device_map="auto", load_in_8bit=True)
```

默认情况下，所有其他模块（如`torch.nn.LayerNorm`）都会转换为`torch.float16`。如果需要，您可以使用`torch_dtype`参数更改这些模块的数据类型：

```py
import torch
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True, torch_dtype=torch.float32)
model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

一旦模型被量化为 8 位，除非您使用最新版本的 Transformers 和 bitsandbytes，否则无法将量化的权重推送到 Hub。如果您有最新版本，则可以使用 push_to_hub()方法将 8 位模型推送到 Hub。首先推送量化的 config.json 文件，然后是量化的模型权重。

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

仅支持使用 8 位和 4 位权重进行训练*额外*参数。

您可以使用`get_memory_footprint`方法检查内存占用情况：

```py
print(model.get_memory_footprint())
```

可以从 from_pretrained()方法中加载量化模型，无需指定`load_in_8bit`或`load_in_4bit`参数：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

### 8 位

在这篇[博客文章](https://huggingface.co/blog/hf-bitsandbytes-integration)中了解更多关于 8 位量化的细节！

本节探讨了 8 位模型的一些特定功能，如转移、异常值阈值、跳过模块转换和微调。

#### 转移

8 位模型可以在 CPU 和 GPU 之间转移权重，以支持将非常大的模型适配到内存中。发送到 CPU 的权重实际上是以**float32**存储的，并且不会转换为 8 位。例如，要为[bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)模型启用转移，请首先创建一个 BitsAndBytesConfig：

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

设计一个自定义设备映射，将所有内容都适配到 GPU 上，除了`lm_head`，这部分将发送到 CPU：

```py
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

现在使用自定义的`device_map`和`quantization_config`加载您的模型：

```py
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

#### 异常值阈值

“异常值”是大于某个阈值的隐藏状态值，这些值是在 fp16 中计算的。虽然这些值通常是正态分布的（[-3.5, 3.5]），但对于大型模型（[-60, 6]或[6, 60]），这种分布可能会有很大不同。8 位量化适用于值约为 5，但超过这个值，会有显著的性能损失。一个很好的默认阈值是 6，但对于更不稳定的模型（小模型或微调），可能需要更低的阈值。

为了找到您的模型的最佳阈值，我们建议尝试在 BitsAndBytesConfig 中使用`llm_int8_threshold`参数进行实验：

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
```

#### 跳过模块转换

对于一些模型，如 Jukebox，您不需要将每个模块量化为 8 位，这实际上可能会导致不稳定性。对于 Jukebox，有几个`lm_head`模块应该使用 BitsAndBytesConfig 中的`llm_int8_skip_modules`参数跳过：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=quantization_config,
)
```

#### 微调

使用[PEFT](https://github.com/huggingface/peft)库，您可以使用 8 位量化微调大型模型，如[flan-t5-large](https://huggingface.co/google/flan-t5-large)和[facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)。在训练时不需要传递`device_map`参数，因为它会自动将您的模型加载到 GPU 上。但是，如果您想要，仍然可以使用`device_map`参数自定义设备映射（`device_map="auto"`仅应用于推断）。

### 4 位

在这个[notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)中尝试 4 位量化，并在这篇[博客文章](https://huggingface.co/blog/4bit-transformers-bitsandbytes)中了解更多细节。

本节探讨了 4 位模型的一些特定功能，如更改计算数据类型、使用 Normal Float 4 (NF4)数据类型和使用嵌套量化。

#### 计算数据类型

为了加速计算，您可以将数据类型从 float32（默认值）更改为 bf16，使用 BitsAndBytesConfig 中的`bnb_4bit_compute_dtype`参数：

```py
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

#### Normal Float 4 (NF4)

NF4 是来自[QLoRA](https://hf.co/papers/2305.14314)论文的 4 位数据类型，适用于从正态分布初始化的权重。您应该使用 NF4 来训练 4 位基础模型。这可以通过 BitsAndBytesConfig 中的`bnb_4bit_quant_type`参数进行配置：

```py
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

对于推断，`bnb_4bit_quant_type`对性能没有太大影响。但是，为了保持与模型权重一致，您应该使用`bnb_4bit_compute_dtype`和`torch_dtype`值。

#### 嵌套量化

嵌套量化是一种技术，可以在不增加性能成本的情况下节省额外的内存。此功能对已经量化的权重执行第二次量化，以节省额外的 0.4 位/参数。例如，使用嵌套量化，您可以在 16GB 的 NVIDIA T4 GPU 上微调[Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)模型，序列长度为 1024，批量大小为 1，并启用梯度累积 4 步。

```py
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b", quantization_config=double_quant_config)
```

## Optimum

[Optimum](https://huggingface.co/docs/optimum/index)库支持 Intel、Furiosa、ONNX Runtime、GPTQ 和较低级别的 PyTorch 量化功能。如果您正在使用像 Intel CPU、Furiosa NPU 或像 ONNX Runtime 这样的模型加速器这样的特定和优化的硬件，请考虑使用 Optimum 进行量化。

## 基准测试

要比较每种量化方案的速度、吞吐量和延迟，请查看从[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)库获得的以下基准测试。该基准测试在 NVIDIA A1000 上运行，用于[TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)和[TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)模型。这些还与 bitsandbytes 量化方法以及本机 fp16 模型进行了测试。

![每批前向峰值内存](img/7f7a45cc2b5704faa291fb7c3ca255d6.png)

前向峰值内存/批处理大小

![每批前向峰值内存](img/c53e9d1c99d4b8ce87793021dc7f7393.png)

每批生成峰值内存/批处理大小

![每批生成吞吐量](img/40d57cb1a4c4282ec140a27de593156a.png)

每批生成吞吐量/批处理大小

![每批前向延迟](img/67c3f5ce2fa384e77579473e9efb77fa.png)

前向延迟/批处理大小

基准测试表明，AWQ 量化在推理、文本生成方面是最快的，并且在文本生成方面具有最低的峰值内存。然而，AWQ 在每个批处理大小上具有最大的前向延迟。要了解每种量化方法的优缺点的更详细讨论，请阅读[🤗 Transformers 中本地支持的量化方案概述](https://huggingface.co/blog/overview-quantization-transformers)博客文章。

### 融合 AWQ 模块

[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)模型在`batch_size=1`下进行了基准测试，有无融合模块。

未融合模块

| 批处理大小 | 预填充长度 | 解码长度 | 预填充标记/秒 | 解码标记/秒 | 内存（VRAM） |
| --: | --: | --: | --: | --: | :-- |
| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |
| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |
| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |
| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |
| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |
| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |
| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |

融合模块

| 批处理大小 | 预填充长度 | 解码长度 | 预填充标记/秒 | 解码标记/秒 | 内存（VRAM） |
| --: | --: | --: | --: | --: | :-- |
| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |
| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |
| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |
| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |
| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |
| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |
| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |

融合和未融合模块的速度和吞吐量也经过了[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)库的测试。

![每批生成吞吐量](img/c73fad074a31449cad262be148000a7b.png)

前向峰值内存/批处理大小

![每批前向延迟](img/a6ca88db796d9aca9bc439edc51f861d.png)

每批生成吞吐量/批处理大小
