# é‡åŒ–

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)

é‡åŒ–æŠ€æœ¯ä¸“æ³¨äºç”¨æ›´å°‘çš„ä¿¡æ¯è¡¨ç¤ºæ•°æ®ï¼ŒåŒæ—¶ä¹Ÿè¯•å›¾ä¸ä¸¢å¤±å¤ªå¤šå‡†ç¡®æ€§ã€‚è¿™é€šå¸¸æ„å‘³ç€å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºç”¨æ›´å°‘çš„ä½è¡¨ç¤ºç›¸åŒä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹æƒé‡å­˜å‚¨ä¸º32ä½æµ®ç‚¹æ•°ï¼Œå¹¶ä¸”å®ƒä»¬è¢«é‡åŒ–ä¸º16ä½æµ®ç‚¹æ•°ï¼Œè¿™å°†ä½¿æ¨¡å‹å¤§å°å‡åŠï¼Œä½¿å…¶æ›´å®¹æ˜“å­˜å‚¨å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è¾ƒä½çš„ç²¾åº¦ä¹Ÿå¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œå› ä¸ºä½¿ç”¨æ›´å°‘çš„ä½è¿›è¡Œè®¡ç®—éœ€è¦æ›´å°‘çš„æ—¶é—´ã€‚

Transformersæ”¯æŒå‡ ç§é‡åŒ–æ–¹æ¡ˆï¼Œå¸®åŠ©æ‚¨åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šè¿è¡Œæ¨ç†å’Œåœ¨é‡åŒ–æ¨¡å‹ä¸Šå¾®è°ƒé€‚é…å™¨ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ï¼ˆAWQï¼‰ã€AutoGPTQå’Œbitsandbytesã€‚

## AWQ

å°è¯•ä½¿ç”¨è¿™ä¸ª[notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)è¿›è¡ŒAWQé‡åŒ–ï¼

[æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ï¼ˆAWQï¼‰](https://hf.co/papers/2306.00978)ä¸ä¼šé‡åŒ–æ¨¡å‹ä¸­çš„æ‰€æœ‰æƒé‡ï¼Œè€Œæ˜¯ä¿ç•™å¯¹LLMæ€§èƒ½é‡è¦çš„ä¸€å°éƒ¨åˆ†æƒé‡ã€‚è¿™æ˜¾è‘—å‡å°‘äº†é‡åŒ–æŸå¤±ï¼Œä½¿æ‚¨å¯ä»¥åœ¨ä¸ç»å†ä»»ä½•æ€§èƒ½é™çº§çš„æƒ…å†µä¸‹ä»¥4ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ã€‚

æœ‰å‡ ä¸ªåº“å¯ç”¨äºä½¿ç”¨AWQç®—æ³•é‡åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚[llm-awq](https://github.com/mit-han-lab/llm-awq)ã€[autoawq](https://github.com/casper-hansen/AutoAWQ)æˆ–[optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)ã€‚ Transformersæ”¯æŒåŠ è½½ä½¿ç”¨llm-awqå’Œautoawqåº“é‡åŒ–çš„æ¨¡å‹ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½ä½¿ç”¨autoawqé‡åŒ–çš„æ¨¡å‹ï¼Œä½†å¯¹äºä½¿ç”¨llm-awqé‡åŒ–çš„æ¨¡å‹ï¼Œè¿‡ç¨‹ç±»ä¼¼ã€‚

ç¡®ä¿æ‚¨å·²å®‰è£…autoawqï¼š

```py
pip install autoawq
```

å¯ä»¥é€šè¿‡æ£€æŸ¥æ¨¡å‹çš„[config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)æ–‡ä»¶ä¸­çš„`quantization_config`å±æ€§æ¥è¯†åˆ«AWQé‡åŒ–æ¨¡å‹ï¼š

```py
{
  "_name_or_path": "/workspace/process/huggingfaceh4_zephyr-7b-alpha/source",
  "architectures": [
    "MistralForCausalLM"
  ],
  ...
  ...
  ...
  "quantization_config": {
    "quant_method": "awq",
    "zero_point": true,
    "group_size": 128,
    "bits": 4,
    "version": "gemm"
  }
}
```

ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•åŠ è½½é‡åŒ–æ¨¡å‹ã€‚å¦‚æœæ‚¨åœ¨CPUä¸ŠåŠ è½½äº†æ¨¡å‹ï¼Œè¯·ç¡®ä¿é¦–å…ˆå°†å…¶ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šã€‚ä½¿ç”¨`device_map`å‚æ•°æŒ‡å®šæ¨¡å‹æ”¾ç½®çš„ä½ç½®ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

åŠ è½½AWQé‡åŒ–æ¨¡å‹ä¼šè‡ªåŠ¨å°†å…¶ä»–æƒé‡é»˜è®¤è®¾ç½®ä¸ºfp16ä»¥æé«˜æ€§èƒ½ã€‚å¦‚æœæ‚¨æƒ³ä»¥ä¸åŒæ ¼å¼åŠ è½½è¿™äº›å…¶ä»–æƒé‡ï¼Œè¯·ä½¿ç”¨`torch_dtype`å‚æ•°ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)
```

AWQé‡åŒ–ä¹Ÿå¯ä»¥ä¸[FlashAttention-2](perf_infer_gpu_one#flashattention-2)ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

### èåˆæ¨¡å—

èåˆæ¨¡å—æä¾›äº†æ”¹è¿›çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ï¼Œå¹¶ä¸”å¯¹äº[Llama](https://huggingface.co/meta-llama)å’Œ[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)æ¶æ„çš„AWQæ¨¡å—æ”¯æŒå¼€ç®±å³ç”¨ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä¸ºä¸æ”¯æŒçš„æ¶æ„èåˆAWQæ¨¡å—ã€‚

èåˆæ¨¡å—ä¸èƒ½ä¸FlashAttention-2ç­‰å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ç»“åˆä½¿ç”¨ã€‚

æ”¯æŒçš„æ¶æ„ä¸æ”¯æŒçš„æ¶æ„

è¦ä¸ºæ”¯æŒçš„æ¶æ„å¯ç”¨èåˆæ¨¡å—ï¼Œè¯·åˆ›å»ºä¸€ä¸ª[AwqConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.AwqConfig)å¹¶è®¾ç½®å‚æ•°`fuse_max_seq_len`å’Œ`do_fuse=True`ã€‚`fuse_max_seq_len`å‚æ•°æ˜¯æ€»åºåˆ—é•¿åº¦ï¼Œåº”åŒ…æ‹¬ä¸Šä¸‹æ–‡é•¿åº¦å’Œé¢„æœŸç”Ÿæˆé•¿åº¦ã€‚æ‚¨å¯ä»¥å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥ç¡®ä¿å®‰å…¨ã€‚

ä¾‹å¦‚ï¼Œè¦èåˆ[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)æ¨¡å‹çš„AWQæ¨¡å—ã€‚

```py
import torch
from transformers import AwqConfig, AutoModelForCausalLM

model_id = "TheBloke/Mistral-7B-OpenOrca-AWQ"

quantization_config = AwqConfig(
    bits=4,
    fuse_max_seq_len=512,
    do_fuse=True,
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)
```

## AutoGPTQ

åœ¨è¿™ä¸ª[notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)ä¸­å°è¯•ä½¿ç”¨PEFTè¿›è¡ŒGPTQé‡åŒ–ï¼Œå¹¶åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/gptq-integration)ä¸­äº†è§£æ›´å¤šç»†èŠ‚ï¼

[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)åº“å®ç°äº†GPTQç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œå…¶ä¸­æƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œéƒ½ç‹¬ç«‹é‡åŒ–ï¼Œä»¥æ‰¾åˆ°æœ€å°åŒ–è¯¯å·®çš„æƒé‡ç‰ˆæœ¬ã€‚è¿™äº›æƒé‡è¢«é‡åŒ–ä¸ºint4ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šåŠ¨æ€æ¢å¤ä¸ºfp16ã€‚è¿™å¯ä»¥é€šè¿‡4å€å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œå› ä¸ºint4æƒé‡åœ¨èåˆå†…æ ¸ä¸­è€Œä¸æ˜¯GPUçš„å…¨å±€å†…å­˜ä¸­è¢«è§£é‡åŒ–ï¼Œæ‚¨è¿˜å¯ä»¥æœŸæœ›æ¨ç†é€Ÿåº¦æå‡ï¼Œå› ä¸ºä½¿ç”¨è¾ƒä½çš„ä½å®½éœ€è¦æ›´å°‘çš„é€šä¿¡æ—¶é—´ã€‚

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹åº“ï¼š

```py
pip install auto-gptq
pip install git+https://github.com/huggingface/optimum.git
pip install git+https://github.com/huggingface/transformers.git
pip install --upgrade accelerate
```

è¦é‡åŒ–ä¸€ä¸ªæ¨¡å‹ï¼ˆç›®å‰ä»…æ”¯æŒæ–‡æœ¬æ¨¡å‹ï¼‰ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)ç±»ï¼Œå¹¶è®¾ç½®è¦é‡åŒ–çš„ä½æ•°ã€ç”¨äºæ ¡å‡†æƒé‡çš„æ•°æ®é›†ä»¥åŠå‡†å¤‡æ•°æ®é›†çš„åˆ†è¯å™¨ã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset="c4", tokenizer=tokenizer)
```

æ‚¨è¿˜å¯ä»¥å°†è‡ªå·±çš„æ•°æ®é›†ä½œä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ï¼Œä½†å¼ºçƒˆå»ºè®®ä½¿ç”¨GPTQè®ºæ–‡ä¸­ç›¸åŒçš„æ•°æ®é›†ã€‚

```py
dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]
gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)
```

åŠ è½½ä¸€ä¸ªè¦é‡åŒ–çš„æ¨¡å‹ï¼Œå¹¶å°†`gptq_config`ä¼ é€’ç»™[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)æ–¹æ³•ã€‚è®¾ç½®`device_map="auto"`ä»¥è‡ªåŠ¨å°†æ¨¡å‹å¸è½½åˆ°CPUï¼Œä»¥å¸®åŠ©å°†æ¨¡å‹é€‚é…åˆ°å†…å­˜ä¸­ï¼Œå¹¶å…è®¸æ¨¡å‹æ¨¡å—åœ¨CPUå’ŒGPUä¹‹é—´ç§»åŠ¨ä»¥è¿›è¡Œé‡åŒ–ã€‚

```py
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```

å¦‚æœç”±äºæ•°æ®é›†è¿‡å¤§è€Œå¯¼è‡´å†…å­˜ä¸è¶³ï¼Œä¸æ”¯æŒç£ç›˜å¸è½½ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œè¯·å°è¯•ä¼ é€’`max_memory`å‚æ•°æ¥åˆ†é…è®¾å¤‡ï¼ˆGPUå’ŒCPUï¼‰ä¸Šè¦ä½¿ç”¨çš„å†…å­˜é‡ï¼š

```py
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", max_memory={0: "30GiB", 1: "46GiB", "cpu": "30GiB"}, quantization_config=gptq_config)
```

æ ¹æ®æ‚¨çš„ç¡¬ä»¶ï¼Œä»å¤´å¼€å§‹é‡åŒ–ä¸€ä¸ªæ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚åœ¨å…è´¹çš„Google Colab GPUä¸Šï¼Œé‡åŒ–faceboook/opt-350mæ¨¡å‹å¯èƒ½éœ€è¦çº¦5åˆ†é’Ÿï¼Œä½†åœ¨NVIDIA A100ä¸Šï¼Œé‡åŒ–ä¸€ä¸ª175Bå‚æ•°æ¨¡å‹å¯èƒ½éœ€è¦çº¦4å°æ—¶ã€‚åœ¨é‡åŒ–æ¨¡å‹ä¹‹å‰ï¼Œæœ€å¥½å…ˆæ£€æŸ¥Hubï¼Œçœ‹çœ‹æ¨¡å‹çš„GPTQé‡åŒ–ç‰ˆæœ¬æ˜¯å¦å·²ç»å­˜åœ¨ã€‚

ä¸€æ—¦æ‚¨çš„æ¨¡å‹è¢«é‡åŒ–ï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹å’Œåˆ†è¯å™¨æ¨é€åˆ°Hubï¼Œè¿™æ ·å¯ä»¥è½»æ¾å…±äº«å’Œè®¿é—®ã€‚ä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•ä¿å­˜[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)ï¼š

```py
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)æ–¹æ³•å°†æ‚¨çš„é‡åŒ–æ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°ã€‚å¦‚æœæ¨¡å‹æ˜¯ä½¿ç”¨`device_map`å‚æ•°é‡åŒ–çš„ï¼Œè¯·ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°GPUæˆ–CPUã€‚ä¾‹å¦‚ï¼Œè¦åœ¨CPUä¸Šä¿å­˜æ¨¡å‹ï¼š

```py
quantized_model.save_pretrained("opt-125m-gptq")
tokenizer.save_pretrained("opt-125m-gptq")

# if quantized with device_map set
quantized_model.to("cpu")
quantized_model.save_pretrained("opt-125m-gptq")
```

ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•é‡æ–°åŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹ï¼Œå¹¶è®¾ç½®`device_map="auto"`ä»¥è‡ªåŠ¨å°†æ¨¡å‹åˆ†å¸ƒåœ¨æ‰€æœ‰å¯ç”¨çš„GPUä¸Šï¼Œä»¥ä¾¿æ›´å¿«åœ°åŠ è½½æ¨¡å‹è€Œä¸ä½¿ç”¨æ¯”æ‰€éœ€æ›´å¤šçš„å†…å­˜ã€‚

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

### ExLlama

[ExLlama](https://github.com/turboderp/exllama)æ˜¯[Llama](model_doc/llama)æ¨¡å‹çš„Python/C++/CUDAå®ç°ï¼Œæ—¨åœ¨é€šè¿‡4ä½GPTQæƒé‡å®ç°æ›´å¿«çš„æ¨ç†ï¼ˆæŸ¥çœ‹è¿™äº›[åŸºå‡†æµ‹è¯•](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)ï¼‰ã€‚å½“æ‚¨åˆ›å»ºä¸€ä¸ª[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)å¯¹è±¡æ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šæ¿€æ´»ExLlamaå†…æ ¸ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨ç†é€Ÿåº¦ï¼Œè¯·é…ç½®`exllama_config`å‚æ•°ä½¿ç”¨[ExLlamaV2](https://github.com/turboderp/exllamav2)å†…æ ¸ï¼š

```py
import torch
from transformers import AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)
```

ä»…æ”¯æŒ4ä½æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬å»ºè®®åœ¨å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶åœç”¨ExLlamaå†…æ ¸ã€‚

åªæœ‰å½“æ•´ä¸ªæ¨¡å‹åœ¨GPUä¸Šæ—¶æ‰æ”¯æŒExLlamaå†…æ ¸ã€‚å¦‚æœæ‚¨åœ¨CPUä¸Šä½¿ç”¨AutoGPTQï¼ˆç‰ˆæœ¬>0.4.2ï¼‰è¿›è¡Œæ¨ç†ï¼Œåˆ™éœ€è¦ç¦ç”¨ExLlamaå†…æ ¸ã€‚è¿™å°†è¦†ç›–config.jsonæ–‡ä»¶ä¸­ä¸ExLlamaå†…æ ¸ç›¸å…³çš„å±æ€§ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, GPTQConfig
gptq_config = GPTQConfig(bits=4, use_exllama=False)
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="cpu", quantization_config=gptq_config)
```

## bitsandbytes

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)æ˜¯å°†æ¨¡å‹é‡åŒ–ä¸º8ä½å’Œ4ä½çš„æœ€ç®€å•é€‰æ‹©ã€‚8ä½é‡åŒ–å°†fp16ä¸­çš„å¼‚å¸¸å€¼ä¸int8ä¸­çš„éå¼‚å¸¸å€¼ç›¸ä¹˜ï¼Œå°†éå¼‚å¸¸å€¼è½¬æ¢å›fp16ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ ä»¥è¿”å›fp16ä¸­çš„æƒé‡ã€‚è¿™å‡å°‘äº†å¼‚å¸¸å€¼å¯¹æ¨¡å‹æ€§èƒ½çš„é™çº§å½±å“ã€‚4ä½é‡åŒ–è¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ï¼Œé€šå¸¸ä¸[QLoRA](https://hf.co/papers/2305.14314)ä¸€èµ·ç”¨äºå¾®è°ƒé‡åŒ–çš„LLMã€‚

è¦ä½¿ç”¨bitsandbytesï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š

8ä½4ä½

```py
pip install transformers accelerate bitsandbytes>0.37.0
```

ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä¸­çš„`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°æ¥é‡åŒ–æ¨¡å‹ã€‚åªè¦æ¨¡å‹æ”¯æŒä½¿ç”¨AccelerateåŠ è½½å¹¶åŒ…å«`torch.nn.Linear`å±‚ï¼Œè¿™å¯¹ä»»ä½•æ¨¡æ€çš„æ¨¡å‹éƒ½é€‚ç”¨ã€‚

8ä½4ä½

å°†æ¨¡å‹é‡åŒ–ä¸º8ä½å¯ä»¥å‡åŠå†…å­˜ä½¿ç”¨é‡ï¼Œå¯¹äºå¤§å‹æ¨¡å‹ï¼Œè®¾ç½®`device_map="auto"`ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨å¯ç”¨çš„GPUï¼š

```py
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b7", device_map="auto", load_in_8bit=True)
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆå¦‚`torch.nn.LayerNorm`ï¼‰éƒ½ä¼šè½¬æ¢ä¸º`torch.float16`ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`torch_dtype`å‚æ•°æ›´æ”¹è¿™äº›æ¨¡å—çš„æ•°æ®ç±»å‹ï¼š

```py
import torch
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True, torch_dtype=torch.float32)
model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

ä¸€æ—¦æ¨¡å‹è¢«é‡åŒ–ä¸º8ä½ï¼Œé™¤éæ‚¨ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„Transformerså’Œbitsandbytesï¼Œå¦åˆ™æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ°Hubã€‚å¦‚æœæ‚¨æœ‰æœ€æ–°ç‰ˆæœ¬ï¼Œåˆ™å¯ä»¥ä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•å°†8ä½æ¨¡å‹æ¨é€åˆ°Hubã€‚é¦–å…ˆæ¨é€é‡åŒ–çš„config.jsonæ–‡ä»¶ï¼Œç„¶åæ˜¯é‡åŒ–çš„æ¨¡å‹æƒé‡ã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

ä»…æ”¯æŒä½¿ç”¨8ä½å’Œ4ä½æƒé‡è¿›è¡Œè®­ç»ƒ*é¢å¤–*å‚æ•°ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨`get_memory_footprint`æ–¹æ³•æ£€æŸ¥å†…å­˜å ç”¨æƒ…å†µï¼š

```py
print(model.get_memory_footprint())
```

å¯ä»¥ä»[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä¸­åŠ è½½é‡åŒ–æ¨¡å‹ï¼Œæ— éœ€æŒ‡å®š`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

### 8ä½

åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)ä¸­äº†è§£æ›´å¤šå…³äº8ä½é‡åŒ–çš„ç»†èŠ‚ï¼

æœ¬èŠ‚æ¢è®¨äº†8ä½æ¨¡å‹çš„ä¸€äº›ç‰¹å®šåŠŸèƒ½ï¼Œå¦‚è½¬ç§»ã€å¼‚å¸¸å€¼é˜ˆå€¼ã€è·³è¿‡æ¨¡å—è½¬æ¢å’Œå¾®è°ƒã€‚

#### è½¬ç§»

8ä½æ¨¡å‹å¯ä»¥åœ¨CPUå’ŒGPUä¹‹é—´è½¬ç§»æƒé‡ï¼Œä»¥æ”¯æŒå°†éå¸¸å¤§çš„æ¨¡å‹é€‚é…åˆ°å†…å­˜ä¸­ã€‚å‘é€åˆ°CPUçš„æƒé‡å®é™…ä¸Šæ˜¯ä»¥**float32**å­˜å‚¨çš„ï¼Œå¹¶ä¸”ä¸ä¼šè½¬æ¢ä¸º8ä½ã€‚ä¾‹å¦‚ï¼Œè¦ä¸º[bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)æ¨¡å‹å¯ç”¨è½¬ç§»ï¼Œè¯·é¦–å…ˆåˆ›å»ºä¸€ä¸ª[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ï¼š

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

è®¾è®¡ä¸€ä¸ªè‡ªå®šä¹‰è®¾å¤‡æ˜ å°„ï¼Œå°†æ‰€æœ‰å†…å®¹éƒ½é€‚é…åˆ°GPUä¸Šï¼Œé™¤äº†`lm_head`ï¼Œè¿™éƒ¨åˆ†å°†å‘é€åˆ°CPUï¼š

```py
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

ç°åœ¨ä½¿ç”¨è‡ªå®šä¹‰çš„`device_map`å’Œ`quantization_config`åŠ è½½æ‚¨çš„æ¨¡å‹ï¼š

```py
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

#### å¼‚å¸¸å€¼é˜ˆå€¼

â€œå¼‚å¸¸å€¼â€æ˜¯å¤§äºæŸä¸ªé˜ˆå€¼çš„éšè—çŠ¶æ€å€¼ï¼Œè¿™äº›å€¼æ˜¯åœ¨fp16ä¸­è®¡ç®—çš„ã€‚è™½ç„¶è¿™äº›å€¼é€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼ˆ[-3.5, 3.5]ï¼‰ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹ï¼ˆ[-60, 6]æˆ–[6, 60]ï¼‰ï¼Œè¿™ç§åˆ†å¸ƒå¯èƒ½ä¼šæœ‰å¾ˆå¤§ä¸åŒã€‚8ä½é‡åŒ–é€‚ç”¨äºå€¼çº¦ä¸º5ï¼Œä½†è¶…è¿‡è¿™ä¸ªå€¼ï¼Œä¼šæœ‰æ˜¾è‘—çš„æ€§èƒ½æŸå¤±ã€‚ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯6ï¼Œä½†å¯¹äºæ›´ä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹æˆ–å¾®è°ƒï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´ä½çš„é˜ˆå€¼ã€‚

ä¸ºäº†æ‰¾åˆ°æ‚¨çš„æ¨¡å‹çš„æœ€ä½³é˜ˆå€¼ï¼Œæˆ‘ä»¬å»ºè®®å°è¯•åœ¨[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­ä½¿ç”¨`llm_int8_threshold`å‚æ•°è¿›è¡Œå®éªŒï¼š

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
```

#### è·³è¿‡æ¨¡å—è½¬æ¢

å¯¹äºä¸€äº›æ¨¡å‹ï¼Œå¦‚[Jukebox](model_doc/jukebox)ï¼Œæ‚¨ä¸éœ€è¦å°†æ¯ä¸ªæ¨¡å—é‡åŒ–ä¸º8ä½ï¼Œè¿™å®é™…ä¸Šå¯èƒ½ä¼šå¯¼è‡´ä¸ç¨³å®šæ€§ã€‚å¯¹äºJukeboxï¼Œæœ‰å‡ ä¸ª`lm_head`æ¨¡å—åº”è¯¥ä½¿ç”¨[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­çš„`llm_int8_skip_modules`å‚æ•°è·³è¿‡ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=quantization_config,
)
```

#### å¾®è°ƒ

ä½¿ç”¨[PEFT](https://github.com/huggingface/peft)åº“ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨8ä½é‡åŒ–å¾®è°ƒå¤§å‹æ¨¡å‹ï¼Œå¦‚[flan-t5-large](https://huggingface.co/google/flan-t5-large)å’Œ[facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)ã€‚åœ¨è®­ç»ƒæ—¶ä¸éœ€è¦ä¼ é€’`device_map`å‚æ•°ï¼Œå› ä¸ºå®ƒä¼šè‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ°GPUä¸Šã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³è¦ï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨`device_map`å‚æ•°è‡ªå®šä¹‰è®¾å¤‡æ˜ å°„ï¼ˆ`device_map="auto"`ä»…åº”ç”¨äºæ¨æ–­ï¼‰ã€‚

### 4ä½

åœ¨è¿™ä¸ª[notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)ä¸­å°è¯•4ä½é‡åŒ–ï¼Œå¹¶åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)ä¸­äº†è§£æ›´å¤šç»†èŠ‚ã€‚

æœ¬èŠ‚æ¢è®¨äº†4ä½æ¨¡å‹çš„ä¸€äº›ç‰¹å®šåŠŸèƒ½ï¼Œå¦‚æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹ã€ä½¿ç”¨Normal Float 4 (NF4)æ•°æ®ç±»å‹å’Œä½¿ç”¨åµŒå¥—é‡åŒ–ã€‚

#### è®¡ç®—æ•°æ®ç±»å‹

ä¸ºäº†åŠ é€Ÿè®¡ç®—ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®ç±»å‹ä»float32ï¼ˆé»˜è®¤å€¼ï¼‰æ›´æ”¹ä¸ºbf16ï¼Œä½¿ç”¨[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­çš„`bnb_4bit_compute_dtype`å‚æ•°ï¼š

```py
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

#### Normal Float 4 (NF4)

NF4æ˜¯æ¥è‡ª[QLoRA](https://hf.co/papers/2305.14314)è®ºæ–‡çš„4ä½æ•°æ®ç±»å‹ï¼Œé€‚ç”¨äºä»æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡ã€‚æ‚¨åº”è¯¥ä½¿ç”¨NF4æ¥è®­ç»ƒ4ä½åŸºç¡€æ¨¡å‹ã€‚è¿™å¯ä»¥é€šè¿‡[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­çš„`bnb_4bit_quant_type`å‚æ•°è¿›è¡Œé…ç½®ï¼š

```py
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

å¯¹äºæ¨æ–­ï¼Œ`bnb_4bit_quant_type`å¯¹æ€§èƒ½æ²¡æœ‰å¤ªå¤§å½±å“ã€‚ä½†æ˜¯ï¼Œä¸ºäº†ä¿æŒä¸æ¨¡å‹æƒé‡ä¸€è‡´ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨`bnb_4bit_compute_dtype`å’Œ`torch_dtype`å€¼ã€‚

#### åµŒå¥—é‡åŒ–

åµŒå¥—é‡åŒ–æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ æ€§èƒ½æˆæœ¬çš„æƒ…å†µä¸‹èŠ‚çœé¢å¤–çš„å†…å­˜ã€‚æ­¤åŠŸèƒ½å¯¹å·²ç»é‡åŒ–çš„æƒé‡æ‰§è¡Œç¬¬äºŒæ¬¡é‡åŒ–ï¼Œä»¥èŠ‚çœé¢å¤–çš„0.4ä½/å‚æ•°ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åµŒå¥—é‡åŒ–ï¼Œæ‚¨å¯ä»¥åœ¨16GBçš„NVIDIA T4 GPUä¸Šå¾®è°ƒ[Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)æ¨¡å‹ï¼Œåºåˆ—é•¿åº¦ä¸º1024ï¼Œæ‰¹é‡å¤§å°ä¸º1ï¼Œå¹¶å¯ç”¨æ¢¯åº¦ç´¯ç§¯4æ­¥ã€‚

```py
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b", quantization_config=double_quant_config)
```

## Optimum

[Optimum](https://huggingface.co/docs/optimum/index)åº“æ”¯æŒIntelã€Furiosaã€ONNX Runtimeã€GPTQå’Œè¾ƒä½çº§åˆ«çš„PyTorché‡åŒ–åŠŸèƒ½ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨åƒIntel CPUã€Furiosa NPUæˆ–åƒONNX Runtimeè¿™æ ·çš„æ¨¡å‹åŠ é€Ÿå™¨è¿™æ ·çš„ç‰¹å®šå’Œä¼˜åŒ–çš„ç¡¬ä»¶ï¼Œè¯·è€ƒè™‘ä½¿ç”¨Optimumè¿›è¡Œé‡åŒ–ã€‚

## åŸºå‡†æµ‹è¯•

è¦æ¯”è¾ƒæ¯ç§é‡åŒ–æ–¹æ¡ˆçš„é€Ÿåº¦ã€ååé‡å’Œå»¶è¿Ÿï¼Œè¯·æŸ¥çœ‹ä»[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)åº“è·å¾—çš„ä»¥ä¸‹åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åœ¨NVIDIA A1000ä¸Šè¿è¡Œï¼Œç”¨äº[TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)å’Œ[TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)æ¨¡å‹ã€‚è¿™äº›è¿˜ä¸bitsandbytesé‡åŒ–æ–¹æ³•ä»¥åŠæœ¬æœºfp16æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚

![æ¯æ‰¹å‰å‘å³°å€¼å†…å­˜](../Images/7f7a45cc2b5704faa291fb7c3ca255d6.png)

å‰å‘å³°å€¼å†…å­˜/æ‰¹å¤„ç†å¤§å°

![æ¯æ‰¹å‰å‘å³°å€¼å†…å­˜](../Images/c53e9d1c99d4b8ce87793021dc7f7393.png)

æ¯æ‰¹ç”Ÿæˆå³°å€¼å†…å­˜/æ‰¹å¤„ç†å¤§å°

![æ¯æ‰¹ç”Ÿæˆååé‡](../Images/40d57cb1a4c4282ec140a27de593156a.png)

æ¯æ‰¹ç”Ÿæˆååé‡/æ‰¹å¤„ç†å¤§å°

![æ¯æ‰¹å‰å‘å»¶è¿Ÿ](../Images/67c3f5ce2fa384e77579473e9efb77fa.png)

å‰å‘å»¶è¿Ÿ/æ‰¹å¤„ç†å¤§å°

åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒAWQé‡åŒ–åœ¨æ¨ç†ã€æ–‡æœ¬ç”Ÿæˆæ–¹é¢æ˜¯æœ€å¿«çš„ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å…·æœ‰æœ€ä½çš„å³°å€¼å†…å­˜ã€‚ç„¶è€Œï¼ŒAWQåœ¨æ¯ä¸ªæ‰¹å¤„ç†å¤§å°ä¸Šå…·æœ‰æœ€å¤§çš„å‰å‘å»¶è¿Ÿã€‚è¦äº†è§£æ¯ç§é‡åŒ–æ–¹æ³•çš„ä¼˜ç¼ºç‚¹çš„æ›´è¯¦ç»†è®¨è®ºï¼Œè¯·é˜…è¯»[ğŸ¤— Transformersä¸­æœ¬åœ°æ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆæ¦‚è¿°](https://huggingface.co/blog/overview-quantization-transformers)åšå®¢æ–‡ç« ã€‚

### èåˆAWQæ¨¡å—

[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)æ¨¡å‹åœ¨`batch_size=1`ä¸‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæœ‰æ— èåˆæ¨¡å—ã€‚

æœªèåˆæ¨¡å—

| æ‰¹å¤„ç†å¤§å° | é¢„å¡«å……é•¿åº¦ | è§£ç é•¿åº¦ | é¢„å¡«å……æ ‡è®°/ç§’ | è§£ç æ ‡è®°/ç§’ | å†…å­˜ï¼ˆVRAMï¼‰ |
| --: | --: | --: | --: | --: | :-- |
| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |
| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |
| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |
| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |
| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |
| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |
| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |

èåˆæ¨¡å—

| æ‰¹å¤„ç†å¤§å° | é¢„å¡«å……é•¿åº¦ | è§£ç é•¿åº¦ | é¢„å¡«å……æ ‡è®°/ç§’ | è§£ç æ ‡è®°/ç§’ | å†…å­˜ï¼ˆVRAMï¼‰ |
| --: | --: | --: | --: | --: | :-- |
| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |
| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |
| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |
| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |
| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |
| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |
| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |

èåˆå’Œæœªèåˆæ¨¡å—çš„é€Ÿåº¦å’Œååé‡ä¹Ÿç»è¿‡äº†[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)åº“çš„æµ‹è¯•ã€‚

![æ¯æ‰¹ç”Ÿæˆååé‡](../Images/c73fad074a31449cad262be148000a7b.png)

å‰å‘å³°å€¼å†…å­˜/æ‰¹å¤„ç†å¤§å°

![æ¯æ‰¹å‰å‘å»¶è¿Ÿ](../Images/a6ca88db796d9aca9bc439edc51f861d.png)

æ¯æ‰¹ç”Ÿæˆååé‡/æ‰¹å¤„ç†å¤§å°
