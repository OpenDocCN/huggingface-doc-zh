- en: Image-to-Video Generation with PIA (Personalized Image Animator)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/pia](https://huggingface.co/docs/diffusers/api/pipelines/pia)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image
    Models](https://arxiv.org/abs/2312.13964) by Yiming Zhang, Zhening Xing, Yanhong
    Zeng, Youqing Fang, Kai Chen'
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in personalized text-to-image (T2I) models have revolutionized
    content creation, empowering non-experts to generate stunning images with unique
    styles. While promising, adding realistic motions into these personalized images
    by text poses significant challenges in preserving distinct styles, high-fidelity
    details, and achieving motion controllability by text. In this paper, we present
    PIA, a Personalized Image Animator that excels in aligning with condition images,
    achieving motion controllability by text, and the compatibility with various personalized
    T2I models without specific tuning. To achieve these goals, PIA builds upon a
    base T2I model with well-trained temporal alignment layers, allowing for the seamless
    transformation of any personalized T2I model into an image animation model. A
    key component of PIA is the introduction of the condition module, which utilizes
    the condition frame and inter-frame affinity as input to transfer appearance information
    guided by the affinity hint for individual frame synthesis in the latent space.
    This design mitigates the challenges of appearance-related image alignment within
    and allows for a stronger focus on aligning with motion-related guidance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Project page](https://pi-animator.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Available Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Pipeline | Tasks | Demo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| [PIAPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pia/pipeline_pia.py)
    | *Image-to-Video Generation with PIA* |  |'
  prefs: []
  type: TYPE_TB
- en: Available checkpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Motion Adapter checkpoints for PIA can be found under the [OpenMMLab org](https://huggingface.co/openmmlab/PIA-condition-adapter).
    These checkpoints are meant to work with any model based on Stable Diffusion 1.5
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PIA works with a MotionAdapter checkpoint and a Stable Diffusion 1.5 model checkpoint.
    The MotionAdapter is a collection of Motion Modules that are responsible for adding
    coherent motion across image frames. These modules are applied after the Resnet
    and Attention blocks in the Stable Diffusion UNet. In addition to the motion modules,
    PIA also replaces the input convolution layer of the SD 1.5 UNet model with a
    9 channel input convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: The following example demonstrates how to use PIA to generate a video from a
    single image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some sample outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| masterpiece, bestquality, sunset. ![cat in a field](../Images/0e6cae78c21b7faca444db4e5ac98357.png)
    |'
  prefs: []
  type: TYPE_TB
- en: If you plan on using a scheduler that can clip samples, make sure to disable
    it by setting `clip_sample=False` in the scheduler as this can also have an adverse
    effect on generated samples. Additionally, the PIA checkpoints can be sensitive
    to the beta schedule of the scheduler. We recommend setting this to `linear`.
  prefs: []
  type: TYPE_NORMAL
- en: Using FreeInit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537)
    by Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu.'
  prefs: []
  type: TYPE_NORMAL
- en: FreeInit is an effective method that improves temporal consistency and overall
    quality of videos generated using video-diffusion-models without any addition
    training. It can be applied to PIA, AnimateDiff, ModelScope, VideoCrafter and
    various other video generation models seamlessly at inference time, and works
    by iteratively refining the latent-initialization noise. More details can be found
    it the paper.
  prefs: []
  type: TYPE_NORMAL
- en: The following example demonstrates the usage of FreeInit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '| masterpiece, bestquality, sunset. ![cat in a field](../Images/2c58eb14c2ee5ab1bb455398e9a71b14.png)
    |'
  prefs: []
  type: TYPE_TB
- en: FreeInit is not really free - the improved quality comes at the cost of extra
    computation. It requires sampling a few extra times depending on the `num_iters`
    parameter that is set when enabling it. Setting the `use_fast_sampling` parameter
    to `True` can improve the overall performance (at the cost of lower quality compared
    to when `use_fast_sampling=False` but still better results than vanilla video
    generation models).
  prefs: []
  type: TYPE_NORMAL
- en: PIAPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.PIAPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L212)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    used to create a UNetMotionModel to denoise the encoded video latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`motion_adapter` (`MotionAdapter`) — A `MotionAdapter` to be used in combination
    with `unet` to denoise the encoded video latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-video generation.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L948)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`PipelineImageInput`) — The input image to be used for video generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strength` (`float`, *optional*, defaults to 1.0) — Indicates extent to transform
    the reference `image`. Must be between 0 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_frames` (`int`, *optional*, defaults to 16) — The number of video frames
    that are generated. Defaults to 16 frames which at 8 frames per seconds amounts
    to 2 seconds of video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality videos at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`. Latents
    should be of shape `(batch_size, num_channel, num_frames, height, width)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image — (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters. motion_scale — (`int`, *optional*, defaults
    to 0): Parameter that controls the amount and type of motion that is added to
    the image. Increasing the value increases the amount of motion, while specific
    ranges of values control the type of motion that is added. Must be between 0 and
    8. Set between 0-2 to only increase the amount of motion. Set between 3-5 to create
    looping motion. Set between 6-8 to perform motion with image style transfer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated video. Choose between `torch.FloatTensor`, `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated frames.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_free_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L620)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Disables the FreeInit mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L568)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L520)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L537)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_free_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L576)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_iters` (`int`, *optional*, defaults to `3`) — Number of FreeInit noise
    re-initialization iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_fast_sampling` (`bool`, *optional*, defaults to `False`) — Whether or
    not to speedup sampling procedure at the cost of probably lower quality results.
    Enables the “Coarse-to-Fine Sampling” strategy, as mentioned in the paper, if
    set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method` (`str`, *optional*, defaults to `butterworth`) — Must be one of `butterworth`,
    `ideal` or `gaussian` to use as the filtering method for the FreeInit low pass
    filter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`order` (`int`, *optional*, defaults to `4`) — Order of the filter used in
    `butterworth` method. Larger values lead to `ideal` method behaviour whereas lower
    values lead to `gaussian` method behaviour.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spatial_stop_frequency` (`float`, *optional*, defaults to `0.25`) — Normalized
    stop frequency for spatial dimensions. Must be between 0 to 1\. Referred to as
    `d_s` in the original implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temporal_stop_frequency` (`float`, *optional*, defaults to `0.25`) — Normalized
    stop frequency for temporal dimensions. Must be between 0 to 1\. Referred to as
    `d_t` in the original implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator`, *optional*, defaults to `0.25`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make FreeInit generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeInit mechanism as in [https://arxiv.org/abs/2312.07537](https://arxiv.org/abs/2312.07537).
  prefs: []
  type: TYPE_NORMAL
- en: This implementation has been adapted from the [official repository](https://github.com/TianxingWu/FreeInit).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L545)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L512)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L528)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L281)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: enable_freeu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disable_freeu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: enable_free_init
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disable_free_init
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: enable_vae_slicing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disable_vae_slicing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: enable_vae_tiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disable_vae_tiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PIAPipelineOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.pipelines.pia.PIAPipelineOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L197)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`frames` (`torch.Tensor`, `np.ndarray`, or List[PIL.Image.Image]) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Nested` list of length `batch_size` with denoised PIL image sequences of length
    `num_frames`, —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NumPy` array of shape `(batch_size, num_frames, channels, height, width, —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Torch` tensor of shape `(batch_size, num_frames, channels, height, width)`.
    —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for PIAPipeline.
  prefs: []
  type: TYPE_NORMAL
