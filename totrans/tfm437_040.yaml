- en: Image captioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾åƒå­—å¹•
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning is the task of predicting a caption for a given image. Common
    real world applications of it include aiding visually impaired people that can
    help them navigate through different situations. Therefore, image captioning helps
    to improve content accessibility for people by describing images to them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒå­—å¹•æ˜¯é¢„æµ‹ç»™å®šå›¾åƒçš„å­—å¹•çš„ä»»åŠ¡ã€‚å®ƒçš„å¸¸è§å®é™…åº”ç”¨åŒ…æ‹¬å¸®åŠ©è§†è§‰éšœç¢äººå£«ï¼Œå¸®åŠ©ä»–ä»¬åœ¨ä¸åŒæƒ…å†µä¸‹å¯¼èˆªã€‚å› æ­¤ï¼Œå›¾åƒå­—å¹•é€šè¿‡å‘äººä»¬æè¿°å›¾åƒæ¥å¸®åŠ©æé«˜äººä»¬å¯¹å†…å®¹çš„å¯è®¿é—®æ€§ã€‚
- en: 'This guide will show you how to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Fine-tune an image captioning model.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾®è°ƒå›¾åƒå­—å¹•æ¨¡å‹ã€‚
- en: Use the fine-tuned model for inference.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºæ¨ç†çš„å¾®è°ƒæ¨¡å‹ã€‚
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to log in to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to log in:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load the PokÃ©mon BLIP captions dataset
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½Pokemon BLIPå­—å¹•æ•°æ®é›†
- en: Use the ğŸ¤— Dataset library to load a dataset that consists of {image-caption}
    pairs. To create your own image captioning dataset in PyTorch, you can follow
    [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“åŠ è½½ä¸€ä¸ªç”±{å›¾åƒ-æ ‡é¢˜}å¯¹ç»„æˆçš„æ•°æ®é›†ã€‚è¦åœ¨PyTorchä¸­åˆ›å»ºè‡ªå·±çš„å›¾åƒå­—å¹•æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥å‚è€ƒ[æ­¤ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb)ã€‚
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The dataset has two features, `image` and `text`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œ`å›¾åƒ`å’Œ`æ–‡æœ¬`ã€‚
- en: Many image captioning datasets contain multiple captions per image. In those
    cases, a common strategy is to randomly sample a caption amongst the available
    ones during training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå›¾åƒå­—å¹•æ•°æ®é›†åŒ…å«æ¯ä¸ªå›¾åƒçš„å¤šä¸ªå­—å¹•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªå¸¸è§çš„ç­–ç•¥æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨å¯ç”¨çš„å­—å¹•ä¸­éšæœºæŠ½å–ä¸€ä¸ªå­—å¹•ã€‚
- en: 'Split the datasetâ€™s train split into a train and test set with the [~datasets.Dataset.train_test_split]
    method:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[~datasets.Dataset.train_test_split]æ–¹æ³•å°†æ•°æ®é›†çš„è®­ç»ƒé›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Letâ€™s visualize a couple of samples from the training set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬ã€‚
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Sample training images](../Images/465bc374aa742ae3c24da1893b1ca2b5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![æ ·æœ¬è®­ç»ƒå›¾åƒ](../Images/465bc374aa742ae3c24da1893b1ca2b5.png)'
- en: Preprocess the dataset
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ•°æ®é›†
- en: Since the dataset has two modalities (image and text), the pre-processing pipeline
    will preprocess images and the captions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ•°æ®é›†å…·æœ‰ä¸¤ç§æ¨¡æ€ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰ï¼Œé¢„å¤„ç†æµæ°´çº¿å°†é¢„å¤„ç†å›¾åƒå’Œæ ‡é¢˜ã€‚
- en: To do so, load the processor class associated with the model you are about to
    fine-tune.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼ŒåŠ è½½ä¸æ‚¨å³å°†å¾®è°ƒçš„æ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ç±»ã€‚
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The processor will internally pre-process the image (which includes resizing,
    and pixel scaling) and tokenize the caption.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†å™¨å°†åœ¨å†…éƒ¨é¢„å¤„ç†å›¾åƒï¼ˆåŒ…æ‹¬è°ƒæ•´å¤§å°å’Œåƒç´ ç¼©æ”¾ï¼‰å¹¶å¯¹æ ‡é¢˜è¿›è¡Œæ ‡è®°ã€‚
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the dataset ready, you can now set up the model for fine-tuning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†å‡†å¤‡å¥½çš„æ•°æ®é›†ï¼Œæ‚¨ç°åœ¨å¯ä»¥ä¸ºå¾®è°ƒè®¾ç½®æ¨¡å‹ã€‚
- en: Load a base model
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½åŸºç¡€æ¨¡å‹
- en: Load the [â€œmicrosoft/git-baseâ€](https://huggingface.co/microsoft/git-base) into
    a [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM)
    object.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[â€œmicrosoft/git-baseâ€](https://huggingface.co/microsoft/git-base)åŠ è½½åˆ°[`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM)å¯¹è±¡ä¸­ã€‚
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Evaluate
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: Image captioning models are typically evaluated with the [Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge)
    or [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer). For this
    guide, you will use the Word Error Rate (WER).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒå­—å¹•æ¨¡å‹é€šå¸¸ä½¿ç”¨[Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge)æˆ–[Word
    Error Rate](https://huggingface.co/spaces/evaluate-metric/wer)è¿›è¡Œè¯„ä¼°ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨Word
    Error Rate (WER)ã€‚
- en: We use the ğŸ¤— Evaluate library to do so. For potential limitations and other
    gotchas of the WER, refer to [this guide](https://huggingface.co/spaces/evaluate-metric/wer).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ğŸ¤—è¯„ä¼°åº“æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æœ‰å…³WERçš„æ½œåœ¨é™åˆ¶å’Œå…¶ä»–æ³¨æ„äº‹é¡¹ï¼Œè¯·å‚è€ƒ[æ­¤æŒ‡å—](https://huggingface.co/spaces/evaluate-metric/wer)ã€‚
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Train!
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒï¼
- en: Now, you are ready to start fine-tuning the model. You will use the ğŸ¤— [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    for this.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹å¾®è°ƒæ¨¡å‹äº†ã€‚æ‚¨å°†ä½¿ç”¨ğŸ¤—[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ¥è¿›è¡Œæ­¤æ“ä½œã€‚
- en: First, define the training arguments using [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä½¿ç”¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å®šä¹‰è®­ç»ƒå‚æ•°ã€‚
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then pass them along with the datasets and the model to ğŸ¤— Trainer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†å®ƒä»¬ä¸æ•°æ®é›†å’Œæ¨¡å‹ä¸€èµ·ä¼ é€’ç»™ğŸ¤— Trainerã€‚
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To start training, simply call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    on the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    object.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹è®­ç»ƒï¼Œåªéœ€åœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¯¹è±¡ä¸Šè°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)ã€‚
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You should see the training loss drop smoothly as training progresses.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥çœ‹åˆ°éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè®­ç»ƒæŸå¤±å¹³ç¨³ä¸‹é™ã€‚
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Inference
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Take a sample image from `test_ds` to test the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä»`test_ds`ä¸­å–ä¸€ä¸ªæ ·æœ¬å›¾åƒæ¥æµ‹è¯•æ¨¡å‹ã€‚
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Test image](../Images/8106832a19bcb32bb4f42e5d637f0640.png)Prepare image
    for the model.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![æµ‹è¯•å›¾ç‰‡](../Images/8106832a19bcb32bb4f42e5d637f0640.png)ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚'
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Call `generate` and decode the predictions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨`generate`å¹¶è§£ç é¢„æµ‹ã€‚
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Looks like the fine-tuned model generated a pretty good caption!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥å¾®è°ƒçš„æ¨¡å‹ç”Ÿæˆäº†ä¸€ä¸ªç›¸å½“ä¸é”™çš„å­—å¹•ï¼
