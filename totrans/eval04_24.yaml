- en: Evaluator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/package_reference/evaluator_classes](https://huggingface.co/docs/evaluate/package_reference/evaluator_classes)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The evaluator classes for automatic evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main entry point for using the evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `evaluate.evaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/__init__.py#L108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`task` (`str`) — The task defining which evaluator will be returned. Currently
    accepted tasks are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"image-classification"`: will return a [ImageClassificationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"question-answering"`: will return a [QuestionAnsweringEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-classification"` (alias `"sentiment-analysis"` available): will return
    a [TextClassificationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"token-classification"`: will return a [TokenClassificationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TokenClassificationEvaluator).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Evaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.Evaluator)'
  prefs: []
  type: TYPE_NORMAL
- en: An evaluator suitable for the task.
  prefs: []
  type: TYPE_NORMAL
- en: Utility factory method to build an [Evaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.Evaluator).
    Evaluators encapsulate a task and a default metric name. They leverage `pipeline`
    functionalify from `transformers` to simplify the evaluation of multiple combinations
    of models, datasets and metrics for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The base class for all evaluator classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.Evaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Evaluator class is the class from which all evaluators inherit. Refer to
    this class for methods shared across different evaluators. Base class implementing
    evaluator operations.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `check_required_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L295)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`) — Specifies the dataset we will run evaluation
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns_names` (`List[str]`) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`List` of column names to check in the dataset. The keys are the arguments
    to the compute() method, —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`while` the values are the column names to check. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the columns required for the evaluation are present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_metric`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L457)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Compute and return metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_dataset_split`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L312)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (`str`) — Name of dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`) — Name of config for datasets with multiple configurations
    (e.g. ‘glue/cola’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to None) — Split to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`split`'
  prefs: []
  type: TYPE_NORMAL
- en: '`str` containing which split to use'
  prefs: []
  type: TYPE_NORMAL
- en: Infers which split to use if None is given.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_data`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L329)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (`Dataset` or `str`, defaults to None) — Specifies the dataset we will
    run evaluation on. If it is of'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type` `str`, we treat it as the dataset name, and load it. Otherwise we assume
    it represents a pre-loaded dataset. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to None) — Specifies dataset subset to be passed
    to `name` in `load_dataset`. To be used with datasets with several configurations
    (e.g. glue/sst2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to None) — User-defined dataset split by name (e.g.
    train, validation, test). Supports slice-split (test[:n]). If not defined and
    data is a `str` type, will automatically select the best one via `choose_split()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: data (`Dataset`)
  prefs: []
  type: TYPE_NORMAL
- en: Loaded dataset which will be used for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Load dataset with given subset and split.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `predictions_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L211)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A core method of the `Evaluator` class, which processes the pipeline outputs
    for compatibility with the metric.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_data`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L356)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (`Dataset`) — Specifies the dataset we will run evaluation on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_column` (`str`, defaults to `"text"`) — the name of the column containing
    the text feature in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_column` (`str`, defaults to `"label"`) — the name of the column containing
    the labels in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict`'
  prefs: []
  type: TYPE_NORMAL
- en: 'metric inputs. `list`: pipeline inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare data.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_metric`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L427)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare metric.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_pipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L375)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defaults` to `None`) — If the argument in not specified, we initialize the
    default pipeline for the task. If the argument is of the type `str` or is a model
    instance, we use it to initialize a new `Pipeline` with the given model. Otherwise
    we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preprocessor` (`PreTrainedTokenizerBase` or `FeatureExtractionMixin`, *optional*,
    defaults to `None`) — Argument can be used to overwrite a default preprocessor
    if `model_or_pipeline` represents a model for which we build a pipeline. If `model_or_pipeline`
    is `None` or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The task specific evaluators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ImageClassificationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.ImageClassificationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/image_classification.py#L45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Image classification evaluator. This image classification evaluator can currently
    be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `image-classification`. Methods in this class assume
    a data format compatible with the `ImageClassificationPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/image_classification.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: QuestionAnsweringEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.QuestionAnsweringEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/question_answering.py#L74)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Question answering evaluator. This evaluator handles [**extractive** question
    answering](https://huggingface.co/docs/transformers/task_summary#extractive-question-answering),
    where the answer to the question is extracted from a context.
  prefs: []
  type: TYPE_NORMAL
- en: This question answering evaluator can currently be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `question-answering`.
  prefs: []
  type: TYPE_NORMAL
- en: Methods in this class assume a data format compatible with the [`QuestionAnsweringPipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/question_answering.py#L143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Datasets where the answer may be missing in the context are supported, for example
    SQuAD v2 dataset. In this case, it is safer to pass `squad_v2_format=True` to
    the compute() call.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: TextClassificationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.TextClassificationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text_classification.py#L47)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Text classification evaluator. This text classification evaluator can currently
    be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `text-classification` or with a `"sentiment-analysis"`
    alias. Methods in this class assume a data format compatible with the `TextClassificationPipeline`
    - a single textual feature as input and a categorical label as output.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text_classification.py#L85)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: TokenClassificationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.TokenClassificationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/token_classification.py#L86)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Token classification evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: This token classification evaluator can currently be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `token-classification`.
  prefs: []
  type: TYPE_NORMAL
- en: Methods in this class assume a data format compatible with the `TokenClassificationPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/token_classification.py#L209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset input and label columns are expected to be formatted as a list of
    words and a list of labels respectively, following [conll2003 dataset](https://huggingface.co/datasets/conll2003).
    Datasets whose inputs are single strings, and labels are a list of offset are
    not supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following dataset format is accepted by the evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following dataset format is **not** accepted by the evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: TextGenerationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.TextGenerationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text_generation.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Text generation evaluator. This Text generation evaluator can currently be loaded
    from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `text-generation`. Methods in this class assume a
    data format compatible with the `TextGenerationPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/base.py#L218)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Text2TextGenerationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.Text2TextGenerationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text2text_generation.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Text2Text generation evaluator. This Text2Text generation evaluator can currently
    be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `text2text-generation`. Methods in this class assume
    a data format compatible with the `Text2TextGenerationPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text2text_generation.py#L52)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_column` (`str`, defaults to `"text"`) — the name of the column containing
    the input text in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_column` (`str`, defaults to `"label"`) — the name of the column containing
    the labels in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_kwargs` (`Dict`, *optional*, defaults to `None`) — The generation
    kwargs are passed to the pipeline and set the text generation strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: SummarizationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.SummarizationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text2text_generation.py#L113)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Text summarization evaluator. This text summarization evaluator can currently
    be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `summarization`. Methods in this class assume a data
    format compatible with the [SummarizationEvaluator](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.SummarizationEvaluator).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text2text_generation.py#L127)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_column` (`str`, defaults to `"text"`) — the name of the column containing
    the input text in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_column` (`str`, defaults to `"label"`) — the name of the column containing
    the labels in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_kwargs` (`Dict`, *optional*, defaults to `None`) — The generation
    kwargs are passed to the pipeline and set the text generation strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: TranslationEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.TranslationEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text2text_generation.py#L184)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Translation evaluator. This translation generation evaluator can currently be
    loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `translation`. Methods in this class assume a data
    format compatible with the `TranslationPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/text2text_generation.py#L198)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_column` (`str`, defaults to `"text"`) — the name of the column containing
    the input text in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_column` (`str`, defaults to `"label"`) — the name of the column containing
    the labels in the dataset specified by `data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_kwargs` (`Dict`, *optional*, defaults to `None`) — The generation
    kwargs are passed to the pipeline and set the text generation strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: AutomaticSpeechRecognitionEvaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class evaluate.AutomaticSpeechRecognitionEvaluator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/automatic_speech_recognition.py#L43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Automatic speech recognition evaluator. This automatic speech recognition evaluator
    can currently be loaded from [evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    using the default task name `automatic-speech-recognition`. Methods in this class
    assume a data format compatible with the `AutomaticSpeechRecognitionPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/automatic_speech_recognition.py#L59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_pipeline` (`str` or `Pipeline` or `Callable` or `PreTrainedModel`
    or `TFPreTrainedModel`, defaults to `None`) — If the argument in not specified,
    we initialize the default pipeline for the task (in this case `text-classification`
    or its alias - `sentiment-analysis`). If the argument is of the type `str` or
    is a model instance, we use it to initialize a new `Pipeline` with the given model.
    Otherwise we assume the argument specifies a pre-initialized pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`str` or `Dataset`, defaults to `None`) — Specifies the dataset we
    will run evaluation on. If it is of type `str`, we treat it as the dataset name,
    and load it. Otherwise we assume it represents a pre-loaded dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` (`str`, defaults to `None`) — Defines which dataset subset to load.
    If `None` is passed the default subset is loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, defaults to `None`) — Defines which dataset split to load.
    If `None` is passed, infers based on the `choose_split` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric` (`str` or `EvaluationModule`, defaults to `None`) — Specifies the
    metric we use in evaluator. If it is of type `str`, we treat it as the metric
    name, and load it. Otherwise we assume it represents a pre-loaded metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`)
    — Argument can be used to overwrite a default tokenizer if `model_or_pipeline`
    represents a model for which we build a pipeline. If `model_or_pipeline` is `None`
    or a pre-initialized pipeline, we ignore this argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy` (`Literal["simple", "bootstrap"]`, defaults to “simple”) — specifies
    the evaluation strategy. Possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"simple"` - we evaluate the metric and return the scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"bootstrap"` - on top of computing the metric scores, we calculate the confidence
    interval for each of the returned metric keys, using `scipy`’s `bootstrap` method
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (`float`, defaults to `0.95`) — The `confidence_level` value
    passed to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resamples` (`int`, defaults to `9999`) — The `n_resamples` value passed
    to `bootstrap` if `"bootstrap"` strategy is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, defaults to `None`) — Device ordinal for CPU/GPU support of
    the pipeline. Setting this to -1 will leverage CPU, a positive integer will run
    the model on the associated CUDA device ID. If `None` is provided it will be inferred
    and CUDA:0 used if available, CPU otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state` (`int`, *optional*, defaults to `None`) — The `random_state`
    value passed to `bootstrap` if `"bootstrap"` strategy is chosen. Useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metric for a given pipeline and dataset combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
