- en: Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/streaming](https://huggingface.co/docs/text-generation-inference/conceptual/streaming)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: What is Streaming?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Token streaming is the mode in which the server returns the tokens one by one
    as the model generates them. This enables showing progressive generations to the
    user rather than waiting for the whole generation. Streaming is an essential aspect
    of the end-user experience as it reduces latency, one of the most critical aspects
    of a smooth experience.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da4a03ca9d5a29fb32448f9771ddb359.png) ![](../Images/e9e351d288d9a7be620080222481534f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With token streaming, the server can start returning the tokens one by one
    before having to generate the whole response. Users can have a sense of the generation’s
    quality earlier than the end of the generation. This has different positive effects:'
  prefs: []
  type: TYPE_NORMAL
- en: Users can get results orders of magnitude earlier for extremely long queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing something in progress allows users to stop the generation if it’s not
    going in the direction they expect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceived latency is lower when results are shown in the early stages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When used in conversational UIs, the experience feels more natural.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a system can generate 100 tokens per second. If the system generates
    1000 tokens, with the non-streaming setup, users need to wait 10 seconds to get
    results. On the other hand, with the streaming setup, users get initial results
    immediately, and although end-to-end latency will be the same, they can see half
    of the generation after five seconds. Below you can see an interactive demo that
    shows non-streaming vs streaming side-by-side. Click **generate** below.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://osanseviero-streaming-vs-non-streaming.hf.space?__theme=light](https://osanseviero-streaming-vs-non-streaming.hf.space?__theme=light)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://osanseviero-streaming-vs-non-streaming.hf.space?__theme=dark](https://osanseviero-streaming-vs-non-streaming.hf.space?__theme=dark)'
  prefs: []
  type: TYPE_NORMAL
- en: How to use Streaming?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Streaming with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To stream tokens with `InferenceClient`, simply pass `stream=True` and iterate
    over the response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you want additional details, you can add `details=True`. In this case, you
    get a `TextGenerationStreamResponse` which contains additional information such
    as the probabilities and the tokens. For the final response in the stream, it
    also returns the full generated text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `huggingface_hub` library also comes with an `AsyncInferenceClient` in case
    you need to handle the requests concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Streaming with cURL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To use the `generate_stream` endpoint with curl, you can add the `-N` flag,
    which disables curl default buffering and shows data as it arrives from the server
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Streaming with JavaScript
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we need to install the `@huggingface/inference` library. `npm install
    @huggingface/inference`
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using the free Inference API, you can use `HfInference`. If you’re
    using inference endpoints, you can use `HfInferenceEndpoint`. Let’s
  prefs: []
  type: TYPE_NORMAL
- en: We can create a `HfInferenceEndpoint` providing our endpoint URL and credential.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How does Streaming work under the hood?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under the hood, TGI uses Server-Sent Events (SSE). In an SSE Setup, a client
    sends a request with the data, opening an HTTP connection and subscribing to updates.
    Afterward, the server sends data to the client. There is no need for further requests;
    the server will keep sending the data. SSEs are unidirectional, meaning the client
    does not send other requests to the server. SSE sends data over HTTP, making it
    easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'SSEs are different than:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Polling: where the client keeps calling the server to get data. This means
    that the server might return empty responses and cause overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Webhooks: where there is a bi-directional connection. The server can send information
    to the client, but the client can also send data to the server after the first
    request. Webhooks are more complex to operate as they don’t only use HTTP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are too many requests at the same time, TGI returns an HTTP Error with
    an `overloaded` error type (`huggingface_hub` returns `OverloadedError`). This
    allows the client to manage the overloaded server (e.g., it could display a busy
    error to the user or retry with a new request). To configure the maximum number
    of concurrent requests, you can specify `--max_concurrent_requests`, allowing
    clients to handle backpressure.
  prefs: []
  type: TYPE_NORMAL
