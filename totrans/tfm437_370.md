# TrOCR

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/trocr`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trocr)

## æ¦‚è¿°

TrOCR æ¨¡å‹æ˜¯ç”± Minghao Liã€Tengchao Lvã€Lei Cuiã€Yijuan Luã€Dinei Florencioã€Cha Zhangã€Zhoujun Liã€Furu Wei åœ¨[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)ä¸­æå‡ºçš„ã€‚TrOCR åŒ…æ‹¬ä¸€ä¸ªå›¾åƒ Transformer ç¼–ç å™¨å’Œä¸€ä¸ªè‡ªå›å½’æ–‡æœ¬ Transformer è§£ç å™¨ï¼Œç”¨äºæ‰§è¡Œ[å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰](https://en.wikipedia.org/wiki/Optical_character_recognition)ã€‚

è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æ–‡æœ¬è¯†åˆ«æ˜¯æ–‡æ¡£æ•°å­—åŒ–çš„ä¸€ä¸ªé•¿æœŸç ”ç©¶é—®é¢˜ã€‚ç°æœ‰çš„æ–‡æœ¬è¯†åˆ«æ–¹æ³•é€šå¸¸åŸºäº CNN è¿›è¡Œå›¾åƒç†è§£å’ŒåŸºäº RNN è¿›è¡Œå­—ç¬¦çº§æ–‡æœ¬ç”Ÿæˆã€‚æ­¤å¤–ï¼Œé€šå¸¸éœ€è¦å¦ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä½œä¸ºåå¤„ç†æ­¥éª¤æ¥æé«˜æ•´ä½“å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ–‡æœ¬è¯†åˆ«æ–¹æ³•ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒ Transformer å’Œæ–‡æœ¬ Transformer æ¨¡å‹ï¼Œå³ TrOCRï¼Œå®ƒåˆ©ç”¨ Transformer æ¶æ„è¿›è¡Œå›¾åƒç†è§£å’Œè¯ç‰‡çº§æ–‡æœ¬ç”Ÿæˆã€‚TrOCR æ¨¡å‹ç®€å•è€Œæœ‰æ•ˆï¼Œå¯ä»¥ä½¿ç”¨å¤§è§„æ¨¡åˆæˆæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨äººå·¥æ ‡è®°çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼ŒTrOCR æ¨¡å‹åœ¨å°åˆ·å’Œæ‰‹å†™æ–‡æœ¬è¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚*

![drawing](img/38c5ef57a7dc1784f1413dccc805e29d.png) TrOCR æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2109.10282)ã€‚

è¯·å‚è€ƒ`VisionEncoderDecoder`ç±»å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   å¼€å§‹ä½¿ç”¨ TrOCR çš„æœ€å¿«æ–¹æ³•æ˜¯æŸ¥çœ‹[æ•™ç¨‹ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/TrOCR)ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨æ¨ç†æ—¶ä½¿ç”¨æ¨¡å‹ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚

+   TrOCR åœ¨è¢«å¾®è°ƒåˆ°ä¸‹æ¸¸æ•°æ®é›†ä¹‹å‰ç»è¿‡ 2 ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒã€‚å®ƒåœ¨å°åˆ·ï¼ˆä¾‹å¦‚[SROIE æ•°æ®é›†](https://paperswithcode.com/dataset/sroie)ï¼‰å’Œæ‰‹å†™ï¼ˆä¾‹å¦‚[IAM æ‰‹å†™æ•°æ®é›†](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database%3E)ï¼‰æ–‡æœ¬è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å®˜æ–¹æ¨¡å‹](https://huggingface.co/models?other=trocr%3E)ã€‚

+   TrOCR å§‹ç»ˆåœ¨ VisionEncoderDecoder æ¡†æ¶å†…ä½¿ç”¨ã€‚

## èµ„æº

ä¸€ä¸ªå®˜æ–¹çš„ Hugging Face å’Œç¤¾åŒºèµ„æºåˆ—è¡¨ï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ TrOCRã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

æ–‡æœ¬åˆ†ç±»

+   ä¸€ä¸ªå…³äº[åŠ é€Ÿæ–‡æ¡£ AI](https://huggingface.co/blog/document-ai)ä¸ TrOCR çš„åšå®¢æ–‡ç« ã€‚

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ TrOCR è¿›è¡Œ[æ–‡æ¡£ AI](https://github.com/philschmid/document-ai-transformers)çš„åšå®¢æ–‡ç« ã€‚

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ Seq2SeqTrainer åœ¨ IAM æ‰‹å†™æ•°æ®åº“ä¸Š[å¾®è°ƒ TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb)çš„ç¬”è®°æœ¬ã€‚

+   ä¸€ä¸ªå…³äº[inference with TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb)å’Œ Gradio æ¼”ç¤ºçš„ç¬”è®°æœ¬ã€‚

+   ä¸€ä¸ªå…³äºåœ¨ IAM æ‰‹å†™æ•°æ®åº“ä¸Š[å¾®è°ƒ TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb)ä½¿ç”¨åŸç”Ÿ PyTorch çš„ç¬”è®°æœ¬ã€‚

+   å…³äº[åœ¨ IAM æµ‹è¯•é›†ä¸Šè¯„ä¼° TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb)çš„ç¬”è®°æœ¬ã€‚

æ–‡æœ¬ç”Ÿæˆ

+   [è¯­è¨€å»ºæ¨¡](https://huggingface.co/docs/transformers/tasks/language_modeling)ä»»åŠ¡æŒ‡å—ã€‚

âš¡ï¸ æ¨ç†

+   å…³äº[TrOCR æ‰‹å†™å­—ç¬¦è¯†åˆ«](https://huggingface.co/spaces/nielsr/TrOCR-handwritten)çš„äº¤äº’å¼æ¼”ç¤ºã€‚

## æ¨ç†

TrOCR çš„`VisionEncoderDecoder`æ¨¡å‹æ¥å—å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨ generate()æ¥è‡ªå›å½’åœ°ç”Ÿæˆç»™å®šè¾“å…¥å›¾åƒçš„æ–‡æœ¬ã€‚

[`ViTImageProcessor`/`DeiTImageProcessor`]ç±»è´Ÿè´£é¢„å¤„ç†è¾“å…¥å›¾åƒï¼Œ[`RobertaTokenizer`/`XLMRobertaTokenizer`]è§£ç ç”Ÿæˆçš„ç›®æ ‡ä»¤ç‰Œä¸ºç›®æ ‡å­—ç¬¦ä¸²ã€‚TrOCRProcessor å°†[`ViTImageProcessor`/`DeiTImageProcessor`]å’Œ[`RobertaTokenizer`/`XLMRobertaTokenizer`]å°è£…æˆå•ä¸ªå®ä¾‹ï¼Œç”¨äºæå–è¾“å…¥ç‰¹å¾å’Œè§£ç é¢„æµ‹çš„ä»¤ç‰Œ IDã€‚

+   é€æ­¥å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰

```py
>>> from transformers import TrOCRProcessor, VisionEncoderDecoderModel
>>> import requests
>>> from PIL import Image

>>> processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
>>> model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

>>> # load image from the IAM dataset
>>> url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

>>> pixel_values = processor(image, return_tensors="pt").pixel_values
>>> generated_ids = model.generate(pixel_values)

>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

æŸ¥çœ‹[æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models?filter=trocr)ä»¥æŸ¥æ‰¾ TrOCR æ£€æŸ¥ç‚¹ã€‚

## TrOCRConfig

### `class transformers.TrOCRConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/configuration_trocr.py#L31)

```py
( vocab_size = 50265 d_model = 1024 decoder_layers = 12 decoder_attention_heads = 16 decoder_ffn_dim = 4096 activation_function = 'gelu' max_position_embeddings = 512 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 decoder_start_token_id = 2 init_std = 0.02 decoder_layerdrop = 0.0 use_cache = True scale_embedding = False use_learned_position_embeddings = True layernorm_embedding = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 50265) â€” TrOCR æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ TrOCRForCausalLM æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚

+   `d_model` (`int`, *optional*, defaults to 1024) â€” å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `decoder_layers` (`int`, *optional*, defaults to 12) â€” è§£ç å™¨å±‚æ•°ã€‚

+   `decoder_attention_heads` (`int`, *optional*, defaults to 16) â€” Transformer è§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 4096) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” æ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ 512ã€1024 æˆ– 2048ï¼‰ã€‚

+   `dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ä¾‹ã€‚

+   `activation_dropout` (`float`, *optional*, defaults to 0.0) â€” å…¨è¿æ¥å±‚å†…æ¿€æ´»çš„ä¸¢å¼ƒæ¯”ä¾‹ã€‚

+   `init_std` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `decoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” è§£ç å™¨çš„ LayerDrop æ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… LayerDrop è®ºæ–‡)ã€‚

+   `use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

+   `scale_embedding` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å°†è¯åµŒå…¥æŒ‰ sqrt(d_model)è¿›è¡Œç¼©æ”¾ã€‚

+   `use_learned_position_embeddings` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä½¿ç”¨å­¦ä¹ çš„ä½ç½®åµŒå…¥ã€‚å¦‚æœä¸æ˜¯ï¼Œåˆ™å°†ä½¿ç”¨æ­£å¼¦ä½ç½®åµŒå…¥ã€‚

+   `layernorm_embedding`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€”æ˜¯å¦åœ¨å•è¯+ä½ç½®åµŒå…¥åä½¿ç”¨ layernormã€‚

è¿™æ˜¯é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ TrOCRForCausalLM çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– TrOCR æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº TrOCR [microsoft/trocr-base-handwritten](https://huggingface.co/microsoft/trocr-base-handwritten)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import TrOCRConfig, TrOCRForCausalLM

>>> # Initializing a TrOCR-base style configuration
>>> configuration = TrOCRConfig()

>>> # Initializing a model (with random weights) from the TrOCR-base style configuration
>>> model = TrOCRForCausalLM(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## TrOCRProcessor

### `class transformers.TrOCRProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/processing_trocr.py#L24)

```py
( image_processor = None tokenizer = None **kwargs )
```

å‚æ•°

+   `image_processor`ï¼ˆ[`ViTImageProcessor`/`DeiTImageProcessor`]ï¼Œ*å¯é€‰*ï¼‰â€”[`ViTImageProcessor`/`DeiTImageProcessor`]çš„å®ä¾‹ã€‚å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer`ï¼ˆ[`RobertaTokenizer`/`XLMRobertaTokenizer`]ï¼Œ*å¯é€‰*ï¼‰â€”[`RobertaTokenizer`/`XLMRobertaTokenizer`]çš„å®ä¾‹ã€‚æ ‡è®°å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª TrOCR å¤„ç†å™¨ï¼Œå°†è§†è§‰å›¾åƒå¤„ç†å™¨å’Œ TrOCR æ ‡è®°å™¨å°è£…åˆ°å•ä¸ªå¤„ç†å™¨ä¸­ã€‚

TrOCRProcessor æä¾›äº†æ‰€æœ‰[`ViTImageProcessor`/`DeiTImageProcessor`]å’Œ[`RobertaTokenizer`/`XLMRobertaTokenizer`]çš„åŠŸèƒ½ã€‚æŸ¥çœ‹**call**()å’Œ decode()ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/processing_trocr.py#L63)

```py
( *args **kwargs )
```

åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ° AutoImageProcessor çš„`__call__()`å¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡`as_target_processor()`ä¸­ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œåˆ™å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ° TrOCRTokenizer çš„`~TrOCRTokenizer.__call__`ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `from_pretrained`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)

```py
( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€”è¿™å¯ä»¥æ˜¯ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„*æ¨¡å‹ ID*ï¼Œæ‰˜ç®¡åœ¨ huggingface.co ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚æœ‰æ•ˆçš„æ¨¡å‹ ID å¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–å‘½åç©ºé—´åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚

    +   ä¸€ä¸ª*ç›®å½•*çš„è·¯å¾„ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨ save_pretrained()æ–¹æ³•ä¿å­˜çš„ç‰¹å¾æå–å™¨æ–‡ä»¶ï¼Œä¾‹å¦‚`./my_model_directory/`ã€‚

    +   ä¸€ä¸ªä¿å­˜çš„ç‰¹å¾æå–å™¨ JSON *æ–‡ä»¶*çš„è·¯å¾„æˆ– URLï¼Œä¾‹å¦‚`./my_model_directory/preprocessor_config.json`ã€‚**kwargs â€”ä¼ é€’ç»™ from_pretrained()å’Œ`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

å®ä¾‹åŒ–ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ã€‚

è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨ç‰¹å¾æå–å™¨ from_pretrained()ã€å›¾åƒå¤„ç†å™¨ ImageProcessingMixin å’Œåˆ†è¯å™¨`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`æ–¹æ³•ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `save_pretrained`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)

```py
( save_directory push_to_hub: bool = False **kwargs )
```

å‚æ•°

+   `save_directory` (`str`æˆ–`os.PathLike`) â€” ç‰¹å¾æå–å™¨ JSON æ–‡ä»¶å’Œåˆ†è¯å™¨æ–‡ä»¶å°†ä¿å­˜åœ¨çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨å°†è¢«åˆ›å»ºï¼‰ã€‚

+   `push_to_hub` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨ä¿å­˜åå°†æ¨¡å‹æ¨é€åˆ° Hugging Face æ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨å‘½åç©ºé—´ä¸­çš„`save_directory`åç§°ï¼‰ã€‚

+   `kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ä¼ é€’ç»™ push_to_hub()æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

å°†æ­¤å¤„ç†å™¨çš„å±æ€§ï¼ˆç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨ç­‰ï¼‰ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ from_pretrained()æ–¹æ³•é‡æ–°åŠ è½½ã€‚

è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨ save_pretrained()å’Œ save_pretrained()ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/processing_trocr.py#L96)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ TrOCRTokenizer çš„ batch_decode()ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/processing_trocr.py#L103)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ TrOCRTokenizer çš„ decode()ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

## TrOCRForCausalLM

### `class transformers.TrOCRForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/modeling_trocr.py#L724)

```py
( config )
```

å‚æ•°

+   `config` (TrOCRConfig) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„ TrOCR è§£ç å™¨ã€‚å¯ç”¨ä½œ EncoderDecoderModel å’Œ`VisionEncoderDecoder`çš„è§£ç å™¨éƒ¨åˆ†ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/trocr/modeling_trocr.py#L762)

```py
( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºæ ‡è®°æ˜¯`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºæ ‡è®°æ˜¯`è¢«æ©ç `ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚

+   `encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨æ­¤æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æ˜¯`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨æ˜¯`è¢«æ©ç `ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æ˜¯`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨æ˜¯`è¢«æ©ç `ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼‰å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚å½“æ¨¡å‹ç”¨ä½œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„è§£ç å™¨æ—¶ï¼Œè¿™ä¸¤ä¸ªé¢å¤–çš„å¼ é‡æ˜¯å¿…éœ€çš„ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`æˆ–-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚å°†ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æ ‡è®°ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.CausalLMOutputWithCrossAttentions æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆTrOCRConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º + æ¯å±‚è¾“å‡ºçš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`torch.FloatTensor`å…ƒç»„çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder = True`æ—¶ç›¸å…³ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import (
...     TrOCRConfig,
...     TrOCRProcessor,
...     TrOCRForCausalLM,
...     ViTConfig,
...     ViTModel,
...     VisionEncoderDecoderModel,
... )
>>> import requests
>>> from PIL import Image

>>> # TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel
>>> # init vision2text model with random weights
>>> encoder = ViTModel(ViTConfig())
>>> decoder = TrOCRForCausalLM(TrOCRConfig())
>>> model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)

>>> # If you want to start from the pretrained model, load the checkpoint with `VisionEncoderDecoderModel`
>>> processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
>>> model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

>>> # load image from the IAM dataset
>>> url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
>>> pixel_values = processor(image, return_tensors="pt").pixel_values
>>> text = "industry, ' Mr. Brown commented icily. ' Let us have a"

>>> # training
>>> model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
>>> model.config.pad_token_id = processor.tokenizer.pad_token_id
>>> model.config.vocab_size = model.config.decoder.vocab_size

>>> labels = processor.tokenizer(text, return_tensors="pt").input_ids
>>> outputs = model(pixel_values, labels=labels)
>>> loss = outputs.loss
>>> round(loss.item(), 2)
5.30

>>> # inference
>>> generated_ids = model.generate(pixel_values)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> generated_text
'industry, " Mr. Brown commented icily. " Let us have a'
```
