# 条件 DETR

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/conditional_detr`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/conditional_detr)

## 概述

条件 DETR 模型是由孟德普、陈晓康、范泽佳、曾刚、李厚强、袁宇辉、孙磊、王京东在[用于快速训练收敛的条件 DETR](https://arxiv.org/abs/2108.06152)中提出的。条件 DETR 提出了一种用于快速 DETR 训练的条件交叉注意力机制。条件 DETR 的收敛速度比 DETR 快 6.7 倍至 10 倍。

论文摘要如下：

*最近开发的 DETR 方法将 Transformer 编码器和解码器架构应用于目标检测，并取得了令人满意的性能。在本文中，我们处理了关键问题，即训练收敛速度慢，并提出了一种用于快速 DETR 训练的条件交叉注意力机制。我们的方法受到了 DETR 中的交叉注意力高度依赖内容嵌入以定位四个极点并预测框的启发，这增加了对高质量内容嵌入的需求，从而增加了训练难度。我们的方法，称为条件 DETR，从解码器嵌入中学习一个条件空间查询，用于解码器多头交叉注意力。好处在于通过条件空间查询，每个交叉注意力头都能关注包含不同区域的带，例如一个对象极点或对象框内的区域。这缩小了用于定位不同区域进行对象分类和框回归的空间范围，从而减轻了对内容嵌入的依赖并简化了训练。实证结果表明，对于骨干网络 R50 和 R101，条件 DETR 的收敛速度提高了 6.7 倍，对于更强大的骨干网络 DC5-R50 和 DC5-R101，提高了 10 倍。代码可在[`github.com/Atten4Vis/ConditionalDETR`](https://github.com/Atten4Vis/ConditionalDETR)找到。*

![图示](img/c6d995d857b4d1e1828dfe68ed28fb0d.png) 条件 DETR 相对于原始 DETR 显示出更快的收敛速度。摘自[原始论文](https://arxiv.org/abs/2108.06152)。

此模型由[DepuMeng](https://huggingface.co/DepuMeng)贡献。原始代码可在[此处](https://github.com/Atten4Vis/ConditionalDETR)找到。

## 资源

+   目标检测任务指南

## ConditionalDetrConfig

### `class transformers.ConditionalDetrConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/configuration_conditional_detr.py#L36)

```py
( use_timm_backbone = True backbone_config = None num_channels = 3 num_queries = 300 encoder_layers = 6 encoder_ffn_dim = 2048 encoder_attention_heads = 8 decoder_layers = 6 decoder_ffn_dim = 2048 decoder_attention_heads = 8 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 is_encoder_decoder = True activation_function = 'relu' d_model = 256 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 init_xavier_std = 1.0 auxiliary_loss = False position_embedding_type = 'sine' backbone = 'resnet50' use_pretrained_backbone = True dilation = False class_cost = 2 bbox_cost = 5 giou_cost = 2 mask_loss_coefficient = 1 dice_loss_coefficient = 1 cls_loss_coefficient = 2 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 focal_alpha = 0.25 **kwargs )
```

参数

+   `use_timm_backbone` (`bool`, *可选*, 默认为`True`) — 是否使用`timm`库作为骨干。如果设置为`False`，将使用`AutoBackbone` API。

+   `backbone_config` (`PretrainedConfig`或`dict`, *可选*) — 骨干模型的配置。仅在`use_timm_backbone`设置为`False`时使用，此时默认为`ResNetConfig()`。

+   `num_channels` (`int`, *可选*, 默认为 3) — 输入通道的数量。

+   `num_queries` (`int`, *可选*, 默认为 100) — 对象查询的数量，即检测槽位。这是 ConditionalDetrModel 在单个图像中可以检测到的对象的最大数量。对于 COCO 数据集，我们建议使用 100 个查询。

+   `d_model` (`int`, *可选*, 默认为 256) — 层的维度。

+   `encoder_layers` (`int`, *可选*, 默认为 6) — 编码器层数。

+   `decoder_layers` (`int`, *可选*, 默认为 6) — 解码器层数。

+   `encoder_attention_heads` (`int`, *可选*, 默认为 8) — Transformer 编码器中每个注意力层的注意力头数。

+   `decoder_attention_heads` (`int`, *可选*, 默认为 8) — Transformer 解码器中每个注意力层的注意力头数。

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 2048) — 解码器中“中间”（通常称为前馈）层的维度。

+   `encoder_ffn_dim` (`int`, *optional*, defaults to 2048) — 解码器中“中间”（通常称为前馈）层的维度。

+   `activation_function` (`str` or `function`, *optional*, defaults to `"relu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `dropout` (`float`, *optional*, defaults to 0.1) — 嵌入、编码器和池化器中所有全连接层的丢弃概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢弃比率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.0) — 全连接层内激活的丢弃比率。

+   `init_std` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `init_xavier_std` (`float`, *optional*, defaults to 1) — 用于 HM Attention map 模块中 Xavier 初始化增益的缩放因子。

+   `encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop 论文)。

+   `decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 解码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop 论文)。

+   `auxiliary_loss` (`bool`, *optional*, defaults to `False`) — 是否使用辅助解码损失（每个解码器层的损失）。

+   `position_embedding_type` (`str`, *optional*, defaults to `"sine"`) — 在图像特征之上使用的位置嵌入的类型。其中之一是`"sine"`或`"learned"`。

+   `backbone` (`str`, *optional*, defaults to `"resnet50"`) — 在`use_timm_backbone` = `True`时要使用的卷积主干的名称。支持 timm 包中的任何卷积主干。有关所有可用模型的列表，请参见[此页面](https://rwightman.github.io/pytorch-image-models/#load-a-pretrained-model)。

+   `use_pretrained_backbone` (`bool`, *optional*, defaults to `True`) — 是否对主干使用预训练权重。仅在`use_timm_backbone` = `True`时支持。

+   `dilation` (`bool`, *optional*, defaults to `False`) — 是否在最后的卷积块（DC5）中用膨胀替换步幅。仅在`use_timm_backbone` = `True`时支持。

+   `class_cost` (`float`, *optional*, defaults to 1) — 匈牙利匹配成本中分类错误的相对权重。

+   `bbox_cost` (`float`, *optional*, defaults to 5) — 匈牙利匹配成本中边界框坐标的 L1 误差的相对权重。

+   `giou_cost` (`float`, *optional*, defaults to 2) — 匈牙利匹配成本中边界框的广义 IoU 损失的相对权重。

+   `mask_loss_coefficient` (`float`, *optional*, defaults to 1) — 全景分割损失中 Focal 损失的相对权重。

+   `dice_loss_coefficient` (`float`, *optional*, defaults to 1) — 全景分割损失中 DICE/F-1 损失的相对权重。

+   `bbox_loss_coefficient` (`float`, *optional*, defaults to 5) — 目标检测损失中 L1 边界框损失的相对权重。

+   `giou_loss_coefficient` (`float`, *optional*, defaults to 2) — 目标检测损失中广义 IoU 损失的相对权重。

+   `eos_coefficient` (`float`, *optional*, defaults to 0.1) — 目标检测损失中“无物体”类别的相对分类权重。

+   `focal_alpha` (`float`, *optional*, defaults to 0.25) — 焦点损失中的 Alpha 参数。

这是用于存储 ConditionalDetrModel 配置的配置类。根据指定的参数实例化一个 Conditional DETR 模型，定义模型架构。使用默认值实例化配置将产生类似于 Conditional DETR [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import ConditionalDetrConfig, ConditionalDetrModel

>>> # Initializing a Conditional DETR microsoft/conditional-detr-resnet-50 style configuration
>>> configuration = ConditionalDetrConfig()

>>> # Initializing a model (with random weights) from the microsoft/conditional-detr-resnet-50 style configuration
>>> model = ConditionalDetrModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ConditionalDetrImageProcessor

### `class transformers.ConditionalDetrImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L758)

```py
( format: Union = <AnnotationFormat.COCO_DETECTION: 'coco_detection'> do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )
```

参数

+   `format` (`str`, *optional*, 默认为 `"coco_detection"`) — 注释的数据格式。其中之一为`"coco_detection"`或“coco_panoptic”。

+   `do_resize` (`bool`, *optional*, 默认为 `True`) — 控制是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。

+   `size` (`Dict[str, int]` *optional*, 默认为 `{"shortest_edge" -- 800, "longest_edge": 1333}`): 调整大小后的图像（高度，宽度）尺寸。可以被`preprocess`方法中的`size`参数覆盖。

+   `resample` (`PILImageResampling`, *optional*, 默认为 `PILImageResampling.BILINEAR`) — 如果调整图像大小，则使用的重采样滤波器。

+   `do_rescale` (`bool`, *optional*, 默认为 `True`) — 控制是否按指定比例`rescale_factor`重新调整图像。可以被`preprocess`方法中的`do_rescale`参数覆盖。

+   `rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 如果重新调整图像，则使用的比例因子。可以被`preprocess`方法中的`rescale_factor`参数覆盖。`do_normalize` — 控制是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。

+   `image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_DEFAULT_MEAN`) — 在归一化图像时使用的均值。可以是单个值或值列表，每个通道一个值。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_DEFAULT_STD`) — 在归一化图像时使用的标准差值。可以是单个值或值列表，每个通道一个值。可以被`preprocess`方法中的`image_std`参数覆盖。

+   `do_pad` (`bool`, *optional*, defaults to `True`) — 控制是否将图像填充到批处理中最大的图像并创建像素掩码。可以被`preprocess`方法中的`do_pad`参数覆盖。

构建一个 Conditional Detr 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1099)

```py
( images: Union annotations: Union = None return_segmentation_masks: bool = None masks_path: Union = None do_resize: Optional = None size: Optional = None resample = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None format: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像或图像批次。期望单个图像或带有像素值范围从 0 到 255 的图像批次。如果传入像素值在 0 到 1 之间的图像，请设置`do_rescale=False`。

+   `annotations` (`AnnotationType` 或 `List[AnnotationType]`, *optional*) — 与图像或图像批次关联的注释列表。如果注释用于目标检测，则注释应为具有以下键的字典：

    +   “image_id” (`int`): 图像 id。

    +   “annotations” (`List[Dict]`): 图像的注释列表。每个注释应为一个字典。一个图像可能没有注释，此时列表应为空。如果注释用于分割，注释应为一个具有以下键的字典：

    +   “image_id” (`int`): 图像 id。

    +   “segments_info” (`List[Dict]`): 图像的分段列表。每个分段应为一个字典。一个图像可能没有分段，此时列表应为空。

    +   “file_name” (`str`): 图像的文件名。

+   `return_segmentation_masks` (`bool`, *可选*, 默认为 self.return_segmentation_masks) — 是否返回分割掩模。

+   `masks_path` (`str` 或 `pathlib.Path`, *可选*) — 包含分割掩模的目录路径。

+   `do_resize` (`bool`, *可选*, 默认为 self.do_resize) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 self.size) — 调整大小后的图像尺寸。

+   `resample` (`PILImageResampling`, *可选*, 默认为 self.resample) — 调整图像大小时使用的重采样滤波器。

+   `do_rescale` (`bool`, *可选*, 默认为 self.do_rescale) — 是否重新缩放图像。

+   `rescale_factor` (`float`, *可选*, 默认为 self.rescale_factor) — 重新缩放图像时使用的缩放因子。

+   `do_normalize` (`bool`, *可选*, 默认为 self.do_normalize) — 是否规范化图像。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 self.image_mean) — 在规范化图像时使用的均值。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 self.image_std) — 在规范化图像时使用的标准差。

+   `do_pad` (`bool`, *可选*, 默认为 self.do_pad) — 是否填充图像。

+   `format` (`str` 或 `AnnotationFormat`, *可选*, 默认为 self.format) — 注释的格式。

+   `return_tensors` (`str` 或 `TensorType`, *可选*, 默认为 self.return_tensors) — 要返回的张量类型。如果为`None`，将返回图像列表。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   未设置: 使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像以 (高度, 宽度) 格式。

对图像或图像批次进行预处理，以便模型可以使用。

`post_process_object_detection`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1376)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None top_k: int = 100 ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`DetrObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *可选*) — 保留对象检测预测的分数阈值。

+   `target_sizes` (`torch.Tensor` 或 `List[Tuple[int, int]]`, *可选*) — 形状为 `(batch_size, 2)` 的张量或包含每个图像目标大小 (高度, 宽度) 的元组列表 (`Tuple[int, int]`)。如果保留为 None，则预测将不会被调整大小。

+   `top_k` (`int`, *可选*, 默认为 100) — 在通过阈值过滤之前仅保留前 k 个边界框。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的批次中图像的分数、标签和框。

将 ConditionalDetrForObjectDetection 的原始输出转换为最终边界框，格式为（top_left_x，top_left_y，bottom_right_x，bottom_right_y）。仅支持 PyTorch。

#### `post_process_instance_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1483)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs`（ConditionalDetrForSegmentation）- 模型的原始输出。

+   `threshold`（`float`，*可选*，默认为 0.5）- 保留预测的实例掩模的概率分数阈值。

+   `mask_threshold`（`float`，*可选*，默认为 0.5）- 在将预测的掩模转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold`（`float`，*可选*，默认为 0.8）- 用于合并或丢弃每个二进制实例掩模中的小不连通部分的重叠掩模区域阈值。

+   `target_sizes`（`List[Tuple]`，*可选*）- 长度为（batch_size）的列表，其中每个列表项（`Tuple[int, int]]`）对应于每个预测的请求最终大小（高度，宽度）。如果未设置，预测将不会被调整大小。

+   `return_coco_annotation`（`bool`，*可选*）- 默认为`False`。如果设置为`True`，则以 COCO 运行长度编码（RLE）格式返回分割地图。

返回

`List[Dict]`

一个字典列表，每个图像一个，每个字典包含两个键：

+   `segmentation` - 形状为（高度，宽度）的张量，其中每个像素表示`segment_id`或`List[List]`的运行长度编码（RLE）的分割地图，如果 return_coco_annotation 设置为`True`。如果未找到任何掩模，则设置为`None`。

+   `segments_info` - 包含每个段的附加信息的字典。

    +   `id` - 表示`segment_id`的整数。

    +   `label_id` - 表示与`segment_id`对应的标签/语义类别 id 的整数。

    +   `score` - 具有`segment_id`的段的预测分数。

将 ConditionalDetrForSegmentation 的输出转换为实例分割预测。仅支持 PyTorch。

#### `post_process_semantic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1435)

```py
( outputs target_sizes: List = None ) → export const metadata = 'undefined';List[torch.Tensor]
```

参数

+   `outputs`（ConditionalDetrForSegmentation）- 模型的原始输出。

+   `target_sizes`（`List[Tuple[int, int]]`，*可选*）- 包含批处理中每个图像的目标大小（高度，宽度）的元组（`Tuple[int, int]`）列表。如果未设置，预测将不会被调整大小。

返回

`List[torch.Tensor]`

长度为`batch_size`的列表，其中每个项目是形状为（高度，宽度）的语义分割地图，对应于目标大小条目（如果指定了`target_sizes`）。每个`torch.Tensor`的每个条目对应于语义类别 id。

将 ConditionalDetrForSegmentation 的输出转换为语义分割地图。仅支持 PyTorch。

#### `post_process_panoptic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1567)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs`（ConditionalDetrForSegmentation）- 来自 ConditionalDetrForSegmentation 的输出。

+   `threshold` (`float`, *可选*, 默认为 0.5) — 保留预测实例掩码的概率分数阈值。

+   `mask_threshold` (`float`, *可选*, 默认为 0.5) — 将预测的掩码转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold` (`float`, *可选*, 默认为 0.8) — 合并或丢弃每个二进制实例掩码中的小断开部分的重叠掩码区域阈值。

+   `label_ids_to_fuse` (`Set[int]`, *可选*) — 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说一张图像中只能有一个天空，但可以有几个人，因此天空的标签 ID 将在该集合中，但人的标签 ID 不在其中。

+   `target_sizes` (`List[Tuple]`, *可选*) — 长度为(batch_size)的列表，其中每个列表项(`Tuple[int, int]]`)对应于批处理中每个预测的请求的最终大小(高度，宽度)。如果未设置，预测将不会被调整大小。

返回

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` — 形状为`(高度，宽度)`的张量，其中每个像素代表一个`segment_id`，如果未找到高于`threshold`的掩码，则为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。

+   `segments_info` — 包含每个段的附加信息的字典。

    +   `id` — 代表`segment_id`的整数。

    +   `label_id` — 代表与`segment_id`对应的标签/语义类别 id 的整数。

    +   `was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。同一类别/标签的多个实例被融合并分配一个单独的`segment_id`。

    +   `score` — 具有`segment_id`的段的预测分数。

将 ConditionalDetrForSegmentation 的输出转换为图像全景分割预测。仅支持 PyTorch。

## ConditionalDetrFeatureExtractor

### `class transformers.ConditionalDetrFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/feature_extraction_conditional_detr.py#L36)

```py
( *args **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理一张图像或一批图像。

#### `post_process_object_detection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1376)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None top_k: int = 100 ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`DetrObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *可选*) — 保留对象检测预测的分数阈值。

+   `target_sizes` (`torch.Tensor`或`List[Tuple[int, int]]`, *可选*) — 形状为`(batch_size, 2)`的张量或包含每个图像的目标大小(高度，宽度)的元组列表(`Tuple[int, int]`)。如果为 None，预测将不会被调整大小。

+   `top_k` (`int`, *可选*, 默认为 100) — 在通过阈值筛选之前仅保留前 k 个边界框。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的批处理中每个图像的分数、标签和框。

将 ConditionalDetrForObjectDetection 的原始输出转换为最终的边界框，格式为(top_left_x, top_left_y, bottom_right_x, bottom_right_y)。仅支持 PyTorch。

#### `post_process_instance_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1483)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (ConditionalDetrForSegmentation) — 模型的原始输出。

+   `threshold`（`float`，*可选*，默认为 0.5）- 保留预测实例掩码的概率分数阈值。

+   `mask_threshold`（`float`，*可选*，默认为 0.5）- 在将预测的掩码转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold`（`float`，*可选*，默认为 0.8）- 用于合并或丢弃每个二进制实例掩码中的小不连续部分的重叠掩码区域阈值。

+   `target_sizes`（`List[Tuple]`，*可选*）- 长度为（batch_size）的列表，其中每个列表项（`Tuple[int, int]]`）对应于每个预测的请求最终大小（高度，宽度）。如果未设置，预测将不会被调整大小。

+   `return_coco_annotation`（`bool`，*可选*）- 默认为`False`。如果设置为`True`，则以 COCO 运行长度编码（RLE）格式返回分割地图。

返回

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` - 形状为（高度，宽度）的张量，其中每个像素表示`segment_id`或分割地图的`List[List]`运行长度编码（RLE），如果 return_coco_annotation 设置为`True`。如果未找到高于`threshold`的掩码，则设置为`None`。

+   `segments_info` - 包含每个段的其他信息的字典。

    +   `id` - 表示`segment_id`的整数。

    +   `label_id` - 表示与`segment_id`对应的标签/语义类别 id 的整数。

    +   `score` - 具有`segment_id`的段的预测分数。

将 ConditionalDetrForSegmentation 的输出转换为实例分割预测。仅支持 PyTorch。

#### `post_process_semantic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1435)

```py
( outputs target_sizes: List = None ) → export const metadata = 'undefined';List[torch.Tensor]
```

参数

+   `outputs`（ConditionalDetrForSegmentation）- 模型的原始输出。

+   `target_sizes`（`List[Tuple[int, int]]`，*可选*）- 包含批处理中每个图像的目标大小（高度，宽度）的元组列表（`Tuple[int, int]`）。如果未设置，预测将不会被调整大小。

返回

`List[torch.Tensor]`

一个长度为`batch_size`的列表，其中每个项目都是一个形状为（高度，宽度）的语义分割地图，对应于`target_sizes`条目（如果指定了`target_sizes`）。每个`torch.Tensor`的每个条目对应一个语义类别 id。

将 ConditionalDetrForSegmentation 的输出转换为语义分割地图。仅支持 PyTorch。

#### `post_process_panoptic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1567)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs`（ConditionalDetrForSegmentation）- 来自 ConditionalDetrForSegmentation 的输出。

+   `threshold`（`float`，*可选*，默认为 0.5）- 保留预测实例掩码的概率分数阈值。

+   `mask_threshold`（`float`，*可选*，默认为 0.5）- 在将预测的掩码转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold`（`float`，*可选*，默认为 0.8）- 用于合并或丢弃每个二进制实例掩码中的小不连续部分的重叠掩码区域阈值。

+   `label_ids_to_fuse`（`Set[int]`，*可选*）- 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说图像中只能有一个天空，但可以有几个人，因此天空的标签 ID 将在该集合中，但人的标签 ID 不在其中。

+   `target_sizes`（`List[Tuple]`，*可选*）— 长度为（batch_size）的列表，其中每个列表项（`Tuple[int, int]`）对应于批处理中每个预测的请求的最终大小（高度，宽度）。如果未设置，预测将不会被调整大小。

返回

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` — 形状为`(height, width)`的张量，其中每个像素表示一个`segment_id`，如果未找到上面`threshold`的掩码，则表示`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。

+   `segments_info` — 包含每个段的附加信息的字典。

    +   `id` — 表示`segment_id`的整数。

    +   `label_id` — 表示与`segment_id`对应的标签/语义类别 id 的整数。

    +   `was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。同一类别/标签的多个实例被融合并分配一个单一的`segment_id`。

    +   `score` — 具有`segment_id`的段的预测分数。

将 ConditionalDetrForSegmentation 的输出转换为图像全景分割预测。仅支持 PyTorch。

## ConditionalDetrModel

### `class transformers.ConditionalDetrModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py#L1565)

```py
( config: ConditionalDetrConfig )
```

参数

+   `config`（ConditionalDetrConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 Conditional DETR 模型（由骨干和编码器-解码器 Transformer 组成），输出原始隐藏状态，没有特定的头部。

此模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py#L1606)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。

    可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 ConditionalDetrImageProcessor.`call`()。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 避免在填充像素值上执行注意力的掩码。掩码值选在`[0, 1]`中：

    +   对于真实的像素（即“未遮罩”），

    +   对于填充的像素（即“遮罩”），值为 0。

    什么是注意力掩码？

+   `decoder_attention_mask`（形状为`(batch_size, num_queries)`的`torch.FloatTensor`，*可选*）— 默认情况下不使用。可用于遮罩对象查询。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`形状为`(batch_size, sequence_length, hidden_size)`，是编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递图像的扁平化表示，而不是传递骨干网络和投影层的输出的扁平化特征图。

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*) — 可选地，您可以选择直接传递一个嵌入表示，而不是用零张量初始化查询。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（ConditionalDetrConfig）和输入。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — 解码器隐藏状态的元组，形状为`(batch_size, sequence_length, hidden_size)`，包括每一层的输出和初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — 解码器的注意力权重元组，形状为`(batch_size, num_heads, sequence_length, sequence_length)`，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — 解码器交叉注意力层的注意力权重元组，形状为`(batch_size, num_heads, sequence_length, sequence_length)`，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — 编码器隐藏状态的元组，形状为`(batch_size, sequence_length, hidden_size)`，包括每一层的输出和初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `intermediate_hidden_states` (`torch.FloatTensor`，形状为 `(config.decoder_layers, batch_size, sequence_length, hidden_size)`，*可选*，当 `config.auxiliary_loss=True` 时返回）— 中间解码器激活，即每个解码器层的输出，每个都经过了 layernorm。

ConditionalDetrModel 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, AutoModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/conditional-detr-resnet-50")
>>> model = AutoModel.from_pretrained("microsoft/conditional-detr-resnet-50")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**inputs)

>>> # the last hidden states are the final query embeddings of the Transformer decoder
>>> # these are of shape (batch_size, num_queries, hidden_size)
>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 300, 256]
```

## ConditionalDetrForObjectDetection

### `class transformers.ConditionalDetrForObjectDetection`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py#L1734)

```py
( config: ConditionalDetrConfig )
```

参数

+   `config` (ConditionalDetrConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained() 方法以加载模型权重。

CONDITIONAL_DETR 模型（由骨干和编码器-解码器 Transformer 组成），在顶部具有用于诸如 COCO 检测等任务的目标检测头。

此模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py#L1767)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。

    可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 ConditionalDetrImageProcessor.`call`()。

+   `pixel_mask` (`torch.LongTensor`，形状为 `(batch_size, height, width)`，*可选*) — 用于避免在填充像素值上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 表示真实像素（即 `未被掩码`），

    +   0 表示填充像素。

    什么是注意力掩码？

+   `decoder_attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, num_queries)`，*可选*) — 默认情况下不使用。可用于屏蔽对象查询。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`，*可选*) — 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`形状为`(batch_size, sequence_length, hidden_size)`，*可选*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递图像的扁平化表示，而不是传递扁平化特征图（骨干网络输出+投影层的输出）。

+   `decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, num_queries, hidden_size)`，*可选*) — 可选地，您可以选择直接传递一个嵌入表示，而不是用零张量初始化查询。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 ModelOutput 而不是一个普通元组。

+   `labels` (`List[Dict]`，长度为`(batch_size,)`，*可选*) — 用于计算二部匹配损失的标签。字典列表，每个字典至少包含以下 2 个键：‘class_labels’和‘boxes’（分别是批次中图像的类别标签和边界框）。类别标签本身应该是长度为‘图像中边界框数量’的`torch.LongTensor`，而边界框是形状为‘图像中边界框数量，4’的`torch.FloatTensor`。

返回

`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（ConditionalDetrConfig）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回） — 总损失，作为类别预测的负对数似然（交叉熵）和边界框损失的线性组合。后者被定义为 L1 损失和广义尺度不变 IoU 损失的线性组合。

+   `loss_dict` (`Dict`，*可选*) — 包含各个损失的字典。用于记录。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, num_queries, num_classes + 1)`) — 所有查询的分类 logits（包括无对象）。

+   `pred_boxes` (`torch.FloatTensor`，形状为`(batch_size, num_queries, 4)`) — 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批次中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `auxiliary_outputs` (`list[Dict]`，*可选*) — 可选，仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。这是一个包含每个解码器层的上述两个键（`logits`和`pred_boxes`）的字典列表。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。解码器在每个层的输出以及初始嵌入输出的隐藏状态。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）的注意力权重。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）的注意力权重。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。编码器在每个层的输出以及初始嵌入输出的隐藏状态。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）的注意力权重。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

ConditionalDetrForObjectDetection 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, AutoModelForObjectDetection
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/conditional-detr-resnet-50")
>>> model = AutoModelForObjectDetection.from_pretrained("microsoft/conditional-detr-resnet-50")

>>> inputs = image_processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)

>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> target_sizes = torch.tensor([image.size[::-1]])
>>> results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[
...     0
... ]
>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]
Detected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]
Detected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]
Detected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]
Detected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]
```

## ConditionalDetrForSegmentation

### `class transformers.ConditionalDetrForSegmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py#L1921)

```py
( config: ConditionalDetrConfig )
```

参数

+   `config`（ConditionalDetrConfig）- 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

CONDITIONAL_DETR 模型（由骨干和编码器-解码器 Transformer 组成），顶部带有分割头，用于诸如 COCO 全景等任务。

这个模型继承自 PreTrainedModel。查看超类文档以了解库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以了解所有与一般用法和行为相关的事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py#L1951)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrSegmentationOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。

    可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 ConditionalDetrImageProcessor.`call`()。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示真实的像素（即`未被掩码`），

    +   0 表示填充像素（即`已被掩码`）。

    什么是注意力掩码？

+   `decoder_attention_mask`（形状为`(batch_size, num_queries)`的`torch.FloatTensor`，*可选*）— 默认情况下不使用。可用于掩盖对象查询。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包含(`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`) `last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层的输出的隐藏状态序列。用于解码器的交叉注意力。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递一个图像的扁平化表示，而不是传递骨干网络和投影层的输出。

+   `decoder_inputs_embeds`（形状为`(batch_size, num_queries, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递一个嵌入表示，而不是用零张量初始化查询。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

+   `labels`（长度为`(batch_size,)`的`List[Dict]`，*可选*）— 用于计算二分匹配损失、DICE/F-1 损失和 Focal 损失的标签。字典列表，每个字典至少包含以下 3 个键：‘class_labels’、‘boxes’和‘masks’（分别是批次中图像的类标签、边界框和分割掩码）。类标签本身应该是长度为`(图像中边界框的数量,)`的`torch.LongTensor`，边界框是形状为`(图像中边界框的数量, 4)`的`torch.FloatTensor`，掩码是形状为`(图像中边界框的数量, height, width)`的`torch.FloatTensor`。

返回

`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrSegmentationOutput` 或 `tuple(torch.FloatTensor)`

一个`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrSegmentationOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（ConditionalDetrConfig）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 总损失，作为类别预测的负对数似然（交叉熵）和边界框损失的线性组合。后者被定义为 L1 损失和广义尺度不变 IoU 损失的线性组合。

+   `loss_dict` (`Dict`，*optional*) — 包含各个损失的字典。用于记录。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, num_queries, num_classes + 1)`) — 所有查询的分类 logits（包括无对象）。

+   `pred_boxes` (`torch.FloatTensor`，形状为`(batch_size, num_queries, 4)`) — 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `pred_masks` (`torch.FloatTensor`，形状为`(batch_size, num_queries, height/4, width/4)`) — 所有查询的分割掩模 logits。另请参阅 post_process_semantic_segmentation()或 post_process_instance_segmentation()post_process_panoptic_segmentation()分别评估语义、实例和全景分割掩模。

+   `auxiliary_outputs` (`list[Dict]`，*optional*) — 可选，仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。这是一个包含每个解码器层的上述两个键（`logits`和`pred_boxes`）的字典列表。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或者当 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True` 或者当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

ConditionalDetrForSegmentation 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import io
>>> import requests
>>> from PIL import Image
>>> import torch
>>> import numpy

>>> from transformers import (
...     AutoImageProcessor,
...     ConditionalDetrConfig,
...     ConditionalDetrForSegmentation,
... )
>>> from transformers.image_transforms import rgb_to_id

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/conditional-detr-resnet-50")

>>> # randomly initialize all weights of the model
>>> config = ConditionalDetrConfig()
>>> model = ConditionalDetrForSegmentation(config)

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**inputs)

>>> # Use the `post_process_panoptic_segmentation` method of the `image_processor` to retrieve post-processed panoptic segmentation maps
>>> # Segmentation results are returned as a list of dictionaries
>>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[(300, 500)])
>>> # A tensor of shape (height, width) where each value denotes a segment id, filled with -1 if no segment is found
>>> panoptic_seg = result[0]["segmentation"]
>>> # Get prediction score and segment_id to class_id mapping of each segment
>>> panoptic_segments_info = result[0]["segments_info"]
```
