- en: Build and load
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/datasets/about_dataset_load](https://huggingface.co/docs/datasets/about_dataset_load)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/datasets/v2.17.0/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/start.146395b0.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/scheduler.bdbef820.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/singletons.98dc5b8b.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.8a885b74.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/paths.a483fec8.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/app.e612c4fb.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.c0aea24a.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/0.5e8dbda6.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/5.2ae604f0.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Tip.31005f7d.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Heading.2eb892cb.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'Nearly every deep learning workflow begins with loading a dataset, which makes
    it one of the most important steps. With 🤗 Datasets, there are more than 900 datasets
    available to help you get started with your NLP task. All you have to do is call:
    [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    to take your first step. This function is a true workhorse in every sense because
    it builds and loads every dataset you use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ELI5: load_dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin with a basic Explain Like I’m Five.
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset is a directory that contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Some data files in generic formats (JSON, CSV, Parquet, text, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset card named `README.md` that contains documentation about the dataset
    as well as a YAML header to define the datasets tags and configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional dataset script if it requires some code to read the data files.
    This is sometimes used to load files of specific formats and structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function fetches the requested dataset locally or from the Hugging Face Hub. The
    Hub is a central repository where all the Hugging Face datasets and models are
    stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the dataset only contains data files, then [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    automatically infers how to load the data files from their extensions (json, csv,
    parquet, txt, etc.). Under the hood, 🤗 Datasets will use an appropriate [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder)
    based on the data files format. There exist one builder per data file format in
    🤗 Datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.text.Text](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.text.Text)
    for text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.csv.Csv](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.csv.Csv)
    for CSV and TSV'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.json.Json](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.json.Json)
    for JSON and JSONL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.parquet.Parquet](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.parquet.Parquet)
    for Parquet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.arrow.Arrow](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.arrow.Arrow)
    for Arrow (streaming file format)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.sql.Sql](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.sql.Sql)
    for SQL databases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.imagefolder.ImageFolder](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.imagefolder.ImageFolder)
    for image folders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datasets.packaged_modules.audiofolder.AudioFolder](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.packaged_modules.audiofolder.AudioFolder)
    for audio folders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the dataset has a dataset script, then it downloads and imports it from the
    Hugging Face Hub. Code in the dataset script defines a custom [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder)
    the dataset information (description, features, URL to the original files, etc.),
    and tells 🤗 Datasets how to generate and display examples from it.
  prefs: []
  type: TYPE_NORMAL
- en: Read the [Share](./upload_dataset) section to learn more about how to share
    a dataset. This section also provides a step-by-step guide on how to write your
    own dataset loading script!
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Datasets downloads the dataset files from the original URL, generates the
    dataset and caches it in an Arrow table on your drive. If you’ve downloaded the
    dataset before, then 🤗 Datasets will reload it from the cache to save you the
    trouble of downloading it again.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a high-level understanding about how datasets are built, let’s
    take a closer look at the nuts and bolts of how all this works.
  prefs: []
  type: TYPE_NORMAL
- en: Building a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you load a dataset for the first time, 🤗 Datasets takes the raw data file
    and builds it into a table of rows and typed columns. There are two main classes
    responsible for building a dataset: [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    and [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd493a651fa07ec8829a2c0ef2818f43.png)'
  prefs: []
  type: TYPE_IMG
- en: BuilderConfig
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    is the configuration class of [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder).
    The [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    contains the following basic attributes about a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attribute | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `name` | Short name of the dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| `version` | Dataset version identifier. |'
  prefs: []
  type: TYPE_TB
- en: '| `data_dir` | Stores the path to a local folder containing the data files.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `data_files` | Stores paths to local data files. |'
  prefs: []
  type: TYPE_TB
- en: '| `description` | Description of the dataset. |'
  prefs: []
  type: TYPE_TB
- en: 'If you want to add additional attributes to your dataset such as the class
    labels, you can subclass the base [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    class. There are two ways to populate the attributes of a [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    class or subclass:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a list of predefined [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    class (or subclass) instances in the datasets `DatasetBuilder.BUILDER_CONFIGS()`
    attribute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you call [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    any keyword arguments that are not specific to the method will be used to set
    the associated attributes of the [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    class. This will override the predefined attributes if a specific configuration
    was selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also set the [DatasetBuilder.BUILDER_CONFIG_CLASS](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    to any custom subclass of [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig).
  prefs: []
  type: TYPE_NORMAL
- en: DatasetBuilder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder)
    accesses all the attributes inside [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    to build the actual dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb7161ccc3fa095689cfa137b5590ded.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three main methods in [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder):'
  prefs: []
  type: TYPE_NORMAL
- en: '`DatasetBuilder._info()` is in charge of defining the dataset attributes. When
    you call `dataset.info`, 🤗 Datasets returns the information stored here. Likewise,
    the [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    are also specified here. Remember, the [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    are like the skeleton of the dataset. It provides the names and types of each
    column.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DatasetBuilder._split_generator` downloads or retrieves the requested data
    files, organizes them into splits, and defines specific arguments for the generation
    process. This method has a [DownloadManager](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager)
    that downloads files or fetches them from your local filesystem. Within the [DownloadManager](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager),
    there is a [DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)
    method that accepts a dictionary of URLs to the original data files, and downloads
    the requested files. Accepted inputs include: a single URL or path, or a list/dictionary
    of URLs or paths. Any compressed file types like TAR, GZIP and ZIP archives will
    be automatically extracted.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the files are downloaded, [SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)
    organizes them into splits. The [SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)
    contains the name of the split, and any keyword arguments that are provided to
    the `DatasetBuilder._generate_examples` method. The keyword arguments can be specific
    to each split, and typically comprise at least the local path to the data files
    for each split.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`DatasetBuilder._generate_examples` reads and parses the data files for a split.
    Then it yields dataset examples according to the format specified in the `features`
    from `DatasetBuilder._info()`. The input of `DatasetBuilder._generate_examples`
    is actually the `filepath` provided in the keyword arguments of the last method.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset is generated with a Python generator, which doesn’t load all the
    data in memory. As a result, the generator can handle large datasets. However,
    before the generated samples are flushed to the dataset file on disk, they are
    stored in an `ArrowWriter` buffer. This means the generated samples are written
    by batch. If your dataset samples consumes a lot of memory (images or videos),
    then make sure to specify a low value for the `DEFAULT_WRITER_BATCH_SIZE` attribute
    in [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder).
    We recommend not exceeding a size of 200 MB.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Maintaining integrity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ensure a dataset is complete, [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    will perform a series of tests on the downloaded files to make sure everything
    is there. This way, you don’t encounter any surprises when your requested dataset
    doesn’t get generated as expected. [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    verifies:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of splits in the generated `DatasetDict`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of samples in each split of the generated `DatasetDict`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list of downloaded files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SHA256 checksums of the downloaded files (disabled by defaut).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the dataset doesn’t pass the verifications, it is likely that the original
    host of the dataset made some changes in the data files.
  prefs: []
  type: TYPE_NORMAL
- en: If it is your own dataset, you’ll need to recompute the information above and
    update the `README.md` file in your dataset repository. Take a look at this [section](dataset_script#optional-generate-dataset-metadata)
    to learn how to generate and update this metadata.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, an error is raised to alert that the dataset has changed. To ignore
    the error, one needs to specify `verification_mode="no_checks"` in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    Anytime you see a verification error, feel free to open a discussion or pull request
    in the corresponding dataset “Community” tab, so that the integrity checks for
    that dataset are updated.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset repositories on the Hub are scanned for malware, see more information
    [here](https://huggingface.co/docs/hub/security#malware-scanning).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover the datasets without a namespace (originally contributed on our GitHub
    repository) have all been reviewed by our maintainers. The code of these datasets
    is considered **safe**. It concerns datasets that are not under a namespace, e.g.
    “squad” or “glue”, unlike the other datasets that are named “username/dataset_name”
    or “org/dataset_name”.
  prefs: []
  type: TYPE_NORMAL
