["```py\npip install -q peft transformers datasets\n```", "```py\nfrom datasets import load_dataset\n\nds = load_dataset(\"food101\")\n```", "```py\nlabels = ds[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nid2label[2]\n\"baklava\"\n```", "```py\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n```", "```py\nfrom torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\ntrain_transforms = Compose(\n    [\n        RandomResizedCrop(image_processor.size[\"height\"]),\n        RandomHorizontalFlip(),\n        ToTensor(),\n        normalize,\n    ]\n)\n\nval_transforms = Compose(\n    [\n        Resize(image_processor.size[\"height\"]),\n        CenterCrop(image_processor.size[\"height\"]),\n        ToTensor(),\n        normalize,\n    ]\n)\n\ndef preprocess_train(example_batch):\n    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n    return example_batch\n\ndef preprocess_val(example_batch):\n    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n    return example_batch\n```", "```py\ntrain_ds = ds[\"train\"]\nval_ds = ds[\"validation\"]\n\ntrain_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\n```", "```py\nimport torch\n\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    labels = torch.tensor([example[\"label\"] for example in examples])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n```", "```py\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True,\n)\n```", "```py\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],\n)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n\"trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7712775047664294\"\n```", "```py\nfrom transformers import TrainingArguments, Trainer\n\naccount = \"stevhliu\"\npeft_model_id = f\"{account}/google/vit-base-patch16-224-in21k-lora\"\nbatch_size = 128\n\nargs = TrainingArguments(\n    peft_model_id,\n    remove_unused_columns=False,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-3,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,\n    num_train_epochs=5,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    label_names=[\"labels\"],\n)\n```", "```py\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=image_processor,\n    data_collator=collate_fn,\n)\ntrainer.train()\n```", "```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```", "```py\nmodel.push_to_hub(peft_model_id)\n```", "```py\nfrom peft import PeftConfig, PeftModel\nfrom transfomers import AutoImageProcessor\nfrom PIL import Image\nimport requests\n\nconfig = PeftConfig.from_pretrained(\"stevhliu/vit-base-patch16-224-in21k-lora\")\nmodel = AutoModelForImageClassification.from_pretrained(\n    config.base_model_name_or_path,\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True,\n)\nmodel = PeftModel.from_pretrained(model, \"stevhliu/vit-base-patch16-224-in21k-lora\")\n\nurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n```", "```py\nencoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n```", "```py\nwith torch.no_grad():\n    outputs = model(**encoding)\n    logits = outputs.logits\n\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n\"Predicted class: beignets\"\n```"]