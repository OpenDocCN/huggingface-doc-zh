# æ£€æŸ¥ç‚¹

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/usage_guides/checkpoint](https://huggingface.co/docs/accelerate/usage_guides/checkpoint)

åœ¨ä½¿ç”¨ğŸ¤— Accelerateè®­ç»ƒPyTorchæ¨¡å‹æ—¶ï¼Œæ‚¨å¯èƒ½ç»å¸¸å¸Œæœ›ä¿å­˜å’Œç»§ç»­è®­ç»ƒçŠ¶æ€ã€‚ä¸ºæ­¤ï¼Œéœ€è¦ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€RNGç”Ÿæˆå™¨å’ŒGradScalerã€‚åœ¨ğŸ¤— Accelerateå†…éƒ¨æœ‰ä¸¤ä¸ªä¾¿åˆ©å‡½æ•°å¯ä»¥å¿«é€Ÿå®ç°è¿™ä¸€ç‚¹ï¼š

+   ä½¿ç”¨[save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)å°†ä¸Šè¿°æ‰€æœ‰å†…å®¹ä¿å­˜åˆ°æ–‡ä»¶å¤¹ä½ç½®

+   ä½¿ç”¨[load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)åŠ è½½å…ˆå‰`save_state`å­˜å‚¨çš„æ‰€æœ‰å†…å®¹

é€šè¿‡[save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)è¿›ä¸€æ­¥è‡ªå®šä¹‰çŠ¶æ€ä¿å­˜çš„ä½ç½®å’Œæ–¹å¼ï¼Œå¯ä»¥ä½¿ç”¨[ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)ç±»ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¯ç”¨äº†`automatic_checkpoint_naming`ï¼Œæ¯ä¸ªä¿å­˜çš„æ£€æŸ¥ç‚¹å°†ä½äº`Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`ã€‚

åº”è¯¥æ³¨æ„åˆ°ï¼ŒæœŸæœ›æ˜¯è¿™äº›çŠ¶æ€æ¥è‡ªç›¸åŒçš„è®­ç»ƒè„šæœ¬ï¼Œè€Œä¸åº”è¯¥æ¥è‡ªä¸¤ä¸ªå•ç‹¬çš„è„šæœ¬ã€‚

+   é€šè¿‡ä½¿ç”¨[register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing)ï¼Œæ‚¨å¯ä»¥æ³¨å†Œè‡ªå®šä¹‰å¯¹è±¡ï¼Œä»¥ä¾¿ä»å‰ä¸¤ä¸ªå‡½æ•°ä¸­è‡ªåŠ¨å­˜å‚¨æˆ–åŠ è½½ï¼Œåªè¦è¯¥å¯¹è±¡å…·æœ‰`state_dict` **å’Œ** `load_state_dict`åŠŸèƒ½ã€‚è¿™å¯èƒ½åŒ…æ‹¬å­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰å¯¹è±¡ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€çŸ­çš„ç¤ºä¾‹ï¼Œä½¿ç”¨æ£€æŸ¥ç‚¹æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜å’Œé‡æ–°åŠ è½½çŠ¶æ€ï¼š

```py
from accelerate import Accelerator
import torch

accelerator = Accelerator(project_dir="my/save/path")

my_scheduler = torch.optim.lr_scheduler.StepLR(my_optimizer, step_size=1, gamma=0.99)
my_model, my_optimizer, my_training_dataloader = accelerator.prepare(my_model, my_optimizer, my_training_dataloader)

# Register the LR scheduler
accelerator.register_for_checkpointing(my_scheduler)

# Save the starting state
accelerator.save_state()

device = accelerator.device
my_model.to(device)

# Perform training
for epoch in range(num_epochs):
    for batch in my_training_dataloader:
        my_optimizer.zero_grad()
        inputs, targets = batch
        inputs = inputs.to(device)
        targets = targets.to(device)
        outputs = my_model(inputs)
        loss = my_loss_function(outputs, targets)
        accelerator.backward(loss)
        my_optimizer.step()
    my_scheduler.step()

# Restore the previous state
accelerator.load_state("my/save/path/checkpointing/checkpoint_0")
```

## æ¢å¤DataLoaderçš„çŠ¶æ€

ä»æ£€æŸ¥ç‚¹æ¢å¤åï¼Œå¦‚æœåœ¨ä¸€ä¸ªepochçš„ä¸­é—´ä¿å­˜äº†çŠ¶æ€ï¼Œå¯èƒ½è¿˜å¸Œæœ›ä»æ´»åŠ¨çš„`DataLoader`ä¸­çš„ç‰¹å®šç‚¹æ¢å¤ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[skip_first_batches()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.skip_first_batches)æ¥å®ç°ã€‚

```py
from accelerate import Accelerator

accelerator = Accelerator(project_dir="my/save/path")

train_dataloader = accelerator.prepare(train_dataloader)
accelerator.load_state("my_state")

# Assume the checkpoint was saved 100 steps into the epoch
skipped_dataloader = accelerator.skip_first_batches(train_dataloader, 100)

# After the first iteration, go back to `train_dataloader`

# First epoch
for batch in skipped_dataloader:
    # Do something
    pass

# Second epoch
for batch in train_dataloader:
    # Do something
    pass
```
