["```py\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n\n>>> sequence_a = \"This is a short sequence.\"\n>>> sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n\n>>> encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n>>> encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]\n```", "```py\n>>> len(encoded_sequence_a), len(encoded_sequence_b)\n(8, 19)\n```", "```py\n>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n```", "```py\n>>> padded_sequences[\"input_ids\"]\n[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n```", "```py\n>>> padded_sequences[\"attention_mask\"]\n[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n```", "```py\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n\n>>> sequence = \"A Titan RTX has 24GB of VRAM\"\n```", "```py\n>>> tokenized_sequence = tokenizer.tokenize(sequence)\n```", "```py\n>>> print(tokenized_sequence)\n['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n```", "```py\n>>> inputs = tokenizer(sequence)\n```", "```py\n>>> encoded_sequence = inputs[\"input_ids\"]\n>>> print(encoded_sequence)\n[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n```", "```py\n>>> decoded_sequence = tokenizer.decode(encoded_sequence)\n```", "```py\n>>> print(decoded_sequence)\n[CLS] A Titan RTX has 24GB of VRAM [SEP]\n```", "```py\n>>> # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\n```", "```py\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n>>> sequence_a = \"HuggingFace is based in NYC\"\n>>> sequence_b = \"Where is HuggingFace based?\"\n\n>>> encoded_dict = tokenizer(sequence_a, sequence_b)\n>>> decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n```", "```py\n>>> print(decoded)\n[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n```", "```py\n>>> encoded_dict[\"token_type_ids\"]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```"]