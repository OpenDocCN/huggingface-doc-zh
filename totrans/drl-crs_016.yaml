- en: Quiz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹éªŒ
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/quiz](https://huggingface.co/learn/deep-rl-course/unit1/quiz)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit1/quiz](https://huggingface.co/learn/deep-rl-course/unit1/quiz)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ å’Œ[é¿å…èƒ½åŠ›é”™è§‰](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)çš„æœ€ä½³æ–¹æ³•æ˜¯æµ‹è¯•è‡ªå·±ã€‚è¿™å°†å¸®åŠ©ä½ æ‰¾åˆ°éœ€è¦åŠ å¼ºçŸ¥è¯†çš„åœ°æ–¹ã€‚
- en: 'Q1: What is Reinforcement Learning?'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q1ï¼šä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ
- en: <details data-svelte-h="svelte-hwjwin"><summary>Solution</summary>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-hwjwin"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: Reinforcement learning is a **framework for solving control tasks (also called
    decision problems)** by building agents that learn from the environment by interacting
    with it through trial and error and **receiving rewards (positive or negative)
    as unique feedback**.</details>
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ æ˜¯é€šè¿‡ä¸ç¯å¢ƒè¿›è¡Œè¯•é”™äº¤äº’å¹¶æ¥æ”¶å¥–åŠ±ï¼ˆç§¯ææˆ–æ¶ˆæï¼‰ä½œä¸ºç‹¬ç‰¹åé¦ˆæ¥è§£å†³æ§åˆ¶ä»»åŠ¡ï¼ˆä¹Ÿç§°ä¸ºå†³ç­–é—®é¢˜ï¼‰çš„æ¡†æ¶ã€‚</details>
- en: 'Q2: Define the RL Loop'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q2ï¼šå®šä¹‰RLå¾ªç¯
- en: '![Exercise RL Loop](../Images/7dd50ea3f5814cad587739a6db3ce516.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹ RLå¾ªç¯](../Images/7dd50ea3f5814cad587739a6db3ce516.png)'
- en: 'At every step:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€æ­¥ï¼š
- en: Our Agent receives **__** from the environment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„Agentä»ç¯å¢ƒä¸­æ¥æ”¶__ã€‚
- en: Based on that **__** the Agent takes an **__**
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºé‚£ä¸ª__ï¼ŒAgenté‡‡å–ä¸€ä¸ª__ã€‚
- en: Our Agent will move to the right
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„Agentå°†å‘å³ç§»åŠ¨
- en: The Environment goes to a **__**
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒè¿›å…¥ä¸€ä¸ª__ã€‚
- en: The Environment gives a **__** to the Agent
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒç»™Agentä¸€ä¸ª__ã€‚
- en: 'Q3: Whatâ€™s the difference between a state and an observation?'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q3ï¼šçŠ¶æ€å’Œè§‚å¯Ÿä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- en: 'Q4: A task is an instance of a Reinforcement Learning problem. What are the
    two types of tasks?'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q4ï¼šä»»åŠ¡æ˜¯å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„ä¸€ä¸ªå®ä¾‹ã€‚æœ‰å“ªä¸¤ç§ç±»å‹çš„ä»»åŠ¡ï¼Ÿ
- en: 'Q5: What is the exploration/exploitation tradeoff?'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q5ï¼šä»€ä¹ˆæ˜¯æ¢ç´¢/å¼€å‘æƒè¡¡ï¼Ÿ
- en: <details data-svelte-h="svelte-lhagbu"><summary>Solution</summary>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-lhagbu"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: In Reinforcement Learning, we need to **balance how much we explore the environment
    and how much we exploit what we know about the environment**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¹³è¡¡æ¢ç´¢ç¯å¢ƒå’Œåˆ©ç”¨æˆ‘ä»¬å¯¹ç¯å¢ƒçš„äº†è§£çš„ç¨‹åº¦ã€‚
- en: '*Exploration* is exploring the environment by **trying random actions in order
    to find more information about the environment**.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ¢ç´¢*æ˜¯é€šè¿‡å°è¯•éšæœºåŠ¨ä½œæ¥æ¢ç´¢ç¯å¢ƒï¼Œä»¥ä¾¿æ›´å¤šåœ°äº†è§£ç¯å¢ƒã€‚'
- en: '*Exploitation* is **exploiting known information to maximize the reward**.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¼€å‘*æ˜¯åˆ©ç”¨å·²çŸ¥ä¿¡æ¯æ¥æœ€å¤§åŒ–å¥–åŠ±ã€‚'
- en: '![Exploration Exploitation Tradeoff](../Images/3a59b593e994b9d356515c58b0fa6a24.png)</details>'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![æ¢ç´¢å¼€å‘æƒè¡¡](../Images/3a59b593e994b9d356515c58b0fa6a24.png)</details>'
- en: 'Q6: What is a policy?'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q6ï¼šä»€ä¹ˆæ˜¯ç­–ç•¥ï¼Ÿ
- en: <details data-svelte-h="svelte-su9zbv"><summary>Solution</summary>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-su9zbv"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: The Policy Ï€ **is the brain of our Agent**. Itâ€™s the function that tells us
    what action to take given the state we are in. So it defines the agentâ€™s behavior
    at a given time.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥Ï€æ˜¯æˆ‘ä»¬Agentçš„å¤§è„‘ã€‚å®ƒæ˜¯å‘Šè¯‰æˆ‘ä»¬åœ¨ç‰¹å®šçŠ¶æ€ä¸‹åº”è¯¥é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨çš„å‡½æ•°ã€‚å› æ­¤ï¼Œå®ƒå®šä¹‰äº†Agentåœ¨ç‰¹å®šæ—¶é—´çš„è¡Œä¸ºã€‚
- en: '![Policy](../Images/83518e23a957f171ab1fe3fa7a6bbe35.png)</details>'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![ç­–ç•¥](../Images/83518e23a957f171ab1fe3fa7a6bbe35.png)</details>'
- en: 'Q7: What are value-based methods?'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q7ï¼šä»€ä¹ˆæ˜¯åŸºäºä»·å€¼çš„æ–¹æ³•ï¼Ÿ
- en: <details data-svelte-h="svelte-g9n3n1"><summary>Solution</summary>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-g9n3n1"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: Value-based methods is one of the main approaches for solving RL problems.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºä»·å€¼çš„æ–¹æ³•æ˜¯è§£å†³RLé—®é¢˜çš„ä¸»è¦æ–¹æ³•ä¹‹ä¸€ã€‚
- en: In Value-based methods, instead of training a policy function, **we train a
    value function that maps a state to the expected value of being at that state**.</details>
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºä»·å€¼çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯è®­ç»ƒä¸€ä¸ªç­–ç•¥å‡½æ•°ï¼Œè€Œæ˜¯è®­ç»ƒä¸€ä¸ªä»·å€¼å‡½æ•°ï¼Œå°†ä¸€ä¸ªçŠ¶æ€æ˜ å°„åˆ°åœ¨è¯¥çŠ¶æ€çš„é¢„æœŸä»·å€¼ã€‚</details>
- en: 'Q8: What are policy-based methods?'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q8ï¼šä»€ä¹ˆæ˜¯åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Ÿ
- en: <details data-svelte-h="svelte-aae5od"><summary>Solution</summary>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-aae5od"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: In *Policy-Based Methods*, we learn a **policy function directly**.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨*åŸºäºç­–ç•¥çš„æ–¹æ³•*ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥å­¦ä¹ ä¸€ä¸ªç­–ç•¥å‡½æ•°ã€‚
- en: This policy function will **map from each state to the best corresponding action
    at that state**. Or a **probability distribution over the set of possible actions
    at that state**.</details>
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç­–ç•¥å‡½æ•°å°†ä»æ¯ä¸ªçŠ¶æ€æ˜ å°„åˆ°è¯¥çŠ¶æ€çš„æœ€ä½³å¯¹åº”åŠ¨ä½œã€‚æˆ–è€…æ˜¯è¯¥çŠ¶æ€å¯èƒ½åŠ¨ä½œé›†åˆä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚</details>
- en: 'Congrats on finishing this Quiz ğŸ¥³, if you missed some elements, take time to
    read again the chapter to reinforce (ğŸ˜) your knowledge, but **do not worry**:
    during the course weâ€™ll go over again of these concepts, and youâ€™ll **reinforce
    your theoretical knowledge with hands-on**.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ å®Œæˆäº†è¿™ä¸ªæµ‹éªŒğŸ¥³ï¼Œå¦‚æœä½ é”™è¿‡äº†ä¸€äº›å…ƒç´ ï¼Œè¯·èŠ±æ—¶é—´å†æ¬¡é˜…è¯»ç« èŠ‚ï¼Œä»¥åŠ å¼ºï¼ˆğŸ˜ï¼‰ä½ çš„çŸ¥è¯†ï¼Œä½†ä¸è¦æ‹…å¿ƒï¼šåœ¨è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å†æ¬¡è®¨è®ºè¿™äº›æ¦‚å¿µï¼Œå¹¶ä¸”ä½ å°†é€šè¿‡å®è·µåŠ å¼ºä½ çš„ç†è®ºçŸ¥è¯†ã€‚
