["```py\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\n\nmodel = nn.Linear(10, 10)\nddp_model = DistributedDataParallel(model)\n```", "```py\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n  import torch.nn as nn\n- from torch.nn.parallel import DistributedDataParallel\n\n  model = nn.Linear(10,10)\n+ model = accelerator.prepare(model)\n```", "```py\nddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n\nfor index, batch in enumerate(dataloader):\n    inputs, targets = batch\n    # Trigger gradient synchronization on the last batch\n    if index != (len(dataloader) - 1):\n        with ddp_model.no_sync():\n            # Gradients only accumulate\n            outputs = ddp_model(inputs)\n            loss = loss_func(outputs)\n            accelerator.backward(loss)\n    else:\n        # Gradients finally sync\n        outputs = ddp_model(inputs)\n        loss = loss_func(outputs)\n        accelerator.backward(loss)\n        optimizer.step()\n```", "```py\n  ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n\n  for index, batch in enumerate(dataloader):\n      inputs, targets = batch\n      # Trigger gradient synchronization on the last batch\n      if index != (len(dataloader)-1):\n-         with ddp_model.no_sync():\n+         with accelerator.no_sync(model):\n              # Gradients only accumulate\n              outputs = ddp_model(inputs)\n              loss = loss_func(outputs, targets)\n              accelerator.backward(loss)\n      else:\n          # Gradients finally sync\n          outputs = ddp_model(inputs)\n          loss = loss_func(outputs)\n          accelerator.backward(loss)\n          optimizer.step()\n          optimizer.zero_grad()\n```", "```py\nddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n\nfor batch in dataloader:\n    with accelerator.accumulate(model):\n        optimizer.zero_grad()\n        inputs, targets = batch\n        outputs = model(inputs)\n        loss = loss_function(outputs, targets)\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n```"]