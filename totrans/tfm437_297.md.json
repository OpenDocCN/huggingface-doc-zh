["```py\n( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 qkv_bias = True decoder_num_attention_heads = 16 decoder_hidden_size = 512 decoder_num_hidden_layers = 8 decoder_intermediate_size = 2048 mask_ratio = 0.75 norm_pix_loss = False **kwargs )\n```", "```py\n>>> from transformers import ViTMAEConfig, ViTMAEModel\n\n>>> # Initializing a ViT MAE vit-mae-base style configuration\n>>> configuration = ViTMAEConfig()\n\n>>> # Initializing a model (with random weights) from the vit-mae-base style configuration\n>>> model = ViTMAEModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None noise: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.vit_mae.modeling_vit_mae.ViTMAEModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, ViTMAEModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n>>> model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None noise: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.vit_mae.modeling_vit_mae.ViTMAEForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, ViTMAEForPreTraining\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n>>> model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n>>> mask = outputs.mask\n>>> ids_restore = outputs.ids_restore\n```", "```py\n( config: ViTMAEConfig *inputs **kwargs )\n```", "```py\n( pixel_values: TFModelInputType | None = None noise: tf.Tensor = None head_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFViTMAEModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n>>> model = TFViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( pixel_values: TFModelInputType | None = None noise: tf.Tensor = None head_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEForPreTrainingOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFViTMAEForPreTraining\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n>>> model = TFViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n>>> mask = outputs.mask\n>>> ids_restore = outputs.ids_restore\n```"]