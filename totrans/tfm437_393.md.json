["```py\n# We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n# Download vocabulary from huggingface.co and cache.\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Download vocabulary from huggingface.co (user-uploaded) and cache.\ntokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n\n# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\ntokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n\n# If the tokenizer uses a single vocabulary file, you can point directly to this file\ntokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n\n# You can link tokens to special vocabulary when instantiating\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", unk_token=\"<unk>\")\n# You should be sure '<unk>' is in the vocabulary when doing that.\n# Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\nassert tokenizer.unk_token == \"<unk>\"\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"my-finetuned-bert\")\n\n# Push the tokenizer to an organization with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n# Let's see how to add a new classification token to GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nspecial_tokens_dict = {\"cls_token\": \"<CLS>\"}\n\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\nassert tokenizer.cls_token == \"<CLS>\"\n```", "```py\n# Let's see how to increase the vocabulary of Bert model and tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\nnum_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```"]