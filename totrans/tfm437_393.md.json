["```py\n( **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( )\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( batch_text_or_text_pairs: Union add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( out_string: str ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( tokens: List ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) \u2192 export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray\n```", "```py\n( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( pretrained_model_name_or_path: Union *init_inputs cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n# We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n# Download vocabulary from huggingface.co and cache.\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Download vocabulary from huggingface.co (user-uploaded) and cache.\ntokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n\n# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\ntokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n\n# If the tokenizer uses a single vocabulary file, you can point directly to this file\ntokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n\n# You can link tokens to special vocabulary when instantiating\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", unk_token=\"<unk>\")\n# You should be sure '<unk>' is in the vocabulary when doing that.\n# Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\nassert tokenizer.unk_token == \"<unk>\"\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';A list of integers in the range [0, 1]\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, int]\n```", "```py\n( encoded_inputs: Union padding: Union = True max_length: Optional = None pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None verbose: bool = True )\n```", "```py\n( ids: List pair_ids: Optional = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True prepend_batch_axis: bool = False **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( src_texts: List tgt_texts: Optional = None max_length: Optional = None max_target_length: Optional = None padding: str = 'longest' return_tensors: str = None truncation: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"my-finetuned-bert\")\n\n# Push the tokenizer to an organization with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( auto_class = 'AutoTokenizer' )\n```", "```py\n( save_directory: Union legacy_format: Optional = None filename_prefix: Optional = None push_to_hub: bool = False **kwargs ) \u2192 export const metadata = 'undefined';A tuple of str\n```", "```py\n( save_directory: str filename_prefix: Optional = None ) \u2192 export const metadata = 'undefined';Tuple(str)\n```", "```py\n( text: str pair: Optional = None add_special_tokens: bool = False **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( ids: List pair_ids: Optional = None num_tokens_to_remove: int = 0 truncation_strategy: Union = 'longest_first' stride: int = 0 ) \u2192 export const metadata = 'undefined';Tuple[List[int], List[int], List[int]]\n```", "```py\n( verbose = False **kwargs )\n```", "```py\n( special_tokens_dict: Dict replace_additional_special_tokens = True ) \u2192 export const metadata = 'undefined';int\n```", "```py\n# Let's see how to add a new classification token to GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nspecial_tokens_dict = {\"cls_token\": \"<CLS>\"}\n\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\nassert tokenizer.cls_token == \"<CLS>\"\n```", "```py\n( new_tokens: Union special_tokens: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n# Let's see how to increase the vocabulary of Bert model and tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\nnum_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```", "```py\n( )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( start: int end: int )\n```", "```py\n( start: int end: int )\n```"]