- en: All about metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºæŒ‡æ ‡çš„ä¸€åˆ‡
- en: 'Original text: [https://huggingface.co/docs/datasets/about_metrics](https://huggingface.co/docs/datasets/about_metrics)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/datasets/about_metrics](https://huggingface.co/docs/datasets/about_metrics)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Metrics is deprecated in ğŸ¤— Datasets. To learn more about how to use metrics,
    take a look at the library ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)!
    In addition to metrics, you can find more tools for evaluating models and datasets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤—æ•°æ®é›†ä¸­ï¼ŒæŒ‡æ ‡å·²è¢«å¼ƒç”¨ã€‚è¦äº†è§£å¦‚ä½•ä½¿ç”¨æŒ‡æ ‡ï¼Œè¯·æŸ¥çœ‹åº“ğŸ¤—[Evaluate](https://huggingface.co/docs/evaluate/index)ï¼é™¤äº†æŒ‡æ ‡ï¼Œæ‚¨è¿˜å¯ä»¥æ‰¾åˆ°æ›´å¤šç”¨äºè¯„ä¼°æ¨¡å‹å’Œæ•°æ®é›†çš„å·¥å…·ã€‚
- en: 'ğŸ¤— Datasets provides access to a wide range of NLP metrics. You can load metrics
    associated with benchmark datasets like GLUE or SQuAD, and complex metrics like
    BLEURT or BERTScore, with a single command: [load_metric()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_metric).
    Once youâ€™ve loaded a metric, easily compute and evaluate a modelâ€™s performance.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—æ•°æ®é›†æä¾›äº†å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŒ‡æ ‡ã€‚æ‚¨å¯ä»¥åŠ è½½ä¸åŸºå‡†æ•°æ®é›†ï¼ˆå¦‚GLUEæˆ–SQuADï¼‰å’Œå¤æ‚æŒ‡æ ‡ï¼ˆå¦‚BLEURTæˆ–BERTScoreï¼‰ç›¸å…³è”çš„æŒ‡æ ‡ï¼Œåªéœ€ä¸€ä¸ªå‘½ä»¤ï¼š[load_metric()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_metric)ã€‚ä¸€æ—¦åŠ è½½äº†æŒ‡æ ‡ï¼Œå°±å¯ä»¥è½»æ¾è®¡ç®—å’Œè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: 'ELI5: load_metric'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELI5ï¼šload_metric
- en: Loading a dataset and loading a metric share many similarities. This was an
    intentional design choice because we wanted to create a simple and unified experience.
    When you call [load_metric()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_metric),
    the metric loading script is downloaded and imported from GitHub (if it hasnâ€™t
    already been downloaded before). It contains information about the metric such
    as itâ€™s citation, homepage, and description.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®é›†å’ŒåŠ è½½æŒ‡æ ‡æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰æ„è®¾è®¡çš„é€‰æ‹©ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›åˆ›é€ ä¸€ä¸ªç®€å•å’Œç»Ÿä¸€çš„ä½“éªŒã€‚å½“æ‚¨è°ƒç”¨[load_metric()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_metric)æ—¶ï¼Œå°†ä»GitHubä¸‹è½½å¹¶å¯¼å…¥æŒ‡æ ‡åŠ è½½è„šæœ¬ï¼ˆå¦‚æœä¹‹å‰å°šæœªä¸‹è½½ï¼‰ã€‚å®ƒåŒ…å«æœ‰å…³æŒ‡æ ‡çš„ä¿¡æ¯ï¼Œä¾‹å¦‚å¼•ç”¨ã€ä¸»é¡µå’Œæè¿°ã€‚
- en: The metric loading script will instantiate and return a [Metric](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric)
    object. This stores the predictions and references, which you need to compute
    the metric values. The [Metric](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric)
    object is stored as an Apache Arrow table. As a result, the predictions and references
    are stored directly on disk with memory-mapping. This enables ğŸ¤— Datasets to do
    a lazy computation of the metric, and makes it easier to gather all the predictions
    in a distributed setting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡æ ‡åŠ è½½è„šæœ¬å°†å®ä¾‹åŒ–å¹¶è¿”å›ä¸€ä¸ª[Metric](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric)å¯¹è±¡ã€‚è¿™å°†å­˜å‚¨æ‚¨éœ€è¦è®¡ç®—æŒ‡æ ‡å€¼çš„é¢„æµ‹å’Œå‚è€ƒå€¼ã€‚[Metric](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric)å¯¹è±¡å­˜å‚¨ä¸ºApache
    Arrowè¡¨ã€‚å› æ­¤ï¼Œé¢„æµ‹å’Œå‚è€ƒå€¼ç›´æ¥å­˜å‚¨åœ¨å…·æœ‰å†…å­˜æ˜ å°„çš„ç£ç›˜ä¸Šã€‚è¿™ä½¿å¾—ğŸ¤—æ•°æ®é›†èƒ½å¤Ÿå¯¹æŒ‡æ ‡è¿›è¡Œå»¶è¿Ÿè®¡ç®—ï¼Œå¹¶ä¸”æ›´å®¹æ˜“åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­æ”¶é›†æ‰€æœ‰é¢„æµ‹å€¼ã€‚
- en: Distributed evaluation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è¯„ä¼°
- en: Computing metrics in a distributed environment can be tricky. Metric evaluation
    is executed in separate Python processes, or nodes, on different subsets of a
    dataset. Typically, when a metric score is additive (`f(AuB) = f(A) + f(B)`),
    you can use distributed reduce operations to gather the scores for each subset
    of the dataset. But when a metric is non-additive (`f(AuB) â‰  f(A) + f(B)`), itâ€™s
    not that simple. For example, you canâ€™t take the sum of the [F1](https://huggingface.co/metrics/f1)
    scores of each data subset as your **final metric**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®¡ç®—æŒ‡æ ‡å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚æŒ‡æ ‡è¯„ä¼°æ˜¯åœ¨ä¸åŒæ•°æ®é›†å­é›†ä¸Šçš„å•ç‹¬Pythonè¿›ç¨‹æˆ–èŠ‚ç‚¹ä¸­æ‰§è¡Œçš„ã€‚é€šå¸¸ï¼Œå½“æŒ‡æ ‡å¾—åˆ†æ˜¯å¯åŠ çš„ï¼ˆ`f(AuB) = f(A)
    + f(B)`ï¼‰æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼reduceæ“ä½œæ¥æ”¶é›†æ¯ä¸ªæ•°æ®é›†å­é›†çš„å¾—åˆ†ã€‚ä½†æ˜¯å½“æŒ‡æ ‡æ˜¯ä¸å¯åŠ çš„ï¼ˆ`f(AuB) â‰  f(A) + f(B)`ï¼‰æ—¶ï¼Œæƒ…å†µå°±ä¸é‚£ä¹ˆç®€å•äº†ã€‚ä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°†æ¯ä¸ªæ•°æ®å­é›†çš„[F1](https://huggingface.co/metrics/f1)å¾—åˆ†ä¹‹å’Œä½œä¸ºæ‚¨çš„**æœ€ç»ˆæŒ‡æ ‡**ã€‚
- en: A common way to overcome this issue is to fallback on single process evaluation.
    The metrics are evaluated on a single GPU, which becomes inefficient.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å…‹æœè¿™ä¸ªé—®é¢˜çš„å¸¸è§æ–¹æ³•æ˜¯å›é€€åˆ°å•è¿›ç¨‹è¯„ä¼°ã€‚æŒ‡æ ‡åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™å˜å¾—ä½æ•ˆã€‚
- en: ğŸ¤— Datasets solves this issue by only computing the final metric on the first
    node. The predictions and references are computed and provided to the metric separately
    for each node. These are temporarily stored in an Apache Arrow table, avoiding
    cluttering the GPU or CPU memory. When you are ready to [Metric.compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute)
    the final metric, the first node is able to access the predictions and references
    stored on all the other nodes. Once it has gathered all the predictions and references,
    [Metric.compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute)
    will perform the final metric evaluation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—æ•°æ®é›†é€šè¿‡ä»…åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè®¡ç®—æœ€ç»ˆæŒ‡æ ‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¢„æµ‹å’Œå‚è€ƒå€¼åˆ†åˆ«ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¡ç®—å¹¶æä¾›ç»™æŒ‡æ ‡ã€‚è¿™äº›æš‚æ—¶å­˜å‚¨åœ¨Apache Arrowè¡¨ä¸­ï¼Œé¿å…äº†GPUæˆ–CPUå†…å­˜çš„æ··ä¹±ã€‚å½“æ‚¨å‡†å¤‡å¥½[æŒ‡æ ‡è®¡ç®—ï¼ˆMetric.compute()ï¼‰](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute)æœ€ç»ˆæŒ‡æ ‡æ—¶ï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹èƒ½å¤Ÿè®¿é—®æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸Šå­˜å‚¨çš„é¢„æµ‹å’Œå‚è€ƒå€¼ã€‚ä¸€æ—¦æ”¶é›†åˆ°æ‰€æœ‰é¢„æµ‹å’Œå‚è€ƒå€¼ï¼Œ[æŒ‡æ ‡è®¡ç®—ï¼ˆMetric.compute()ï¼‰](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute)å°†æ‰§è¡Œæœ€ç»ˆæŒ‡æ ‡è¯„ä¼°ã€‚
- en: This solution allows ğŸ¤— Datasets to perform distributed predictions, which is
    important for evaluation speed in distributed settings. At the same time, you
    can also use complex non-additive metrics without wasting valuable GPU or CPU
    memory.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è§£å†³æ–¹æ¡ˆå…è®¸ğŸ¤—æ•°æ®é›†æ‰§è¡Œåˆ†å¸ƒå¼é¢„æµ‹ï¼Œè¿™å¯¹äºåˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è¯„ä¼°é€Ÿåº¦è‡³å…³é‡è¦ã€‚åŒæ—¶ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨å¤æ‚çš„éå¯åŠ æŒ‡æ ‡ï¼Œè€Œä¸ä¼šæµªè´¹å®è´µçš„GPUæˆ–CPUå†…å­˜ã€‚
