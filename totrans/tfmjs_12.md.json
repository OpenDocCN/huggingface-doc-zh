["```py\nnpm init -y\nnpm i @xenova/transformers\n```", "```py\n{\n  ...\n  \"type\": \"module\",\n  ...\n}\n```", "```py\nimport http from 'http';\nimport querystring from 'querystring';\nimport url from 'url';\n```", "```py\nimport { pipeline, env } from '@xenova/transformers';\n\nclass MyClassificationPipeline {\n  static task = 'text-classification';\n  static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';\n  static instance = null;\n\n  static async getInstance(progress_callback = null) {\n    if (this.instance === null) {\n      // NOTE: Uncomment this to change the cache directory\n      // env.cacheDir = './.cache';\n\n      this.instance = pipeline(this.task, this.model, { progress_callback });\n    }\n\n    return this.instance;\n  }\n}\n```", "```py\nconst http = require('http');\nconst querystring = require('querystring');\nconst url = require('url');\n```", "```py\nclass MyClassificationPipeline {\n  static task = 'text-classification';\n  static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';\n  static instance = null;\n\n  static async getInstance(progress_callback = null) {\n    if (this.instance === null) {\n      // Dynamically import the Transformers.js library\n      let { pipeline, env } = await import('@xenova/transformers');\n\n      // NOTE: Uncomment this to change the cache directory\n      // env.cacheDir = './.cache';\n\n      this.instance = pipeline(this.task, this.model, { progress_callback });\n    }\n\n    return this.instance;\n  }\n}\n```", "```py\n// Define the HTTP server\nconst server = http.createServer();\nconst hostname = '127.0.0.1';\nconst port = 3000;\n\n// Listen for requests made to the server\nserver.on('request', async (req, res) => {\n  // Parse the request URL\n  const parsedUrl = url.parse(req.url);\n\n  // Extract the query parameters\n  const { text } = querystring.parse(parsedUrl.query);\n\n  // Set the response headers\n  res.setHeader('Content-Type', 'application/json');\n\n  let response;\n  if (parsedUrl.pathname === '/classify' && text) {\n    const classifier = await MyClassificationPipeline.getInstance();\n    response = await classifier(text);\n    res.statusCode = 200;\n  } else {\n    response = { 'error': 'Bad request' }\n    res.statusCode = 400;\n  }\n\n  // Send the JSON response\n  res.end(JSON.stringify(response));\n});\n\nserver.listen(port, hostname, () => {\n  console.log(`Server running at http://${hostname}:${port}/`);\n});\n\n```", "```py\nMyClassificationPipeline.getInstance();\n```", "```py\nnode app.js\n```", "```py\n{\"error\":\"Bad request\"}\n```", "```py\n[{\"label\":\"POSITIVE\",\"score\":0.9996721148490906}]\n```", "```py\nenv.cacheDir = './.cache';\n```", "```py\n// Specify a custom location for models (defaults to '/models/').\nenv.localModelPath = '/path/to/models/';\n```", "```py\n// Disable the loading of remote models from the Hugging Face Hub:\nenv.allowRemoteModels = false;\n```"]