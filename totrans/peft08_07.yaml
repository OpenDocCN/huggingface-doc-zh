- en: PEFT integrations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/tutorial/peft_integrations](https://huggingface.co/docs/peft/tutorial/peft_integrations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: PEFT’s practical benefits extends to other Hugging Face libraries like [Diffusers](https://hf.co/docs/diffusers)
    and [Transformers](https://hf.co/docs/transformers). One of the main benefits
    of PEFT is that an adapter file generated by a PEFT method is a lot smaller than
    the original model, which makes it super easy to manage and use multiple adapters.
    You can use one pretrained base model for multiple tasks by simply loading a new
    adapter finetuned for the task you’re solving. Or you can combine multiple adapters
    with a text-to-image diffusion model to create new effects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will show you how PEFT can help you manage adapters in Diffusers
    and Transformers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Diffusers
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diffusers is a generative AI library for creating images and videos from text
    or images with diffusion models. LoRA is an especially popular training method
    for diffusion models because you can very quickly train and share diffusion models
    to generate images in new styles. To make it easier to use and try multiple LoRA
    models, Diffusers uses the PEFT library to help manage different adapters for
    inference.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: For example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1)
    adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    method. The `adapter_name` argument in the loading method is enabled by PEFT and
    allows you to set a name for the adapter so it is easier to reference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/44d7d3f44fb0552b1f55c92fcbce7784.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Now let’s try another cool LoRA model, [ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora).
    All you need to do is load and name this new adapter with `adapter_name`, and
    use the [`set_adapters`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to set it as the currently active adapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/93e1d1297cecfeb1cad00cf824e024ed.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Finally, you can call the [`disable_lora`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora)
    method to restore the base model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Learn more about how PEFT supports Diffusers in the [Inference with PEFT](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference)
    tutorial.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers is a collection of pretrained models for all types of tasks in
    all modalities. You can load these models for training or inference. Many of the
    models are large language models (LLMs), so it makes sense to integrate PEFT with
    Transformers to manage and train adapters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Load a base pretrained model to train.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, add an adapter configuration to specify how to adapt the model parameters.
    Call the [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    method to add the configuration to the base model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now you can train the model with Transformer’s [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class or whichever training framework you prefer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: To use the newly trained model for inference, the [AutoModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    class uses PEFT on the backend to load the adapter weights and configuration file
    into a base pretrained model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you’re interested in comparing or using more than one adapter, you can also
    call the [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    method to add the adapter configuration to the base model. The only requirement
    is the adapter type must be the same (you can’t mix a LoRA and LoHa adapter).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Call [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    again to attach a new adapter to the base model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 再次调用[add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)以将新的适配器附加到基础模型。
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then you can use [set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.set_adapter)
    to set the currently active adapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用[set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.set_adapter)来设置当前活动的适配器。
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To disable the adapter, call the [disable_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.disable_adapter)
    method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用适配器，请调用[disable_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.disable_adapter)方法。
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you’re curious, check out the [Load and train adapters with PEFT](https://huggingface.co/docs/transformers/main/peft)
    tutorial to learn more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，可以查看[使用PEFT加载和训练适配器](https://huggingface.co/docs/transformers/main/peft)教程以了解更多。
