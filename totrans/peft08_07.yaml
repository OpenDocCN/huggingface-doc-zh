- en: PEFT integrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/tutorial/peft_integrations](https://huggingface.co/docs/peft/tutorial/peft_integrations)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/38.3d727cf6.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
  prefs: []
  type: TYPE_NORMAL
- en: PEFT’s practical benefits extends to other Hugging Face libraries like [Diffusers](https://hf.co/docs/diffusers)
    and [Transformers](https://hf.co/docs/transformers). One of the main benefits
    of PEFT is that an adapter file generated by a PEFT method is a lot smaller than
    the original model, which makes it super easy to manage and use multiple adapters.
    You can use one pretrained base model for multiple tasks by simply loading a new
    adapter finetuned for the task you’re solving. Or you can combine multiple adapters
    with a text-to-image diffusion model to create new effects.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will show you how PEFT can help you manage adapters in Diffusers
    and Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diffusers is a generative AI library for creating images and videos from text
    or images with diffusion models. LoRA is an especially popular training method
    for diffusion models because you can very quickly train and share diffusion models
    to generate images in new styles. To make it easier to use and try multiple LoRA
    models, Diffusers uses the PEFT library to help manage different adapters for
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: For example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1)
    adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    method. The `adapter_name` argument in the loading method is enabled by PEFT and
    allows you to set a name for the adapter so it is easier to reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/44d7d3f44fb0552b1f55c92fcbce7784.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let’s try another cool LoRA model, [ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora).
    All you need to do is load and name this new adapter with `adapter_name`, and
    use the [`set_adapters`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to set it as the currently active adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/93e1d1297cecfeb1cad00cf824e024ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, you can call the [`disable_lora`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora)
    method to restore the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about how PEFT supports Diffusers in the [Inference with PEFT](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference)
    tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers is a collection of pretrained models for all types of tasks in
    all modalities. You can load these models for training or inference. Many of the
    models are large language models (LLMs), so it makes sense to integrate PEFT with
    Transformers to manage and train adapters.
  prefs: []
  type: TYPE_NORMAL
- en: Load a base pretrained model to train.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, add an adapter configuration to specify how to adapt the model parameters.
    Call the [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    method to add the configuration to the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now you can train the model with Transformer’s [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class or whichever training framework you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: To use the newly trained model for inference, the [AutoModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    class uses PEFT on the backend to load the adapter weights and configuration file
    into a base pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you’re interested in comparing or using more than one adapter, you can also
    call the [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    method to add the adapter configuration to the base model. The only requirement
    is the adapter type must be the same (you can’t mix a LoRA and LoHa adapter).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Call [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    again to attach a new adapter to the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then you can use [set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.set_adapter)
    to set the currently active adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To disable the adapter, call the [disable_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.disable_adapter)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you’re curious, check out the [Load and train adapters with PEFT](https://huggingface.co/docs/transformers/main/peft)
    tutorial to learn more.
  prefs: []
  type: TYPE_NORMAL
