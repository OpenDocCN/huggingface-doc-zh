["```py\n( tokenizer: CLIPTokenizer text_encoder: ContextCLIPTextModel vae: AutoencoderKL unet: UNet2DConditionModel scheduler: PNDMScheduler qformer: Blip2QFormerModel image_processor: BlipImageProcessor ctx_begin_pos: int = 2 mean: List = None std: List = None )\n```", "```py\n( prompt: List reference_image: Image source_subject_category: List target_subject_category: List latents: Optional = None guidance_scale: float = 7.5 height: int = 512 width: int = 512 num_inference_steps: int = 50 generator: Union = None neg_prompt: Optional = '' prompt_strength: float = 1.0 prompt_reps: int = 20 output_type: Optional = 'pil' return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n>>> from diffusers.pipelines import BlipDiffusionPipeline\n>>> from diffusers.utils import load_image\n>>> import torch\n\n>>> blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained(\n...     \"Salesforce/blipdiffusion\", torch_dtype=torch.float16\n... ).to(\"cuda\")\n\n>>> cond_subject = \"dog\"\n>>> tgt_subject = \"dog\"\n>>> text_prompt_input = \"swimming underwater\"\n\n>>> cond_image = load_image(\n...     \"https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/dog.jpg\"\n... )\n>>> guidance_scale = 7.5\n>>> num_inference_steps = 25\n>>> negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n\n>>> output = blip_diffusion_pipe(\n...     text_prompt_input,\n...     cond_image,\n...     cond_subject,\n...     tgt_subject,\n...     guidance_scale=guidance_scale,\n...     num_inference_steps=num_inference_steps,\n...     neg_prompt=negative_prompt,\n...     height=512,\n...     width=512,\n... ).images\n>>> output[0].save(\"image.png\")\n```", "```py\n( tokenizer: CLIPTokenizer text_encoder: ContextCLIPTextModel vae: AutoencoderKL unet: UNet2DConditionModel scheduler: PNDMScheduler qformer: Blip2QFormerModel controlnet: ControlNetModel image_processor: BlipImageProcessor ctx_begin_pos: int = 2 mean: List = None std: List = None )\n```", "```py\n( prompt: List reference_image: Image condtioning_image: Image source_subject_category: List target_subject_category: List latents: Optional = None guidance_scale: float = 7.5 height: int = 512 width: int = 512 num_inference_steps: int = 50 generator: Union = None neg_prompt: Optional = '' prompt_strength: float = 1.0 prompt_reps: int = 20 output_type: Optional = 'pil' return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n>>> from diffusers.pipelines import BlipDiffusionControlNetPipeline\n>>> from diffusers.utils import load_image\n>>> from controlnet_aux import CannyDetector\n>>> import torch\n\n>>> blip_diffusion_pipe = BlipDiffusionControlNetPipeline.from_pretrained(\n...     \"Salesforce/blipdiffusion-controlnet\", torch_dtype=torch.float16\n... ).to(\"cuda\")\n\n>>> style_subject = \"flower\"\n>>> tgt_subject = \"teapot\"\n>>> text_prompt = \"on a marble table\"\n\n>>> cldm_cond_image = load_image(\n...     \"https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/kettle.jpg\"\n... ).resize((512, 512))\n>>> canny = CannyDetector()\n>>> cldm_cond_image = canny(cldm_cond_image, 30, 70, output_type=\"pil\")\n>>> style_image = load_image(\n...     \"https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/flower.jpg\"\n... )\n>>> guidance_scale = 7.5\n>>> num_inference_steps = 50\n>>> negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n\n>>> output = blip_diffusion_pipe(\n...     text_prompt,\n...     style_image,\n...     cldm_cond_image,\n...     style_subject,\n...     tgt_subject,\n...     guidance_scale=guidance_scale,\n...     num_inference_steps=num_inference_steps,\n...     neg_prompt=negative_prompt,\n...     height=512,\n...     width=512,\n... ).images\n>>> output[0].save(\"image.png\")\n```"]