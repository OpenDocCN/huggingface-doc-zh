["```py\n>>> from diffusers.pipelines import BlipDiffusionPipeline\n>>> from diffusers.utils import load_image\n>>> import torch\n\n>>> blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained(\n...     \"Salesforce/blipdiffusion\", torch_dtype=torch.float16\n... ).to(\"cuda\")\n\n>>> cond_subject = \"dog\"\n>>> tgt_subject = \"dog\"\n>>> text_prompt_input = \"swimming underwater\"\n\n>>> cond_image = load_image(\n...     \"https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/dog.jpg\"\n... )\n>>> guidance_scale = 7.5\n>>> num_inference_steps = 25\n>>> negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n\n>>> output = blip_diffusion_pipe(\n...     text_prompt_input,\n...     cond_image,\n...     cond_subject,\n...     tgt_subject,\n...     guidance_scale=guidance_scale,\n...     num_inference_steps=num_inference_steps,\n...     neg_prompt=negative_prompt,\n...     height=512,\n...     width=512,\n... ).images\n>>> output[0].save(\"image.png\")\n```", "```py\n>>> from diffusers.pipelines import BlipDiffusionControlNetPipeline\n>>> from diffusers.utils import load_image\n>>> from controlnet_aux import CannyDetector\n>>> import torch\n\n>>> blip_diffusion_pipe = BlipDiffusionControlNetPipeline.from_pretrained(\n...     \"Salesforce/blipdiffusion-controlnet\", torch_dtype=torch.float16\n... ).to(\"cuda\")\n\n>>> style_subject = \"flower\"\n>>> tgt_subject = \"teapot\"\n>>> text_prompt = \"on a marble table\"\n\n>>> cldm_cond_image = load_image(\n...     \"https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/kettle.jpg\"\n... ).resize((512, 512))\n>>> canny = CannyDetector()\n>>> cldm_cond_image = canny(cldm_cond_image, 30, 70, output_type=\"pil\")\n>>> style_image = load_image(\n...     \"https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/flower.jpg\"\n... )\n>>> guidance_scale = 7.5\n>>> num_inference_steps = 50\n>>> negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n\n>>> output = blip_diffusion_pipe(\n...     text_prompt,\n...     style_image,\n...     cldm_cond_image,\n...     style_subject,\n...     tgt_subject,\n...     guidance_scale=guidance_scale,\n...     num_inference_steps=num_inference_steps,\n...     neg_prompt=negative_prompt,\n...     height=512,\n...     width=512,\n... ).images\n>>> output[0].save(\"image.png\")\n```"]