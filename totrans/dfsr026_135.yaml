- en: Consistency Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性模型
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/consistency_models](https://huggingface.co/docs/diffusers/api/pipelines/consistency_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/consistency_models](https://huggingface.co/docs/diffusers/api/pipelines/consistency_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Consistency Models were proposed in [Consistency Models](https://huggingface.co/papers/2303.01469)
    by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性模型是由杨松、Prafulla Dhariwal、Mark Chen和Ilya Sutskever在[一致性模型](https://huggingface.co/papers/2303.01469)中提出的。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Diffusion models have significantly advanced the fields of image, audio, and
    video generation, but they depend on an iterative sampling process that causes
    slow generation. To overcome this limitation, we propose consistency models, a
    new family of models that generate high quality samples by directly mapping noise
    to data. They support fast one-step generation by design, while still allowing
    multistep sampling to trade compute for sample quality. They also support zero-shot
    data editing, such as image inpainting, colorization, and super-resolution, without
    requiring explicit training on these tasks. Consistency models can be trained
    either by distilling pre-trained diffusion models, or as standalone generative
    models altogether. Through extensive experiments, we demonstrate that they outperform
    existing distillation techniques for diffusion models in one- and few-step sampling,
    achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet
    64x64 for one-step generation. When trained in isolation, consistency models become
    a new family of generative models that can outperform existing one-step, non-adversarial
    generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and
    LSUN 256x256.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*扩散模型显著推动了图像、音频和视频生成领域的发展，但它们依赖于引起生成速度缓慢的迭代采样过程。为了克服这一限制，我们提出了一致性模型，这是一类通过直接将噪声映射到数据来生成高质量样本的新模型。它们通过设计支持快速一步生成，同时仍允许多步采样以在计算和样本质量之间进行交换。它们还支持零样本数据编辑，如图像修补、着色和超分辨率，而无需在这些任务上进行明确训练。一致性模型可以通过蒸馏预训练的扩散模型或作为独立的生成模型进行训练。通过大量实验，我们证明它们在一步和少步采样方面优于现有的扩散模型蒸馏技术，实现了CIFAR-10的新的FID最佳值为3.55，ImageNet
    64x64为6.20。当单独训练时，一致性模型成为一类新的生成模型，可以在标准基准测试中优于现有的一步、非对抗性生成模型，如CIFAR-10、ImageNet
    64x64和LSUN 256x256。*'
- en: The original codebase can be found at [openai/consistency_models](https://github.com/openai/consistency_models),
    and additional checkpoints are available at [openai](https://huggingface.co/openai).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码库可以在[openai/consistency_models](https://github.com/openai/consistency_models)找到，还可以在[openai](https://huggingface.co/openai)获得额外的检查点。
- en: The pipeline was contributed by [dg845](https://github.com/dg845) and [ayushtues](https://huggingface.co/ayushtues).
    ❤️
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道是由[dg845](https://github.com/dg845)和[ayushtues](https://huggingface.co/ayushtues)贡献的。❤️
- en: Tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: 'For an additional speed-up, use `torch.compile` to generate multiple images
    in <1 second:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步加速，使用`torch.compile`在<1秒内生成多个图像：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ConsistencyModelPipeline
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConsistencyModelPipeline
- en: '### `class diffusers.ConsistencyModelPipeline`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ConsistencyModelPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py#L63)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py#L63)'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`unet` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — A `UNet2DModel` to denoise the encoded image latents.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel））—
    用于去噪编码图像潜在特征的`UNet2DModel`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Currently only compatible with [CMStochasticIterativeScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/cm_stochastic_iterative#diffusers.CMStochasticIterativeScheduler).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin））—
    与`unet`结合使用以去噪编码图像潜在特征的调度器。目前仅与[CMStochasticIterativeScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/cm_stochastic_iterative#diffusers.CMStochasticIterativeScheduler)兼容。'
- en: Pipeline for unconditional or class-conditional image generation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 用于无条件或类别条件图像生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py#L167)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py#L167)'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_size` (`int`, *optional*, defaults to 1) — The number of images to generate.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）— 要生成的图像数量。'
- en: '`class_labels` (`torch.Tensor` or `List[int]` or `int`, *optional*) — Optional
    class labels for conditioning class-conditional consistency models. Not used if
    the model is not class-conditional.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_labels`（`torch.Tensor`或`List[int]`或`int`，*可选*）— 用于条件类别一致性模型的可选类别标签。如果模型不是类别条件的，则不使用。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 1) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，*可选*，默认为1）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`timesteps` (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps`（`List[int]`，*可选*）— 用于去噪过程的自定义时间步长。如果未定义，则使用等间距的`num_inference_steps`时间步长。必须按降序排列。'
- en: '`generator` (`torch.Generator`, *optional*) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（`torch.Generator`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`（`torch.FloatTensor`，*可选*）— 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*，默认为`True`）— 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通的元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`（`Callable`，*可选*）— 在推断期间每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps`（`int`，*可选*，默认为1）— 调用`callback`函数的频率。如果未指定，则在每一步调用回调。'
- en: Returns
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。
- en: 'Examples:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ImagePipelineOutput
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImagePipelineOutput
- en: '### `class diffusers.ImagePipelineOutput`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ImagePipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images`（`List[PIL.Image.Image]`或`np.ndarray`）— 长度为`batch_size`的去噪PIL图像列表或形状为`(batch_size,
    height, width, num_channels)`的NumPy数组。'
- en: Output class for image pipelines.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图像管道的输出类。
