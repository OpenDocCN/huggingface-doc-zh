- en: Export a model to Inferentia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型导出到Inferentia
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/guides/export_model](https://huggingface.co/docs/optimum-neuron/guides/export_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/optimum-neuron/guides/export_model](https://huggingface.co/docs/optimum-neuron/guides/export_model)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Exporting a PyTorch model to Neuron model is as simple as
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 将PyTorch模型导出为Neuron模型就像这样简单
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Check out the help for more options:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多选项的帮助：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Why compile to Neuron model?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要编译成Neuron模型？
- en: 'AWS provides two generations of the Inferentia accelerator built for machine
    learning inference with higher throughput, lower latency but lower cost: [inf2
    (NeuronCore-v2)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf2-arch.html)
    and [inf1 (NeuronCore-v1)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf1-arch.html#aws-inf1-arch).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了两代用于机器学习推理的Inferentia加速器，具有更高的吞吐量、更低的延迟但更低的成本：[inf2（NeuronCore-v2）](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf2-arch.html)
    和 [inf1（NeuronCore-v1）](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf1-arch.html#aws-inf1-arch)。
- en: In production environments, to deploy 🤗 [Transformers](https://huggingface.co/docs/transformers/index)
    models on Neuron devices, you need to compile your models and export them to a
    serialized format before inference. Through Ahead-Of-Time (AOT) compilation with
    Neuron Compiler( [neuronx-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html)
    or [neuron-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuron-cc/neuron-cc.html)
    ), your models will be converted to serialized and optimized [TorchScript modules](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，要在Neuron设备上部署🤗 [Transformers](https://huggingface.co/docs/transformers/index)
    模型，您需要在推理之前将模型编译并导出到序列化格式。通过使用Neuron编译器（[neuronx-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html)
    或 [neuron-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuron-cc/neuron-cc.html)
    ）进行提前编译，您的模型将被转换为序列化和优化的[TorchScript模块](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)。
- en: 'To understand a little bit more about the compilation, here are general steps
    executed under the hood: ![Compilation flow](../Images/3da5312c8a92cea4b2c1cbc3d4bb83f0.png
    "Compilation flow")'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解编译过程，这里是在幕后执行的一般步骤：![编译流程](../Images/3da5312c8a92cea4b2c1cbc3d4bb83f0.png
    "编译流程")
- en: '**NEFF**: Neuron Executable File Format which is a binary executable on Neuron
    devices.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**NEFF**：Neuron可执行文件格式，是Neuron设备上的二进制可执行文件。'
- en: 'Although pre-compilation avoids overhead during the inference, traced Neuron
    module has some limitations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预编译可以避免推理期间的开销，但跟踪的Neuron模块有一些限制：
- en: Traced Neuron module will be static, which requires fixed input shapes and data
    types used during the compilation. As the model won’t be dynamically recompiled,
    the inference will fail if any of the above conditions change. (*But these limitations
    could be bypass with [dynamic batching](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching)
    and [bucketing](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuron/bucketing-app-note.html#bucketing-app-note)*).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪的Neuron模块将是静态的，这需要在编译期间使用固定的输入形状和数据类型。由于模型不会动态重新编译，如果上述条件中的任何一个发生变化，推理将失败。(*但这些限制可以通过[动态批处理](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching)和[分桶](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuron/bucketing-app-note.html#bucketing-app-note)*)。
- en: 'Neuron models are hardware-specialized, which means:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neuron模型是硬件专用的，这意味着：
- en: Models traced with Neuron can no longer be executed in non-Neuron environment.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Neuron跟踪的模型将无法在非Neuron环境中执行。
- en: Models compiled for inf1 (NeuronCore-v1) are not compatible with inf2 (NeuronCore-v2),
    and vice versa.
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为inf1（NeuronCore-v1）编译的模型与inf2（NeuronCore-v2）不兼容，反之亦然。
- en: In this guide, we’ll show you how to export your models to serialized models
    optimized for Neuron devices.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们将向您展示如何将您的模型导出为针对Neuron设备进行优化的序列化模型。
- en: 🤗 Optimum provides support for the Neuron export by leveraging configuration
    objects. These configuration objects come ready made for a number of model architectures,
    and are designed to be easily extendable to other architectures.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Optimum通过利用配置对象提供了对Neuron导出的支持。这些配置对象已经为许多模型架构准备好，并且设计为易于扩展到其他架构。
- en: '**To check the supported architectures, go to the [configuration reference
    page](../package_reference/configuration).**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**要检查支持的架构，请转到[配置参考页面](../package_reference/configuration)。**'
- en: Exporting a model to Neuron using the CLI
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CLI将模型导出到Neuron
- en: 'To export a 🤗 Transformers model to Neuron, you’ll first need to install some
    extra dependencies:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将🤗 Transformers模型导出到Neuron，您首先需要安装一些额外的依赖项：
- en: '**For Inf2**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于Inf2**'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**For Inf1**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于Inf1**'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Optimum Neuron export can be used through Optimum command-line:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的Neuron导出可以通过Optimum命令行使用：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the last section, you can see some input shape options to pass for exporting
    static neuron model, meaning that exact shape inputs should be used during the
    inference as given during compilation. If you are going to use variable-size inputs,
    you can pad your inputs to the shape used for compilation as a workaround. If
    you want the batch size to be dynamic, you can pass `--dynamic-batch-size` to
    enable dynamic batching, which means that you will be able to use inputs with
    difference batch size during inference, but it comes with a potential tradeoff
    in terms of latency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，您可以看到一些输入形状选项，用于传递给导出静态神经元模型，这意味着在推理期间应使用与编译期间给定的确切形状输入相同的输入。如果您要使用可变大小的输入，您可以将输入填充到用于编译的形状作为解决方法。如果要使批处理大小是动态的，可以传递`--dynamic-batch-size`以启用动态批处理，这意味着您将能够在推理期间使用不同批处理大小的输入，但这可能会在延迟方面产生潜在的折衷。
- en: 'Exporting a checkpoint can be done as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 导出检查点可以按以下方式完成：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should see the following logs which validate the model on Neuron devices
    by comparing with PyTorch model on CPU:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下日志，通过与CPU上的PyTorch模型进行比较，验证在Neuron设备上的模型：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This exports a neuron-compiled TorchScript module of the checkpoint defined
    by the `--model` argument.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导出由`--model`参数定义的检查点的神经元编译的TorchScript模块。
- en: 'As you can see, the task was automatically detected. This was possible because
    the model was on the Hub. For local models, providing the `--task` argument is
    needed or it will default to the model architecture without any task specific
    head:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，任务已被自动检测到。这是因为模型在Hub上。对于本地模型，需要提供`--task`参数，否则将默认为没有任何特定任务头的模型架构：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that providing the `--task` argument for a model on the Hub will disable
    the automatic task detection. The resulting `model.neuron` file, can then be loaded
    and run on Neuron devices.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于Hub上的模型，提供`--task`参数将禁用自动任务检测。然后，生成的`model.neuron`文件可以加载并在Neuron设备上运行。
- en: Exporting a model to Neuron via NeuronModel
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过NeuronModel将模型导出到Neuron
- en: 'You will also be able to export your models to Neuron format with `optimum.neuron.NeuronModelForXXX`
    model classes. Here is an example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`optimum.neuron.NeuronModelForXXX`模型类将模型导出到Neuron格式。这里是一个例子：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And the exported model can be used for inference directly with the `NeuronModelForXXX`
    class:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 导出的模型可以直接使用`NeuronModelForXXX`类进行推断：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Exporting Stable Diffusion to Neuron
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将稳定扩散导出到Neuron
- en: With the Optimum CLI you can compile components in the Stable Diffusion pipeline
    to gain acceleration on neuron devices during the inference.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Optimum CLI，您可以编译稳定扩散管道中的组件，以在推断期间在神经元设备上获得加速。
- en: 'So far, we support the export of following components in the pipeline:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们支持在管道中导出以下组件：
- en: CLIP text encoder
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP文本编码器
- en: U-Net
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net
- en: VAE encoder
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAE编码器
- en: VAE decoder
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAE解码器
- en: “These blocks are chosen because they represent the bulk of the compute in the
    pipeline, and performance benchmarking has shown that running them on Neuron yields
    significant performance benefit.”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: “选择这些块是因为它们代表管道中的大部分计算量，并且性能基准测试表明在Neuron上运行它们会带来显著的性能优势。”
- en: Besides, don’t hesitate to tweak the compilation configuration to find the best
    tradeoff between performance v.s accuracy in your use case. By default, we suggest
    casting FP32 matrix multiplication operations to BF16 which offers good performance
    with moderate sacrifice of the accuracy. Check out the guide from [AWS Neuron
    documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)
    to better understand the options for your compilation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请随时调整编译配置，以在您的用例中找到性能与准确性之间的最佳权衡。默认情况下，我们建议将FP32矩阵乘法运算转换为BF16，这在适度牺牲准确性的情况下提供良好的性能。查看[AWS
    Neuron文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)中的指南，以更好地了解编译选项。
- en: 'Exporting a stable diffusion checkpoint can be done using the CLI:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用CLI导出稳定扩散检查点：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Exporting Stable Diffusion XL to Neuron
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将稳定扩散XL导出到Neuron
- en: Similar to Stable Diffusion, you will be able to use Optimum CLI to compile
    components in the SDXL pipeline for inference on neuron devices.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与稳定扩散类似，您将能够使用Optimum CLI在SDXL管道上编译组件，以便在神经元设备上进行推断。
- en: 'We support the export of following components in the pipeline to boost the
    speed:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们支持将以下组件导出到管道中以提高速度：
- en: Text encoder
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本编码器
- en: Second text encoder
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个文本编码器
- en: U-Net (a three times larger UNet than the one in Stable Diffusion pipeline)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net（比稳定扩散管道中的UNet大三倍）
- en: VAE encoder
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAE编码器
- en: VAE decoder
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAE解码器
- en: “Stable Diffusion XL works especially well with images between 768 and 1024.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: “稳定扩散XL在768到1024之间的图像上表现特别好。”
- en: 'Exporting a SDXL checkpoint can be done using the CLI:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用CLI导出SDXL检查点：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Selecting a task
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个任务
- en: Specifying a `--task` should not be necessary in most cases when exporting from
    a model on the Hugging Face Hub.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在从Hugging Face Hub上的模型导出时，大多数情况下不需要指定`--task`。
- en: However, in case you need to check for a given a model architecture what tasks
    the Neuron export supports, we got you covered. First, you can check the list
    of supported tasks [here](https://huggingface.co/docs/optimum/exporters/task_manager#pytorch).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您需要检查给定模型架构的Neuron导出支持哪些任务，我们已经为您提供了。首先，您可以在[这里](https://huggingface.co/docs/optimum/exporters/task_manager#pytorch)检查支持的任务列表。
- en: 'For each model architecture, you can find the list of supported tasks via the
    `~exporters.tasks.TasksManager`. For example, for DistilBERT, for the Neuron export,
    we have:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型架构，您可以通过`~exporters.tasks.TasksManager`找到支持的任务列表。例如，对于DistilBERT，对于Neuron导出，我们有：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can then pass one of these tasks to the `--task` argument in the `optimum-cli
    export neuron` command, as mentioned above.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以将这些任务之一传递给`optimum-cli export neuron`命令中的`--task`参数，如上所述。
