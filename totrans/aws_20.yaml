- en: Export a model to Inferentia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹å¯¼å‡ºåˆ°Inferentia
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/guides/export_model](https://huggingface.co/docs/optimum-neuron/guides/export_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/optimum-neuron/guides/export_model](https://huggingface.co/docs/optimum-neuron/guides/export_model)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: Exporting a PyTorch model to Neuron model is as simple as
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å°†PyTorchæ¨¡å‹å¯¼å‡ºä¸ºNeuronæ¨¡å‹å°±åƒè¿™æ ·ç®€å•
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Check out the help for more options:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ›´å¤šé€‰é¡¹çš„å¸®åŠ©ï¼š
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Why compile to Neuron model?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦ç¼–è¯‘æˆNeuronæ¨¡å‹ï¼Ÿ
- en: 'AWS provides two generations of the Inferentia accelerator built for machine
    learning inference with higher throughput, lower latency but lower cost: [inf2
    (NeuronCore-v2)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf2-arch.html)
    and [inf1 (NeuronCore-v1)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf1-arch.html#aws-inf1-arch).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AWSæä¾›äº†ä¸¤ä»£ç”¨äºæœºå™¨å­¦ä¹ æ¨ç†çš„InferentiaåŠ é€Ÿå™¨ï¼Œå…·æœ‰æ›´é«˜çš„ååé‡ã€æ›´ä½çš„å»¶è¿Ÿä½†æ›´ä½çš„æˆæœ¬ï¼š[inf2ï¼ˆNeuronCore-v2ï¼‰](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf2-arch.html)
    å’Œ [inf1ï¼ˆNeuronCore-v1ï¼‰](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf1-arch.html#aws-inf1-arch)ã€‚
- en: In production environments, to deploy ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index)
    models on Neuron devices, you need to compile your models and export them to a
    serialized format before inference. Through Ahead-Of-Time (AOT) compilation with
    Neuron Compiler( [neuronx-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html)
    or [neuron-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuron-cc/neuron-cc.html)
    ), your models will be converted to serialized and optimized [TorchScript modules](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¦åœ¨Neuronè®¾å¤‡ä¸Šéƒ¨ç½²ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index)
    æ¨¡å‹ï¼Œæ‚¨éœ€è¦åœ¨æ¨ç†ä¹‹å‰å°†æ¨¡å‹ç¼–è¯‘å¹¶å¯¼å‡ºåˆ°åºåˆ—åŒ–æ ¼å¼ã€‚é€šè¿‡ä½¿ç”¨Neuronç¼–è¯‘å™¨ï¼ˆ[neuronx-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html)
    æˆ– [neuron-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuron-cc/neuron-cc.html)
    ï¼‰è¿›è¡Œæå‰ç¼–è¯‘ï¼Œæ‚¨çš„æ¨¡å‹å°†è¢«è½¬æ¢ä¸ºåºåˆ—åŒ–å’Œä¼˜åŒ–çš„[TorchScriptæ¨¡å—](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)ã€‚
- en: 'To understand a little bit more about the compilation, here are general steps
    executed under the hood: ![Compilation flow](../Images/3da5312c8a92cea4b2c1cbc3d4bb83f0.png
    "Compilation flow")'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°äº†è§£ç¼–è¯‘è¿‡ç¨‹ï¼Œè¿™é‡Œæ˜¯åœ¨å¹•åæ‰§è¡Œçš„ä¸€èˆ¬æ­¥éª¤ï¼š![ç¼–è¯‘æµç¨‹](../Images/3da5312c8a92cea4b2c1cbc3d4bb83f0.png
    "ç¼–è¯‘æµç¨‹")
- en: '**NEFF**: Neuron Executable File Format which is a binary executable on Neuron
    devices.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**NEFF**ï¼šNeuronå¯æ‰§è¡Œæ–‡ä»¶æ ¼å¼ï¼Œæ˜¯Neuronè®¾å¤‡ä¸Šçš„äºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶ã€‚'
- en: 'Although pre-compilation avoids overhead during the inference, traced Neuron
    module has some limitations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡é¢„ç¼–è¯‘å¯ä»¥é¿å…æ¨ç†æœŸé—´çš„å¼€é”€ï¼Œä½†è·Ÿè¸ªçš„Neuronæ¨¡å—æœ‰ä¸€äº›é™åˆ¶ï¼š
- en: Traced Neuron module will be static, which requires fixed input shapes and data
    types used during the compilation. As the model wonâ€™t be dynamically recompiled,
    the inference will fail if any of the above conditions change. (*But these limitations
    could be bypass with [dynamic batching](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching)
    and [bucketing](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuron/bucketing-app-note.html#bucketing-app-note)*).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªçš„Neuronæ¨¡å—å°†æ˜¯é™æ€çš„ï¼Œè¿™éœ€è¦åœ¨ç¼–è¯‘æœŸé—´ä½¿ç”¨å›ºå®šçš„è¾“å…¥å½¢çŠ¶å’Œæ•°æ®ç±»å‹ã€‚ç”±äºæ¨¡å‹ä¸ä¼šåŠ¨æ€é‡æ–°ç¼–è¯‘ï¼Œå¦‚æœä¸Šè¿°æ¡ä»¶ä¸­çš„ä»»ä½•ä¸€ä¸ªå‘ç”Ÿå˜åŒ–ï¼Œæ¨ç†å°†å¤±è´¥ã€‚(*ä½†è¿™äº›é™åˆ¶å¯ä»¥é€šè¿‡[åŠ¨æ€æ‰¹å¤„ç†](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching)å’Œ[åˆ†æ¡¶](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuron/bucketing-app-note.html#bucketing-app-note)*)ã€‚
- en: 'Neuron models are hardware-specialized, which means:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neuronæ¨¡å‹æ˜¯ç¡¬ä»¶ä¸“ç”¨çš„ï¼Œè¿™æ„å‘³ç€ï¼š
- en: Models traced with Neuron can no longer be executed in non-Neuron environment.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Neuronè·Ÿè¸ªçš„æ¨¡å‹å°†æ— æ³•åœ¨éNeuronç¯å¢ƒä¸­æ‰§è¡Œã€‚
- en: Models compiled for inf1 (NeuronCore-v1) are not compatible with inf2 (NeuronCore-v2),
    and vice versa.
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºinf1ï¼ˆNeuronCore-v1ï¼‰ç¼–è¯‘çš„æ¨¡å‹ä¸inf2ï¼ˆNeuronCore-v2ï¼‰ä¸å…¼å®¹ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: In this guide, weâ€™ll show you how to export your models to serialized models
    optimized for Neuron devices.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å°†æ‚¨çš„æ¨¡å‹å¯¼å‡ºä¸ºé’ˆå¯¹Neuronè®¾å¤‡è¿›è¡Œä¼˜åŒ–çš„åºåˆ—åŒ–æ¨¡å‹ã€‚
- en: ğŸ¤— Optimum provides support for the Neuron export by leveraging configuration
    objects. These configuration objects come ready made for a number of model architectures,
    and are designed to be easily extendable to other architectures.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimumé€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›äº†å¯¹Neuronå¯¼å‡ºçš„æ”¯æŒã€‚è¿™äº›é…ç½®å¯¹è±¡å·²ç»ä¸ºè®¸å¤šæ¨¡å‹æ¶æ„å‡†å¤‡å¥½ï¼Œå¹¶ä¸”è®¾è®¡ä¸ºæ˜“äºæ‰©å±•åˆ°å…¶ä»–æ¶æ„ã€‚
- en: '**To check the supported architectures, go to the [configuration reference
    page](../package_reference/configuration).**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¦æ£€æŸ¥æ”¯æŒçš„æ¶æ„ï¼Œè¯·è½¬åˆ°[é…ç½®å‚è€ƒé¡µé¢](../package_reference/configuration)ã€‚**'
- en: Exporting a model to Neuron using the CLI
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨CLIå°†æ¨¡å‹å¯¼å‡ºåˆ°Neuron
- en: 'To export a ğŸ¤— Transformers model to Neuron, youâ€™ll first need to install some
    extra dependencies:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºåˆ°Neuronï¼Œæ‚¨é¦–å…ˆéœ€è¦å®‰è£…ä¸€äº›é¢å¤–çš„ä¾èµ–é¡¹ï¼š
- en: '**For Inf2**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯¹äºInf2**'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**For Inf1**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯¹äºInf1**'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Optimum Neuron export can be used through Optimum command-line:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³çš„Neuronå¯¼å‡ºå¯ä»¥é€šè¿‡Optimumå‘½ä»¤è¡Œä½¿ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the last section, you can see some input shape options to pass for exporting
    static neuron model, meaning that exact shape inputs should be used during the
    inference as given during compilation. If you are going to use variable-size inputs,
    you can pad your inputs to the shape used for compilation as a workaround. If
    you want the batch size to be dynamic, you can pass `--dynamic-batch-size` to
    enable dynamic batching, which means that you will be able to use inputs with
    difference batch size during inference, but it comes with a potential tradeoff
    in terms of latency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åä¸€èŠ‚ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸€äº›è¾“å…¥å½¢çŠ¶é€‰é¡¹ï¼Œç”¨äºä¼ é€’ç»™å¯¼å‡ºé™æ€ç¥ç»å…ƒæ¨¡å‹ï¼Œè¿™æ„å‘³ç€åœ¨æ¨ç†æœŸé—´åº”ä½¿ç”¨ä¸ç¼–è¯‘æœŸé—´ç»™å®šçš„ç¡®åˆ‡å½¢çŠ¶è¾“å…¥ç›¸åŒçš„è¾“å…¥ã€‚å¦‚æœæ‚¨è¦ä½¿ç”¨å¯å˜å¤§å°çš„è¾“å…¥ï¼Œæ‚¨å¯ä»¥å°†è¾“å…¥å¡«å……åˆ°ç”¨äºç¼–è¯‘çš„å½¢çŠ¶ä½œä¸ºè§£å†³æ–¹æ³•ã€‚å¦‚æœè¦ä½¿æ‰¹å¤„ç†å¤§å°æ˜¯åŠ¨æ€çš„ï¼Œå¯ä»¥ä¼ é€’`--dynamic-batch-size`ä»¥å¯ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼Œè¿™æ„å‘³ç€æ‚¨å°†èƒ½å¤Ÿåœ¨æ¨ç†æœŸé—´ä½¿ç”¨ä¸åŒæ‰¹å¤„ç†å¤§å°çš„è¾“å…¥ï¼Œä½†è¿™å¯èƒ½ä¼šåœ¨å»¶è¿Ÿæ–¹é¢äº§ç”Ÿæ½œåœ¨çš„æŠ˜è¡·ã€‚
- en: 'Exporting a checkpoint can be done as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼å‡ºæ£€æŸ¥ç‚¹å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼å®Œæˆï¼š
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should see the following logs which validate the model on Neuron devices
    by comparing with PyTorch model on CPU:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥çœ‹åˆ°ä»¥ä¸‹æ—¥å¿—ï¼Œé€šè¿‡ä¸CPUä¸Šçš„PyTorchæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼ŒéªŒè¯åœ¨Neuronè®¾å¤‡ä¸Šçš„æ¨¡å‹ï¼š
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This exports a neuron-compiled TorchScript module of the checkpoint defined
    by the `--model` argument.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å¯¼å‡ºç”±`--model`å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ç¥ç»å…ƒç¼–è¯‘çš„TorchScriptæ¨¡å—ã€‚
- en: 'As you can see, the task was automatically detected. This was possible because
    the model was on the Hub. For local models, providing the `--task` argument is
    needed or it will default to the model architecture without any task specific
    head:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œä»»åŠ¡å·²è¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨Hubä¸Šã€‚å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œéœ€è¦æä¾›`--task`å‚æ•°ï¼Œå¦åˆ™å°†é»˜è®¤ä¸ºæ²¡æœ‰ä»»ä½•ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹æ¶æ„ï¼š
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that providing the `--task` argument for a model on the Hub will disable
    the automatic task detection. The resulting `model.neuron` file, can then be loaded
    and run on Neuron devices.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¯¹äºHubä¸Šçš„æ¨¡å‹ï¼Œæä¾›`--task`å‚æ•°å°†ç¦ç”¨è‡ªåŠ¨ä»»åŠ¡æ£€æµ‹ã€‚ç„¶åï¼Œç”Ÿæˆçš„`model.neuron`æ–‡ä»¶å¯ä»¥åŠ è½½å¹¶åœ¨Neuronè®¾å¤‡ä¸Šè¿è¡Œã€‚
- en: Exporting a model to Neuron via NeuronModel
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡NeuronModelå°†æ¨¡å‹å¯¼å‡ºåˆ°Neuron
- en: 'You will also be able to export your models to Neuron format with `optimum.neuron.NeuronModelForXXX`
    model classes. Here is an example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`optimum.neuron.NeuronModelForXXX`æ¨¡å‹ç±»å°†æ¨¡å‹å¯¼å‡ºåˆ°Neuronæ ¼å¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And the exported model can be used for inference directly with the `NeuronModelForXXX`
    class:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼å‡ºçš„æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨`NeuronModelForXXX`ç±»è¿›è¡Œæ¨æ–­ï¼š
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Exporting Stable Diffusion to Neuron
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†ç¨³å®šæ‰©æ•£å¯¼å‡ºåˆ°Neuron
- en: With the Optimum CLI you can compile components in the Stable Diffusion pipeline
    to gain acceleration on neuron devices during the inference.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Optimum CLIï¼Œæ‚¨å¯ä»¥ç¼–è¯‘ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„ç»„ä»¶ï¼Œä»¥åœ¨æ¨æ–­æœŸé—´åœ¨ç¥ç»å…ƒè®¾å¤‡ä¸Šè·å¾—åŠ é€Ÿã€‚
- en: 'So far, we support the export of following components in the pipeline:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ”¯æŒåœ¨ç®¡é“ä¸­å¯¼å‡ºä»¥ä¸‹ç»„ä»¶ï¼š
- en: CLIP text encoder
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIPæ–‡æœ¬ç¼–ç å™¨
- en: U-Net
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net
- en: VAE encoder
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEç¼–ç å™¨
- en: VAE decoder
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEè§£ç å™¨
- en: â€œThese blocks are chosen because they represent the bulk of the compute in the
    pipeline, and performance benchmarking has shown that running them on Neuron yields
    significant performance benefit.â€
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: â€œé€‰æ‹©è¿™äº›å—æ˜¯å› ä¸ºå®ƒä»¬ä»£è¡¨ç®¡é“ä¸­çš„å¤§éƒ¨åˆ†è®¡ç®—é‡ï¼Œå¹¶ä¸”æ€§èƒ½åŸºå‡†æµ‹è¯•è¡¨æ˜åœ¨Neuronä¸Šè¿è¡Œå®ƒä»¬ä¼šå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚â€
- en: Besides, donâ€™t hesitate to tweak the compilation configuration to find the best
    tradeoff between performance v.s accuracy in your use case. By default, we suggest
    casting FP32 matrix multiplication operations to BF16 which offers good performance
    with moderate sacrifice of the accuracy. Check out the guide from [AWS Neuron
    documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)
    to better understand the options for your compilation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¯·éšæ—¶è°ƒæ•´ç¼–è¯‘é…ç½®ï¼Œä»¥åœ¨æ‚¨çš„ç”¨ä¾‹ä¸­æ‰¾åˆ°æ€§èƒ½ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æœ€ä½³æƒè¡¡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å»ºè®®å°†FP32çŸ©é˜µä¹˜æ³•è¿ç®—è½¬æ¢ä¸ºBF16ï¼Œè¿™åœ¨é€‚åº¦ç‰ºç‰²å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æä¾›è‰¯å¥½çš„æ€§èƒ½ã€‚æŸ¥çœ‹[AWS
    Neuronæ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)ä¸­çš„æŒ‡å—ï¼Œä»¥æ›´å¥½åœ°äº†è§£ç¼–è¯‘é€‰é¡¹ã€‚
- en: 'Exporting a stable diffusion checkpoint can be done using the CLI:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨CLIå¯¼å‡ºç¨³å®šæ‰©æ•£æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Exporting Stable Diffusion XL to Neuron
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†ç¨³å®šæ‰©æ•£XLå¯¼å‡ºåˆ°Neuron
- en: Similar to Stable Diffusion, you will be able to use Optimum CLI to compile
    components in the SDXL pipeline for inference on neuron devices.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç¨³å®šæ‰©æ•£ç±»ä¼¼ï¼Œæ‚¨å°†èƒ½å¤Ÿä½¿ç”¨Optimum CLIåœ¨SDXLç®¡é“ä¸Šç¼–è¯‘ç»„ä»¶ï¼Œä»¥ä¾¿åœ¨ç¥ç»å…ƒè®¾å¤‡ä¸Šè¿›è¡Œæ¨æ–­ã€‚
- en: 'We support the export of following components in the pipeline to boost the
    speed:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ”¯æŒå°†ä»¥ä¸‹ç»„ä»¶å¯¼å‡ºåˆ°ç®¡é“ä¸­ä»¥æé«˜é€Ÿåº¦ï¼š
- en: Text encoder
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç¼–ç å™¨
- en: Second text encoder
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨
- en: U-Net (a three times larger UNet than the one in Stable Diffusion pipeline)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Netï¼ˆæ¯”ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„UNetå¤§ä¸‰å€ï¼‰
- en: VAE encoder
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEç¼–ç å™¨
- en: VAE decoder
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEè§£ç å™¨
- en: â€œStable Diffusion XL works especially well with images between 768 and 1024.â€
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: â€œç¨³å®šæ‰©æ•£XLåœ¨768åˆ°1024ä¹‹é—´çš„å›¾åƒä¸Šè¡¨ç°ç‰¹åˆ«å¥½ã€‚â€
- en: 'Exporting a SDXL checkpoint can be done using the CLI:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨CLIå¯¼å‡ºSDXLæ£€æŸ¥ç‚¹ï¼š
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Selecting a task
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªä»»åŠ¡
- en: Specifying a `--task` should not be necessary in most cases when exporting from
    a model on the Hugging Face Hub.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»Hugging Face Hubä¸Šçš„æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ä¸éœ€è¦æŒ‡å®š`--task`ã€‚
- en: However, in case you need to check for a given a model architecture what tasks
    the Neuron export supports, we got you covered. First, you can check the list
    of supported tasks [here](https://huggingface.co/docs/optimum/exporters/task_manager#pytorch).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå¦‚æœæ‚¨éœ€è¦æ£€æŸ¥ç»™å®šæ¨¡å‹æ¶æ„çš„Neuronå¯¼å‡ºæ”¯æŒå“ªäº›ä»»åŠ¡ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨æä¾›äº†ã€‚é¦–å…ˆï¼Œæ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/docs/optimum/exporters/task_manager#pytorch)æ£€æŸ¥æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚
- en: 'For each model architecture, you can find the list of supported tasks via the
    `~exporters.tasks.TasksManager`. For example, for DistilBERT, for the Neuron export,
    we have:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæ¨¡å‹æ¶æ„ï¼Œæ‚¨å¯ä»¥é€šè¿‡`~exporters.tasks.TasksManager`æ‰¾åˆ°æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œå¯¹äºDistilBERTï¼Œå¯¹äºNeuronå¯¼å‡ºï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can then pass one of these tasks to the `--task` argument in the `optimum-cli
    export neuron` command, as mentioned above.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥å°†è¿™äº›ä»»åŠ¡ä¹‹ä¸€ä¼ é€’ç»™`optimum-cli export neuron`å‘½ä»¤ä¸­çš„`--task`å‚æ•°ï¼Œå¦‚ä¸Šæ‰€è¿°ã€‚
