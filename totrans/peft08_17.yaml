- en: Mixed adapter types
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合适配器类型
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/mixed_models](https://huggingface.co/docs/peft/developer_guides/mixed_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/peft/developer_guides/mixed_models](https://huggingface.co/docs/peft/developer_guides/mixed_models)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Normally, it isn’t possible to mix different adapter types in 🤗 PEFT. You can
    create a PEFT model with two different LoRA adapters (which can have different
    config options), but it is not possible to combine a LoRA and LoHa adapter. With
    [PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    however, this works as long as the adapter types are compatible. The main purpose
    of allowing mixed adapter types is to combine trained adapters for inference.
    While it is possible to train a mixed adapter model, this has not been tested
    and is not recommended.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在🤗 PEFT中无法混合不同类型的适配器。您可以创建一个具有两个不同LoRA适配器的PEFT模型（可以具有不同的配置选项），但无法组合LoRA和LoHa适配器。但是，使用[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)，只要适配器类型兼容，就可以实现这一点。允许混合适配器类型的主要目的是为了结合推理的训练适配器。虽然可以训练混合适配器模型，但尚未经过测试，不建议使用。
- en: 'To load different adapter types into a PEFT model, use [PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    instead of [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要将不同类型的适配器加载到PEFT模型中，请使用[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)而不是[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The [set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel.set_adapter)
    method is necessary to activate both adapters, otherwise only the first adapter
    would be active. You can keep adding more adapters by calling [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    repeatedly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 必须使用[set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel.set_adapter)方法来激活两个适配器，否则只有第一个适配器会处于活动状态。您可以通过反复调用[add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)来继续添加更多适配器。
- en: '[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    does not support saving and loading mixed adapters. The adapters should already
    be trained, and loading the model requires a script to be run each time.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)不支持保存和加载混合适配器。适配器应该已经训练好，加载模型需要每次运行一个脚本。'
- en: Tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: Not all adapter types can be combined. See [`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35)
    for a list of compatible types. An error will be raised if you try to combine
    incompatible adapter types.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有适配器类型都可以组合。请参阅[`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35)以获取兼容类型的列表。如果尝试组合不兼容的适配器类型，将会引发错误。
- en: It is possible to mix multiple adapters of the same type which can be useful
    for combining adapters with very different configs.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以混合多个相同类型的适配器，这对于将具有非常不同配置的适配器组合在一起非常有用。
- en: If you want to combine a lot of different adapters, the most performant way
    to do it is to consecutively add the same adapter types. For example, add LoRA1,
    LoRA2, LoHa1, LoHa2 in this order, instead of LoRA1, LoHa1, LoRA2, and LoHa2\.
    While the order can affect the output, there is no inherently *best* order, so
    it is best to choose the fastest one.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果要组合许多不同的适配器，最有效的方法是依次添加相同类型的适配器。例如，按照LoRA1、LoRA2、LoHa1、LoHa2的顺序添加，而不是LoRA1、LoHa1、LoRA2和LoHa2。虽然顺序可能会影响输出，但并没有固定的*最佳*顺序，因此最好选择最快的顺序。
