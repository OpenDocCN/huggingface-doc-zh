- en: Mixed adapter types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/peft/developer_guides/mixed_models](https://huggingface.co/docs/peft/developer_guides/mixed_models)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/11.333f326a.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Normally, it isnâ€™t possible to mix different adapter types in ðŸ¤— PEFT. You can
    create a PEFT model with two different LoRA adapters (which can have different
    config options), but it is not possible to combine a LoRA and LoHa adapter. With
    [PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    however, this works as long as the adapter types are compatible. The main purpose
    of allowing mixed adapter types is to combine trained adapters for inference.
    While it is possible to train a mixed adapter model, this has not been tested
    and is not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load different adapter types into a PEFT model, use [PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    instead of [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The [set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel.set_adapter)
    method is necessary to activate both adapters, otherwise only the first adapter
    would be active. You can keep adding more adapters by calling [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    does not support saving and loading mixed adapters. The adapters should already
    be trained, and loading the model requires a script to be run each time.'
  prefs: []
  type: TYPE_NORMAL
- en: Tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all adapter types can be combined. See [`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35)
    for a list of compatible types. An error will be raised if you try to combine
    incompatible adapter types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to mix multiple adapters of the same type which can be useful
    for combining adapters with very different configs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to combine a lot of different adapters, the most performant way
    to do it is to consecutively add the same adapter types. For example, add LoRA1,
    LoRA2, LoHa1, LoHa2 in this order, instead of LoRA1, LoHa1, LoRA2, and LoHa2\.
    While the order can affect the output, there is no inherently *best* order, so
    it is best to choose the fastest one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
