- en: Mixed adapter types
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ··åˆé€‚é…å™¨ç±»å‹
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/mixed_models](https://huggingface.co/docs/peft/developer_guides/mixed_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/peft/developer_guides/mixed_models](https://huggingface.co/docs/peft/developer_guides/mixed_models)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Normally, it isnâ€™t possible to mix different adapter types in ğŸ¤— PEFT. You can
    create a PEFT model with two different LoRA adapters (which can have different
    config options), but it is not possible to combine a LoRA and LoHa adapter. With
    [PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    however, this works as long as the adapter types are compatible. The main purpose
    of allowing mixed adapter types is to combine trained adapters for inference.
    While it is possible to train a mixed adapter model, this has not been tested
    and is not recommended.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æƒ…å†µä¸‹ï¼Œåœ¨ğŸ¤— PEFTä¸­æ— æ³•æ··åˆä¸åŒç±»å‹çš„é€‚é…å™¨ã€‚æ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªä¸åŒLoRAé€‚é…å™¨çš„PEFTæ¨¡å‹ï¼ˆå¯ä»¥å…·æœ‰ä¸åŒçš„é…ç½®é€‰é¡¹ï¼‰ï¼Œä½†æ— æ³•ç»„åˆLoRAå’ŒLoHaé€‚é…å™¨ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)ï¼Œåªè¦é€‚é…å™¨ç±»å‹å…¼å®¹ï¼Œå°±å¯ä»¥å®ç°è¿™ä¸€ç‚¹ã€‚å…è®¸æ··åˆé€‚é…å™¨ç±»å‹çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†ç»“åˆæ¨ç†çš„è®­ç»ƒé€‚é…å™¨ã€‚è™½ç„¶å¯ä»¥è®­ç»ƒæ··åˆé€‚é…å™¨æ¨¡å‹ï¼Œä½†å°šæœªç»è¿‡æµ‹è¯•ï¼Œä¸å»ºè®®ä½¿ç”¨ã€‚
- en: 'To load different adapter types into a PEFT model, use [PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    instead of [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†ä¸åŒç±»å‹çš„é€‚é…å™¨åŠ è½½åˆ°PEFTæ¨¡å‹ä¸­ï¼Œè¯·ä½¿ç”¨[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)è€Œä¸æ˜¯[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ï¼š
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The [set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel.set_adapter)
    method is necessary to activate both adapters, otherwise only the first adapter
    would be active. You can keep adding more adapters by calling [add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)
    repeatedly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¿…é¡»ä½¿ç”¨[set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel.set_adapter)æ–¹æ³•æ¥æ¿€æ´»ä¸¤ä¸ªé€‚é…å™¨ï¼Œå¦åˆ™åªæœ‰ç¬¬ä¸€ä¸ªé€‚é…å™¨ä¼šå¤„äºæ´»åŠ¨çŠ¶æ€ã€‚æ‚¨å¯ä»¥é€šè¿‡åå¤è°ƒç”¨[add_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.add_adapter)æ¥ç»§ç»­æ·»åŠ æ›´å¤šé€‚é…å™¨ã€‚
- en: '[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)
    does not support saving and loading mixed adapters. The adapters should already
    be trained, and loading the model requires a script to be run each time.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[PeftMixedModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftMixedModel)ä¸æ”¯æŒä¿å­˜å’ŒåŠ è½½æ··åˆé€‚é…å™¨ã€‚é€‚é…å™¨åº”è¯¥å·²ç»è®­ç»ƒå¥½ï¼ŒåŠ è½½æ¨¡å‹éœ€è¦æ¯æ¬¡è¿è¡Œä¸€ä¸ªè„šæœ¬ã€‚'
- en: Tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤º
- en: Not all adapter types can be combined. See [`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35)
    for a list of compatible types. An error will be raised if you try to combine
    incompatible adapter types.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰é€‚é…å™¨ç±»å‹éƒ½å¯ä»¥ç»„åˆã€‚è¯·å‚é˜…[`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35)ä»¥è·å–å…¼å®¹ç±»å‹çš„åˆ—è¡¨ã€‚å¦‚æœå°è¯•ç»„åˆä¸å…¼å®¹çš„é€‚é…å™¨ç±»å‹ï¼Œå°†ä¼šå¼•å‘é”™è¯¯ã€‚
- en: It is possible to mix multiple adapters of the same type which can be useful
    for combining adapters with very different configs.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥æ··åˆå¤šä¸ªç›¸åŒç±»å‹çš„é€‚é…å™¨ï¼Œè¿™å¯¹äºå°†å…·æœ‰éå¸¸ä¸åŒé…ç½®çš„é€‚é…å™¨ç»„åˆåœ¨ä¸€èµ·éå¸¸æœ‰ç”¨ã€‚
- en: If you want to combine a lot of different adapters, the most performant way
    to do it is to consecutively add the same adapter types. For example, add LoRA1,
    LoRA2, LoHa1, LoHa2 in this order, instead of LoRA1, LoHa1, LoRA2, and LoHa2\.
    While the order can affect the output, there is no inherently *best* order, so
    it is best to choose the fastest one.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè¦ç»„åˆè®¸å¤šä¸åŒçš„é€‚é…å™¨ï¼Œæœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä¾æ¬¡æ·»åŠ ç›¸åŒç±»å‹çš„é€‚é…å™¨ã€‚ä¾‹å¦‚ï¼ŒæŒ‰ç…§LoRA1ã€LoRA2ã€LoHa1ã€LoHa2çš„é¡ºåºæ·»åŠ ï¼Œè€Œä¸æ˜¯LoRA1ã€LoHa1ã€LoRA2å’ŒLoHa2ã€‚è™½ç„¶é¡ºåºå¯èƒ½ä¼šå½±å“è¾“å‡ºï¼Œä½†å¹¶æ²¡æœ‰å›ºå®šçš„*æœ€ä½³*é¡ºåºï¼Œå› æ­¤æœ€å¥½é€‰æ‹©æœ€å¿«çš„é¡ºåºã€‚
