["```py\npip install -U bitsandbytes\n```", "```py\nfrom transformers import T5EncoderModel\nfrom diffusers import PixArtAlphaPipeline\nimport torch\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    subfolder=\"text_encoder\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n\n)\npipe = PixArtAlphaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    text_encoder=text_encoder,\n    transformer=None,\n    device_map=\"auto\"\n)\n```", "```py\nwith torch.no_grad():\n    prompt = \"cute cat\"\n    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe.encode_prompt(prompt)\n```", "```py\nimport gc \n\ndef flush():\n    gc.collect()\n    torch.cuda.empty_cache()\n\ndel text_encoder\ndel pipe\nflush()\n```", "```py\npipe = PixArtAlphaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    text_encoder=None,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\nlatents = pipe(\n    negative_prompt=None, \n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    prompt_attention_mask=prompt_attention_mask,\n    negative_prompt_attention_mask=negative_prompt_attention_mask,\n    num_images_per_prompt=1,\n    output_type=\"latent\",\n).images\n\ndel pipe.transformer\nflush()\n```", "```py\nwith torch.no_grad():\n    image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\nimage = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\nimage.save(\"cat.png\")\n```", "```py\n>>> import torch\n>>> from diffusers import PixArtAlphaPipeline\n\n>>> # You can replace the checkpoint id with \"PixArt-alpha/PixArt-XL-2-512x512\" too.\n>>> pipe = PixArtAlphaPipeline.from_pretrained(\"PixArt-alpha/PixArt-XL-2-1024-MS\", torch_dtype=torch.float16)\n>>> # Enable memory optimizations.\n>>> pipe.enable_model_cpu_offload()\n\n>>> prompt = \"A small cactus with a happy face in the Sahara desert.\"\n>>> image = pipe(prompt).images[0]\n```"]