# éæ ¸å¿ƒæ¨¡å‹æœåŠ¡

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models)

TGIæ”¯æŒå„ç§LLMæ¶æ„ï¼ˆè¯·å‚è§å®Œæ•´åˆ—è¡¨[æ­¤å¤„](../supported_models)ï¼‰ã€‚å¦‚æœæ‚¨å¸Œæœ›æä¾›çš„æ¨¡å‹ä¸æ˜¯å—æ”¯æŒçš„æ¨¡å‹ä¹‹ä¸€ï¼ŒTGIå°†é€€å›åˆ°è¯¥æ¨¡å‹çš„`transformers`å®ç°ã€‚è¿™æ„å‘³ç€æ‚¨å°†æ— æ³•ä½¿ç”¨TGIå¼•å…¥çš„ä¸€äº›åŠŸèƒ½ï¼Œä¾‹å¦‚å¼ é‡å¹¶è¡Œåˆ†ç‰‡æˆ–é—ªå…‰æ³¨æ„åŠ›ã€‚ä½†æ˜¯ï¼Œæ‚¨ä»ç„¶å¯ä»¥è·å¾—TGIçš„è®¸å¤šå¥½å¤„ï¼Œä¾‹å¦‚è¿ç»­æ‰¹å¤„ç†æˆ–æµå¼è¾“å‡ºã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ä¸å®Œå…¨æ”¯æŒçš„æ¨¡å‹ç›¸åŒçš„Dockerå‘½ä»¤è¡Œè°ƒç”¨æ¥æä¾›è¿™äº›æ¨¡å‹ğŸ‘‡

```py
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id gpt2
```

å¦‚æœæ‚¨å¸Œæœ›æä¾›çš„æ¨¡å‹æ˜¯è‡ªå®šä¹‰çš„transformersæ¨¡å‹ï¼Œå¹¶ä¸”å…¶æƒé‡å’Œå®ç°åœ¨Hubä¸Šå¯ç”¨ï¼Œæ‚¨ä»ç„¶å¯ä»¥é€šè¿‡å‘`docker run`å‘½ä»¤ä¼ é€’`--trust-remote-code`æ ‡å¿—æ¥æä¾›æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡

```py
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id <CUSTOM_MODEL_ID> --trust-remote-code
```

æœ€åï¼Œå¦‚æœæ¨¡å‹ä¸åœ¨Hugging Face Hubä¸Šï¼Œè€Œæ˜¯åœ¨æœ¬åœ°ï¼Œæ‚¨å¯ä»¥åƒä¸‹é¢è¿™æ ·ä¼ é€’åŒ…å«æ‚¨çš„æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ğŸ‘‡

```py
# Make sure your model is in the $volume directory
docker run --shm-size 1g -p 8080:80 -v $volume:/data  ghcr.io/huggingface/text-generation-inference:latest --model-id /data/<PATH-TO-FOLDER>
```

æ‚¨å¯ä»¥å‚è€ƒ[transformersæ–‡æ¡£ä¸Šçš„è‡ªå®šä¹‰æ¨¡å‹](https://huggingface.co/docs/transformers/main/en/custom_models)è·å–æ›´å¤šä¿¡æ¯ã€‚
