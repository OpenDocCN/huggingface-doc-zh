["```py\n>>> from transformers import OpenLlamaModel, OpenLlamaConfig\n\n>>> # Initializing a Open-Llama open_llama-7b style configuration\n>>> configuration = OpenLlamaConfig()\n\n>>> # Initializing a model from the open_llama-7b style configuration\n>>> model = OpenLlamaModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, OpenLlamaForCausalLM\n\n>>> model = OpenLlamaForCausalLM.from_pretrained(\"openlm-research/open_llama_7b\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n\n>>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n```"]