["```py\n>>> from transformers import GPTJForCausalLM\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\n...     \"EleutherAI/gpt-j-6B\",\n...     revision=\"float16\",\n...     torch_dtype=torch.float16,\n... ).to(device)\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```", "```py\n>>> from transformers import GPTJForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```", "```py\n>>> from transformers import GPTJModel, GPTJConfig\n\n>>> # Initializing a GPT-J 6B configuration\n>>> configuration = GPTJConfig()\n\n>>> # Initializing a model from the configuration\n>>> model = GPTJModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, GPTJModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n>>> model = GPTJModel.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTJForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n>>> model = GPTJForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTJForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\")\n>>> model = GPTJForSequenceClassification.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = GPTJForSequenceClassification.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTJForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\")\n>>> model = GPTJForSequenceClassification.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = GPTJForSequenceClassification.from_pretrained(\n...     \"ydshieh/tiny-random-gptj-for-sequence-classification\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, GPTJForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n>>> model = GPTJForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJModel.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJForCausalLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFGPTJForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJForQuestionAnswering\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJForQuestionAnswering.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n\n>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n```", "```py\n>>> # target is \"nice puppet\"\n>>> target_start_index = tf.constant([14])\n>>> target_end_index = tf.constant([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = tf.math.reduce_mean(outputs.loss)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxGPTJModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gptj\")\n>>> model = FlaxGPTJModel.from_pretrained(\"gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxGPTJForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gptj\")\n>>> model = FlaxGPTJForCausalLM.from_pretrained(\"gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```"]