["```py\n>>> from transformers import GPTJForCausalLM\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\n...     \"EleutherAI/gpt-j-6B\",\n...     revision=\"float16\",\n...     torch_dtype=torch.float16,\n... ).to(device)\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```", "```py\n>>> from transformers import GPTJForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```", "```py\n( vocab_size = 50400 n_positions = 2048 n_embd = 4096 n_layer = 28 n_head = 16 rotary_dim = 64 n_inner = None activation_function = 'gelu_new' resid_pdrop = 0.0 embd_pdrop = 0.0 attn_pdrop = 0.0 layer_norm_epsilon = 1e-05 initializer_range = 0.02 use_cache = True bos_token_id = 50256 eos_token_id = 50256 tie_word_embeddings = False **kwargs )\n```", "```py\n>>> from transformers import GPTJModel, GPTJConfig\n\n>>> # Initializing a GPT-J 6B configuration\n>>> configuration = GPTJConfig()\n\n>>> # Initializing a model from the configuration\n>>> model = GPTJModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, GPTJModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n>>> model = GPTJModel.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTJForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n>>> model = GPTJForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTJForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\")\n>>> model = GPTJForSequenceClassification.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = GPTJForSequenceClassification.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTJForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\")\n>>> model = GPTJForSequenceClassification.from_pretrained(\"ydshieh/tiny-random-gptj-for-sequence-classification\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = GPTJForSequenceClassification.from_pretrained(\n...     \"ydshieh/tiny-random-gptj-for-sequence-classification\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, GPTJForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n>>> model = GPTJForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJModel.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJForCausalLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFGPTJForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPTJForQuestionAnswering\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> model = TFGPTJForQuestionAnswering.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n\n>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n```", "```py\n>>> # target is \"nice puppet\"\n>>> target_start_index = tf.constant([14])\n>>> target_end_index = tf.constant([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = tf.math.reduce_mean(outputs.loss)\n```", "```py\n( config: GPTJConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids attention_mask = None position_ids = None params: dict = None past_key_values: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxGPTJModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gptj\")\n>>> model = FlaxGPTJModel.from_pretrained(\"gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: GPTJConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids attention_mask = None position_ids = None params: dict = None past_key_values: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxGPTJForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gptj\")\n>>> model = FlaxGPTJForCausalLM.from_pretrained(\"gptj\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```"]