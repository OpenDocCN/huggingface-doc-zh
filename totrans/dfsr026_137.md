# 带有 Stable Diffusion XL 的 ControlNet

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl`](https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl)

ControlNet 在 [向文本到图像扩散模型添加条件控制](https://huggingface.co/papers/2302.05543) 中由 Lvmin Zhang、Anyi Rao 和 Maneesh Agrawala 提出。

使用 ControlNet 模型，您可以提供额外的控制图像来调节和控制 Stable Diffusion 生成。例如，如果您提供深度图，ControlNet 模型将生成一个保留深度图中空间信息的图像。这是一种更灵活和准确的控制图像生成过程的方式。

该论文的摘要为：

*我们提出了 ControlNet，这是一种神经网络架构，用于向大型、预训练的文本到图像扩散模型添加空间调节控制。ControlNet 锁定了生产就绪的大型扩散模型，并重复使用它们的深度和稳健的编码层，这些编码层经过数十亿张图像的预训练，作为学习各种条件控制的强大支撑。神经架构连接了“零卷积”（从零初始化的卷积层），逐渐增加参数，确保没有有害噪音会影响微调。我们测试了各种调节控制，例如边缘、深度、分割、人体姿势等，使用单个或多个条件，有或没有提示，与 Stable Diffusion 结合。我们展示了 ControlNet 的训练对于小型（<50k）和大型（>1m）数据集是稳健的。广泛的结果表明，ControlNet 可能促进更广泛的应用，以控制图像扩散模型。*

您可以在🤗 [Diffusers](https://huggingface.co/diffusers) Hub 组织中找到额外的较小的 Stable Diffusion XL（SDXL）ControlNet 检查点，并在 Hub 上浏览[社区训练的](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)检查点。

🧪 许多 SDXL ControlNet 检查点是实验性的，有很大的改进空间。欢迎打开一个[Issue](https://github.com/huggingface/diffusers/issues/new/choose)，告诉我们如何改进！

如果您没有看到您感兴趣的检查点，您可以使用我们的训练脚本训练自己的 SDXL ControlNet。

确保查看调度器指南以了解如何探索调度器速度和质量之间的权衡，并查看跨管道重用组件部分，以了解如何有效地将相同组件加载到多个管道中。

## StableDiffusionXLControlNetPipeline

`diffusers.StableDiffusionXLControlNetPipeline` 类

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L117)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `text_encoder_2` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)) — 第二个冻结的文本编码器（[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `tokenizer_2` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` (UNet2DConditionModel) — 用于去噪编码图像潜变量的 `UNet2DConditionModel`。

+   `controlnet` (ControlNetModel 或 `List[ControlNetModel]`) — 在去噪过程中为 `unet` 提供额外的条件。如果将多个 ControlNet 设置为列表，则每个 ControlNet 的输出将相加，以创建一个组合的额外条件。

+   `scheduler` (SchedulerMixin) — 用于与 `unet` 结合使用以去噪编码图像潜变量的调度程序。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 中的一个。

+   `force_zeros_for_empty_prompt` (`bool`, *可选*, 默认为 `"True"`) — 是否始终将负提示嵌入设置为 0。也请参阅 `stabilityai/stable-diffusion-xl-base-1-0` 的配置。

+   `add_watermarker` (`bool`, *可选*) — 是否使用 [invisible_watermark](https://github.com/ShieldMnt/invisible-watermark/) 库对输出图像进行水印处理。如果未定义，且安装了该软件包，则默认为 `True`；否则不使用水印处理。

使用 Stable Diffusion XL 和 ControlNet 指导的文本到图像生成流水线。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承了以下加载方法:

+   load_textual_inversion() 用于加载文本反演嵌入

+   load_lora_weights() 用于加载 LoRA 权重

+   `save_lora_weights()` 用于保存 LoRA 权重

+   from_single_file() 用于加载 `.ckpt` 文件

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L943)

```py
( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 5.0 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 1.0 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 original_size: Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None negative_original_size: Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示。如果未定义，则需要传递 `prompt_embeds`。

+   `prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到 `tokenizer_2` 和 `text_encoder_2` 的提示。如果未定义，则在两个文本编码器中使用 `prompt`。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, `List[np.ndarray]`, — `List[List[torch.FloatTensor]]`, `List[List[np.ndarray]]` 或 `List[List[PIL.Image.Image]]`): 用于为`unet`提供指导的 ControlNet 输入条件。如果指定类型为`torch.FloatTensor`，则按原样传递给 ControlNet。`PIL.Image.Image`也可以作为图像接受。输出图像的尺寸默认为`image`的尺寸。如果传递了高度和/或宽度，`image`将相应调整大小。如果在`init`中指定了多个 ControlNets，则必须将图像作为列表传递，以便列表的每个元素可以正确批处理为单个 ControlNet 的输入。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。低于 512 像素的任何内容都不适用于[stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)和未经专门调整以适应低分辨率的检查点。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。低于 512 像素的任何内容都不适用于[stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)和未经专门调整以适应低分辨率的检查点。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 5.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）将被忽略。

+   `negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 指导图像生成中不包括的提示或提示。这将发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中都使用`negative_prompt`。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中预先生成的噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预先生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预先生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的池化文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，池化文本嵌入将从 `prompt` 输入参数生成。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面池化文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，负面池化文本嵌入将从 `negative_prompt` 输入参数生成。ip_adapter_image — (`PipelineImageInput`, *optional*): 可选的图像输入，用于与 IP 适配器一起使用。

+   `output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 `AttentionProcessor` 的 kwargs 字典，如 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义。

+   `controlnet_conditioning_scale` (`float` 或 `List[float]`, *optional*, 默认为 1.0) — 在将 ControlNet 的输出添加到原始 `unet` 中的残差之前，将其乘以 `controlnet_conditioning_scale`。如果在 `init` 中指定了多个 ControlNets，可以将相应的比例设置为列表。

+   `guess_mode` (`bool`, *optional*, 默认为 `False`) — 控制网络编码器尝试识别输入图像的内容，即使您删除了所有提示。建议设置 `guidance_scale` 值在 3.0 到 5.0 之间。

+   `control_guidance_start` (`float` 或 `List[float]`, *optional*, 默认为 0.0) — 控制网络开始应用的总步骤百分比。

+   `control_guidance_end` (`float` 或 `List[float]`, *optional*, 默认为 1.0) — 控制网络停止应用的总步骤百分比。

+   `original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 如果 `original_size` 与 `target_size` 不同，图像将呈现为缩小或放大。如果未指定，`original_size` 默认为 `(height, width)`。作为 SDXL 的微调条件的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 第 2.2 节。

+   `crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — `crops_coords_top_left` 可用于生成一个看起来从位置 `crops_coords_top_left` 向下“裁剪”的图像。通常通过将 `crops_coords_top_left` 设置为 (0, 0) 来实现有利的、居中的图像。作为 SDXL 的微调条件的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 第 2.2 节。

+   `target_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 对于大多数情况，`target_size` 应设置为生成图像的期望高度和宽度。如果未指定，将默认为 `(height, width)`。作为 SDXL 的微调条件的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 第 2.2 节。

+   `negative_original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 基于特定图像分辨率否定地调整生成过程。作为 SDXL 的微调条件的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 第 2.2 节。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_crops_coords_top_left`（`Tuple[int]`，*可选*，默认为(0, 0)）—根据特定裁剪坐标对生成过程进行负条件化。作为 SDXL 的微条件化的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节中所述。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_target_size`（`Tuple[int]`，*可选*，默认为(1024, 1024)）—根据目标图像分辨率对生成过程进行负条件化。对于大多数情况，它应与`target_size`相同。作为 SDXL 的微条件化的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节中所述。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `clip_skip`（`int`，*可选*）—在计算提示嵌入时要跳过的 CLIP 层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end`（`Callable`，*可选*）—在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs`（`List`，*可选*）—`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

StableDiffusionPipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 StableDiffusionPipelineOutput，否则返回一个包含输出图像的`tuple`。

用于生成的管道的调用函数。

示例：

```py
>>> # !pip install opencv-python transformers accelerate
>>> from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> import cv2
>>> from PIL import Image

>>> prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
>>> negative_prompt = "low quality, bad quality, sketches"

>>> # download an image
>>> image = load_image(
...     "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
... )

>>> # initialize the models and pipeline
>>> controlnet_conditioning_scale = 0.5  # recommended for good generalization
>>> controlnet = ControlNetModel.from_pretrained(
...     "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16
... )
>>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
>>> pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16
... )
>>> pipe.enable_model_cpu_offload()

>>> # get canny image
>>> image = np.array(image)
>>> image = cv2.Canny(image, 100, 200)
>>> image = image[:, :, None]
>>> image = np.concatenate([image, image, image], axis=2)
>>> canny_image = Image.fromarray(image)

>>> # generate image
>>> image = pipe(
...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image
... ).images[0]
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L887)

```py
( )
```

如果启用，禁用 FreeU 机制。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L234)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L251)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L864)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1`（`float`）—用于减弱跳过特征贡献的第 1 阶段的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2`（`float`）—用于减弱跳过特征贡献的第 2 阶段的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1`（`float`）—用于放大第 1 阶段的骨干特征贡献的缩放因子。

+   `b2`（`float`）—用于放大第 2 阶段的骨干特征贡献的缩放因子。

启用 FreeU 机制，如[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)。

缩放因子后缀表示应用它们的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L226)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将在几个步骤中将输入张量分割成切片以进行解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L242)

```py
( )
```

启用平铺 VAE 解码。启用此选项时，VAE 将将输入张量分成瓦片以在几个步骤中进行解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py#L259)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示

+   `prompt_2` (`str` or `List[str]`, *optional*) — 要发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器设备中使用`prompt` — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `negative_prompt_2` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示或提示，要发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中使用`negative_prompt`

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负 negative_prompt_embeds。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化的文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成池化的 negative_prompt_embeds。

+   `lora_scale` (`float`, *optional*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 lora 比例。

+   `clip_skip` (`int`, *optional*) — 计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 表示将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。
