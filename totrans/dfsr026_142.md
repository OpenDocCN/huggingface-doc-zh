# DiffEdit

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/diffedit](https://huggingface.co/docs/diffusers/api/pipelines/diffedit)

[DiffEdit：基于扩散的带蒙版指导的语义图像编辑](https://huggingface.co/papers/2210.11427)由Guillaume Couairon、Jakob Verbeek、Holger Schwenk和Matthieu Cord撰写。

论文摘要如下：

*图像生成最近取得了巨大进展，扩散模型允许根据各种文本提示合成令人信服的图像。在本文中，我们提出了DiffEdit，一种利用文本条件扩散模型进行语义图像编辑任务的方法，其目标是根据文本查询编辑图像。语义图像编辑是图像生成的延伸，额外的约束是生成的图像应尽可能与给定的输入图像相似。基于扩散模型的当前编辑方法通常需要提供一个蒙版，通过将其视为条件修补任务，使任务变得更容易得多。相比之下，我们的主要贡献是能够自动生成一个蒙版，突出显示需要编辑的输入图像区域，通过对比受不同文本提示条件的扩散模型的预测。此外，我们依赖潜在推断来保留感兴趣区域的内容，并与基于蒙版的扩散表现出卓越的协同作用。DiffEdit在ImageNet上实现了最先进的编辑性能。此外，我们使用来自COCO数据集的图像以及基于文本生成的图像来评估更具挑战性的语义图像编辑。*

原始代码库可以在[Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion)找到，您可以在此[演示](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html)中尝试。

这个流水线是由[clarencechen](https://github.com/clarencechen)贡献的。❤️

## 提示

+   该流水线可以生成可供其他修补流水线使用的蒙版。

+   为了使用这个流水线生成图像，必须在调用流水线生成最终编辑的图像时提供图像蒙版（源和目标提示可以手动指定或生成，并传递给[generate_mask()](/docs/diffusers/v0.26.3/en/api/pipelines/diffedit#diffusers.StableDiffusionDiffEditPipeline.generate_mask))和一组部分反转的潜在变量（使用[invert()](/docs/diffusers/v0.26.3/en/api/pipelines/diffedit#diffusers.StableDiffusionDiffEditPipeline.invert)生成）*必须*作为参数提供。

+   函数[generate_mask()](/docs/diffusers/v0.26.3/en/api/pipelines/diffedit#diffusers.StableDiffusionDiffEditPipeline.generate_mask)公开了两个提示参数，`source_prompt`和`target_prompt`，让您控制最终生成的图像中语义编辑的位置。假设您想要从“cat”翻译为“dog”。在这种情况下，编辑方向将是“cat -> dog”。为了在生成的蒙版中反映这一点，您只需将与包含“cat”短语相关的嵌入设置为`source_prompt`，将“dog”设置为`target_prompt`。

+   在使用`invert`生成部分反转潜在变量时，将描述整体图像的标题或文本嵌入分配给`prompt`参数，以帮助指导反向潜在抽样过程。在大多数情况下，源概念足够描述性，可以产生良好的结果，但请随时探索其他选择。

+   在调用流水线生成最终编辑的图像时，将源概念分配给`negative_prompt`，将目标概念分配给`prompt`。以上面的例子为例，您只需将与包含“cat”短语相关的嵌入设置为`negative_prompt`，将“dog”设置为`prompt`。

+   如果您想要在上面的示例中反转方向，即“dog -> cat”，那么建议：

    +   在调用`generate_mask`时，交换参数中的`source_prompt`和`target_prompt`。

    +   在 [invert()](/docs/diffusers/v0.26.3/en/api/pipelines/diffedit#diffusers.StableDiffusionDiffEditPipeline.invert) 中更改输入提示以包含“dog”。

    +   在调用管道生成最终编辑图像的参数中交换`prompt`和`negative_prompt`。

+   源和目标提示，或它们对应的嵌入，也可以自动生成。请参考 [DiffEdit](../../using-diffusers/diffedit) 指南获取更多详细信息。

## StableDiffusionDiffEditPipeline

### `class diffusers.StableDiffusionDiffEditPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L238)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor inverse_scheduler: DDIMInverseScheduler requires_safety_checker: bool = True )
```

参数

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 一个`CLIPTokenizer`用于对文本进行标记化。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 一个`UNet2DConditionModel`用于去噪编码图像潜在部分。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 一个调度器，用于与`unet`结合使用，去噪编码图像潜在部分。

+   `inverse_scheduler` ([DDIMInverseScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim_inverse#diffusers.DDIMInverseScheduler)) — 一个调度器，用于与`unet`结合使用，填充输入潜在部分的未遮罩部分。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成的图像是否可能被视为具有冒犯性或有害的分类模块。请参考 [模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5) 以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 一个`CLIPImageProcessor`用于从生成的图像中提取特征；作为`安全检查器`的输入。

这是一个实验性功能！

使用 Stable Diffusion 和 DiffEdit 进行文本引导图像修补的管道。

此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承以下加载和保存方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入。

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载 LoRA 权重。

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存 LoRA 权重。

#### `generate_mask`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L857)

```py
( image: Union = None target_prompt: Union = None target_negative_prompt: Union = None target_prompt_embeds: Optional = None target_negative_prompt_embeds: Optional = None source_prompt: Union = None source_negative_prompt: Union = None source_prompt_embeds: Optional = None source_negative_prompt_embeds: Optional = None num_maps_per_mask: Optional = 10 mask_encode_strength: Optional = 0.5 mask_thresholding_ratio: Optional = 3.0 num_inference_steps: int = 50 guidance_scale: float = 7.5 generator: Union = None output_type: Optional = 'np' cross_attention_kwargs: Optional = None ) → export const metadata = 'undefined';List[PIL.Image.Image] or np.array
```

参数

+   `image` (`PIL.Image.Image`) — 代表要用于计算蒙版的图像批次的`Image`或张量。

+   `target_prompt` (`str` 或 `List[str]`，*可选*) — 用于指导语义蒙版生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `target_negative_prompt` (`str` 或 `List[str]`, *optional*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时被忽略（`guidance_scale < 1`）。

+   `target_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `target_negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `source_prompt` (`str` 或 `List[str]`, *optional*) — 使用DiffEdit指导语义蒙版生成的提示或提示。如果未定义，则需要传递`source_prompt_embeds`或`source_image`。

+   `source_negative_prompt` (`str` 或 `List[str]`, *optional*) — 指导语义蒙版生成远离DiffEdit的提示或提示。如果未定义，则需要传递`source_negative_prompt_embeds`或`source_image`。

+   `source_prompt_embeds` (`torch.FloatTensor`, *optional*) — 用于指导语义蒙版生成的预生成文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`source_prompt`输入参数生成文本嵌入。

+   `source_negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 用于负向指导语义蒙版生成的预生成文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`source_negative_prompt`输入参数生成文本嵌入。

+   `num_maps_per_mask` (`int`, *optional*, 默认为10) — 采样生成语义蒙版所需的噪声图数量，使用DiffEdit。

+   `mask_encode_strength` (`float`, *optional*, 默认为0.5) — 用于生成语义蒙版的噪声图的强度，使用DiffEdit。必须在0和1之间。

+   `mask_thresholding_ratio` (`float`, *optional*, 默认为3.0) — 在进行蒙版二值化之前，用于夹紧语义引导图的平均绝对差的最大倍数。

+   `num_inference_steps` (`int`, *optional*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给[AttnProcessor](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnProcessor)中定义的`self.processor`的kwargs字典。

返回

`List[PIL.Image.Image]` 或 `np.array`

当返回`List[PIL.Image.Image]`时，列表由一批单通道二值图像组成，尺寸为`(height // self.vae_scale_factor, width // self.vae_scale_factor)`。如果是`np.array`，形状为`(batch_size, height // self.vae_scale_factor, width // self.vae_scale_factor)`。

给定蒙版提示、目标提示和图像生成潜在蒙版。

```py
>>> import PIL
>>> import requests
>>> import torch
>>> from io import BytesIO

>>> from diffusers import StableDiffusionDiffEditPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"

>>> init_image = download_image(img_url).resize((768, 768))

>>> pipe = StableDiffusionDiffEditPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.enable_model_cpu_offload()

>>> mask_prompt = "A bowl of fruits"
>>> prompt = "A bowl of pears"

>>> mask_image = pipe.generate_mask(image=init_image, source_prompt=prompt, target_prompt=mask_prompt)
>>> image_latents = pipe.invert(image=init_image, prompt=mask_prompt).latents
>>> image = pipe(prompt=prompt, mask_image=mask_image, image_latents=image_latents).images[0]
```

#### `invert`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L1076)

```py
( prompt: Union = None image: Union = None num_inference_steps: int = 50 inpaint_strength: float = 0.8 guidance_scale: float = 7.5 negative_prompt: Union = None generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None decode_latents: bool = False output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None lambda_auto_corr: float = 20.0 lambda_kl: float = 20.0 num_reg_steps: int = 0 num_auto_corr_rolls: int = 5 )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`PIL.Image.Image`) — 代表要生成的反转潜变量的图像批次的`Image`或张量。

+   `inpaint_strength` (`float`, *optional*, defaults to 0.8) — 表示运行潜变量反转的噪声过程的程度。必须在0和1之间。当`inpaint_strength`为1时，反转过程将运行指定的`num_inference_steps`的全部迭代。`image`用作反转过程的参考，并增加更多噪声会增加`inpaint_strength`。如果`inpaint_strength`为0，则不进行修补。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步数。更多的降噪步骤通常会导致更高质量的图像，但会降低推断速度。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用指导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导不包含在图像生成中的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时被忽略（`guidance_scale < 1`）。

+   `generator` (`torch.Generator`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。

+   `decode_latents` (`bool`, *optional*, defaults to `False`) — 是否解码反转的潜变量为生成的图像。将此参数设置为`True`会将每个时间步的所有反转的潜变量解码为生成的图像列表。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回`~pipelines.stable_diffusion.DiffEditInversionPipelineOutput`而不是普通元组。

+   `callback` (`Callable`, *optional*) — 推断过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, defaults to 1) — 调用`callback`函数的频率。如果未指定，回调将在每一步调用。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给[AttnProcessor](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnProcessor)中定义的`self.processor`的kwargs字典。

+   `lambda_auto_corr` (`float`, *optional*, defaults to 20.0) — 控制自动校正的Lambda参数。

+   `lambda_kl` (`float`, *optional*, defaults to 20.0) — 控制Kullback-Leibler散度输出的Lambda参数。

+   `num_reg_steps` (`int`, *optional*, defaults to 0) — 正则化损失步数。

+   `num_auto_corr_rolls` (`int`, *optional*, defaults to 5) — 自动校正滚动步数。

给定提示和图像生成反转潜变量。

```py
>>> import PIL
>>> import requests
>>> import torch
>>> from io import BytesIO

>>> from diffusers import StableDiffusionDiffEditPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"

>>> init_image = download_image(img_url).resize((768, 768))

>>> pipe = StableDiffusionDiffEditPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.enable_model_cpu_offload()

>>> prompt = "A bowl of fruits"

>>> inverted_latents = pipe.invert(image=init_image, prompt=prompt).latents
```

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L1314)

```py
( prompt: Union = None mask_image: Union = None image_latents: Union = None inpaint_strength: Optional = 0.8 num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_ckip: int = None ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `mask_image` (`PIL.Image.Image`) — 代表要遮罩生成图像的图像批次的`Image`或张量。遮罩中的白色像素被重新绘制，而黑色像素被保留。如果`mask_image`是PIL图像，则在使用之前将其转换为单通道（亮度）。如果是张量，则应该包含一个颜色通道（L）而不是3个，因此预期形状将是`(B, 1, H, W)`。

+   `image_latents` (`PIL.Image.Image` or `torch.FloatTensor`) — 部分噪声图像潜变量，用于图像生成的反演过程的输入。

+   `inpaint_strength` (`float`, *optional*, defaults to 0.8) — 表示修复遮罩区域的程度。必须在0和1之间。当`inpaint_strength`为1时，对遮罩区域进行完整数量的迭代指定的`num_inference_steps`运行去噪过程。`image_latents`用作遮罩区域的参考，向区域添加更多噪声会增加`inpaint_strength`。如果`inpaint_strength`为0，则不进行修复。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成中不包括什么的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时被忽略（`guidance_scale < 1`）。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。

+   `generator` (`torch.Generator`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回一个[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)而不是一个普通的元组。

+   `callback` (`Callable`, *optional*) — 一个函数，在推断过程中每 `callback_steps` 步调用一次。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`.

+   `callback_steps` (`int`, *optional*, defaults to 1) — 指定 `callback` 函数被调用的频率。如果未指定，则在每一步都会调用回调函数。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor` 的 kwargs 字典。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的 CLIP 层的数量。值为 1 表示将使用预最终层的输出来计算提示嵌入。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 或 `tuple`

如果 `return_dict` 为 `True`，则返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则返回一个 `tuple`，其中第一个元素是生成的图像列表，第二个元素是一个指示相应生成图像是否包含“不安全内容”（nsfw）的 `bool` 列表。

用于生成的管道的调用函数。

```py
>>> import PIL
>>> import requests
>>> import torch
>>> from io import BytesIO

>>> from diffusers import StableDiffusionDiffEditPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"

>>> init_image = download_image(img_url).resize((768, 768))

>>> pipe = StableDiffusionDiffEditPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.enable_model_cpu_offload()

>>> mask_prompt = "A bowl of fruits"
>>> prompt = "A bowl of pears"

>>> mask_image = pipe.generate_mask(image=init_image, source_prompt=prompt, target_prompt=mask_prompt)
>>> image_latents = pipe.invert(image=init_image, prompt=mask_prompt).latents
>>> image = pipe(prompt=prompt, mask_image=mask_image, image_latents=image_latents).images[0]
```

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L383)

```py
( )
```

禁用切片式 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将回到一步计算解码。

#### `disable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L400)

```py
( )
```

禁用平铺式 VAE 解码。如果之前启用了 `enable_vae_tiling`，则此方法将回到一步计算解码。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L375)

```py
( )
```

启用切片式 VAE 解码。启用此选项时，VAE 将把输入张量切片以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L391)

```py
( )
```

启用平铺式 VAE 解码。启用此选项时，VAE 将把输入张量分割成多个瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py#L441)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。当不使用指导时（即，如果 `guidance_scale` 小于 `1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，则将从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *可选*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用前一层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为`batch_size`的去噪PIL图像列表或形状为`(batch_size, height, width, num_channels)`的NumPy数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表，指示相应生成的图像是否包含“不安全内容”（nsfw）或如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。
