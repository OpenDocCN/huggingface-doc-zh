- en: CPU inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_cpu](https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_cpu)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/318.6bbb4903.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: With some optimizations, it is possible to efficiently run large model inference
    on a CPU. One of these optimization techniques involves compiling the PyTorch
    code into an intermediate format for high-performance environments like C++. The
    other technique fuses multiple operations into one kernel to reduce the overhead
    of running each operation separately.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn how to use [BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    for faster inference, and how to convert your PyTorch code to [TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html).
    If you’re using an Intel CPU, you can also use [graph optimizations](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features.html#graph-optimization)
    from [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/index.html)
    to boost inference speed even more. Finally, learn how to use 🤗 Optimum to accelerate
    inference with ONNX Runtime or OpenVINO (if you’re using an Intel CPU).
  prefs: []
  type: TYPE_NORMAL
- en: BetterTransformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BetterTransformer accelerates inference with its fastpath (native PyTorch specialized
    implementation of Transformer functions) execution. The two optimizations in the
    fastpath execution are:'
  prefs: []
  type: TYPE_NORMAL
- en: fusion, which combines multiple sequential operations into a single “kernel”
    to reduce the number of computation steps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: skipping the inherent sparsity of padding tokens to avoid unnecessary computation
    with nested tensors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BetterTransformer also converts all attention operations to use the more memory-efficient
    [scaled dot product attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention).
  prefs: []
  type: TYPE_NORMAL
- en: BetterTransformer is not supported for all models. Check this [list](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models)
    to see if a model supports BetterTransformer.
  prefs: []
  type: TYPE_NORMAL
- en: Before you start, make sure you have 🤗 Optimum [installed](https://huggingface.co/docs/optimum/installation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable BetterTransformer with the [PreTrainedModel.to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: TorchScript
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TorchScript is an intermediate PyTorch model representation that can be run
    in production environments where performance is important. You can train a model
    in PyTorch and then export it to TorchScript to free the model from Python performance
    constraints. PyTorch [traces](https://pytorch.org/docs/stable/generated/torch.jit.trace.html)
    a model to return a `ScriptFunction` that is optimized with just-in-time compilation
    (JIT). Compared to the default eager mode, JIT mode in PyTorch typically yields
    better performance for inference using optimization techniques like operator fusion.
  prefs: []
  type: TYPE_NORMAL
- en: For a gentle introduction to TorchScript, see the [Introduction to PyTorch TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)
    tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, you can enable JIT mode for CPU inference by setting the `--jit_mode_eval`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For PyTorch >= 1.14.0, JIT-mode could benefit any model for prediction and evaluaion
    since the dict input is supported in `jit.trace`.
  prefs: []
  type: TYPE_NORMAL
- en: For PyTorch < 1.14.0, JIT-mode could benefit a model if its forward parameter
    order matches the tuple input order in `jit.trace`, such as a question-answering
    model. If the forward parameter order does not match the tuple input order in
    `jit.trace`, like a text classification model, `jit.trace` will fail and we are
    capturing this with the exception here to make it fallback. Logging is used to
    notify users.
  prefs: []
  type: TYPE_NORMAL
- en: IPEX graph optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Intel® Extension for PyTorch (IPEX) provides further optimizations in JIT mode
    for Intel CPUs, and we recommend combining it with TorchScript for even faster
    performance. The IPEX [graph optimization](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html)
    fuses operations like Multi-head attention, Concat Linear, Linear + Add, Linear
    + Gelu, Add + LayerNorm, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'To take advantage of these graph optimizations, make sure you have IPEX [installed](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the `--use_ipex` and `--jit_mode_eval` flags in the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class to enable JIT mode with the graph optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 🤗 Optimum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learn more details about using ORT with 🤗 Optimum in the [Optimum Inference
    with ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)
    guide. This section only provides a brief and simple example.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime (ORT) is a model accelerator that runs inference on CPUs by default.
    ORT is supported by 🤗 Optimum which can be used in 🤗 Transformers, without making
    too many changes to your code. You only need to replace the 🤗 Transformers `AutoClass`
    with its equivalent [ORTModel](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)
    for the task you’re solving, and load a checkpoint in the ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you’re running inference on a question answering task, load
    the [optimum/roberta-base-squad2](https://huggingface.co/optimum/roberta-base-squad2)
    checkpoint which contains a `model.onnx` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you have an Intel CPU, take a look at 🤗 [Optimum Intel](https://huggingface.co/docs/optimum/intel/index)
    which supports a variety of compression techniques (quantization, pruning, knowledge
    distillation) and tools for converting models to the [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)
    format for higher performance inference.
  prefs: []
  type: TYPE_NORMAL
