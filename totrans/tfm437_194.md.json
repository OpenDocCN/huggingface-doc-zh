["```py\n>>> from transformers import MarianMTModel, MarianTokenizer\n\n>>> src_text = [\n...     \">>fra<< this is a sentence in english that we want to translate to french\",\n...     \">>por<< This should go to portuguese\",\n...     \">>esp<< And this to Spanish\",\n... ]\n\n>>> model_name = \"Helsinki-NLP/opus-mt-en-roa\"\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\n>>> print(tokenizer.supported_language_codes)\n['>>zlm_Latn<<', '>>mfe<<', '>>hat<<', '>>pap<<', '>>ast<<', '>>cat<<', '>>ind<<', '>>glg<<', '>>wln<<', '>>spa<<', '>>fra<<', '>>ron<<', '>>por<<', '>>ita<<', '>>oci<<', '>>arg<<', '>>min<<']\n\n>>> model = MarianMTModel.from_pretrained(model_name)\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n>>> [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n[\"c'est une phrase en anglais que nous voulons traduire en fran\u00e7ais\",\n 'Isto deve ir para o portugu\u00eas.',\n 'Y esto al espa\u00f1ol']\n```", "```py\nfrom huggingface_hub import list_models\n\nmodel_list = list_models()\norg = \"Helsinki-NLP\"\nmodel_ids = [x.modelId for x in model_list if x.modelId.startswith(org)]\nsuffix = [x.split(\"/\")[1] for x in model_ids]\nold_style_multi_models = [f\"{org}/{s}\" for s in suffix if s != s.lower()]\n```", "```py\n['Helsinki-NLP/opus-mt-NORTH_EU-NORTH_EU',\n 'Helsinki-NLP/opus-mt-ROMANCE-en',\n 'Helsinki-NLP/opus-mt-SCANDINAVIA-SCANDINAVIA',\n 'Helsinki-NLP/opus-mt-de-ZH',\n 'Helsinki-NLP/opus-mt-en-CELTIC',\n 'Helsinki-NLP/opus-mt-en-ROMANCE',\n 'Helsinki-NLP/opus-mt-es-NORWAY',\n 'Helsinki-NLP/opus-mt-fi-NORWAY',\n 'Helsinki-NLP/opus-mt-fi-ZH',\n 'Helsinki-NLP/opus-mt-fi_nb_no_nn_ru_sv_en-SAMI',\n 'Helsinki-NLP/opus-mt-sv-NORWAY',\n 'Helsinki-NLP/opus-mt-sv-ZH']\nGROUP_MEMBERS = {\n 'ZH': ['cmn', 'cn', 'yue', 'ze_zh', 'zh_cn', 'zh_CN', 'zh_HK', 'zh_tw', 'zh_TW', 'zh_yue', 'zhs', 'zht', 'zh'],\n 'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lmo', 'es', 'es_AR', 'es_CL', 'es_CO', 'es_CR', 'es_DO', 'es_EC', 'es_ES', 'es_GT', 'es_HN', 'es_MX', 'es_NI', 'es_PA', 'es_PE', 'es_PR', 'es_SV', 'es_UY', 'es_VE', 'pt', 'pt_br', 'pt_BR', 'pt_PT', 'gl', 'lad', 'an', 'mwl', 'it', 'it_IT', 'co', 'nap', 'scn', 'vec', 'sc', 'ro', 'la'],\n 'NORTH_EU': ['de', 'nl', 'fy', 'af', 'da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\n 'SCANDINAVIA': ['da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\n 'SAMI': ['se', 'sma', 'smj', 'smn', 'sms'],\n 'NORWAY': ['nb_NO', 'nb', 'nn_NO', 'nn', 'nog', 'no_nb', 'no'],\n 'CELTIC': ['ga', 'cy', 'br', 'gd', 'kw', 'gv']\n}\n```", "```py\n>>> from transformers import MarianMTModel, MarianTokenizer\n\n>>> src_text = [\n...     \">>fr<< this is a sentence in english that we want to translate to french\",\n...     \">>pt<< This should go to portuguese\",\n...     \">>es<< And this to Spanish\",\n... ]\n\n>>> model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\n\n>>> model = MarianMTModel.from_pretrained(model_name)\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n>>> tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n[\"c'est une phrase en anglais que nous voulons traduire en fran\u00e7ais\", \n 'Isto deve ir para o portugu\u00eas.',\n 'Y esto al espa\u00f1ol']\n```", "```py\n( vocab_size = 58101 decoder_vocab_size = None max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 58100 scale_embedding = False pad_token_id = 58100 eos_token_id = 0 forced_eos_token_id = 0 share_encoder_decoder_embeddings = True **kwargs )\n```", "```py\n>>> from transformers import MarianModel, MarianConfig\n\n>>> # Initializing a Marian Helsinki-NLP/opus-mt-en-de style configuration\n>>> configuration = MarianConfig()\n\n>>> # Initializing a model from the Helsinki-NLP/opus-mt-en-de style configuration\n>>> model = MarianModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( source_spm target_spm vocab target_vocab_file = None source_lang = None target_lang = None unk_token = '<unk>' eos_token = '</s>' pad_token = '<pad>' model_max_length = 512 sp_model_kwargs: Optional = None separate_vocabs = False **kwargs )\n```", "```py\n>>> from transformers import MarianForCausalLM, MarianTokenizer\n\n>>> model = MarianForCausalLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n>>> tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n>>> src_texts = [\"I am a small frog.\", \"Tom asked his teacher for advice.\"]\n>>> tgt_texts = [\"Ich bin ein kleiner Frosch.\", \"Tom bat seinen Lehrer um Rat.\"]  # optional\n>>> inputs = tokenizer(src_texts, text_target=tgt_texts, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)  # should work\n```", "```py\n( token_ids_0 token_ids_1 = None )\n```", "```py\n( config: MarianConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Union = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MarianModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n>>> model = MarianModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n\n>>> inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n>>> decoder_inputs = tokenizer(\n...     \"<pad> Studien haben gezeigt dass es hilfreich ist einen Hund zu besitzen\",\n...     return_tensors=\"pt\",\n...     add_special_tokens=False,\n... )\n>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 26, 512]\n```", "```py\n( config: MarianConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Union = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MarianMTModel\n\n>>> src = \"fr\"  # source language\n>>> trg = \"en\"  # target language\n\n>>> model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n>>> model = MarianMTModel.from_pretrained(model_name)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n>>> sample_text = \"o\u00f9 est l'arr\u00eat de bus ?\"\n>>> batch = tokenizer([sample_text], return_tensors=\"pt\")\n\n>>> generated_ids = model.generate(**batch)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\"Where's the bus stop?\"\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MarianForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n>>> model = MarianForCausalLM.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n( config: MarianConfig *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: tf.Tensor | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFMarianModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n>>> model = TFMarianModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: TFBaseModelOutput | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None labels: tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFMarianMTModel\n>>> from typing import List\n\n>>> src = \"fr\"  # source language\n>>> trg = \"en\"  # target language\n>>> sample_text = \"o\u00f9 est l'arr\u00eat de bus ?\"\n>>> model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n\n>>> model = TFMarianMTModel.from_pretrained(model_name)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n>>> batch = tokenizer([sample_text], return_tensors=\"tf\")\n>>> gen = model.generate(**batch)\n>>> tokenizer.batch_decode(gen, skip_special_tokens=True)\n\"Where is the bus stop ?\"\n```", "```py\n( config: MarianConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMarianModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n>>> model = FlaxMarianModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: MarianConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMarianMTModel\n\n>>> model = FlaxMarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> input_ids = tokenizer(text, max_length=64, return_tensors=\"jax\").input_ids\n\n>>> sequences = model.generate(input_ids, max_length=64, num_beams=2).sequences\n\n>>> outputs = tokenizer.batch_decode(sequences, skip_special_tokens=True)\n>>> # should give *Meine Freunde sind cool, aber sie essen zu viele Kohlenhydrate.*\n```"]