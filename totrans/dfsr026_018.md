# 加载社区管道和组件

> 原文链接：[https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)

## 社区管道

社区管道是任何与原始实现不同的[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)类，如其论文中所述（例如，[StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)对应于[使用ControlNet条件生成文本到图像](https://arxiv.org/abs/2302.05543)的论文）。它们提供额外的功能或扩展了管道的原始实现。

有许多很酷的社区管道，比如[语音到图像](https://github.com/huggingface/diffusers/tree/main/examples/community#speech-to-image)或[可组合稳定扩散](https://github.com/huggingface/diffusers/tree/main/examples/community#composable-stable-diffusion)，您可以在[这里](https://github.com/huggingface/diffusers/tree/main/examples/community)找到所有官方社区管道。

要在Hub上加载任何社区管道，请将社区管道的存储库ID传递给`custom_pipeline`参数，并指定您希望从中加载管道权重和组件的模型存储库。例如，下面的示例从[`hf-internal-testing/diffusers-dummy-pipeline`](https://huggingface.co/hf-internal-testing/diffusers-dummy-pipeline/blob/main/pipeline.py)加载一个虚拟管道，从[`google/ddpm-cifar10-32`](https://huggingface.co/google/ddpm-cifar10-32)加载管道权重和组件：

🔒 通过从Hugging Face Hub加载社区管道，您正在信任您加载的代码是安全的。在自动加载和运行之前，请确保在线检查代码！

```py
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "google/ddpm-cifar10-32", custom_pipeline="hf-internal-testing/diffusers-dummy-pipeline", use_safetensors=True
)
```

加载官方社区管道类似，但您可以混合从官方存储库ID加载权重并直接传递管道组件。下面的示例加载了社区[CLIP引导稳定扩散](https://github.com/huggingface/diffusers/tree/main/examples/community#clip-guided-stable-diffusion)管道，您可以直接传递CLIP模型组件给它：

```py
from diffusers import DiffusionPipeline
from transformers import CLIPImageProcessor, CLIPModel

clip_model_id = "laion/CLIP-ViT-B-32-laion2B-s34B-b79K"

feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)
clip_model = CLIPModel.from_pretrained(clip_model_id)

pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    custom_pipeline="clip_guided_stable_diffusion",
    clip_model=clip_model,
    feature_extractor=feature_extractor,
    use_safetensors=True,
)
```

有关社区管道的更多信息，请查看[社区管道](custom_pipeline_examples)指南，了解如何使用它们，如果您有兴趣添加社区管道，请查看[如何贡献社区管道](contribute_pipeline)指南！

## 社区组件

社区组件允许用户构建可能具有自定义组件的管道，这些组件不是Diffusers的一部分。如果您的管道具有Diffusers尚未支持的自定义组件，您需要提供它们的实现作为Python模块。这些定制组件可以是VAE、UNet和调度器。在大多数情况下，文本编码器是从Transformers库导入的。管道代码本身也可以定制。

本节展示了用户应如何使用社区组件来构建社区管道。

您将使用[showlab/show-1-base](https://huggingface.co/showlab/show-1-base)管道检查点作为示例。因此，让我们开始加载组件：

1.  从Transformers导入并加载文本编码器：

```py
from transformers import T5Tokenizer, T5EncoderModel

pipe_id = "showlab/show-1-base"
tokenizer = T5Tokenizer.from_pretrained(pipe_id, subfolder="tokenizer")
text_encoder = T5EncoderModel.from_pretrained(pipe_id, subfolder="text_encoder")
```

1.  加载调度器：

```py
from diffusers import DPMSolverMultistepScheduler

scheduler = DPMSolverMultistepScheduler.from_pretrained(pipe_id, subfolder="scheduler")
```

1.  加载图像处理器：

```py
from transformers import CLIPFeatureExtractor

feature_extractor = CLIPFeatureExtractor.from_pretrained(pipe_id, subfolder="feature_extractor")
```

在步骤4和5中，自定义[UNet](https://github.com/showlab/Show-1/blob/main/showone/models/unet_3d_condition.py)和[pipeline](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)的实现必须与其文件中显示的格式匹配，以使此示例正常工作。

1.  现在，您将加载一个[自定义UNet](https://github.com/showlab/Show-1/blob/main/showone/models/unet_3d_condition.py)，在本示例中，已经在`showone_unet_3d_condition.py`[脚本](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)中实现了这个UNet，以方便您使用。您会注意到`UNet3DConditionModel`类名已更改为`ShowOneUNet3DConditionModel`，因为[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)已经存在于Diffusers中。`ShowOneUNet3DConditionModel`类所需的任何组件都应放在`showone_unet_3d_condition.py`脚本中。

完成后，您可以初始化UNet：

```py
from showone_unet_3d_condition import ShowOneUNet3DConditionModel

unet = ShowOneUNet3DConditionModel.from_pretrained(pipe_id, subfolder="unet")
```

1.  最后，您将加载自定义管道代码。在本示例中，已经为您在`pipeline_t2v_base_pixel.py`[脚本](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/pipeline_t2v_base_pixel.py)中创建了这个脚本，其中包含用于从文本生成视频的自定义`TextToVideoIFPipeline`类。与自定义UNet一样，任何用于使自定义管道正常工作的代码都应放在`pipeline_t2v_base_pixel.py`脚本中。

一切就绪后，您可以使用`ShowOneUNet3DConditionModel`初始化`TextToVideoIFPipeline`：

```py
from pipeline_t2v_base_pixel import TextToVideoIFPipeline
import torch

pipeline = TextToVideoIFPipeline(
    unet=unet,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    scheduler=scheduler,
    feature_extractor=feature_extractor
)
pipeline = pipeline.to(device="cuda")
pipeline.torch_dtype = torch.float16
```

将管道推送到Hub以与社区共享！

```py
pipeline.push_to_hub("custom-t2v-pipeline")
```

成功推送管道后，您需要进行一些更改：

1.  在[`model_index.json`](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/model_index.json#L2)中更改`_class_name`属性为`"pipeline_t2v_base_pixel"`和`"TextToVideoIFPipeline"`。

1.  将`showone_unet_3d_condition.py`上传到`unet`[目录](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)。

1.  将`pipeline_t2v_base_pixel.py`上传到管道基础[目录](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)。

要运行推理，只需在初始化管道时添加`trust_remote_code`参数，以处理所有幕后的“魔术”。

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(
    "<change-username>/<change-id>", trust_remote_code=True, torch_dtype=torch.float16
).to("cuda")

prompt = "hello"

# Text embeds
prompt_embeds, negative_embeds = pipeline.encode_prompt(prompt)

# Keyframes generation (8x64x40, 2fps)
video_frames = pipeline(
    prompt_embeds=prompt_embeds,
    negative_prompt_embeds=negative_embeds,
    num_frames=8,
    height=40,
    width=64,
    num_inference_steps=2,
    guidance_scale=9.0,
    output_type="pt"
).frames
```

作为额外的参考示例，您可以参考[stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl/)的存储库结构，该存储库利用了`trust_remote_code`功能：

```py

from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/japanese-stable-diffusion-xl", trust_remote_code=True
)
pipeline.to("cuda")

# if using torch < 2.0
# pipeline.enable_xformers_memory_efficient_attention()

prompt = "柴犬、カラフルアート"

image = pipeline(prompt=prompt).images[0]

```
