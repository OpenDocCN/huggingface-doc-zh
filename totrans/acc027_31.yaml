- en: Handling big models for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤„ç†æ¨æ–­ä¸­çš„å¤§å‹æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/accelerate/concept_guides/big_model_inference](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/concept_guides/big_model_inference](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When loading a pre-trained model in PyTorch, the usual workflow looks like
    this:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œé€šå¸¸çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In plain English, those steps are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨ç®€å•çš„è‹±è¯­æ¥è¯´ï¼Œè¿™äº›æ­¥éª¤æ˜¯ï¼š
- en: Create the model with randomly initialized weights
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºå…·æœ‰éšæœºåˆå§‹åŒ–æƒé‡çš„æ¨¡å‹
- en: Load the model weights (in a dictionary usually called a state dict) from the
    disk
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç£ç›˜åŠ è½½æ¨¡å‹æƒé‡ï¼ˆé€šå¸¸ç§°ä¸ºçŠ¶æ€å­—å…¸çš„å­—å…¸ï¼‰
- en: Load those weights inside the model
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è¿™äº›æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­
- en: 'While this works very well for regularly sized models, this workflow has some
    clear limitations when we deal with a huge model: in step 1, we load a full version
    of the model in RAM, and spend some time randomly initializing the weights (which
    will be discarded in step 3). In step 2, we load another full version of the model
    in RAM, with the pre-trained weights. If youâ€™re loading a model with 6 billion
    parameters, this means you will need 24GB of RAM for each copy of the model, so
    48GB in total (half of it to load the model in FP16).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å¯¹äºå¸¸è§„å¤§å°çš„æ¨¡å‹éå¸¸æœ‰æ•ˆï¼Œä½†æ˜¯å½“æˆ‘ä»¬å¤„ç†ä¸€ä¸ªå·¨å¤§çš„æ¨¡å‹æ—¶ï¼Œè¿™ä¸ªå·¥ä½œæµç¨‹æœ‰ä¸€äº›æ˜æ˜¾çš„å±€é™æ€§ï¼šåœ¨æ­¥éª¤1ä¸­ï¼Œæˆ‘ä»¬åœ¨RAMä¸­åŠ è½½æ¨¡å‹çš„å®Œæ•´ç‰ˆæœ¬ï¼Œå¹¶èŠ±è´¹ä¸€äº›æ—¶é—´éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆè¿™äº›æƒé‡å°†åœ¨æ­¥éª¤3ä¸­è¢«ä¸¢å¼ƒï¼‰ã€‚åœ¨æ­¥éª¤2ä¸­ï¼Œæˆ‘ä»¬åœ¨RAMä¸­åŠ è½½å¦ä¸€ä¸ªæ¨¡å‹çš„å®Œæ•´ç‰ˆæœ¬ï¼Œå…¶ä¸­åŒ…å«é¢„è®­ç»ƒçš„æƒé‡ã€‚å¦‚æœæ‚¨åŠ è½½ä¸€ä¸ªå…·æœ‰60äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¿™æ„å‘³ç€æ‚¨å°†éœ€è¦æ¯ä¸ªæ¨¡å‹å‰¯æœ¬24GBçš„RAMï¼Œå› æ­¤æ€»å…±éœ€è¦48GBï¼ˆå…¶ä¸­ä¸€åŠç”¨äºä»¥FP16åŠ è½½æ¨¡å‹ï¼‰ã€‚
- en: This API is quite new and still in its experimental stage. While we strive to
    provide a stable API, itâ€™s possible some small parts of the public API will change
    in the future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªAPIæ˜¯ç›¸å½“æ–°çš„ï¼Œä»å¤„äºå®éªŒé˜¶æ®µã€‚è™½ç„¶æˆ‘ä»¬åŠªåŠ›æä¾›ä¸€ä¸ªç¨³å®šçš„APIï¼Œä½†æœªæ¥å¯èƒ½ä¼šæ›´æ”¹å…¬å…±APIçš„ä¸€äº›å°éƒ¨åˆ†ã€‚
- en: 'How the Process Works: A Quick Overview'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµç¨‹å¦‚ä½•å·¥ä½œï¼šå¿«é€Ÿæ¦‚è¿°
- en: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
- en: 'How the Process Works: Working with Code'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµç¨‹å¦‚ä½•å·¥ä½œï¼šä½¿ç”¨ä»£ç 
- en: Instantiating an empty model
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹
- en: 'The first tool ğŸ¤— Accelerate introduces to help with big models is a context
    manager [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)
    that helps you initialize a model without using any RAM so that step 1 can be
    done on models of any size. Here is how it works:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateå¼•å…¥çš„ç¬¬ä¸€ä¸ªå·¥å…·æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)ï¼Œå®ƒå¯ä»¥å¸®åŠ©æ‚¨åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•RAMï¼Œä»¥ä¾¿å¯ä»¥å¯¹ä»»ä½•å¤§å°çš„æ¨¡å‹æ‰§è¡Œæ­¥éª¤1ã€‚è¿™æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼š
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For instance:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: initializes an empty model with a bit more than 100B parameters. Behind the
    scenes, this relies on the meta device introduced in PyTorch 1.9\. During the
    initialization under the context manager, each time a parameter is created, it
    is instantly moved to that device.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¶…è¿‡100Bå‚æ•°åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹ã€‚åœ¨å¹•åï¼Œè¿™ä¾èµ–äºPyTorch 1.9ä¸­å¼•å…¥çš„å…ƒè®¾å¤‡ã€‚åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹çš„åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡åˆ›å»ºå‚æ•°æ—¶ï¼Œå®ƒéƒ½ä¼šç«‹å³ç§»åŠ¨åˆ°è¯¥è®¾å¤‡ã€‚
- en: You canâ€™t move a model initialized like this on CPU or another device directly,
    since it doesnâ€™t have any data. Itâ€™s also very likely that a forward pass with
    that empty model will fail, as not all operations are supported on the meta device.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¸èƒ½ç›´æ¥å°†åƒè¿™æ ·åˆå§‹åŒ–çš„æ¨¡å‹ç§»åŠ¨åˆ°CPUæˆ–å¦ä¸€ä¸ªè®¾å¤‡ï¼Œå› ä¸ºå®ƒæ²¡æœ‰ä»»ä½•æ•°æ®ã€‚ä½¿ç”¨è¿™ç§ç©ºæ¨¡å‹è¿›è¡Œå‰å‘ä¼ é€’å¾ˆå¯èƒ½ä¼šå¤±è´¥ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰æ“ä½œéƒ½æ”¯æŒåœ¨å…ƒè®¾å¤‡ä¸Šè¿›è¡Œã€‚
- en: Sharded checkpoints
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡æ£€æŸ¥ç‚¹
- en: 'Itâ€™s possible your model is so big that even a single copy wonâ€™t fit in RAM.
    That doesnâ€™t mean it canâ€™t be loaded: if you have one or several GPUs, this is
    more memory available to store your model. In this case, itâ€™s better if your checkpoint
    is split into several smaller files that we call checkpoint shards.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æ‚¨çš„æ¨¡å‹å¤ªå¤§ï¼Œå³ä½¿å•ä¸ªå‰¯æœ¬ä¹Ÿæ— æ³•æ”¾å…¥RAMã€‚è¿™å¹¶ä¸æ„å‘³ç€å®ƒä¸èƒ½è¢«åŠ è½½ï¼šå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªGPUï¼Œè¿™æ„å‘³ç€æœ‰æ›´å¤šå†…å­˜å¯ç”¨äºå­˜å‚¨æ‚¨çš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€å¥½å°†æ‚¨çš„æ£€æŸ¥ç‚¹åˆ†æˆå‡ ä¸ªæ›´å°çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ£€æŸ¥ç‚¹åˆ†ç‰‡ã€‚
- en: 'ğŸ¤— Accelerate will handle sharded checkpoints as long as you follow the following
    format: your checkpoint should be in a folder, with several files containing the
    partial state dicts, and there should be an index in the JSON format that contains
    a dictionary mapping parameter names to the file containing their weights. You
    can easily shard your model with [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model).
    For instance, we could have a folder containing:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateå°†å¤„ç†åˆ†ç‰‡æ£€æŸ¥ç‚¹ï¼Œåªè¦æ‚¨éµå¾ªä»¥ä¸‹æ ¼å¼ï¼šæ‚¨çš„æ£€æŸ¥ç‚¹åº”è¯¥åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œå…¶ä¸­åŒ…å«å‡ ä¸ªåŒ…å«éƒ¨åˆ†çŠ¶æ€å­—å…¸çš„æ–‡ä»¶ï¼Œå¹¶ä¸”åº”è¯¥æœ‰ä¸€ä¸ªä»¥JSONæ ¼å¼çš„ç´¢å¼•ï¼Œå…¶ä¸­åŒ…å«å°†å‚æ•°åç§°æ˜ å°„åˆ°åŒ…å«å…¶æƒé‡çš„æ–‡ä»¶çš„å­—å…¸ã€‚æ‚¨å¯ä»¥ä½¿ç”¨
    [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    è½»æ¾åœ°å°†æ¨¡å‹åˆ†ç‰‡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰ä¸€ä¸ªåŒ…å«çš„æ–‡ä»¶å¤¹ï¼š
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'with index.json being the following file:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: index.json æ˜¯ä»¥ä¸‹æ–‡ä»¶ï¼š
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and `first_state_dict.bin` containing the weights for `"linear1.weight"` and
    `"linear1.bias"`, `second_state_dict.bin` the ones for `"linear2.weight"` and
    `"linear2.bias"`
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ `first_state_dict.bin` åŒ…å« `"linear1.weight"` å’Œ `"linear1.bias"` çš„æƒé‡ï¼Œ`second_state_dict.bin`
    åŒ…å« `"linear2.weight"` å’Œ `"linear2.bias"` çš„æƒé‡
- en: Loading weights
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ è½½æƒé‡
- en: The second tool ğŸ¤— Accelerate introduces is a function [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch),
    that will allow you to load a checkpoint inside your empty model. This supports
    full checkpoints (a single file containing the whole state dict) as well as sharded
    checkpoints. It will also automatically dispatch those weights across the devices
    you have available (GPUs, CPU RAM), so if you are loading a sharded checkpoint,
    the maximum RAM usage will be the size of the biggest shard.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateå¼•å…¥çš„ç¬¬äºŒä¸ªå·¥å…·æ˜¯ä¸€ä¸ªå‡½æ•° [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)ï¼Œå®ƒå°†å…è®¸æ‚¨åœ¨ç©ºæ¨¡å‹ä¸­åŠ è½½æ£€æŸ¥ç‚¹ã€‚è¿™æ”¯æŒå®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«æ•´ä¸ªçŠ¶æ€å­—å…¸çš„å•ä¸ªæ–‡ä»¶ï¼‰ä»¥åŠåˆ†ç‰‡æ£€æŸ¥ç‚¹ã€‚å®ƒè¿˜å°†è‡ªåŠ¨å°†è¿™äº›æƒé‡åˆ†é…åˆ°æ‚¨å¯ç”¨çš„è®¾å¤‡ä¸Šï¼ˆGPUã€CPU
    RAMï¼‰ï¼Œå› æ­¤å¦‚æœæ‚¨åŠ è½½ä¸€ä¸ªåˆ†ç‰‡æ£€æŸ¥ç‚¹ï¼Œæœ€å¤§çš„RAMä½¿ç”¨é‡å°†æ˜¯æœ€å¤§åˆ†ç‰‡çš„å¤§å°ã€‚
- en: If you want to use big model inference with ğŸ¤— Transformers models, check out
    this [documentation](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³ä½¿ç”¨ ğŸ¤— Transformers æ¨¡å‹è¿›è¡Œå¤§å‹æ¨¡å‹æ¨æ–­ï¼Œè¯·æŸ¥çœ‹è¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading)ã€‚
- en: Here is how we can use this to load the [GPT2-1.5B](https://huggingface.co/marcsun13/gpt2-xl-linear-sharded)
    model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨è¿™ä¸ªåŠ è½½ [GPT2-1.5B](https://huggingface.co/marcsun13/gpt2-xl-linear-sharded)
    æ¨¡å‹ã€‚
- en: Letâ€™s download the sharded version of this model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸‹è½½è¿™ä¸ªæ¨¡å‹çš„åˆ†ç‰‡ç‰ˆæœ¬ã€‚
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to initialize the model, we will use the library minGPT.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆå§‹åŒ–æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ minGPT åº“ã€‚
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, load the checkpoint we just downloaded with:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼ŒåŠ è½½æˆ‘ä»¬åˆšä¸‹è½½çš„æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'By passing `device_map="auto"`, we tell ğŸ¤— Accelerate to determine automatically
    where to put each layer of the model depending on the available resources:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¼ é€’ `device_map="auto"`ï¼Œæˆ‘ä»¬å‘Šè¯‰ ğŸ¤— Accelerate æ ¹æ®å¯ç”¨èµ„æºè‡ªåŠ¨ç¡®å®šå°†æ¨¡å‹çš„æ¯ä¸€å±‚æ”¾åœ¨å“ªé‡Œï¼š
- en: first, we use the maximum space available on the GPU(s)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ GPU ä¸Šå¯ç”¨çš„æœ€å¤§ç©ºé—´
- en: if we still need space, we store the remaining weights on the CPU
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä»ç„¶éœ€è¦ç©ºé—´ï¼Œæˆ‘ä»¬å°†å‰©ä½™çš„æƒé‡å­˜å‚¨åœ¨ CPU ä¸Š
- en: if there is not enough RAM, we store the remaining weights on the hard drive
    as memory-mapped tensors
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ RAMï¼Œæˆ‘ä»¬å°†å‰©ä½™çš„æƒé‡å­˜å‚¨åœ¨ç¡¬ç›˜ä¸Šä½œä¸ºå†…å­˜æ˜ å°„å¼ é‡
- en: no_split_module_classes
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸è¦åˆ†å‰²æ¨¡å—ç±»
- en: This parameter will indicate that some of the modules with the name `"Block"`
    should not be split across different devices. You should set here all blocks that
    include a residual connection of some kind.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‚æ•°å°†æŒ‡ç¤ºä¸€äº›åç§°ä¸º "Block" çš„æ¨¡å—ä¸åº”è¯¥åœ¨ä¸åŒè®¾å¤‡ä¹‹é—´åˆ†å‰²ã€‚æ‚¨åº”è¯¥åœ¨è¿™é‡Œè®¾ç½®åŒ…æ‹¬æŸç§æ®‹å·®è¿æ¥çš„æ‰€æœ‰å—ã€‚
- en: The device_map
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¾å¤‡æ˜ å°„
- en: 'You can see the `device_map` that ğŸ¤— Accelerate picked by accessing the `hf_device_map`
    attribute of your model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡è®¿é—®æ‚¨çš„æ¨¡å‹çš„ `hf_device_map` å±æ€§æ¥æŸ¥çœ‹ ğŸ¤— Accelerate é€‰æ‹©çš„ `device_map`ï¼š
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Itâ€™s fully possible to create your own device map for the layers to use as
    well, specifying the GPU device to use (a number), `"cpu"`, or `"disk"` and pass
    this in:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨å¯ä»¥ä¸ºå±‚åˆ›å»ºè‡ªå·±çš„è®¾å¤‡æ˜ å°„ï¼ŒæŒ‡å®šè¦ä½¿ç”¨çš„ GPU è®¾å¤‡ï¼ˆä¸€ä¸ªæ•°å­—ï¼‰ã€"cpu" æˆ– "disk"ï¼Œç„¶åä¼ å…¥ï¼š
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Run the model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿è¡Œæ¨¡å‹
- en: 'Now that we have done this, our model lies across several devices, and maybe
    the hard drive. But it can still be used as a regular PyTorch model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åˆ†å¸ƒåœ¨å‡ ä¸ªè®¾å¤‡ä¸Šï¼Œä¹Ÿè®¸è¿˜æœ‰ç¡¬ç›˜ã€‚ä½†å®ƒä»ç„¶å¯ä»¥åƒå¸¸è§„çš„ PyTorch æ¨¡å‹ä¸€æ ·ä½¿ç”¨ï¼š
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Behind the scenes, ğŸ¤— Accelerate added hooks to the model, so that:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹•åï¼ŒğŸ¤— Accelerate ä¸ºæ¨¡å‹æ·»åŠ äº†é’©å­ï¼Œä»¥ä¾¿ï¼š
- en: at each layer, the inputs are put on the right device (so even if your model
    is spread across several GPUs, it works)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€å±‚ï¼Œè¾“å…¥éƒ½è¢«æ”¾åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆæ‰€ä»¥å³ä½¿æ‚¨çš„æ¨¡å‹åˆ†å¸ƒåœ¨å‡ ä¸ª GPU ä¸Šï¼Œå®ƒä¹Ÿå¯ä»¥å·¥ä½œï¼‰
- en: for the weights offloaded on the CPU, they are put on a GPU just before the
    forward pass and cleaned up just after
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¸è½½åˆ° CPU ä¸Šçš„æƒé‡ï¼Œåœ¨å‰å‘ä¼ é€’ä¹‹å‰å°†å®ƒä»¬æ”¾åœ¨ GPU ä¸Šï¼Œç„¶ååœ¨åé¢æ¸…ç†
- en: for the weights offloaded on the hard drive, they are loaded in RAM then put
    on a GPU just before the forward pass and cleaned up just after
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¸è½½åˆ°ç¡¬ç›˜ä¸Šçš„æƒé‡ï¼Œåœ¨å‰å‘ä¼ é€’ä¹‹å‰å°†å®ƒä»¬åŠ è½½åˆ° RAM ä¸­ï¼Œç„¶ååœ¨åé¢æ¸…ç†
- en: This way, your model can run for inference even if it doesnâ€™t fit on one of
    the GPUs or the CPU RAM!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œå³ä½¿æ¨¡å‹ä¸é€‚åˆåœ¨ä¸€ä¸ª GPU æˆ– CPU RAM ä¸Šè¿è¡Œæ¨æ–­ï¼Œä¹Ÿå¯ä»¥è¿è¡Œæ‚¨çš„æ¨¡å‹ï¼
- en: This only supports the inference of your model, not training. Most of the computation
    happens behind `torch.no_grad()` context managers to avoid spending some GPU memory
    with intermediate activations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ”¯æŒæ¨¡å‹çš„æ¨æ–­ï¼Œä¸æ”¯æŒè®­ç»ƒã€‚å¤§éƒ¨åˆ†è®¡ç®—å‘ç”Ÿåœ¨ `torch.no_grad()` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼Œä»¥é¿å…ç”¨ä¸­é—´æ¿€æ´»æ¶ˆè€—ä¸€äº› GPU å†…å­˜ã€‚
- en: Designing a device map
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾è®¡è®¾å¤‡æ˜ å°„
- en: You can let ğŸ¤— Accelerate handle the device map computation by setting `device_map`
    to one of the supported options (`"auto"`, `"balanced"`, `"balanced_low_0"`, `"sequential"`)
    or create one yourself if you want more control over where each layer should go.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥è®© ğŸ¤— Accelerate å¤„ç†è®¾å¤‡æ˜ å°„è®¡ç®—ï¼Œæ–¹æ³•æ˜¯å°† `device_map` è®¾ç½®ä¸ºæ”¯æŒçš„é€‰é¡¹ä¹‹ä¸€ï¼ˆ"auto"ã€"balanced"ã€"balanced_low_0"ã€"sequential"ï¼‰ï¼Œæˆ–è€…å¦‚æœæ‚¨æƒ³æ›´å¤šåœ°æ§åˆ¶æ¯ä¸ªå±‚åº”è¯¥æ”¾åœ¨å“ªé‡Œï¼Œå¯ä»¥è‡ªå·±åˆ›å»ºä¸€ä¸ªã€‚
- en: You can derive all sizes of the model (and thus compute a `device_map`) on a
    model that is on the meta device.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨å…ƒè®¾å¤‡ä¸Šçš„æ¨¡å‹ä¸Šæ¨å¯¼å‡ºæ¨¡å‹çš„æ‰€æœ‰å¤§å°ï¼ˆä»è€Œè®¡ç®—å‡º `device_map`ï¼‰ã€‚
- en: All the options will produce the same result when you donâ€™t have enough GPU
    memory to accommodate the whole model (which is to fit everything that can on
    the GPU, then offload weights on the CPU or even on the disk if there is not enough
    RAM).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ²¡æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ¥å®¹çº³æ•´ä¸ªæ¨¡å‹æ—¶ï¼Œæ‰€æœ‰é€‰é¡¹éƒ½ä¼šäº§ç”Ÿç›¸åŒçš„ç»“æœï¼ˆå³å°†å¯ä»¥æ”¾åœ¨ GPU ä¸Šçš„æ‰€æœ‰å†…å®¹æ”¾åœ¨ GPU ä¸Šï¼Œç„¶åå°†æƒé‡å¸è½½åˆ° CPU ä¸Šï¼Œç”šè‡³åœ¨æ²¡æœ‰è¶³å¤Ÿçš„
    RAM æ—¶ä¹Ÿå¯ä»¥æ”¾åœ¨ç£ç›˜ä¸Šï¼‰ã€‚
- en: 'When you have more GPU memory available than the model size, here is the difference
    between each option:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨æ‹¥æœ‰æ¯”æ¨¡å‹å¤§å°æ›´å¤šçš„ GPU å†…å­˜æ—¶ï¼Œæ¯ä¸ªé€‰é¡¹ä¹‹é—´çš„åŒºåˆ«å¦‚ä¸‹ï¼š
- en: '`"auto"` and `"balanced"` evenly split the model on all available GPUs, making
    it possible for you to use a batch size greater than 1.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"auto" å’Œ "balanced" å°†æ¨¡å‹å‡åŒ€åˆ†å‰²åœ¨æ‰€æœ‰å¯ç”¨çš„ GPU ä¸Šï¼Œä½¿æ‚¨å¯ä»¥ä½¿ç”¨å¤§äº 1 çš„æ‰¹é‡å¤§å°ã€‚'
- en: '`"balanced_low_0"` evenly splits the model on all GPUs except the first one,
    and only puts on GPU 0 what does not fit on the others. This option is great when
    you need to use GPU 0 for some processing of the outputs, like when using the
    `generate` function for Transformers models'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"balanced_low_0" å°†æ¨¡å‹å‡åŒ€åˆ†å‰²åœ¨é™¤ç¬¬ä¸€ä¸ªä¹‹å¤–çš„æ‰€æœ‰ GPU ä¸Šï¼Œå¹¶ä¸”åªå°†ä¸é€‚åˆå…¶ä»– GPU çš„å†…å®¹æ”¾åœ¨ GPU 0 ä¸Šã€‚å½“æ‚¨éœ€è¦ä½¿ç”¨
    GPU 0 å¤„ç†è¾“å‡ºæ—¶ï¼Œæ¯”å¦‚ä½¿ç”¨ Transformers æ¨¡å‹çš„ `generate` å‡½æ•°æ—¶ï¼Œè¿™ä¸ªé€‰é¡¹éå¸¸é€‚ç”¨'
- en: '`"sequential"` will fit what it can on GPU 0, then move on GPU 1 and so forth
    (so wonâ€™t use the last GPUs if it doesnâ€™t need to).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"sequential" å°†å°½å¯èƒ½å¤šåœ°æ”¾åœ¨ GPU 0 ä¸Šï¼Œç„¶åç§»åŠ¨åˆ° GPU 1 ç­‰ç­‰ï¼ˆå¦‚æœä¸éœ€è¦ï¼Œå°±ä¸ä¼šä½¿ç”¨æœ€åçš„ GPUï¼‰ã€‚'
- en: The options `"auto"` and `"balanced"` produce the same results for now, but
    the behavior of `"auto"` might change in the future if we find a strategy that
    makes more sense, while `"balanced"` will stay stable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '"auto" å’Œ "balanced" é€‰é¡¹ç›®å‰äº§ç”Ÿç›¸åŒçš„ç»“æœï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æ‰¾åˆ°æ›´åˆç†çš„ç­–ç•¥ï¼Œ"auto" çš„è¡Œä¸ºå¯èƒ½ä¼šåœ¨å°†æ¥å‘ç”Ÿå˜åŒ–ï¼Œè€Œ "balanced"
    å°†ä¿æŒç¨³å®šã€‚'
- en: First note that you can limit the memory used on each GPU by using the `max_memory`
    argument (available in [infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    and in all functions using it). When setting `max_memory`, you should pass along
    a dictionary containing the GPU identifiers (for instance `0`, `1` etc.) and the
    `"cpu"` key for the maximum RAM you want to use for CPU offload. The values can
    either be an integer (in bytes) or a string representing a number with its unit,
    such as `"10GiB"` or `"10GB"`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`max_memory`å‚æ•°ï¼ˆåœ¨[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)å’Œæ‰€æœ‰ä½¿ç”¨å®ƒçš„å‡½æ•°ä¸­å¯ç”¨ï¼‰é™åˆ¶æ¯ä¸ªGPUä¸Šä½¿ç”¨çš„å†…å­˜ã€‚è®¾ç½®`max_memory`æ—¶ï¼Œåº”ä¼ é€’ä¸€ä¸ªåŒ…å«GPUæ ‡è¯†ç¬¦ï¼ˆä¾‹å¦‚`0`ã€`1`ç­‰ï¼‰å’Œ`"cpu"`é”®çš„å­—å…¸ï¼Œç”¨äºCPUå¸è½½çš„æœ€å¤§RAMã€‚å€¼å¯ä»¥æ˜¯æ•´æ•°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰æˆ–è¡¨ç¤ºå¸¦å•ä½çš„æ•°å­—çš„å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚`"10GiB"`æˆ–`"10GB"`ã€‚
- en: 'Here is an example where we donâ€™t want to use more than 10GiB on each of the
    two GPUs and no more than 30GiB of CPU RAM for the model weights:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›åœ¨ä¸¤ä¸ªGPUä¸Šä½¿ç”¨è¶…è¿‡10GiBï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡çš„CPU RAMä¸è¶…è¿‡30GiBï¼š
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When a first allocation happens in PyTorch, it loads CUDA kernels which take
    about 1-2GB of memory depending on the GPU. Therefore you always have less usable
    memory than the actual size of the GPU. To see how much memory is actually used
    do `torch.ones(1).cuda()` and look at the memory usage.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å½“PyTorchä¸­å‘ç”Ÿç¬¬ä¸€æ¬¡åˆ†é…æ—¶ï¼Œå®ƒä¼šåŠ è½½å¤§çº¦1-2GBçš„CUDAå†…æ ¸ï¼Œå…·ä½“å–å†³äºGPUã€‚å› æ­¤ï¼Œæ‚¨å§‹ç»ˆæ¯”GPUçš„å®é™…å¤§å°å¯ç”¨å†…å­˜å°‘ã€‚è¦æŸ¥çœ‹å®é™…ä½¿ç”¨äº†å¤šå°‘å†…å­˜ï¼Œè¯·æ‰§è¡Œ`torch.ones(1).cuda()`å¹¶æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µã€‚
- en: Therefore when you create memory maps with `max_memory` make sure to adjust
    the available memory accordingly to avoid out-of-memory errors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“æ‚¨ä½¿ç”¨`max_memory`åˆ›å»ºå†…å­˜æ˜ å°„æ—¶ï¼Œè¯·ç¡®ä¿æ ¹æ®å¯ç”¨å†…å­˜è¿›è¡Œç›¸åº”è°ƒæ•´ï¼Œä»¥é¿å…å†…å­˜ä¸è¶³é”™è¯¯ã€‚
- en: 'Additionally, if you do some additional operations with your outputs without
    placing them back on the CPU (for instance inside the `generate` method of Transformers)
    and if you placed your inputs on a GPU, that GPU will consume more memory than
    the others (Accelerate always place the output back to the device of the input).
    Therefore if you would like to optimize the maximum batch size and you have many
    GPUs, give the first GPU less memory. For example, with BLOOM-176B on 8x80 A100
    setup, the close-to-ideal map is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¦‚æœæ‚¨å¯¹è¾“å‡ºæ‰§è¡Œä¸€äº›é¢å¤–çš„æ“ä½œï¼Œè€Œä¸å°†å…¶æ”¾å›CPUï¼ˆä¾‹å¦‚åœ¨Transformersçš„`generate`æ–¹æ³•ä¸­ï¼‰ï¼Œå¹¶ä¸”å°†è¾“å…¥æ”¾åœ¨GPUä¸Šï¼Œåˆ™è¯¥GPUå°†æ¶ˆè€—æ¯”å…¶ä»–GPUæ›´å¤šçš„å†…å­˜ï¼ˆAccelerateå§‹ç»ˆå°†è¾“å‡ºæ”¾å›åˆ°è¾“å…¥è®¾å¤‡ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨æƒ³è¦ä¼˜åŒ–æœ€å¤§æ‰¹å¤„ç†å¤§å°ï¼Œå¹¶ä¸”æ‚¨æœ‰è®¸å¤šGPUï¼Œè¯·ç»™ç¬¬ä¸€ä¸ªGPUåˆ†é…æ›´å°‘çš„å†…å­˜ã€‚ä¾‹å¦‚ï¼Œåœ¨8x80
    A100è®¾ç½®ä¸­ä½¿ç”¨BLOOM-176Bï¼Œæ¥è¿‘ç†æƒ³çš„æ˜ å°„æ˜¯ï¼š
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: as you can see we gave the remaining 7 GPUs ~50% more memory than GPU 0.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ç»™å‰©ä¸‹çš„7ä¸ªGPUåˆ†é…äº†æ¯”GPU 0å¤šçº¦50%çš„å†…å­˜ã€‚
- en: 'If you opt to fully design the `device_map` yourself, it should be a dictionary
    with keys being module names of your model and values being a valid device identifier
    (for instance an integer for the GPUs) or `"cpu"` for CPU offload, `"disk"` for
    disk offload. The keys need to cover the whole model, you can then define your
    device map as you wish: for instance, if your model has two blocks (letâ€™s say
    `block1` and `block2`) which each contain three linear layers (letâ€™s say `linear1`,
    `linear2` and `linear3`), a valid device map can be:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨é€‰æ‹©å®Œå…¨è‡ªå·±è®¾è®¡`device_map`ï¼Œå®ƒåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®æ˜¯æ‚¨æ¨¡å‹çš„æ¨¡å—åç§°ï¼Œå€¼æ˜¯æœ‰æ•ˆçš„è®¾å¤‡æ ‡è¯†ç¬¦ï¼ˆä¾‹å¦‚GPUçš„æ•´æ•°ï¼‰æˆ–`"cpu"`è¡¨ç¤ºCPUå¸è½½ï¼Œ`"disk"`è¡¨ç¤ºç£ç›˜å¸è½½ã€‚é”®éœ€è¦è¦†ç›–æ•´ä¸ªæ¨¡å‹ï¼Œç„¶åæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šä¹‰è®¾å¤‡æ˜ å°„ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹æœ‰ä¸¤ä¸ªå—ï¼ˆå‡è®¾æ˜¯`block1`å’Œ`block2`ï¼‰ï¼Œæ¯ä¸ªå—åŒ…å«ä¸‰ä¸ªçº¿æ€§å±‚ï¼ˆå‡è®¾æ˜¯`linear1`ã€`linear2`å’Œ`linear3`ï¼‰ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„è®¾å¤‡æ˜ å°„å¯ä»¥æ˜¯ï¼š
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'another one that is valid could be:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰æ•ˆçš„ç¤ºä¾‹å¯èƒ½æ˜¯ï¼š
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On the other hand, this one is not valid as it does not cover every parameter
    of the model:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œè¿™ä¸ªæ˜ å°„æ˜¯æ— æ•ˆçš„ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æ¶µç›–æ¨¡å‹çš„æ¯ä¸ªå‚æ•°ï¼š
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To be the most efficient, make sure your device map puts the parameters on the
    GPUs in a sequential manner (e.g. donâ€™t put one of the first weights on GPU 0,
    then weights on GPU 1 and the last weight back to GPU 0) to avoid making many
    transfers of data between the GPUs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æœ€æœ‰æ•ˆåœ°ä½¿ç”¨ï¼Œè¯·ç¡®ä¿æ‚¨çš„è®¾å¤‡æ˜ å°„ä»¥é¡ºåºæ–¹å¼å°†å‚æ•°æ”¾åœ¨GPUä¸Šï¼ˆä¾‹å¦‚ï¼Œä¸è¦å°†ç¬¬ä¸€ä¸ªæƒé‡æ”¾åœ¨GPU 0ä¸Šï¼Œç„¶åå°†æƒé‡æ”¾åœ¨GPU 1ä¸Šï¼Œæœ€åå°†æƒé‡æ”¾å›GPU
    0ï¼‰ï¼Œä»¥é¿å…åœ¨GPUä¹‹é—´è¿›è¡Œå¤šæ¬¡æ•°æ®ä¼ è¾“ã€‚
- en: CPU offload only
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»…CPUå¸è½½
- en: If you want to offload your model on CPU, you can use [cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload).
    As a result, all parameters of the model will be offloaded and only one copy of
    the state dict of the model will be kept. During the forward pass, parameters
    will be extracted from that state dict and put on the execution device and passed
    as they are needed, then offloaded again.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦åœ¨CPUä¸Šå¸è½½æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨[cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload)ã€‚ç»“æœï¼Œæ¨¡å‹çš„æ‰€æœ‰å‚æ•°å°†è¢«å¸è½½ï¼Œæ¨¡å‹çš„çŠ¶æ€å­—å…¸åªä¼šä¿ç•™ä¸€ä»½å‰¯æœ¬ã€‚åœ¨å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­ï¼Œå‚æ•°å°†ä»è¯¥çŠ¶æ€å­—å…¸ä¸­æå–å‡ºæ¥ï¼Œå¹¶æ”¾åœ¨æ‰§è¡Œè®¾å¤‡ä¸Šï¼Œå¹¶åœ¨éœ€è¦æ—¶ä¼ é€’ï¼Œç„¶åå†æ¬¡å¸è½½ã€‚
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can also use [cpu_offload_with_hook()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload_with_hook).
    This function will offloads a model on the CPU and puts it back to an execution
    device when executed. The difference with [cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload)
    is that the model stays on the execution device after the forward and is only
    offloaded again when the `offload` method of the returned `hook` is called. Furthermore,
    [cpu_offload_with_hook()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload_with_hook)
    is more performant but less memory saving. It is useful for pipelines running
    a model in a loop:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[cpu_offload_with_hook()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload_with_hook)ã€‚æ­¤å‡½æ•°å°†æ¨¡å‹å¸è½½åˆ°CPUï¼Œå¹¶åœ¨æ‰§è¡Œæ—¶å°†å…¶æ”¾å›æ‰§è¡Œè®¾å¤‡ã€‚ä¸[cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload)çš„åŒºåˆ«åœ¨äºï¼Œæ¨¡å‹åœ¨å‰å‘ä¼ é€’åä»ä¿ç•™åœ¨æ‰§è¡Œè®¾å¤‡ä¸Šï¼Œå¹¶ä¸”ä»…åœ¨è°ƒç”¨è¿”å›çš„`hook`çš„`offload`æ–¹æ³•æ—¶å†æ¬¡å¸è½½ã€‚æ­¤å¤–ï¼Œ[cpu_offload_with_hook()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload_with_hook)æ›´é«˜æ•ˆä½†èŠ‚çœçš„å†…å­˜è¾ƒå°‘ã€‚å®ƒé€‚ç”¨äºåœ¨å¾ªç¯ä¸­è¿è¡Œæ¨¡å‹çš„ç®¡é“ï¼š
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Disk offload only
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»…ç£ç›˜å¸è½½
- en: To perform disk offload, you can use [disk_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.disk_offload).
    As a result, all parameters of the model will be offloaded as memory-mapped array
    in a given folder. During the forward pass, parameters will be accessed from that
    folder and put on the execution device passed as they are needed, then offloaded
    again.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œç£ç›˜å¸è½½ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[disk_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.disk_offload)ã€‚ç»“æœï¼Œæ¨¡å‹çš„æ‰€æœ‰å‚æ•°å°†ä½œä¸ºå†…å­˜æ˜ å°„æ•°ç»„å¸è½½åˆ°ç»™å®šæ–‡ä»¶å¤¹ä¸­ã€‚åœ¨å‰å‘ä¼ é€’æœŸé—´ï¼Œå‚æ•°å°†ä»è¯¥æ–‡ä»¶å¤¹è®¿é—®ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ”¾åœ¨ä¼ é€’çš„æ‰§è¡Œè®¾å¤‡ä¸Šï¼Œç„¶åå†æ¬¡å¸è½½ã€‚
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Limits and further development
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™åˆ¶å’Œè¿›ä¸€æ­¥å‘å±•
- en: 'We are aware of the current limitations in the API:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ„è¯†åˆ°APIä¸­ç›®å‰çš„é™åˆ¶ï¼š
- en: '[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    (or `device_map="auto"` in [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch))
    tries to maximize GPU and CPU RAM it sees available when you execute it. While
    PyTorch is very good at managing GPU RAM efficiently (and giving it back when
    not needed), itâ€™s not entirely true with Python and CPU RAM. Therefore, an automatically
    computed device map might be too intense on the CPU. Move a few modules to the
    disk device if you get crashes due to a lack of RAM.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)ï¼ˆæˆ–åœ¨[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)ä¸­ä½¿ç”¨`device_map="auto"`ï¼‰ä¼šå°è¯•åœ¨æ‰§è¡Œæ—¶æœ€å¤§åŒ–å®ƒçœ‹åˆ°çš„å¯ç”¨GPUå’ŒCPU
    RAMã€‚è™½ç„¶PyTorchéå¸¸æ“…é•¿æœ‰æ•ˆç®¡ç†GPU RAMï¼ˆå¹¶åœ¨ä¸éœ€è¦æ—¶å°†å…¶å½’è¿˜ï¼‰ï¼Œä½†å¯¹äºPythonå’ŒCPU RAMæ¥è¯´å¹¶éå®Œå…¨å¦‚æ­¤ã€‚å› æ­¤ï¼Œè‡ªåŠ¨è®¡ç®—çš„è®¾å¤‡æ˜ å°„å¯èƒ½å¯¹CPUå¤ªè¿‡å¯†é›†ã€‚å¦‚æœç”±äºRAMä¸è¶³è€Œå¯¼è‡´å´©æºƒï¼Œè¯·å°†ä¸€äº›æ¨¡å—ç§»åŠ¨åˆ°ç£ç›˜è®¾å¤‡ã€‚'
- en: '[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    (or `device_map="auto"` in [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch))
    attributes devices sequentially (to avoid moving things back and forth) so if
    your first layer is bigger than the size of the GPU you have, it will end up with
    everything on the CPU/Disk.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)ï¼ˆæˆ–åœ¨[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)ä¸­ä½¿ç”¨`device_map="auto"`ï¼‰å±æ€§ä¼šæŒ‰é¡ºåºåˆ†é…è®¾å¤‡ï¼ˆä»¥é¿å…æ¥å›ç§»åŠ¨ï¼‰ï¼Œå› æ­¤å¦‚æœæ‚¨çš„ç¬¬ä¸€å±‚æ¯”æ‚¨æ‹¥æœ‰çš„GPUå¤§å°è¿˜è¦å¤§ï¼Œå®ƒæœ€ç»ˆä¼šå°†æ‰€æœ‰å†…å®¹æ”¾åœ¨CPU/ç£ç›˜ä¸Šã€‚'
- en: '[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)
    and [load_checkpoint_in_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.load_checkpoint_in_model)
    do not perform any check on the correctness of your state dict compared to your
    model at the moment (this will be fixed in a future version), so you may get some
    weird errors if trying to load a checkpoint with mismatched or missing keys.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)å’Œ[load_checkpoint_in_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.load_checkpoint_in_model)ç›®å‰ä¸ä¼šå¯¹æ‚¨çš„çŠ¶æ€å­—å…¸ä¸æ¨¡å‹çš„æ­£ç¡®æ€§è¿›è¡Œä»»ä½•æ£€æŸ¥ï¼ˆè¿™å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­ä¿®å¤ï¼‰ï¼Œå› æ­¤å¦‚æœå°è¯•åŠ è½½å…·æœ‰ä¸åŒ¹é…æˆ–ç¼ºå¤±é”®çš„æ£€æŸ¥ç‚¹ï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°ä¸€äº›å¥‡æ€ªçš„é”™è¯¯ã€‚'
- en: The model parallelism used when your model is split on several GPUs is naive
    and not optimized, meaning that only one GPU works at a given time and the other
    sits idle.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æ‚¨çš„æ¨¡å‹åœ¨å¤šä¸ªGPUä¸Šåˆ†å‰²æ—¶ä½¿ç”¨çš„æ¨¡å‹å¹¶è¡Œæ€§æ˜¯å¤©çœŸçš„å¹¶ä¸”æ²¡æœ‰ä¼˜åŒ–ï¼Œè¿™æ„å‘³ç€åªæœ‰ä¸€ä¸ªGPUåœ¨ç»™å®šæ—¶é—´å·¥ä½œï¼Œè€Œå¦ä¸€ä¸ªå¤„äºç©ºé—²çŠ¶æ€ã€‚
- en: When weights are offloaded on the CPU/hard drive, there is no pre-fetching (yet,
    we will work on this for future versions) which means the weights are put on the
    GPU when they are needed and not before.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æƒé‡è¢«è½¬ç§»åˆ°CPU/ç¡¬ç›˜ä¸Šæ—¶ï¼Œæ²¡æœ‰é¢„å–ï¼ˆä½†æˆ‘ä»¬å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­è§£å†³è¿™ä¸ªé—®é¢˜ï¼‰ï¼Œè¿™æ„å‘³ç€æƒé‡åœ¨éœ€è¦æ—¶æ‰ä¼šæ”¾åœ¨GPUä¸Šï¼Œè€Œä¸æ˜¯ä¹‹å‰ã€‚
- en: Hard-drive offloading might be very slow if the hardware you run on does not
    have fast communication between disk and CPU (like NVMes).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è¿è¡Œçš„ç¡¬ä»¶ä¹‹é—´çš„ç£ç›˜å’ŒCPUä¹‹é—´æ²¡æœ‰å¿«é€Ÿé€šä¿¡ï¼ˆå¦‚NVMesï¼‰ï¼Œç¡¬ç›˜å¸è½½å¯èƒ½ä¼šéå¸¸æ…¢ã€‚
