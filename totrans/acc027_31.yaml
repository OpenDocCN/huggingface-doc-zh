- en: Handling big models for inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/concept_guides/big_model_inference](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/8.6c7eb742.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Youtube.b682de5d.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'When loading a pre-trained model in PyTorch, the usual workflow looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In plain English, those steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the model with randomly initialized weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the model weights (in a dictionary usually called a state dict) from the
    disk
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load those weights inside the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While this works very well for regularly sized models, this workflow has some
    clear limitations when we deal with a huge model: in step 1, we load a full version
    of the model in RAM, and spend some time randomly initializing the weights (which
    will be discarded in step 3). In step 2, we load another full version of the model
    in RAM, with the pre-trained weights. If youâ€™re loading a model with 6 billion
    parameters, this means you will need 24GB of RAM for each copy of the model, so
    48GB in total (half of it to load the model in FP16).'
  prefs: []
  type: TYPE_NORMAL
- en: This API is quite new and still in its experimental stage. While we strive to
    provide a stable API, itâ€™s possible some small parts of the public API will change
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'How the Process Works: A Quick Overview'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
  prefs: []
  type: TYPE_NORMAL
- en: 'How the Process Works: Working with Code'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instantiating an empty model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first tool ðŸ¤— Accelerate introduces to help with big models is a context
    manager [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)
    that helps you initialize a model without using any RAM so that step 1 can be
    done on models of any size. Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: initializes an empty model with a bit more than 100B parameters. Behind the
    scenes, this relies on the meta device introduced in PyTorch 1.9\. During the
    initialization under the context manager, each time a parameter is created, it
    is instantly moved to that device.
  prefs: []
  type: TYPE_NORMAL
- en: You canâ€™t move a model initialized like this on CPU or another device directly,
    since it doesnâ€™t have any data. Itâ€™s also very likely that a forward pass with
    that empty model will fail, as not all operations are supported on the meta device.
  prefs: []
  type: TYPE_NORMAL
- en: Sharded checkpoints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Itâ€™s possible your model is so big that even a single copy wonâ€™t fit in RAM.
    That doesnâ€™t mean it canâ€™t be loaded: if you have one or several GPUs, this is
    more memory available to store your model. In this case, itâ€™s better if your checkpoint
    is split into several smaller files that we call checkpoint shards.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ¤— Accelerate will handle sharded checkpoints as long as you follow the following
    format: your checkpoint should be in a folder, with several files containing the
    partial state dicts, and there should be an index in the JSON format that contains
    a dictionary mapping parameter names to the file containing their weights. You
    can easily shard your model with [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model).
    For instance, we could have a folder containing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'with index.json being the following file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: and `first_state_dict.bin` containing the weights for `"linear1.weight"` and
    `"linear1.bias"`, `second_state_dict.bin` the ones for `"linear2.weight"` and
    `"linear2.bias"`
  prefs: []
  type: TYPE_NORMAL
- en: Loading weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second tool ðŸ¤— Accelerate introduces is a function [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch),
    that will allow you to load a checkpoint inside your empty model. This supports
    full checkpoints (a single file containing the whole state dict) as well as sharded
    checkpoints. It will also automatically dispatch those weights across the devices
    you have available (GPUs, CPU RAM), so if you are loading a sharded checkpoint,
    the maximum RAM usage will be the size of the biggest shard.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use big model inference with ðŸ¤— Transformers models, check out
    this [documentation](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading).
  prefs: []
  type: TYPE_NORMAL
- en: Here is how we can use this to load the [GPT2-1.5B](https://huggingface.co/marcsun13/gpt2-xl-linear-sharded)
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s download the sharded version of this model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In order to initialize the model, we will use the library minGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, load the checkpoint we just downloaded with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'By passing `device_map="auto"`, we tell ðŸ¤— Accelerate to determine automatically
    where to put each layer of the model depending on the available resources:'
  prefs: []
  type: TYPE_NORMAL
- en: first, we use the maximum space available on the GPU(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if we still need space, we store the remaining weights on the CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if there is not enough RAM, we store the remaining weights on the hard drive
    as memory-mapped tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: no_split_module_classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This parameter will indicate that some of the modules with the name `"Block"`
    should not be split across different devices. You should set here all blocks that
    include a residual connection of some kind.
  prefs: []
  type: TYPE_NORMAL
- en: The device_map
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can see the `device_map` that ðŸ¤— Accelerate picked by accessing the `hf_device_map`
    attribute of your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Itâ€™s fully possible to create your own device map for the layers to use as
    well, specifying the GPU device to use (a number), `"cpu"`, or `"disk"` and pass
    this in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Run the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have done this, our model lies across several devices, and maybe
    the hard drive. But it can still be used as a regular PyTorch model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Behind the scenes, ðŸ¤— Accelerate added hooks to the model, so that:'
  prefs: []
  type: TYPE_NORMAL
- en: at each layer, the inputs are put on the right device (so even if your model
    is spread across several GPUs, it works)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for the weights offloaded on the CPU, they are put on a GPU just before the
    forward pass and cleaned up just after
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for the weights offloaded on the hard drive, they are loaded in RAM then put
    on a GPU just before the forward pass and cleaned up just after
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, your model can run for inference even if it doesnâ€™t fit on one of
    the GPUs or the CPU RAM!
  prefs: []
  type: TYPE_NORMAL
- en: This only supports the inference of your model, not training. Most of the computation
    happens behind `torch.no_grad()` context managers to avoid spending some GPU memory
    with intermediate activations.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a device map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can let ðŸ¤— Accelerate handle the device map computation by setting `device_map`
    to one of the supported options (`"auto"`, `"balanced"`, `"balanced_low_0"`, `"sequential"`)
    or create one yourself if you want more control over where each layer should go.
  prefs: []
  type: TYPE_NORMAL
- en: You can derive all sizes of the model (and thus compute a `device_map`) on a
    model that is on the meta device.
  prefs: []
  type: TYPE_NORMAL
- en: All the options will produce the same result when you donâ€™t have enough GPU
    memory to accommodate the whole model (which is to fit everything that can on
    the GPU, then offload weights on the CPU or even on the disk if there is not enough
    RAM).
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have more GPU memory available than the model size, here is the difference
    between each option:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"auto"` and `"balanced"` evenly split the model on all available GPUs, making
    it possible for you to use a batch size greater than 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"balanced_low_0"` evenly splits the model on all GPUs except the first one,
    and only puts on GPU 0 what does not fit on the others. This option is great when
    you need to use GPU 0 for some processing of the outputs, like when using the
    `generate` function for Transformers models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"sequential"` will fit what it can on GPU 0, then move on GPU 1 and so forth
    (so wonâ€™t use the last GPUs if it doesnâ€™t need to).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The options `"auto"` and `"balanced"` produce the same results for now, but
    the behavior of `"auto"` might change in the future if we find a strategy that
    makes more sense, while `"balanced"` will stay stable.
  prefs: []
  type: TYPE_NORMAL
- en: First note that you can limit the memory used on each GPU by using the `max_memory`
    argument (available in [infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    and in all functions using it). When setting `max_memory`, you should pass along
    a dictionary containing the GPU identifiers (for instance `0`, `1` etc.) and the
    `"cpu"` key for the maximum RAM you want to use for CPU offload. The values can
    either be an integer (in bytes) or a string representing a number with its unit,
    such as `"10GiB"` or `"10GB"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example where we donâ€™t want to use more than 10GiB on each of the
    two GPUs and no more than 30GiB of CPU RAM for the model weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When a first allocation happens in PyTorch, it loads CUDA kernels which take
    about 1-2GB of memory depending on the GPU. Therefore you always have less usable
    memory than the actual size of the GPU. To see how much memory is actually used
    do `torch.ones(1).cuda()` and look at the memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore when you create memory maps with `max_memory` make sure to adjust
    the available memory accordingly to avoid out-of-memory errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, if you do some additional operations with your outputs without
    placing them back on the CPU (for instance inside the `generate` method of Transformers)
    and if you placed your inputs on a GPU, that GPU will consume more memory than
    the others (Accelerate always place the output back to the device of the input).
    Therefore if you would like to optimize the maximum batch size and you have many
    GPUs, give the first GPU less memory. For example, with BLOOM-176B on 8x80 A100
    setup, the close-to-ideal map is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: as you can see we gave the remaining 7 GPUs ~50% more memory than GPU 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you opt to fully design the `device_map` yourself, it should be a dictionary
    with keys being module names of your model and values being a valid device identifier
    (for instance an integer for the GPUs) or `"cpu"` for CPU offload, `"disk"` for
    disk offload. The keys need to cover the whole model, you can then define your
    device map as you wish: for instance, if your model has two blocks (letâ€™s say
    `block1` and `block2`) which each contain three linear layers (letâ€™s say `linear1`,
    `linear2` and `linear3`), a valid device map can be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'another one that is valid could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, this one is not valid as it does not cover every parameter
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To be the most efficient, make sure your device map puts the parameters on the
    GPUs in a sequential manner (e.g. donâ€™t put one of the first weights on GPU 0,
    then weights on GPU 1 and the last weight back to GPU 0) to avoid making many
    transfers of data between the GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: CPU offload only
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to offload your model on CPU, you can use [cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload).
    As a result, all parameters of the model will be offloaded and only one copy of
    the state dict of the model will be kept. During the forward pass, parameters
    will be extracted from that state dict and put on the execution device and passed
    as they are needed, then offloaded again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use [cpu_offload_with_hook()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload_with_hook).
    This function will offloads a model on the CPU and puts it back to an execution
    device when executed. The difference with [cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload)
    is that the model stays on the execution device after the forward and is only
    offloaded again when the `offload` method of the returned `hook` is called. Furthermore,
    [cpu_offload_with_hook()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload_with_hook)
    is more performant but less memory saving. It is useful for pipelines running
    a model in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Disk offload only
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To perform disk offload, you can use [disk_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.disk_offload).
    As a result, all parameters of the model will be offloaded as memory-mapped array
    in a given folder. During the forward pass, parameters will be accessed from that
    folder and put on the execution device passed as they are needed, then offloaded
    again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Limits and further development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are aware of the current limitations in the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    (or `device_map="auto"` in [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch))
    tries to maximize GPU and CPU RAM it sees available when you execute it. While
    PyTorch is very good at managing GPU RAM efficiently (and giving it back when
    not needed), itâ€™s not entirely true with Python and CPU RAM. Therefore, an automatically
    computed device map might be too intense on the CPU. Move a few modules to the
    disk device if you get crashes due to a lack of RAM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    (or `device_map="auto"` in [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch))
    attributes devices sequentially (to avoid moving things back and forth) so if
    your first layer is bigger than the size of the GPU you have, it will end up with
    everything on the CPU/Disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)
    and [load_checkpoint_in_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.load_checkpoint_in_model)
    do not perform any check on the correctness of your state dict compared to your
    model at the moment (this will be fixed in a future version), so you may get some
    weird errors if trying to load a checkpoint with mismatched or missing keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model parallelism used when your model is split on several GPUs is naive
    and not optimized, meaning that only one GPU works at a given time and the other
    sits idle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When weights are offloaded on the CPU/hard drive, there is no pre-fetching (yet,
    we will work on this for future versions) which means the weights are put on the
    GPU when they are needed and not before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard-drive offloading might be very slow if the hardware you run on does not
    have fast communication between disk and CPU (like NVMes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
