- en: Transformer2D
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/models/transformer2d](https://huggingface.co/docs/diffusers/api/models/transformer2d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/21.3cb18066.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: A Transformer model for image-like data from [CompVis](https://huggingface.co/CompVis)
    that is based on the [Vision Transformer](https://huggingface.co/papers/2010.11929)
    introduced by Dosovitskiy et al. The [Transformer2DModel](/docs/diffusers/v0.26.3/en/api/models/transformer2d#diffusers.Transformer2DModel)
    accepts discrete (classes of vector embeddings) or continuous (actual embeddings)
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the input is **continuous**:'
  prefs: []
  type: TYPE_NORMAL
- en: Project the input and reshape it to `(batch_size, sequence_length, feature_dimension)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the Transformer blocks in the standard way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape to image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When the input is **discrete**:'
  prefs: []
  type: TYPE_NORMAL
- en: It is assumed one of the input classes is the masked latent pixel. The predicted
    classes of the unnoised image don’t contain a prediction for the masked pixel
    because the unnoised image cannot be masked.
  prefs: []
  type: TYPE_NORMAL
- en: Convert input (classes of latent pixels) to embeddings and apply positional
    embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the Transformer blocks in the standard way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict classes of unnoised image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer2DModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.Transformer2DModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_2d.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '( num_attention_heads: int = 16 attention_head_dim: int = 88 in_channels: Optional
    = None out_channels: Optional = None num_layers: int = 1 dropout: float = 0.0
    norm_num_groups: int = 32 cross_attention_dim: Optional = None attention_bias:
    bool = False sample_size: Optional = None num_vector_embeds: Optional = None patch_size:
    Optional = None activation_fn: str = ''geglu'' num_embeds_ada_norm: Optional =
    None use_linear_projection: bool = False only_cross_attention: bool = False double_self_attention:
    bool = False upcast_attention: bool = False norm_type: str = ''layer_norm'' norm_elementwise_affine:
    bool = True norm_eps: float = 1e-05 attention_type: str = ''default'' caption_channels:
    int = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 16) — The number of
    heads to use for multi-head attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_head_dim** (`int`, *optional*, defaults to 88) — The number of
    channels in each head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**in_channels** (`int`, *optional*) — The number of channels in the input and
    output (specify if the input is **continuous**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_layers** (`int`, *optional*, defaults to 1) — The number of layers of
    Transformer blocks to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout** (`float`, *optional*, defaults to 0.0) — The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*) — The number of `encoder_hidden_states`
    dimensions to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sample_size** (`int`, *optional*) — The width of the latent images (specify
    if the input is **discrete**). This is fixed during training since it is used
    to learn a number of position embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_vector_embeds** (`int`, *optional*) — The number of classes of the vector
    embeddings of the latent pixels (specify if the input is **discrete**). Includes
    the class for the masked latent pixel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**activation_fn** (`str`, *optional*, defaults to `"geglu"`) — Activation function
    to use in feed-forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_embeds_ada_norm** ( `int`, *optional*) — The number of diffusion steps
    used during training. Pass if at least one of the norm_layers is `AdaLayerNorm`.
    This is fixed during training since it is used to learn a number of embeddings
    that are added to the hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During inference, you can denoise for up to but not more steps than `num_embeds_ada_norm`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_bias** (`bool`, *optional*) — Configure if the `TransformerBlocks`
    attention should contain a bias parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 2D Transformer model for image-like data.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_2d.py#L245)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_states: Tensor encoder_hidden_states: Optional = None timestep: Optional
    = None added_cond_kwargs: Dict = None class_labels: Optional = None cross_attention_kwargs:
    Dict = None attention_mask: Optional = None encoder_attention_mask: Optional =
    None return_dict: bool = True )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_states** (`torch.LongTensor` of shape `(batch size, num latent pixels)`
    if discrete, `torch.FloatTensor` of shape `(batch size, channel, height, width)`
    if continuous) — Input `hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** ( `torch.FloatTensor` of shape `(batch size, sequence
    len, embed dims)`, *optional*) — Conditional embeddings for cross attention layer.
    If not given, cross-attention defaults to self-attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timestep** ( `torch.LongTensor`, *optional*) — Used to indicate denoising
    step. Optional timestep to be applied as an embedding in `AdaLayerNorm`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**class_labels** ( `torch.LongTensor` of shape `(batch size, num classes)`,
    *optional*) — Used to indicate class labels conditioning. Optional class labels
    to be applied as an embedding in `AdaLayerZeroNorm`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_kwargs** ( `Dict[str, Any]`, *optional*) — A kwargs dictionary
    that if specified is passed along to the `AttentionProcessor` as defined under
    `self.processor` in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** ( `torch.Tensor`, *optional*) — An attention mask of shape
    `(batch, key_tokens)` is applied to `encoder_hidden_states`. If `1` the mask is
    kept, otherwise if `0` it is discarded. Mask will be converted into a bias, which
    adds large negative values to the attention scores corresponding to “discard”
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_attention_mask** ( `torch.Tensor`, *optional*) — Cross-attention
    mask applied to `encoder_hidden_states`. Two formats supported:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mask `(batch, sequence_length)` True = keep, False = discard.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias `(batch, 1, sequence_length)` 0 = keep, -10000 = discard.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `ndim == 2`: will be interpreted as a mask, then converted into a bias consistent
    with the format above. This bias will be added to the cross-attention scores.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [Transformer2DModel](/docs/diffusers/v0.26.3/en/api/models/transformer2d#diffusers.Transformer2DModel)
    forward method.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer2DModelOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.transformers.transformer_2d.Transformer2DModelOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_2d.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: '( sample: FloatTensor )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**sample** (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)` or `(batch size, num_vector_embeds - 1, num_latent_pixels)` if [Transformer2DModel](/docs/diffusers/v0.26.3/en/api/models/transformer2d#diffusers.Transformer2DModel)
    is discrete) — The hidden states output conditioned on the `encoder_hidden_states`
    input. If discrete, returns probability distributions for the unnoised latent
    pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of [Transformer2DModel](/docs/diffusers/v0.26.3/en/api/models/transformer2d#diffusers.Transformer2DModel).
  prefs: []
  type: TYPE_NORMAL
