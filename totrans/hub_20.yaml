- en: Inference API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/hub/models-inference](https://huggingface.co/docs/hub/models-inference)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [Inference API Documentation](https://huggingface.co/docs/api-inference)
    for detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: What technology do you use to power the inference API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For ðŸ¤— Transformers models, [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
    power the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of `Pipelines` and depending on the model type, there are several production
    optimizations like:'
  prefs: []
  type: TYPE_NORMAL
- en: compiling models to optimized intermediary representations (e.g. [ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: maintaining a Least Recently Used cache, ensuring that the most popular models
    are always loaded,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scaling the underlying compute infrastructure on the fly depending on the load
    constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For models from [other libraries](./models-libraries), the API uses [Starlette](https://www.starlette.io)
    and runs in [Docker containers](https://github.com/huggingface/api-inference-community/tree/main/docker_images).
    Each library defines the implementation of [different pipelines](https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines).
  prefs: []
  type: TYPE_NORMAL
- en: How can I turn off the inference API for my model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Specify `inference: false` in your model cardâ€™s metadata.'
  prefs: []
  type: TYPE_NORMAL
- en: Why donâ€™t I see an inference widget or why canâ€™t I use the inference API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some tasks, there might not be support in the inference API, and, hence,
    there is no widget. For all libraries (except ðŸ¤— Transformers), there is a [library-to-tasks.ts
    file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-to-tasks.ts)
    of supported tasks in the API. When a model repository has a task that is not
    supported by the repository library, the repository has `inference: false` by
    default.'
  prefs: []
  type: TYPE_NORMAL
- en: Can I send large volumes of requests? Can I get accelerated APIs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are interested in accelerated inference, higher volumes of requests,
    or an SLA, please contact us at `api-enterprise at huggingface.co`.
  prefs: []
  type: TYPE_NORMAL
- en: How can I see my usage?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/).
    Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage).
  prefs: []
  type: TYPE_NORMAL
- en: Is there programmatic access to the Inference API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yes, the `huggingface_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface_hub/how-to-inference).
  prefs: []
  type: TYPE_NORMAL
