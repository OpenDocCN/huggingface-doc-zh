- en: Inference API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨ç†API
- en: 'Original text: [https://huggingface.co/docs/hub/models-inference](https://huggingface.co/docs/hub/models-inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/hub/models-inference](https://huggingface.co/docs/hub/models-inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [Inference API Documentation](https://huggingface.co/docs/api-inference)
    for detailed information.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒ[æ¨ç†APIæ–‡æ¡£](https://huggingface.co/docs/api-inference)è·å–è¯¦ç»†ä¿¡æ¯ã€‚
- en: What technology do you use to power the inference API?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‚¨ä½¿ç”¨ä»€ä¹ˆæŠ€æœ¯æ¥æ”¯æŒæ¨ç†APIï¼Ÿ
- en: For ğŸ¤— Transformers models, [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
    power the API.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºğŸ¤— Transformersæ¨¡å‹ï¼Œ[Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)æ”¯æŒAPIã€‚
- en: 'On top of `Pipelines` and depending on the model type, there are several production
    optimizations like:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†`Pipelines`ä¹‹å¤–ï¼Œæ ¹æ®æ¨¡å‹ç±»å‹ï¼Œè¿˜æœ‰å‡ ç§ç”Ÿäº§ä¼˜åŒ–ï¼Œæ¯”å¦‚ï¼š
- en: compiling models to optimized intermediary representations (e.g. [ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)),
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼–è¯‘æ¨¡å‹ä¸ºä¼˜åŒ–çš„ä¸­é—´è¡¨ç¤ºï¼ˆä¾‹å¦‚[ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)ï¼‰ï¼Œ
- en: maintaining a Least Recently Used cache, ensuring that the most popular models
    are always loaded,
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»´æŠ¤ä¸€ä¸ªæœ€è¿‘æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜ï¼Œç¡®ä¿æœ€å—æ¬¢è¿çš„æ¨¡å‹å§‹ç»ˆè¢«åŠ è½½ï¼Œ
- en: scaling the underlying compute infrastructure on the fly depending on the load
    constraints.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¹æ®è´Ÿè½½çº¦æŸåŠ¨æ€æ‰©å±•åŸºç¡€è®¡ç®—åŸºç¡€è®¾æ–½ã€‚
- en: For models from [other libraries](./models-libraries), the API uses [Starlette](https://www.starlette.io)
    and runs in [Docker containers](https://github.com/huggingface/api-inference-community/tree/main/docker_images).
    Each library defines the implementation of [different pipelines](https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¥è‡ª[å…¶ä»–åº“](./models-libraries)çš„æ¨¡å‹ï¼ŒAPIä½¿ç”¨[Starlette](https://www.starlette.io)å¹¶åœ¨[Dockerå®¹å™¨](https://github.com/huggingface/api-inference-community/tree/main/docker_images)ä¸­è¿è¡Œã€‚æ¯ä¸ªåº“å®šä¹‰äº†[ä¸åŒæµæ°´çº¿çš„å®ç°](https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines)ã€‚
- en: How can I turn off the inference API for my model?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•å…³é—­æˆ‘çš„æ¨¡å‹çš„æ¨ç†APIï¼Ÿ
- en: 'Specify `inference: false` in your model cardâ€™s metadata.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æ‚¨çš„æ¨¡å‹å¡ç‰‡å…ƒæ•°æ®ä¸­æŒ‡å®š`inference: false`ã€‚'
- en: Why donâ€™t I see an inference widget or why canâ€™t I use the inference API?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘çœ‹ä¸åˆ°æ¨ç†å°éƒ¨ä»¶æˆ–è€…ä¸ºä»€ä¹ˆæˆ‘ä¸èƒ½ä½¿ç”¨æ¨ç†APIï¼Ÿ
- en: 'For some tasks, there might not be support in the inference API, and, hence,
    there is no widget. For all libraries (except ğŸ¤— Transformers), there is a [library-to-tasks.ts
    file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-to-tasks.ts)
    of supported tasks in the API. When a model repository has a task that is not
    supported by the repository library, the repository has `inference: false` by
    default.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºä¸€äº›ä»»åŠ¡ï¼Œæ¨ç†APIå¯èƒ½æ²¡æœ‰æ”¯æŒï¼Œå› æ­¤æ²¡æœ‰å°éƒ¨ä»¶ã€‚å¯¹äºæ‰€æœ‰åº“ï¼ˆé™¤äº†ğŸ¤— Transformersï¼‰ï¼ŒAPIä¸­æœ‰ä¸€ä¸ª[library-to-tasks.tsæ–‡ä»¶](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-to-tasks.ts)åˆ—å‡ºäº†æ”¯æŒçš„ä»»åŠ¡ã€‚å½“ä¸€ä¸ªæ¨¡å‹å­˜å‚¨åº“æœ‰ä¸€ä¸ªå­˜å‚¨åº“åº“ä¸æ”¯æŒçš„ä»»åŠ¡æ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹å­˜å‚¨åº“ä¸º`inference:
    false`ã€‚'
- en: Can I send large volumes of requests? Can I get accelerated APIs?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥å‘é€å¤§é‡è¯·æ±‚å—ï¼Ÿæˆ‘å¯ä»¥è·å¾—åŠ é€Ÿçš„APIå—ï¼Ÿ
- en: If you are interested in accelerated inference, higher volumes of requests,
    or an SLA, please contact us at `api-enterprise at huggingface.co`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹åŠ é€Ÿæ¨ç†ã€æ›´é«˜é‡çš„è¯·æ±‚æˆ–SLAæ„Ÿå…´è¶£ï¼Œè¯·è”ç³»æˆ‘ä»¬ï¼Œé‚®ç®±ä¸º`api-enterprise at huggingface.co`ã€‚
- en: How can I see my usage?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘å¦‚ä½•æŸ¥çœ‹æˆ‘çš„ä½¿ç”¨æƒ…å†µï¼Ÿ
- en: You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/).
    Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å‰å¾€[æ¨ç†APIä»ªè¡¨æ¿](https://api-inference.huggingface.co/dashboard/)ã€‚åœ¨[æ¨ç†APIæ–‡æ¡£](https://huggingface.co/docs/api-inference/usage)ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: Is there programmatic access to the Inference API?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†APIæ˜¯å¦å¯ä»¥é€šè¿‡ç¼–ç¨‹è®¿é—®ï¼Ÿ
- en: Yes, the `huggingface_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface_hub/how-to-inference).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œ`huggingface_hub`åº“æœ‰ä¸€ä¸ªå®¢æˆ·ç«¯åŒ…è£…å™¨ï¼Œæ–‡æ¡£åœ¨[è¿™é‡Œ](https://huggingface.co/docs/huggingface_hub/how-to-inference)ã€‚
