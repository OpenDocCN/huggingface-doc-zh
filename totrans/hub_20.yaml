- en: Inference API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理API
- en: 'Original text: [https://huggingface.co/docs/hub/models-inference](https://huggingface.co/docs/hub/models-inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/hub/models-inference](https://huggingface.co/docs/hub/models-inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [Inference API Documentation](https://huggingface.co/docs/api-inference)
    for detailed information.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[推理API文档](https://huggingface.co/docs/api-inference)获取详细信息。
- en: What technology do you use to power the inference API?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您使用什么技术来支持推理API？
- en: For 🤗 Transformers models, [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
    power the API.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于🤗 Transformers模型，[Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)支持API。
- en: 'On top of `Pipelines` and depending on the model type, there are several production
    optimizations like:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`Pipelines`之外，根据模型类型，还有几种生产优化，比如：
- en: compiling models to optimized intermediary representations (e.g. [ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)),
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译模型为优化的中间表示（例如[ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)），
- en: maintaining a Least Recently Used cache, ensuring that the most popular models
    are always loaded,
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护一个最近最少使用的缓存，确保最受欢迎的模型始终被加载，
- en: scaling the underlying compute infrastructure on the fly depending on the load
    constraints.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据负载约束动态扩展基础计算基础设施。
- en: For models from [other libraries](./models-libraries), the API uses [Starlette](https://www.starlette.io)
    and runs in [Docker containers](https://github.com/huggingface/api-inference-community/tree/main/docker_images).
    Each library defines the implementation of [different pipelines](https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自[其他库](./models-libraries)的模型，API使用[Starlette](https://www.starlette.io)并在[Docker容器](https://github.com/huggingface/api-inference-community/tree/main/docker_images)中运行。每个库定义了[不同流水线的实现](https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines)。
- en: How can I turn off the inference API for my model?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何关闭我的模型的推理API？
- en: 'Specify `inference: false` in your model card’s metadata.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '在您的模型卡片元数据中指定`inference: false`。'
- en: Why don’t I see an inference widget or why can’t I use the inference API?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我看不到推理小部件或者为什么我不能使用推理API？
- en: 'For some tasks, there might not be support in the inference API, and, hence,
    there is no widget. For all libraries (except 🤗 Transformers), there is a [library-to-tasks.ts
    file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-to-tasks.ts)
    of supported tasks in the API. When a model repository has a task that is not
    supported by the repository library, the repository has `inference: false` by
    default.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '对于一些任务，推理API可能没有支持，因此没有小部件。对于所有库（除了🤗 Transformers），API中有一个[library-to-tasks.ts文件](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-to-tasks.ts)列出了支持的任务。当一个模型存储库有一个存储库库不支持的任务时，默认情况下存储库为`inference:
    false`。'
- en: Can I send large volumes of requests? Can I get accelerated APIs?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我可以发送大量请求吗？我可以获得加速的API吗？
- en: If you are interested in accelerated inference, higher volumes of requests,
    or an SLA, please contact us at `api-enterprise at huggingface.co`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对加速推理、更高量的请求或SLA感兴趣，请联系我们，邮箱为`api-enterprise at huggingface.co`。
- en: How can I see my usage?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我如何查看我的使用情况？
- en: You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/).
    Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以前往[推理API仪表板](https://api-inference.huggingface.co/dashboard/)。在[推理API文档](https://huggingface.co/docs/api-inference/usage)中了解更多信息。
- en: Is there programmatic access to the Inference API?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理API是否可以通过编程访问？
- en: Yes, the `huggingface_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface_hub/how-to-inference).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，`huggingface_hub`库有一个客户端包装器，文档在[这里](https://huggingface.co/docs/huggingface_hub/how-to-inference)。
