["```py\n>>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot-400M-distill\"\n>>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n>>> reply_ids = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(reply_ids))\n[\"<s> That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s>\"]\n```", "```py\n( vocab_size = 8008 max_position_embeddings = 128 encoder_layers = 2 encoder_ffn_dim = 10240 encoder_attention_heads = 32 decoder_layers = 24 decoder_ffn_dim = 10240 decoder_attention_heads = 32 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 2560 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 1 scale_embedding = False pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 encoder_no_repeat_ngram_size = 3 forced_eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import BlenderbotConfig, BlenderbotModel\n\n>>> # Initializing a Blenderbot facebook/blenderbot-3B style configuration\n>>> configuration = BlenderbotConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/blenderbot-3B style configuration\n>>> model = BlenderbotModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False **kwargs )\n```", "```py\n>>> from transformers import BlenderbotTokenizer\n\n>>> tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n>>> tokenizer.add_prefix_space = False\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[47, 921, 86, 1085, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[6950, 1085, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False trim_offsets = True **kwargs )\n```", "```py\n>>> from transformers import BlenderbotTokenizerFast\n\n>>> tokenizer = BlenderbotTokenizerFast.from_pretrained(\"facebook/blenderbot-3B\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[6950, 1085, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[6950, 1085, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: BlenderbotConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Union = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotModel\n\n>>> model = BlenderbotModel.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 6, 1280]\n```", "```py\n( config: BlenderbotConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Union = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot-400M-distill\"\n>>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> print(\"Human: \", UTTERANCE)\nHuman:  My friends are cool but they eat too many carbs.\n\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n>>> reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\nBot: That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?\n\n>>> REPLY = \"I'm not sure\"\n>>> print(\"Human: \", REPLY)\nHuman: I'm not sure\n\n>>> NEXT_UTTERANCE = (\n...     \"My friends are cool but they eat too many carbs.</s> <s>That's unfortunate. \"\n...     \"Are they trying to lose weight or are they just trying to be healthier?</s> \"\n...     \"<s> I'm not sure.\"\n... )\n>>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n>>> next_reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\nBot:   I see. Well, it's good that they're trying to change their eating habits.\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> model = BlenderbotForCausalLM.from_pretrained(\"facebook/blenderbot-400M-distill\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n( config: BlenderbotConfig *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None past_key_values: List[tf.Tensor] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFBlenderbotModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> model = TFBlenderbotModel.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None past_key_values: List[tf.Tensor] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFBlenderbotForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot-400M-distill\"\n>>> model = TFBlenderbotForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> print(\"Human: \", UTTERANCE)\n\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"tf\")\n>>> reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n\n>>> REPLY = \"I'm not sure\"\n>>> print(\"Human: \", REPLY)\n>>> NEXT_UTTERANCE = (\n...     \"My friends are cool but they eat too many carbs.</s> <s>That's unfortunate. \"\n...     \"Are they trying to lose weight or are they just trying to be healthier?</s> \"\n...     \"<s> I'm not sure.\"\n... )\n>>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"tf\")\n>>> next_reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n```", "```py\n( config: BlenderbotConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> model = FlaxBlenderbotModel.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotForConditionalGeneration\n\n>>> model = FlaxBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBlenderbotForConditionalGeneration\n\n>>> model = FlaxBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: BlenderbotConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotForConditionalGeneration\n\n>>> model = FlaxBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([UTTERANCE], max_length=1024, return_tensors=\"np\")\n\n>>> # Generate Reply\n>>> reply_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5, early_stopping=True).sequences\n>>> print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in reply_ids])\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotForConditionalGeneration\n\n>>> model = FlaxBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBlenderbotForConditionalGeneration\n\n>>> model = FlaxBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]