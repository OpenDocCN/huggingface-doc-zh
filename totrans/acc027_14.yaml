- en: Handling big models for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤„ç†æ¨æ–­çš„å¤§æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest advancements ğŸ¤— Accelerate provides is the concept of [large
    model inference](../concept_guides/big_model_inference) wherein you can perform
    *inference* on models that cannot fully fit on your graphics card.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateæä¾›çš„æœ€å¤§çš„è¿›æ­¥ä¹‹ä¸€æ˜¯[å¤§å‹æ¨¡å‹æ¨æ–­](../concept_guides/big_model_inference)çš„æ¦‚å¿µï¼Œæ‚¨å¯ä»¥åœ¨æ— æ³•å®Œå…¨é€‚åº”æ‚¨çš„æ˜¾å¡çš„æ¨¡å‹ä¸Šæ‰§è¡Œ*æ¨æ–­*ã€‚
- en: This tutorial will be broken down into two parts showcasing how to use both
    ğŸ¤— Accelerate and ğŸ¤— Transformers (a higher API-level) to make use of this idea.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å°†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå±•ç¤ºå¦‚ä½•åŒæ—¶ä½¿ç”¨ğŸ¤— Accelerateå’ŒğŸ¤— Transformersï¼ˆæ›´é«˜çº§åˆ«çš„APIï¼‰æ¥åˆ©ç”¨è¿™ä¸ªæƒ³æ³•ã€‚
- en: Using ğŸ¤— Accelerate
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerate
- en: 'For these tutorials, weâ€™ll assume a typical workflow for loading your model
    in such that:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™äº›æ•™ç¨‹ï¼Œæˆ‘ä»¬å°†å‡è®¾ä¸€ä¸ªå…¸å‹çš„å·¥ä½œæµç¨‹ï¼Œç”¨äºåŠ è½½æ‚¨çš„æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that here we assume that `ModelClass` is a model that takes up more video-card
    memory than what can fit on your device (be it `mps` or `cuda`).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾`ModelClass`æ˜¯ä¸€ä¸ªå ç”¨æ¯”è®¾å¤‡ï¼ˆæ— è®ºæ˜¯`mps`è¿˜æ˜¯`cuda`ï¼‰ä¸Šå¯å®¹çº³çš„è§†é¢‘å†…å­˜æ›´å¤šçš„æ¨¡å‹ã€‚
- en: 'The first step is to init an empty skeleton of the model which wonâ€™t take up
    any RAM using the [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)
    context manager:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹çš„ç©ºéª¨æ¶ï¼Œä½¿ç”¨[init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè¿™æ ·ä¸ä¼šå ç”¨ä»»ä½•RAMï¼š
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this `my_model` currently is â€œparameterlessâ€, hence leaving the smaller
    footprint than what one would normally get loading this onto the CPU directly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰è¿™ä¸ª`my_model`æ˜¯â€œæ— å‚æ•°â€çš„ï¼Œå› æ­¤æ¯”ç›´æ¥åŠ è½½åˆ°CPUä¸Šç•™ä¸‹äº†æ›´å°çš„å°è®°ã€‚
- en: Next we need to load in the weights to our model so we can perform inference.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åŠ è½½æƒé‡åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿è¿›è¡Œæ¨æ–­ã€‚
- en: For this we will use [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch),
    which as the name implies will load a checkpoint inside your empty model and dispatch
    the weights for each layer across all the devices you have available (GPU/MPS
    and CPU RAM).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)ï¼Œå¦‚å…¶åç§°æ‰€ç¤ºï¼Œå°†åœ¨æ‚¨çš„ç©ºæ¨¡å‹å†…åŠ è½½ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¹¶å°†æ¯ä¸ªå±‚çš„æƒé‡åˆ†é…åˆ°æ‚¨å¯ç”¨çš„æ‰€æœ‰è®¾å¤‡ï¼ˆGPU/MPSå’ŒCPU
    RAMï¼‰ä¸Šã€‚
- en: To determine how this `dispatch` can be performed, generally specifying `device_map="auto"`
    will be good enough as ğŸ¤— Accelerate will attempt to fill all the space in your
    GPU(s), then loading them to the CPU, and finally if there is not enough RAM it
    will be loaded to the disk (the absolute slowest option).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¡®å®šå¦‚ä½•æ‰§è¡Œè¿™ä¸ª`dispatch`ï¼Œé€šå¸¸æŒ‡å®š`device_map="auto"`å°±è¶³å¤Ÿäº†ï¼Œå› ä¸ºğŸ¤— Accelerateä¼šå°è¯•å¡«æ»¡GPU(s)ä¸Šçš„æ‰€æœ‰ç©ºé—´ï¼Œç„¶ååŠ è½½åˆ°CPUï¼Œæœ€åå¦‚æœå†…å­˜ä¸å¤Ÿï¼Œå®ƒå°†è¢«åŠ è½½åˆ°ç£ç›˜ï¼ˆç»å¯¹æœ€æ…¢çš„é€‰é¡¹ï¼‰ã€‚
- en: For more details on designing your own device map, see this section of the [concept
    guide](../concept_guide/big_model_inference#designing-a-device-map)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è®¾è®¡è‡ªå·±çš„è®¾å¤‡æ˜ å°„çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ¦‚å¿µæŒ‡å—](../concept_guide/big_model_inference#designing-a-device-map)çš„æœ¬èŠ‚
- en: 'See an example below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If there are certain â€œchunksâ€ of layers that shouldnâ€™t be split, you can pass
    them in as `no_split_module_classes`. Read more about it [here](../concept_guides/big_model_inference#loading-weights)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰æŸäº›â€œå—â€å±‚ä¸åº”è¯¥è¢«åˆ†å‰²ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä½œä¸º`no_split_module_classes`ä¼ é€’ã€‚äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](../concept_guides/big_model_inference#loading-weights)
- en: Also to save on memory (such as if the `state_dict` will not fit in RAM), a
    modelâ€™s weights can be divided and split into multiple checkpoint files. Read
    more about it [here](../concept_guides/big_model_inference#sharded-checkpoints)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼ˆä¾‹å¦‚å¦‚æœ`state_dict`æ— æ³•é€‚åº”RAMï¼‰ï¼Œæ¨¡å‹çš„æƒé‡å¯ä»¥åˆ†å‰²å¹¶æ‹†åˆ†ä¸ºå¤šä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶ã€‚äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](../concept_guides/big_model_inference#sharded-checkpoints)
- en: 'Now that the model is dispatched fully, you can perform inference as normal
    with the model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ¨¡å‹å·²ç»å®Œå…¨åˆ†å‘ï¼Œæ‚¨å¯ä»¥åƒæ­£å¸¸æƒ…å†µä¸‹ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼š
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What will happen now is each time the input gets passed through a layer, it
    will be sent from the CPU to the GPU (or disk to CPU to GPU), the output is calculated,
    and then the layer is pulled back off the GPU going back down the line. While
    this adds some overhead to the inference being performed, through this method
    it is possible to run **any size model** on your system, as long as the largest
    layer is capable of fitting on your GPU.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ¯æ¬¡è¾“å…¥é€šè¿‡ä¸€ä¸ªå±‚æ—¶ï¼Œå®ƒå°†ä»CPUå‘é€åˆ°GPUï¼ˆæˆ–ä»ç£ç›˜åˆ°CPUåˆ°GPUï¼‰ï¼Œè®¡ç®—è¾“å‡ºï¼Œç„¶åå°†è¯¥å±‚ä»GPUæ‹‰å›åˆ°çº¿è·¯ä¸‹æ–¹ã€‚è™½ç„¶è¿™ä¼šå¢åŠ æ‰§è¡Œæ¨æ–­çš„å¼€é”€ï¼Œä½†é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ‚¨çš„ç³»ç»Ÿä¸Šè¿è¡Œ**ä»»ä½•å¤§å°çš„æ¨¡å‹**ï¼Œåªè¦æœ€å¤§çš„å±‚èƒ½å¤Ÿé€‚åˆåœ¨æ‚¨çš„GPUä¸Šã€‚
- en: Multiple GPUs can be utilized, however this is considered â€œmodel parallelismâ€
    and as a result only one GPU will be active at a given moment, waiting for the
    prior one to send it the output. You should launch your script normally with `python`
    and not need `torchrun`, `accelerate launch`, etc.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šä¸ªGPUå¯ä»¥è¢«åˆ©ç”¨ï¼Œä½†è¿™è¢«è®¤ä¸ºæ˜¯â€œæ¨¡å‹å¹¶è¡Œâ€ï¼Œå› æ­¤åœ¨ä»»ä½•ç»™å®šæ—¶åˆ»åªæœ‰ä¸€ä¸ªGPUä¼šå¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œç­‰å¾…å‰ä¸€ä¸ªGPUå‘é€è¾“å‡ºã€‚æ‚¨åº”è¯¥æ­£å¸¸å¯åŠ¨æ‚¨çš„è„šæœ¬ä½¿ç”¨`python`ï¼Œè€Œä¸éœ€è¦`torchrun`ï¼Œ`accelerate
    launch`ç­‰ã€‚
- en: 'For a visual representation of this, check out the animation below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹æ­¤å†…å®¹çš„å¯è§†åŒ–è¡¨ç¤ºï¼Œè¯·æŸ¥çœ‹ä¸‹é¢çš„åŠ¨ç”»ï¼š
- en: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
- en: Complete Example
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®Œæ•´ç¤ºä¾‹
- en: 'Below is the full example showcasing what we performed above:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å®Œæ•´ç¤ºä¾‹ï¼Œå±•ç¤ºæˆ‘ä»¬ä¸Šé¢æ‰§è¡Œçš„æ“ä½œï¼š
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using ğŸ¤— Transformers, ğŸ¤— Diffusers, and other ğŸ¤— Open Source Libraries
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Transformersï¼ŒğŸ¤— Diffuserså’Œå…¶ä»–ğŸ¤—å¼€æºåº“
- en: Libraries that support ğŸ¤— Accelerate big model inference include all of the earlier
    logic in their `from_pretrained` constructors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒğŸ¤— Accelerateå¤§å‹æ¨¡å‹æ¨æ–­çš„åº“åŒ…æ‹¬å®ƒä»¬çš„`from_pretrained`æ„é€ å‡½æ•°ä¸­çš„æ‰€æœ‰å…ˆå‰é€»è¾‘ã€‚
- en: These operate by specifying a string representing the model to download from
    the [ğŸ¤— Hub](https://hf.co/models) and then denoting `device_map="auto"` along
    with a few extra parameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æŒ‡å®šä¸€ä¸ªè¡¨ç¤ºè¦ä»[ğŸ¤— Hub](https://hf.co/models)ä¸‹è½½çš„æ¨¡å‹çš„å­—ç¬¦ä¸²ï¼Œç„¶åä½¿ç”¨`device_map="auto"`ä»¥åŠä¸€äº›é¢å¤–çš„å‚æ•°æ¥æ“ä½œã€‚
- en: As a brief example, we will look at using `transformers` and loading in Big
    Scienceâ€™s T0pp model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªç®€çŸ­çš„ä¾‹å­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨`transformers`åŠ è½½Big Scienceçš„T0ppæ¨¡å‹ã€‚
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After loading the model in, the initial steps from before to prepare a model
    have all been done and the model is fully ready to make use of all the resources
    in your machine. Through these constructors, you can also save *more* memory by
    specifying the precision the model is loaded into as well, through the `torch_dtype`
    parameter, such as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹åï¼Œä¹‹å‰å‡†å¤‡æ¨¡å‹çš„åˆå§‹æ­¥éª¤éƒ½å·²å®Œæˆï¼Œæ¨¡å‹å·²å®Œå…¨å‡†å¤‡å¥½åˆ©ç”¨æœºå™¨ä¸­çš„æ‰€æœ‰èµ„æºã€‚é€šè¿‡è¿™äº›æ„é€ å‡½æ•°ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹åŠ è½½çš„ç²¾åº¦æ¥èŠ‚çœ*æ›´å¤š*å†…å­˜ï¼Œé€šè¿‡`torch_dtype`å‚æ•°ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To learn more about this, check out the ğŸ¤— Transformers documentation available
    [here](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œæä¾›çš„ğŸ¤— Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading)ã€‚
- en: Where to go from here
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥è¯¥å»å“ªé‡Œ
- en: For a much more detailed look at big model inference, be sure to check out the
    [Conceptual Guide on it](../concept_guides/big_model_inference)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´è¯¦ç»†çš„å¤§æ¨¡å‹æ¨æ–­ï¼Œè¯·åŠ¡å¿…æŸ¥çœ‹[å…³äºå®ƒçš„æ¦‚å¿µæŒ‡å—](../concept_guides/big_model_inference)
