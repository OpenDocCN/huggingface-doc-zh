- en: Handling big models for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理推断的大模型
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/accelerate/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest advancements 🤗 Accelerate provides is the concept of [large
    model inference](../concept_guides/big_model_inference) wherein you can perform
    *inference* on models that cannot fully fit on your graphics card.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate提供的最大的进步之一是[大型模型推断](../concept_guides/big_model_inference)的概念，您可以在无法完全适应您的显卡的模型上执行*推断*。
- en: This tutorial will be broken down into two parts showcasing how to use both
    🤗 Accelerate and 🤗 Transformers (a higher API-level) to make use of this idea.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将分为两部分，展示如何同时使用🤗 Accelerate和🤗 Transformers（更高级别的API）来利用这个想法。
- en: Using 🤗 Accelerate
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用🤗 Accelerate
- en: 'For these tutorials, we’ll assume a typical workflow for loading your model
    in such that:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些教程，我们将假设一个典型的工作流程，用于加载您的模型，如下所示：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that here we assume that `ModelClass` is a model that takes up more video-card
    memory than what can fit on your device (be it `mps` or `cuda`).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们假设`ModelClass`是一个占用比设备（无论是`mps`还是`cuda`）上可容纳的视频内存更多的模型。
- en: 'The first step is to init an empty skeleton of the model which won’t take up
    any RAM using the [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)
    context manager:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是初始化一个模型的空骨架，使用[init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)上下文管理器，这样不会占用任何RAM：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this `my_model` currently is “parameterless”, hence leaving the smaller
    footprint than what one would normally get loading this onto the CPU directly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前这个`my_model`是“无参数”的，因此比直接加载到CPU上留下了更小的印记。
- en: Next we need to load in the weights to our model so we can perform inference.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要加载权重到我们的模型中，以便进行推断。
- en: For this we will use [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch),
    which as the name implies will load a checkpoint inside your empty model and dispatch
    the weights for each layer across all the devices you have available (GPU/MPS
    and CPU RAM).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)，如其名称所示，将在您的空模型内加载一个检查点，并将每个层的权重分配到您可用的所有设备（GPU/MPS和CPU
    RAM）上。
- en: To determine how this `dispatch` can be performed, generally specifying `device_map="auto"`
    will be good enough as 🤗 Accelerate will attempt to fill all the space in your
    GPU(s), then loading them to the CPU, and finally if there is not enough RAM it
    will be loaded to the disk (the absolute slowest option).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定如何执行这个`dispatch`，通常指定`device_map="auto"`就足够了，因为🤗 Accelerate会尝试填满GPU(s)上的所有空间，然后加载到CPU，最后如果内存不够，它将被加载到磁盘（绝对最慢的选项）。
- en: For more details on designing your own device map, see this section of the [concept
    guide](../concept_guide/big_model_inference#designing-a-device-map)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有关设计自己的设备映射的更多详细信息，请参阅[概念指南](../concept_guide/big_model_inference#designing-a-device-map)的本节
- en: 'See an example below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的示例：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If there are certain “chunks” of layers that shouldn’t be split, you can pass
    them in as `no_split_module_classes`. Read more about it [here](../concept_guides/big_model_inference#loading-weights)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有某些“块”层不应该被分割，您可以将它们作为`no_split_module_classes`传递。了解更多信息，请点击[这里](../concept_guides/big_model_inference#loading-weights)
- en: Also to save on memory (such as if the `state_dict` will not fit in RAM), a
    model’s weights can be divided and split into multiple checkpoint files. Read
    more about it [here](../concept_guides/big_model_inference#sharded-checkpoints)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了节省内存（例如如果`state_dict`无法适应RAM），模型的权重可以分割并拆分为多个检查点文件。了解更多信息，请点击[这里](../concept_guides/big_model_inference#sharded-checkpoints)
- en: 'Now that the model is dispatched fully, you can perform inference as normal
    with the model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经完全分发，您可以像正常情况下使用模型进行推断：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What will happen now is each time the input gets passed through a layer, it
    will be sent from the CPU to the GPU (or disk to CPU to GPU), the output is calculated,
    and then the layer is pulled back off the GPU going back down the line. While
    this adds some overhead to the inference being performed, through this method
    it is possible to run **any size model** on your system, as long as the largest
    layer is capable of fitting on your GPU.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每次输入通过一个层时，它将从CPU发送到GPU（或从磁盘到CPU到GPU），计算输出，然后将该层从GPU拉回到线路下方。虽然这会增加执行推断的开销，但通过这种方法，可以在您的系统上运行**任何大小的模型**，只要最大的层能够适合在您的GPU上。
- en: Multiple GPUs can be utilized, however this is considered “model parallelism”
    and as a result only one GPU will be active at a given moment, waiting for the
    prior one to send it the output. You should launch your script normally with `python`
    and not need `torchrun`, `accelerate launch`, etc.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 多个GPU可以被利用，但这被认为是“模型并行”，因此在任何给定时刻只有一个GPU会处于活动状态，等待前一个GPU发送输出。您应该正常启动您的脚本使用`python`，而不需要`torchrun`，`accelerate
    launch`等。
- en: 'For a visual representation of this, check out the animation below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此内容的可视化表示，请查看下面的动画：
- en: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/MWCSGj9jEAo](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)'
- en: Complete Example
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整示例
- en: 'Below is the full example showcasing what we performed above:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整示例，展示我们上面执行的操作：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using 🤗 Transformers, 🤗 Diffusers, and other 🤗 Open Source Libraries
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用🤗 Transformers，🤗 Diffusers和其他🤗开源库
- en: Libraries that support 🤗 Accelerate big model inference include all of the earlier
    logic in their `from_pretrained` constructors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 支持🤗 Accelerate大型模型推断的库包括它们的`from_pretrained`构造函数中的所有先前逻辑。
- en: These operate by specifying a string representing the model to download from
    the [🤗 Hub](https://hf.co/models) and then denoting `device_map="auto"` along
    with a few extra parameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定一个表示要从[🤗 Hub](https://hf.co/models)下载的模型的字符串，然后使用`device_map="auto"`以及一些额外的参数来操作。
- en: As a brief example, we will look at using `transformers` and loading in Big
    Science’s T0pp model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简短的例子，我们将看看如何使用`transformers`加载Big Science的T0pp模型。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After loading the model in, the initial steps from before to prepare a model
    have all been done and the model is fully ready to make use of all the resources
    in your machine. Through these constructors, you can also save *more* memory by
    specifying the precision the model is loaded into as well, through the `torch_dtype`
    parameter, such as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型后，之前准备模型的初始步骤都已完成，模型已完全准备好利用机器中的所有资源。通过这些构造函数，您还可以通过指定模型加载的精度来节省*更多*内存，通过`torch_dtype`参数，例如：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To learn more about this, check out the 🤗 Transformers documentation available
    [here](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请查看[这里提供的🤗 Transformers文档](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading)。
- en: Where to go from here
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来该去哪里
- en: For a much more detailed look at big model inference, be sure to check out the
    [Conceptual Guide on it](../concept_guides/big_model_inference)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更详细的大模型推断，请务必查看[关于它的概念指南](../concept_guides/big_model_inference)
