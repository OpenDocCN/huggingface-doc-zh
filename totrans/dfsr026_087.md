# 如何在Core ML中运行稳定扩散

> 原始文本：[https://huggingface.co/docs/diffusers/optimization/coreml](https://huggingface.co/docs/diffusers/optimization/coreml)

Core ML是由Apple框架支持的模型格式和机器学习库。如果您有兴趣在macOS或iOS/iPadOS应用程序中运行Stable Diffusion模型，本指南将向您展示如何将现有的PyTorch检查点转换为Core ML格式，并使用Python或Swift进行推断。

Core ML模型可以利用Apple设备中的所有计算引擎：CPU、GPU和Apple神经引擎（或ANE，一种在Apple Silicon Mac和现代iPhone/iPad中可用的张量优化加速器）。根据模型和运行设备，Core ML也可以混合匹配计算引擎，因此模型的某些部分可能在CPU上运行，而其他部分可能在GPU上运行，例如。

您还可以在Apple Silicon Mac上使用内置于PyTorch中的mps加速器运行`diffusers` Python代码库。这种方法在mps指南中有详细说明，但不兼容本机应用程序。

## 稳定扩散Core ML检查点

稳定扩散权重（或检查点）存储在PyTorch格式中，因此您需要将它们转换为Core ML格式，然后我们才能在本机应用程序中使用它们。

幸运的是，Apple工程师基于`diffusers`开发了[一个转换工具](https://github.com/apple/ml-stable-diffusion#-converting-models-to-core-ml)，用于将PyTorch检查点转换为Core ML。

在转换模型之前，请花点时间探索Hugging Face Hub - 你感兴趣的模型很可能已经以Core ML格式可用：

+   Apple组织包括稳定扩散版本1.4、1.5、2.0基础和2.1基础

+   `coreml community`包括自定义微调模型

+   使用此过滤器返回所有可用的Core ML检查点

如果找不到您感兴趣的模型，我们建议您按照Apple提供的[将模型转换为Core ML](https://github.com/apple/ml-stable-diffusion#-converting-models-to-core-ml)说明进行操作。

## 选择要使用的Core ML变体

稳定扩散模型可以转换为用于不同目的的不同Core ML变体：

+   使用的注意力块类型。注意力操作用于“关注”图像表示中不同区域之间的关系，并理解图像和文本表示之间的关系。注意力是计算和内存密集型的，因此存在不同的实现，考虑了不同设备的硬件特性。对于Core ML Stable Diffusion模型，有两种注意力变体：

    +   `split_einsum`（由Apple引入）经过优化，适用于现代iPhone、iPad和M系列计算机中可用的ANE设备。

    +   “原始”注意力（在`diffusers`中使用的基本实现）仅与CPU/GPU兼容，而不与ANE兼容。使用`original`注意力在CPU + GPU上运行模型可能比ANE更快。查看此性能基准以及社区提供的一些其他措施，以获取更多详细信息。

+   支持的推断框架。

    +   `packages`适用于Python推断。这可以用于在尝试将其集成到本机应用程序之前测试转换的Core ML模型，或者如果您想探索Core ML性能但不需要支持本机应用程序。例如，具有Web UI的应用程序可以完美地使用Python Core ML后端。

    +   Swift代码需要`compiled`模型。Hub中的`compiled`模型将大型UNet模型权重拆分为几个文件，以便与iOS和iPadOS设备兼容。这对应于[`--chunk-unet`转换选项](https://github.com/apple/ml-stable-diffusion#-converting-models-to-core-ml)。如果要支持原生应用程序，则需要选择`compiled`变体。

官方的Core ML Stable Diffusion [模型](https://huggingface.co/apple/coreml-stable-diffusion-v1-4/tree/main)包括这些变体，但社区的可能会有所不同：

```py
coreml-stable-diffusion-v1-4 ├── README.md
├── original │   ├── compiled
│   └── packages
└── split_einsum
    ├── compiled
    └── packages
```

您可以按照下面的方式下载并使用您需要的变体。

## Python中的Core ML推断

安装以下库以在Python中运行Core ML推断：

```py
pip install huggingface_hub
pip install git+https://github.com/apple/ml-stable-diffusion
```

### 下载模型检查点

要在Python中运行推断，请使用存储在`packages`文件夹中的版本，因为`compiled`版本仅与Swift兼容。您可以选择是否要使用`original`或`split_einsum`注意力。

这是您如何从Hub下载`original`注意力变体到名为`models`的目录中：

```py
from huggingface_hub import snapshot_download
from pathlib import Path

repo_id = "apple/coreml-stable-diffusion-v1-4"
variant = "original/packages"

model_path = Path("./models") / (repo_id.split("/")[-1] + "_" + variant.replace("/", "_"))
snapshot_download(repo_id, allow_patterns=f"{variant}/*", local_dir=model_path, local_dir_use_symlinks=False)
print(f"Model downloaded at {model_path}")
```

### 推断

一旦您下载了模型的快照，您可以使用Apple的Python脚本进行测试。

```py
python -m python_coreml_stable_diffusion.pipeline --prompt "a photo of an astronaut riding a horse on mars" -i models/coreml-stable-diffusion-v1-4_original_packages -o </path/to/output/image> --compute-unit CPU_AND_GPU --seed 93
```

使用`-i`标志将下载的检查点路径传递给脚本。`--compute-unit`指示您要允许用于推断的硬件。它必须是以下选项之一：`ALL`、`CPU_AND_GPU`、`CPU_ONLY`、`CPU_AND_NE`。您还可以提供可选的输出路径和一个用于可重现性的种子。

推断脚本假定您正在使用Stable Diffusion模型的原始版本，`CompVis/stable-diffusion-v1-4`。如果您使用另一个模型，则*必须*在推断命令行中指定其Hub id，使用`--model-version`选项。这适用于已支持的模型和您自己训练或微调的自定义模型。

例如，如果您想使用[`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)：

```py
python -m python_coreml_stable_diffusion.pipeline --prompt "a photo of an astronaut riding a horse on mars" --compute-unit ALL -o output --seed 93 -i models/coreml-stable-diffusion-v1-5_original_packages --model-version runwayml/stable-diffusion-v1-5
```

## Swift中的Core ML推断

在Swift中运行推断比在Python中稍快，因为模型已经以`mlmodelc`格式编译。这在应用程序启动时加载模型时会有所注意，但如果之后运行几代，就不应该有明显的感觉。

### 下载

要在Mac上的Swift中运行推断，您需要其中一个`compiled`检查点版本。我们建议您使用类似于上一个示例的Python代码在本地下载它们，但使用其中一个`compiled`变体：

```py
from huggingface_hub import snapshot_download
from pathlib import Path

repo_id = "apple/coreml-stable-diffusion-v1-4"
variant = "original/compiled"

model_path = Path("./models") / (repo_id.split("/")[-1] + "_" + variant.replace("/", "_"))
snapshot_download(repo_id, allow_patterns=f"{variant}/*", local_dir=model_path, local_dir_use_symlinks=False)
print(f"Model downloaded at {model_path}")
```

### 推断

要运行推断，请克隆Apple的存储库：

```py
git clone https://github.com/apple/ml-stable-diffusion
cd ml-stable-diffusion
```

然后使用Apple的命令行工具，[Swift Package Manager](https://www.swift.org/package-manager/#)：

```py
swift run StableDiffusionSample --resource-path models/coreml-stable-diffusion-v1-4_original_compiled --compute-units all "a photo of an astronaut riding a horse on mars"
```

在`--resource-path`中指定在上一步中下载的检查点之一，因此请确保它包含扩展名为`.mlmodelc`的已编译的Core ML捆绑包。`--compute-units`必须是以下值之一：`all`、`cpuOnly`、`cpuAndGPU`、`cpuAndNeuralEngine`。

有关更多详细信息，请参考[Apple存储库中的说明](https://github.com/apple/ml-stable-diffusion)。

## 支持的Diffusers功能

Core ML模型和推断代码不支持🧨 Diffusers的许多功能、选项和灵活性。这些是需要牢记的一些限制：

+   Core ML模型仅适用于推断。它们不能用于训练或微调。

+   只有两个调度程序已经移植到Swift，一个是Stable Diffusion使用的默认调度程序，另一个是我们从我们的`diffusers`实现中移植到Swift的`DPMSolverMultistepScheduler`。我们建议您使用`DPMSolverMultistepScheduler`，因为它在大约一半的步骤中产生相同的质量。

+   负面提示、无分类器指导比例和图像到图像任务在推断代码中可用。高级功能，如深度指导、ControlNet和潜在的放大器目前还不可用。

苹果的[转换和推理存储库](https://github.com/apple/ml-stable-diffusion)和我们自己的[swift-coreml-diffusers](https://github.com/huggingface/swift-coreml-diffusers)存储库旨在作为技术演示，以使其他开发人员能够构建在其基础上。

如果您对任何缺失的功能感到强烈，请随时提出功能请求或更好地提交贡献 PR🙂。

## 本地扩散器 Swift 应用程序

在您自己的苹果硬件上运行 Stable Diffusion 的一种简单方法是使用[我们的开源 Swift 存储库](https://github.com/huggingface/swift-coreml-diffusers)，基于 `diffusers` 和苹果的转换和推理存储库。您可以研究代码，使用[Xcode](https://developer.apple.com/xcode/)编译它，并根据自己的需求进行调整。为了方便起见，还有一个[独立的 Mac 应用程序在 App Store 中](https://apps.apple.com/app/diffusers/id1666309574)，这样您就可以在不必处理代码或 IDE 的情况下进行操作。如果您是开发人员，并且已确定 Core ML 是构建 Stable Diffusion 应用程序的最佳解决方案，那么您可以使用本指南的其余部分开始您的项目。我们迫不及待地想看到您将会构建的内容🙂。
