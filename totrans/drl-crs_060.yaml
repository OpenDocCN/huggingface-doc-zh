- en: The advantages and disadvantages of policy-gradient methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might ask, “but Deep Q-Learning is excellent! Why use policy-gradient
    methods?“. To answer this question, let’s study the **advantages and disadvantages
    of policy-gradient methods**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple advantages over value-based methods. Let’s see some of them:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity of integration
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can estimate the policy directly without storing additional data (action
    values).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradient methods can learn a stochastic policy
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Policy-gradient methods can **learn a stochastic policy while value functions
    can’t**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'This has two consequences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: We **don’t need to implement an exploration/exploitation trade-off by hand**.
    Since we output a probability distribution over actions, the agent explores **the
    state space without always taking the same trajectory.**
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing
    is when two states seem (or are) the same but need different actions.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s take an example: we have an intelligent vacuum cleaner whose goal is
    to suck the dust and avoid killing the hamsters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/1b61fde218600e239ec27f3af716584c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Our vacuum cleaner can only perceive where the walls are.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the **two red (colored) states are aliased states because
    the agent perceives an upper and lower wall for each**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/63123f815fb96086071da70edf73b3fd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Under a deterministic policy, the policy will either always move right when
    in a red state or always move left. **Either case will cause our agent to get
    stuck and never suck the dust**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic
    policy** (“greedy epsilon strategy”). Consequently, our agent can **spend a lot
    of time before finding the dust**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an optimal stochastic policy **will randomly move left or
    right in red (colored) states**. Consequently, **it will not be stuck and will
    reach the goal state with a high probability**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Policy-gradient methods are more effective in high-dimensional action spaces
    and continuous actions spaces
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with Deep Q-learning is that their **predictions assign a score
    (maximum expected future reward) for each possible action**, at each time step,
    given the current state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: But what if we have an infinite possibility of actions?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: For instance, with a self-driving car, at each state, you can have a (near)
    infinite choice of actions (turning the wheel at 15°, 17.2°, 19,4°, honking, etc.).
    **We’ll need to output a Q-value for each possible action**! And **taking the
    max action of a continuous output is an optimization problem itself**!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Instead, with policy-gradient methods, we output a **probability distribution
    over actions.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradient methods have better convergence properties
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In value-based methods, we use an aggressive operator to **change the value
    function: we take the maximum over Q-estimates**. Consequently, the action probabilities
    may change dramatically for an arbitrarily small change in the estimated action
    values if that change results in a different action having the maximal value.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if during the training, the best action was left (with a Q-value
    of 0.22) and the training step after it’s right (since the right Q-value becomes
    0.23), we dramatically changed the policy since now the policy will take most
    of the time right instead of left.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in policy-gradient methods, stochastic policy action preferences
    (probability of taking action) **change smoothly over time**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naturally, policy-gradient methods also have some disadvantages:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequently, policy-gradient methods converges to a local maximum instead
    of a global optimum.**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policy-gradient goes slower, **step by step: it can take longer to train (inefficient).**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度走得更慢，**一步一步：训练可能需要更长时间（低效）。**
- en: Policy-gradient can have high variance. We’ll see in the actor-critic unit why,
    and how we can solve this problem.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度可能具有很高的方差。我们将在演员-评论单元中看到为什么，以及我们如何解决这个问题。
- en: 👉 If you want to go deeper into the advantages and disadvantages of policy-gradient
    methods, [you can check this video](https://youtu.be/y3oqOjHilio).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 如果您想深入了解策略梯度方法的优缺点，[您可以查看这个视频](https://youtu.be/y3oqOjHilio)。
