- en: The advantages and disadvantages of policy-gradient methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might ask, â€œbut Deep Q-Learning is excellent! Why use policy-gradient
    methods?â€œ. To answer this question, letâ€™s study the **advantages and disadvantages
    of policy-gradient methods**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple advantages over value-based methods. Letâ€™s see some of them:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity of integration
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can estimate the policy directly without storing additional data (action
    values).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradient methods can learn a stochastic policy
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Policy-gradient methods canÂ **learn a stochastic policy while value functions
    canâ€™t**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'This has two consequences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: We **donâ€™t need to implement an exploration/exploitation trade-off by hand**.
    Since we output a probability distribution over actions, the agent exploresÂ **the
    state space without always taking the same trajectory.**
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing
    is when two states seem (or are) the same but need different actions.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Letâ€™s take an example: we have an intelligent vacuum cleaner whose goal is
    to suck the dust and avoid killing the hamsters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/1b61fde218600e239ec27f3af716584c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Our vacuum cleaner can only perceive where the walls are.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the **two red (colored) states are aliased states because
    the agent perceives an upper and lower wall for each**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/63123f815fb96086071da70edf73b3fd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Under a deterministic policy, the policy will either always move right when
    in a red state or always move left. **Either case will cause our agent to get
    stuck and never suck the dust**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic
    policy** (â€œgreedy epsilon strategyâ€). Consequently, our agent can **spend a lot
    of time before finding the dust**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an optimal stochastic policy **will randomly move left or
    right in red (colored) states**. Consequently, **it will not be stuck and will
    reach the goal state with a high probability**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Policy-gradient methods are more effective in high-dimensional action spaces
    and continuous actions spaces
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with Deep Q-learning is that their **predictions assign a score
    (maximum expected future reward) for each possible action**, at each time step,
    given the current state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: But what if we have an infinite possibility of actions?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: For instance, with a self-driving car, at each state, you can have a (near)
    infinite choice of actions (turning the wheel at 15Â°, 17.2Â°, 19,4Â°, honking, etc.).
    **Weâ€™ll need to output a Q-value for each possible action**! And **taking the
    max action of a continuous output is an optimization problem itself**!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Instead, with policy-gradient methods, we output aÂ **probability distribution
    over actions.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradient methods have better convergence properties
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In value-based methods, we use an aggressive operator to **change the value
    function: we take the maximum over Q-estimates**. Consequently, the action probabilities
    may change dramatically for an arbitrarily small change in the estimated action
    values if that change results in a different action having the maximal value.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if during the training, the best action was left (with a Q-value
    of 0.22) and the training step after itâ€™s right (since the right Q-value becomes
    0.23), we dramatically changed the policy since now the policy will take most
    of the time right instead of left.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in policy-gradient methods, stochastic policy action preferences
    (probability of taking action) **change smoothly over time**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naturally, policy-gradient methods also have some disadvantages:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequently, policy-gradient methods converges to a local maximum instead
    of a global optimum.**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policy-gradient goes slower,Â **step by step: it can take longer to train (inefficient).**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦èµ°å¾—æ›´æ…¢ï¼Œ**ä¸€æ­¥ä¸€æ­¥ï¼šè®­ç»ƒå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼ˆä½æ•ˆï¼‰ã€‚**
- en: Policy-gradient can have high variance. Weâ€™ll see in the actor-critic unit why,
    and how we can solve this problem.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦å¯èƒ½å…·æœ‰å¾ˆé«˜çš„æ–¹å·®ã€‚æˆ‘ä»¬å°†åœ¨æ¼”å‘˜-è¯„è®ºå•å…ƒä¸­çœ‹åˆ°ä¸ºä»€ä¹ˆï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: ğŸ‘‰ If you want to go deeper into the advantages and disadvantages of policy-gradient
    methods, [you can check this video](https://youtu.be/y3oqOjHilio).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œ[æ‚¨å¯ä»¥æŸ¥çœ‹è¿™ä¸ªè§†é¢‘](https://youtu.be/y3oqOjHilio)ã€‚
