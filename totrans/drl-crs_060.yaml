- en: The advantages and disadvantages of policy-gradient methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/46.4a79fe2e.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might ask, ‚Äúbut Deep Q-Learning is excellent! Why use policy-gradient
    methods?‚Äú. To answer this question, let‚Äôs study the **advantages and disadvantages
    of policy-gradient methods**.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple advantages over value-based methods. Let‚Äôs see some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity of integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can estimate the policy directly without storing additional data (action
    values).
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradient methods can learn a stochastic policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Policy-gradient methods can¬†**learn a stochastic policy while value functions
    can‚Äôt**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has two consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: We **don‚Äôt need to implement an exploration/exploitation trade-off by hand**.
    Since we output a probability distribution over actions, the agent explores¬†**the
    state space without always taking the same trajectory.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing
    is when two states seem (or are) the same but need different actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let‚Äôs take an example: we have an intelligent vacuum cleaner whose goal is
    to suck the dust and avoid killing the hamsters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/1b61fde218600e239ec27f3af716584c.png)'
  prefs: []
  type: TYPE_IMG
- en: Our vacuum cleaner can only perceive where the walls are.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the **two red (colored) states are aliased states because
    the agent perceives an upper and lower wall for each**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/63123f815fb96086071da70edf73b3fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Under a deterministic policy, the policy will either always move right when
    in a red state or always move left. **Either case will cause our agent to get
    stuck and never suck the dust**.
  prefs: []
  type: TYPE_NORMAL
- en: Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic
    policy** (‚Äúgreedy epsilon strategy‚Äù). Consequently, our agent can **spend a lot
    of time before finding the dust**.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an optimal stochastic policy **will randomly move left or
    right in red (colored) states**. Consequently, **it will not be stuck and will
    reach the goal state with a high probability**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamster 1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy-gradient methods are more effective in high-dimensional action spaces
    and continuous actions spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with Deep Q-learning is that their **predictions assign a score
    (maximum expected future reward) for each possible action**, at each time step,
    given the current state.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we have an infinite possibility of actions?
  prefs: []
  type: TYPE_NORMAL
- en: For instance, with a self-driving car, at each state, you can have a (near)
    infinite choice of actions (turning the wheel at 15¬∞, 17.2¬∞, 19,4¬∞, honking, etc.).
    **We‚Äôll need to output a Q-value for each possible action**! And **taking the
    max action of a continuous output is an optimization problem itself**!
  prefs: []
  type: TYPE_NORMAL
- en: Instead, with policy-gradient methods, we output a¬†**probability distribution
    over actions.**
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradient methods have better convergence properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In value-based methods, we use an aggressive operator to **change the value
    function: we take the maximum over Q-estimates**. Consequently, the action probabilities
    may change dramatically for an arbitrarily small change in the estimated action
    values if that change results in a different action having the maximal value.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if during the training, the best action was left (with a Q-value
    of 0.22) and the training step after it‚Äôs right (since the right Q-value becomes
    0.23), we dramatically changed the policy since now the policy will take most
    of the time right instead of left.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in policy-gradient methods, stochastic policy action preferences
    (probability of taking action) **change smoothly over time**.
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naturally, policy-gradient methods also have some disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequently, policy-gradient methods converges to a local maximum instead
    of a global optimum.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policy-gradient goes slower,¬†**step by step: it can take longer to train (inefficient).**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy-gradient can have high variance. We‚Äôll see in the actor-critic unit why,
    and how we can solve this problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üëâ If you want to go deeper into the advantages and disadvantages of policy-gradient
    methods, [you can check this video](https://youtu.be/y3oqOjHilio).
  prefs: []
  type: TYPE_NORMAL
