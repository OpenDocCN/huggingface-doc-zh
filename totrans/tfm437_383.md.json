["```py\n( prediction_length: Optional = None context_length: Optional = None distribution_output: str = 'student_t' loss: str = 'nll' input_size: int = 1 lags_sequence: List = [1, 2, 3, 4, 5, 6, 7] scaling: bool = True num_time_features: int = 0 num_dynamic_real_features: int = 0 num_static_categorical_features: int = 0 num_static_real_features: int = 0 cardinality: Optional = None embedding_dimension: Optional = None d_model: int = 64 encoder_attention_heads: int = 2 decoder_attention_heads: int = 2 encoder_layers: int = 2 decoder_layers: int = 2 encoder_ffn_dim: int = 32 decoder_ffn_dim: int = 32 activation_function: str = 'gelu' dropout: float = 0.1 encoder_layerdrop: float = 0.1 decoder_layerdrop: float = 0.1 attention_dropout: float = 0.1 activation_dropout: float = 0.1 num_parallel_samples: int = 100 init_std: float = 0.02 use_cache: bool = True is_encoder_decoder = True label_length: int = 10 moving_average: int = 25 autocorrelation_factor: int = 3 **kwargs )\n```", "```py\n>>> from transformers import AutoformerConfig, AutoformerModel\n\n>>> # Initializing a default Autoformer configuration\n>>> configuration = AutoformerConfig()\n\n>>> # Randomly initializing a model (with random weights) from the configuration\n>>> model = AutoformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: AutoformerConfig )\n```", "```py\n( past_values: Tensor past_time_features: Tensor past_observed_mask: Tensor static_categorical_features: Optional = None static_real_features: Optional = None future_values: Optional = None future_time_features: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None use_cache: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.autoformer.modeling_autoformer.AutoformerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from huggingface_hub import hf_hub_download\n>>> import torch\n>>> from transformers import AutoformerModel\n\n>>> file = hf_hub_download(\n...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n... )\n>>> batch = torch.load(file)\n\n>>> model = AutoformerModel.from_pretrained(\"huggingface/autoformer-tourism-monthly\")\n\n>>> # during training, one provides both past and future values\n>>> # as well as possible additional features\n>>> outputs = model(\n...     past_values=batch[\"past_values\"],\n...     past_time_features=batch[\"past_time_features\"],\n...     past_observed_mask=batch[\"past_observed_mask\"],\n...     static_categorical_features=batch[\"static_categorical_features\"],\n...     future_values=batch[\"future_values\"],\n...     future_time_features=batch[\"future_time_features\"],\n... )\n\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n( config: AutoformerConfig )\n```", "```py\n( past_values: Tensor past_time_features: Tensor past_observed_mask: Tensor static_categorical_features: Optional = None static_real_features: Optional = None future_values: Optional = None future_time_features: Optional = None future_observed_mask: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None use_cache: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqTSPredictionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from huggingface_hub import hf_hub_download\n>>> import torch\n>>> from transformers import AutoformerForPrediction\n\n>>> file = hf_hub_download(\n...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n... )\n>>> batch = torch.load(file)\n\n>>> model = AutoformerForPrediction.from_pretrained(\"huggingface/autoformer-tourism-monthly\")\n\n>>> # during training, one provides both past and future values\n>>> # as well as possible additional features\n>>> outputs = model(\n...     past_values=batch[\"past_values\"],\n...     past_time_features=batch[\"past_time_features\"],\n...     past_observed_mask=batch[\"past_observed_mask\"],\n...     static_categorical_features=batch[\"static_categorical_features\"],\n...     static_real_features=batch[\"static_real_features\"],\n...     future_values=batch[\"future_values\"],\n...     future_time_features=batch[\"future_time_features\"],\n... )\n\n>>> loss = outputs.loss\n>>> loss.backward()\n\n>>> # during inference, one only provides past values\n>>> # as well as possible additional features\n>>> # the model autoregressively generates future values\n>>> outputs = model.generate(\n...     past_values=batch[\"past_values\"],\n...     past_time_features=batch[\"past_time_features\"],\n...     past_observed_mask=batch[\"past_observed_mask\"],\n...     static_categorical_features=batch[\"static_categorical_features\"],\n...     static_real_features=batch[\"static_real_features\"],\n...     future_time_features=batch[\"future_time_features\"],\n... )\n\n>>> mean_prediction = outputs.sequences.mean(dim=1)\n```"]