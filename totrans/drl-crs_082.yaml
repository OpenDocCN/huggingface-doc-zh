- en: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym 🤖
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit6/hands-on](https://huggingface.co/learn/deep-rl-course/unit6/hands-on)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/67.75875c5b.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/CodeBlock.a6d3f852.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/CourseFloatingBanner.36c274d0.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">[![Ask
    a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’ve studied the theory behind Advantage Actor Critic (A2C), **you’re
    ready to train your A2C agent** using Stable-Baselines3 in a robotic environment.
    And train a:'
  prefs: []
  type: TYPE_NORMAL
- en: A robotic arm 🦾 to move to the correct position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re going to use
  prefs: []
  type: TYPE_NORMAL
- en: '[panda-gym](https://github.com/qgallouedec/panda-gym)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To validate this hands-on for the certification process, you need to push your
    two trained models to the Hub and get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PandaReachDense-v3` get a result of >= -3.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To find your result, [go to the leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on Open In Colab button** 👇 :'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym
    🤖'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '🎮 Environments:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Panda-Gym](https://github.com/qgallouedec/panda-gym)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '📚 RL-Library:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use **Panda-Gym**, the environment library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **train robots using A2C**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand why **we need to normalize the input**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score 🔥.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites 🏗️
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: 🔲 📚 Study [Actor-Critic methods by reading Unit 6](https://huggingface.co/deep-rl-course/unit6/introduction)
    🤗
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train our first robots 🤖
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set the GPU 💪
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  prefs: []
  type: TYPE_IMG
- en: Create a virtual display 🔽
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the notebook, we’ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  prefs: []
  type: TYPE_NORMAL
- en: The following cell will install the librairies and create and run a virtual
    screen 🖥
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Install dependencies 🔽
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll install multiple ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gymnasium`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`panda-gym`: Contains the robotics arm environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stable-baselines3`: The SB3 deep reinforcement learning library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face 🤗 Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huggingface_hub`: Library allowing anyone to work with the Hub repositories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages 📦
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: PandaReachDense-v3 🦾
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The agent we’re going to train is a robotic arm that needs to do controls (moving
    the arm and using the end-effector).
  prefs: []
  type: TYPE_NORMAL
- en: In robotics, the *end-effector* is the device at the end of a robotic arm designed
    to interact with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: In `PandaReach`, the robot must place its end-effector at a target position
    (green ball).
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to use the dense version of this environment. It means we’ll get
    a *dense reward function* that **will provide a reward at each timestep** (the
    closer the agent is to completing the task, the higher the reward). Contrary to
    a *sparse reward function* where the environment **return a reward if and only
    if the task is completed**.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we’re going to use the *End-effector displacement control*, it means the
    **action corresponds to the displacement of the end-effector**. We don’t control
    the individual motion of each joint (joint control).
  prefs: []
  type: TYPE_NORMAL
- en: '![Robotics](../Images/d79d62b53f91999defb0b4eae0db003d.png)'
  prefs: []
  type: TYPE_IMG
- en: This way **the training will be easier**.
  prefs: []
  type: TYPE_NORMAL
- en: Create the environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The environment 🎮
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In `PandaReachDense-v3` the robotic arm must place its end-effector at a target
    position (green ball).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The observation space **is a dictionary with 3 different elements**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`achieved_goal`: (x,y,z) position of the goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desired_goal`: (x,y,z) distance between the goal position and the current
    object position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given it’s a dictionary as observation, **we will need to use a MultiInputPolicy
    policy instead of MlpPolicy**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The action space is a vector with 3 values:'
  prefs: []
  type: TYPE_NORMAL
- en: Control x, y, z movement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize observation and rewards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).
  prefs: []
  type: TYPE_NORMAL
- en: For that purpose, there is a wrapper that will compute a running average and
    standard deviation of input features.
  prefs: []
  type: TYPE_NORMAL
- en: We also normalize rewards with this same wrapper by adding `norm_reward = True`
  prefs: []
  type: TYPE_NORMAL
- en: '[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Create the A2C Model 🤖
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information about A2C implementation with StableBaselines3 check:
    [https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes)'
  prefs: []
  type: TYPE_NORMAL
- en: To find the best parameters I checked the [official trained agents by Stable-Baselines3
    team](https://huggingface.co/sb3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Train the A2C agent 🏃
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s train our agent for 1,000,000 timesteps, don’t forget to use GPU on Colab.
    It will take approximately ~25-40min
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the agent 📈
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that’s our agent is trained, we need to **check its performance**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Publish your trained model on the Hub 🔥
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the Hub with one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: 📚 The libraries documentation 👉 [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face—x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  prefs: []
  type: TYPE_NORMAL
- en: By using `package_to_hub`, as we already mentionned in the former units, **you
    evaluate, record a replay, generate a model card of your agent and push it to
    the hub**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way:'
  prefs: []
  type: TYPE_NORMAL
- en: You can **showcase our work** 🔥
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **visualize your agent playing** 👀
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **share with the community an agent that others can use** 💾
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **access a leaderboard 🏆 to see how well your agent is performing compared
    to your classmates** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and paste the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  prefs: []
  type: TYPE_NORMAL
- en: 3️⃣ We’re now ready to push our trained agent to the 🤗 Hub 🔥 using `package_to_hub()`
    function. For this environment, **running this cell can take approximately 10min**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Some additional challenges 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to learn **is to try things by your own**! Why not trying `PandaPickAndPlace-v3`?
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to try more advanced tasks for panda-gym, you need to check what
    was done using **TQC or SAC** (a more sample-efficient algorithm suited for robotics
    tasks). In real robotics, you’ll use a more sample-efficient algorithm for a simple
    reason: contrary to a simulation **if you move your robotic arm too much, you
    have a risk of breaking it**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PandaPickAndPlace-v1 (this model uses the v1 version of the environment): [https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1](https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'And don’t hesitate to check panda-gym documentation here: [https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html](https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide you the steps to train another agent (optional):'
  prefs: []
  type: TYPE_NORMAL
- en: Define the environment called “PandaPickAndPlace-v3”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a vectorized environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a wrapper to normalize the observations and rewards. [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the A2C Model (don’t forget verbose=1 to print the training logs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train it for 1M Timesteps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the model and VecNormalize statistics when saving the agent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate your agent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Publish your trained model on the Hub 🔥 with `package_to_hub`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution (optional)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: See you on Unit 7! 🔥
  prefs: []
  type: TYPE_NORMAL
- en: Keep learning, stay awesome 🤗
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
