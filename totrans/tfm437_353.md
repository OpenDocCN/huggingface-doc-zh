# LayoutLMv3

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv3)

## 概述

LayoutLMv3模型由Yupan Huang、Tengchao Lv、Lei Cui、Yutong Lu、Furu Wei在[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387)中提出。LayoutLMv3通过使用补丁嵌入（如[ViT](vit)中的方式）简化了[LayoutLMv2](layoutlmv2)，并在3个目标上对模型进行了预训练：掩码语言建模（MLM）、掩码图像建模（MIM）和单词-补丁对齐（WPA）。

论文摘要如下：

*自监督预训练技术在文档AI领域取得了显著进展。大多数多模态预训练模型使用掩码语言建模目标来学习文本模态上的双向表示，但它们在图像模态的预训练目标上有所不同。这种差异增加了多模态表示学习的难度。在本文中，我们提出了LayoutLMv3，用于为文档AI预训练多模态Transformers，统一文本和图像掩码。此外，LayoutLMv3还使用了单词-补丁对齐目标进行预训练，通过预测文本单词的相应图像补丁是否被掩码来学习跨模态对齐。简单的统一架构和训练目标使LayoutLMv3成为文本中心和图像中心文档AI任务的通用预训练模型。实验结果表明，LayoutLMv3不仅在文本中心任务（如表单理解、收据理解和文档视觉问答）中取得了最先进的性能，而且在图像中心任务（如文档图像分类和文档布局分析）中也取得了最先进的性能。*

![drawing](../Images/af2a6f9e13d097092393e586cc161f9d.png) LayoutLMv3架构。摘自[原始论文](https://arxiv.org/abs/2204.08387)。

该模型由[nielsr](https://huggingface.co/nielsr)贡献。该模型的TensorFlow版本由[chriskoo](https://huggingface.co/chriskoo)、[tokec](https://huggingface.co/tokec)和[lre](https://huggingface.co/lre)添加。原始代码可以在[这里](https://github.com/microsoft/unilm/tree/master/layoutlmv3)找到。

## 使用提示

+   在数据处理方面，LayoutLMv3与其前身[LayoutLMv2](layoutlmv2)相同，只是：

    +   图像需要调整大小并使用常规RGB格式的通道进行归一化。另一方面，LayoutLMv2在内部对图像进行归一化，并期望通道以BGR格式提供。

    +   文本使用字节对编码（BPE）进行标记化，而不是WordPiece。由于数据预处理中的这些差异，可以使用[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)，它内部结合了[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)（用于图像模态）和[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)/[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)（用于文本模态）来为模型准备所有数据。

+   关于[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)的使用，我们参考其前身的[使用指南](layoutlmv2#usage-layoutlmv2processor)。

## 资源

Hugging Face官方和社区（🌎表示）资源列表，帮助您开始使用LayoutLMv3。如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审核！资源应该最好展示一些新东西，而不是重复现有资源。

LayoutLMv3几乎与LayoutLMv2相同，因此我们还包含了您可以为LayoutLMv3任务调整的LayoutLMv2资源。在准备模型数据时，请务必使用[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)！

+   LayoutLMv3的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LayoutLMv3)找到。

+   演示脚本可以在[这里](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3)找到。

文本分类

+   这个[笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb)支持[LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)。

+   [文本分类任务指南](../tasks/sequence_classification)

标记分类

+   这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3)和[笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb)支持[LayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification)。

+   一个关于如何使用[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)进行推断的[笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb)，以及一个关于如何在没有标签的情况下使用[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)进行推断的[笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb)。

+   一个关于如何使用🤗 Trainer对[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)进行微调的[笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb)。

+   [标记分类任务指南](../tasks/token_classification)

问答

+   这个[笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb)支持[LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering)。

+   [问答任务指南](../tasks/question_answering)

**文档问答**

+   [文档问答任务指南](../tasks/document_question_answering)

## LayoutLMv3Config

### `class transformers.LayoutLMv3Config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/configuration_layoutlmv3.py#L40)

```py
( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-05 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 max_2d_position_embeddings = 1024 coordinate_size = 128 shape_size = 128 has_relative_attention_bias = True rel_pos_bins = 32 max_rel_pos = 128 rel_2d_pos_bins = 64 max_rel_2d_pos = 256 has_spatial_attention_bias = True text_embed = True visual_embed = True input_size = 224 num_channels = 3 patch_size = 16 classifier_dropout = None **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为50265) — LayoutLMv3模型的词汇量。定义了在调用[LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer编码器中隐藏层的数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 12) — Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *optional*, 默认为 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` 或 `function`, *optional*, 默认为 `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为 0.1) — 嵌入、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为 0.1) — 注意力概率的dropout比率。

+   `max_position_embeddings` (`int`, *optional*, 默认为 512) — 该模型可能使用的最大序列长度。通常设置为一个较大的值以防万一（例如，512或1024或2048）。

+   `type_vocab_size` (`int`, *optional*, 默认为 2) — 在调用 [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model) 时传递的 `token_type_ids` 的词汇表大小。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-5) — 层归一化层使用的epsilon。

+   `max_2d_position_embeddings` (`int`, *optional*, 默认为 1024) — 2D位置嵌入可能使用的最大值。通常设置为一个较大的值以防万一（例如，1024）。

+   `coordinate_size` (`int`, *optional*, 默认为 `128`) — 坐标嵌入的维度。

+   `shape_size` (`int`, *optional*, 默认为 `128`) — 宽度和高度嵌入的维度。

+   `has_relative_attention_bias` (`bool`, *optional*, 默认为 `True`) — 是否在自注意力机制中使用相对注意力偏置。

+   `rel_pos_bins` (`int`, *optional*, 默认为 32) — 在自注意力机制中使用的相对位置桶的数量。

+   `max_rel_pos` (`int`, *optional*, 默认为 128) — 在自注意力机制中使用的最大相对位置数。

+   `max_rel_2d_pos` (`int`, *optional*, 默认为 256) — 自注意力机制中的最大2D相对位置数。

+   `rel_2d_pos_bins` (`int`, *optional*, 默认为 64) — 自注意力机制中的2D相对位置桶的数量。

+   `has_spatial_attention_bias` (`bool`, *optional*, 默认为 `True`) — 是否在自注意力机制中使用空间注意偏置。

+   `visual_embed` (`bool`, *optional*, 默认为 `True`) — 是否添加补丁嵌入。

+   `input_size` (`int`, *optional*, 默认为 `224`) — 图像的大小（分辨率）。

+   `num_channels` (`int`, *optional*, 默认为 `3`) — 图像的通道数。

+   `patch_size` (`int`, *optional*, 默认为 `16`) — 补丁的大小（分辨率）。

+   `classifier_dropout` (`float`, *optional*) — 分类头的dropout比率。

这是一个配置类，用于存储 [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model) 的配置。它用于根据指定的参数实例化一个 LayoutLMv3 模型，定义模型架构。使用默认值实例化配置将产生类似于 LayoutLMv3 [microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import LayoutLMv3Config, LayoutLMv3Model

>>> # Initializing a LayoutLMv3 microsoft/layoutlmv3-base style configuration
>>> configuration = LayoutLMv3Config()

>>> # Initializing a model (with random weights) from the microsoft/layoutlmv3-base style configuration
>>> model = LayoutLMv3Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LayoutLMv3FeatureExtractor

### `class transformers.LayoutLMv3FeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/feature_extraction_layoutlmv3.py#L28)

```py
( *args **kwargs )
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理一张图像或一批图像。

## LayoutLMv3ImageProcessor

### `class transformers.LayoutLMv3ImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py#L95)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_value: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None apply_ocr: bool = True ocr_lang: Optional = None tesseract_config: Optional = '' **kwargs )
```

参数

+   `do_resize` (`bool`, *optional*, 默认为 `True`) — 是否将图像的 (高度，宽度) 尺寸调整为 `(size["height"], size["width"])`。可以被 `preprocess` 中的 `do_resize` 覆盖。

+   `size` (`Dict[str, int]` *optional*, 默认为 `{"height" -- 224, "width": 224}`): 调整大小后的图像尺寸。可以被 `preprocess` 中的 `size` 覆盖。

+   `resample` (`PILImageResampling`, *optional*, 默认为 `PILImageResampling.BILINEAR`) — 如果调整图像大小，要使用的重采样滤波器。可以被 `preprocess` 中的 `resample` 覆盖。

+   `do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否按指定的 `rescale_value` 重新缩放图像的像素值。可以被 `preprocess` 中的 `do_rescale` 覆盖。

+   `rescale_factor` (`float`, *optional*, 默认为 1 / 255) — 图像的像素值被重新缩放的值。可以被 `preprocess` 中的 `rescale_factor` 覆盖。

+   `do_normalize` (`bool`, *optional*, 默认为 `True`) — 是否对图像进行规范化。可以被 `preprocess` 方法中的 `do_normalize` 参数覆盖。

+   `image_mean` (`Iterable[float]` 或 `float`, *optional*, 默认为 `IMAGENET_STANDARD_MEAN`) — 如果规范化图像要使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。

+   `image_std` (`Iterable[float]` 或 `float`, *optional*, 默认为 `IMAGENET_STANDARD_STD`) — 如果规范化图像要使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

+   `apply_ocr` (`bool`, *optional*, 默认为 `True`) — 是否应用 Tesseract OCR 引擎以获取单词 + 规范化边界框。可以被 `preprocess` 方法中的 `apply_ocr` 参数覆盖。

+   `ocr_lang` (`str`, *optional*) — Tesseract OCR 引擎要使用的语言，由其 ISO 代码指定。默认情况下使用英语。可以被 `preprocess` 方法中的 `ocr_lang` 参数覆盖。

+   `tesseract_config` (`str`, *optional*) — 转发到调用 Tesseract 时 `config` 参数的任何额外自定义配置标志。例如：‘—psm 6’。可以被 `preprocess` 方法中的 `tesseract_config` 参数覆盖。

构建一个 LayoutLMv3 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py#L216)

```py
( images: Union do_resize: bool = None size: Dict = None resample = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None apply_ocr: bool = None ocr_lang: Optional = None tesseract_config: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`, *optional*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *optional*, 默认为 `self.size`) — 应用 `resize` 后输出图像的期望大小。

+   `resample` (`int`, *optional*, defaults to `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是 `PILImageResampling` 滤波器之一。仅在 `do_resize` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — 是否将图像像素值重新缩放到 [0, 1] 之间。

+   `rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) — 应用于图像像素值的重新缩放因子。仅在 `do_rescale` 设置为 `True` 时有效。

+   `do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` or `Iterable[float]`, *optional*, defaults to `self.image_mean`) — 用于归一化的均值。仅在 `do_normalize` 设置为 `True` 时有效。

+   `image_std` (`float` or `Iterable[float]`, *optional*, defaults to `self.image_std`) — 用于归一化的标准差值。仅在 `do_normalize` 设置为 `True` 时有效。

+   `apply_ocr` (`bool`, *optional*, defaults to `self.apply_ocr`) — 是否应用 Tesseract OCR 引擎以获取单词 + 规范化边界框。

+   `ocr_lang` (`str`, *optional*, defaults to `self.ocr_lang`) — Tesseract OCR 引擎使用的语言，由其 ISO 代码指定。默认情况下使用英语。

+   `tesseract_config` (`str`, *optional*, defaults to `self.tesseract_config`) — 转发到调用 Tesseract 时 `config` 参数的任何额外自定义配置标志。

+   `return_tensors` (`str` or `TensorType`, *optional*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回 `np.ndarray` 的列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回类型为 `tf.Tensor` 的批次。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回类型为 `torch.Tensor` 的批次。

    +   `TensorType.NUMPY` 或 `'np'`：返回类型为 `np.ndarray` 的批次。

    +   `TensorType.JAX` 或 `'jax'`：返回类型为 `jax.numpy.ndarray` 的批次。

+   `data_format` (`ChannelDimension` 或 `str`, *optional*, defaults to `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`：图像以 (通道数, 高度, 宽度) 格式。

    +   `ChannelDimension.LAST`：图像以 (高度, 宽度, 通道数) 格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *optional*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`：图像以 (高度, 宽度) 格式。

预处理图像或图像批次。

## LayoutLMv3Tokenizer

### `class transformers.LayoutLMv3Tokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L200)

```py
( vocab_file merges_file errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = True cls_token_box = [0, 0, 0, 0] sep_token_box = [0, 0, 0, 0] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `merges_file` (`str`) — 合并文件的路径。

+   `errors` (`str`, *optional*, defaults to `"replace"`) — 解码字节为 UTF-8 时要遵循的范例。有关更多信息，请参阅 [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。

+   `bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开头标记。可用作序列分类器标记。

    构建序列时使用特殊标记，这不是序列开头使用的标记。使用的标记是 `cls_token`。

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。

    构建序列时使用特殊标记，这不是序列末尾使用的标记。使用的标记是 `sep_token`。

+   `sep_token` (`str`, *optional*, 默认为 `"</s>"`) — 分隔符标记，在构建来自多个序列的序列时使用，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *optional*, 默认为 `"<s>"`) — 分类器标记，用于进行序列分类（对整个序列进行分类，而不是每个标记进行分类）。它是使用特殊标记构建的序列的第一个标记。

+   `unk_token` (`str`, *optional*, 默认为 `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。

+   `pad_token` (`str`, *optional*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *optional*, 默认为 `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `add_prefix_space` (`bool`, *optional*, 默认为 `True`) — 是否在输入中添加初始空格。这允许将前导单词视为任何其他单词。（RoBERTa分词器通过前面的空格检测单词的开头）。

+   `cls_token_box` (`List[int]`, *optional*, 默认为 `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。

+   `sep_token_box` (`List[int]`, *optional*, 默认为 `[0, 0, 0, 0]`) — 用于特殊[SEP]标记的边界框。

+   `pad_token_box` (`List[int]`, *optional*, 默认为 `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。

+   `pad_token_label` (`int`, *optional*, 默认为 -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。

+   `only_label_first_subword` (`bool`, *optional*, 默认为 `True`) — 是否仅标记第一个子词，如果提供了单词标签。

构建一个LayoutLMv3分词器。基于`RoBERTatokenizer`（字节对编码或BPE）。[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer) 可用于将单词、单词级边界框和可选单词标签转换为标记级`input_ids`、`attention_mask`、`token_type_ids`、`bbox`和可选`labels`（用于标记分类）。

此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer) 运行端到端的分词：标点符号拆分和词块。它还将单词级边界框转换为标记级边界框。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L550)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs )
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或批次序列。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题）或一个字符串列表的列表（一批单词）。

+   `text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或批次序列。每个序列应该是一个字符串列表（预分词的字符串）。

+   `boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级边界框。每个边界框应标准化为0-1000的比例。

+   `word_labels` (`List[int]`, `List[List[int]]`, *optional*) — 单词级整数标签（用于诸如FUNSD、CORD之类的标记分类任务）。

+   `add_special_tokens` (`bool`, *optional*, defaults to `True`) — 是否对序列进行编码，使用相对于其模型的特殊标记。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, defaults to `False`) — 激活并控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 填充到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：无填充（即，可以输出具有不同长度序列的批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, defaults to `False`) — 激活并控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 截断到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则逐标记截断，从最长序列中删除一个标记。

    +   `'only_first'`: 截断到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第一个序列。

    +   `'only_second'`: 截断到指定的最大长度（使用参数 `max_length`）或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：无截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。

+   `max_length` (`int`, *optional*) — 控制截断/填充参数使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *optional*, defaults to 0) — 如果设置为一个数字，并且与 `max_length` 一起使用，当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `add_special_tokens` (`bool`, *optional*, defaults to `True`) — 是否对序列进行编码，使用相对于其模型的特殊标记。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, defaults to `False`) — 激活并控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 填充到指定的最大长度，该长度由参数 `max_length` 指定，或者填充到模型的最大可接受输入长度，如果未提供该参数。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度序列的批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批序列对），则将逐个标记截断，从较长序列中删除一个标记。

    +   `'only_first'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批序列对），则仅截断第一个序列。

    +   `'only_second'`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批序列对），则仅截断第二个序列。 

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。

+   `max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数需要最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True` 时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

用于对一个或多个序列或一个或多个序列对进行标记化和为模型准备的主要方法，其中包括单词级别的归一化边界框和可选标签。

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L431)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LayoutLMv3TokenizerFast

### `class transformers.LayoutLMv3TokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py#L65)

```py
( vocab_file = None merges_file = None tokenizer_file = None errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = True trim_offsets = True cls_token_box = [0, 0, 0, 0] sep_token_box = [0, 0, 0, 0] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇表文件的路径。

+   `merges_file` (`str`) — 合并文件的路径。

+   `errors` (`str`, *可选*, 默认为 `"replace"`) — 解码字节为 UTF-8 时要遵循的范例。有关更多信息，请参阅 [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。

+   `bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    构建序列时使用特殊标记时，这不是用于序列开头的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

    构建序列时使用特殊标记时，这不是用于序列结尾的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 在进行序列分类（整个序列而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *可选*, 默认为 `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `add_prefix_space` (`bool`, *可选*, 默认为 `False`) — 是否在输入中添加初始空格。这允许将前导单词视为任何其他单词。（RoBERTa标记器通过前面的空格检测单词的开头）。

+   `trim_offsets` (`bool`, *可选*, 默认为 `True`) — 后处理步骤是否应修剪偏移量以避免包含空格。

+   `cls_token_box` (`List[int]`, *可选*, 默认为 `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。

+   `sep_token_box` (`List[int]`, *可选*, 默认为 `[0, 0, 0, 0]`) — 用于特殊[SEP]标记的边界框。

+   `pad_token_box` (`List[int]`, *可选*, 默认为 `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。

+   `pad_token_label` (`int`, *可选*, 默认为 -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。

+   `only_label_first_subword` (`bool`, *可选*, 默认为 `True`) — 是否仅标记第一个子词，如果提供了单词标签。

构建“快速”LayoutLMv3标记器（由HuggingFace的*tokenizers*库支持）。基于BPE。

此标记器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py#L224)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs )
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题）或一个字符串列表的列表（单词批次）。

+   `text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列应该是一个字符串列表（预先标记化的字符串）。

+   `boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级别的边界框。每个边界框应该被归一化为0-1000的比例。

+   `word_labels`（`List[int]`、`List[List[int]]`，*可选*） — 单词级整数标签（用于诸如 FUNSD、CORD 等标记分类任务）。

+   `add_special_tokens`（`bool`，*可选*，默认为 `True`） — 是否使用相对于其模型的特殊标记对序列进行编码。

+   `padding`（`bool`、`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为 `False`） — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。

    +   `'max_length'`：填充到由参数 `max_length` 指定的最大长度，或者填充到模型可接受的最大输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：不进行填充（即可以输出长度不同的批次）。

+   `truncation`（`bool`、`str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为 `False`） — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则将逐个标记进行截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`：截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`：截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不进行截断（即可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length`（`int`，*可选*） — 控制截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为 0） — 如果设置为一个数字，并且与 `max_length` 一起使用，当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `pad_to_multiple_of`（`int`，*可选*） — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors`（`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*） — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`：返回 Numpy `np.ndarray` 对象。

+   `add_special_tokens`（`bool`，*可选*，默认为 `True`） — 是否使用相对于其模型的特殊标记对序列进行编码。

+   `padding`（`bool`、`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为 `False`） — 激活和控制填充。接受以下值：

    +   `True`或`'longest'`: 填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度填充，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False`或`'do_not_pad'`（默认）：不填充（即可以输出具有不同长度序列的批次）。

+   `truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`） - 激活和控制截断。接受以下值：

    +   `True`或`'longest_first'`: 使用参数`max_length`指定的最大长度截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`: 使用参数`max_length`指定的最大长度截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`: 使用参数`max_length`指定的最大长度截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第二个序列。

    +   `False`或`'do_not_truncate'`（默认）：不截断（即可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length`（`int`，*可选*） - 控制截断/填充参数之一使用的最大长度。如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为0） - 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `pad_to_multiple_of`（`int`，*可选*） - 如果设置，将序列填充到提供的值的倍数。这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*） - 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回Numpy `np.ndarray`对象。

用于标记和准备一个或多个序列或一个或多个序列对的主要方法，具有单词级别的归一化边界框和可选标签。

## LayoutLMv3Processor

### `class transformers.LayoutLMv3Processor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/processing_layoutlmv3.py#L27)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（`LayoutLMv3ImageProcessor`，*可选*） - [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)的一个实例。图像处理器是必需的输入。

+   `tokenizer`（`LayoutLMv3Tokenizer`或`LayoutLMv3TokenizerFast`，*可选*）- [LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)或[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)的实例。标记器是必需的输入。

构建一个LayoutLMv3处理器，将LayoutLMv3图像处理器和LayoutLMv3标记器组合成一个单一处理器。

[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)提供了准备数据给模型所需的所有功能。

它首先使用[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)来调整和规范文档图像，并可选择应用OCR以获取单词和规范化的边界框。然后将它们提供给[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)或[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)，将单词和边界框转换为标记级`input_ids`、`attention_mask`、`token_type_ids`、`bbox`。可选地，可以提供整数`word_labels`，这些标签被转换为用于标记分类任务（如FUNSD、CORD）的标记级`labels`。 

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/processing_layoutlmv3.py#L69)

```py
( images text: Union = None text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

此方法首先将`images`参数转发到[**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。如果[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)初始化时将`apply_ocr`设置为`True`，则将获得的单词和边界框连同其他参数传递给[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer.__call__)并返回输出，以及调整大小和规范化的`pixel_values`。如果[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)初始化时将`apply_ocr`设置为`False`，则将用户指定的单词（`text`/``text_pair`）和`boxes`连同其他参数传递给[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer.__call__)并返回输出，以及调整大小和规范化的`pixel_values`。

更多信息请参考上述两种方法的文档字符串。

Pytorch隐藏 Pytorch内容

## LayoutLMv3Model

### `class transformers.LayoutLMv3Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L734)

```py
( config )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）- 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的LayoutLMv3模型变换器输出没有特定头部的原始隐藏状态。这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L827)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`) — 词汇表中输入序列标记的索引。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `(batch_size, token_sequence_length, 4)`, *optional*) — 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是一个规范化版本，格式为(x0, y0, x1, y1)，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 文档图像的批处理。每个图像被分成形状为`(num_channels, config.patch_size, config.patch_size)`的补丁，并且补丁的总数(=`patch_sequence_length`)等于`((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, token_sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被屏蔽的标记，

    +   0表示被屏蔽的标记。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`, *optional*) — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`之间：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被屏蔽，

    +   0表示头部被屏蔽。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, token_sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入的各种元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）— 模型最后一层输出的隐藏状态序列。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为嵌入层的输出+每层的输出）。

    模型在每一层的输出隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, AutoModel
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = AutoModel.from_pretrained("microsoft/layoutlmv3-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = processor(image, words, boxes=boxes, return_tensors="pt")

>>> outputs = model(**encoding)
>>> last_hidden_states = outputs.last_hidden_state
```

## LayoutLMv3ForSequenceClassification

### `class transformers.LayoutLMv3ForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1259)

```py
( config )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv3模型在顶部具有序列分类头部（在[CLS]标记的最终隐藏状态之上的线性层），例如用于文档图像分类任务，如[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)数据集。

这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1277)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None bbox: Optional = None pixel_values: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `bbox` (`torch.LongTensor`，形状为 `(batch_size, sequence_length, 4)`，*optional*) — 每个输入序列标记的边界框。选定范围为 `[0, config.max_2d_position_embeddings-1]`。每个边界框应该是 (x0, y0, x1, y1) 格式的归一化版本，其中 (x0, y0) 对应于边界框左上角的位置，(x1, y1) 表示右下角的位置。

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 文档图像的批处理。每个图像被分成形状为 `(num_channels, config.patch_size, config.patch_size)` 的补丁，并且补丁的总数（=`patch_sequence_length`）等于 `((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在 `[0, 1]` 之间：

    +   1 代表未被 `masked` 的标记，

    +   0 代表被 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选定在 `[0, 1]` 之间：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选定范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*optional*) — 用于使自注意力模块中的特定头部失效的掩码。掩码值选定在 `[0, 1]` 之间：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 *input_ids* 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 分类（如果config.num_labels==1则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv3ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传播的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, AutoModelForSequenceClassification
>>> from datasets import load_dataset
>>> import torch

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = AutoModelForSequenceClassification.from_pretrained("microsoft/layoutlmv3-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = processor(image, words, boxes=boxes, return_tensors="pt")
>>> sequence_label = torch.tensor([1])

>>> outputs = model(**encoding, labels=sequence_label)
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## LayoutLMv3ForTokenClassification

### `class transformers.LayoutLMv3ForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1016)

```py
( config )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv3模型，顶部带有一个标记分类头（最终隐藏状态之上的线性层），例如用于序列标记（信息提取）任务的[FUNSD](https://guillaumejaume.github.io/FUNSD/)、[SROIE](https://rrc.cvc.uab.es/?ch=13)、[CORD](https://github.com/clovaai/cord)和[Kleister-NDA](https://github.com/applicaai/kleister-nda)。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。

#### `前向传播`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1039)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None pixel_values: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) — 每个输入序列标记的边界框。在范围 `[0, config.max_2d_position_embeddings-1]` 中选择。每个边界框应该是 (x0, y0, x1, y1) 格式的归一化版本，其中 (x0, y0) 对应于边界框左上角的位置，而 (x1, y1) 表示右下角的位置。

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 文档图像的批处理。每个图像被分成形状为 `(num_channels, config.patch_size, config.patch_size)` 的补丁，并且补丁的总数（=`patch_sequence_length`）等于 `((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   对于未被 `masked` 的标记为 1，

    +   对于被 `masked` 的标记为 0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将 *input_ids* 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算标记分类损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 中。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，具体取决于配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回) — 分类损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`) — 分类得分（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    模型每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, AutoModelForTokenClassification
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = AutoModelForTokenClassification.from_pretrained("microsoft/layoutlmv3-base", num_labels=7)

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]
>>> word_labels = example["ner_tags"]

>>> encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors="pt")

>>> outputs = model(**encoding)
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## LayoutLMv3ForQuestionAnswering

### `class transformers.LayoutLMv3ForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1129)

```py
( config )
```

参数

+   `config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv3模型，顶部带有一个用于提取问答任务的跨度分类头，例如[DocVQA](https://rrc.cvc.uab.es/?ch=17)（在隐藏状态输出的文本部分顶部的线性层，用于计算`span start logits`和`span end logits`）。

这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1147)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None bbox: Optional = None pixel_values: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。详情请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入ID？

+   `bbox` (`torch.LongTensor`，形状为`(batch_size, sequence_length, 4)`，*可选的*) — 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 文档图像的批处理。每个图像被分成形状为 `(num_channels, config.patch_size, config.patch_size)` 的补丁，并且补丁的总数（=`patch_sequence_length`）等于 `((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被屏蔽的标记，

    +   0 表示被屏蔽的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被屏蔽，

    +   0 表示头部被屏蔽。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 *input_ids* 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算标记范围开始位置的标签（索引）。位置被夹紧到序列的长度 (`sequence_length`)。超出序列范围的位置不会用于计算损失。

+   `end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算标记范围结束位置的标签（索引）。位置被夹紧到序列的长度 (`sequence_length`)。超出序列范围的位置不会用于计算损失。

返回

[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回) — 总跨度提取损失是开始和结束位置的交叉熵之和。

+   `start_logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）-跨度开始分数（SoftMax之前）。

+   `end_logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）-跨度结束分数（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出+每个层的输出）。

    模型在每个层的输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[LayoutLMv3ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, AutoModelForQuestionAnswering
>>> from datasets import load_dataset
>>> import torch

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = AutoModelForQuestionAnswering.from_pretrained("microsoft/layoutlmv3-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> question = "what's his name?"
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = processor(image, question, words, boxes=boxes, return_tensors="pt")
>>> start_positions = torch.tensor([1])
>>> end_positions = torch.tensor([3])

>>> outputs = model(**encoding, start_positions=start_positions, end_positions=end_positions)
>>> loss = outputs.loss
>>> start_scores = outputs.start_logits
>>> end_scores = outputs.end_logits
```

TensorFlow隐藏TensorFlow内容

## TFLayoutLMv3Model

### `class transformers.TFLayoutLMv3Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1257)

```py
( config *inputs **kwargs )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config））-模型配置类，具有模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

裸的LayoutLMv3模型变压器输出原始隐藏状态，没有特定的头部。此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

TensorFlow模型和`transformers`中的层接受两种格式的输入：

+   将所有输入作为关键字参数（如PyTorch模型），

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该“只需工作”-只需传递您的输入和标签以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：

+   只有`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个具有一个或多个输入张量的变长列表，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1269)

```py
( input_ids: tf.Tensor | None = None bbox: tf.Tensor | None = None attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None pixel_values: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`）- 词汇表中输入序列标记的索引。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox`（形状为`(batch_size, sequence_length, 4)`的`Numpy`数组或`tf.Tensor`，*可选*）- 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`）- 文档图像的批处理。每个图像被分成形状为`(num_channels, config.patch_size, config.patch_size)`的补丁，并且补丁的总数（=`patch_sequence_length`）等于`((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被掩码的标记，为1，

    +   对于被掩码的标记为0。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）- 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`之间：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*可选*）- 用于使自注意力模块中的特定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被掩码，

    +   0表示头部被掩码。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，这很有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions`（*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或`tuple(tf.Tensor)`

[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含各种元素，取决于配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 模型最后一层的隐藏状态序列输出。

+   `hidden_states`（*可选的*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层的输出隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选的*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力权重在注意力softmax之后。

[TFLayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3Model)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, TFAutoModel
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = TFAutoModel.from_pretrained("microsoft/layoutlmv3-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = processor(image, words, boxes=boxes, return_tensors="tf")

>>> outputs = model(**encoding)
>>> last_hidden_states = outputs.last_hidden_state
```

## TFLayoutLMv3ForSequenceClassification

### `class transformers.TFLayoutLMv3ForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1391)

```py
( config: LayoutLMv3Config **kwargs )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv3模型在顶部带有序列分类头（在[CLS]标记的最终隐藏状态之上的线性层），例如用于文档图像分类任务的[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)数据集。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集所有输入张量放在第一个位置参数中：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，在使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1409)

```py
( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None labels: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None bbox: tf.Tensor | None = None pixel_values: tf.Tensor | None = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`）- 词汇表中输入序列标记的索引。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。查看`pixel_values`以获取`patch_sequence_length`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox`（形状为`(batch_size, sequence_length, 4)`的`Numpy`数组或`tf.Tensor`，*可选*）- 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。查看`pixel_values`以获取`patch_sequence_length`。

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`）- 批量文档图像。每个图像被分成形状为`(num_channels, config.patch_size, config.patch_size)`的补丁，并且补丁的总数（=`patch_sequence_length`）等于`((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示未被“masked”的标记，

    +   对于被`masked`的标记。

    请注意`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（`Numpy数组`或形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    请注意`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（`Numpy数组`或形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    请注意`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`，请参阅`pixel_values`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示头部未被“masked”，

    +   0表示头部被`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或`tuple(tf.Tensor)`

[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括根据配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入的各种元素。

+   `loss`（形状为`(batch_size, )`的`tf.Tensor`，*可选*，在提供`labels`时返回）— 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`tf.Tensor`）— 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，在传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[TFLayoutLMv3ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForSequenceClassification)的前向方法覆盖了`__call__`特殊方法。

尽管需要在此函数内定义前向传递的步骤，但应该在此之后调用`Module`实例，而不是调用此函数，因为前者会负责运行前后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, TFAutoModelForSequenceClassification
>>> from datasets import load_dataset
>>> import tensorflow as tf

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = TFAutoModelForSequenceClassification.from_pretrained("microsoft/layoutlmv3-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = processor(image, words, boxes=boxes, return_tensors="tf")
>>> sequence_label = tf.convert_to_tensor([1])

>>> outputs = model(**encoding, labels=sequence_label)
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## TFLayoutLMv3ForTokenClassification

### `class transformers.TFLayoutLMv3ForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1505)

```py
( config: LayoutLMv3Config **kwargs )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config））- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv3模型在顶部带有一个标记分类头（在最终隐藏状态的顶部有一个线性层），例如用于序列标记（信息提取）任务的[FUNSD](https://guillaumejaume.github.io/FUNSD/)、[SROIE](https://rrc.cvc.uab.es/?ch=13)、[CORD](https://github.com/clovaai/cord)和[Kleister-NDA](https://github.com/applicaai/kleister-nda)。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

第二种格式得到支持的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于这种支持，当使用诸如`model.fit()`之类的方法时，对您来说应该“只需工作” - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：

+   只包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   按照文档字符串中给定的顺序，长度可变的列表，包含一个或多个输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1534)

```py
( input_ids: tf.Tensor | None = None bbox: tf.Tensor | None = None attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None labels: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None pixel_values: tf.Tensor | None = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFTokenClassifierOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`）— 输入序列令牌在词汇表中的索引。

    注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]令牌。查看`pixel_values`以获取`patch_sequence_length`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入ID？

+   `bbox`（形状为`(batch_size, sequence_length, 4)`的`Numpy`数组或`tf.Tensor`，*可选*）— 每个输入序列令牌的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

    注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]令牌。查看`pixel_values`以获取`patch_sequence_length`。

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`）— 文档图像的批处理。每个图像被分成形状为`(num_channels, config.patch_size, config.patch_size)`的补丁，并且补丁的总数（=`patch_sequence_length`）等于`((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用于避免在填充令牌索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   1表示未被`掩码`的令牌。

    +   0表示被`掩码`的令牌。

    注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]令牌。查看`pixel_values`以获取`patch_sequence_length`。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）— 段令牌索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*令牌，

    +   1对应于*句子B*令牌。

    注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]令牌。查看`pixel_values`以获取`patch_sequence_length`。

    什么是令牌类型ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）— 每个输入序列令牌在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]令牌。查看`pixel_values`以获取`patch_sequence_length`。

    什么是位置ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。选择在`[0, 1]`范围内的掩码值：

    +   1表示头部未被`掩码`，

    +   0表示头部被`掩码`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`，*可选*） — 用于计算标记分类损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。

返回

[transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入的各种元素。

+   `loss` (`tf.Tensor` of shape `(n,)`，*可选*，当提供`labels`时返回，其中n是未屏蔽标签的数量） — 分类损失。

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`) — 分类得分（SoftMax之前）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[TFLayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForTokenClassification)前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, TFAutoModelForTokenClassification
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = TFAutoModelForTokenClassification.from_pretrained("microsoft/layoutlmv3-base", num_labels=7)

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]
>>> word_labels = example["ner_tags"]

>>> encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors="tf")

>>> outputs = model(**encoding)
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## TFLayoutLMv3ForQuestionAnswering

### `class transformers.TFLayoutLMv3ForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1643)

```py
( config: LayoutLMv3Config **kwargs )
```

参数

+   `config`（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

LayoutLMv3模型，顶部带有用于提取问答任务的跨度分类头，例如[DocVQA](https://rrc.cvc.uab.es/?ch=17)（在隐藏状态输出的文本部分顶部的线性层，用于计算`span start logits`和`span end logits`）。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或者

+   将所有输入作为列表、元组或字典的第一个位置参数。

支持第二种格式的原因是，Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可用于在第一个位置参数中收集所有输入张量：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1663)

```py
( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None start_positions: tf.Tensor | None = None end_positions: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None bbox: tf.Tensor | None = None pixel_values: tf.Tensor | None = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy数组`或`tf.Tensor`） - 词汇表中输入序列标记的索引。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。查看`pixel_values`以获取`patch_sequence_length`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox`（形状为`(batch_size, sequence_length, 4)`的`Numpy数组`或`tf.Tensor`，*可选*） - 每个输入序列标记的边界框。在范围`[0, config.max_2d_position_embeddings-1]`中选择。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。查看`pixel_values`以获取`patch_sequence_length`。

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`） - 批量文档图像。每个图像被分成形状为`(num_channels, config.patch_size, config.patch_size)`的补丁，并且补丁的总数（=`patch_sequence_length`）等于`((height / config.patch_size) * (width / config.patch_size))`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的蒙版。蒙版值在`[0, 1]`中选择：

    +   1表示标记未被遮蔽，

    +   0表示标记被遮蔽。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`的详细信息，请参见`pixel_values`。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）— 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于一个*句子A*的标记，

    +   1对应于一个*句子B*的标记。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`的详细信息，请参见`pixel_values`。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    请注意，`sequence_length = token_sequence_length + patch_sequence_length + 1`，其中`1`代表[CLS]标记。有关`patch_sequence_length`的详细信息，请参见`pixel_values`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*可选*）— 用于使自注意力模块的选定头部无效的蒙版。蒙版值在`[0, 1]`中选择：

    +   1表示头部未被遮蔽，

    +   0表示头部被遮蔽。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将*input_ids*索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `start_positions`（形状为`(batch_size,)`的`tf.Tensor`，*可选*）— 用于计算标记分类损失的标记（索引）的标签的起始位置。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。

+   `end_positions`（形状为`(batch_size,)`的`tf.Tensor`，*可选*）— 用于计算标记分类损失的标记（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。

返回

[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)）和输入的不同元素。

+   `loss` (`tf.Tensor`，形状为 `(batch_size, )`，*可选*，当提供 `start_positions` 和 `end_positions` 时返回） — 总跨度提取损失是起始位置和结束位置的交叉熵之和。

+   `start_logits` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`) — 跨度起始得分（SoftMax 之前）。

+   `end_logits` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`) — 跨度结束得分（SoftMax 之前）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出以及初始嵌入输出的隐藏状态。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每个层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

[TFLayoutLMv3ForQuestionAnswering](/docs/transformers/v4.37.2/zh/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForQuestionAnswering) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, TFAutoModelForQuestionAnswering
>>> from datasets import load_dataset
>>> import tensorflow as tf

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
>>> model = TFAutoModelForQuestionAnswering.from_pretrained("microsoft/layoutlmv3-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> image = example["image"]
>>> question = "what's his name?"
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = processor(image, question, words, boxes=boxes, return_tensors="tf")
>>> start_positions = tf.convert_to_tensor([1])
>>> end_positions = tf.convert_to_tensor([3])

>>> outputs = model(**encoding, start_positions=start_positions, end_positions=end_positions)
>>> loss = outputs.loss
>>> start_scores = outputs.start_logits
>>> end_scores = outputs.end_logits
```
