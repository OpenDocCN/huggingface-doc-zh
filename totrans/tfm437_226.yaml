- en: Reformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Reformer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer)
- en: '[![Models](../Images/39c0a0def8fb2c57ad6c8ff17747e239.png)](https://huggingface.co/models?filter=reformer)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/reformer-crime-and-punishment)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Models](../Images/39c0a0def8fb2c57ad6c8ff17747e239.png)](https://huggingface.co/models?filter=reformer)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/reformer-crime-and-punishment)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reformer模型是由Nikita Kitaev，Łukasz Kaiser，Anselm Levskaya在论文[Reformer: The Efficient
    Transformer](https://arxiv.org/abs/2001.04451.pdf)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*Large Transformer models routinely achieve state-of-the-art results on a number
    of tasks but training these models can be prohibitively costly, especially on
    long sequences. We introduce two techniques to improve the efficiency of Transformers.
    For one, we replace dot-product attention by one that uses locality-sensitive
    hashing, changing its complexity from O(L^2) to O(Llog(L)), where L is the length
    of the sequence. Furthermore, we use reversible residual layers instead of the
    standard residuals, which allows storing activations only once in the training
    process instead of N times, where N is the number of layers. The resulting model,
    the Reformer, performs on par with Transformer models while being much more memory-efficient
    and much faster on long sequences.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*大型Transformer模型通常在许多任务上取得最先进的结果，但训练这些模型可能成本过高，特别是在长序列上。我们引入了两种技术来提高Transformer的效率。首先，我们通过使用局部敏感哈希来替换点积注意力，将其复杂度从O(L^2)改为O(Llog(L))，其中L是序列的长度。此外，我们使用可逆残差层而不是标准残差，这允许在训练过程中仅存储激活一次，而不是N次，其中N是层数。结果模型Reformer在与Transformer模型相媲美的同时，内存效率更高，在长序列上速度更快。*'
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The Authors’ code can be found [here](https://github.com/google/trax/tree/master/trax/models/reformer).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献。作者的代码可以在[这里](https://github.com/google/trax/tree/master/trax/models/reformer)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: 'Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch,
    see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于PyTorch中的一个错误，Reformer不与torch.nn.DataParallel一起工作，请参阅[问题＃36035](https://github.com/pytorch/pytorch/issues/36035)。
- en: Use Axial position encoding (see below for more details). It’s a mechanism to
    avoid having a huge positional encoding matrix (when the sequence length is very
    big) by factorizing it into smaller matrices.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用轴向位置编码（有关更多详细信息，请参见下文）。这是一种通过将其分解为较小的矩阵来避免具有巨大位置编码矩阵（当序列长度非常大时）的机制。
- en: Replace traditional attention by LSH (local-sensitive hashing) attention (see
    below for more details). It’s a technique to avoid computing the full product
    query-key in the attention layers.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过LSH（局部敏感哈希）注意力替换传统的注意力（有关更多详细信息，请参见下文）。这是一种避免在注意力层中计算完整的查询-键乘积的技术。
- en: Avoid storing the intermediate results of each layer by using reversible transformer
    layers to obtain them during the backward pass (subtracting the residuals from
    the input of the next layer gives them back) or recomputing them for results inside
    a given layer (less efficient than storing them but saves memory).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用可逆Transformer层避免存储每层的中间结果，可以在反向传播过程中获取这些结果（从下一层的输入中减去残差即可获得它们），或者在给定层内重新计算结果（不如存储它们高效，但可以节省内存）。
- en: Compute the feedforward operations by chunks and not on the whole batch.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按块计算前向操作，而不是整个批次。
- en: Axial Positional Encodings
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轴向位置编码
- en: 'Axial Positional Encodings were first implemented in Google’s [trax library](https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29)
    and developed by the authors of this model’s paper. In models that are treating
    very long input sequences, the conventional position id encodings store an embedings
    vector of size<math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math>d
    being the `config.hidden_size` for every position<math><semantics><mrow><mi>i</mi><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">i, \ldots, n_s</annotation></semantics></math>i,…,ns​,
    with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being `config.max_embedding_size`.
    This means that having a sequence length of<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msup><mn>2</mn><mn>19</mn></msup><mo>≈</mo><mn>0.5</mn><mi>M</mi></mrow><annotation
    encoding="application/x-tex">n_s = 2^{19} \approx 0.5M</annotation></semantics></math>ns​=219≈0.5M
    and a `config.hidden_size` of<math><semantics><mrow><mi>d</mi><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup><mo>≈</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">d = 2^{10} \approx 1000</annotation></semantics></math>d=210≈1000
    would result in a position encoding matrix: <math display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>d</mi><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X_{i,j},
    \text{ with } i \in \left[1,\ldots, d\right] \text{ and } j \in \left[1,\ldots,
    n_s\right]</annotation></semantics></math>Xi,j​, with i∈[1,…,d] and j∈[1,…,ns​]'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 轴向位置编码首次在Google的trax库中实现，并由该模型论文的作者开发。在处理非常长的输入序列的模型中，传统的位置id编码会为每个位置i,…,ns​存储一个大小为d的嵌入向量，其中ns​为config.max_embedding_size。这意味着当序列长度为ns​=219≈0.5M，而config.hidden_size为d=210≈1000时，将得到一个位置编码矩阵：Xi,j​，其中i∈[1,…,d]，j∈[1,…,ns​]
- en: 'which alone has over 500M parameters to store. Axial positional encodings factorize<math><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{i,j}</annotation></semantics></math>Xi,j​
    into two matrices: <math display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>1</mn></msup><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{1}_{i,j},
    \text{ with } i \in \left[1,\ldots, d^1\right] \text{ and } j \in \left[1,\ldots,
    n_s^1\right]</annotation></semantics></math>Xi,j1​, with i∈[1,…,d1] and j∈[1,…,ns1​]'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中单独有超过500M个参数需要存储。轴向位置编码将Xi,j分解为两个矩阵：Xi,j1​，其中i∈[1,…,d1]，j∈[1,…,ns1​]
- en: and <math display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{2}_{i,j},
    \text{ with } i \in \left[1,\ldots, d^2\right] \text{ and } j \in \left[1,\ldots,
    n_s^2\right]</annotation></semantics></math>Xi,j2​, with i∈[1,…,d2] and j∈[1,…,ns2​]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 和Xi,j2​，其中i∈[1,…,d2]，且j∈[1,…,ns2​]
- en: 'with: <math display="block"><semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>d</mi><mn>1</mn></msup><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mtext> and </mtext><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mi
    mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">d =
    d^1 + d^2 \text{ and } n_s = n_s^1 \times n_s^2 .</annotation></semantics></math>d=d1+d2 and ns​=ns1​×ns2​.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下成立：
- en: 'Therefore the following holds: <math display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable
    rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>k</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><mtext>if  </mtext><mi>i</mi><mo><</mo><msup><mi>d</mi><mn>1</mn></msup><mtext> with </mtext><mi>k</mi><mo>=</mo><mi>j</mi><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo>−</mo><msup><mi>d</mi><mn>1</mn></msup><mo
    separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mo>≥</mo><msup><mi>d</mi><mn>1</mn></msup><mtext> with </mtext><mi>l</mi><mo>=</mo><mo
    stretchy="false">⌊</mo><mfrac><mi>j</mi><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mfrac><mo
    stretchy="false">⌋</mo></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex">X_{i,j} = \begin{cases} X^{1}_{i, k}, & \text{if
    }\ i < d^1 \text{ with } k = j \mod n_s^1 \\ X^{2}_{i - d^1, l}, & \text{if }
    i \ge d^1 \text{ with } l = \lfloor\frac{j}{n_s^1}\rfloor \end{cases}</annotation></semantics></math>Xi,j​={Xi,k1​,Xi−d1,l2​,​if  i<d1 with k=jmodns1​if i≥d1 with l=⌊ns1​j​⌋​'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Xi,j = {Xi,k1​，如果i < d1，且k = j mod ns1​Xi−d1,l2​，如果i ≥ d1，且l = ⌊j/ns1​⌋}
- en: Intuitively, this means that a position embedding vector<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>∈</mo><msup><mi
    mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_j
    \in \mathbb{R}^{d}</annotation></semantics></math>xj​∈Rd is now the composition
    of two factorized embedding vectors:<math><semantics><mrow><msubsup><mi>x</mi><mrow><mi>k</mi><mo
    separator="true">,</mo><mi>l</mi></mrow><mn>1</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mrow><mi>l</mi><mo
    separator="true">,</mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow><annotation
    encoding="application/x-tex">x^1_{k, l} + x^2_{l, k}</annotation></semantics></math>xk,l1​+xl,k2​,
    where as the `config.max_embedding_size` dimension<math><semantics><mrow><mi>j</mi></mrow><annotation
    encoding="application/x-tex">j</annotation></semantics></math>j is factorized
    into<math><semantics><mrow><mi>k</mi><mtext> and </mtext><mi>l</mi></mrow><annotation
    encoding="application/x-tex">k \text{ and } l</annotation></semantics></math>k and l.
    This design ensures that each position embedding vector<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">x_j</annotation></semantics></math>xj​ is unique.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，这意味着位置嵌入向量xj∈Rd现在是两个分解嵌入向量的组合：xk,l1+xl,k2，其中`config.max_embedding_size`维度j被分解为k和l。这种设计确保每个位置嵌入向量xj是唯一的。
- en: Using the above example again, axial position encoding with<math><semantics><mrow><msup><mi>d</mi><mn>1</mn></msup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup></mrow><annotation
    encoding="application/x-tex">d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}</annotation></semantics></math>d1=29,d2=29,ns1​=29,ns2​=210
    can drastically reduced the number of parameters from 500 000 000 to<math><semantics><mrow><msup><mn>2</mn><mn>18</mn></msup><mo>+</mo><msup><mn>2</mn><mn>19</mn></msup><mo>≈</mo><mn>780000</mn></mrow><annotation
    encoding="application/x-tex">2^{18} + 2^{19} \approx 780 000</annotation></semantics></math>218+219≈780000
    parameters, this means 85% less memory usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用上面的示例，轴向位置编码与d1=29，d2=29，ns1=29，ns2=210可以将参数数量从500,000,000大幅减少到218+219≈780,000个参数，这意味着内存使用减少了85%。
- en: In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple<math><semantics><mrow><mo
    stretchy="false">(</mo><msup><mi>d</mi><mn>1</mn></msup><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(d^1, d^2)</annotation></semantics></math>(d1,d2)
    which sum has to be equal to `config.hidden_size` and `config.axial_pos_shape`
    is set to a tuple<math><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">(n_s^1, n_s^2)</annotation></semantics></math>(ns1​,ns2​)
    which product has to be equal to `config.max_embedding_size`, which during training
    has to be equal to the *sequence length* of the `input_ids`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，参数`config.axial_pos_embds_dim`设置为一个元组(d1, d2)，其总和必须等于`config.hidden_size`，而`config.axial_pos_shape`设置为一个元组(ns1,
    ns2)，其乘积必须等于`config.max_embedding_size`，在训练期间必须等于`input_ids`的*序列长度*。
- en: LSH Self Attention
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSH自注意力
- en: In Locality sensitive hashing (LSH) self attention the key and query projection
    weights are tied. Therefore, the key query embedding vectors are also tied. LSH
    self attention uses the locality sensitive hashing mechanism proposed in [Practical
    and Optimal LSH for Angular Distance](https://arxiv.org/abs/1509.02897) to assign
    each of the tied key query embedding vectors to one of `config.num_buckets` possible
    buckets. The premise is that the more “similar” key query embedding vectors (in
    terms of *cosine similarity*) are to each other, the more likely they are assigned
    to the same bucket.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在局部敏感哈希（LSH）自注意力中，键和查询投影权重是绑定的。因此，键查询嵌入向量也是绑定的。LSH自注意力使用了[角距离的实用和最优LSH](https://arxiv.org/abs/1509.02897)中提出的局部敏感哈希机制，将这些绑定的键查询嵌入向量分配给`config.num_buckets`可能的桶之一。前提是，键查询嵌入向量（按*余弦相似度*）越“相似”，它们被分配到同一个桶的可能性就越大。
- en: The accuracy of the LSH mechanism can be improved by increasing `config.num_hashes`
    or directly the argument `num_hashes` of the forward function so that the output
    of the LSH self attention better approximates the output of the “normal” full
    self attention. The buckets are then sorted and chunked into query key embedding
    vector chunks each of length `config.lsh_chunk_length`. For each chunk, the query
    embedding vectors attend to its key vectors (which are tied to themselves) and
    to the key embedding vectors of `config.lsh_num_chunks_before` previous neighboring
    chunks and `config.lsh_num_chunks_after` following neighboring chunks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LSH机制的准确性可以通过增加`config.num_hashes`或直接增加前向函数的参数`num_hashes`来提高，以便LSH自注意力的输出更好地逼近“正常”完全自注意力的输出。然后桶被排序并分块成查询键嵌入向量块，每个块的长度为`config.lsh_chunk_length`。对于每个块，查询嵌入向量会关注其键向量（它们与自身相连）以及`config.lsh_num_chunks_before`之前相邻块和`config.lsh_num_chunks_after`之后相邻块的键嵌入向量。
- en: For more information, see the [original Paper](https://arxiv.org/abs/2001.04451)
    or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[原始论文](https://arxiv.org/abs/2001.04451)或这篇很棒的[博客文章](https://www.pragmatic.ml/reformer-deep-dive/)。
- en: Note that `config.num_buckets` can also be factorized into a list<math><semantics><mrow><mo
    stretchy="false">(</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_{\text{buckets}}^1,
    n_{\text{buckets}}^2)</annotation></semantics></math>(nbuckets1​,nbuckets2​).
    This way instead of assigning the query key embedding vectors to one of<math><semantics><mrow><mo
    stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mtext>buckets</mtext></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,\ldots,
    n_{\text{buckets}})</annotation></semantics></math>(1,…,nbuckets​) they are assigned
    to one of<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-1,\ldots,
    n_{\text{buckets}}^1-1, \ldots, 1-n_{\text{buckets}}^2, \ldots, n_{\text{buckets}}^1-n_{\text{buckets}}^2)</annotation></semantics></math>(1−1,…,nbuckets1​−1,…,1−nbuckets2​,…,nbuckets1​−nbuckets2​).
    This is crucial for very long sequences to save memory.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`config.num_buckets`也可以分解为一个列表<math><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_{\text{buckets}}^1,
    n_{\text{buckets}}^2)</annotation></semantics></math>(nbuckets1​,nbuckets2​)。这样，与其将查询键嵌入向量分配给<math><semantics><mrow><mo
    stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mtext>buckets</mtext></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,\ldots,
    n_{\text{buckets}})</annotation></semantics></math>(1,…,nbuckets​)中的一个，它们被分配给<math><semantics><mrow><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo
    separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-1,\ldots,
    n_{\text{buckets}}^1-1, \ldots, 1-n_{\text{buckets}}^2, \ldots, n_{\text{buckets}}^1-n_{\text{buckets}}^2)</annotation></semantics></math>(1−1,…,nbuckets1​−1,…,1−nbuckets2​,…,nbuckets1​−nbuckets2​)。这对于非常长的序列来说非常重要，可以节省内存。
- en: When training a model from scratch, it is recommended to leave `config.num_buckets=None`,
    so that depending on the sequence length a good value for `num_buckets` is calculated
    on the fly. This value will then automatically be saved in the config and should
    be reused for inference.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在从头开始训练模型时，建议将`config.num_buckets=None`，这样根据序列长度会动态计算出一个好的`num_buckets`值。然后这个值会自动保存在配置中，并应该在推断中重复使用。
- en: Using LSH self attention, the memory and time complexity of the query-key matmul
    operation can be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)), which usually
    represents the memory and time bottleneck in a transformer model, with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSH自注意力，查询-键乘法操作的内存和时间复杂度可以从<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​)减少到<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​))，这通常代表了变压器模型中的内存和时间瓶颈，其中<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​是序列长度。
- en: Local Self Attention
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地自注意力
- en: Local self attention is essentially a “normal” self attention layer with key,
    query and value projections, but is chunked so that in each chunk of length `config.local_chunk_length`
    the query embedding vectors only attends to the key embedding vectors in its chunk
    and to the key embedding vectors of `config.local_num_chunks_before` previous
    neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 局部自注意力本质上是一个“普通”的自注意力层，具有键、查询和值投影，但被分块处理，以便在每个长度为`config.local_chunk_length`的块中，查询嵌入向量只关注其块中的键嵌入向量以及`config.local_num_chunks_before`之前相邻块和`config.local_num_chunks_after`之后相邻块的键嵌入向量。
- en: Using Local self attention, the memory and time complexity of the query-key
    matmul operation can be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)), which usually
    represents the memory and time bottleneck in a transformer model, with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用局部自注意力，查询-键乘法操作的内存和时间复杂度可以从<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​)减少到<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)，这通常代表了变压器模型中的内存和时间瓶颈，其中<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​是序列长度。
- en: Training
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: During training, we must ensure that the sequence length is set to a value that
    can be divided by the least common multiple of `config.lsh_chunk_length` and `config.local_chunk_length`
    and that the parameters of the Axial Positional Encodings are correctly set as
    described above. Reformer is very memory efficient so that the model can easily
    be trained on sequences as long as 64000 tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们必须确保序列长度设置为可以被`config.lsh_chunk_length`和`config.local_chunk_length`的最小公倍数整除，并且Axial
    Positional Encodings的参数设置正确如上所述。Reformer非常节省内存，因此模型可以轻松地在长达64000个标记的序列上进行训练。
- en: 'For training, the [ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)
    should be used as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，应该使用[ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)如下所示：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掩码语言建模任务指南](../tasks/masked_language_modeling)'
- en: ReformerConfig
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerConfig
- en: '### `class transformers.ReformerConfig`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/configuration_reformer.py#L32)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/configuration_reformer.py#L32)'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`attention_head_size` (`int`, *optional*, defaults to 64) — Dimensionality
    of the projected key, query and value vectors'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_head_size` (`int`, *可选*, 默认为64) — 投影键、查询和值向量的维度'
- en: '`attn_layers` (`List[str]`, *optional*, defaults to `["local", "lsh", "local",
    "lsh", "local", "lsh"]`) — List of attention layer types in ascending order. It
    can be chosen between a LSHSelfAttention layer (`"lsh"`) and a LocalSelfAttention
    layer (`"local"`).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_layers` (`List[str]`, *可选*, 默认为`["local", "lsh", "local", "lsh", "local",
    "lsh"]`) — 按升序排列的注意力层类型列表。可以在LSHSelfAttention层(`"lsh"`)和LocalSelfAttention层(`"local"`)之间进行选择。'
- en: For more information on LSHSelfAttention layer, see [LSH Self Attention](reformer#lsh-self-attention).
    For more information on LocalSelfAttention layer, see [Local Self Attention](reformer#local-self-attention).
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关LSHSelfAttention层的更多信息，请参阅[LSH Self Attention](reformer#lsh-self-attention)。有关LocalSelfAttention层的更多信息，请参阅[Local
    Self Attention](reformer#local-self-attention)。
- en: '`axial_pos_embds` (`bool`, *optional*, defaults to `True`) — Whether or not
    to use axial position embeddings. For more information on how axial position embeddings
    work, see [Axial Position Encodings](reformer#axial-positional-encodings).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axial_pos_embds` (`bool`, *可选*, 默认为`True`) — 是否使用轴向位置嵌入。有关轴向位置嵌入工作原理的更多信息，请参阅[Axial
    Position Encodings](reformer#axial-positional-encodings)。'
- en: '`axial_norm_std` (`float`, *optional*, defaults to 1.0) — The standard deviation
    of the normal_initializer for initializing the weight matrices of the axial positional
    encodings.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axial_norm_std` (`float`, *可选*, 默认为1.0) — 用于初始化轴向位置编码的权重矩阵的正态初始化器的标准差。'
- en: '`axial_pos_shape` (`List[int]`, *optional*, defaults to `[64, 64]`) — The position
    dims of the axial position encodings. During training, the product of the position
    dims has to be equal to the sequence length.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axial_pos_shape` (`List[int]`, *可选*, 默认为`[64, 64]`) — 轴向位置编码的位置维度。在训练过程中，位置维度的乘积必须等于序列长度。'
- en: For more information on how axial position embeddings work, see [Axial Position
    Encodings](reformer#axial-positional-encodings).
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关轴向位置编码工作原理的更多信息，请参阅[Axial Position Encodings](reformer#axial-positional-encodings)。
- en: '`axial_pos_embds_dim` (`List[int]`, *optional*, defaults to `[64, 192]`) —
    The embedding dims of the axial position encodings. The sum of the embedding dims
    has to be equal to the hidden size.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axial_pos_embds_dim` (`List[int]`, *optional*, defaults to `[64, 192]`) —
    轴向位置编码的嵌入维度。嵌入维度之和必须等于隐藏大小。'
- en: For more information on how axial position embeddings work, see [Axial Position
    Encodings](reformer#axial-positional-encodings).
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关轴向位置编码的更多信息，请参阅 [Axial Position Encodings](reformer#axial-positional-encodings)。
- en: '`chunk_size_lm_head` (`int`, *optional*, defaults to 0) — The chunk size of
    the final language model feed forward head layer. A chunk size of 0 means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward
    layer processes n < sequence_length embeddings at a time.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_size_lm_head` (`int`, *optional*, defaults to 0) — 最终语言模型前馈头层的块大小。块大小为
    0 表示前馈层未分块。块大小为 n 表示前馈层一次处理 n < 序列长度的嵌入。'
- en: For more information on feed forward chunking, see [How does Feed Forward Chunking
    work?](../glossary#feed-forward-chunking).
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关前馈分块的更多信息，请参阅 [How does Feed Forward Chunking work?](../glossary#feed-forward-chunking)。
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — The token id for the end-of-sentence
    token.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to 2) — 句子结束标记的标记 ID。'
- en: '`feed_forward_size` (`int`, *optional*, defaults to 512) — Dimensionality of
    the feed_forward layer in the residual attention block.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feed_forward_size` (`int`, *optional*, defaults to 512) — 残差注意力块中前馈层的维度。'
- en: '`hash_seed` (`int`, *optional*) — Seed that can be used to make local sensitive
    hashing in `LSHSelfAttention` deterministic. This should only be set for testing
    purposed. For evaluation and training purposes `hash_seed` should be left as `None`
    to ensure fully random rotations in local sensitive hashing scheme.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hash_seed` (`int`, *optional*) — 可用于使 `LSHSelfAttention` 中的局部敏感哈希确定性的种子。这仅应用于测试目的。在评估和训练过程中，应将
    `hash_seed` 设置为 `None`，以确保局部敏感哈希方案中完全随机的旋转。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the feed forward layer
    in the residual attention block. If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"`
    are supported.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"relu"`) — 残差注意力块中前馈层的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.05) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.05) — 嵌入层、编码器和池化器中所有全连接层的
    dropout 概率。'
- en: '`hidden_size` (`int`, *optional*, defaults to 256) — Dimensionality of the
    output hidden states of the residual attention blocks.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 256) — 残差注意力块的输出隐藏状态的维度。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`is_decoder` (`bool`, *optional*, defaults to `False`) — Whether or not to
    use a causal mask in addition to the `attention_mask` passed to [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).
    When using the Reformer for causal language modeling, this argument should be
    set to `True`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_decoder` (`bool`, *optional*, defaults to `False`) — 是否在 [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)
    中使用因果掩码。在使用 Reformer 进行因果语言建模时，此参数应设置为 `True`。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。'
- en: '`local_chunk_length` (`int`, *optional*, defaults to 64) — Length of chunk
    which attends to itself in `LocalSelfAttention`. Chunking reduces memory complexity
    from sequence length x sequence length (self attention) to chunk length x chunk
    length x sequence length / chunk length (chunked self attention).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_chunk_length` (`int`, *optional*, defaults to 64) — 在 `LocalSelfAttention`
    中自身关注的块的长度。分块可将内存复杂度从序列长度 x 序列长度（自注意力）降低到块长度 x 块长度 x 序列长度 / 块长度（分块自注意力）。'
- en: '`local_num_chunks_before` (`int`, *optional*, defaults to 1) — Number of previous
    neighbouring chunks to attend to in `LocalSelfAttention` layer to itself.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_num_chunks_before` (`int`, *optional*, defaults to 1) — 在 `LocalSelfAttention`
    层中要关注的前面相邻块的数量。'
- en: '`local_num_chunks_after` (`int`, *optional*, defaults to 0) — Number of following
    neighbouring chunks to attend to in `LocalSelfAttention` layer in addition to
    itself.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_num_chunks_after` (`int`, *optional*, defaults to 0) — 在 `LocalSelfAttention`
    层中除自身外要关注的后续相邻块的数量。'
- en: '`local_attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1)
    — The dropout ratio for the attention probabilities in `LocalSelfAttention`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1)
    — `LocalSelfAttention` 中注意力概率的 dropout 比例。'
- en: '`lsh_attn_chunk_length` (`int`, *optional*, defaults to 64) — Length of chunk
    which attends to itself in `LSHSelfAttention`. Chunking reduces memory complexity
    from sequence length x sequence length (self attention) to chunk length x chunk
    length x sequence length / chunk length (chunked self attention).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lsh_attn_chunk_length` (`int`, *optional*, defaults to 64) — 在 `LSHSelfAttention`
    中自身关注的块的长度。分块可将内存复杂度从序列长度 x 序列长度（自注意力）降低到块长度 x 块长度 x 序列长度 / 块长度（分块自注意力）。'
- en: '`lsh_num_chunks_before` (`int`, *optional*, defaults to 1) — Number of previous
    neighbouring chunks to attend to in `LSHSelfAttention` layer to itself.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lsh_num_chunks_before` (`int`, *optional*, defaults to 1) — 在 `LSHSelfAttention`
    层中要关注的前面相邻块的数量。'
- en: '`lsh_num_chunks_after` (`int`, *optional*, defaults to 0) — Number of following
    neighbouring chunks to attend to in `LSHSelfAttention` layer to itself.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lsh_num_chunks_after` (`int`, *optional*, defaults to 0) — 在 `LSHSelfAttention`
    层中自身关注的后续相邻块的数量。'
- en: '`lsh_attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) —
    The dropout ratio for the attention probabilities in `LSHSelfAttention`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lsh_attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) —
    `LSHSelfAttention` 中注意力概率的 dropout 比例。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 4096) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 4096) — 此模型可能会使用的最大序列长度。通常将其设置为一个较大的值以防万一（例如，512或1024或2048）。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`num_buckets` (`int` or `List[int]`, *optional*) — Number of buckets, the key
    query vectors can be “hashed into” using the locality sensitive hashing scheme.
    Each query key vector is hashed into a hash in `1, ..., num_buckets`. The number
    of buckets can also be factorized into a list for improved memory complexity.
    In this case, each query key vector is hashed into a hash in `1-1, 1-2, ..., num_buckets[0]-1,
    ..., num_buckets[0]-num_buckets[1]` if `num_buckets` is factorized into two factors.
    The number of buckets (or the product the factors) should approximately equal
    sequence length / lsh_chunk_length. If `num_buckets` not set, a good value is
    calculated on the fly.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_buckets` (`int` or `List[int]`, *optional*) — 桶的数量，可以使用局部敏感哈希方案将查询向量“哈希”到其中。每个查询键向量被哈希到`1,
    ..., num_buckets`中的一个哈希值。桶的数量也可以分解为一个列表，以提高内存复杂度。在这种情况下，如果`num_buckets`分解为两个因子，则每个查询键向量被哈希到`1-1,
    1-2, ..., num_buckets[0]-1, ..., num_buckets[0]-num_buckets[1]`中的一个哈希值。桶的数量（或因子的乘积）应大致等于序列长度/
    lsh_chunk_length。如果未设置`num_buckets`，则会动态计算一个良好的值。'
- en: '`num_hashes` (`int`, *optional*, defaults to 1) — Number of hashing rounds
    (e.g., number of random rotations) in Local Sensitive Hashing scheme. The higher
    `num_hashes`, the more accurate the `LSHSelfAttention` becomes, but also the more
    memory and time intensive the hashing becomes.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hashes` (`int`, *optional*, defaults to 1) — 局部敏感哈希方案中的哈希轮数（例如，随机旋转的次数）。`num_hashes`越高，`LSHSelfAttention`越准确，但哈希变得更加耗费内存和时间。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The token id for the padding
    token.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, defaults to 0) — 用于填充标记的标记ID。'
- en: '`vocab_size` (`int`, *optional*, defaults to 320) —\ Vocabulary size of the
    Reformer model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 320) — Reformer模型的词汇大小。定义了在调用[ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)时可以表示的不同标记的数量。'
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether to
    tie input and output embeddings.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — 是否绑定输入和输出嵌入。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`classifier_dropout` (`float`, *optional*) — The dropout ratio for the classification
    head.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout` (`float`, *optional*) — 分类头的丢失比率。'
- en: This is the configuration class to store the configuration of a [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).
    It is used to instantiate a Reformer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ReFormer [google/reformer-crime-and-punishment](https://huggingface.co/google/reformer-crime-and-punishment)
    architecture.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)配置的配置类。根据指定的参数实例化Reformer模型，定义模型架构。使用默认值实例化配置将产生与ReFormer
    [google/reformer-crime-and-punishment](https://huggingface.co/google/reformer-crime-and-punishment)架构类似的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ReformerTokenizer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerTokenizer
- en: '### `class transformers.ReformerTokenizer`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.ReformerTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L48)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L48)'
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含实例化分词器所需词汇表的[SentencePiece](https://github.com/google/sentencepiece)文件（通常具有*.spm*扩展名）。'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在构建使用特殊标记的序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `[]`) — Additional
    special tokens used by the tokenizer.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `[]`) — 分词器使用的额外特殊标记。'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs`（`dict`，*可选*）— 将传递给 `SentencePieceProcessor.__init__()` 方法。[SentencePiece
    的 Python 包装器](https://github.com/google/sentencepiece/tree/master/python) 可以用于设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`：启用子词正则化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`：unigram 的抽样参数。对于 BPE-Dropout 无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`：不执行抽样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`：从 nbest_size 结果中抽样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`: 假设 nbest_size 是无限的，并使用前向过滤和后向抽样算法从所有假设（格子）中抽样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：unigram 抽样的平滑参数，以及 BPE-dropout 合并操作的丢弃概率。'
- en: Construct a Reformer tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)
    .
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Reformer 分词器。基于 [SentencePiece](https://github.com/google/sentencepiece)。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `save_vocabulary`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L171)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L171)'
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ReformerTokenizerFast
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerTokenizerFast
- en: '### `class transformers.ReformerTokenizerFast`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer_fast.py#L57)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer_fast.py#L57)'
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`）— 包含实例化分词器所需词汇表的 [SentencePiece](https://github.com/google/sentencepiece)
    文件（通常具有 *.spm* 扩展名）。'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为 `"</s>"`）— 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是 `sep_token`。
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为 `"<unk>"`）— 未知标记。词汇表中不存在的标记无法转换为 ID，并被设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为 `"<pad>"`）— 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`additional_special_tokens` (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`（`List[str]`，*可选*）— 分词器使用的其他特殊标记。'
- en: Construct a “fast” Reformer tokenizer (backed by HuggingFace’s *tokenizers*
    library). Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速” Reformer 分词器（由 HuggingFace 的 *tokenizers* 库支持）。基于 [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models)。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: ReformerModel
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerModel
- en: '### `class transformers.ReformerModel`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L1972)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L1972)'
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: 'The bare Reformer Model transformer outputting raw hidden-stateswithout any
    specific head on top. Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '裸的 Reformer 模型变压器输出原始隐藏状态，没有特定的头部。Reformer 是由 Nikita Kitaev、Łukasz Kaiser、Anselm
    Levskaya 在 [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2004)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2004)'
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。在训练期间，input_ids的sequence_length必须是相关模型块长度（lsh的、local的或两者的）的倍数。在评估期间，这些索引会自动填充为块长度的倍数。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被掩盖的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被掩盖的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *可选*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩盖。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *可选*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hashes` (`int`, *可选*) — 在分桶期间应执行的哈希轮数。设置此参数会覆盖`config.num_hashes`中定义的默认值。'
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)中的`num_hashes`。
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *可选*) — 长度为`config.n_layers`的`Tuple(torch.LongTensor, torch.FloatTensor)`列表，第一个元素是先前的*桶*的形状为`(batch_size,
    num_heads, num_hashes, sequence_length)`，第二个元素是先前的*隐藏状态*的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态和桶（仅与LSH自注意力相关）。可用于加速顺序解码。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.reformer.modeling_reformer.ReformerModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.reformer.modeling_reformer.ReformerModelOutput` 或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.reformer.modeling_reformer.ReformerModelOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.reformer.modeling_reformer.ReformerModelOutput` 或一个
    `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置
    ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    和输入而异的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, num_predict, hidden_size)`)
    — 模型最后一层的隐藏状态序列。'
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`num_predict` 对应于 `target_mapping.shape[1]`。如果 `target_mapping` 为 `None`，则
    `num_predict` 对应于 `sequence_length`。'
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`)
    — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)`，*可选*，当传递
    `use_cache=True` 或 `config.use_cache=True` 时返回） — 长度为 `config.n_layers` 的 `Tuple(torch.LongTensor,
    torch.FloatTensor` 列表，第一个元素是形状为 `(batch_size, num_heads, num_hashes, sequence_length)`
    的先前 *桶*，第二个元素是形状为 `(batch_size, sequence_length, hidden_size)` 的先前 *隐藏状态*)。'
- en: Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states`
    input) to speed up sequential decoding.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的桶和隐藏状态，可用于加速顺序解码（参见 `past_buckets_states` 输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings and one for the output of each layer) of
    shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出，一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ReformerModelWithLMHead
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerModelWithLMHead
- en: '### `class transformers.ReformerModelWithLMHead`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerModelWithLMHead`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2187)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2187)'
- en: '[PRE9]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: 'Reformer Model with a `language modeling` head on top. Reformer was proposed
    in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by
    Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '在顶部带有 `语言建模` 头的 Reformer 模型。Reformer 是由 Nikita Kitaev、Łukasz Kaiser、Anselm
    Levskaya 在 [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2215)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2215)'
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`） — 词汇表中输入序列标记的索引。在训练期间，input_ids的sequence_length必须是相关模型的块长度（lsh的、local的或两者的）的倍数。在评估期间，这些索引会自动填充为块长度的倍数。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被屏蔽的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被屏蔽的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被屏蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hashes` (`int`, *optional*) — 在分桶过程中应执行的哈希轮数。设置此参数会覆盖`config.num_hashes`中定义的默认值。'
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)中的`num_hashes`。
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — 长度为`config.n_layers`的`Tuple(torch.LongTensor, torch.FloatTensor`列表，第一个元素是形状为`(batch_size,
    num_heads, num_hashes, sequence_length)`的先前*桶*，第二个元素是形状为`(batch_size, sequence_length,
    hidden_size)`的先前*隐藏状态*)。'
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态和桶（仅与LSH自注意力相关）。可用于加速顺序解码。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在`[-100,
    0, ..., config.vocab_size - 1]`范围内。所有设置为`-100`的标签都会被忽略（掩码），损失仅计算标签在`[0, ..., config.vocab_size]`范围内的。'
- en: Returns
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    语言建模损失（用于下一个标记的预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)
    forward method, overrides the `__call__` special method.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ReformerForMaskedLM
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerForMaskedLM
- en: '### `class transformers.ReformerForMaskedLM`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerForMaskedLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2313)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2313)'
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'Reformer Model with a `language modeling` head on top. Reformer was proposed
    in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by
    Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reformer模型顶部带有`语言建模`头。Reformer是由Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya在[Reformer:
    The Efficient Transformer](https://arxiv.org/abs/2001.04451)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2335)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2335)'
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。在训练期间，input_ids的sequence_length必须是相关模型的块长度（lsh的、local的或两者的）的倍数。在评估期间，这些索引会自动填充为块长度的倍数。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“掩盖”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩盖”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hashes`（`int`，*可选*）— 在分桶期间应执行的哈希轮数。设置此参数会覆盖`config.num_hashes`中定义的默认值。'
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关更多信息，请参见[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)中的`num_hashes`。
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_buckets_states`（`List[Tuple(torch.LongTensor, torch.FloatTensor)]`，*可选*）—
    长度为`config.n_layers`的`Tuple(torch.LongTensor, torch.FloatTensor)`列表，第一个元素是形状为`(batch_size,
    num_heads, num_hashes, sequence_length)`的先前*桶*，第二个元素是形状为`(batch_size, sequence_length,
    hidden_size)`的先前*隐藏状态*）。'
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态和桶（仅与LSH自注意力相关）。可用于加速顺序解码。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]` 范围内（参见 `input_ids`
    文档字符串）。索引设置为 `-100` 的标记将被忽略（掩码），损失仅计算具有标签的标记'
- en: Returns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回)
    — 掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型具有嵌入层，则为嵌入输出的一个 + 每层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型每层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力权重。
- en: The [ReformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForMaskedLM)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: This example uses a false checkpoint since we don’t have any available pretrained
    model for the masked language modeling task with the Reformer architecture.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用了一个虚假的检查点，因为我们没有任何可用的预训练模型，用于 Reformer 架构的掩码语言建模任务。
- en: 'Example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ReformerForSequenceClassification
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerForSequenceClassification
- en: '### `class transformers.ReformerForSequenceClassification`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2437)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2437)'
- en: '[PRE16]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: Reformer Model transformer with a sequence classification/regression head on
    top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Reformer 模型变压器，顶部带有一个序列分类/回归头（池化输出的线性层），例如用于 GLUE 任务。
- en: 'Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reformer 是由 Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya 在 [Reformer: The Efficient
    Transformer](https://arxiv.org/abs/2001.04451) 中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2458)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2458)'
- en: '[PRE17]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。在训练期间，input_ids 序列长度必须是相关模型的块长度（lsh''s、local''s 或两者）的倍数。在评估期间，索引会自动填充为块长度的倍数。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表未被遮蔽的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表被遮蔽的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]`
    中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被遮蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids`
    索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hashes` (`int`, *optional*) — 在分桶期间应执行的哈希轮数。设置此参数会覆盖 `config.num_hashes`
    中定义的默认值。'
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关更多信息，请参见 [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)
    中的 `num_hashes`。
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — 长度为 `config.n_layers` 的 `Tuple(torch.LongTensor, torch.FloatTensor`
    列表，第一个元素是形状为 `(batch_size, num_heads, num_hashes, sequence_length)` 的先前 *buckets*，第二个元素是形状为
    `(batch_size, sequence_length, hidden_size)` 的先前 *hidden_states*)。'
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态和桶（仅与 LSH 自注意力相关）。可用于加速顺序解码。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为 `True`，则返回 `past_key_values` 键值状态，可用于加速解码（参见
    `past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或者`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或者一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时），包括根据配置（[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)）和输入不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *可选*，当提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个，加上每一层的输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_attentions=True`或者当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ReformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类示例：
- en: '[PRE18]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ReformerForQuestionAnswering
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReformerForQuestionAnswering
- en: '### `class transformers.ReformerForQuestionAnswering`'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ReformerForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2584)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2584)'
- en: '[PRE20]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Reformer Model with a span classification head on top for extractive question-answering
    tasks like SQuAD / TriviaQA ( a linear layer on top of hidden-states output to
    compute `span start logits` and `span end logits`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Reformer模型，顶部带有一个用于提取问题回答任务（如SQuAD / TriviaQA）的跨度分类头（在隐藏状态输出的顶部有一个线性层，用于计算`span
    start logits`和`span end logits`。
- en: 'Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reformer是由Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya在[Reformer: The Efficient
    Transformer](https://arxiv.org/abs/2001.04451)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2603)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2603)'
- en: '[PRE21]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。在训练期间，`input_ids` 序列长度必须是相关模型块长度（lsh''s、local''s或两者的倍数）。在评估期间，这些索引会自动填充为块长度的倍数。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hashes` (`int`, *optional*) — 在分桶期间应执行的哈希轮数。设置此参数会覆盖`config.num_hashes`中定义的默认值。'
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)中的`num_hashes`。
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — 长度为`config.n_layers`的`Tuple(torch.LongTensor, torch.FloatTensor`列表，第一个元素是先前的*桶*的形状`(batch_size,
    num_heads, num_hashes, sequence_length)`，第二个元素是先前的*隐藏状态*的形状`(batch_size, sequence_length,
    hidden_size)`。'
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态和桶（仅与LSH自注意力相关）。可用于加速顺序解码。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — 用于计算标记范围的起始位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度(`sequence_length`)。序列之外的位置不会被考虑在内计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    用于计算标记范围的结束位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。序列之外的位置不会被考虑在内计算损失。'
- en: Returns
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（[ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    总跨度提取损失是起始位置和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 跨度开始得分（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 跨度结束得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ReformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForQuestionAnswering)
    的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
