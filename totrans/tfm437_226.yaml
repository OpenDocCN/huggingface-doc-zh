- en: Reformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/230.c033c89b.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">[![Models](../Images/39c0a0def8fb2c57ad6c8ff17747e239.png)](https://huggingface.co/models?filter=reformer)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/reformer-crime-and-punishment)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large Transformer models routinely achieve state-of-the-art results on a number
    of tasks but training these models can be prohibitively costly, especially on
    long sequences. We introduce two techniques to improve the efficiency of Transformers.
    For one, we replace dot-product attention by one that uses locality-sensitive
    hashing, changing its complexity from O(L^2) to O(Llog(L)), where L is the length
    of the sequence. Furthermore, we use reversible residual layers instead of the
    standard residuals, which allows storing activations only once in the training
    process instead of N times, where N is the number of layers. The resulting model,
    the Reformer, performs on par with Transformer models while being much more memory-efficient
    and much faster on long sequences.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The Authors’ code can be found [here](https://github.com/google/trax/tree/master/trax/models/reformer).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch,
    see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Axial position encoding (see below for more details). It’s a mechanism to
    avoid having a huge positional encoding matrix (when the sequence length is very
    big) by factorizing it into smaller matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace traditional attention by LSH (local-sensitive hashing) attention (see
    below for more details). It’s a technique to avoid computing the full product
    query-key in the attention layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid storing the intermediate results of each layer by using reversible transformer
    layers to obtain them during the backward pass (subtracting the residuals from
    the input of the next layer gives them back) or recomputing them for results inside
    a given layer (less efficient than storing them but saves memory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the feedforward operations by chunks and not on the whole batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axial Positional Encodings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Axial Positional Encodings were first implemented in Google’s [trax library](https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29)
    and developed by the authors of this model’s paper. In models that are treating
    very long input sequences, the conventional position id encodings store an embedings
    vector of size<math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math>d
    being the `config.hidden_size` for every position<math><semantics><mrow><mi>i</mi><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">i, \ldots, n_s</annotation></semantics></math>i,…,ns​,
    with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being `config.max_embedding_size`.
    This means that having a sequence length of<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msup><mn>2</mn><mn>19</mn></msup><mo>≈</mo><mn>0.5</mn><mi>M</mi></mrow><annotation
    encoding="application/x-tex">n_s = 2^{19} \approx 0.5M</annotation></semantics></math>ns​=219≈0.5M
    and a `config.hidden_size` of<math><semantics><mrow><mi>d</mi><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup><mo>≈</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">d = 2^{10} \approx 1000</annotation></semantics></math>d=210≈1000
    would result in a position encoding matrix: <math display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>d</mi><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X_{i,j},
    \text{ with } i \in \left[1,\ldots, d\right] \text{ and } j \in \left[1,\ldots,
    n_s\right]</annotation></semantics></math>Xi,j​, with i∈[1,…,d] and j∈[1,…,ns​]'
  prefs: []
  type: TYPE_NORMAL
- en: 'which alone has over 500M parameters to store. Axial positional encodings factorize<math><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{i,j}</annotation></semantics></math>Xi,j​
    into two matrices: <math display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>1</mn></msup><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{1}_{i,j},
    \text{ with } i \in \left[1,\ldots, d^1\right] \text{ and } j \in \left[1,\ldots,
    n_s^1\right]</annotation></semantics></math>Xi,j1​, with i∈[1,…,d1] and j∈[1,…,ns1​]'
  prefs: []
  type: TYPE_NORMAL
- en: and <math display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{2}_{i,j},
    \text{ with } i \in \left[1,\ldots, d^2\right] \text{ and } j \in \left[1,\ldots,
    n_s^2\right]</annotation></semantics></math>Xi,j2​, with i∈[1,…,d2] and j∈[1,…,ns2​]
  prefs: []
  type: TYPE_NORMAL
- en: 'with: <math display="block"><semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>d</mi><mn>1</mn></msup><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mtext> and </mtext><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mi
    mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">d =
    d^1 + d^2 \text{ and } n_s = n_s^1 \times n_s^2 .</annotation></semantics></math>d=d1+d2 and ns​=ns1​×ns2​.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore the following holds: <math display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable
    rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>k</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><mtext>if  </mtext><mi>i</mi><mo><</mo><msup><mi>d</mi><mn>1</mn></msup><mtext> with </mtext><mi>k</mi><mo>=</mo><mi>j</mi><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo>−</mo><msup><mi>d</mi><mn>1</mn></msup><mo
    separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mo>≥</mo><msup><mi>d</mi><mn>1</mn></msup><mtext> with </mtext><mi>l</mi><mo>=</mo><mo
    stretchy="false">⌊</mo><mfrac><mi>j</mi><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mfrac><mo
    stretchy="false">⌋</mo></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex">X_{i,j} = \begin{cases} X^{1}_{i, k}, & \text{if
    }\ i < d^1 \text{ with } k = j \mod n_s^1 \\ X^{2}_{i - d^1, l}, & \text{if }
    i \ge d^1 \text{ with } l = \lfloor\frac{j}{n_s^1}\rfloor \end{cases}</annotation></semantics></math>Xi,j​={Xi,k1​,Xi−d1,l2​,​if  i<d1 with k=jmodns1​if i≥d1 with l=⌊ns1​j​⌋​'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this means that a position embedding vector<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>∈</mo><msup><mi
    mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_j
    \in \mathbb{R}^{d}</annotation></semantics></math>xj​∈Rd is now the composition
    of two factorized embedding vectors:<math><semantics><mrow><msubsup><mi>x</mi><mrow><mi>k</mi><mo
    separator="true">,</mo><mi>l</mi></mrow><mn>1</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mrow><mi>l</mi><mo
    separator="true">,</mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow><annotation
    encoding="application/x-tex">x^1_{k, l} + x^2_{l, k}</annotation></semantics></math>xk,l1​+xl,k2​,
    where as the `config.max_embedding_size` dimension<math><semantics><mrow><mi>j</mi></mrow><annotation
    encoding="application/x-tex">j</annotation></semantics></math>j is factorized
    into<math><semantics><mrow><mi>k</mi><mtext> and </mtext><mi>l</mi></mrow><annotation
    encoding="application/x-tex">k \text{ and } l</annotation></semantics></math>k and l.
    This design ensures that each position embedding vector<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">x_j</annotation></semantics></math>xj​ is unique.
  prefs: []
  type: TYPE_NORMAL
- en: Using the above example again, axial position encoding with<math><semantics><mrow><msup><mi>d</mi><mn>1</mn></msup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup></mrow><annotation
    encoding="application/x-tex">d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}</annotation></semantics></math>d1=29,d2=29,ns1​=29,ns2​=210
    can drastically reduced the number of parameters from 500 000 000 to<math><semantics><mrow><msup><mn>2</mn><mn>18</mn></msup><mo>+</mo><msup><mn>2</mn><mn>19</mn></msup><mo>≈</mo><mn>780000</mn></mrow><annotation
    encoding="application/x-tex">2^{18} + 2^{19} \approx 780 000</annotation></semantics></math>218+219≈780000
    parameters, this means 85% less memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple<math><semantics><mrow><mo
    stretchy="false">(</mo><msup><mi>d</mi><mn>1</mn></msup><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(d^1, d^2)</annotation></semantics></math>(d1,d2)
    which sum has to be equal to `config.hidden_size` and `config.axial_pos_shape`
    is set to a tuple<math><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">(n_s^1, n_s^2)</annotation></semantics></math>(ns1​,ns2​)
    which product has to be equal to `config.max_embedding_size`, which during training
    has to be equal to the *sequence length* of the `input_ids`.
  prefs: []
  type: TYPE_NORMAL
- en: LSH Self Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Locality sensitive hashing (LSH) self attention the key and query projection
    weights are tied. Therefore, the key query embedding vectors are also tied. LSH
    self attention uses the locality sensitive hashing mechanism proposed in [Practical
    and Optimal LSH for Angular Distance](https://arxiv.org/abs/1509.02897) to assign
    each of the tied key query embedding vectors to one of `config.num_buckets` possible
    buckets. The premise is that the more “similar” key query embedding vectors (in
    terms of *cosine similarity*) are to each other, the more likely they are assigned
    to the same bucket.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the LSH mechanism can be improved by increasing `config.num_hashes`
    or directly the argument `num_hashes` of the forward function so that the output
    of the LSH self attention better approximates the output of the “normal” full
    self attention. The buckets are then sorted and chunked into query key embedding
    vector chunks each of length `config.lsh_chunk_length`. For each chunk, the query
    embedding vectors attend to its key vectors (which are tied to themselves) and
    to the key embedding vectors of `config.lsh_num_chunks_before` previous neighboring
    chunks and `config.lsh_num_chunks_after` following neighboring chunks.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, see the [original Paper](https://arxiv.org/abs/2001.04451)
    or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).
  prefs: []
  type: TYPE_NORMAL
- en: Note that `config.num_buckets` can also be factorized into a list<math><semantics><mrow><mo
    stretchy="false">(</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_{\text{buckets}}^1,
    n_{\text{buckets}}^2)</annotation></semantics></math>(nbuckets1​,nbuckets2​).
    This way instead of assigning the query key embedding vectors to one of<math><semantics><mrow><mo
    stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mtext>buckets</mtext></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,\ldots,
    n_{\text{buckets}})</annotation></semantics></math>(1,…,nbuckets​) they are assigned
    to one of<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-1,\ldots,
    n_{\text{buckets}}^1-1, \ldots, 1-n_{\text{buckets}}^2, \ldots, n_{\text{buckets}}^1-n_{\text{buckets}}^2)</annotation></semantics></math>(1−1,…,nbuckets1​−1,…,1−nbuckets2​,…,nbuckets1​−nbuckets2​).
    This is crucial for very long sequences to save memory.
  prefs: []
  type: TYPE_NORMAL
- en: When training a model from scratch, it is recommended to leave `config.num_buckets=None`,
    so that depending on the sequence length a good value for `num_buckets` is calculated
    on the fly. This value will then automatically be saved in the config and should
    be reused for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Using LSH self attention, the memory and time complexity of the query-key matmul
    operation can be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)), which usually
    represents the memory and time bottleneck in a transformer model, with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Local Self Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local self attention is essentially a “normal” self attention layer with key,
    query and value projections, but is chunked so that in each chunk of length `config.local_chunk_length`
    the query embedding vectors only attends to the key embedding vectors in its chunk
    and to the key embedding vectors of `config.local_num_chunks_before` previous
    neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Using Local self attention, the memory and time complexity of the query-key
    matmul operation can be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)), which usually
    represents the memory and time bottleneck in a transformer model, with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During training, we must ensure that the sequence length is set to a value that
    can be divided by the least common multiple of `config.lsh_chunk_length` and `config.local_chunk_length`
    and that the parameters of the Axial Positional Encodings are correctly set as
    described above. Reformer is very memory efficient so that the model can easily
    be trained on sequences as long as 64000 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'For training, the [ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)
    should be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text classification task guide](../tasks/sequence_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Question answering task guide](../tasks/question_answering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReformerConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/configuration_reformer.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: ( attention_head_size = 64 attn_layers = ['local', 'lsh', 'local', 'lsh', 'local',
    'lsh'] axial_norm_std = 1.0 axial_pos_embds = True axial_pos_shape = [64, 64]
    axial_pos_embds_dim = [64, 192] chunk_size_lm_head = 0 eos_token_id = 2 feed_forward_size
    = 512 hash_seed = None hidden_act = 'relu' hidden_dropout_prob = 0.05 hidden_size
    = 256 initializer_range = 0.02 is_decoder = False layer_norm_eps = 1e-12 local_num_chunks_before
    = 1 local_num_chunks_after = 0 local_attention_probs_dropout_prob = 0.05 local_attn_chunk_length
    = 64 lsh_attn_chunk_length = 64 lsh_attention_probs_dropout_prob = 0.0 lsh_num_chunks_before
    = 1 lsh_num_chunks_after = 0 max_position_embeddings = 4096 num_attention_heads
    = 12 num_buckets = None num_hashes = 1 pad_token_id = 0 vocab_size = 320 tie_word_embeddings
    = False use_cache = True classifier_dropout = None **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**attention_head_size** (`int`, *optional*, defaults to 64) — Dimensionality
    of the projected key, query and value vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attn_layers** (`List[str]`, *optional*, defaults to `["local", "lsh", "local",
    "lsh", "local", "lsh"]`) — List of attention layer types in ascending order. It
    can be chosen between a LSHSelfAttention layer (`"lsh"`) and a LocalSelfAttention
    layer (`"local"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on LSHSelfAttention layer, see [LSH Self Attention](reformer#lsh-self-attention).
    For more information on LocalSelfAttention layer, see [Local Self Attention](reformer#local-self-attention).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**axial_pos_embds** (`bool`, *optional*, defaults to `True`) — Whether or not
    to use axial position embeddings. For more information on how axial position embeddings
    work, see [Axial Position Encodings](reformer#axial-positional-encodings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**axial_norm_std** (`float`, *optional*, defaults to 1.0) — The standard deviation
    of the normal_initializer for initializing the weight matrices of the axial positional
    encodings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**axial_pos_shape** (`List[int]`, *optional*, defaults to `[64, 64]`) — The
    position dims of the axial position encodings. During training, the product of
    the position dims has to be equal to the sequence length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how axial position embeddings work, see [Axial Position
    Encodings](reformer#axial-positional-encodings).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**axial_pos_embds_dim** (`List[int]`, *optional*, defaults to `[64, 192]`)
    — The embedding dims of the axial position encodings. The sum of the embedding
    dims has to be equal to the hidden size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how axial position embeddings work, see [Axial Position
    Encodings](reformer#axial-positional-encodings).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**chunk_size_lm_head** (`int`, *optional*, defaults to 0) — The chunk size
    of the final language model feed forward head layer. A chunk size of 0 means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward
    layer processes n < sequence_length embeddings at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on feed forward chunking, see [How does Feed Forward Chunking
    work?](../glossary#feed-forward-chunking).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**eos_token_id** (`int`, *optional*, defaults to 2) — The token id for the
    end-of-sentence token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**feed_forward_size** (`int`, *optional*, defaults to 512) — Dimensionality
    of the feed_forward layer in the residual attention block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hash_seed** (`int`, *optional*) — Seed that can be used to make local sensitive
    hashing in `LSHSelfAttention` deterministic. This should only be set for testing
    purposed. For evaluation and training purposes `hash_seed` should be left as `None`
    to ensure fully random rotations in local sensitive hashing scheme.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `Callable`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the feed forward layer
    in the residual attention block. If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"`
    are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_dropout_prob** (`float`, *optional*, defaults to 0.05) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 256) — Dimensionality of the
    output hidden states of the residual attention blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_decoder** (`bool`, *optional*, defaults to `False`) — Whether or not to
    use a causal mask in addition to the `attention_mask` passed to [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).
    When using the Reformer for causal language modeling, this argument should be
    set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_chunk_length** (`int`, *optional*, defaults to 64) — Length of chunk
    which attends to itself in `LocalSelfAttention`. Chunking reduces memory complexity
    from sequence length x sequence length (self attention) to chunk length x chunk
    length x sequence length / chunk length (chunked self attention).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_num_chunks_before** (`int`, *optional*, defaults to 1) — Number of
    previous neighbouring chunks to attend to in `LocalSelfAttention` layer to itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_num_chunks_after** (`int`, *optional*, defaults to 0) — Number of following
    neighbouring chunks to attend to in `LocalSelfAttention` layer in addition to
    itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_attention_probs_dropout_prob** (`float`, *optional*, defaults to 0.1)
    — The dropout ratio for the attention probabilities in `LocalSelfAttention`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lsh_attn_chunk_length** (`int`, *optional*, defaults to 64) — Length of chunk
    which attends to itself in `LSHSelfAttention`. Chunking reduces memory complexity
    from sequence length x sequence length (self attention) to chunk length x chunk
    length x sequence length / chunk length (chunked self attention).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lsh_num_chunks_before** (`int`, *optional*, defaults to 1) — Number of previous
    neighbouring chunks to attend to in `LSHSelfAttention` layer to itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lsh_num_chunks_after** (`int`, *optional*, defaults to 0) — Number of following
    neighbouring chunks to attend to in `LSHSelfAttention` layer to itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lsh_attention_probs_dropout_prob** (`float`, *optional*, defaults to 0.1)
    — The dropout ratio for the attention probabilities in `LSHSelfAttention`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_position_embeddings** (`int`, *optional*, defaults to 4096) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_buckets** (`int` or `List[int]`, *optional*) — Number of buckets, the
    key query vectors can be “hashed into” using the locality sensitive hashing scheme.
    Each query key vector is hashed into a hash in `1, ..., num_buckets`. The number
    of buckets can also be factorized into a list for improved memory complexity.
    In this case, each query key vector is hashed into a hash in `1-1, 1-2, ..., num_buckets[0]-1,
    ..., num_buckets[0]-num_buckets[1]` if `num_buckets` is factorized into two factors.
    The number of buckets (or the product the factors) should approximately equal
    sequence length / lsh_chunk_length. If `num_buckets` not set, a good value is
    calculated on the fly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hashes** (`int`, *optional*, defaults to 1) — Number of hashing rounds
    (e.g., number of random rotations) in Local Sensitive Hashing scheme. The higher
    `num_hashes`, the more accurate the `LSHSelfAttention` becomes, but also the more
    memory and time intensive the hashing becomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_id** (`int`, *optional*, defaults to 0) — The token id for the
    padding token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 320) —\ Vocabulary size of the
    Reformer model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tie_word_embeddings** (`bool`, *optional*, defaults to `False`) — Whether
    to tie input and output embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*, defaults to `True`) — Whether or not the
    model should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**classifier_dropout** (`float`, *optional*) — The dropout ratio for the classification
    head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).
    It is used to instantiate a Reformer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ReFormer [google/reformer-crime-and-punishment](https://huggingface.co/google/reformer-crime-and-punishment)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ReformerTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L48)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vocab_file eos_token = ''</s>'' unk_token = ''<unk>'' additional_special_tokens
    = [] sp_model_kwargs: Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**additional_special_tokens** (`List[str]`, *optional*, defaults to `[]`) —
    Additional special tokens used by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model_kwargs** (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a Reformer tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)
    .
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L171)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: str filename_prefix: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: ReformerTokenizerFast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerTokenizerFast'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer_fast.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_file = None tokenizer_file = None eos_token = '</s>' unk_token = '<unk>'
    additional_special_tokens = [] **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**additional_special_tokens** (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a “fast” Reformer tokenizer (backed by HuggingFace’s *tokenizers*
    library). Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: ReformerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L1972)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bare Reformer Model transformer outputting raw hidden-stateswithout any
    specific head on top. Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2004)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None position_ids:
    Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes:
    Optional = None past_buckets_states: Optional = None use_cache: Optional = None
    output_hidden_states: Optional = None output_attentions: Optional = None return_dict:
    Optional = None ) → `transformers.models.reformer.modeling_reformer.ReformerModelOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hashes** (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_buckets_states** (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.reformer.modeling_reformer.ReformerModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.reformer.modeling_reformer.ReformerModelOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_buckets_states** (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`)
    — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states`
    input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings and one for the output of each layer) of
    shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ReformerModelWithLMHead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerModelWithLMHead'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2187)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reformer Model with a `language modeling` head on top. Reformer was proposed
    in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by
    Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2215)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None position_ids: Optional = None attention_mask:
    Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes:
    Optional = None past_buckets_states: Optional = None use_cache: Optional = None
    output_hidden_states: Optional = None output_attentions: Optional = None return_dict:
    Optional = None labels: Optional = None ) → [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hashes** (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_buckets_states** (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ReformerForMaskedLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerForMaskedLM'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2313)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reformer Model with a `language modeling` head on top. Reformer was proposed
    in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by
    Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2335)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None position_ids: Optional = None attention_mask:
    Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes:
    Optional = None labels: Optional = None output_hidden_states: Optional = None
    output_attentions: Optional = None return_dict: Optional = None ) → [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hashes** (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_buckets_states** (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForMaskedLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a false checkpoint since we don’t have any available pretrained
    model for the masked language modeling task with the Reformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ReformerForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerForSequenceClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2437)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reformer Model transformer with a sequence classification/regression head on
    top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2458)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None position_ids: Optional = None attention_mask:
    Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes:
    Optional = None labels: Optional = None output_hidden_states: Optional = None
    output_attentions: Optional = None return_dict: Optional = None ) → [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hashes** (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_buckets_states** (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example of single-label classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ReformerForQuestionAnswering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ReformerForQuestionAnswering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2584)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reformer Model with a span classification head on top for extractive question-answering
    tasks like SQuAD / TriviaQA ( a linear layer on top of hidden-states output to
    compute `span start logits` and `span end logits`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2603)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None position_ids: Optional = None attention_mask:
    Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes:
    Optional = None start_positions: Optional = None end_positions: Optional = None
    output_hidden_states: Optional = None output_attentions: Optional = None return_dict:
    Optional = None ) → [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hashes** (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_buckets_states** (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_positions** (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
