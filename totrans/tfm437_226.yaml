- en: Reformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/reformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Models](../Images/39c0a0def8fb2c57ad6c8ff17747e239.png)](https://huggingface.co/models?filter=reformer)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/reformer-crime-and-punishment)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*Large Transformer models routinely achieve state-of-the-art results on a number
    of tasks but training these models can be prohibitively costly, especially on
    long sequences. We introduce two techniques to improve the efficiency of Transformers.
    For one, we replace dot-product attention by one that uses locality-sensitive
    hashing, changing its complexity from O(L^2) to O(Llog(L)), where L is the length
    of the sequence. Furthermore, we use reversible residual layers instead of the
    standard residuals, which allows storing activations only once in the training
    process instead of N times, where N is the number of layers. The resulting model,
    the Reformer, performs on par with Transformer models while being much more memory-efficient
    and much faster on long sequences.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The Authors’ code can be found [here](https://github.com/google/trax/tree/master/trax/models/reformer).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch,
    see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Axial position encoding (see below for more details). It’s a mechanism to
    avoid having a huge positional encoding matrix (when the sequence length is very
    big) by factorizing it into smaller matrices.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace traditional attention by LSH (local-sensitive hashing) attention (see
    below for more details). It’s a technique to avoid computing the full product
    query-key in the attention layers.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid storing the intermediate results of each layer by using reversible transformer
    layers to obtain them during the backward pass (subtracting the residuals from
    the input of the next layer gives them back) or recomputing them for results inside
    a given layer (less efficient than storing them but saves memory).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the feedforward operations by chunks and not on the whole batch.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axial Positional Encodings
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Axial Positional Encodings were first implemented in Google’s [trax library](https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29)
    and developed by the authors of this model’s paper. In models that are treating
    very long input sequences, the conventional position id encodings store an embedings
    vector of size<math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math>d
    being the `config.hidden_size` for every position<math><semantics><mrow><mi>i</mi><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">i, \ldots, n_s</annotation></semantics></math>i,…,ns​,
    with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being `config.max_embedding_size`.
    This means that having a sequence length of<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msup><mn>2</mn><mn>19</mn></msup><mo>≈</mo><mn>0.5</mn><mi>M</mi></mrow><annotation
    encoding="application/x-tex">n_s = 2^{19} \approx 0.5M</annotation></semantics></math>ns​=219≈0.5M
    and a `config.hidden_size` of<math><semantics><mrow><mi>d</mi><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup><mo>≈</mo><mn>1000</mn></mrow><annotation
    encoding="application/x-tex">d = 2^{10} \approx 1000</annotation></semantics></math>d=210≈1000
    would result in a position encoding matrix: <math display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>d</mi><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X_{i,j},
    \text{ with } i \in \left[1,\ldots, d\right] \text{ and } j \in \left[1,\ldots,
    n_s\right]</annotation></semantics></math>Xi,j​, with i∈[1,…,d] and j∈[1,…,ns​]'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 轴向位置编码首次在Google的trax库中实现，并由该模型论文的作者开发。在处理非常长的输入序列的模型中，传统的位置id编码会为每个位置i,…,ns​存储一个大小为d的嵌入向量，其中ns​为config.max_embedding_size。这意味着当序列长度为ns​=219≈0.5M，而config.hidden_size为d=210≈1000时，将得到一个位置编码矩阵：Xi,j​，其中i∈[1,…,d]，j∈[1,…,ns​]
- en: 'which alone has over 500M parameters to store. Axial positional encodings factorize<math><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{i,j}</annotation></semantics></math>Xi,j​
    into two matrices: <math display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>1</mn></msup><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{1}_{i,j},
    \text{ with } i \in \left[1,\ldots, d^1\right] \text{ and } j \in \left[1,\ldots,
    n_s^1\right]</annotation></semantics></math>Xi,j1​, with i∈[1,…,d1] and j∈[1,…,ns1​]'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中单独有超过500M个参数需要存储。轴向位置编码将Xi,j分解为两个矩阵：Xi,j1​，其中i∈[1,…,d1]，j∈[1,…,ns1​]
- en: and <math display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo><mtext> with </mtext><mi>i</mi><mo>∈</mo><mrow><mo
    fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    fence="true">]</mo></mrow><mtext> and </mtext><mi>j</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo
    fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{2}_{i,j},
    \text{ with } i \in \left[1,\ldots, d^2\right] \text{ and } j \in \left[1,\ldots,
    n_s^2\right]</annotation></semantics></math>Xi,j2​, with i∈[1,…,d2] and j∈[1,…,ns2​]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'with: <math display="block"><semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>d</mi><mn>1</mn></msup><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mtext> and </mtext><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mi
    mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">d =
    d^1 + d^2 \text{ and } n_s = n_s^1 \times n_s^2 .</annotation></semantics></math>d=d1+d2 and ns​=ns1​×ns2​.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore the following holds: <math display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable
    rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>k</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><mtext>if  </mtext><mi>i</mi><mo><</mo><msup><mi>d</mi><mn>1</mn></msup><mtext> with </mtext><mi>k</mi><mo>=</mo><mi>j</mi><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo>−</mo><msup><mi>d</mi><mn>1</mn></msup><mo
    separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle
    scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mo>≥</mo><msup><mi>d</mi><mn>1</mn></msup><mtext> with </mtext><mi>l</mi><mo>=</mo><mo
    stretchy="false">⌊</mo><mfrac><mi>j</mi><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mfrac><mo
    stretchy="false">⌋</mo></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex">X_{i,j} = \begin{cases} X^{1}_{i, k}, & \text{if
    }\ i < d^1 \text{ with } k = j \mod n_s^1 \\ X^{2}_{i - d^1, l}, & \text{if }
    i \ge d^1 \text{ with } l = \lfloor\frac{j}{n_s^1}\rfloor \end{cases}</annotation></semantics></math>Xi,j​={Xi,k1​,Xi−d1,l2​,​if  i<d1 with k=jmodns1​if i≥d1 with l=⌊ns1​j​⌋​'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this means that a position embedding vector<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>∈</mo><msup><mi
    mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_j
    \in \mathbb{R}^{d}</annotation></semantics></math>xj​∈Rd is now the composition
    of two factorized embedding vectors:<math><semantics><mrow><msubsup><mi>x</mi><mrow><mi>k</mi><mo
    separator="true">,</mo><mi>l</mi></mrow><mn>1</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mrow><mi>l</mi><mo
    separator="true">,</mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow><annotation
    encoding="application/x-tex">x^1_{k, l} + x^2_{l, k}</annotation></semantics></math>xk,l1​+xl,k2​,
    where as the `config.max_embedding_size` dimension<math><semantics><mrow><mi>j</mi></mrow><annotation
    encoding="application/x-tex">j</annotation></semantics></math>j is factorized
    into<math><semantics><mrow><mi>k</mi><mtext> and </mtext><mi>l</mi></mrow><annotation
    encoding="application/x-tex">k \text{ and } l</annotation></semantics></math>k and l.
    This design ensures that each position embedding vector<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">x_j</annotation></semantics></math>xj​ is unique.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，这意味着位置嵌入向量xj∈Rd现在是两个分解嵌入向量的组合：xk,l1+xl,k2，其中`config.max_embedding_size`维度j被分解为k和l。这种设计确保每个位置嵌入向量xj是唯一的。
- en: Using the above example again, axial position encoding with<math><semantics><mrow><msup><mi>d</mi><mn>1</mn></msup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup></mrow><annotation
    encoding="application/x-tex">d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}</annotation></semantics></math>d1=29,d2=29,ns1​=29,ns2​=210
    can drastically reduced the number of parameters from 500 000 000 to<math><semantics><mrow><msup><mn>2</mn><mn>18</mn></msup><mo>+</mo><msup><mn>2</mn><mn>19</mn></msup><mo>≈</mo><mn>780000</mn></mrow><annotation
    encoding="application/x-tex">2^{18} + 2^{19} \approx 780 000</annotation></semantics></math>218+219≈780000
    parameters, this means 85% less memory usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用上面的示例，轴向位置编码与d1=29，d2=29，ns1=29，ns2=210可以将参数数量从500,000,000大幅减少到218+219≈780,000个参数，这意味着内存使用减少了85%。
- en: In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple<math><semantics><mrow><mo
    stretchy="false">(</mo><msup><mi>d</mi><mn>1</mn></msup><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(d^1, d^2)</annotation></semantics></math>(d1,d2)
    which sum has to be equal to `config.hidden_size` and `config.axial_pos_shape`
    is set to a tuple<math><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">(n_s^1, n_s^2)</annotation></semantics></math>(ns1​,ns2​)
    which product has to be equal to `config.max_embedding_size`, which during training
    has to be equal to the *sequence length* of the `input_ids`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，参数`config.axial_pos_embds_dim`设置为一个元组(d1, d2)，其总和必须等于`config.hidden_size`，而`config.axial_pos_shape`设置为一个元组(ns1,
    ns2)，其乘积必须等于`config.max_embedding_size`，在训练期间必须等于`input_ids`的*序列长度*。
- en: LSH Self Attention
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSH自注意力
- en: In Locality sensitive hashing (LSH) self attention the key and query projection
    weights are tied. Therefore, the key query embedding vectors are also tied. LSH
    self attention uses the locality sensitive hashing mechanism proposed in [Practical
    and Optimal LSH for Angular Distance](https://arxiv.org/abs/1509.02897) to assign
    each of the tied key query embedding vectors to one of `config.num_buckets` possible
    buckets. The premise is that the more “similar” key query embedding vectors (in
    terms of *cosine similarity*) are to each other, the more likely they are assigned
    to the same bucket.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在局部敏感哈希（LSH）自注意力中，键和查询投影权重是绑定的。因此，键查询嵌入向量也是绑定的。LSH自注意力使用了[角距离的实用和最优LSH](https://arxiv.org/abs/1509.02897)中提出的局部敏感哈希机制，将这些绑定的键查询嵌入向量分配给`config.num_buckets`可能的桶之一。前提是，键查询嵌入向量（按*余弦相似度*）越“相似”，它们被分配到同一个桶的可能性就越大。
- en: The accuracy of the LSH mechanism can be improved by increasing `config.num_hashes`
    or directly the argument `num_hashes` of the forward function so that the output
    of the LSH self attention better approximates the output of the “normal” full
    self attention. The buckets are then sorted and chunked into query key embedding
    vector chunks each of length `config.lsh_chunk_length`. For each chunk, the query
    embedding vectors attend to its key vectors (which are tied to themselves) and
    to the key embedding vectors of `config.lsh_num_chunks_before` previous neighboring
    chunks and `config.lsh_num_chunks_after` following neighboring chunks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: For more information, see the [original Paper](https://arxiv.org/abs/2001.04451)
    or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Note that `config.num_buckets` can also be factorized into a list<math><semantics><mrow><mo
    stretchy="false">(</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo
    separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_{\text{buckets}}^1,
    n_{\text{buckets}}^2)</annotation></semantics></math>(nbuckets1​,nbuckets2​).
    This way instead of assigning the query key embedding vectors to one of<math><semantics><mrow><mo
    stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>n</mi><mtext>buckets</mtext></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,\ldots,
    n_{\text{buckets}})</annotation></semantics></math>(1,…,nbuckets​) they are assigned
    to one of<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><mn>1</mn><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>−</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-1,\ldots,
    n_{\text{buckets}}^1-1, \ldots, 1-n_{\text{buckets}}^2, \ldots, n_{\text{buckets}}^1-n_{\text{buckets}}^2)</annotation></semantics></math>(1−1,…,nbuckets1​−1,…,1−nbuckets2​,…,nbuckets1​−nbuckets2​).
    This is crucial for very long sequences to save memory.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: When training a model from scratch, it is recommended to leave `config.num_buckets=None`,
    so that depending on the sequence length a good value for `num_buckets` is calculated
    on the fly. This value will then automatically be saved in the config and should
    be reused for inference.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Using LSH self attention, the memory and time complexity of the query-key matmul
    operation can be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)), which usually
    represents the memory and time bottleneck in a transformer model, with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Local Self Attention
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local self attention is essentially a “normal” self attention layer with key,
    query and value projections, but is chunked so that in each chunk of length `config.local_chunk_length`
    the query embedding vectors only attends to the key embedding vectors in its chunk
    and to the key embedding vectors of `config.local_num_chunks_before` previous
    neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Using Local self attention, the memory and time complexity of the query-key
    matmul operation can be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times \log(n_s))</annotation></semantics></math>O(ns​×log(ns​)), which usually
    represents the memory and time bottleneck in a transformer model, with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During training, we must ensure that the sequence length is set to a value that
    can be divided by the least common multiple of `config.lsh_chunk_length` and `config.local_chunk_length`
    and that the parameters of the Axial Positional Encodings are correctly set as
    described above. Reformer is very memory efficient so that the model can easily
    be trained on sequences as long as 64000 tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'For training, the [ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)
    should be used as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReformerConfig
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerConfig`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/configuration_reformer.py#L32)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_head_size` (`int`, *optional*, defaults to 64) — Dimensionality
    of the projected key, query and value vectors'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_layers` (`List[str]`, *optional*, defaults to `["local", "lsh", "local",
    "lsh", "local", "lsh"]`) — List of attention layer types in ascending order. It
    can be chosen between a LSHSelfAttention layer (`"lsh"`) and a LocalSelfAttention
    layer (`"local"`).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on LSHSelfAttention layer, see [LSH Self Attention](reformer#lsh-self-attention).
    For more information on LocalSelfAttention layer, see [Local Self Attention](reformer#local-self-attention).
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`axial_pos_embds` (`bool`, *optional*, defaults to `True`) — Whether or not
    to use axial position embeddings. For more information on how axial position embeddings
    work, see [Axial Position Encodings](reformer#axial-positional-encodings).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`axial_norm_std` (`float`, *optional*, defaults to 1.0) — The standard deviation
    of the normal_initializer for initializing the weight matrices of the axial positional
    encodings.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`axial_pos_shape` (`List[int]`, *optional*, defaults to `[64, 64]`) — The position
    dims of the axial position encodings. During training, the product of the position
    dims has to be equal to the sequence length.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how axial position embeddings work, see [Axial Position
    Encodings](reformer#axial-positional-encodings).
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`axial_pos_embds_dim` (`List[int]`, *optional*, defaults to `[64, 192]`) —
    The embedding dims of the axial position encodings. The sum of the embedding dims
    has to be equal to the hidden size.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how axial position embeddings work, see [Axial Position
    Encodings](reformer#axial-positional-encodings).
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`chunk_size_lm_head` (`int`, *optional*, defaults to 0) — The chunk size of
    the final language model feed forward head layer. A chunk size of 0 means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward
    layer processes n < sequence_length embeddings at a time.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on feed forward chunking, see [How does Feed Forward Chunking
    work?](../glossary#feed-forward-chunking).
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — The token id for the end-of-sentence
    token.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feed_forward_size` (`int`, *optional*, defaults to 512) — Dimensionality of
    the feed_forward layer in the residual attention block.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hash_seed` (`int`, *optional*) — Seed that can be used to make local sensitive
    hashing in `LSHSelfAttention` deterministic. This should only be set for testing
    purposed. For evaluation and training purposes `hash_seed` should be left as `None`
    to ensure fully random rotations in local sensitive hashing scheme.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the feed forward layer
    in the residual attention block. If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"`
    are supported.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.05) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 256) — Dimensionality of the
    output hidden states of the residual attention blocks.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_decoder` (`bool`, *optional*, defaults to `False`) — Whether or not to
    use a causal mask in addition to the `attention_mask` passed to [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).
    When using the Reformer for causal language modeling, this argument should be
    set to `True`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_chunk_length` (`int`, *optional*, defaults to 64) — Length of chunk
    which attends to itself in `LocalSelfAttention`. Chunking reduces memory complexity
    from sequence length x sequence length (self attention) to chunk length x chunk
    length x sequence length / chunk length (chunked self attention).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_num_chunks_before` (`int`, *optional*, defaults to 1) — Number of previous
    neighbouring chunks to attend to in `LocalSelfAttention` layer to itself.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_num_chunks_after` (`int`, *optional*, defaults to 0) — Number of following
    neighbouring chunks to attend to in `LocalSelfAttention` layer in addition to
    itself.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1)
    — The dropout ratio for the attention probabilities in `LocalSelfAttention`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lsh_attn_chunk_length` (`int`, *optional*, defaults to 64) — Length of chunk
    which attends to itself in `LSHSelfAttention`. Chunking reduces memory complexity
    from sequence length x sequence length (self attention) to chunk length x chunk
    length x sequence length / chunk length (chunked self attention).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lsh_num_chunks_before` (`int`, *optional*, defaults to 1) — Number of previous
    neighbouring chunks to attend to in `LSHSelfAttention` layer to itself.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lsh_num_chunks_after` (`int`, *optional*, defaults to 0) — Number of following
    neighbouring chunks to attend to in `LSHSelfAttention` layer to itself.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lsh_attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) —
    The dropout ratio for the attention probabilities in `LSHSelfAttention`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 4096) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_buckets` (`int` or `List[int]`, *optional*) — Number of buckets, the key
    query vectors can be “hashed into” using the locality sensitive hashing scheme.
    Each query key vector is hashed into a hash in `1, ..., num_buckets`. The number
    of buckets can also be factorized into a list for improved memory complexity.
    In this case, each query key vector is hashed into a hash in `1-1, 1-2, ..., num_buckets[0]-1,
    ..., num_buckets[0]-num_buckets[1]` if `num_buckets` is factorized into two factors.
    The number of buckets (or the product the factors) should approximately equal
    sequence length / lsh_chunk_length. If `num_buckets` not set, a good value is
    calculated on the fly.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hashes` (`int`, *optional*, defaults to 1) — Number of hashing rounds
    (e.g., number of random rotations) in Local Sensitive Hashing scheme. The higher
    `num_hashes`, the more accurate the `LSHSelfAttention` becomes, but also the more
    memory and time intensive the hashing becomes.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The token id for the padding
    token.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 320) —\ Vocabulary size of the
    Reformer model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether to
    tie input and output embeddings.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`classifier_dropout` (`float`, *optional*) — The dropout ratio for the classification
    head.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel).
    It is used to instantiate a Reformer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ReFormer [google/reformer-crime-and-punishment](https://huggingface.co/google/reformer-crime-and-punishment)
    architecture.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ReformerTokenizer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerTokenizer`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L48)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `[]`) — Additional
    special tokens used by the tokenizer.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a Reformer tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece)
    .
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer.py#L171)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ReformerTokenizerFast
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerTokenizerFast`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/tokenization_reformer_fast.py#L57)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`additional_special_tokens` (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a “fast” Reformer tokenizer (backed by HuggingFace’s *tokenizers*
    library). Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: ReformerModel
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerModel`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L1972)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bare Reformer Model transformer outputting raw hidden-stateswithout any
    specific head on top. Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2004)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.reformer.modeling_reformer.ReformerModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.reformer.modeling_reformer.ReformerModelOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_predict,
    hidden_size)`) — Sequence of hidden-states at the last layer of the model.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping`
    is `None`, then `num_predict` corresponds to `sequence_length`.'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`)
    — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states`
    input) to speed up sequential decoding.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings and one for the output of each layer) of
    shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerModel](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ReformerModelWithLMHead
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerModelWithLMHead`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2187)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reformer Model with a `language modeling` head on top. Reformer was proposed
    in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by
    Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2215)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerModelWithLMHead](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead)
    forward method, overrides the `__call__` special method.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ReformerForMaskedLM
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerForMaskedLM`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2313)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reformer Model with a `language modeling` head on top. Reformer was proposed
    in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by
    Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2335)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a false checkpoint since we don’t have any available pretrained
    model for the masked language modeling task with the Reformer architecture.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ReformerForSequenceClassification
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerForSequenceClassification`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2437)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reformer Model transformer with a sequence classification/regression head on
    top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2458)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Example of single-label classification:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ReformerForQuestionAnswering
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ReformerForQuestionAnswering`'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2584)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reformer Model with a span classification head on top for extractive question-answering
    tasks like SQuAD / TriviaQA ( a linear layer on top of hidden-states output to
    compute `span start logits` and `span end logits`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/reformer/modeling_reformer.py#L2603)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. During training the input_ids
    sequence_length has to be a multiple of the relevant model’s chunk lengths (lsh’s,
    local’s or both). During evaluation, the indices are automatically padded to be
    a multiple of the chunk length.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hashes` (`int`, *optional*) — The number of hashing rounds that should
    be performed during bucketing. Setting this argument overwrites the default defined
    in `config.num_hashes`.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, see `num_hashes` in [ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig).
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_buckets_states` (`List[Tuple(torch.LongTensor, torch.FloatTensor)]`,
    *optional*) — List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`,
    with the first element being the previous *buckets* of shape `(batch_size, num_heads,
    num_hashes, sequence_length)`) and the second being the previous *hidden_states*
    of shape `(batch_size, sequence_length, hidden_size)`).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention).
    Can be used to speed up sequential decoding.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ReformerConfig](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerConfig))
    and inputs.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ReformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/reformer#transformers.ReformerForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
