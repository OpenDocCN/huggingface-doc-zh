- en: Performing inference with LCM-LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LCM-LoRA进行推理
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora](https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora](https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Latent Consistency Models (LCM) enable quality image generation in typically
    2-4 steps making it possible to use diffusion models in almost real-time settings.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在一致性模型（LCM）使得在通常2-4步内生成高质量图像成为可能，从而可以在几乎实时的设置中使用扩散模型。
- en: 'From the [official website](https://latent-consistency-models.github.io/):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从[官方网站](https://latent-consistency-models.github.io/)：
- en: LCMs can be distilled from any pre-trained Stable Diffusion (SD) in only 4,000
    training steps (~32 A100 GPU Hours) for generating high quality 768 x 768 resolution
    images in 2~4 steps or even one step, significantly accelerating text-to-image
    generation. We employ LCM to distill the Dreamshaper-V7 version of SD in just
    4,000 training iterations.
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LCMs可以从任何预训练的稳定扩散（SD）中提炼出来，仅需4,000个训练步骤（~32 A100 GPU小时）即可生成高质量的768 x 768分辨率图像，仅需2~4步甚至一步，显著加速文本到图像的生成。我们使用LCM来提炼Dreamshaper-V7版本的SD，仅需4,000个训练迭代。
- en: For a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有关LCMs的更多技术概述，请参阅[论文](https://huggingface.co/papers/2310.04378)。
- en: However, each model needs to be distilled separately for latent consistency
    distillation. The core idea with LCM-LoRA is to train just a few adapter layers,
    the adapter being LoRA in this case. This way, we don’t have to train the full
    model and keep the number of trainable parameters manageable. The resulting LoRAs
    can then be applied to any fine-tuned version of the model without distilling
    them separately. Additionally, the LoRAs can be applied to image-to-image, ControlNet/T2I-Adapter,
    inpainting, AnimateDiff etc. The LCM-LoRA can also be combined with other LoRAs
    to generate styled images in very few steps (4-8).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个模型需要单独进行潜在一致性提炼。LCM-LoRA的核心思想是仅训练几个适配器层，适配器在这种情况下是LoRA。这样，我们就不必训练完整的模型，并保持可管理的可训练参数数量。然后，生成的LoRAs可以应用于任何经过微调的模型版本，而无需单独提炼它们。此外，LoRAs可以应用于图像到图像、ControlNet/T2I-Adapter、修补、AnimateDiff等。LCM-LoRA还可以与其他LoRAs结合，以在很少的步骤（4-8）中生成样式化图像。
- en: LCM-LoRAs are available for [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5),
    [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0),
    and the [SSD-1B](https://huggingface.co/segmind/SSD-1B) model. All the checkpoints
    can be found in this [collection](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LCM-LoRAs可用于[stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)、[stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)和[SSD-1B](https://huggingface.co/segmind/SSD-1B)模型。所有检查点都可以在此[集合](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6)中找到。
- en: For more details about LCM-LoRA, refer to [the technical report](https://huggingface.co/papers/2311.05556).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有关LCM-LoRA的更多详细信息，请参阅[技术报告](https://huggingface.co/papers/2311.05556)。
- en: This guide shows how to perform inference with LCM-LoRAs for
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南展示了如何使用LCM-LoRAs进行推理
- en: text-to-image
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像
- en: image-to-image
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像到图像
- en: combined with styled LoRAs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与样式LoRAs结合
- en: ControlNet/T2I-Adapter
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ControlNet/T2I-Adapter
- en: inpainting
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修补
- en: AnimateDiff
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AnimateDiff
- en: Before going through this guide, we’ll take a look at the general workflow for
    performing inference with LCM-LoRAs. LCM-LoRAs are similar to other Stable Diffusion
    LoRAs so they can be used with any [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    that supports LoRAs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本指南之前，我们将看一下使用LCM-LoRAs进行推理的一般工作流程。LCM-LoRAs类似于其他稳定扩散LoRAs，因此它们可以与支持LoRAs的任何[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)一起使用。
- en: Load the task specific pipeline and model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载特定任务的流水线和模型。
- en: Set the scheduler to [LCMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lcm#diffusers.LCMScheduler).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将调度程序设置为[LCMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lcm#diffusers.LCMScheduler)。
- en: Load the LCM-LoRA weights for the model.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载模型的LCM-LoRA权重。
- en: Reduce the `guidance_scale` between `[1.0, 2.0]` and set the `num_inference_steps`
    between [4, 8].
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`guidance_scale`减小到`[1.0, 2.0]`之间，并将`num_inference_steps`设置在[4, 8]之间。
- en: Perform inference with the pipeline with the usual parameters.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用常规参数对流水线进行推理。
- en: Let’s look at how we can perform inference with LCM-LoRAs for different tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在不同任务中使用LCM-LoRAs进行推理。
- en: First, make sure you have [peft](https://github.com/huggingface/peft) installed,
    for better LoRA support.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保您已安装[peft](https://github.com/huggingface/peft)，以获得更好的LoRA支持。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Text-to-image
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像
- en: 'You’ll use the [StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)
    with the scheduler: [LCMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lcm#diffusers.LCMScheduler)
    and then load the LCM-LoRA. Together with the LCM-LoRA and the scheduler, the
    pipeline enables a fast inference workflow overcoming the slow iterative nature
    of diffusion models.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用[StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)与调度程序：[LCMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lcm#diffusers.LCMScheduler)，然后加载LCM-LoRA。与LCM-LoRA和调度程序一起，该流水线实现了快速推理工作流，克服了扩散模型缓慢的迭代性质。
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/2ca06b97873717e791e8cb391d65c97e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ca06b97873717e791e8cb391d65c97e.png)'
- en: Notice that we use only 4 steps for generation which is way less than what’s
    typically used for standard SDXL.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们只使用4个生成步骤，这比标准SDXL通常使用的步骤要少得多。
- en: You may have noticed that we set `guidance_scale=1.0`, which disables classifer-free-guidance.
    This is because the LCM-LoRA is trained with guidance, so the batch size does
    not have to be doubled in this case. This leads to a faster inference time, with
    the drawback that negative prompts don’t have any effect on the denoising process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们设置了`guidance_scale=1.0`，这会禁用无分类器引导。这是因为LCM-LoRA是在有引导的情况下训练的，所以在这种情况下批量大小不必加倍。这会导致更快的推理时间，但负面提示对去噪过程没有任何影响。
- en: You can also use guidance with LCM-LoRA, but due to the nature of training the
    model is very sensitve to the `guidance_scale` values, high values can lead to
    artifacts in the generated images. In our experiments, we found that the best
    values are in the range of [1.0, 2.0].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在LCM-LoRA中使用引导，但由于训练模型的性质对`guidance_scale`值非常敏感，高值可能会导致生成图像中的伪影。在我们的实验中，我们发现最佳值在[1.0,
    2.0]范围内。
- en: Inference with a fine-tuned model
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用经过微调的模型进行推理
- en: As mentioned above, the LCM-LoRA can be applied to any fine-tuned version of
    the model without having to distill them separately. Let’s look at how we can
    perform inference with a fine-tuned model. In this example, we’ll use the [animagine-xl](https://huggingface.co/Linaqruf/animagine-xl)
    model, which is a fine-tuned version of the SDXL model for generating anime.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，LCM-LoRA可以应用于任何经过微调的模型版本，而无需单独提取它们。让我们看看如何使用经过微调的模型进行推理。在这个例子中，我们将使用[animagine-xl](https://huggingface.co/Linaqruf/animagine-xl)模型，这是用于生成动漫的SDXL模型的经过微调版本。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/7ca0f83858b96602e6d6532ddecb9a22.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ca0f83858b96602e6d6532ddecb9a22.png)'
- en: Image-to-image
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像到图像
- en: LCM-LoRA can be applied to image-to-image tasks too. Let’s look at how we can
    perform image-to-image generation with LCMs. For this example we’ll use the [dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7)
    model and the LCM-LoRA for `stable-diffusion-v1-5` .
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LCM-LoRA也可以应用于图像到图像的任务。让我们看看如何使用LCM进行图像到图像的生成。在这个例子中，我们将使用[dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7)模型和LCM-LoRA进行`stable-diffusion-v1-5`。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/57491291a2825c214ac6c8fda2b34cfe.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57491291a2825c214ac6c8fda2b34cfe.png)'
- en: You can get different results based on your prompt and the image you provide.
    To get the best results, we recommend trying different values for `num_inference_steps`,
    `strength`, and `guidance_scale` parameters and choose the best one.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的提示和提供的图像，您可以获得不同的结果。为了获得最佳结果，我们建议尝试不同的值来选择`num_inference_steps`、`strength`和`guidance_scale`参数中的最佳值。
- en: Combine with styled LoRAs
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与风格化LoRA结合
- en: LCM-LoRA can be combined with other LoRAs to generate styled-images in very
    few steps (4-8). In the following example, we’ll use the LCM-LoRA with the [papercut
    LoRA](TheLastBen/Papercut_SDXL). To learn more about how to combine LoRAs, refer
    to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LCM-LoRA可以与其他LoRA结合，以在很少的步骤（4-8）中生成风格化图像。在下面的例子中，我们将使用LCM-LoRA与[papercut LoRA](TheLastBen/Papercut_SDXL)。要了解如何组合LoRA，请参考[此指南](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters)。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/13ecbe2ca5401bbe4f7c504d04cc6b95.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13ecbe2ca5401bbe4f7c504d04cc6b95.png)'
- en: ControlNet/T2I-Adapter
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ControlNet/T2I-Adapter
- en: Let’s look at how we can perform inference with ControlNet/T2I-Adapter and LCM-LoRA.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用ControlNet/T2I-Adapter和LCM-LoRA进行推理。
- en: ControlNet
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ControlNet
- en: For this example, we’ll use the SD-v1-5 model and the LCM-LoRA for SD-v1-5 with
    canny ControlNet.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用SD-v1-5模型和带有canny ControlNet的SD-v1-5的LCM-LoRA。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/e92fb093ce9ab261a6081a45cf6ba97c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e92fb093ce9ab261a6081a45cf6ba97c.png)'
- en: The inference parameters in this example might not work for all examples, so
    we recommend you to try different values for `num_inference_steps`, `guidance_scale`,
    `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose
    the best one.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中的推理参数可能不适用于所有例子，所以我们建议您尝试不同的值来选择`num_inference_steps`、`guidance_scale`、`controlnet_conditioning_scale`和`cross_attention_kwargs`参数中的最佳值。
- en: T2I-Adapter
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: T2I-Adapter
- en: This example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0)
    and SDXL.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了如何使用LCM-LoRA与[Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0)和SDXL。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/2ee6c4de3add72aae6b96e5673406b21.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ee6c4de3add72aae6b96e5673406b21.png)'
- en: Inpainting
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修补
- en: LCM-LoRA can be used for inpainting as well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LCM-LoRA也可以用于修补。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/b5493832202c5c129c84462826847ba1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5493832202c5c129c84462826847ba1.png)'
- en: AnimateDiff
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AnimateDiff
- en: '`AnimateDiff` allows you to animate images using Stable Diffusion models. To
    get good results, we need to generate multiple frames (16-24), and doing this
    with standard SD models can be very slow. LCM-LoRA can be used to speed up the
    process significantly, as you just need to do 4-8 steps for each frame. Let’s
    look at how we can perform animation with LCM-LoRA and AnimateDiff.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`AnimateDiff`允许您使用稳定扩散模型对图像进行动画处理。为了获得良好的结果，我们需要生成多个帧（16-24），使用标准SD模型可能会非常慢。LCM-LoRA可以显著加快这个过程，因为每个帧只需要进行4-8步。让我们看看如何使用LCM-LoRA和AnimateDiff进行动画处理。'
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/43e783334497532daab457d32f83c51a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43e783334497532daab457d32f83c51a.png)'
