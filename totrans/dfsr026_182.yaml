- en: WÃ¼rstchen
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¦™è‚ 
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen](https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen](https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e29b341d4d756c0a6cf4d1a787243bfc.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e29b341d4d756c0a6cf4d1a787243bfc.png)'
- en: '[Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion
    Models](https://huggingface.co/papers/2306.00637) is by Pablo Pernias, Dominic
    Rampas, Mats L. Richter and Christopher Pal and Marc Aubreville.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wuerstchenï¼šå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¶æ„](https://huggingface.co/papers/2306.00637) ç”±Pablo
    Perniasã€Dominic Rampasã€Mats L. Richterã€Christopher Palå’ŒMarc Aubrevilleæ’°å†™ã€‚'
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We introduce WÃ¼rstchen, a novel architecture for text-to-image synthesis that
    combines competitive performance with unprecedented cost-effectiveness for large-scale
    text-to-image diffusion models. A key contribution of our work is to develop a
    latent diffusion technique in which we learn a detailed but extremely compact
    semantic image representation used to guide the diffusion process. This highly
    compressed representation of an image provides much more detailed guidance compared
    to latent representations of language and this significantly reduces the computational
    requirements to achieve state-of-the-art results. Our approach also improves the
    quality of text-conditioned image generation based on our user preference study.
    The training requirements of our approach consists of 24,602 A100-GPU hours -
    compared to Stable Diffusion 2.1â€™s 200,000 GPU hours. Our approach also requires
    less training data to achieve these results. Furthermore, our compact latent representations
    allows us to perform inference over twice as fast, slashing the usual costs and
    carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without
    compromising the end performance. In a broader comparison against SOTA models
    our approach is substantially more efficient and compares favorably in terms of
    image quality. We believe that this work motivates more emphasis on the prioritization
    of both performance and computational accessibility.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬ä»‹ç»äº†é¦™è‚ ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ¶æ„ï¼Œç»“åˆäº†ç«äº‰æ€§èƒ½å’Œå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‰æ‰€æœªæœ‰çš„æˆæœ¬æ•ˆç›Šã€‚æˆ‘ä»¬å·¥ä½œçš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯å¼€å‘ä¸€ç§æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œé€šè¿‡è¿™ç§æŠ€æœ¯æˆ‘ä»¬å­¦ä¹ äº†ä¸€ç§è¯¦ç»†ä½†æå…¶ç´§å‡‘çš„è¯­ä¹‰å›¾åƒè¡¨ç¤ºï¼Œç”¨äºå¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚ä¸è¯­è¨€çš„æ½œåœ¨è¡¨ç¤ºç›¸æ¯”ï¼Œè¿™ç§é«˜åº¦å‹ç¼©çš„å›¾åƒè¡¨ç¤ºæä¾›äº†æ›´è¯¦ç»†çš„å¼•å¯¼ï¼Œè¿™æ˜¾è‘—é™ä½äº†å®ç°æœ€å…ˆè¿›ç»“æœæ‰€éœ€çš„è®¡ç®—è¦æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ ¹æ®ç”¨æˆ·åå¥½ç ”ç©¶æ”¹è¿›äº†åŸºäºæ–‡æœ¬æ¡ä»¶çš„å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„è®­ç»ƒéœ€æ±‚ä¸º24,602ä¸ªA100-GPUå°æ—¶
    - ç›¸æ¯”äºStable Diffusion 2.1çš„200,000ä¸ªGPUå°æ—¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜éœ€è¦æ›´å°‘çš„è®­ç»ƒæ•°æ®æ¥å®ç°è¿™äº›ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œä¸¤å€é€Ÿåº¦çš„æ¨æ–­ï¼Œå¤§å¹…å‰Šå‡äº†ä¸€ç§æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æ‰©æ•£æ¨¡å‹çš„é€šå¸¸æˆæœ¬å’Œç¢³è¶³è¿¹ï¼Œè€Œä¸ä¼šå½±å“æœ€ç»ˆæ€§èƒ½ã€‚åœ¨ä¸SOTAæ¨¡å‹çš„æ›´å¹¿æ³›æ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡ä¸Šæ›´ä¸ºæ˜¾è‘—ï¼Œå¹¶åœ¨å›¾åƒè´¨é‡æ–¹é¢å…·æœ‰å¯æ¯”æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¿ƒä½¿æ›´å¤šå…³æ³¨æ€§èƒ½å’Œè®¡ç®—å¯è®¿é—®æ€§çš„ä¼˜å…ˆè€ƒè™‘ã€‚*'
- en: WÃ¼rstchen Overview
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¦™è‚ æ¦‚è¿°
- en: WÃ¼rstchen is a diffusion model, whose text-conditional model works in a highly
    compressed latent space of images. Why is this important? Compressing data can
    reduce computational costs for both training and inference by magnitudes. Training
    on 1024x1024 images is way more expensive than training on 32x32\. Usually, other
    works make use of a relatively small compression, in the range of 4x - 8x spatial
    compression. WÃ¼rstchen takes this to an extreme. Through its novel design, we
    achieve a 42x spatial compression. This was unseen before because common methods
    fail to faithfully reconstruct detailed images after 16x spatial compression.
    WÃ¼rstchen employs a two-stage compression, what we call Stage A and Stage B. Stage
    A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found
    in the [paper](https://huggingface.co/papers/2306.00637)). A third model, Stage
    C, is learned in that highly compressed latent space. This training requires fractions
    of the compute used for current top-performing models, while also allowing cheaper
    and faster inference.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é¦™è‚ æ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œå…¶æ–‡æœ¬æ¡ä»¶æ¨¡å‹åœ¨å›¾åƒçš„é«˜åº¦å‹ç¼©æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œã€‚ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿå‹ç¼©æ•°æ®å¯ä»¥å°†è®­ç»ƒå’Œæ¨æ–­çš„è®¡ç®—æˆæœ¬é™ä½æ•°å€ã€‚åœ¨1024x1024å›¾åƒä¸Šè®­ç»ƒæ¯”åœ¨32x32å›¾åƒä¸Šè®­ç»ƒè¦æ˜‚è´µå¾—å¤šã€‚é€šå¸¸ï¼Œå…¶ä»–ä½œå“ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„å‹ç¼©ï¼ŒèŒƒå›´åœ¨4å€è‡³8å€çš„ç©ºé—´å‹ç¼©ã€‚é¦™è‚ å°†è¿™ä¸€ç‚¹æ¨å‘äº†æç«¯ã€‚é€šè¿‡å…¶æ–°é¢–çš„è®¾è®¡ï¼Œæˆ‘ä»¬å®ç°äº†42å€çš„ç©ºé—´å‹ç¼©ã€‚è¿™ä¹‹å‰æ˜¯çœ‹ä¸åˆ°çš„ï¼Œå› ä¸ºå¸¸è§æ–¹æ³•åœ¨16å€ç©ºé—´å‹ç¼©åæ— æ³•å¿ å®åœ°é‡å»ºè¯¦ç»†å›¾åƒã€‚é¦™è‚ é‡‡ç”¨äº†ä¸¤é˜¶æ®µå‹ç¼©ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºAé˜¶æ®µå’ŒBé˜¶æ®µã€‚Aé˜¶æ®µæ˜¯VQGANï¼ŒBé˜¶æ®µæ˜¯æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ˆæ›´å¤šç»†èŠ‚å¯ä»¥åœ¨[è®ºæ–‡](https://huggingface.co/papers/2306.00637)ä¸­æ‰¾åˆ°ï¼‰ã€‚ç¬¬ä¸‰ä¸ªæ¨¡å‹ï¼ŒCé˜¶æ®µï¼Œåœ¨è¿™ä¸ªé«˜åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ ã€‚è¿™ç§è®­ç»ƒåªéœ€è¦å½“å‰æ€§èƒ½æœ€ä½³æ¨¡å‹ä½¿ç”¨çš„è®¡ç®—çš„ä¸€å°éƒ¨åˆ†ï¼ŒåŒæ—¶è¿˜å¯ä»¥å®ç°æ›´ä¾¿å®œå’Œæ›´å¿«çš„æ¨æ–­ã€‚
- en: WÃ¼rstchen v2 comes to Diffusers
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¦™è‚ v2æ¥åˆ°Diffusers
- en: After the initial paper release, we have improved numerous things in the architecture,
    training and sampling, making WÃ¼rstchen competitive to current state-of-the-art
    models in many ways. We are excited to release this new version together with
    Diffusers. Here is a list of the improvements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åˆçš„è®ºæ–‡å‘å¸ƒåï¼Œæˆ‘ä»¬åœ¨æ¶æ„ã€è®­ç»ƒå’Œé‡‡æ ·æ–¹é¢æ”¹è¿›äº†è®¸å¤šå†…å®¹ï¼Œä½¿é¦™è‚ åœ¨è®¸å¤šæ–¹é¢ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç«äº‰åŠ›åè¶³ã€‚æˆ‘ä»¬å¾ˆé«˜å…´ä¸Diffusersä¸€èµ·å‘å¸ƒè¿™ä¸ªæ–°ç‰ˆæœ¬ã€‚ä»¥ä¸‹æ˜¯æ”¹è¿›åˆ—è¡¨ã€‚
- en: Higher resolution (1024x1024 up to 2048x2048)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´é«˜åˆ†è¾¨ç‡ï¼ˆä»1024x1024åˆ°2048x2048ï¼‰
- en: Faster inference
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å¿«çš„æ¨æ–­
- en: Multi Aspect Resolution Sampling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ–¹é¢åˆ†è¾¨ç‡é‡‡æ ·
- en: Better quality
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„è´¨é‡
- en: 'We are releasing 3 checkpoints for the text-conditional image generation model
    (Stage C). Those are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘å¸ƒäº†æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆCé˜¶æ®µï¼‰çš„3ä¸ªæ£€æŸ¥ç‚¹ã€‚å®ƒä»¬æ˜¯ï¼š
- en: v2-base
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: v2-åŸºç¡€
- en: v2-aesthetic
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: v2-ç¾å­¦
- en: '**(default)** v2-interpolated (50% interpolation between v2-base and v2-aesthetic)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ï¼ˆé»˜è®¤ï¼‰v2-æ’å€¼ï¼ˆåœ¨v2-åŸºç¡€å’Œv2-ç¾å­¦ä¹‹é—´è¿›è¡Œ50%çš„æ’å€¼ï¼‰
- en: 'We recommend using v2-interpolated, as it has a nice touch of both photorealism
    and aesthetics. Use v2-base for finetunings as it does not have a style bias and
    use v2-aesthetic for very artistic generations. A comparison can be seen here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®ä½¿ç”¨v2-æ’å€¼ï¼Œå› ä¸ºå®ƒæ—¢å…·æœ‰ç…§ç‰‡é€¼çœŸæ„Ÿåˆå…·æœ‰ç¾å­¦æ„Ÿã€‚å¯¹äºå¾®è°ƒï¼Œè¯·ä½¿ç”¨v2-åŸºç¡€ï¼Œå› ä¸ºå®ƒæ²¡æœ‰é£æ ¼åè§ï¼Œå¯¹äºéå¸¸è‰ºæœ¯çš„ç”Ÿæˆï¼Œè¯·ä½¿ç”¨v2-ç¾å­¦ã€‚å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ¯”è¾ƒï¼š
- en: '![](../Images/b06dcd2bc467532747508fc8c277921a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b06dcd2bc467532747508fc8c277921a.png)'
- en: Text-to-Image Generation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
- en: 'For the sake of usability, WÃ¼rstchen can be used with a single pipeline. This
    pipeline can be used as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜å¯ç”¨æ€§ï¼ŒWÃ¼rstchen å¯ä»¥ä¸å•ä¸ªç®¡é“ä¸€èµ·ä½¿ç”¨ã€‚å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼ä½¿ç”¨æ­¤ç®¡é“ï¼š
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For explanation purposes, we can also initialize the two main pipelines of
    WÃ¼rstchen individually. WÃ¼rstchen consists of 3 stages: Stage C, Stage B, Stage
    A. They all have different jobs and work only together. When generating text-conditional
    images, Stage C will first generate the latents in a very compressed latent space.
    This is what happens in the `prior_pipeline`. Afterwards, the generated latents
    will be passed to Stage B, which decompresses the latents into a bigger latent
    space of a VQGAN. These latents can then be decoded by Stage A, which is a VQGAN,
    into the pixel-space. Stage B & Stage A are both encapsulated in the `decoder_pipeline`.
    For more details, take a look at the [paper](https://huggingface.co/papers/2306.00637).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£é‡Šç›®çš„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å•ç‹¬åˆå§‹åŒ– WÃ¼rstchen çš„ä¸¤ä¸ªä¸»è¦ç®¡é“ã€‚WÃ¼rstchen ç”± 3 ä¸ªé˜¶æ®µç»„æˆï¼šé˜¶æ®µ Cã€é˜¶æ®µ Bã€é˜¶æ®µ Aã€‚å®ƒä»¬éƒ½æœ‰ä¸åŒçš„å·¥ä½œï¼Œå¹¶ä¸”åªèƒ½ä¸€èµ·å·¥ä½œã€‚åœ¨ç”Ÿæˆæ–‡æœ¬æ¡ä»¶å›¾åƒæ—¶ï¼Œé˜¶æ®µ
    C å°†é¦–å…ˆåœ¨éå¸¸å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆæ½œåœ¨å˜é‡ã€‚è¿™å°±æ˜¯åœ¨ `prior_pipeline` ä¸­å‘ç”Ÿçš„äº‹æƒ…ã€‚ç„¶åï¼Œç”Ÿæˆçš„æ½œåœ¨å˜é‡å°†ä¼ é€’ç»™é˜¶æ®µ Bï¼Œåè€…å°†æ½œåœ¨å˜é‡è§£å‹ç¼©ä¸º
    VQGAN çš„æ›´å¤§æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œè¿™äº›æ½œåœ¨å˜é‡å¯ä»¥ç”±é˜¶æ®µ A è§£ç ä¸ºåƒç´ ç©ºé—´ã€‚é˜¶æ®µ B å’Œé˜¶æ®µ A éƒ½å°è£…åœ¨ `decoder_pipeline` ä¸­ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹
    [paper](https://huggingface.co/papers/2306.00637)ã€‚
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Speed-Up Inference
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ é€Ÿæ¨ç†
- en: 'You can make use of `torch.compile` function and gain a speed-up of about 2-3x:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ `torch.compile` å‡½æ•°ï¼Œè·å¾—å¤§çº¦ 2-3 å€çš„åŠ é€Ÿï¼š
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Limitations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™åˆ¶
- en: Due to the high compression employed by WÃ¼rstchen, generations can lack a good
    amount of detail. To our human eye, this is especially noticeable in faces, hands
    etc.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äº WÃ¼rstchen ä½¿ç”¨äº†é«˜åº¦å‹ç¼©ï¼Œç”Ÿæˆçš„å›¾åƒå¯èƒ½ç¼ºä¹å¤§é‡ç»†èŠ‚ã€‚å¯¹äºæˆ‘ä»¬çš„è‚‰çœ¼æ¥è¯´ï¼Œè¿™åœ¨é¢éƒ¨ã€æ‰‹éƒ¨ç­‰æ–¹é¢å°¤ä¸ºæ˜æ˜¾ã€‚
- en: '**Images can only be generated in 128-pixel steps**, e.g. the next higher resolution
    after 1024x1024 is 1152x1152'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›¾åƒåªèƒ½ä»¥ 128 åƒç´ çš„æ­¥é•¿ç”Ÿæˆ**ï¼Œä¾‹å¦‚ï¼Œ1024x1024 ä¹‹åçš„ä¸‹ä¸€ä¸ªæ›´é«˜åˆ†è¾¨ç‡æ˜¯ 1152x1152'
- en: The model lacks the ability to render correct text in images
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç¼ºä¹åœ¨å›¾åƒä¸­æ­£ç¡®å‘ˆç°æ–‡æœ¬çš„èƒ½åŠ›
- en: The model often does not achieve photorealism
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹é€šå¸¸æ— æ³•å®ç°ç…§ç‰‡çº§é€¼çœŸåº¦
- en: Difficult compositional prompts are hard for the model
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤æ‚çš„ç»„åˆæç¤ºå¯¹æ¨¡å‹æ¥è¯´å¾ˆéš¾
- en: The original codebase, as well as experimental ideas, can be found at [dome272/Wuerstchen](https://github.com/dome272/Wuerstchen).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ä»£ç åº“ä»¥åŠå®éªŒæ€§æƒ³æ³•å¯ä»¥åœ¨ [dome272/Wuerstchen](https://github.com/dome272/Wuerstchen)
    æ‰¾åˆ°ã€‚
- en: WuerstchenCombinedPipeline
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenCombinedPipeline
- en: '### `class diffusers.WuerstchenCombinedPipeline`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.WuerstchenCombinedPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L43)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L43)'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tokenizer` (`CLIPTokenizer`) â€” The decoder tokenizer to be used for text inputs.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨åˆ†è¯å™¨ã€‚'
- en: '`text_encoder` (`CLIPTextModel`) â€” The decoder text encoder to be used for
    text inputs.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`decoder` (`WuerstchenDiffNeXt`) â€” The decoder model to be used for decoder
    image generation pipeline.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` (`WuerstchenDiffNeXt`) â€” ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„è§£ç å™¨æ¨¡å‹ã€‚'
- en: '`scheduler` (`DDPMWuerstchenScheduler`) â€” The scheduler to be used for decoder
    image generation pipeline.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`DDPMWuerstchenScheduler`) â€” ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„è°ƒåº¦å™¨ã€‚'
- en: '`vqgan` (`PaellaVQModel`) â€” The VQGAN model to be used for decoder image generation
    pipeline.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqgan` (`PaellaVQModel`) â€” ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„ VQGAN æ¨¡å‹ã€‚'
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” The prior tokenizer to be used for text
    inputs.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_tokenizer` (`CLIPTokenizer`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„å…ˆå‰åˆ†è¯å™¨ã€‚'
- en: '`prior_text_encoder` (`CLIPTextModel`) â€” The prior text encoder to be used
    for text inputs.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_text_encoder` (`CLIPTextModel`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„å…ˆå‰æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`prior_prior` (`WuerstchenPrior`) â€” The prior model to be used for prior pipeline.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_prior` (`WuerstchenPrior`) â€” ç”¨äºå…ˆå‰ç®¡é“çš„å…ˆå‰æ¨¡å‹ã€‚'
- en: '`prior_scheduler` (`DDPMWuerstchenScheduler`) â€” The scheduler to be used for
    prior pipeline.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler` (`DDPMWuerstchenScheduler`) â€” ç”¨äºå…ˆå‰ç®¡é“çš„è°ƒåº¦å™¨ã€‚'
- en: Combined Pipeline for text-to-image generation using Wuerstchen
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Wuerstchen è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L143)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L143)'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation for the prior and decoder.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`) â€” ç”¨äºæŒ‡å¯¼å…ˆå‰å’Œè§£ç å™¨å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *å¯é€‰*) â€” ä¸æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ
    `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated text embeddings
    for the prior. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, text embeddings will be generated from `prompt` input argument.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” ç”¨äºå…ˆå‰çš„é¢„ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»
    `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆã€‚'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated
    negative text embeddings for the prior. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” ç”¨äºå…ˆå‰çš„é¢„ç”Ÿæˆè´Ÿé¢æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»
    `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆè´Ÿé¢æç¤ºåµŒå…¥ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `prior_guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `prior_guidance_scale > 1`. Higher guidance
    scale encourages to generate images that are closely linked to the text `prompt`,
    usually at the expense of lower image quality.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º4.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`prior_guidance_scale`å®šä¹‰ä¸º[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹2çš„`w`ã€‚é€šè¿‡è®¾ç½®`prior_guidance_scale
    > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`prior_num_inference_steps` (`Union[int, Dict[float, int]]`, *optional*, defaults
    to 60) â€” The number of prior denoising steps. More denoising steps usually lead
    to a higher quality image at the expense of slower inference. For more specific
    timestep spacing, you can pass customized `prior_timesteps`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`Union[int, Dict[float, int]]`, *optional*, é»˜è®¤ä¸º60)
    â€” å…ˆéªŒå»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚è¦è·å¾—æ›´å…·ä½“çš„æ—¶é—´æ­¥é•¿é—´è·ï¼Œå¯ä»¥ä¼ é€’è‡ªå®šä¹‰çš„`prior_timesteps`ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 12) â€” The number of decoder
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference. For more specific timestep spacing, you can pass
    customized `timesteps`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º12) â€” è§£ç å™¨å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚è¦è·å¾—æ›´å…·ä½“çš„æ—¶é—´æ­¥é•¿é—´è·ï¼Œå¯ä»¥ä¼ é€’è‡ªå®šä¹‰çš„`timesteps`ã€‚'
- en: '`prior_timesteps` (`List[float]`, *optional*) â€” Custom timesteps to use for
    the denoising process for the prior. If not defined, equal spaced `prior_num_inference_steps`
    timesteps are used. Must be in descending order.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_timesteps` (`List[float]`, *optional*) â€” ç”¨äºå…ˆéªŒå»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥é•¿ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´è·çš„`prior_num_inference_steps`æ—¶é—´æ­¥é•¿ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚'
- en: '`decoder_timesteps` (`List[float]`, *optional*) â€” Custom timesteps to use for
    the denoising process for the decoder. If not defined, equal spaced `num_inference_steps`
    timesteps are used. Must be in descending order.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_timesteps` (`List[float]`, *optional*) â€” ç”¨äºè§£ç å™¨å»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥é•¿ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´è·çš„`num_inference_steps`æ—¶é—´æ­¥é•¿ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚'
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 0.0) â€” Guidance
    scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`å®šä¹‰ä¸º[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹2çš„`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`æˆ–`List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œåœ¨å‘é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œåœ¨å‘é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯åœ¨`"pil"`ï¼ˆ`PIL.Image.Image`ï¼‰ã€`"np"`ï¼ˆ`np.array`ï¼‰æˆ–`"pt"`ï¼ˆ`torch.Tensor`ï¼‰ä¹‹é—´é€‰æ‹©ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`prior_callback_on_step_end` (`Callable`, *optional*) â€” A function that calls
    at the end of each denoising steps during the inference. The function is called
    with the following arguments: `prior_callback_on_step_end(self: DiffusionPipeline,
    step: int, timestep: int, callback_kwargs: Dict)`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_callback_on_step_end` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`prior_callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚'
- en: '`prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” The list
    of tensor inputs for the `prior_callback_on_step_end` function. The tensors specified
    in the list will be passed as `callback_kwargs` argument. You will only be able
    to include variables listed in the `._callback_tensor_inputs` attribute of your
    pipeline class.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `prior_callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚'
- en: '`callback_on_step_end` (`Callable`, *optional*) â€” A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨æ–­æœŸé—´æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…æ‹¬ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *å¯é€‰*) â€” `callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“ä»¥è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `enable_model_cpu_offload`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_model_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L115)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L115)'
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Offloads all models to CPU using accelerate, reducing memory usage with a low
    impact on performance. Compared to `enable_sequential_cpu_offload`, this method
    moves one whole model at a time to the GPU when its `forward` method is called,
    and the model remains in GPU until the next model runs. Memory savings are lower
    than with `enable_sequential_cpu_offload`, but performance is much better due
    to the iterative execution of the `unet`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŠ é€Ÿå°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ°CPUï¼Œå‡å°‘å†…å­˜ä½¿ç”¨å¹¶å¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚ä¸`enable_sequential_cpu_offload`ç›¸æ¯”ï¼Œæ­¤æ–¹æ³•åœ¨è°ƒç”¨å…¶`forward`æ–¹æ³•æ—¶ä¸€æ¬¡å°†ä¸€ä¸ªå®Œæ•´æ¨¡å‹ç§»è‡³GPUï¼Œå¹¶ä¸”è¯¥æ¨¡å‹ä¿æŒåœ¨GPUä¸­ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹è¿è¡Œã€‚ä¸`enable_sequential_cpu_offload`ç›¸æ¯”ï¼Œå†…å­˜èŠ‚çœè¾ƒä½ï¼Œä½†ç”±äº`unet`çš„è¿­ä»£æ‰§è¡Œï¼Œæ€§èƒ½æ›´å¥½ã€‚
- en: '#### `enable_sequential_cpu_offload`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_sequential_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L125)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L125)'
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Offloads all models (`unet`, `text_encoder`, `vae`, and `safety checker` state
    dicts) to CPU using ğŸ¤— Accelerate, significantly reducing memory usage. Models
    are moved to a `torch.device('meta')` and loaded on a GPU only when their specific
    submoduleâ€™s `forward` method is called. Offloading happens on a submodule basis.
    Memory savings are higher than using `enable_model_cpu_offload`, but performance
    is lower.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerateå°†æ‰€æœ‰æ¨¡å‹ï¼ˆ`unet`ã€`text_encoder`ã€`vae`å’Œ`safety checker`çŠ¶æ€å­—å…¸ï¼‰å…¨éƒ¨è½¬ç§»åˆ°CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¨¡å‹è¢«ç§»åŠ¨åˆ°`torch.device('meta')`ï¼Œä»…å½“è°ƒç”¨å…¶ç‰¹å®šå­æ¨¡å—çš„`forward`æ–¹æ³•æ—¶æ‰ä¼šåœ¨GPUä¸ŠåŠ è½½ã€‚è½¬ç§»æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚å†…å­˜èŠ‚çœé«˜äºä½¿ç”¨`enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
- en: WuerstchenPriorPipeline
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenPriorPipeline
- en: '### `class diffusers.WuerstchenPriorPipeline`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.WuerstchenPriorPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L65)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L65)'
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prior` (`Prior`) â€” The canonical unCLIP prior to approximate the image embedding
    from the text embedding.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior` (`Prior`) â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥é€¼è¿‘å›¾åƒåµŒå…¥çš„ç»å…¸unCLIPå…ˆéªŒã€‚'
- en: '`text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) â€” ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚'
- en: '`scheduler` (`DDPMWuerstchenScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`DDPMWuerstchenScheduler`) â€” ä¸`prior`ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºç”Ÿæˆå›¾åƒåµŒå…¥ã€‚'
- en: '`latent_mean` (â€˜floatâ€™, *optional*, defaults to 42.0) â€” Mean value for latent
    diffusers.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_mean`ï¼ˆâ€˜floatâ€™, *å¯é€‰*, é»˜è®¤ä¸º42.0ï¼‰â€” æ½œåœ¨æ‰©æ•£çš„å‡å€¼ã€‚'
- en: '`latent_std` (â€˜floatâ€™, *optional*, defaults to 1.0) â€” Standard value for latent
    diffusers.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_std`ï¼ˆâ€˜floatâ€™, *å¯é€‰*, é»˜è®¤ä¸º1.0ï¼‰â€” æ½œåœ¨æ‰©æ•£çš„æ ‡å‡†å€¼ã€‚'
- en: '`resolution_multiple` (â€˜floatâ€™, *optional*, defaults to 42.67) â€” Default resolution
    for multiple images generated.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resolution_multiple`ï¼ˆâ€˜floatâ€™, *å¯é€‰*, é»˜è®¤ä¸º42.67ï¼‰â€” ç”Ÿæˆå¤šä¸ªå›¾åƒçš„é»˜è®¤åˆ†è¾¨ç‡ã€‚'
- en: Pipeline for generating image prior for Wuerstchen.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”ŸæˆWuerstchenå›¾åƒå…ˆéªŒçš„ç®¡é“ã€‚
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®¡é“è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    ç”¨äºåŠ è½½LoRAæƒé‡'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    ç”¨äºä¿å­˜LoRAæƒé‡'
- en: '#### `__call__`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L280)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L280)'
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚'
- en: '`height` (`int`, *optional*, defaults to 1024) â€” The height in pixels of the
    generated image.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1024) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 1024) â€” The width in pixels of the
    generated image.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, defaults to 1024) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 60) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 60) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†æ¨ç†é€Ÿåº¦ä¼šå˜æ…¢ã€‚'
- en: '`timesteps` (`List[int]`, *optional*) â€” Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` (`List[int]`, *optional*) â€” ç”¨äºå»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´è·çš„ `num_inference_steps`
    ä¸ªæ—¶é—´æ­¥ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 8.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `decoder_guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `decoder_guidance_scale > 1`. Higher guidance
    scale encourages to generate images that are closely linked to the text `prompt`,
    usually at the expense of lower image quality.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 8.0) â€” å¦‚ [Classifier-Free
    Diffusion Guidance](https://arxiv.org/abs/2207.12598) ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`decoder_guidance_scale`
    å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `decoder_guidance_scale
    > 1` å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `decoder_guidance_scale` is less than `1`).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆå³ï¼Œå¦‚æœ
    `decoder_guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚è°ƒæ•´æç¤ºæƒé‡ã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»
    `prompt` è¾“å…¥å‚æ•°ä¸­ç”Ÿæˆã€‚'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚è°ƒæ•´æç¤ºæƒé‡ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»
    `negative_prompt` è¾“å…¥å‚æ•°ä¸­ç”Ÿæˆè´Ÿæ–‡æœ¬åµŒå…¥ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª
    [torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº
    `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©çš„æ ¼å¼åŒ…æ‹¬ï¼š`"pil"`
    (`PIL.Image.Image`)ã€`"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: '`callback_on_step_end` (`Callable`, *optional*) â€” A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`
    å°†åŒ…æ‹¬ç”± `callback_on_step_end_tensor_inputs` æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `callback_on_step_end`
    å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º `callback_kwargs` å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„ `._callback_tensor_inputs`
    å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: WuerstchenPriorPipelineOutput
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenPriorPipelineOutput
- en: '### `class diffusers.pipelines.wuerstchen.pipeline_wuerstchen_prior.WuerstchenPriorPipelineOutput`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.wuerstchen.pipeline_wuerstchen_prior.WuerstchenPriorPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L51)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L51)'
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_embeddings` (`torch.FloatTensor` or `np.ndarray`) â€” Prior image embeddings
    for text prompt'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeddings` (`torch.FloatTensor` or `np.ndarray`) â€” ç”¨äºæ–‡æœ¬æç¤ºçš„å…ˆéªŒå›¾åƒåµŒå…¥ã€‚'
- en: Output class for WuerstchenPriorPipeline.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: WuerstchenPriorPipeline çš„è¾“å‡ºç±»ã€‚
- en: WuerstchenDecoderPipeline
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenDecoderPipeline
- en: '### `class diffusers.WuerstchenDecoderPipeline`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.WuerstchenDecoderPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L51)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L51)'
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tokenizer` (`CLIPTokenizer`) â€” The CLIP tokenizer.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) â€” CLIP åˆ†è¯å™¨ã€‚'
- en: '`text_encoder` (`CLIPTextModel`) â€” The CLIP text encoder.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) â€” CLIP æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`decoder` (`WuerstchenDiffNeXt`) â€” The WuerstchenDiffNeXt unet decoder.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` (`WuerstchenDiffNeXt`) â€” WuerstchenDiffNeXt unet è§£ç å™¨ã€‚'
- en: '`vqgan` (`PaellaVQModel`) â€” The VQGAN model.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqgan` (`PaellaVQModel`) â€” VQGAN æ¨¡å‹ã€‚'
- en: '`scheduler` (`DDPMWuerstchenScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`DDPMWuerstchenScheduler`) â€” ç”¨äºä¸`prior`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚'
- en: '`latent_dim_scale` (float, `optional`, defaults to 10.67) â€” Multiplier to determine
    the VQ latent space size from the image embeddings. If the image embeddings are
    height=24 and width=24, the VQ latent shape needs to be height=int(24*10.67)=256
    and width=int(24*10.67)=256 in order to match the training conditions.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_dim_scale` (float, *å¯é€‰*, é»˜è®¤ä¸º10.67) â€” ä»å›¾åƒåµŒå…¥ç¡®å®š VQ æ½œå˜é‡ç©ºé—´å¤§å°çš„ä¹˜æ•°ã€‚å¦‚æœå›¾åƒåµŒå…¥çš„é«˜åº¦=24ï¼Œå®½åº¦=24ï¼Œåˆ™
    VQ æ½œå˜é‡å½¢çŠ¶éœ€è¦ä¸ºé«˜åº¦=int(24*10.67)=256ï¼Œå®½åº¦=int(24*10.67)=256ï¼Œä»¥åŒ¹é…è®­ç»ƒæ¡ä»¶ã€‚'
- en: Pipeline for generating images from the Wuerstchen model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºä» Wuerstchen æ¨¡å‹ç”Ÿæˆå›¾åƒçš„ç®¡é“ã€‚
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L208)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L208)'
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_embedding` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” Image
    Embeddings either extracted from an image or generated by a Prior Model.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embedding` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” å›¾åƒåµŒå…¥ï¼Œå¯ä»¥æ˜¯ä»å›¾åƒä¸­æå–çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”±å…ˆéªŒæ¨¡å‹ç”Ÿæˆçš„ã€‚'
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 12) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º12) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚'
- en: '`timesteps` (`List[int]`, *optional*) â€” Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` (`List[int]`, *å¯é€‰*) â€” ç”¨äºå»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´éš”çš„`num_inference_steps`æ—¶é—´æ­¥ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 0.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `decoder_guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `decoder_guidance_scale > 1`. Higher guidance
    scale encourages to generate images that are closely linked to the text `prompt`,
    usually at the expense of lower image quality.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£æŒ‡å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚`decoder_guidance_scale`å®šä¹‰ä¸º[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹2çš„`w`ã€‚é€šè¿‡è®¾ç½®`decoder_guidance_scale
    > 1`å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `decoder_guidance_scale` is less than `1`).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ`decoder_guidance_scale`å°äº`1`ï¼‰ï¼Œåˆ™å¿½ç•¥ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torch
    ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹© `"pil"` (`PIL.Image.Image`)ã€`"np"`
    (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`callback_on_step_end` (`Callable`, *optional*) â€” A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…æ‹¬ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs`ï¼ˆ`List`ï¼Œ*å¯é€‰*ï¼‰â€”`callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨ç®¡é“ç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Citation
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼•ç”¨
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
