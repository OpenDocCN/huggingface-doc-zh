# 有效和高效的扩散

> 原始文本：[https://huggingface.co/docs/diffusers/stable_diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)

让[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)生成特定风格的图像或包含您想要的内容可能有些棘手。通常情况下，您必须多次运行[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)才能得到满意的图像。但是从无到有地生成东西是一个计算密集型的过程，特别是如果您一遍又一遍地运行推理。

这就是为什么从管道中获得最高的*计算*（速度）和*内存*（GPU vRAM）效率非常重要，以减少推理周期之间的时间，使您可以更快地迭代。

本教程将指导您如何使用[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)更快更好地生成。

首先加载[`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)模型：

```py
from diffusers import DiffusionPipeline

model_id = "runwayml/stable-diffusion-v1-5"
pipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True)
```

您将使用的示例提示是一位老战士首领的肖像，但请随意使用您自己的提示：

```py
prompt = "portrait photo of a old warrior chief"
```

## 速度

💡 如果您没有GPU访问权限，您可以免费使用GPU提供商（如[Colab](https://colab.research.google.com/)）的GPU！

加快推理的最简单方法之一是将管道放在GPU上，就像您对任何PyTorch模块所做的那样：

```py
pipeline = pipeline.to("cuda")
```

为了确保您可以使用相同的图像并对其进行改进，请使用[`Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)并为[可重现性](./using-diffusers/reproducibility)设置一个种子：

```py
import torch

generator = torch.Generator("cuda").manual_seed(0)
```

现在您可以生成图像：

```py
image = pipeline(prompt, generator=generator).images[0]
image
```

![](../Images/97b4b76d7c052f96d96b7a8b402a25ce.png)

这个过程在T4 GPU上花费了~30秒（如果您分配的GPU比T4更好，则可能会更快）。默认情况下，[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)以完整的`float32`精度运行50个推理步骤。您可以通过切换到较低精度（如`float16`）或减少推理步骤来加快速度。

让我们从在`float16`中加载模型并生成图像开始：

```py
import torch

pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True)
pipeline = pipeline.to("cuda")
generator = torch.Generator("cuda").manual_seed(0)
image = pipeline(prompt, generator=generator).images[0]
image
```

![](../Images/66880d2cc1c9bb78b8e03483b16d1ca3.png)

这次，生成图像仅花费了~11秒，比以前快了近3倍！

💡 我们强烈建议始终在`float16`中运行您的管道，到目前为止，我们很少看到输出质量下降。

另一个选项是减少推理步骤的数量。选择更高效的调度器可以帮助减少步骤数量，而不会牺牲输出质量。您可以通过调用`compatibles`方法在[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)中找到与当前模型兼容的调度器：

```py
pipeline.scheduler.compatibles
[
    diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,
    diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler,
    diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler,
    diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler,
    diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler,
    diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler,
    diffusers.schedulers.scheduling_ddpm.DDPMScheduler,
    diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler,
    diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler,
    diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler,
    diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler,
    diffusers.schedulers.scheduling_pndm.PNDMScheduler,
    diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler,
    diffusers.schedulers.scheduling_ddim.DDIMScheduler,
]
```

稳定的扩散模型默认使用[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)，通常需要~50个推理步骤，但性能更好的调度器如[DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)，只需要~20或25个推理步骤。使用[from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)方法加载新的调度器：

```py
from diffusers import DPMSolverMultistepScheduler

pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)
```

现在将`num_inference_steps`设置为20：

```py
generator = torch.Generator("cuda").manual_seed(0)
image = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]
image
```

![](../Images/a0e3931ebf91a12e3caef2d780586fec.png)

很好，您已经成功将推理时间缩短到仅为4秒！⚡️

## 内存

改进管道性能的另一个关键是消耗更少的内存，这间接意味着更快的速度，因为您通常试图最大化每秒生成的图像数量。查看您可以一次生成多少图像的最简单方法是尝试不同的批处理大小，直到出现`OutOfMemoryError`（OOM）。

创建一个函数，从提示和`Generators`列表中生成一批图像。确保为每个`Generator`分配一个种子，这样如果它产生了好的结果，你就可以重复使用它。

```py
def get_inputs(batch_size=1):
    generator = [torch.Generator("cuda").manual_seed(i) for i in range(batch_size)]
    prompts = batch_size * [prompt]
    num_inference_steps = 20

    return {"prompt": prompts, "generator": generator, "num_inference_steps": num_inference_steps}
```

从`batch_size=4`开始，看看你消耗了多少内存：

```py
from diffusers.utils import make_image_grid

images = pipeline(**get_inputs(batch_size=4)).images
make_image_grid(images, 2, 2)
```

除非你有更多vRAM的GPU，上面的代码可能返回了一个`OOM`错误！大部分内存被交叉注意力层占用。你可以将这个操作顺序运行而不是批量运行，以节省大量内存。你只需要配置管道使用[enable_attention_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_attention_slicing)函数：

```py
pipeline.enable_attention_slicing()
```

现在尝试将`batch_size`增加到8！

```py
images = pipeline(**get_inputs(batch_size=8)).images
make_image_grid(images, rows=2, cols=4)
```

![](../Images/f242684980096d86bea7fa383ccd606a.png)

以前你甚至不能生成一批4张图像，现在你可以以每张图像约3.5秒的速度生成一批8张图像！这可能是在T4 GPU上保持质量的最快速度了。

## 质量

在最后两节中，你学会了如何通过使用`fp16`来优化管道的速度，通过使用更高性能的调度程序减少推理步骤的数量，并启用注意力切片来减少内存消耗。现在你要专注于如何提高生成图像的质量。

### 更好的检查点

最明显的一步是使用更好的检查点。稳定扩散模型是一个很好的起点，自从它正式推出以来，也发布了几个改进版本。然而，使用更新版本并不意味着你会自动获得更好的结果。你仍然需要自己尝试不同的检查点，并进行一些研究（比如使用[负面提示](https://minimaxir.com/2022/11/stable-diffusion-negative-prompt/)）来获得最佳结果。

随着领域的发展，有越来越多的经过微调的高质量检查点，用于产生特定风格。尝试探索[Hub](https://huggingface.co/models?library=diffusers&sort=downloads)和[Diffusers Gallery](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery)，找到你感兴趣的一个！

### 更好的管道组件

你也可以尝试用更新版本替换当前的管道组件。让我们尝试将Stability AI的最新[自动编码器](https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main/vae)加载到管道中，并生成一些图像：

```py
from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse", torch_dtype=torch.float16).to("cuda")
pipeline.vae = vae
images = pipeline(**get_inputs(batch_size=8)).images
make_image_grid(images, rows=2, cols=4)
```

![](../Images/f35b7f9a028460ebed6e9b753a1241d3.png)

### 更好的提示工程

你用来生成图像的文本提示非常重要，以至于被称为*提示工程*。在进行提示工程时要考虑的一些因素是：

+   图像或类似我想生成的图像在互联网上是如何存储的？

+   我可以提供什么额外的细节来引导模型朝着我想要的风格发展？

考虑到这一点，让我们改进提示，包括颜色和更高质量的细节：

```py
prompt += ", tribal panther make up, blue on red, side profile, looking away, serious eyes"
prompt += " 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta"
```

用新提示生成一批图像：

```py
images = pipeline(**get_inputs(batch_size=8)).images
make_image_grid(images, rows=2, cols=4)
```

![](../Images/52c687d1d7c73bcf4a482dc45feea714.png)

相当令人印象深刻！让我们微调第二张图像 - 对应于种子为`1`的`Generator` - 添加一些关于主题年龄的文字：

```py
prompts = [
    "portrait photo of the oldest warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta",
    "portrait photo of a old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta",
    "portrait photo of a warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta",
    "portrait photo of a young warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta",
]

generator = [torch.Generator("cuda").manual_seed(1) for _ in range(len(prompts))]
images = pipeline(prompt=prompts, generator=generator, num_inference_steps=25).images
make_image_grid(images, 2, 2)
```

![](../Images/181d5d3b3b595630cddf8064c033ea4f.png)

## 下一步

在本教程中，你学会了如何为计算和内存效率以及改善生成输出质量优化[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。如果你有兴趣让你的管道运行得更快，可以查看以下资源：

+   了解如何使用[PyTorch 2.0](./optimization/torch2.0)和[`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)可以提高5-300%的推理速度。在A100 GPU上，推理速度可以提高50%！

+   如果你不能使用PyTorch 2，我们建议你安装[xFormers](./optimization/xformers)。它的内存高效的注意力机制与PyTorch 1.13.1非常搭配，可以实现更快的速度和减少内存消耗。

+   其他优化技术，比如模型卸载，在[这个指南](./optimization/fp16)中有介绍。
