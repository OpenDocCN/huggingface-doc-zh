- en: CPM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPM
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpm)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpm)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The CPM model was proposed in [CPM: A Large-scale Generative Chinese Pre-trained
    Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao
    Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan,
    Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,
    Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi
    Li, Xiaoyan Zhu, Maosong Sun.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'CPM模型是由张政彦、韩旭、周浩、柯培、顾宇贤、叶德明、秦宇佳、苏玉生、季浩哲、关健、齐凡超、王晓智、郑亚楠、曾国阳、曹焕琦、陈胜奇、李代轩、孙振波、刘知远、黄民烈、韩文涛、唐杰、李娟姿、朱小燕、孙茂松在[CPM:
    A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Pre-trained Language Models (PLMs) have proven to be beneficial for various
    downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training
    data, drew a lot of attention due to the capacity of few-shot (even zero-shot)
    learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging,
    as the training corpus of GPT-3 is primarily English, and the parameters are not
    publicly available. In this technical report, we release the Chinese Pre-trained
    Language Model (CPM) with generative pre-training on large-scale Chinese training
    data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB
    Chinese training data, is the largest Chinese pre-trained language model, which
    could facilitate several downstream Chinese NLP tasks, such as conversation, essay
    generation, cloze test, and language understanding. Extensive experiments demonstrate
    that CPM achieves strong performance on many NLP tasks in the settings of few-shot
    (even zero-shot) learning.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*预训练语言模型（PLMs）已被证明对各种下游NLP任务有益。最近，拥有1750亿参数和570GB训练数据的GPT-3因其少样本（甚至零样本）学习能力而引起了很多关注。然而，将GPT-3应用于解决中文NLP任务仍然具有挑战性，因为GPT-3的训练语料主要是英文，参数也不是公开的。在这份技术报告中，我们发布了在大规模中文训练数据上进行生成式预训练的中文预训练语言模型（CPM）。据我们所知，CPM拥有26亿参数和100GB中文训练数据，是目前最大的中文预训练语言模型，可以促进多个下游中文NLP任务，如对话、文章生成、填空测试和语言理解。大量实验证明，CPM在少样本（甚至零样本）学习的情况下在许多NLP任务上取得了强大的性能。*'
- en: 'This model was contributed by [canwenxu](https://huggingface.co/canwenxu).
    The original implementation can be found here: [https://github.com/TsinghuaAI/CPM-Generate](https://github.com/TsinghuaAI/CPM-Generate)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[canwenxu](https://huggingface.co/canwenxu)贡献。原始实现可在此处找到：[https://github.com/TsinghuaAI/CPM-Generate](https://github.com/TsinghuaAI/CPM-Generate)
- en: CPM’s architecture is the same as GPT-2, except for tokenization method. Refer
    to [GPT-2 documentation](gpt2) for API reference information.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CPM的架构与GPT-2相同，除了分词方法。有关API参考信息，请参阅[GPT-2文档](gpt2)。
- en: CpmTokenizer
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CpmTokenizer
- en: '### `class transformers.CpmTokenizer`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CpmTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L38)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L38)'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结巴分词工具进行预分词。用于CPM模型。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L245)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L245)'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 第二个序列对应的ID列表（可选）。'
- en: Returns
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLNet sequence has the following
    format:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，为序列分类任务构建模型输入，输入为单个序列或序列对。一个XLNet序列的格式如下：
- en: 'single sequence: `X <sep> <cls>`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`X <sep> <cls>`
- en: 'pair of sequences: `A <sep> B <sep> <cls>`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`A <sep> B <sep> <cls>`
- en: '#### `convert_tokens_to_string`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L239)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L239)'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Converts a sequence of tokens (strings for sub-words) in a single string.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（子词的字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L300)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L300)'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 第二个序列对应的ID列表（可选）。'
- en: Returns
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定序列的[token类型ID](../glossary#token-type-ids)列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLNet
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个掩码，用于在序列对分类任务中使用。一个XLNet
- en: 'sequence pair mask has the following format:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对掩码的格式如下：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。
- en: '#### `get_special_tokens_mask`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L271)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L271)'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经格式化为模型的特殊标记。'
- en: Returns
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。当使用分词器`prepare_for_model`方法添加特殊标记时，将调用此方法。
- en: CpmTokenizerFast
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CpmTokenizerFast
- en: '### `class transformers.CpmTokenizerFast`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CpmTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L38)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L38)'
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结巴分词工具进行预分词。用于CPM模型。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L160)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L160)'
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLNet sequence has the following
    format:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，从序列或序列对构建模型输入，用于序列分类任务。一个XLNet序列具有以下格式：
- en: 'single sequence: `X <sep> <cls>`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`X <sep> <cls>`
- en: 'pair of sequences: `A <sep> B <sep> <cls>`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`A <sep> B <sep> <cls>`
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L186)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L186)'
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定序列的[token type IDs](../glossary#token-type-ids)列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLNet
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。一个XLNet
- en: 'sequence pair mask has the following format:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对掩码具有以下格式：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `token_ids_1` 是 `None`，则此方法仅返回掩码的第一部分（0s）。
