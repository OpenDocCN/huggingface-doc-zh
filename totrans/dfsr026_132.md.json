["```py\n( vae: AutoencoderKL text_encoder: ClapModel text_encoder_2: T5EncoderModel projection_model: AudioLDM2ProjectionModel language_model: GPT2Model tokenizer: Union tokenizer_2: Union feature_extractor: ClapFeatureExtractor unet: AudioLDM2UNet2DConditionModel scheduler: KarrasDiffusionSchedulers vocoder: SpeechT5HifiGan )\n```", "```py\n( prompt: Union = None audio_length_in_s: Optional = None num_inference_steps: int = 200 guidance_scale: float = 3.5 negative_prompt: Union = None num_waveforms_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None generated_prompt_embeds: Optional = None negative_generated_prompt_embeds: Optional = None attention_mask: Optional = None negative_attention_mask: Optional = None max_new_tokens: Optional = None return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None output_type: Optional = 'np' ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import scipy\n>>> import torch\n>>> from diffusers import AudioLDM2Pipeline\n\n>>> repo_id = \"cvssp/audioldm2\"\n>>> pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n\n>>> # define the prompts\n>>> prompt = \"The sound of a hammer hitting a wooden surface.\"\n>>> negative_prompt = \"Low quality.\"\n\n>>> # set the seed for generator\n>>> generator = torch.Generator(\"cuda\").manual_seed(0)\n\n>>> # run the generation\n>>> audio = pipe(\n...     prompt,\n...     negative_prompt=negative_prompt,\n...     num_inference_steps=200,\n...     audio_length_in_s=10.0,\n...     num_waveforms_per_prompt=3,\n...     generator=generator,\n... ).audios\n\n>>> # save the best audio sample (index 0) as a .wav file\n>>> scipy.io.wavfile.write(\"techno.wav\", rate=16000, data=audio[0])\n```", "```py\n( )\n```", "```py\n( gpu_id = 0 )\n```", "```py\n( )\n```", "```py\n( prompt device num_waveforms_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None generated_prompt_embeds: Optional = None negative_generated_prompt_embeds: Optional = None attention_mask: Optional = None negative_attention_mask: Optional = None max_new_tokens: Optional = None ) \u2192 export const metadata = 'undefined';prompt_embeds (torch.FloatTensor)\n```", "```py\n>>> import scipy\n>>> import torch\n>>> from diffusers import AudioLDM2Pipeline\n\n>>> repo_id = \"cvssp/audioldm2\"\n>>> pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n\n>>> # Get text embedding vectors\n>>> prompt_embeds, attention_mask, generated_prompt_embeds = pipe.encode_prompt(\n...     prompt=\"Techno music with a strong, upbeat tempo and high melodic riffs\",\n...     device=\"cuda\",\n...     do_classifier_free_guidance=True,\n... )\n\n>>> # Pass text embeddings to pipeline for text-conditional audio generation\n>>> audio = pipe(\n...     prompt_embeds=prompt_embeds,\n...     attention_mask=attention_mask,\n...     generated_prompt_embeds=generated_prompt_embeds,\n...     num_inference_steps=200,\n...     audio_length_in_s=10.0,\n... ).audios[0]\n\n>>> # save generated audio sample\n>>> scipy.io.wavfile.write(\"techno.wav\", rate=16000, data=audio)\n```", "```py\n( inputs_embeds: Tensor = None max_new_tokens: int = 8 **model_kwargs ) \u2192 export const metadata = 'undefined';inputs_embeds (torch.FloatTensorof shape(batch_size, sequence_length, hidden_size)`)\n```", "```py\n( text_encoder_dim text_encoder_1_dim langauge_model_dim )\n```", "```py\n( hidden_states: Optional = None hidden_states_1: Optional = None attention_mask: Optional = None attention_mask_1: Optional = None )\n```", "```py\n( sample_size: Optional = None in_channels: int = 4 out_channels: int = 4 flip_sin_to_cos: bool = True freq_shift: int = 0 down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D') mid_block_type: Optional = 'UNetMidBlock2DCrossAttn' up_block_types: Tuple = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D') only_cross_attention: Union = False block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: Union = 2 downsample_padding: int = 1 mid_block_scale_factor: float = 1 act_fn: str = 'silu' norm_num_groups: Optional = 32 norm_eps: float = 1e-05 cross_attention_dim: Union = 1280 transformer_layers_per_block: Union = 1 attention_head_dim: Union = 8 num_attention_heads: Union = None use_linear_projection: bool = False class_embed_type: Optional = None num_class_embeds: Optional = None upcast_attention: bool = False resnet_time_scale_shift: str = 'default' time_embedding_type: str = 'positional' time_embedding_dim: Optional = None time_embedding_act_fn: Optional = None timestep_post_act: Optional = None time_cond_proj_dim: Optional = None conv_in_kernel: int = 3 conv_out_kernel: int = 3 projection_class_embeddings_input_dim: Optional = None class_embeddings_concat: bool = False )\n```", "```py\n( sample: FloatTensor timestep: Union encoder_hidden_states: Tensor class_labels: Optional = None timestep_cond: Optional = None attention_mask: Optional = None cross_attention_kwargs: Optional = None encoder_attention_mask: Optional = None return_dict: bool = True encoder_hidden_states_1: Optional = None encoder_attention_mask_1: Optional = None ) \u2192 export const metadata = 'undefined';UNet2DConditionOutput or tuple\n```", "```py\n( audios: ndarray )\n```"]