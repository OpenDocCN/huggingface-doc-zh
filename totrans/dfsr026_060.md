# Kandinsky 2.2

> 原始文本：[`huggingface.co/docs/diffusers/training/kandinsky`](https://huggingface.co/docs/diffusers/training/kandinsky)

这个脚本是实验性的，很容易过拟合并遇到像灾难性遗忘这样的问题。尝试探索不同的超参数，以获得您数据集的最佳结果。

Kandinsky 2.2 是一个多语言文本到图像模型，能够生成更逼真的图像。该模型包括一个图像先验模型，用于从文本提示创建图像嵌入，以及一个解码器模型，根据先验模型的嵌入生成图像。这就是为什么在 Diffusers 中为 Kandinsky 2.2 提供了两个单独的脚本，一个用于训练先验模型，一个用于训练解码器模型。您可以分别训练这两个模型，但为了获得最佳结果，您应该同时训练先验和解码器模型。

根据您的 GPU，您可能需要启用`gradient_checkpointing`（⚠️不支持先前模型！），`mixed_precision`和`gradient_accumulation_steps`来帮助将模型适应内存并加速训练。您还可以通过启用 xFormers 的内存高效注意力来进一步减少内存使用量（版本[v0.0.16](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212)在某些 GPU 上训练失败，因此您可能需要安装开发版本）。

本指南探讨了[train_text_to_image_prior.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py)和[train_text_to_image_decoder.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py)脚本，以帮助您更熟悉它，并了解如何为您自己的用例进行调整。

在运行脚本之前，请确保您从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

然后导航到包含训练脚本的示例文件夹，并为您正在使用的脚本安装所需的依赖项：

```py
cd examples/kandinsky2_2/text_to_image
pip install -r requirements.txt
```

🤗 Accelerate 是一个帮助您在多个 GPU/TPU 上进行训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。

初始化🤗 Accelerate 环境：

```py
accelerate config
```

设置默认🤗 Accelerate 环境而不选择任何配置：

```py
accelerate config default
```

或者，如果您的环境不支持交互式 shell，比如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看创建用于训练的数据集指南，了解如何创建一个适用于训练脚本的数据集。

以下部分突出显示了训练脚本的一些重要部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读脚本，并告诉我们您是否有任何问题或疑虑。

## 脚本参数

训练脚本提供了许多参数，帮助您自定义训练运行。所有参数及其描述都可以在[`parse_args()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L190)函数中找到。训练脚本为每个参数提供了默认值，如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。

例如，要使用 fp16 格式加速混合精度训练，请在训练命令中添加`--mixed_precision`参数：

```py
accelerate launch train_text_to_image_prior.py \
  --mixed_precision="fp16"
```

大多数参数与文本到图像训练指南中的参数相同，因此让我们直接开始 Kandinsky 训练脚本的演练！

### 最小-SNR 加权

[Min-SNR](https://huggingface.co/papers/2303.09556)加权策略可以通过重新平衡损失来帮助训练，以实现更快的收敛。训练脚本支持预测`epsilon`（噪声）或`v_prediction`，但 Min-SNR 与两种预测类型兼容。这种加权策略仅受 PyTorch 支持，在 Flax 训练脚本中不可用。

添加`--snr_gamma`参数，并将其设置为推荐值 5.0：

```py
accelerate launch train_text_to_image_prior.py \
  --snr_gamma=5.0
```

## 训练脚本

训练脚本也类似于文本到图像训练指南，但已经修改以支持训练先验和解码器模型。本指南侧重于 Kandinsky 2.2 训练脚本中独特的代码。

先验模型解码器模型

[`main()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L441)函数包含准备数据集和训练模型的代码。

您立即注意到的主要区别之一是，训练脚本还加载了一个[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor) - 除了调度器和标记器之外 - 用于预处理图像，以及一个[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)模型用于编码图像：

```py
noise_scheduler = DDPMScheduler(beta_schedule="squaredcos_cap_v2", prediction_type="sample")
image_processor = CLIPImageProcessor.from_pretrained(
    args.pretrained_prior_model_name_or_path, subfolder="image_processor"
)
tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder="tokenizer")

with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
    image_encoder = CLIPVisionModelWithProjection.from_pretrained(
        args.pretrained_prior_model_name_or_path, subfolder="image_encoder", torch_dtype=weight_dtype
    ).eval()
    text_encoder = CLIPTextModelWithProjection.from_pretrained(
        args.pretrained_prior_model_name_or_path, subfolder="text_encoder", torch_dtype=weight_dtype
    ).eval()
```

Kandinsky 使用 PriorTransformer 生成图像嵌入，因此您需要设置优化器来学习先验模式的参数。

```py
prior = PriorTransformer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder="prior")
prior.train()
optimizer = optimizer_cls(
    prior.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

接下来，输入标题被标记化，图像由[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)进行[预处理](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L632)：

```py
def preprocess_train(examples):
    images = [image.convert("RGB") for image in examples[image_column]]
    examples["clip_pixel_values"] = image_processor(images, return_tensors="pt").pixel_values
    examples["text_input_ids"], examples["text_mask"] = tokenize_captions(examples)
    return examples
```

最后，[训练循环](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L718)将输入图像转换为潜变量，向图像嵌入添加噪声，并进行预测：

```py
model_pred = prior(
    noisy_latents,
    timestep=timesteps,
    proj_embedding=prompt_embeds,
    encoder_hidden_states=text_encoder_hidden_states,
    attention_mask=text_mask,
).predicted_image_embedding
```

如果您想了解训练循环的工作原理，请查看了解管道、模型和调度器教程，该教程详细介绍了去噪过程的基本模式。

## 启动脚本

一旦您完成所有更改或对默认配置满意，您就可以启动训练脚本！🚀

您将在[Pokémon BLIP 标题](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)数据集上进行训练，以生成您自己的 Pokémon，但您也可以通过按照创建用于训练的数据集指南来创建和训练自己的数据集。将环境变量`DATASET_NAME`设置为 Hub 上数据集的名称，或者如果您在自己的文件上进行训练，请将环境变量`TRAIN_DIR`设置为数据集的路径。

如果您在多个 GPU 上进行训练，请在`accelerate launch`命令中添加`--multi_gpu`参数。

要使用 Weights & Biases 监视训练进度，请在训练命令中添加`--report_to=wandb`参数。您还需要在训练命令中添加`--validation_prompt`以跟踪结果。这对于调试模型和查看中间结果非常有用。

先验模型解码器模型

```py
export DATASET_NAME="lambdalabs/pokemon-blip-captions"

accelerate launch --mixed_precision="fp16"  train_text_to_image_prior.py \
  --dataset_name=$DATASET_NAME \
  --resolution=768 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --checkpoints_total_limit=3 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --validation_prompts="A robot pokemon, 4k photo" \
  --report_to="wandb" \
  --push_to_hub \
  --output_dir="kandi2-prior-pokemon-model" 
```

训练完成后，您可以使用新训练的模型进行推断！

先验模型解码器模型

```py
from diffusers import AutoPipelineForText2Image, DiffusionPipeline
import torch

prior_pipeline = DiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16)
prior_components = {"prior_" + k: v for k,v in prior_pipeline.components.items()}
pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", **prior_components, torch_dtype=torch.float16)

pipe.enable_model_cpu_offload()
prompt="A robot pokemon, 4k photo"
image = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]
```

请随意将`kandinsky-community/kandinsky-2-2-decoder`替换为您自己训练的解码器检查点！

## 下一步

祝贺您训练了一个 Kandinsky 2.2 模型！要了解如何使用您的新模型，以下指南可能会有所帮助：

+   阅读 Kandinsky 指南，了解如何将其用于各种不同任务（文本到图像，图像到图像，修补，插值），以及如何与 ControlNet 结合使用。

+   查看 DreamBooth 和 LoRA 培训指南，学习如何仅使用几个示例图像训练个性化的 Kandinsky 模型。甚至可以结合这两种训练技术！
