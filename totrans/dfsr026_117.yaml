- en: VQModel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VQModel
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/vq](https://huggingface.co/docs/diffusers/api/models/vq)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/api/models/vq](https://huggingface.co/docs/diffusers/api/models/vq)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The VQ-VAE model was introduced in [Neural Discrete Representation Learning](https://huggingface.co/papers/1711.00937)
    by Aaron van den Oord, Oriol Vinyals and Koray Kavukcuoglu. The model is used
    in ğŸ¤— Diffusers to decode latent representations into images. Unlike [AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL),
    the [VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel) works
    in a quantized latent space.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-VAEæ¨¡å‹æ˜¯ç”±Aaron van den Oordã€Oriol Vinyalså’ŒKoray Kavukcuogluåœ¨[ç¥ç»ç¦»æ•£è¡¨ç¤ºå­¦ä¹ ](https://huggingface.co/papers/1711.00937)ä¸­å¼•å…¥çš„ã€‚è¯¥æ¨¡å‹ç”¨äºåœ¨ğŸ¤—
    Diffusersä¸­å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒã€‚ä¸[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)ä¸åŒï¼Œ[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)åœ¨é‡åŒ–çš„æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œã€‚
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Learning useful representations without supervision remains a key challenge
    in machine learning. In this paper, we propose a simple yet powerful generative
    model that learns such discrete representations. Our model, the Vector Quantised-Variational
    AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs
    discrete, rather than continuous, codes; and the prior is learnt rather than static.
    In order to learn a discrete latent representation, we incorporate ideas from
    vector quantisation (VQ). Using the VQ method allows the model to circumvent issues
    of â€œposterior collapseâ€ â€” where the latents are ignored when they are paired with
    a powerful autoregressive decoder â€” typically observed in the VAE framework. Pairing
    these representations with an autoregressive prior, the model can generate high
    quality images, videos, and speech as well as doing high quality speaker conversion
    and unsupervised learning of phonemes, providing further evidence of the utility
    of the learnt representations.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æ²¡æœ‰ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ æœ‰ç”¨çš„è¡¨ç¤ºä»ç„¶æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå­¦ä¹ è¿™ç§ç¦»æ•£è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå³çŸ¢é‡é‡åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQ-VAEï¼‰ï¼Œä¸VAEåœ¨ä¸¤ä¸ªå…³é”®æ–¹é¢ä¸åŒï¼šç¼–ç å™¨ç½‘ç»œè¾“å‡ºç¦»æ•£è€Œä¸æ˜¯è¿ç»­çš„ä»£ç ï¼›å…ˆéªŒæ˜¯å­¦ä¹ è€Œä¸æ˜¯é™æ€çš„ã€‚ä¸ºäº†å­¦ä¹ ç¦»æ•£çš„æ½œåœ¨è¡¨ç¤ºï¼Œæˆ‘ä»¬èå…¥äº†çŸ¢é‡é‡åŒ–ï¼ˆVQï¼‰çš„æ€æƒ³ã€‚ä½¿ç”¨VQæ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿé¿å¼€â€œåéªŒåç¼©â€çš„é—®é¢˜â€”â€”å³å½“æ½œåœ¨ç©ºé—´ä¸å¼ºå¤§çš„è‡ªå›å½’è§£ç å™¨é…å¯¹æ—¶ï¼Œé€šå¸¸åœ¨VAEæ¡†æ¶ä¸­è§‚å¯Ÿåˆ°çš„é—®é¢˜ã€‚å°†è¿™äº›è¡¨ç¤ºä¸è‡ªå›å½’å…ˆéªŒé…å¯¹ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ï¼Œä»¥åŠè¿›è¡Œé«˜è´¨é‡çš„è¯´è¯äººè½¬æ¢å’Œæ— ç›‘ç£å­¦ä¹ éŸ³ç´ ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å­¦ä¹ è¡¨ç¤ºçš„å®ç”¨æ€§ã€‚*'
- en: VQModel
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQModel
- en: '### `class diffusers.VQModel`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.VQModel`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L40)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L40)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`in_channels` (int, *optional*, defaults to 3) â€” Number of channels in the
    input image.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`ï¼ˆintï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰â€” è¾“å…¥å›¾åƒä¸­çš„é€šé“æ•°ã€‚'
- en: '`out_channels` (int, *optional*, defaults to 3) â€” Number of channels in the
    output.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels`ï¼ˆintï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰â€” è¾“å‡ºä¸­çš„é€šé“æ•°ã€‚'
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`)
    â€” Tuple of downsample block types.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`("DownEncoderBlock2D",)`ï¼‰â€” ä¸‹é‡‡æ ·å—ç±»å‹çš„å…ƒç»„ã€‚'
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`)
    â€” Tuple of upsample block types.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_types`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`("UpDecoderBlock2D",)`ï¼‰â€” ä¸Šé‡‡æ ·å—ç±»å‹çš„å…ƒç»„ã€‚'
- en: '`block_out_channels` (`Tuple[int]`, *optional*, defaults to `(64,)`) â€” Tuple
    of block output channels.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_out_channels`ï¼ˆ`Tuple[int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(64,)`ï¼‰â€” å—è¾“å‡ºé€šé“çš„å…ƒç»„ã€‚'
- en: '`layers_per_block` (`int`, *optional*, defaults to `1`) â€” Number of layers
    per block.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_block`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`1`ï¼‰â€” æ¯ä¸ªå—çš„å±‚æ•°ã€‚'
- en: '`act_fn` (`str`, *optional*, defaults to `"silu"`) â€” The activation function
    to use.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"silu"`ï¼‰â€” è¦ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚'
- en: '`latent_channels` (`int`, *optional*, defaults to `3`) â€” Number of channels
    in the latent space.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`3`ï¼‰â€” æ½œåœ¨ç©ºé—´ä¸­çš„é€šé“æ•°ã€‚'
- en: '`sample_size` (`int`, *optional*, defaults to `32`) â€” Sample input size.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`32`ï¼‰â€” æ ·æœ¬è¾“å…¥å¤§å°ã€‚'
- en: '`num_vq_embeddings` (`int`, *optional*, defaults to `256`) â€” Number of codebook
    vectors in the VQ-VAE.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_vq_embeddings`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`256`ï¼‰â€” VQ-VAEä¸­ç ä¹¦å‘é‡çš„æ•°é‡ã€‚'
- en: '`norm_num_groups` (`int`, *optional*, defaults to `32`) â€” Number of groups
    for normalization layers.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_num_groups`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`32`ï¼‰â€” è§„èŒƒåŒ–å±‚çš„ç»„æ•°ã€‚'
- en: '`vq_embed_dim` (`int`, *optional*) â€” Hidden dim of codebook vectors in the
    VQ-VAE.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vq_embed_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” VQ-VAEä¸­ç ä¹¦å‘é‡çš„éšè—ç»´åº¦ã€‚'
- en: '`scaling_factor` (`float`, *optional*, defaults to `0.18215`) â€” The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`0.18215`ï¼‰â€” ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€æ‰¹è®¡ç®—å‡ºçš„è®­ç»ƒæ½œåœ¨ç©ºé—´çš„åˆ†é‡æ ‡å‡†å·®ã€‚è¿™ç”¨äºåœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶å°†æ½œåœ¨ç©ºé—´ç¼©æ”¾ä¸ºå•ä½æ–¹å·®ã€‚åœ¨ä¼ é€’ç»™æ‰©æ•£æ¨¡å‹ä¹‹å‰ï¼Œæ½œåœ¨ç©ºé—´ä¼šæŒ‰ç…§å…¬å¼`z
    = z * scaling_factor`è¿›è¡Œç¼©æ”¾ã€‚åœ¨è§£ç æ—¶ï¼Œæ½œåœ¨ç©ºé—´ä¼šæŒ‰ç…§å…¬å¼è¿›è¡Œç¼©æ”¾å›åŸå§‹æ¯”ä¾‹ï¼š`z = 1 / scaling_factor * z`ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[å…·æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ](https://arxiv.org/abs/2112.10752)è®ºæ–‡çš„4.3.2å’ŒD.1èŠ‚ã€‚'
- en: '`norm_type` (`str`, *optional*, defaults to `"group"`) â€” Type of normalization
    layer to use. Can be one of `"group"` or `"spatial"`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_type`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"group"`ï¼‰â€” è¦ä½¿ç”¨çš„è§„èŒƒåŒ–å±‚ç±»å‹ã€‚å¯ä»¥æ˜¯`"group"`æˆ–`"spatial"`ä¹‹ä¸€ã€‚'
- en: A VQ-VAE model for decoding latent representations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºè§£ç æ½œåœ¨è¡¨ç¤ºçš„VQ-VAEæ¨¡å‹ã€‚
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for itâ€™s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ã€‚
- en: '#### `forward`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L158)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L158)'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sample` (`torch.FloatTensor`) â€” Input sample.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`ï¼ˆ`torch.FloatTensor`ï¼‰â€” è¾“å…¥æ ·æœ¬ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [models.vq_model.VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    instead of a plain tuple.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[models.vq_model.VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    or `tuple`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    æˆ– `tuple`'
- en: If return_dict is True, a [VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    is returned, otherwise a plain `tuple` is returned.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœreturn_dictä¸ºTrueï¼Œåˆ™è¿”å›ä¸€ä¸ª[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªæ™®é€šçš„`tuple`ã€‚
- en: The [VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel) forward
    method.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)çš„å‰å‘æ–¹æ³•ã€‚'
- en: VQEncoderOutput
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQEncoderOutput
- en: '### `class diffusers.models.vq_model.VQEncoderOutput`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.vq_model.VQEncoderOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L27)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L27)'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`latents` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” The encoded output sample from the last layer of the model.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    æ¨¡å‹æœ€åä¸€å±‚çš„ç¼–ç è¾“å‡ºæ ·æœ¬ã€‚'
- en: Output of VQModel encoding method.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: VQModelç¼–ç æ–¹æ³•çš„è¾“å‡ºã€‚
