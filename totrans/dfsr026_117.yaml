- en: VQModel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/diffusers/api/models/vq](https://huggingface.co/docs/diffusers/api/models/vq)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/29.b3afb2c5.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: The VQ-VAE model was introduced in [Neural Discrete Representation Learning](https://huggingface.co/papers/1711.00937)
    by Aaron van den Oord, Oriol Vinyals and Koray Kavukcuoglu. The model is used
    in ü§ó Diffusers to decode latent representations into images. Unlike [AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL),
    the [VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel) works
    in a quantized latent space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning useful representations without supervision remains a key challenge
    in machine learning. In this paper, we propose a simple yet powerful generative
    model that learns such discrete representations. Our model, the Vector Quantised-Variational
    AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs
    discrete, rather than continuous, codes; and the prior is learnt rather than static.
    In order to learn a discrete latent representation, we incorporate ideas from
    vector quantisation (VQ). Using the VQ method allows the model to circumvent issues
    of ‚Äúposterior collapse‚Äù ‚Äî where the latents are ignored when they are paired with
    a powerful autoregressive decoder ‚Äî typically observed in the VAE framework. Pairing
    these representations with an autoregressive prior, the model can generate high
    quality images, videos, and speech as well as doing high quality speaker conversion
    and unsupervised learning of phonemes, providing further evidence of the utility
    of the learnt representations.*'
  prefs: []
  type: TYPE_NORMAL
- en: VQModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.VQModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L40)'
  prefs: []
  type: TYPE_NORMAL
- en: '( in_channels: int = 3 out_channels: int = 3 down_block_types: Tuple = (''DownEncoderBlock2D'',)
    up_block_types: Tuple = (''UpDecoderBlock2D'',) block_out_channels: Tuple = (64,)
    layers_per_block: int = 1 act_fn: str = ''silu'' latent_channels: int = 3 sample_size:
    int = 32 num_vq_embeddings: int = 256 norm_num_groups: int = 32 vq_embed_dim:
    Optional = None scaling_factor: float = 0.18215 norm_type: str = ''group'' mid_block_add_attention
    = True lookup_from_codebook = False force_upcast = False )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**in_channels** (int, *optional*, defaults to 3) ‚Äî Number of channels in the
    input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_channels** (int, *optional*, defaults to 3) ‚Äî Number of channels in the
    output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**down_block_types** (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`)
    ‚Äî Tuple of downsample block types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**up_block_types** (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`)
    ‚Äî Tuple of upsample block types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**block_out_channels** (`Tuple[int]`, *optional*, defaults to `(64,)`) ‚Äî Tuple
    of block output channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layers_per_block** (`int`, *optional*, defaults to `1`) ‚Äî Number of layers
    per block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**act_fn** (`str`, *optional*, defaults to `"silu"`) ‚Äî The activation function
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latent_channels** (`int`, *optional*, defaults to `3`) ‚Äî Number of channels
    in the latent space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sample_size** (`int`, *optional*, defaults to `32`) ‚Äî Sample input size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_vq_embeddings** (`int`, *optional*, defaults to `256`) ‚Äî Number of codebook
    vectors in the VQ-VAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**norm_num_groups** (`int`, *optional*, defaults to `32`) ‚Äî Number of groups
    for normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vq_embed_dim** (`int`, *optional*) ‚Äî Hidden dim of codebook vectors in the
    VQ-VAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scaling_factor** (`float`, *optional*, defaults to `0.18215`) ‚Äî The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**norm_type** (`str`, *optional*, defaults to `"group"`) ‚Äî Type of normalization
    layer to use. Can be one of `"group"` or `"spatial"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A VQ-VAE model for decoding latent representations.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for it‚Äôs generic methods implemented for all
    models (such as downloading or saving).
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L158)'
  prefs: []
  type: TYPE_NORMAL
- en: '( sample: FloatTensor return_dict: bool = True ) ‚Üí [VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**sample** (`torch.FloatTensor`) ‚Äî Input sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [models.vq_model.VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If return_dict is True, a [VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    is returned, otherwise a plain `tuple` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: The [VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel) forward
    method.
  prefs: []
  type: TYPE_NORMAL
- en: VQEncoderOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.vq_model.VQEncoderOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: '( latents: FloatTensor )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) ‚Äî The encoded output sample from the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output of VQModel encoding method.
  prefs: []
  type: TYPE_NORMAL
