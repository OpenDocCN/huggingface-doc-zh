["```py\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"google/ddpm-cifar10-32\", custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\", use_safetensors=True\n)\n```", "```py\nfrom diffusers import DiffusionPipeline\nfrom transformers import CLIPImageProcessor, CLIPModel\n\nclip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n\nfeature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)\nclip_model = CLIPModel.from_pretrained(clip_model_id)\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    custom_pipeline=\"clip_guided_stable_diffusion\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    use_safetensors=True,\n)\n```", "```py\nfrom transformers import T5Tokenizer, T5EncoderModel\n\npipe_id = \"showlab/show-1-base\"\ntokenizer = T5Tokenizer.from_pretrained(pipe_id, subfolder=\"tokenizer\")\ntext_encoder = T5EncoderModel.from_pretrained(pipe_id, subfolder=\"text_encoder\")\n```", "```py\nfrom diffusers import DPMSolverMultistepScheduler\n\nscheduler = DPMSolverMultistepScheduler.from_pretrained(pipe_id, subfolder=\"scheduler\")\n```", "```py\nfrom transformers import CLIPFeatureExtractor\n\nfeature_extractor = CLIPFeatureExtractor.from_pretrained(pipe_id, subfolder=\"feature_extractor\")\n```", "```py\nfrom showone_unet_3d_condition import ShowOneUNet3DConditionModel\n\nunet = ShowOneUNet3DConditionModel.from_pretrained(pipe_id, subfolder=\"unet\")\n```", "```py\nfrom pipeline_t2v_base_pixel import TextToVideoIFPipeline\nimport torch\n\npipeline = TextToVideoIFPipeline(\n    unet=unet,\n    text_encoder=text_encoder,\n    tokenizer=tokenizer,\n    scheduler=scheduler,\n    feature_extractor=feature_extractor\n)\npipeline = pipeline.to(device=\"cuda\")\npipeline.torch_dtype = torch.float16\n```", "```py\npipeline.push_to_hub(\"custom-t2v-pipeline\")\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"<change-username>/<change-id>\", trust_remote_code=True, torch_dtype=torch.float16\n).to(\"cuda\")\n\nprompt = \"hello\"\n\n# Text embeds\nprompt_embeds, negative_embeds = pipeline.encode_prompt(prompt)\n\n# Keyframes generation (8x64x40, 2fps)\nvideo_frames = pipeline(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    num_frames=8,\n    height=40,\n    width=64,\n    num_inference_steps=2,\n    guidance_scale=9.0,\n    output_type=\"pt\"\n).frames\n```", "```py\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/japanese-stable-diffusion-xl\", trust_remote_code=True\n)\npipeline.to(\"cuda\")\n\n# if using torch < 2.0\n# pipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"\u67f4\u72ac\u3001\u30ab\u30e9\u30d5\u30eb\u30a2\u30fc\u30c8\"\n\nimage = pipeline(prompt=prompt).images[0]\n\n```"]