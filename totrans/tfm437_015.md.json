["```py\npip install transformers bitsandbytes>=0.39.0 -q\n```", "```py\n>>> from transformers import AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\n...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n... )\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n>>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n```", "```py\n>>> generated_ids = model.generate(**model_inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'A list of colors: red, blue, green, yellow, orange, purple, pink,'\n```", "```py\n>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n>>> model_inputs = tokenizer(\n...     [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n... ).to(\"cuda\")\n>>> generated_ids = model.generate(**model_inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n'Portugal is a country in southwestern Europe, on the Iber']\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n>>> model = AutoModelForCausalLM.from_pretrained(\n...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n... )\n```", "```py\n>>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n\n>>> # By default, the output will contain up to 20 tokens\n>>> generated_ids = model.generate(**model_inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'A sequence of numbers: 1, 2, 3, 4, 5'\n\n>>> # Setting `max_new_tokens` allows you to control the maximum length\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'\n```", "```py\n>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility\n>>> from transformers import set_seed\n>>> set_seed(42)\n\n>>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n\n>>> # LLM + greedy decoding = repetitive, boring output\n>>> generated_ids = model.generate(**model_inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'I am a cat. I am a cat. I am a cat. I am a cat'\n\n>>> # With sampling, the output becomes more creative!\n>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'I am a cat.  Specifically, I am an indoor-only cat.  I'\n```", "```py\n>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,\n>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.\n>>> model_inputs = tokenizer(\n...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n... ).to(\"cuda\")\n>>> generated_ids = model.generate(**model_inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'1, 2, 33333333333'\n\n>>> # With left-padding, it works as expected!\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n>>> model_inputs = tokenizer(\n...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n... ).to(\"cuda\")\n>>> generated_ids = model.generate(**model_inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'1, 2, 3, 4, 5, 6,'\n```", "```py\n>>> tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n>>> model = AutoModelForCausalLM.from_pretrained(\n...     \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n... )\n>>> set_seed(0)\n>>> prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n>>> input_length = model_inputs.input_ids.shape[1]\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n\"I'm not a thug, but i can tell you that a human cannot eat\"\n>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write\n>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)\n\n>>> set_seed(0)\n>>> messages = [\n...     {\n...         \"role\": \"system\",\n...         \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n...     },\n...     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n... ]\n>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n>>> input_length = model_inputs.shape[1]\n>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n'None, you thug. How bout you try to focus on more useful questions?'\n>>> # As we can see, it followed a proper thug style \ud83d\ude0e\n```"]