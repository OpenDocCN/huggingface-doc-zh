["```py\nimport torch\nimport torch.distributed as dist\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n```", "```py\ndef run_inference(rank, world_size):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    pipe.to(rank)\n\n    if torch.distributed.get_rank() == 0:\n        prompt = \"a dog\"\n    elif torch.distributed.get_rank() == 1:\n        prompt = \"a cat\"\n\n    result = pipe(prompt).images[0]\n    result.save(f\"result_{rank}.png\")\n```", "```py\nfrom accelerate import PartialState  # Can also be Accelerator or AcceleratorState\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\ndistributed_state = PartialState()\npipe.to(distributed_state.device)\n\n# Assume two processes\nwith distributed_state.split_between_processes([\"a dog\", \"a cat\"]) as prompt:\n    result = pipe(prompt).images[0]\n    result.save(f\"result_{distributed_state.process_index}.png\")\n```", "```py\naccelerate launch distributed_inference.py\n```", "```py\naccelerate launch --config_file my_config.json distributed_inference.py\n```", "```py\naccelerate launch --num_processes 2 distributed_inference.py\n```", "```py\nfrom accelerate import PartialState  # Can also be Accelerator or AcceleratorState\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\ndistributed_state = PartialState()\npipe.to(distributed_state.device)\n\n# Assume two processes\nwith distributed_state.split_between_processes([\"a dog\", \"a cat\", \"a chicken\"], apply_padding=True) as prompt:\n    result = pipe(prompt).images\n```", "```py\npip install torchpippy\n```", "```py\nfrom transformers import GPT2ForSequenceClassification, GPT2Config\n\nconfig = GPT2Config()\nmodel = GPT2ForSequenceClassification(config)\nmodel.eval()\n```", "```py\ninput = torch.randint(\n    low=0,\n    high=config.vocab_size,\n    size=(2, 1024),  # bs x seq_len\n    device=\"cpu\",\n    dtype=torch.int64,\n    requires_grad=False,\n)\n```", "```py\nfrom accelerate.inference import prepare_pippy\nexample_inputs = {\"input_ids\": input}\nmodel = prepare_pippy(model, example_args=(input,))\n```", "```py\nargs = some_more_arguments\nwith torch.no_grad():\n    output = model(*args)\n```", "```py\nfrom accelerate import PartialState\nif PartialState().is_last_process:\n    print(output)\n```"]