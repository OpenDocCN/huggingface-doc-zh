- en: RWKV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The RWKV model was proposed in [this repo](https://github.com/BlinkDL/RWKV-LM)
  prefs: []
  type: TYPE_NORMAL
- en: 'It suggests a tweak in the traditional Transformer attention to make it linear.
    This way, the model can be used as recurrent network: passing inputs for timestamp
    0 and timestamp 1 together is the same as passing inputs at timestamp 0, then
    inputs at timestamp 1 along with the state of timestamp 0 (see example below).'
  prefs: []
  type: TYPE_NORMAL
- en: This can be more efficient than a regular Transformer and can deal with sentence
    of any length (even if the model uses a fixed context length for training).
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [sgugger](https://huggingface.co/sgugger). The
    original code can be found [here](https://github.com/BlinkDL/RWKV-LM).
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to make sure the model stops generating when `''\n\n''` is detected,
    we recommend using the following stopping criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: RwkvConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RwkvConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/configuration_rwkv.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 50277) — Vocabulary size of the
    RWKV model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_length` (`int`, *optional*, defaults to 1024) — The maximum sequence
    length that this model can be be used with in a single forward (using it in RNN
    mode lets use any sequence length).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimensionality of the
    embeddings and hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 32) — Number of hidden
    layers in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_hidden_size` (`int`, *optional*) — Dimensionality of the attention
    hidden states. Will default to `hidden_size` if unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*) — Dimensionality of the inner feed-forward
    layers. Will default to 4 times `hidden_size` if unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — The epsilon
    to use in the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 0) — The id of the beginning
    of sentence token in the vocabulary. Defaults to 0 as RWKV uses the same tokenizer
    as GPTNeoX.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 0) — The id of the end of sentence
    token in the vocabulary. Defaults to 0 as RWKV uses the same tokenizer as GPTNeoX.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_every` (`int`, *optional*, defaults to 6) — At inference, the hidden
    states (and weights of the correponding output layers) are divided by 2 every
    `rescale_every` layer. If set to 0 or a negative number, no rescale is done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether or
    not to tie the word embeddings with the input token embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel).
    It is used to instantiate a RWKV model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the RWVK-4 [RWKV/rwkv-4-169m-pile](https://huggingface.co/RWKV/rwkv-4-169m-pile)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: RwkvModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RwkvModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L592)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare RWKV Model transformer outputting raw hidden-states without any specific
    head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L617)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is currently not used by `RwkvModel`, but will be supported in the future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state` (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`, *optional*) — If passed along, the model uses the previous
    state in all the blocks (which will give the output for the `input_ids` provided
    as if the model add `state_input_ids + input_ids` as context).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, the last state is returned
    and can be used to quickly generate the next logits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.rwkv.modeling_rwkv.RwkvOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.rwkv.modeling_rwkv.RwkvOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state` (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`) — The state of the model at the last time step. Can be used
    in a forward method with the next `input_ids` to avoid providing the old `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: RwkvLMHeadModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RwkvForCausalLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L757)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RWKV Model transformer with a language modeling head on top (linear layer
    with weights tied to the input embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L795)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is currently not used by `RwkvModel`, but will be supported in the future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state` (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`, *optional*) — If passed along, the model uses the previous
    state in all the blocks (which will give the output for the `input_ids` provided
    as if the model add `state_input_ids + input_ids` as context).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, the last state is returned
    and can be used to quickly generate the next logits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state` (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`) — The state of the model at the last time step. Can be used
    in a forward method with the next `input_ids` to avoid providing the old `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [RwkvForCausalLM](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvForCausalLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Rwkv attention and the recurrent formulas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a traditional auto-regressive Transformer, attention is written as <math
    display="block"><semantics><mrow><mi>O</mi><mo>=</mo><mrow><mtext>softmax</mtext></mrow><mo
    stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt><mo
    stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">O
    = \hbox{softmax}(QK^{T} / \sqrt{d}) V</annotation></semantics></math>O=softmax(QKT/d​)V
  prefs: []
  type: TYPE_NORMAL
- en: with<math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math>Q,<math><semantics><mrow><mi>K</mi></mrow><annotation
    encoding="application/x-tex">K</annotation></semantics></math>K and<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V are matrices of
    shape `seq_len x hidden_size` named query, key and value (they are actually bigger
    matrices with a batch dimension and an attention head dimension but we’re only
    interested in the last two, which is where the matrix product is taken, so for
    the sake of simplicity we only consider those two). The product<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^{T}</annotation></semantics></math>QKT then has
    shape `seq_len x seq_len` and we can take the maxtrix product with<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V to get the output<math><semantics><mrow><mi>O</mi></mrow><annotation
    encoding="application/x-tex">O</annotation></semantics></math>O of the same shape
    as the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Replacing the softmax by its value gives: <math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi
    mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi
    mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \frac{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} /
    \sqrt{d}} V_{j}}{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \sqrt{d}}}</annotation></semantics></math>Oi​=∑j=1i​eQi​KjT​/d​∑j=1i​eQi​KjT​/d​Vj​​'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the entries in<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^{T}</annotation></semantics></math>QKT corresponding
    to<math><semantics><mrow><mi>j</mi><mo>></mo><mi>i</mi></mrow><annotation encoding="application/x-tex">j
    > i</annotation></semantics></math>j>i are masked (the sum stops at j) because
    the attention is not allowed to look at future tokens (only past ones).
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, the RWKV attention is given by <math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mi>σ</mi><mo
    stretchy="false">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>K</mi><mi>j</mi></msub></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>K</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \sigma(R_{i}) \frac{\sum_{j=1}^{i} e^{W_{i-j}
    + K_{j}} V_{j}}{\sum_{j=1}^{i} e^{W_{i-j} + K_{j}}}</annotation></semantics></math>Oi​=σ(Ri​)∑j=1i​eWi−j​+Kj​∑j=1i​eWi−j​+Kj​Vj​​
  prefs: []
  type: TYPE_NORMAL
- en: where<math><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math>R
    is a new matrix called receptance by the author,<math><semantics><mrow><mi>K</mi></mrow><annotation
    encoding="application/x-tex">K</annotation></semantics></math>K and<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V are still the
    key and value (\(\sigma\) here is the sigmoid function).<math><semantics><mrow><mi>W</mi></mrow><annotation
    encoding="application/x-tex">W</annotation></semantics></math>W is a new vector
    that represents the position of the token and is given by <math display="block"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>=</mo><mi>u</mi><mrow><mtext> and </mtext></mrow><msub><mi>W</mi><mi>k</mi></msub><mo>=</mo><mo
    stretchy="false">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi>w</mi><mrow><mtext> for </mtext></mrow><mi>k</mi><mo>≥</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">W_{0} = u \hbox{ and } W_{k} = (k-1)w \hbox{ for
    } k \geq 1</annotation></semantics></math>W0​=u and Wk​=(k−1)w for k≥1
  prefs: []
  type: TYPE_NORMAL
- en: 'with<math><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math>u
    and<math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math>w
    learnable parameters called in the code `time_first` and `time_decay` respectively.
    The numerator and denominator can both be expressed recursively. Naming them<math><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">N_{i}</annotation></semantics></math>Ni​ and<math><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">D_{i}</annotation></semantics></math>Di​ we have:
    <math display="block"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub></mrow></msup><msub><mi>V</mi><mi>i</mi></msub><mo>+</mo><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></msup><msub><mi>V</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow></msup><msub><mi>V</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub><mo>⋯</mo><mo>+</mo><msup><mi>e</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>K</mi><mn>1</mn></msub></mrow></msup><msub><mi>V</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">N_{i} = e^{u + K_{i}} V_{i} + \hat{N}_{i} \hbox{
    where } \hat{N}_{i} = e^{K_{i-1}} V_{i-1} + e^{w + K_{i-2}} V_{i-2} \cdots + e^{(i-2)w
    + K_{1}} V_{1}</annotation></semantics></math>Ni​=eu+Ki​Vi​+N^i​ where N^i​=eKi−1​Vi−1​+ew+Ki−2​Vi−2​⋯+e(i−2)w+K1​V1​'
  prefs: []
  type: TYPE_NORMAL
- en: so<math><semantics><mrow><msub><mover accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{N}_{i}</annotation></semantics></math>N^i​ (called
    `numerator_state` in the code) satistfies <math display="block"><semantics><mrow><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mi>j</mi></msub></msup><msub><mi>V</mi><mi>j</mi></msub><mo>+</mo><msup><mi>e</mi><mi>w</mi></msup><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{N}_{0} = 0 \hbox{ and } \hat{N}_{j+1} = e^{K_{j}}
    V_{j} + e^{w} \hat{N}_{j}</annotation></semantics></math>N^0​=0 and N^j+1​=eKj​Vj​+ewN^j​
  prefs: []
  type: TYPE_NORMAL
- en: and <math display="block"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow></msup><mo>⋯</mo><mo>+</mo><msup><mi>e</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>K</mi><mn>1</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">D_{i} = e^{u + K_{i}} + \hat{D}_{i} \hbox{ where
    } \hat{D}_{i} = e^{K_{i-1}} + e^{w + K_{i-2}} \cdots + e^{(i-2)w + K_{1}}</annotation></semantics></math>Di​=eu+Ki​+D^i​ where D^i​=eKi−1​+ew+Ki−2​⋯+e(i−2)w+K1​
  prefs: []
  type: TYPE_NORMAL
- en: so<math><semantics><mrow><msub><mover accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{D}_{i}</annotation></semantics></math>D^i​ (called
    `denominator_state` in the code) satistfies <math display="block"><semantics><mrow><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mi>j</mi></msub></msup><mo>+</mo><msup><mi>e</mi><mi>w</mi></msup><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{D}_{0} = 0 \hbox{ and } \hat{D}_{j+1} = e^{K_{j}}
    + e^{w} \hat{D}_{j}</annotation></semantics></math>D^0​=0 and D^j+1​=eKj​+ewD^j​
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual recurrent formula used are a tiny bit more complex, as for numerical
    stability we don’t want to compute exponentials of big numbers. Usually the softmax
    is not computed as is, but the exponential of the maximum term is divided of the
    numerator and denominator: <math display="block"><semantics><mrow><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>M</mi></mrow></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><mi>M</mi></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\frac{e^{x_{i}}}{\sum_{j=1}^{n} e^{x_{j}}} = \frac{e^{x_{i}
    - M}}{\sum_{j=1}^{n} e^{x_{j} - M}}</annotation></semantics></math>∑j=1n​exj​exi​​=∑j=1n​exj​−Mexi​−M​'
  prefs: []
  type: TYPE_NORMAL
- en: with<math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math>M
    the maximum of all<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">x_{j}</annotation></semantics></math>xj​. So here
    on top of saving the numerator state (\(\hat{N}\)) and the denominator state (\(\hat{D}\))
    we also keep track of the maximum of all terms encountered in the exponentials.
    So we actually use <math display="block"><semantics><mrow><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><msub><mi>M</mi><mi>i</mi></msub></mrow></msup><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><msub><mi>M</mi><mi>i</mi></msub></mrow></msup><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">\tilde{N}_{i} = e^{-M_{i}} \hat{N}_{i} \hbox{ and
    } \tilde{D}_{i} = e^{-M_{i}} \hat{D}_{i}</annotation></semantics></math>N~i​=e−Mi​N^i​ and D~i​=e−Mi​D^i​
  prefs: []
  type: TYPE_NORMAL
- en: 'defined by the following recurrent formulas: <math display="block"><semantics><mrow><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>j</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{N}_{0}
    = 0 \hbox{ and } \tilde{N}_{j+1} = e^{K_{j} - q} V_{j} + e^{w + M_{j} - q} \tilde{N}_{j}
    \hbox{ where } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>N~0​=0 and N~j+1​=eKj​−qVj​+ew+Mj​−qN~j​ where q=max(Kj​,w+Mj​)'
  prefs: []
  type: TYPE_NORMAL
- en: and <math display="block"><semantics><mrow><msub><mover accent="true"><mi>D</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{D}_{0}
    = 0 \hbox{ and } \tilde{D}_{j+1} = e^{K_{j} - q} + e^{w + M_{j} - q} \tilde{D}_{j}
    \hbox{ where } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>D~0​=0 and D~j+1​=eKj​−q+ew+Mj​−qD~j​ where q=max(Kj​,w+Mj​)
  prefs: []
  type: TYPE_NORMAL
- en: and<math><semantics><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>q</mi></mrow><annotation
    encoding="application/x-tex">M_{j+1} = q</annotation></semantics></math>Mj+1​=q.
    With those, we can then compute <math display="block"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>i</mi></msub><mo>+</mo><msup><mi>e</mi><msub><mi>M</mi><mi>i</mi></msub></msup><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">N_{i} = e^{u + K_{i} - q} V_{i} + e^{M_{i}} \tilde{N}_{i}
    \hbox{ where } q = \max(u + K_{i}, M_{i})</annotation></semantics></math>Ni​=eu+Ki​−qVi​+eMi​N~i​ where q=max(u+Ki​,Mi​)
  prefs: []
  type: TYPE_NORMAL
- en: and <math display="block"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><msub><mi>M</mi><mi>i</mi></msub></msup><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">D_{i} = e^{u + K_{i} - q} + e^{M_{i}} \tilde{D}_{i}
    \hbox{ where } q = \max(u + K_{i}, M_{i})</annotation></semantics></math>Di​=eu+Ki​−q+eMi​D~i​ where q=max(u+Ki​,Mi​)
  prefs: []
  type: TYPE_NORMAL
- en: which finally gives us <math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mi>σ</mi><mo
    stretchy="false">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mfrac><msub><mi>N</mi><mi>i</mi></msub><msub><mi>D</mi><mi>i</mi></msub></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \sigma(R_{i}) \frac{N_{i}}{D_{i}}</annotation></semantics></math>Oi​=σ(Ri​)Di​Ni​​
  prefs: []
  type: TYPE_NORMAL
