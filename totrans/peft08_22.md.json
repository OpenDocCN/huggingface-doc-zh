["```py\naccelerate config --config_file fsdp_config.yaml\n```", "```py\n`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\n`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP \n`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, \n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...\n`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`\n`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH\n`State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  \n```", "```py\ncommand_file: null\ncommands: null\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_offload_params: true\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: T5Block\ngpu_ids: null\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmegatron_lm_config: {}\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_name: null\ntpu_zone: null\nuse_cpu: false\n```", "```py\n def main():\n+    accelerator = Accelerator()\n     model_name_or_path = \"t5-base\"\n     base_path = \"temp/data/FinancialPhraseBank-v1.0\"\n+    peft_config = LoraConfig(\n         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n     )\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n+   model = get_peft_model(model, peft_config)\n```", "```py\nif getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\n    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n```", "```py\nmodel, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n)\n```", "```py\naccelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py\n```"]