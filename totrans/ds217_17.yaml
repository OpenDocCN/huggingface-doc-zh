- en: Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤„ç†
- en: 'Original text: [https://huggingface.co/docs/datasets/process](https://huggingface.co/docs/datasets/process)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/process](https://huggingface.co/docs/datasets/process)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Datasets provides many tools for modifying the structure and content of a
    dataset. These tools are important for tidying up a dataset, creating additional
    columns, converting between features and formats, and much more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Datasetsæä¾›äº†è®¸å¤šå·¥å…·æ¥ä¿®æ”¹æ•°æ®é›†çš„ç»“æ„å’Œå†…å®¹ã€‚è¿™äº›å·¥å…·å¯¹äºæ•´ç†æ•°æ®é›†ã€åˆ›å»ºé¢å¤–åˆ—ã€åœ¨ç‰¹å¾å’Œæ ¼å¼ä¹‹é—´è½¬æ¢ç­‰æ–¹é¢éå¸¸é‡è¦ã€‚
- en: 'This guide will show you how to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Reorder rows and split the dataset.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡æ–°æ’åºè¡Œå¹¶æ‹†åˆ†æ•°æ®é›†ã€‚
- en: Rename and remove columns, and other common column operations.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡å‘½åå’Œåˆ é™¤åˆ—ï¼Œä»¥åŠå…¶ä»–å¸¸è§çš„åˆ—æ“ä½œã€‚
- en: Apply processing functions to each example in a dataset.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹åº”ç”¨å¤„ç†å‡½æ•°ã€‚
- en: Concatenate datasets.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿æ¥æ•°æ®é›†ã€‚
- en: Apply a custom formatting transform.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨è‡ªå®šä¹‰æ ¼å¼è½¬æ¢ã€‚
- en: Save and export processed datasets.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿å­˜å’Œå¯¼å‡ºå¤„ç†è¿‡çš„æ•°æ®é›†ã€‚
- en: For more details specific to processing other dataset modalities, take a look
    at the [process audio dataset guide](./audio_process), the [process image dataset
    guide](./image_process), or the [process text dataset guide](./nlp_process).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å¤„ç†å…¶ä»–æ•°æ®é›†æ¨¡æ€çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å¤„ç†éŸ³é¢‘æ•°æ®é›†æŒ‡å—](./audio_process)ã€[å¤„ç†å›¾åƒæ•°æ®é›†æŒ‡å—](./image_process)æˆ–[å¤„ç†æ–‡æœ¬æ•°æ®é›†æŒ‡å—](./nlp_process)ã€‚
- en: The examples in this guide use the MRPC dataset, but feel free to load any dataset
    of your choice and follow along!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—ä¸­çš„ç¤ºä¾‹ä½¿ç”¨MRPCæ•°æ®é›†ï¼Œä½†è¯·éšæ„åŠ è½½æ‚¨é€‰æ‹©çš„ä»»ä½•æ•°æ®é›†å¹¶è·Ÿéšæ“ä½œï¼
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All processing methods in this guide return a new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object. Modification is not done in-place. Be careful about overriding your previous
    dataset!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—ä¸­çš„æ‰€æœ‰å¤„ç†æ–¹æ³•éƒ½ä¼šè¿”å›ä¸€ä¸ªæ–°çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ã€‚ä¿®æ”¹ä¸æ˜¯å°±åœ°è¿›è¡Œçš„ã€‚è¯·æ³¨æ„ä¸è¦è¦†ç›–æ‚¨ä¹‹å‰çš„æ•°æ®é›†ï¼
- en: Sort, shuffle, select, split, and shard
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ’åºã€æ´—ç‰Œã€é€‰æ‹©ã€æ‹†åˆ†å’Œåˆ†ç‰‡
- en: There are several functions for rearranging the structure of a dataset. These
    functions are useful for selecting only the rows you want, creating train and
    test splits, and sharding very large datasets into smaller chunks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ä¸ªå‡½æ•°ç”¨äºé‡æ–°æ’åˆ—æ•°æ®é›†çš„ç»“æ„ã€‚è¿™äº›å‡½æ•°å¯¹äºä»…é€‰æ‹©æ‚¨æƒ³è¦çš„è¡Œã€åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ä»¥åŠå°†éå¸¸å¤§çš„æ•°æ®é›†åˆ†æˆè¾ƒå°çš„å—éå¸¸æœ‰ç”¨ã€‚
- en: Sort
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ’åº
- en: Use [sort()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.sort)
    to sort column values according to their numerical values. The provided column
    must be NumPy compatible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[sort()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.sort)æ ¹æ®å…¶æ•°å€¼å€¼å¯¹åˆ—å€¼è¿›è¡Œæ’åºã€‚æä¾›çš„åˆ—å¿…é¡»æ˜¯NumPyå…¼å®¹çš„ã€‚
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Under the hood, this creates a list of indices that is sorted according to values
    of the column. This indices mapping is then used to access the right rows in the
    underlying Arrow table.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†…éƒ¨ï¼Œè¿™ä¼šåˆ›å»ºä¸€ä¸ªæ ¹æ®åˆ—çš„å€¼æ’åºçš„ç´¢å¼•åˆ—è¡¨ã€‚ç„¶åä½¿ç”¨è¿™ä¸ªç´¢å¼•æ˜ å°„æ¥è®¿é—®åº•å±‚Arrowè¡¨ä¸­çš„æ­£ç¡®è¡Œã€‚
- en: Shuffle
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ´—ç‰Œ
- en: The [shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)
    function randomly rearranges the column values. You can specify the `generator`
    parameter in this function to use a different `numpy.random.Generator` if you
    want more control over the algorithm used to shuffle the dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)å‡½æ•°ä¼šéšæœºé‡æ–°æ’åˆ—åˆ—å€¼ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å‡½æ•°ä¸­æŒ‡å®š`generator`å‚æ•°ï¼Œä»¥ä½¿ç”¨ä¸åŒçš„`numpy.random.Generator`æ¥æ›´å¥½åœ°æ§åˆ¶ç”¨äºæ´—ç‰Œæ•°æ®é›†çš„ç®—æ³•ã€‚'
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Shuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to
    create an indices mapping. However as soon as your [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    has an indices mapping, the speed can become 10x slower. This is because there
    is an extra step to get the row index to read using the indices mapping, and most
    importantly, you arenâ€™t reading contiguous chunks of data anymore. To restore
    the speed, youâ€™d need to rewrite the entire dataset on your disk again using [Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices),
    which removes the indices mapping. Alternatively, you can switch to an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    and leverage its fast approximate shuffling [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ´—ç‰Œä¼šå–å¾—ç´¢å¼•åˆ—è¡¨`[0:len(my_dataset)]`å¹¶å¯¹å…¶è¿›è¡Œæ´—ç‰Œä»¥åˆ›å»ºä¸€ä¸ªç´¢å¼•æ˜ å°„ã€‚ç„¶è€Œï¼Œä¸€æ—¦æ‚¨çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)æœ‰äº†ä¸€ä¸ªç´¢å¼•æ˜ å°„ï¼Œé€Ÿåº¦å¯èƒ½ä¼šå˜æ…¢10å€ã€‚è¿™æ˜¯å› ä¸ºéœ€è¦é¢å¤–çš„æ­¥éª¤æ¥ä½¿ç”¨ç´¢å¼•æ˜ å°„è·å–è¦è¯»å–çš„è¡Œç´¢å¼•ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œæ‚¨ä¸å†è¯»å–è¿ç»­çš„æ•°æ®å—ã€‚ä¸ºæ¢å¤é€Ÿåº¦ï¼Œæ‚¨éœ€è¦å†æ¬¡ä½¿ç”¨[Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices)å°†æ•´ä¸ªæ•°æ®é›†é‡å†™åˆ°ç£ç›˜ä¸Šï¼Œä»¥åˆ é™¤ç´¢å¼•æ˜ å°„ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥åˆ‡æ¢åˆ°[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)å¹¶åˆ©ç”¨å…¶å¿«é€Ÿçš„è¿‘ä¼¼æ´—ç‰Œ[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)ï¼š
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Select and Filter
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹©å’Œè¿‡æ»¤
- en: 'There are two options for filtering rows in a dataset: [select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)
    and [filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®é›†ä¸­æœ‰ä¸¤ç§è¿‡æ»¤è¡Œçš„é€‰é¡¹ï¼š[select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)å’Œ[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)ã€‚
- en: '[select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)
    returns rows according to a list of indices:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)æ ¹æ®ç´¢å¼•åˆ—è¡¨è¿”å›è¡Œï¼š'
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)
    returns rows that match a specified condition:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)è¿”å›ç¬¦åˆæŒ‡å®šæ¡ä»¶çš„è¡Œï¼š'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)
    can also filter by indices if you set `with_indices=True`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®`with_indices=True`æ¥æŒ‰ç´¢å¼•è¿›è¡Œè¿‡æ»¤ï¼š'
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Unless the list of indices to keep is contiguous, those methods also create
    an indices mapping under the hood.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éè¦ä¿ç•™çš„ç´¢å¼•åˆ—è¡¨æ˜¯è¿ç»­çš„ï¼Œå¦åˆ™è¿™äº›æ–¹æ³•åœ¨å†…éƒ¨ä¹Ÿä¼šåˆ›å»ºä¸€ä¸ªç´¢å¼•æ˜ å°„ã€‚
- en: Split
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‹†åˆ†
- en: 'The [train_test_split()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    function creates train and test splits if your dataset doesnâ€™t already have them.
    This allows you to adjust the relative proportions or an absolute number of samples
    in each split. In the example below, use the `test_size` parameter to create a
    test split that is 10% of the original dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_test_split()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.train_test_split)å‡½æ•°åœ¨æ•°æ®é›†æ²¡æœ‰è®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†æ—¶åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ã€‚è¿™å…è®¸æ‚¨è°ƒæ•´æ¯ä¸ªæ‹†åˆ†ä¸­çš„ç›¸å¯¹æ¯”ä¾‹æˆ–ç»å¯¹æ•°é‡çš„æ ·æœ¬ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨`test_size`å‚æ•°åˆ›å»ºä¸€ä¸ªåŸå§‹æ•°æ®é›†çš„10%çš„æµ‹è¯•æ‹†åˆ†ï¼š'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The splits are shuffled by default, but you can set `shuffle=False` to prevent
    shuffling.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹æ‹†åˆ†æ˜¯éšæœºçš„ï¼Œä½†æ‚¨å¯ä»¥è®¾ç½®`shuffle=False`æ¥é˜²æ­¢æ´—ç‰Œã€‚
- en: Shard
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡
- en: ğŸ¤— Datasets supports sharding to divide a very large dataset into a predefined
    number of chunks. Specify the `num_shards` parameter in [shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard)
    to determine the number of shards to split the dataset into. Youâ€™ll also need
    to provide the shard you want to return with the `index` parameter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— æ•°æ®é›†æ”¯æŒåˆ†ç‰‡ï¼Œå°†ä¸€ä¸ªéå¸¸å¤§çš„æ•°æ®é›†åˆ†æˆé¢„å®šä¹‰æ•°é‡çš„å—ã€‚åœ¨[shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard)ä¸­æŒ‡å®š`num_shards`å‚æ•°ä»¥ç¡®å®šè¦å°†æ•°æ®é›†åˆ†å‰²æˆçš„åˆ†ç‰‡æ•°ã€‚æ‚¨è¿˜éœ€è¦æä¾›è¦è¿”å›çš„åˆ†ç‰‡çš„`index`å‚æ•°ã€‚
- en: 'For example, the [imdb](https://huggingface.co/datasets/imdb) dataset has 25000
    examples:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ[imdb](https://huggingface.co/datasets/imdb)æ•°æ®é›†æœ‰25000ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After sharding the dataset into four chunks, the first shard will only have
    6250 examples:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†åˆ†æˆå››ä¸ªå—åï¼Œç¬¬ä¸€ä¸ªå—å°†åªæœ‰6250ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Rename, remove, cast, and flatten
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡å‘½åã€ç§»é™¤ã€è½¬æ¢å’Œå±•å¹³
- en: The following functions allow you to modify the columns of a dataset. These
    functions are useful for renaming or removing columns, changing columns to a new
    set of features, and flattening nested column structures.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å‡½æ•°å…è®¸æ‚¨ä¿®æ”¹æ•°æ®é›†çš„åˆ—ã€‚è¿™äº›å‡½æ•°å¯¹é‡å‘½åæˆ–ç§»é™¤åˆ—ã€å°†åˆ—æ›´æ”¹ä¸ºæ–°çš„ç‰¹å¾é›†ä»¥åŠå±•å¹³åµŒå¥—åˆ—ç»“æ„å¾ˆæœ‰ç”¨ã€‚
- en: Rename
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡å‘½å
- en: Use [rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)
    when you need to rename a column in your dataset. Features associated with the
    original column are actually moved under the new column name, instead of just
    replacing the original column in-place.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å½“éœ€è¦åœ¨æ•°æ®é›†ä¸­é‡å‘½ååˆ—æ—¶ï¼Œè¯·ä½¿ç”¨[rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)ã€‚ä¸åŸå§‹åˆ—ç›¸å…³çš„ç‰¹å¾å®é™…ä¸Šè¢«ç§»åŠ¨åˆ°æ–°åˆ—åä¸‹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›¿æ¢åŸå§‹åˆ—ã€‚
- en: 'Provide [rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)
    with the name of the original column, and the new column name:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸå§‹åˆ—çš„åç§°å’Œæ–°åˆ—åç§°æä¾›[rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)ï¼š
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remove
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç§»é™¤
- en: 'When you need to remove one or more columns, provide the column name to remove
    to the [remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)
    function. Remove more than one column by providing a list of column names:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å½“éœ€è¦ç§»é™¤ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—æ—¶ï¼Œæä¾›è¦ç§»é™¤çš„åˆ—åç»™[remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)å‡½æ•°ã€‚é€šè¿‡æä¾›åˆ—ååˆ—è¡¨æ¥ç§»é™¤å¤šä¸ªåˆ—ï¼š
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Conversely, [select_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select_columns)
    selects one or more columns to keep and removes the rest. This function takes
    either one or a list of column names:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œ[select_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select_columns)é€‰æ‹©è¦ä¿ç•™çš„ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—ï¼Œå¹¶ç§»é™¤å…¶ä½™åˆ—ã€‚æ­¤å‡½æ•°æ¥å—ä¸€ä¸ªæˆ–åˆ—ååˆ—è¡¨ï¼š
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Cast
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è½¬æ¢
- en: 'The [cast()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast)
    function transforms the feature type of one or more columns. This function accepts
    your new [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    as its argument. The example below demonstrates how to change the [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    and [Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)
    features:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[cast()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast)å‡½æ•°è½¬æ¢ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—çš„ç‰¹å¾ç±»å‹ã€‚æ­¤å‡½æ•°æ¥å—æ‚¨çš„æ–°[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)ä½œä¸ºå…¶å‚æ•°ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•æ›´æ”¹[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)å’Œ[Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)ç‰¹å¾ï¼š'
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Casting only works if the original feature type and new feature type are compatible.
    For example, you can cast a column with the feature type `Value("int32")` to `Value("bool")`
    if the original column only contains ones and zeros.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…å½“åŸå§‹ç‰¹å¾ç±»å‹å’Œæ–°ç‰¹å¾ç±»å‹å…¼å®¹æ—¶ï¼Œè½¬æ¢æ‰èµ·ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœåŸå§‹åˆ—ä»…åŒ…å«1å’Œ0ï¼Œåˆ™å¯ä»¥å°†å…·æœ‰ç‰¹å¾ç±»å‹`Value("int32")`çš„åˆ—è½¬æ¢ä¸º`Value("bool")`ã€‚
- en: 'Use the [cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)
    function to change the feature type of a single column. Pass the column name and
    its new feature type as arguments:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)å‡½æ•°æ¥æ›´æ”¹å•ä¸ªåˆ—çš„ç‰¹å¾ç±»å‹ã€‚å°†åˆ—åå’Œå…¶æ–°ç‰¹å¾ç±»å‹ä½œä¸ºå‚æ•°ä¼ é€’ï¼š
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Flatten
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å±•å¹³
- en: 'Sometimes a column can be a nested structure of several types. Take a look
    at the nested structure below from the SQuAD dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œä¸€åˆ—å¯ä»¥æ˜¯å‡ ç§ç±»å‹çš„åµŒå¥—ç»“æ„ã€‚çœ‹ä¸€ä¸‹æ¥è‡ªSQuADæ•°æ®é›†çš„ä¸‹é¢çš„åµŒå¥—ç»“æ„ï¼š
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `answers` field contains two subfields: `text` and `answer_start`. Use
    the [flatten()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten)
    function to extract the subfields into their own separate columns:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`answers`å­—æ®µåŒ…å«ä¸¤ä¸ªå­å­—æ®µï¼š`text`å’Œ`answer_start`ã€‚ä½¿ç”¨[flatten()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten)å‡½æ•°å°†å­å­—æ®µæå–åˆ°å®ƒä»¬è‡ªå·±çš„ç‹¬ç«‹åˆ—ä¸­ï¼š'
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Notice how the subfields are now their own independent columns: `answers.text`
    and `answers.answer_start`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„å­å­—æ®µç°åœ¨æ˜¯ç‹¬ç«‹çš„åˆ—ï¼š`answers.text`å’Œ`answers.answer_start`ã€‚
- en: Map
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜ å°„
- en: Some of the more powerful applications of ğŸ¤— Datasets come from using the [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    function. The primary purpose of [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    is to speed up processing functions. It allows you to apply a processing function
    to each example in a dataset, independently or in batches. This function can even
    create new rows and columns.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, prefix each `sentence1` value in the dataset with
    `''My sentence: ''`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a function that adds `''My sentence: ''` to the beginning
    of each sentence. The function needs to accept and output a `dict`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to apply the `add_prefix` function to the entire dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Letâ€™s take a look at another example, except this time, youâ€™ll remove a column
    with [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map).
    When you remove a column, it is only removed after the example has been provided
    to the mapped function. This allows the mapped function to use the content of
    the columns before they are removed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the column to remove with the `remove_columns` parameter in [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ğŸ¤— Datasets also has a [remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)
    function which is faster because it doesnâ€™t copy the data of the remaining columns.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with indices if you set `with_indices=True`. The example below adds the index
    to the beginning of each sentence:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Multiprocessing
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multiprocessing significantly speeds up processing by parallelizing processes
    on the CPU. Set the `num_proc` parameter in [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to set the number of processes to use:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    also works with the rank of the process if you set `with_rank=True`. This is analogous
    to the `with_indices` parameter. The `with_rank` parameter in the mapped function
    goes after the `index` one if it is already present.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The main use-case for rank is to parallelize computation across several GPUs.
    This requires setting `multiprocess.set_start_method("spawn")`. If you donâ€™t youâ€™ll
    receive the following CUDA error:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Batch processing
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    function supports working with batches of examples. Operate on batches by setting
    `batched=True`. The default batch size is 1000, but you can adjust it with the
    `batch_size` parameter. Batch processing enables interesting applications such
    as splitting long sentences into shorter chunks and data augmentation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Split long examples
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When examples are too long, you may want to split them into several smaller
    chunks. Begin by creating a function that:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Splits the `sentence1` field into chunks of 50 characters.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stacks all the chunks together to create the new dataset.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Apply the function with [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Notice how the sentences are split into shorter chunks now, and there are more
    rows in the dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Data augmentation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    function could also be used for data augmentation. The following example generates
    additional words for a masked token in a sentence.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Load and use the [RoBERTA](https://huggingface.co/roberta-base) model in ğŸ¤—
    Transformersâ€™ [FillMaskPipeline](https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Create a function to randomly select a word to mask in the sentence. The function
    should also return the original sentence and the top two replacements generated
    by RoBERTA.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to apply the function over the whole dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: For each original sentence, RoBERTA augmented a random word with three alternatives.
    The original word `distorting` is supplemented by `withholding`, `suppressing`,
    and `destroying`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Process multiple splits
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many datasets have splits that can be processed simultaneously with [DatasetDict.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.map).
    For example, tokenize the `sentence1` field in the train and test split by:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Distributed usage
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    in a distributed setting, you should also use [torch.distributed.barrier](https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier).
    This ensures the main process performs the mapping, while the other processes
    load the results, thereby avoiding duplicate work.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how you can use `torch.distributed.barrier` to
    synchronize the processes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Concatenate
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Separate datasets can be concatenated if they share the same column types.
    Concatenate datasets with [concatenate_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.concatenate_datasets):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can also concatenate two datasets horizontally by setting `axis=1` as long
    as the datasets have the same number of rows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Interleave
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also mix several datasets together by taking alternating examples from
    each one to create a new dataset. This is known as *interleaving*, which is enabled
    by the [interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)
    function. Both [interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)
    and [concatenate_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.concatenate_datasets)
    work with regular [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    and [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    objects. Refer to the [Stream](./stream#interleave) guide for an example of how
    to interleave [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    objects.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: You can define sampling probabilities for each of the original datasets to specify
    how to interleave the datasets. In this case, the new dataset is constructed by
    getting examples one by one from a random dataset until one of the datasets runs
    out of samples.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can also specify the `stopping_strategy`. The default strategy, `first_exhausted`,
    is a subsampling strategy, i.e the dataset construction is stopped as soon one
    of the dataset runs out of samples. You can specify `stopping_strategy=all_exhausted`
    to execute an oversampling strategy. In this case, the dataset construction is
    stopped as soon as every samples in every dataset has been added at least once.
    In practice, it means that if a dataset is exhausted, it will return to the beginning
    of this dataset until the stop criterion has been reached. Note that if no sampling
    probabilities are specified, the new dataset will have `max_length_datasets*nb_dataset
    samples`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Format
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    function changes the format of a column to be compatible with some common data
    formats. Specify the output youâ€™d like in the `type` parameter and the columns
    you want to format. Formatting is applied on-the-fly.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, create PyTorch tensors by setting `type="torch"`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The [with_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_format)
    function also changes the format of a column, except it returns a new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ğŸ¤— Datasets also provides support for other common data formats such as NumPy,
    Pandas, and JAX. Check out the [Using Datasets with TensorFlow](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset)
    guide for more details on how to efficiently create a TensorFlow dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to reset the dataset to its original format, use the [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format)
    function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Format transform
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function applies a custom formatting transform on-the-fly. This function replaces
    any previously specified format. For example, you can use this function to tokenize
    and pad tokens on-the-fly. Tokenization is only applied when examples are accessed:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can also use the [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function to decode formats not supported by [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features).
    For example, the [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature uses [`soundfile`](https://python-soundfile.readthedocs.io/en/0.11.0/)
    - a fast and simple library to install - but it does not provide support for less
    common audio formats. Here is where you can use [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)
    to apply a custom decoding transform on the fly. Youâ€™re free to use any library
    you like to decode the audio files.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'The example below uses the [`pydub`](http://pydub.com/) package to open an
    audio format not supported by `soundfile`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Save
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you are done processing your dataset, you can save and reuse it later with
    [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Save your dataset by providing the path to the directory you wish to save it
    to:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Use the [load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_from_disk)
    function to reload the dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Want to save your dataset to a cloud storage provider? Read our [Cloud Storage](./filesystems)
    guide to learn how to save your dataset to AWS or Google Cloud Storage.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Export
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ğŸ¤— Datasets supports exporting as well so you can work with your dataset in
    other applications. The following table shows currently supported file formats
    you can export to:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '| File type | Export method |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| CSV | [Dataset.to_csv()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_csv)
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| JSON | [Dataset.to_json()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_json)
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Parquet | [Dataset.to_parquet()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_parquet)
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| SQL | [Dataset.to_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_sql)
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| In-memory Python object | [Dataset.to_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_pandas)
    or [Dataset.to_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_dict)
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: 'For example, export your dataset to a CSV file like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
