- en: Load a dataset from the Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/datasets/load_hub](https://huggingface.co/docs/datasets/load_hub)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Finding high-quality datasets that are reproducible and accessible can be difficult.
    One of ðŸ¤— Datasets main goals is to provide a simple way to load a dataset of any
    format or type. The easiest way to get started is to discover an existing dataset
    on the [Hugging Face Hub](https://huggingface.co/datasets) - a community-driven
    collection of datasets for tasks in NLP, computer vision, and audio - and use
    ðŸ¤— Datasets to download and generate the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial uses the [rotten_tomatoes](https://huggingface.co/datasets/rotten_tomatoes)
    and [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) datasets, but feel
    free to load any dataset you want and follow along. Head over to the Hub now and
    find a dataset for your task!
  prefs: []
  type: TYPE_NORMAL
- en: Load a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you take the time to download a dataset, itâ€™s often helpful to quickly
    get some general information about a dataset. A datasetâ€™s information is stored
    inside [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    and can include information such as the dataset description, features, and dataset
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the [load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)
    function to load a dataset builder and inspect a datasetâ€™s attributes without
    committing to downloading it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If youâ€™re happy with the dataset, then load it with [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A split is a specific subset of a dataset like `train` and `test`. List a datasetâ€™s
    split names with the [get_dataset_split_names()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.get_dataset_split_names)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can load a specific split with the `split` parameter. Loading a dataset
    `split` returns a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you donâ€™t specify a `split`, ðŸ¤— Datasets returns a [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    object instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some datasets contain several sub-datasets. For example, the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset has several sub-datasets, each one containing audio data in a different
    language. These sub-datasets are known as *configurations*, and you must explicitly
    select one when loading the dataset. If you donâ€™t provide a configuration name,
    ðŸ¤— Datasets will raise a `ValueError` and remind you to choose a configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the [get_dataset_config_names()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.get_dataset_config_names)
    function to retrieve a list of all the possible configurations available to your
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then load the configuration you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Remote code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certain datasets repositories contain a loading script with the Python code
    used to generate the dataset. Those datasets are generally exported to Parquet
    by Hugging Face, so that ðŸ¤— Datasets can load the dataset fast and without running
    a loading script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if a Parquet export is not available, you can still use any dataset with
    Python code in its repository with `load_dataset`. All files and code uploaded
    to the Hub are scanned for malware (refer to the Hub security documentation for
    more information), but you should still review the dataset loading scripts and
    authors to avoid executing malicious code on your machine. You should set `trust_remote_code=True`
    to use a dataset with a loading script, or you will get a warning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the next major release, the new safety features of ðŸ¤— Datasets will disable
    running dataset loading scripts by default, and you will have to pass `trust_remote_code=True`
    to load datasets that require running a dataset script.
  prefs: []
  type: TYPE_NORMAL
