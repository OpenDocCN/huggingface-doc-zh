- en: Distributed inference with multiple GPUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/distributed_inference](https://huggingface.co/docs/diffusers/training/distributed_inference)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: On distributed setups, you can run inference across multiple GPUs with ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)
    or [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html),
    which is useful for generating with multiple prompts in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: This guide will show you how to use ðŸ¤— Accelerate and PyTorch Distributed for
    distributed inference.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) is a library designed
    to make it easy to train or run inference across distributed setups. It simplifies
    the process of setting up the distributed environment, allowing you to focus on
    your PyTorch code.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, create a Python file and initialize an [accelerate.PartialState](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState)
    to create a distributed environment; your setup is automatically detected so you
    donâ€™t need to explicitly define the `rank` or `world_size`. Move the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    to `distributed_state.device` to assign a GPU to each process.
  prefs: []
  type: TYPE_NORMAL
- en: Now use the [split_between_processes](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState.split_between_processes)
    utility as a context manager to automatically distribute the prompts between the
    number of processes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `--num_processes` argument to specify the number of GPUs to use, and
    call `accelerate launch` to run the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To learn more, take a look at the [Distributed Inference with ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Distributed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    which enables data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, create a Python file and import `torch.distributed` and `torch.multiprocessing`
    to set up the distributed process group and to spawn the processes for inference
    on each GPU. You should also initialize a [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Youâ€™ll want to create a function to run inference; [`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)
    handles creating a distributed environment with the type of backend to use, the
    `rank` of the current process, and the `world_size` or the number of processes
    participating. If youâ€™re running inference in parallel over 2 GPUs, then the `world_size`
    is 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Move the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    to `rank` and use `get_rank` to assign a GPU to each process, where each process
    handles a different prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)
    to run the `run_inference` function on the number of GPUs defined in `world_size`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once youâ€™ve completed the inference script, use the `--nproc_per_node` argument
    to specify the number of GPUs to use and call `torchrun` to run the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
