- en: ONNX Runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/onnx](https://huggingface.co/docs/diffusers/optimization/onnx)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ¤— [Optimum](https://github.com/huggingface/optimum) provides a Stable Diffusion
    pipeline compatible with ONNX Runtime. Youâ€™ll need to install ðŸ¤— Optimum with the
    following command for ONNX Runtime support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This guide will show you how to use the Stable Diffusion and Stable Diffusion
    XL (SDXL) pipelines with ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load and run inference, use the [ORTStableDiffusionPipeline](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTStableDiffusionPipeline).
    If you want to load a PyTorch model and convert it to the ONNX format on-the-fly,
    set `export=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Generating multiple prompts in a batch seems to take too much memory. While
    we look into it, you may need to iterate instead of batching.
  prefs: []
  type: TYPE_NORMAL
- en: 'To export the pipeline in the ONNX format offline and use it later for inference,
    use the [`optimum-cli export`](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then to perform inference (you donâ€™t have to specify `export=True` again):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fef3f7dc8a705f8b94622883fd936702.png)'
  prefs: []
  type: TYPE_IMG
- en: You can find more examples in ðŸ¤— Optimum [documentation](https://huggingface.co/docs/optimum/),
    and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion XL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load and run inference with SDXL, use the [ORTStableDiffusionXLPipeline](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTStableDiffusionXLPipeline):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To export the pipeline in the ONNX format and use it later for inference, use
    the [`optimum-cli export`](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: SDXL in the ONNX format is supported for text-to-image and image-to-image.
  prefs: []
  type: TYPE_NORMAL
