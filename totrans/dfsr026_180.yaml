- en: UniDiffuser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser](https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/84.5329dcbc.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
  prefs: []
  type: TYPE_NORMAL
- en: The UniDiffuser model was proposed in [One Transformer Fits All Distributions
    in Multi-Modal Diffusion at Scale](https://huggingface.co/papers/2303.06555) by
    Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue
    Cao, Hang Su, Jun Zhu.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*This paper proposes a unified diffusion framework (dubbed UniDiffuser) to
    fit all distributions relevant to a set of multi-modal data in one model. Our
    key insight is — learning diffusion models for marginal, conditional, and joint
    distributions can be unified as predicting the noise in the perturbed data, where
    the perturbation levels (i.e. timesteps) can be different for different modalities.
    Inspired by the unified view, UniDiffuser learns all distributions simultaneously
    with a minimal modification to the original diffusion model — perturbs data in
    all modalities instead of a single modality, inputs individual timesteps in different
    modalities, and predicts the noise of all modalities instead of a single modality.
    UniDiffuser is parameterized by a transformer for diffusion models to handle input
    types of different modalities. Implemented on large-scale paired image-text data,
    UniDiffuser is able to perform image, text, text-to-image, image-to-text, and
    image-text pair generation by setting proper timesteps without additional overhead.
    In particular, UniDiffuser is able to produce perceptually realistic samples in
    all tasks and its quantitative results (e.g., the FID and CLIP score) are not
    only superior to existing general-purpose models but also comparable to the bespoken
    models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image
    generation).*'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the original codebase at [thu-ml/unidiffuser](https://github.com/thu-ml/unidiffuser)
    and additional checkpoints at [thu-ml](https://huggingface.co/thu-ml).
  prefs: []
  type: TYPE_NORMAL
- en: There is currently an issue on PyTorch 1.X where the output images are all black
    or the pixel values become `NaNs`. This issue can be mitigated by switching to
    PyTorch 2.X.
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline was contributed by [dg845](https://github.com/dg845). ❤️
  prefs: []
  type: TYPE_NORMAL
- en: Usage Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because the UniDiffuser model is trained to model the joint distribution of
    (image, text) pairs, it is capable of performing a diverse range of generation
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Unconditional Image and Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unconditional generation (where we start from only latents sampled from a standard
    Gaussian prior) from a [UniDiffuserPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline)
    will produce a (image, text) pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is also called “joint” generation in the UniDiffuser paper, since we are
    sampling from the joint image-text distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the generation task is inferred from the inputs used when calling
    the pipeline. It is also possible to manually specify the unconditional generation
    task (“mode”) manually with [UniDiffuserPipeline.set_joint_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_joint_mode):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When the mode is set manually, subsequent calls to the pipeline will use the
    set mode without attempting to infer the mode. You can reset the mode with [UniDiffuserPipeline.reset_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.reset_mode),
    after which the pipeline will once again infer the mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also generate only an image or only text (which the UniDiffuser paper
    calls “marginal” generation since we sample from the marginal distribution of
    images and text, respectively):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Text-to-Image Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'UniDiffuser is also capable of sampling from conditional distributions; that
    is, the distribution of images conditioned on a text prompt or the distribution
    of texts conditioned on an image. Here is an example of sampling from the conditional
    image distribution (text-to-image generation or text-conditioned image generation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `text2img` mode requires that either an input `prompt` or `prompt_embeds`
    be supplied. You can set the `text2img` mode manually with [UniDiffuserPipeline.set_text_to_image_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_text_to_image_mode).
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, UniDiffuser can also produce text samples given an image (image-to-text
    or image-conditioned text generation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `img2text` mode requires that an input `image` be supplied. You can set
    the `img2text` mode manually with [UniDiffuserPipeline.set_image_to_text_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_image_to_text_mode).
  prefs: []
  type: TYPE_NORMAL
- en: Image Variation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The UniDiffuser authors suggest performing image variation through a “round-trip”
    generation method, where given an input image, we first perform an image-to-text
    generation, and then perform a text-to-image generation on the outputs of the
    first generation. This produces a new image which is semantically similar to the
    input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Text Variation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, text variation can be performed on an input prompt with a text-to-image
    generation followed by a image-to-text generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: UniDiffuserPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.UniDiffuserPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L51)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vae: AutoencoderKL text_encoder: CLIPTextModel image_encoder: CLIPVisionModelWithProjection
    clip_image_processor: CLIPImageProcessor clip_tokenizer: CLIPTokenizer text_decoder:
    UniDiffuserTextDecoder text_tokenizer: GPT2Tokenizer unet: UniDiffuserModel scheduler:
    KarrasDiffusionSchedulers )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vae** ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations. This is part of the UniDiffuser image representation along
    with the CLIP vision encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_encoder** (`CLIPVisionModel`) — A [CLIPVisionModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)
    to encode images as part of its image representation along with the VAE latent
    representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_processor** (`CLIPImageProcessor`) — [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    to preprocess an image before CLIP encoding it with `image_encoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clip_tokenizer** (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize the prompt before encoding it with `text_encoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_decoder** (`UniDiffuserTextDecoder`) — Frozen text decoder. This is
    a GPT-style model which is used to generate text from the UniDiffuser embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_tokenizer** (`GPT2Tokenizer`) — A [GPT2Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Tokenizer)
    to decode text for text generation; used along with the `text_decoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet** (`UniDiffuserModel`) — A [U-ViT](https://github.com/baofff/U-ViT)
    model with UNNet-style skip connections between transformer layers to denoise
    the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler** ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    and/or text latents. The original UniDiffuser paper uses the [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)
    scheduler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for a bimodal image-text model which supports unconditional text and
    image generation, text-conditioned image generation, image-conditioned text generation,
    and joint image-text generation.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L1079)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: Union = None image: Union = None height: Optional = None width: Optional
    = None data_type: Optional = 1 num_inference_steps: int = 50 guidance_scale: float
    = 8.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 num_prompts_per_image:
    Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None
    prompt_latents: Optional = None vae_latents: Optional = None clip_latents: Optional
    = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None
    output_type: Optional = ''pil'' return_dict: bool = True callback: Optional =
    None callback_steps: int = 1 ) → [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`. Required for
    text-conditioned image generation (`text2img`) mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image** (`torch.FloatTensor` or `PIL.Image.Image`, *optional*) — `Image`
    or tensor representing an image batch. Required for image-conditioned text generation
    (`img2text`) mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*, defaults to `self.unet.config.sample_size *
    self.vae_scale_factor`) — The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data_type** (`int`, *optional*, defaults to 1) — The data type (either 0
    or 1). Only used if you are loading a checkpoint which supports a data type embedding;
    this is added for compatibility with the [UniDiffuser-v1](https://huggingface.co/thu-ml/unidiffuser-v1)
    checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 50) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 8.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`). Used in text-conditioned image generation (`text2img`) mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt. Used in `text2img` (text-conditioned image generation)
    and `img` mode. If the mode is joint and both `num_images_per_prompt` and `num_prompts_per_image`
    are supplied, `min(num_images_per_prompt, num_prompts_per_image)` samples are
    generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_prompts_per_image** (`int`, *optional*, defaults to 1) — The number of
    prompts to generate per image. Used in `img2text` (image-conditioned text generation)
    and `text` mode. If the mode is joint and both `num_images_per_prompt` and `num_prompts_per_image`
    are supplied, `min(num_images_per_prompt, num_prompts_per_image)` samples are
    generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eta** (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) —
    A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for joint image-text
    generation. Can be used to tweak the same generation with different prompts. If
    not provided, a latents tensor is generated by sampling using the supplied random
    `generator`. This assumes a full set of VAE, CLIP, and text latents, if supplied,
    overrides the value of `prompt_latents`, `vae_latents`, and `clip_latents`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_latents** (`torch.FloatTensor`, *optional*) — Pre-generated noisy
    latents sampled from a Gaussian distribution, to be used as inputs for text generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vae_latents** (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clip_latents** (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument. Used in text-conditioned
    image generation (`text2img`) mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are be generated from the `negative_prompt`
    input argument. Used in text-conditioned image generation (`text2img`) mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback** (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_steps** (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of generated texts.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_vae_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L147)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_vae_tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_vae_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L139)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_vae_tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L155)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### encode_prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L382)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt
    = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None
    lora_scale: Optional = None clip_skip: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) — prompt to be encoded device
    — (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`) — number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_classifier_free_guidance** (`bool`) — whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lora_scale** (`float`, *optional*) — A LoRA scale that will be applied to
    all LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clip_skip** (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### reset_mode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L268)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Removes a manually set mode; after calling this, the pipeline will infer the
    mode from inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### set_image_mode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L252)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Manually set the generation mode to unconditional (“marginal”) image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### set_image_to_text_mode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L260)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Manually set the generation mode to image-conditioned text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### set_joint_mode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L264)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Manually set the generation mode to unconditional joint image-text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### set_text_mode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L248)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Manually set the generation mode to unconditional (“marginal”) text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### set_text_to_image_mode'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L256)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Manually set the generation mode to text-conditioned image generation.
  prefs: []
  type: TYPE_NORMAL
- en: ImageTextPipelineOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.ImageTextPipelineOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L33)'
  prefs: []
  type: TYPE_NORMAL
- en: '( images: Union text: Union )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**images** (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL
    images of length `batch_size` or NumPy array of shape `(batch_size, height, width,
    num_channels)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text** (`List[str]` or `List[List[str]]`) — List of generated text strings
    of length `batch_size` or a list of list of strings whose outer list has length
    `batch_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for joint image-text pipelines.
  prefs: []
  type: TYPE_NORMAL
