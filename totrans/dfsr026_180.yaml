- en: UniDiffuser
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser](https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The UniDiffuser model was proposed in [One Transformer Fits All Distributions
    in Multi-Modal Diffusion at Scale](https://huggingface.co/papers/2303.06555) by
    Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue
    Cao, Hang Su, Jun Zhu.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '*This paper proposes a unified diffusion framework (dubbed UniDiffuser) to
    fit all distributions relevant to a set of multi-modal data in one model. Our
    key insight is — learning diffusion models for marginal, conditional, and joint
    distributions can be unified as predicting the noise in the perturbed data, where
    the perturbation levels (i.e. timesteps) can be different for different modalities.
    Inspired by the unified view, UniDiffuser learns all distributions simultaneously
    with a minimal modification to the original diffusion model — perturbs data in
    all modalities instead of a single modality, inputs individual timesteps in different
    modalities, and predicts the noise of all modalities instead of a single modality.
    UniDiffuser is parameterized by a transformer for diffusion models to handle input
    types of different modalities. Implemented on large-scale paired image-text data,
    UniDiffuser is able to perform image, text, text-to-image, image-to-text, and
    image-text pair generation by setting proper timesteps without additional overhead.
    In particular, UniDiffuser is able to produce perceptually realistic samples in
    all tasks and its quantitative results (e.g., the FID and CLIP score) are not
    only superior to existing general-purpose models but also comparable to the bespoken
    models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image
    generation).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: You can find the original codebase at [thu-ml/unidiffuser](https://github.com/thu-ml/unidiffuser)
    and additional checkpoints at [thu-ml](https://huggingface.co/thu-ml).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: There is currently an issue on PyTorch 1.X where the output images are all black
    or the pixel values become `NaNs`. This issue can be mitigated by switching to
    PyTorch 2.X.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline was contributed by [dg845](https://github.com/dg845). ❤️
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Usage Examples
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because the UniDiffuser model is trained to model the joint distribution of
    (image, text) pairs, it is capable of performing a diverse range of generation
    tasks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Unconditional Image and Text Generation
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unconditional generation (where we start from only latents sampled from a standard
    Gaussian prior) from a [UniDiffuserPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline)
    will produce a (image, text) pair:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is also called “joint” generation in the UniDiffuser paper, since we are
    sampling from the joint image-text distribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the generation task is inferred from the inputs used when calling
    the pipeline. It is also possible to manually specify the unconditional generation
    task (“mode”) manually with [UniDiffuserPipeline.set_joint_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_joint_mode):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When the mode is set manually, subsequent calls to the pipeline will use the
    set mode without attempting to infer the mode. You can reset the mode with [UniDiffuserPipeline.reset_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.reset_mode),
    after which the pipeline will once again infer the mode.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also generate only an image or only text (which the UniDiffuser paper
    calls “marginal” generation since we sample from the marginal distribution of
    images and text, respectively):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Text-to-Image Generation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'UniDiffuser is also capable of sampling from conditional distributions; that
    is, the distribution of images conditioned on a text prompt or the distribution
    of texts conditioned on an image. Here is an example of sampling from the conditional
    image distribution (text-to-image generation or text-conditioned image generation):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `text2img` mode requires that either an input `prompt` or `prompt_embeds`
    be supplied. You can set the `text2img` mode manually with [UniDiffuserPipeline.set_text_to_image_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_text_to_image_mode).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-Text Generation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, UniDiffuser can also produce text samples given an image (image-to-text
    or image-conditioned text generation):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `img2text` mode requires that an input `image` be supplied. You can set
    the `img2text` mode manually with [UniDiffuserPipeline.set_image_to_text_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_image_to_text_mode).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Image Variation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The UniDiffuser authors suggest performing image variation through a “round-trip”
    generation method, where given an input image, we first perform an image-to-text
    generation, and then perform a text-to-image generation on the outputs of the
    first generation. This produces a new image which is semantically similar to the
    input image:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Text Variation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, text variation can be performed on an input prompt with a text-to-image
    generation followed by a image-to-text generation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: UniDiffuserPipeline
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.UniDiffuserPipeline`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L51)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations. This is part of the UniDiffuser image representation along
    with the CLIP vision encoding.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_encoder` (`CLIPVisionModel`) — A [CLIPVisionModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)
    to encode images as part of its image representation along with the VAE latent
    representation.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_processor` (`CLIPImageProcessor`) — [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    to preprocess an image before CLIP encoding it with `image_encoder`.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize the prompt before encoding it with `text_encoder`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_decoder` (`UniDiffuserTextDecoder`) — Frozen text decoder. This is a
    GPT-style model which is used to generate text from the UniDiffuser embedding.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_tokenizer` (`GPT2Tokenizer`) — A [GPT2Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Tokenizer)
    to decode text for text generation; used along with the `text_decoder`.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` (`UniDiffuserModel`) — A [U-ViT](https://github.com/baofff/U-ViT) model
    with UNNet-style skip connections between transformer layers to denoise the encoded
    image latents.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    and/or text latents. The original UniDiffuser paper uses the [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)
    scheduler.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for a bimodal image-text model which supports unconditional text and
    image generation, text-conditioned image generation, image-conditioned text generation,
    and joint image-text generation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L1079)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`. Required for
    text-conditioned image generation (`text2img`) mode.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor` or `PIL.Image.Image`, *optional*) — `Image` or
    tensor representing an image batch. Required for image-conditioned text generation
    (`img2text`) mode.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_type` (`int`, *optional*, defaults to 1) — The data type (either 0 or
    1). Only used if you are loading a checkpoint which supports a data type embedding;
    this is added for compatibility with the [UniDiffuser-v1](https://huggingface.co/thu-ml/unidiffuser-v1)
    checkpoint.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 8.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`). Used in text-conditioned image generation (`text2img`) mode.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt. Used in `text2img` (text-conditioned image generation)
    and `img` mode. If the mode is joint and both `num_images_per_prompt` and `num_prompts_per_image`
    are supplied, `min(num_images_per_prompt, num_prompts_per_image)` samples are
    generated.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_prompts_per_image` (`int`, *optional*, defaults to 1) — The number of
    prompts to generate per image. Used in `img2text` (image-conditioned text generation)
    and `text` mode. If the mode is joint and both `num_images_per_prompt` and `num_prompts_per_image`
    are supplied, `min(num_images_per_prompt, num_prompts_per_image)` samples are
    generated.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for joint image-text generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.
    This assumes a full set of VAE, CLIP, and text latents, if supplied, overrides
    the value of `prompt_latents`, `vae_latents`, and `clip_latents`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for text generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vae_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument. Used in text-conditioned
    image generation (`text2img`) mode.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are be generated from the `negative_prompt`
    input argument. Used in text-conditioned image generation (`text2img`) mode.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    instead of a plain tuple.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    or `tuple`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of generated texts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_slicing`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L147)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_tiling`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L164)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L139)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_tiling`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L155)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L382)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reset_mode`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L268)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Removes a manually set mode; after calling this, the pipeline will infer the
    mode from inputs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_image_mode`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L252)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Manually set the generation mode to unconditional (“marginal”) image generation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_image_to_text_mode`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L260)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Manually set the generation mode to image-conditioned text generation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_joint_mode`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L264)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Manually set the generation mode to unconditional joint image-text generation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_text_mode`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L248)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Manually set the generation mode to unconditional (“marginal”) text generation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_text_to_image_mode`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L256)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Manually set the generation mode to text-conditioned image generation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: ImageTextPipelineOutput
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.ImageTextPipelineOutput`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L33)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL
    图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。'
- en: '`text` (`List[str]` or `List[List[str]]`) — List of generated text strings
    of length `batch_size` or a list of list of strings whose outer list has length
    `batch_size`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`List[str]` or `List[List[str]]`) — 长度为 `batch_size` 的生成文本字符串列表或外部列表长度为
    `batch_size` 的字符串列表的列表。'
- en: Output class for joint image-text pipelines.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 联合图像文本管道的输出类。
