["```py\n( image_size = 224 spectrogram_length = 2048 frequency_length = 128 image_patch_size = [16, 16] audio_patch_size = [16, 16] num_image_channels = 3 num_audio_channels = 1 num_frames = 8 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-06 qkv_bias = True use_mean_pooling = False decoder_num_attention_heads = 16 decoder_hidden_size = 512 decoder_num_hidden_layers = 8 decoder_intermediate_size = 2048 pixel_mask_ratio = 0.75 audio_mask_ratio = 0.15 audio_mask_type = 'frame-level' task_matching = True task_mae = True loss_type = 'classification' **kwargs )\n```", "```py\n>>> from transformers import TvltConfig, TvltModel\n\n>>> # # Initializing a TVLT ZinengTang/tvlt-base style configuration\n>>> configuration = TvltConfig()\n\n>>> # # Initializing a model (with random weights) from the ZinengTang/tvlt-base style configuration\n>>> model = TvltModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor feature_extractor )\n```", "```py\n( images = None audio = None images_mixed = None sampling_rate = None mask_audio = False mask_pixel = False *args **kwargs )\n```", "```py\n( do_resize: bool = True size: Dict = None patch_size: List = [16, 16] num_frames: int = 8 resample: Resampling = <Resampling.BILINEAR: 2> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = [0.5, 0.5, 0.5] image_std: Union = [0.5, 0.5, 0.5] init_mask_generator = False **kwargs )\n```", "```py\n( videos: Union do_resize: bool = None size: Dict = None patch_size: List = None num_frames: int = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None is_mixed: bool = False return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs ) \u2192 export const metadata = 'undefined';BatchFeature\n```", "```py\n( spectrogram_length = 2048 num_channels = 1 patch_size = [16, 16] feature_size = 128 sampling_rate = 44100 hop_length_to_sampling_rate = 86 n_fft = 2048 padding_value = 0.0 **kwargs )\n```", "```py\n( raw_speech: Union return_tensors: Union = None return_attention_mask: Optional = True sampling_rate: Optional = None resample: bool = False mask_audio: bool = False **kwargs ) \u2192 export const metadata = 'undefined';BatchFeature\n```", "```py\n( config )\n```", "```py\n( pixel_values: FloatTensor audio_values: FloatTensor pixel_mask: Optional = None audio_mask: Optional = None mask_pixel: bool = False mask_audio: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.tvlt.modeling_tvlt.TvltModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import TvltProcessor, TvltModel\n>>> import numpy as np\n>>> import torch\n\n>>> num_frames = 8\n>>> images = list(np.random.randn(num_frames, 3, 224, 224))\n>>> audio = list(np.random.randn(10000))\n\n>>> processor = TvltProcessor.from_pretrained(\"ZinengTang/tvlt-base\")\n>>> model = TvltModel.from_pretrained(\"ZinengTang/tvlt-base\")\n\n>>> input_dict = processor(images, audio, sampling_rate=44100, return_tensors=\"pt\")\n\n>>> outputs = model(**input_dict)\n>>> loss = outputs.loss\n```", "```py\n( config )\n```", "```py\n( pixel_values: FloatTensor audio_values: FloatTensor pixel_mask: Optional = None audio_mask: Optional = None labels: Optional = None pixel_values_mixed: Optional = None pixel_mask_mixed: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.tvlt.modeling_tvlt.TvltForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import TvltProcessor, TvltForPreTraining\n>>> import numpy as np\n>>> import torch\n\n>>> num_frames = 8\n>>> images = list(np.random.randn(num_frames, 3, 224, 224))\n>>> images_mixed = list(np.random.randn(num_frames, 3, 224, 224))\n>>> audio = list(np.random.randn(10000))\n>>> processor = TvltProcessor.from_pretrained(\"ZinengTang/tvlt-base\")\n>>> model = TvltForPreTraining.from_pretrained(\"ZinengTang/tvlt-base\")\n>>> input_dict = processor(\n...     images, audio, images_mixed, sampling_rate=44100, mask_pixel=True, mask_audio=True, return_tensors=\"pt\"\n... )\n\n>>> outputs = model(**input_dict)\n>>> loss = outputs.loss\n```", "```py\n( config )\n```", "```py\n( pixel_values: FloatTensor audio_values: FloatTensor pixel_mask: Optional = None audio_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import TvltProcessor, TvltForAudioVisualClassification\n>>> import numpy as np\n>>> import torch\n\n>>> num_frames = 8\n>>> images = list(np.random.randn(num_frames, 3, 224, 224))\n>>> audio = list(np.random.randn(10000))\n>>> processor = TvltProcessor.from_pretrained(\"ZinengTang/tvlt-base\")\n>>> model = TvltForAudioVisualClassification.from_pretrained(\"ZinengTang/tvlt-base\")\n>>> input_dict = processor(images, audio, sampling_rate=44100, return_tensors=\"pt\")\n\n>>> outputs = model(**input_dict)\n>>> loss = outputs.loss\n```"]