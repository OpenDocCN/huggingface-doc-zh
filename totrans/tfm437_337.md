# BROS

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bros](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bros)

## 概述

BROS模型是由Teakgyu Hong、Donghyun Kim、Mingi Ji、Wonseok Hwang、Daehyun Nam、Sungrae Park在[BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539)中提出的。

BROS代表*BERT依赖空间性*。它是一个仅编码器的Transformer模型，接受一系列标记和它们的边界框作为输入，并输出一系列隐藏状态。BROS编码相对空间信息而不是使用绝对空间信息。

它通过两个目标进行预训练：BERT中使用的标记掩码语言建模目标（TMLM）和一种新颖的区域掩码语言建模目标（AMLM）。在TMLM中，标记被随机掩码，模型使用空间信息和其他未掩码的标记来预测掩码的标记。AMLM是TMLM的二维版本。它随机掩码文本标记，并使用与TMLM相同的信息进行预测，但它掩码文本块（区域）。

`BrosForTokenClassification`在BrosModel之上有一个简单的线性层。它预测每个标记的标签。`BrosSpadeEEForTokenClassification`在BrosModel之上有一个`initial_token_classifier`和`subsequent_token_classifier`。`initial_token_classifier`用于预测每个实体的第一个标记，`subsequent_token_classifier`用于预测实体内的下一个标记。`BrosSpadeELForTokenClassification`在BrosModel之上有一个`entity_linker`。`entity_linker`用于预测两个实体之间的关系。

`BrosForTokenClassification`和`BrosSpadeEEForTokenClassification`本质上执行相同的任务。然而，`BrosForTokenClassification`假设输入标记是完全串行化的（这是一个非常具有挑战性的任务，因为它们存在于二维空间），而`BrosSpadeEEForTokenClassification`允许更灵活地处理串行化错误，因为它从一个标记预测下一个连接标记。

`BrosSpadeELForTokenClassification`执行实体内链接任务。如果这两个实体共享某种关系，则它预测一个标记（一个实体）到另一个标记（另一个实体）的关系。

BROS在关键信息提取（KIE）基准测试中取得了可比较或更好的结果，如FUNSD、SROIE、CORD和SciTSR，而不依赖于显式的视觉特征。

论文摘要如下：

*从文档图像中提取关键信息（KIE）需要理解二维空间中文本的上下文和空间语义。许多最近的研究尝试通过开发专注于将文档图像的视觉特征与文本及其布局结合的预训练语言模型来解决该任务。另一方面，本文通过回归基本问题来解决问题：文本和布局的有效组合。具体而言，我们提出了一个名为BROS（BERT依赖空间性）的预训练语言模型，它编码了二维空间中文本的相对位置，并通过区域掩码策略从未标记的文档中学习。通过这种针对理解二维空间中文本的优化训练方案，BROS在四个KIE基准测试（FUNSD、SROIE、CORD和SciTSR）上显示出与先前方法相当或更好的性能，而不依赖于视觉特征。本文还揭示了KIE任务中的两个现实挑战-(1)减少由于不正确的文本排序而产生的错误和(2)有效地从更少的下游示例中学习-并展示了BROS相对于先前方法的优越性。*

这个模型是由[jinho8345](https://huggingface.co/jinho8345)贡献的。原始代码可以在[这里](https://github.com/clovaai/bros)找到。

## 用法提示和示例

+   [forward()](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel.forward) 需要 `input_ids` 和 `bbox`（边界框）。每个边界框应该以 (x0, y0, x1, y1) 格式（左上角，右下角）表示。边界框的获取取决于外部 OCR 系统。`x` 坐标应该通过文档图像宽度进行归一化，`y` 坐标应该通过文档图像高度进行归一化。

```py
def expand_and_normalize_bbox(bboxes, doc_width, doc_height):
    # here, bboxes are numpy array

    # Normalize bbox -> 0 ~ 1
    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] / width
    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] / height
```

+   [`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`] 需要不仅 `input_ids` 和 `bbox`，还需要 `box_first_token_mask` 用于损失计算。这是一个用于过滤每个框的非第一个标记的掩码。您可以通过保存从单词创建 `input_ids` 时的边界框的起始标记索引来获得此掩码。您可以使用以下代码生成 `box_first_token_mask`，

```py
def make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):

    box_first_token_mask = np.zeros(max_seq_length, dtype=np.bool_)

    # encode(tokenize) each word from words (List[str])
    input_ids_list: List[List[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]

    # get the length of each box
    tokens_length_list: List[int] = [len(l) for l in input_ids_list]

    box_end_token_indices = np.array(list(itertools.accumulate(tokens_length_list)))
    box_start_token_indices = box_end_token_indices - np.array(tokens_length_list)

    # filter out the indices that are out of max_seq_length
    box_end_token_indices = box_end_token_indices[box_end_token_indices < max_seq_length - 1]
    if len(box_start_token_indices) > len(box_end_token_indices):
        box_start_token_indices = box_start_token_indices[: len(box_end_token_indices)]

    # set box_start_token_indices to True
    box_first_token_mask[box_start_token_indices] = True

    return box_first_token_mask

```

## 资源

+   演示脚本可以在 [这里](https://github.com/clovaai/bros) 找到。

## BrosConfig

### `class transformers.BrosConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/configuration_bros.py#L29)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 dim_bbox = 8 bbox_scale = 100.0 n_relations = 1 classifier_dropout_prob = 0.1 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 30522) — Bros 模型的词汇表大小。定义了在调用 [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel) 或 `TFBrosModel` 时可以由 `inputs_ids` 表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢失比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024或2048）。

+   `type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用 [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel) 或 `TFBrosModel` 时传递的 `token_type_ids` 的词汇表大小。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。

+   `pad_token_id` (`int`, *optional*, defaults to 0) — 令牌词汇表中填充令牌的索引。

+   `dim_bbox` (`int`, *optional*, defaults to 8) — 边界框坐标的维度。 (x0, y1, x1, y0, x1, y1, x0, y1)

+   `bbox_scale` (`float`, *optional*, defaults to 100.0) — 边界框坐标的缩放因子。

+   `n_relations` (`int`, *optional*, defaults to 1) — SpadeEE（实体提取）、SpadeEL（实体链接）头部的关系数量。

+   `classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) — 分类器头部的丢失比率。

这是用于存储[BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)或`TFBrosModel`配置的配置类。根据指定的参数实例化一个Bros模型，定义模型架构。使用默认值实例化配置将产生类似于Bros [jinho8345/bros-base-uncased](https://huggingface.co/jinho8345/bros-base-uncased)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import BrosConfig, BrosModel

>>> # Initializing a BROS jinho8345/bros-base-uncased style configuration
>>> configuration = BrosConfig()

>>> # Initializing a model from the jinho8345/bros-base-uncased style configuration
>>> model = BrosModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## BrosProcessor

### `class transformers.BrosProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/processing_bros.py#L26)

```py
( tokenizer = None **kwargs )
```

参数

+   `tokenizer` (`BertTokenizerFast`, *可选*) — 一个[‘BertTokenizerFast`]的实例。这是一个必需的输入。

构建一个包装了BERT tokenizer的Bros处理器。

[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)提供了[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)的所有功能。查看[**call**()](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor.__call__)和`decode()`的文档字符串获取更多信息。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/processing_bros.py#L47)

```py
( text: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

此方法使用[BertTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)准备文本以供模型使用。

请参考上述两种方法的文档字符串获取更多信息。

## BrosModel

### `class transformers.BrosModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L784)

```py
( config add_pooling_layer = True )
```

参数

+   `config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸Bros模型变换器输出原始隐藏状态，没有特定的头部。这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L815)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox`（形状为`(batch_size, num_boxes, 4)`的‘torch.FloatTensor’） — 输入序列中每个标记的边界框坐标。每个边界框是四个值的列表（x1, y1, x2, y2），其中（x1, y1）是左上角，（x2, y2）是右下角的边界框。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充令牌索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   1表示未被“掩盖”的令牌，

    +   对于被“掩盖”的令牌为0。

    什么是注意力掩码？

+   `bbox_first_token_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于指示每个边界框的第一个令牌的掩码。选择的掩码值为`[0, 1]`：

    +   1表示未被“掩盖”的令牌，

    +   0表示被“掩盖”的令牌。

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 段令牌索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0对应于*句子A*令牌，

    +   1对应于*句子B*令牌。

    什么是令牌类型ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列令牌在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。选择的掩码值为`[0, 1]`：

    +   1表示头部未被“掩盖”。

    +   0表示头部被“掩盖”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这是很有用的，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列的输出。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后，序列的第一个令牌（分类令牌）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回通过线性层和tanh激活函数处理后的分类令牌。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    每层模型的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中，如果`config.is_encoder_decoder=True`，还可以使用）可用（见`past_key_values`输入）以加速顺序解码。

[BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import BrosProcessor, BrosModel

>>> processor = BrosProcessor.from_pretrained("jinho8345/bros-base-uncased")

>>> model = BrosModel.from_pretrained("jinho8345/bros-base-uncased")

>>> encoding = processor("Hello, my dog is cute", add_special_tokens=False, return_tensors="pt")
>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding["input_ids"].shape[-1], 1)
>>> encoding["bbox"] = bbox

>>> outputs = model(**encoding)
>>> last_hidden_states = outputs.last_hidden_state
```

## BrosForTokenClassification

### `class transformers.BrosForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L959)

```py
( config )
```

参数

+   `config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部带有标记分类头的Bros模型（隐藏状态输出的线性层），例如用于命名实体识别（NER）任务。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L982)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None bbox_first_token_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。

    可以使用 [BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `bbox` (‘torch.FloatTensor’ of shape ‘(batch_size, num_boxes, 4)’) — 输入序列中每个标记的边界框坐标。每个边界框都是四个值的列表（x1, y1, x2, y2），其中 (x1, y1) 是左上角，(x2, y2) 是右下角的边界框。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被 `masked` 的标记，

    +   0 表示被 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `bbox_first_token_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于指示每个边界框的第一个标记的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被 `masked` 的标记，

    +   0 表示被 `masked` 的标记。

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记的位置嵌入的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回) — 分类损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`) — 分类分数（SoftMax 之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个，+ 每一层的输出一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BrosForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import BrosProcessor, BrosForTokenClassification

>>> processor = BrosProcessor.from_pretrained("jinho8345/bros-base-uncased")

>>> model = BrosForTokenClassification.from_pretrained("jinho8345/bros-base-uncased")

>>> encoding = processor("Hello, my dog is cute", add_special_tokens=False, return_tensors="pt")
>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding["input_ids"].shape[-1], 1)
>>> encoding["bbox"] = bbox

>>> outputs = model(**encoding)
```

## BrosSpadeEEForTokenClassification

### `class transformers.BrosSpadeEEForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1063)

```py
( config )
```

参数

+   `config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Bros模型在顶部带有一个标记分类头（在隐藏状态输出的顶部有初始标记层和后续标记层），例如用于命名实体识别（NER）任务。初始标记分类器用于预测每个实体的第一个标记，后续标记分类器用于预测实体内的后续标记。与BrosForTokenClassification相比，该模型对序列化错误更加稳健，因为它从一个标记预测下一个标记。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1101)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None bbox_first_token_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None initial_token_labels: Optional = None subsequent_token_labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.bros.modeling_bros.BrosSpadeOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox`（形状为`(batch_size, num_boxes, 4)`的`torch.FloatTensor`）- 输入序列中每个标记的边界框坐标。每个边界框是四个值的列表（x1，y1，x2，y2），其中（x1，y1）是左上角，（x2，y2）是边界框的右下角。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于`未屏蔽`的标记为1，

    +   对于`已屏蔽`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `bbox_first_token_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*optional*) — 用于指示每个边界框的第一个标记的掩码。掩码值选在 `[0, 1]`：

    +   1 表示未被 `masked` 的标记，

    +   0 表示被 `masked` 的标记。

+   `token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引选在 `[0, 1]`：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选在范围 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选在 `[0, 1]`：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

`transformers.models.bros.modeling_bros.BrosSpadeOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.bros.modeling_bros.BrosSpadeOutput` 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False` 时）包含根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入不同元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*optional*，当提供 `labels` 时返回） — 分类损失。

+   `initial_token_logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.num_labels)`) — 实体初始标记的分类分数（SoftMax 之前）。

+   `subsequent_token_logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, sequence_length+1)`) — 实体序列标记的分类分数（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每个层的输出）。

    每层模型的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

[BrosSpadeEEForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosSpadeEEForTokenClassification) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import BrosProcessor, BrosSpadeEEForTokenClassification

>>> processor = BrosProcessor.from_pretrained("jinho8345/bros-base-uncased")

>>> model = BrosSpadeEEForTokenClassification.from_pretrained("jinho8345/bros-base-uncased")

>>> encoding = processor("Hello, my dog is cute", add_special_tokens=False, return_tensors="pt")
>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding["input_ids"].shape[-1], 1)
>>> encoding["bbox"] = bbox

>>> outputs = model(**encoding)
```

## BrosSpadeELForTokenClassification

### `class transformers.BrosSpadeELForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1209)

```py
( config )
```

参数

+   `config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在隐藏状态输出的顶部有一个标记分类头的Bros模型（在隐藏状态输出的顶部有一个实体链接层），例如用于实体链接。实体链接器用于预测实体之间的内部实体链接（一个实体到另一个实体）。

这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1233)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None bbox_first_token_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    输入ID是什么？

+   `bbox`（形状为`(batch_size, num_boxes, 4)`的‘torch.FloatTensor’）- 输入序列中每个标记的边界框坐标。每个边界框是一个包含四个值（x1，y1，x2，y2）的列表，其中（x1，y1）是左上角，（x2，y2）是边界框的右下角。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被屏蔽的标记为1，

    +   对于被屏蔽的标记为0。

    注意掩码是什么？

+   `bbox_first_token_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于指示每个边界框的第一个标记的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被屏蔽的标记为1，

    +   对于被屏蔽的标记为0。

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    令牌类型ID是什么？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 位置嵌入中每个输入序列标记的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    位置ID是什么？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被屏蔽。

    +   0表示头部被屏蔽。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 分类损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.num_labels)`） — 分类得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）的形状为`(batch_size, sequence_length, hidden_size)`。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BrosSpadeELForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosSpadeELForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import BrosProcessor, BrosSpadeELForTokenClassification

>>> processor = BrosProcessor.from_pretrained("jinho8345/bros-base-uncased")

>>> model = BrosSpadeELForTokenClassification.from_pretrained("jinho8345/bros-base-uncased")

>>> encoding = processor("Hello, my dog is cute", add_special_tokens=False, return_tensors="pt")
>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding["input_ids"].shape[-1], 1)
>>> encoding["bbox"] = bbox

>>> outputs = model(**encoding)
```
