# 将您的代码迁移到 🤗 Accelerate

> 原文链接：[https://huggingface.co/docs/accelerate/basic_tutorials/migration](https://huggingface.co/docs/accelerate/basic_tutorials/migration)

本教程将详细介绍如何轻松将现有的 PyTorch 代码转换为使用 🤗 Accelerate！您将看到，通过仅更改几行代码，🤗 Accelerate 就可以发挥其魔力，让您轻松地将代码运行在分布式系统上！

## 基本训练循环

首先，编写一个非常基本的 PyTorch 训练循环。

我们假设 `training_dataloader`、`model`、`optimizer`、`scheduler` 和 `loss_function` 已经事先定义好。

```py
device = "cuda"
model.to(device)

for batch in training_dataloader:
    optimizer.zero_grad()
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    loss.backward()
    optimizer.step()
    scheduler.step()
```

## 添加 🤗 Accelerate

要开始使用 🤗 Accelerate，请首先导入并创建一个 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator) 实例：

```py
from accelerate import Accelerator

accelerator = Accelerator()
```

[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator) 是利用分布式训练的所有可能选项的主要力量！

### 设置正确的设备

[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator) 类知道任何时候将任何 PyTorch 对象移动到正确设备，因此您应该将 `device` 的定义更改为来自 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)：

```py
- device = 'cuda'
+ device = accelerator.device
  model.to(device)
```

### 准备您的对象

接下来，您需要将所有与训练相关的重要对象传递给 [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)。🤗 Accelerate 将确保一切都在当前环境中设置好，以便您开始训练：

```py
model, optimizer, training_dataloader, scheduler = accelerator.prepare(
 model, optimizer, training_dataloader, scheduler
)
```

这些对象以发送顺序返回。默认情况下，当使用 `device_placement=True` 时，所有可以发送到正确设备的对象都会被发送。如果您需要处理未传递给 [~Accelerator.prepare] 但应该在活动设备上的数据，您应该传入之前创建的 `device`。

Accelerate 仅会准备从各自的 PyTorch 类继承的对象（如 `torch.optim.Optimizer`）。

### 修改训练循环

最后，在训练循环中需要更改三行代码。🤗 Accelerate 的 DataLoader 类将默认自动处理设备放置，并且应使用 [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward) 来执行反向传播：

```py
-   inputs = inputs.to(device)
-   targets = targets.to(device)
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
-   loss.backward()
+   accelerator.backward(loss)
```

有了这些，您的训练循环现在已准备好使用 🤗 Accelerate！

## 完成的代码

以下是转换后代码的最终版本：

```py
from accelerate import Accelerator

accelerator = Accelerator()

model, optimizer, training_dataloader, scheduler = accelerator.prepare(
    model, optimizer, training_dataloader, scheduler
)

for batch in training_dataloader:
    optimizer.zero_grad()
    inputs, targets = batch
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    scheduler.step()
```

## 更多资源

要了解更多关于如何迁移到 🤗 Accelerate 的方法，请查看我们的 [交互式迁移教程](https://huggingface.co/docs/accelerate/usage_guides/explore)，展示了在使用 Accelerate 时需要注意的其他项目以及如何快速实现。
