["```py\nfrom diffusers.utils import load_image, make_image_grid\n\nimage = load_image(\"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_ref.png\")\n```", "```py\nfrom PIL import Image\n\ncolor_palette = image.resize((8, 8))\ncolor_palette = color_palette.resize((512, 512), resample=Image.Resampling.NEAREST)\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_color_sd14v1\", torch_dtype=torch.float16)\npipe = StableDiffusionAdapterPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    adapter=adapter,\n    torch_dtype=torch.float16,\n)\npipe.to(\"cuda\")\n```", "```py\n# fix the random seed, so you will get the same result as the example\ngenerator = torch.Generator(\"cuda\").manual_seed(7)\n\nout_image = pipe(\n    \"At night, glowing cubes in front of the beach\",\n    image=color_palette,\n    generator=generator,\n).images[0]\nmake_image_grid([image, color_palette, out_image], rows=1, cols=3)\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\n\nsketch_image = load_image(\"https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch.png\").convert(\"L\")\n```", "```py\nimport torch\nfrom diffusers import (\n    T2IAdapter,\n    StableDiffusionXLAdapterPipeline,\n    DDPMScheduler\n)\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nadapter = T2IAdapter.from_pretrained(\"Adapter/t2iadapter\", subfolder=\"sketch_sdxl_1.0\", torch_dtype=torch.float16, adapter_type=\"full_adapter_xl\")\nscheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    model_id, adapter=adapter, safety_checker=None, torch_dtype=torch.float16, variant=\"fp16\", scheduler=scheduler\n)\n\npipe.to(\"cuda\")\n```", "```py\n# fix the random seed, so you will get the same result as the example\ngenerator = torch.Generator().manual_seed(42)\n\nsketch_image_out = pipe(\n    prompt=\"a photo of a dog in real world, high quality\",\n    negative_prompt=\"extra digit, fewer digits, cropped, worst quality, low quality\",\n    image=sketch_image,\n    generator=generator,\n    guidance_scale=7.5\n).images[0]\nmake_image_grid([sketch_image, sketch_image_out], rows=1, cols=2)\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\n\ncond_keypose = load_image(\n    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png\"\n)\ncond_depth = load_image(\n    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png\"\n)\ncond = [cond_keypose, cond_depth]\n\nprompt = [\"A man walking in an office room with a nice view\"]\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionAdapterPipeline, MultiAdapter, T2IAdapter\n\nadapters = MultiAdapter(\n    [\n        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_keypose_sd14v1\"),\n        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_depth_sd14v1\"),\n    ]\n)\nadapters = adapters.to(torch.float16)\n\npipe = StableDiffusionAdapterPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    torch_dtype=torch.float16,\n    adapter=adapters,\n).to(\"cuda\")\n\nimage = pipe(prompt, cond, adapter_conditioning_scale=[0.8, 0.8]).images[0]\nmake_image_grid([cond_keypose, cond_depth, image], rows=1, cols=3)\n```", "```py\n>>> from PIL import Image\n>>> from diffusers.utils import load_image\n>>> import torch\n>>> from diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n\n>>> image = load_image(\n...     \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_ref.png\"\n... )\n\n>>> color_palette = image.resize((8, 8))\n>>> color_palette = color_palette.resize((512, 512), resample=Image.Resampling.NEAREST)\n\n>>> adapter = T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_color_sd14v1\", torch_dtype=torch.float16)\n>>> pipe = StableDiffusionAdapterPipeline.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\",\n...     adapter=adapter,\n...     torch_dtype=torch.float16,\n... )\n\n>>> pipe.to(\"cuda\")\n\n>>> out_image = pipe(\n...     \"At night, glowing cubes in front of the beach\",\n...     image=color_palette,\n... ).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\n>>> import torch\n>>> from diffusers import T2IAdapter, StableDiffusionXLAdapterPipeline, DDPMScheduler\n>>> from diffusers.utils import load_image\n\n>>> sketch_image = load_image(\"https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch.png\").convert(\"L\")\n\n>>> model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n\n>>> adapter = T2IAdapter.from_pretrained(\n...     \"Adapter/t2iadapter\",\n...     subfolder=\"sketch_sdxl_1.0\",\n...     torch_dtype=torch.float16,\n...     adapter_type=\"full_adapter_xl\",\n... )\n>>> scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n\n>>> pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n...     model_id, adapter=adapter, torch_dtype=torch.float16, variant=\"fp16\", scheduler=scheduler\n... ).to(\"cuda\")\n\n>>> generator = torch.manual_seed(42)\n>>> sketch_image_out = pipe(\n...     prompt=\"a photo of a dog in real world, high quality\",\n...     negative_prompt=\"extra digit, fewer digits, cropped, worst quality, low quality\",\n...     image=sketch_image,\n...     generator=generator,\n...     guidance_scale=7.5,\n... ).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```"]