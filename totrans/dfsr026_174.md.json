["```py\nfrom diffusers.utils import load_image, make_image_grid\n\nimage = load_image(\"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_ref.png\")\n```", "```py\nfrom PIL import Image\n\ncolor_palette = image.resize((8, 8))\ncolor_palette = color_palette.resize((512, 512), resample=Image.Resampling.NEAREST)\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_color_sd14v1\", torch_dtype=torch.float16)\npipe = StableDiffusionAdapterPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    adapter=adapter,\n    torch_dtype=torch.float16,\n)\npipe.to(\"cuda\")\n```", "```py\n# fix the random seed, so you will get the same result as the example\ngenerator = torch.Generator(\"cuda\").manual_seed(7)\n\nout_image = pipe(\n    \"At night, glowing cubes in front of the beach\",\n    image=color_palette,\n    generator=generator,\n).images[0]\nmake_image_grid([image, color_palette, out_image], rows=1, cols=3)\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\n\nsketch_image = load_image(\"https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch.png\").convert(\"L\")\n```", "```py\nimport torch\nfrom diffusers import (\n    T2IAdapter,\n    StableDiffusionXLAdapterPipeline,\n    DDPMScheduler\n)\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nadapter = T2IAdapter.from_pretrained(\"Adapter/t2iadapter\", subfolder=\"sketch_sdxl_1.0\", torch_dtype=torch.float16, adapter_type=\"full_adapter_xl\")\nscheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    model_id, adapter=adapter, safety_checker=None, torch_dtype=torch.float16, variant=\"fp16\", scheduler=scheduler\n)\n\npipe.to(\"cuda\")\n```", "```py\n# fix the random seed, so you will get the same result as the example\ngenerator = torch.Generator().manual_seed(42)\n\nsketch_image_out = pipe(\n    prompt=\"a photo of a dog in real world, high quality\",\n    negative_prompt=\"extra digit, fewer digits, cropped, worst quality, low quality\",\n    image=sketch_image,\n    generator=generator,\n    guidance_scale=7.5\n).images[0]\nmake_image_grid([sketch_image, sketch_image_out], rows=1, cols=2)\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\n\ncond_keypose = load_image(\n    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png\"\n)\ncond_depth = load_image(\n    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png\"\n)\ncond = [cond_keypose, cond_depth]\n\nprompt = [\"A man walking in an office room with a nice view\"]\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionAdapterPipeline, MultiAdapter, T2IAdapter\n\nadapters = MultiAdapter(\n    [\n        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_keypose_sd14v1\"),\n        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_depth_sd14v1\"),\n    ]\n)\nadapters = adapters.to(torch.float16)\n\npipe = StableDiffusionAdapterPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    torch_dtype=torch.float16,\n    adapter=adapters,\n).to(\"cuda\")\n\nimage = pipe(prompt, cond, adapter_conditioning_scale=[0.8, 0.8]).images[0]\nmake_image_grid([cond_keypose, cond_depth, image], rows=1, cols=3)\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel adapter: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None adapter_conditioning_scale: Union = 1.0 clip_skip: Optional = None ) \u2192 export const metadata = 'undefined';~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput or tuple\n```", "```py\n>>> from PIL import Image\n>>> from diffusers.utils import load_image\n>>> import torch\n>>> from diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n\n>>> image = load_image(\n...     \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_ref.png\"\n... )\n\n>>> color_palette = image.resize((8, 8))\n>>> color_palette = color_palette.resize((512, 512), resample=Image.Resampling.NEAREST)\n\n>>> adapter = T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_color_sd14v1\", torch_dtype=torch.float16)\n>>> pipe = StableDiffusionAdapterPipeline.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\",\n...     adapter=adapter,\n...     torch_dtype=torch.float16,\n... )\n\n>>> pipe.to(\"cuda\")\n\n>>> out_image = pipe(\n...     \"At night, glowing cubes in front of the beach\",\n...     image=color_palette,\n... ).images[0]\n```", "```py\n( slice_size: Union = 'auto' )\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( attention_op: Optional = None )\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( w embedding_dim = 512 dtype = torch.float32 ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel adapter: Union scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )\n```", "```py\n( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: List = None denoising_end: Optional = None guidance_scale: float = 5.0 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Optional = None crops_coords_top_left: Tuple = (0, 0) target_size: Optional = None negative_original_size: Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size: Optional = None adapter_conditioning_scale: Union = 1.0 adapter_conditioning_factor: float = 1.0 clip_skip: Optional = None ) \u2192 export const metadata = 'undefined';~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import T2IAdapter, StableDiffusionXLAdapterPipeline, DDPMScheduler\n>>> from diffusers.utils import load_image\n\n>>> sketch_image = load_image(\"https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch.png\").convert(\"L\")\n\n>>> model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n\n>>> adapter = T2IAdapter.from_pretrained(\n...     \"Adapter/t2iadapter\",\n...     subfolder=\"sketch_sdxl_1.0\",\n...     torch_dtype=torch.float16,\n...     adapter_type=\"full_adapter_xl\",\n... )\n>>> scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n\n>>> pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n...     model_id, adapter=adapter, torch_dtype=torch.float16, variant=\"fp16\", scheduler=scheduler\n... ).to(\"cuda\")\n\n>>> generator = torch.manual_seed(42)\n>>> sketch_image_out = pipe(\n...     prompt=\"a photo of a dog in real world, high quality\",\n...     negative_prompt=\"extra digit, fewer digits, cropped, worst quality, low quality\",\n...     image=sketch_image,\n...     generator=generator,\n...     guidance_scale=7.5,\n... ).images[0]\n```", "```py\n( slice_size: Union = 'auto' )\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( attention_op: Optional = None )\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( w embedding_dim = 512 dtype = torch.float32 ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```"]