["```py\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n).images[0]\n```", "```py\nimport torch\nfrom diffusers import AutoPipelineForImage2Image, UNet2DConditionModel, LCMScheduler\nfrom diffusers.utils import make_image_grid, load_image\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"SimianLuo/LCM_Dreamshaper_v7\",\n    subfolder=\"unet\",\n    torch_dtype=torch.float16,\n)\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    unet=unet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt,\n    image=init_image,\n    num_inference_steps=4,\n    guidance_scale=7.5,\n    strength=0.5,\n    generator=generator\n).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n\nprompt = \"papercut, a cute fox\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n).images[0]\nimage\n```", "```py\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((512, 512))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"SimianLuo/LCM_Dreamshaper_v7\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    \"the mona lisa\",\n    image=canny_image,\n    num_inference_steps=4,\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```", "```py\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# Prepare image\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1216))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    unet=unet,\n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\", \n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=5,\n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\ngrid = make_image_grid([canny_image, image], rows=1, cols=2)\n```"]