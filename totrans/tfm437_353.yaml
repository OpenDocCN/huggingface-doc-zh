- en: LayoutLMv3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LayoutLMv3
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv3)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv3)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The LayoutLMv3 model was proposed in [LayoutLMv3: Pre-training for Document
    AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan
    Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei. LayoutLMv3 simplifies [LayoutLMv2](layoutlmv2)
    by using patch embeddings (as in [ViT](vit)) instead of leveraging a CNN backbone,
    and pre-trains the model on 3 objectives: masked language modeling (MLM), masked
    image modeling (MIM) and word-patch alignment (WPA).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'LayoutLMv3æ¨¡å‹ç”±Yupan Huangã€Tengchao Lvã€Lei Cuiã€Yutong Luã€Furu Weiåœ¨[LayoutLMv3:
    Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387)ä¸­æå‡ºã€‚LayoutLMv3é€šè¿‡ä½¿ç”¨è¡¥ä¸åµŒå…¥ï¼ˆå¦‚[ViT](vit)ä¸­çš„æ–¹å¼ï¼‰ç®€åŒ–äº†[LayoutLMv2](layoutlmv2)ï¼Œå¹¶åœ¨3ä¸ªç›®æ ‡ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒï¼šæ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ã€æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå•è¯-è¡¥ä¸å¯¹é½ï¼ˆWPAï¼‰ã€‚'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Self-supervised pre-training techniques have achieved remarkable progress
    in Document AI. Most multimodal pre-trained models use a masked language modeling
    objective to learn bidirectional representations on the text modality, but they
    differ in pre-training objectives for the image modality. This discrepancy adds
    difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3
    to pre-train multimodal Transformers for Document AI with unified text and image
    masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective
    to learn cross-modal alignment by predicting whether the corresponding image patch
    of a text word is masked. The simple unified architecture and training objectives
    make LayoutLMv3 a general-purpose pre-trained model for both text-centric and
    image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves
    state-of-the-art performance not only in text-centric tasks, including form understanding,
    receipt understanding, and document visual question answering, but also in image-centric
    tasks such as document image classification and document layout analysis.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*è‡ªç›‘ç£é¢„è®­ç»ƒæŠ€æœ¯åœ¨æ–‡æ¡£AIé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å¤§å¤šæ•°å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡ç›®æ ‡æ¥å­¦ä¹ æ–‡æœ¬æ¨¡æ€ä¸Šçš„åŒå‘è¡¨ç¤ºï¼Œä½†å®ƒä»¬åœ¨å›¾åƒæ¨¡æ€çš„é¢„è®­ç»ƒç›®æ ‡ä¸Šæœ‰æ‰€ä¸åŒã€‚è¿™ç§å·®å¼‚å¢åŠ äº†å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„éš¾åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LayoutLMv3ï¼Œç”¨äºä¸ºæ–‡æ¡£AIé¢„è®­ç»ƒå¤šæ¨¡æ€Transformersï¼Œç»Ÿä¸€æ–‡æœ¬å’Œå›¾åƒæ©ç ã€‚æ­¤å¤–ï¼ŒLayoutLMv3è¿˜ä½¿ç”¨äº†å•è¯-è¡¥ä¸å¯¹é½ç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡é¢„æµ‹æ–‡æœ¬å•è¯çš„ç›¸åº”å›¾åƒè¡¥ä¸æ˜¯å¦è¢«æ©ç æ¥å­¦ä¹ è·¨æ¨¡æ€å¯¹é½ã€‚ç®€å•çš„ç»Ÿä¸€æ¶æ„å’Œè®­ç»ƒç›®æ ‡ä½¿LayoutLMv3æˆä¸ºæ–‡æœ¬ä¸­å¿ƒå’Œå›¾åƒä¸­å¿ƒæ–‡æ¡£AIä»»åŠ¡çš„é€šç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutLMv3ä¸ä»…åœ¨æ–‡æœ¬ä¸­å¿ƒä»»åŠ¡ï¼ˆå¦‚è¡¨å•ç†è§£ã€æ”¶æ®ç†è§£å’Œæ–‡æ¡£è§†è§‰é—®ç­”ï¼‰ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨å›¾åƒä¸­å¿ƒä»»åŠ¡ï¼ˆå¦‚æ–‡æ¡£å›¾åƒåˆ†ç±»å’Œæ–‡æ¡£å¸ƒå±€åˆ†æï¼‰ä¸­ä¹Ÿå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚*'
- en: '![drawing](../Images/af2a6f9e13d097092393e586cc161f9d.png) LayoutLMv3 architecture.
    Taken from the [original paper](https://arxiv.org/abs/2204.08387).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/af2a6f9e13d097092393e586cc161f9d.png) LayoutLMv3æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2204.08387)ã€‚'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow
    version of this model was added by [chriskoo](https://huggingface.co/chriskoo),
    [tokec](https://huggingface.co/tokec), and [lre](https://huggingface.co/lre).
    The original code can be found [here](https://github.com/microsoft/unilm/tree/master/layoutlmv3).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚è¯¥æ¨¡å‹çš„TensorFlowç‰ˆæœ¬ç”±[chriskoo](https://huggingface.co/chriskoo)ã€[tokec](https://huggingface.co/tokec)å’Œ[lre](https://huggingface.co/lre)æ·»åŠ ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/microsoft/unilm/tree/master/layoutlmv3)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: 'In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2](layoutlmv2),
    except that:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®å¤„ç†æ–¹é¢ï¼ŒLayoutLMv3ä¸å…¶å‰èº«[LayoutLMv2](layoutlmv2)ç›¸åŒï¼Œåªæ˜¯ï¼š
- en: images need to be resized and normalized with channels in regular RGB format.
    LayoutLMv2 on the other hand normalizes the images internally and expects the
    channels in BGR format.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒéœ€è¦è°ƒæ•´å¤§å°å¹¶ä½¿ç”¨å¸¸è§„RGBæ ¼å¼çš„é€šé“è¿›è¡Œå½’ä¸€åŒ–ã€‚å¦ä¸€æ–¹é¢ï¼ŒLayoutLMv2åœ¨å†…éƒ¨å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æœŸæœ›é€šé“ä»¥BGRæ ¼å¼æä¾›ã€‚
- en: text is tokenized using byte-pair encoding (BPE), as opposed to WordPiece. Due
    to these differences in data preprocessing, one can use [LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)
    which internally combines a [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)
    (for the image modality) and a [LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)/[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)
    (for the text modality) to prepare all data for the model.
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ä½¿ç”¨å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰è¿›è¡Œæ ‡è®°åŒ–ï¼Œè€Œä¸æ˜¯WordPieceã€‚ç”±äºæ•°æ®é¢„å¤„ç†ä¸­çš„è¿™äº›å·®å¼‚ï¼Œå¯ä»¥ä½¿ç”¨[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)ï¼Œå®ƒå†…éƒ¨ç»“åˆäº†[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)ï¼ˆç”¨äºå›¾åƒæ¨¡æ€ï¼‰å’Œ[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)/[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)ï¼ˆç”¨äºæ–‡æœ¬æ¨¡æ€ï¼‰æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ‰€æœ‰æ•°æ®ã€‚
- en: Regarding usage of [LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor),
    we refer to the [usage guide](layoutlmv2#usage-layoutlmv2processor) of its predecessor.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äº[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬å‚è€ƒå…¶å‰èº«çš„[ä½¿ç”¨æŒ‡å—](layoutlmv2#usage-layoutlmv2processor)ã€‚
- en: Resources
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with LayoutLMv3\. If youâ€™re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and weâ€™ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceå®˜æ–¹å’Œç¤¾åŒºï¼ˆğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨LayoutLMv3ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥æœ€å¥½å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: LayoutLMv3 is nearly identical to LayoutLMv2, so weâ€™ve also included LayoutLMv2
    resources you can adapt for LayoutLMv3 tasks. For these notebooks, take care to
    use [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    instead when preparing data for the model!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv3å‡ ä¹ä¸LayoutLMv2ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬è¿˜åŒ…å«äº†æ‚¨å¯ä»¥ä¸ºLayoutLMv3ä»»åŠ¡è°ƒæ•´çš„LayoutLMv2èµ„æºã€‚åœ¨å‡†å¤‡æ¨¡å‹æ•°æ®æ—¶ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)ï¼
- en: Demo notebooks for LayoutLMv3 can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LayoutLMv3).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LayoutLMv3çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LayoutLMv3)æ‰¾åˆ°ã€‚
- en: Demo scripts can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¼”ç¤ºè„šæœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3)æ‰¾åˆ°ã€‚
- en: Text Classification
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»
- en: '[LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)
    is supported by this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb)æ”¯æŒ[LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)ã€‚
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)'
- en: Token Classification
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ†ç±»
- en: '[LayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3)
    and [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb)æ”¯æŒ[LayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification)ã€‚
- en: A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb)
    for how to perform inference with [LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)
    and a [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb)
    for how to perform inference when no labels are available with [LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)è¿›è¡Œæ¨æ–­çš„[ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb)ï¼Œä»¥åŠä¸€ä¸ªå…³äºå¦‚ä½•åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ä½¿ç”¨[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)è¿›è¡Œæ¨æ–­çš„[ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb)ã€‚
- en: A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb)
    for how to finetune [LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)
    with the ğŸ¤— Trainer.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ğŸ¤— Trainerå¯¹[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)è¿›è¡Œå¾®è°ƒçš„[ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb)ã€‚
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)'
- en: Question Answering
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”
- en: '[LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering)
    is supported by this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb)æ”¯æŒ[LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering)ã€‚
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)'
- en: '**Document question answering**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ–‡æ¡£é—®ç­”**'
- en: '[Document question answering task guide](../tasks/document_question_answering)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æ¡£é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/document_question_answering)'
- en: LayoutLMv3Config
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3Config
- en: '### `class transformers.LayoutLMv3Config`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/configuration_layoutlmv3.py#L40)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/configuration_layoutlmv3.py#L40)'
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 50265) â€” Vocabulary size of the
    LayoutLMv3 model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, é»˜è®¤ä¸º50265) â€” LayoutLMv3æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimension of the encoder
    layers and the pooler layer.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, é»˜è®¤ä¸º768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­éšè—å±‚çš„æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º 12) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimension of the
    â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, é»˜è®¤ä¸º 3072) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` æˆ– `function`, *optional*, é»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ
    `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) â€” The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º 512) â€” è¯¥æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512æˆ–1024æˆ–2048ï¼‰ã€‚'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) â€” The vocabulary size
    of the `token_type_ids` passed when calling [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º 2) â€” åœ¨è°ƒç”¨ [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)
    æ—¶ä¼ é€’çš„ `token_type_ids` çš„è¯æ±‡è¡¨å¤§å°ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º 1e-5) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`max_2d_position_embeddings` (`int`, *optional*, defaults to 1024) â€” The maximum
    value that the 2D position embedding might ever be used with. Typically set this
    to something large just in case (e.g., 1024).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_2d_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º 1024) â€” 2Dä½ç½®åµŒå…¥å¯èƒ½ä½¿ç”¨çš„æœ€å¤§å€¼ã€‚é€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ1024ï¼‰ã€‚'
- en: '`coordinate_size` (`int`, *optional*, defaults to `128`) â€” Dimension of the
    coordinate embeddings.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coordinate_size` (`int`, *optional*, é»˜è®¤ä¸º `128`) â€” åæ ‡åµŒå…¥çš„ç»´åº¦ã€‚'
- en: '`shape_size` (`int`, *optional*, defaults to `128`) â€” Dimension of the width
    and height embeddings.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shape_size` (`int`, *optional*, é»˜è®¤ä¸º `128`) â€” å®½åº¦å’Œé«˜åº¦åµŒå…¥çš„ç»´åº¦ã€‚'
- en: '`has_relative_attention_bias` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to use a relative attention bias in the self-attention mechanism.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_relative_attention_bias` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨ç›¸å¯¹æ³¨æ„åŠ›åç½®ã€‚'
- en: '`rel_pos_bins` (`int`, *optional*, defaults to 32) â€” The number of relative
    position bins to be used in the self-attention mechanism.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rel_pos_bins` (`int`, *optional*, é»˜è®¤ä¸º 32) â€” åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨çš„ç›¸å¯¹ä½ç½®æ¡¶çš„æ•°é‡ã€‚'
- en: '`max_rel_pos` (`int`, *optional*, defaults to 128) â€” The maximum number of
    relative positions to be used in the self-attention mechanism.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_rel_pos` (`int`, *optional*, é»˜è®¤ä¸º 128) â€” åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨çš„æœ€å¤§ç›¸å¯¹ä½ç½®æ•°ã€‚'
- en: '`max_rel_2d_pos` (`int`, *optional*, defaults to 256) â€” The maximum number
    of relative 2D positions in the self-attention mechanism.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_rel_2d_pos` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æœ€å¤§2Dç›¸å¯¹ä½ç½®æ•°ã€‚'
- en: '`rel_2d_pos_bins` (`int`, *optional*, defaults to 64) â€” The number of 2D relative
    position bins in the self-attention mechanism.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rel_2d_pos_bins` (`int`, *optional*, é»˜è®¤ä¸º 64) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„2Dç›¸å¯¹ä½ç½®æ¡¶çš„æ•°é‡ã€‚'
- en: '`has_spatial_attention_bias` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to use a spatial attention bias in the self-attention mechanism.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_spatial_attention_bias` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨ç©ºé—´æ³¨æ„åç½®ã€‚'
- en: '`visual_embed` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    add patch embeddings.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`visual_embed` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ·»åŠ è¡¥ä¸åµŒå…¥ã€‚'
- en: '`input_size` (`int`, *optional*, defaults to `224`) â€” The size (resolution)
    of the images.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size` (`int`, *optional*, é»˜è®¤ä¸º `224`) â€” å›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`num_channels` (`int`, *optional*, defaults to `3`) â€” The number of channels
    of the images.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, é»˜è®¤ä¸º `3`) â€” å›¾åƒçš„é€šé“æ•°ã€‚'
- en: '`patch_size` (`int`, *optional*, defaults to `16`) â€” The size (resolution)
    of the patches.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, é»˜è®¤ä¸º `16`) â€” è¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`classifier_dropout` (`float`, *optional*) â€” The dropout ratio for the classification
    head.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout` (`float`, *optional*) â€” åˆ†ç±»å¤´çš„dropoutæ¯”ç‡ã€‚'
- en: This is the configuration class to store the configuration of a [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model).
    It is used to instantiate an LayoutLMv3 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the LayoutLMv3 [microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base)
    architecture.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)
    çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª LayoutLMv3 æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº LayoutLMv3 [microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: LayoutLMv3FeatureExtractor
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3FeatureExtractor
- en: '### `class transformers.LayoutLMv3FeatureExtractor`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3FeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/feature_extraction_layoutlmv3.py#L28)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/feature_extraction_layoutlmv3.py#L28)'
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `__call__`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess an image or a batch of images.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: LayoutLMv3ImageProcessor
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3ImageProcessor
- en: '### `class transformers.LayoutLMv3ImageProcessor`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3ImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py#L95)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py#L95)'
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    imageâ€™s (height, width) dimensions to `(size["height"], size["width"])`. Can be
    overridden by `do_resize` in `preprocess`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ (é«˜åº¦ï¼Œå®½åº¦) å°ºå¯¸è°ƒæ•´ä¸º `(size["height"],
    size["width"])`ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `do_resize` è¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 224, "width":
    224}`): Size of the image after resizing. Can be overridden by `size` in `preprocess`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *optional*, é»˜è®¤ä¸º `{"height" -- 224, "width": 224}`):
    è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `size` è¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`)
    â€” Resampling filter to use if resizing the image. Can be overridden by `resample`
    in `preprocess`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, é»˜è®¤ä¸º `PILImageResampling.BILINEAR`)
    â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œè¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `resample` è¦†ç›–ã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) â€” Whether to rescale
    the imageâ€™s pixel values by the specified `rescale_value`. Can be overridden by
    `do_rescale` in `preprocess`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šçš„ `rescale_value` é‡æ–°ç¼©æ”¾å›¾åƒçš„åƒç´ å€¼ã€‚å¯ä»¥è¢«
    `preprocess` ä¸­çš„ `do_rescale` è¦†ç›–ã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to 1 / 255) â€” Value by which
    the imageâ€™s pixel values are rescaled. Can be overridden by `rescale_factor` in
    `preprocess`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, é»˜è®¤ä¸º 1 / 255) â€” å›¾åƒçš„åƒç´ å€¼è¢«é‡æ–°ç¼©æ”¾çš„å€¼ã€‚å¯ä»¥è¢« `preprocess`
    ä¸­çš„ `rescale_factor` è¦†ç›–ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œè§„èŒƒåŒ–ã€‚å¯ä»¥è¢« `preprocess`
    æ–¹æ³•ä¸­çš„ `do_normalize` å‚æ•°è¦†ç›–ã€‚'
- en: '`image_mean` (`Iterable[float]` or `float`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    â€” Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`Iterable[float]` æˆ– `float`, *optional*, é»˜è®¤ä¸º `IMAGENET_STANDARD_MEAN`)
    â€” å¦‚æœè§„èŒƒåŒ–å›¾åƒè¦ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_mean` å‚æ•°è¦†ç›–ã€‚'
- en: '`image_std` (`Iterable[float]` or `float`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    â€” Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`Iterable[float]` æˆ– `float`, *optional*, é»˜è®¤ä¸º `IMAGENET_STANDARD_STD`)
    â€” å¦‚æœè§„èŒƒåŒ–å›¾åƒè¦ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¦†ç›–ã€‚'
- en: '`apply_ocr` (`bool`, *optional*, defaults to `True`) â€” Whether to apply the
    Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden
    by the `apply_ocr` parameter in the `preprocess` method.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_ocr` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åº”ç”¨ Tesseract OCR å¼•æ“ä»¥è·å–å•è¯ +
    è§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `apply_ocr` å‚æ•°è¦†ç›–ã€‚'
- en: '`ocr_lang` (`str`, *optional*) â€” The language, specified by its ISO code, to
    be used by the Tesseract OCR engine. By default, English is used. Can be overridden
    by the `ocr_lang` parameter in the `preprocess` method.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ocr_lang` (`str`, *optional*) â€” Tesseract OCR å¼•æ“è¦ä½¿ç”¨çš„è¯­è¨€ï¼Œç”±å…¶ ISO ä»£ç æŒ‡å®šã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨è‹±è¯­ã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `ocr_lang` å‚æ•°è¦†ç›–ã€‚'
- en: '`tesseract_config` (`str`, *optional*) â€” Any additional custom configuration
    flags that are forwarded to the `config` parameter when calling Tesseract. For
    example: â€˜â€”psm 6â€™. Can be overridden by the `tesseract_config` parameter in the
    `preprocess` method.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tesseract_config` (`str`, *optional*) â€” è½¬å‘åˆ°è°ƒç”¨ Tesseract æ—¶ `config` å‚æ•°çš„ä»»ä½•é¢å¤–è‡ªå®šä¹‰é…ç½®æ ‡å¿—ã€‚ä¾‹å¦‚ï¼šâ€˜â€”psm
    6â€™ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `tesseract_config` å‚æ•°è¦†ç›–ã€‚'
- en: Constructs a LayoutLMv3 image processor.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª LayoutLMv3 å›¾åƒå¤„ç†å™¨ã€‚
- en: '#### `preprocess`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py#L216)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py#L216)'
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®
    `do_rescale=False`ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) â€” Whether to
    resize the image.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, é»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” Desired size
    of the output image after applying `resize`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, é»˜è®¤ä¸º `self.size`) â€” åº”ç”¨ `resize` åè¾“å‡ºå›¾åƒçš„æœŸæœ›å¤§å°ã€‚'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) â€” Resampling filter
    to use if resizing the image. This can be one of the `PILImageResampling` filters.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *optional*, defaults to `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚è¿™å¯ä»¥æ˜¯
    `PILImageResampling` æ»¤æ³¢å™¨ä¹‹ä¸€ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) â€” Whether
    to rescale the image pixel values between [0, 1].'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒåƒç´ å€¼é‡æ–°ç¼©æ”¾åˆ°
    [0, 1] ä¹‹é—´ã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) â€”
    Rescale factor to apply to the image pixel values. Only has an effect if `do_rescale`
    is set to `True`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) â€”
    åº”ç”¨äºå›¾åƒåƒç´ å€¼çš„é‡æ–°ç¼©æ”¾å› å­ã€‚ä»…åœ¨ `do_rescale` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) â€” Whether
    to normalize the image.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '`image_mean` (`float` or `Iterable[float]`, *optional*, defaults to `self.image_mean`)
    â€” Mean values to be used for normalization. Only has an effect if `do_normalize`
    is set to `True`.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` or `Iterable[float]`, *optional*, defaults to `self.image_mean`)
    â€” ç”¨äºå½’ä¸€åŒ–çš„å‡å€¼ã€‚ä»…åœ¨ `do_normalize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`image_std` (`float` or `Iterable[float]`, *optional*, defaults to `self.image_std`)
    â€” Standard deviation values to be used for normalization. Only has an effect if
    `do_normalize` is set to `True`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` or `Iterable[float]`, *optional*, defaults to `self.image_std`)
    â€” ç”¨äºå½’ä¸€åŒ–çš„æ ‡å‡†å·®å€¼ã€‚ä»…åœ¨ `do_normalize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`apply_ocr` (`bool`, *optional*, defaults to `self.apply_ocr`) â€” Whether to
    apply the Tesseract OCR engine to get words + normalized bounding boxes.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_ocr` (`bool`, *optional*, defaults to `self.apply_ocr`) â€” æ˜¯å¦åº”ç”¨ Tesseract
    OCR å¼•æ“ä»¥è·å–å•è¯ + è§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚'
- en: '`ocr_lang` (`str`, *optional*, defaults to `self.ocr_lang`) â€” The language,
    specified by its ISO code, to be used by the Tesseract OCR engine. By default,
    English is used.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ocr_lang` (`str`, *optional*, defaults to `self.ocr_lang`) â€” Tesseract OCR
    å¼•æ“ä½¿ç”¨çš„è¯­è¨€ï¼Œç”±å…¶ ISO ä»£ç æŒ‡å®šã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨è‹±è¯­ã€‚'
- en: '`tesseract_config` (`str`, *optional*, defaults to `self.tesseract_config`)
    â€” Any additional custom configuration flags that are forwarded to the `config`
    parameter when calling Tesseract.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tesseract_config` (`str`, *optional*, defaults to `self.tesseract_config`)
    â€” è½¬å‘åˆ°è°ƒç”¨ Tesseract æ—¶ `config` å‚æ•°çš„ä»»ä½•é¢å¤–è‡ªå®šä¹‰é…ç½®æ ‡å¿—ã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) â€” The type of tensors
    to return. Can be one of:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` or `TensorType`, *optional*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªè®¾ç½®ï¼šè¿”å› `np.ndarray` çš„åˆ—è¡¨ã€‚
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` æˆ– `''tf''`ï¼šè¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹æ¬¡ã€‚'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` æˆ– `''pt''`ï¼šè¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹æ¬¡ã€‚'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` æˆ– `''np''`ï¼šè¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹æ¬¡ã€‚'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` æˆ– `''jax''`ï¼šè¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹æ¬¡ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` æˆ– `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ (é€šé“æ•°, é«˜åº¦, å®½åº¦) æ ¼å¼ã€‚'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ (é«˜åº¦, å®½åº¦, é€šé“æ•°) æ ¼å¼ã€‚'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` æˆ– `str`, *optional*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ (é€šé“æ•°, é«˜åº¦, å®½åº¦) æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ (é«˜åº¦, å®½åº¦, é€šé“æ•°) æ ¼å¼ã€‚'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` æˆ– `ChannelDimension.NONE`ï¼šå›¾åƒä»¥ (é«˜åº¦, å®½åº¦) æ ¼å¼ã€‚'
- en: Preprocess an image or batch of images.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚
- en: LayoutLMv3Tokenizer
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3Tokenizer
- en: '### `class transformers.LayoutLMv3Tokenizer`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3Tokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L200)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L200)'
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” Path to the vocabulary file.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`merges_file` (`str`) â€” Path to the merges file.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) â€” Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *optional*, defaults to `"replace"`) â€” è§£ç å­—èŠ‚ä¸º UTF-8 æ—¶è¦éµå¾ªçš„èŒƒä¾‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
    [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)ã€‚'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) â€” The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å¤´æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œè¿™ä¸æ˜¯åºåˆ—å¼€å¤´ä½¿ç”¨çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯ `cls_token`ã€‚
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) â€” The end of sequence
    token.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œè¿™ä¸æ˜¯åºåˆ—æœ«å°¾ä½¿ç”¨çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯ `sep_token`ã€‚
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) â€” The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, é»˜è®¤ä¸º `"</s>"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œåœ¨æ„å»ºæ¥è‡ªå¤šä¸ªåºåˆ—çš„åºåˆ—æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºé—®é¢˜å›ç­”çš„æ–‡æœ¬å’Œé—®é¢˜ã€‚å®ƒè¿˜ç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) â€” The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, é»˜è®¤ä¸º `"<s>"`) â€” åˆ†ç±»å™¨æ ‡è®°ï¼Œç”¨äºè¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆå¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œè€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼‰ã€‚å®ƒæ˜¯ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) â€” The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, é»˜è®¤ä¸º `"<pad>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) â€” The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, é»˜è®¤ä¸º `"<mask>"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚è¿™æ˜¯åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `True`) â€” Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (RoBERTa tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨è¾“å…¥ä¸­æ·»åŠ åˆå§‹ç©ºæ ¼ã€‚è¿™å…è®¸å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚ï¼ˆRoBERTaåˆ†è¯å™¨é€šè¿‡å‰é¢çš„ç©ºæ ¼æ£€æµ‹å•è¯çš„å¼€å¤´ï¼‰ã€‚'
- en: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [CLS] token.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_box` (`List[int]`, *optional*, é»˜è®¤ä¸º `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[CLS]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`sep_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [SEP] token.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token_box` (`List[int]`, *optional*, é»˜è®¤ä¸º `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[SEP]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [PAD] token.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_box` (`List[int]`, *optional*, é»˜è®¤ä¸º `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[PAD]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pad_token_label` (`int`, *optional*, defaults to -100) â€” The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorchâ€™s
    CrossEntropyLoss.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_label` (`int`, *optional*, é»˜è®¤ä¸º -100) â€” ç”¨äºå¡«å……æ ‡è®°çš„æ ‡ç­¾ã€‚é»˜è®¤ä¸º-100ï¼Œè¿™æ˜¯PyTorchçš„CrossEntropyLossçš„`ignore_index`ã€‚'
- en: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to only label the first subword, in case word labels are provided.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_label_first_subword` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦ä»…æ ‡è®°ç¬¬ä¸€ä¸ªå­è¯ï¼Œå¦‚æœæä¾›äº†å•è¯æ ‡ç­¾ã€‚'
- en: Construct a LayoutLMv3 tokenizer. Based on `RoBERTatokenizer` (Byte Pair Encoding
    or BPE). [LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)
    can be used to turn words, word-level bounding boxes and optional word labels
    to token-level `input_ids`, `attention_mask`, `token_type_ids`, `bbox`, and optional
    `labels` (for token classification).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªLayoutLMv3åˆ†è¯å™¨ã€‚åŸºäº`RoBERTatokenizer`ï¼ˆå­—èŠ‚å¯¹ç¼–ç æˆ–BPEï¼‰ã€‚[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)
    å¯ç”¨äºå°†å•è¯ã€å•è¯çº§è¾¹ç•Œæ¡†å’Œå¯é€‰å•è¯æ ‡ç­¾è½¬æ¢ä¸ºæ ‡è®°çº§`input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`å’Œå¯é€‰`labels`ï¼ˆç”¨äºæ ‡è®°åˆ†ç±»ï¼‰ã€‚
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)
    runs end-to-end tokenization: punctuation splitting and wordpiece. It also turns
    the word-level bounding boxes into token-level bounding boxes.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)
    è¿è¡Œç«¯åˆ°ç«¯çš„åˆ†è¯ï¼šæ ‡ç‚¹ç¬¦å·æ‹†åˆ†å’Œè¯å—ã€‚å®ƒè¿˜å°†å•è¯çº§è¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§è¾¹ç•Œæ¡†ã€‚'
- en: '#### `__call__`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L550)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L550)'
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå•ä¸ªç¤ºä¾‹çš„å•è¯æˆ–ä¸€æ‰¹ç¤ºä¾‹çš„é—®é¢˜ï¼‰æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆä¸€æ‰¹å•è¯ï¼‰ã€‚'
- en: '`text_pair` (`List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” å•è¯çº§è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”æ ‡å‡†åŒ–ä¸º0-1000çš„æ¯”ä¾‹ã€‚'
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” å•è¯çº§æ•´æ•°æ ‡ç­¾ï¼ˆç”¨äºè¯¸å¦‚FUNSDã€CORDä¹‹ç±»çš„æ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å¯¹åºåˆ—è¿›è¡Œç¼–ç ï¼Œä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” æ¿€æ´»å¹¶æ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›äº†å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: å¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” æ¿€æ´»å¹¶æ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`: æˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™é€æ ‡è®°æˆªæ–­ï¼Œä»æœ€é•¿åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: æˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: æˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—é•¿åº¦çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, defaults to 0) â€” å¦‚æœè®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œå¹¶ä¸”ä¸ `max_length` ä¸€èµ·ä½¿ç”¨ï¼Œå½“
    `return_overflowing_tokens=True` æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ› `>=
    7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å¯¹åºåˆ—è¿›è¡Œç¼–ç ï¼Œä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” æ¿€æ´»å¹¶æ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›äº†å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: å¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œè¯¥é•¿åº¦ç”±å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼Œå¦‚æœæœªæä¾›è¯¥å‚æ•°ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`: æˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼Œå¦‚æœæœªæä¾›è¯¥å‚æ•°ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™å°†é€ä¸ªæ ‡è®°æˆªæ–­ï¼Œä»è¾ƒé•¿åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: æˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼Œå¦‚æœæœªæä¾›è¯¥å‚æ•°ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: æˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼Œå¦‚æœæœªæä¾›è¯¥å‚æ•°ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚ '
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—é•¿åº¦çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters. If left unset or set to `None`, this will
    use the predefined model maximum length if a maximum length is required by one
    of the truncation/padding parameters. If the model has no specific maximum input
    length (like XLNet) truncation/padding to a maximum length will be deactivated.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *å¯é€‰*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚
    XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚'
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœä¸ `max_length` ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“ `return_overflowing_tokens=True`
    æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«ä»æˆªæ–­åºåˆ—æœ«å°¾è¿”å›çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰çš„
    NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œæ ‡è®°åŒ–å’Œä¸ºæ¨¡å‹å‡†å¤‡çš„ä¸»è¦æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬å•è¯çº§åˆ«çš„å½’ä¸€åŒ–è¾¹ç•Œæ¡†å’Œå¯é€‰æ ‡ç­¾ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L431)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py#L431)'
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: LayoutLMv3TokenizerFast
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3TokenizerFast
- en: '### `class transformers.LayoutLMv3TokenizerFast`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3TokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py#L65)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py#L65)'
- en: '[PRE9]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” Path to the vocabulary file.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) â€” è¯æ±‡è¡¨æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`merges_file` (`str`) â€” Path to the merges file.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) â€” Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"replace"`) â€” è§£ç å­—èŠ‚ä¸º UTF-8 æ—¶è¦éµå¾ªçš„èŒƒä¾‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)ã€‚'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) â€” The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ä»¥ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—å¼€å¤´çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`cls_token`ã€‚
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) â€” The end of sequence
    token.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—ç»“å°¾çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`sep_token`ã€‚
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) â€” The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œç”¨äºä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚å®ƒä¹Ÿç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) â€” The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” åœ¨è¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆæ•´ä¸ªåºåˆ—è€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°çš„åˆ†ç±»ï¼‰æ—¶ä½¿ç”¨çš„åˆ†ç±»å™¨æ ‡è®°ã€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) â€” The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<pad>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) â€” The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<mask>"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚è¿™æ˜¯åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (RoBERTa tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è¾“å…¥ä¸­æ·»åŠ åˆå§‹ç©ºæ ¼ã€‚è¿™å…è®¸å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚ï¼ˆRoBERTaæ ‡è®°å™¨é€šè¿‡å‰é¢çš„ç©ºæ ¼æ£€æµ‹å•è¯çš„å¼€å¤´ï¼‰ã€‚'
- en: '`trim_offsets` (`bool`, *optional*, defaults to `True`) â€” Whether the post
    processing step should trim offsets to avoid including whitespaces.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trim_offsets` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” åå¤„ç†æ­¥éª¤æ˜¯å¦åº”ä¿®å‰ªåç§»é‡ä»¥é¿å…åŒ…å«ç©ºæ ¼ã€‚'
- en: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [CLS] token.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_box` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[CLS]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`sep_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [SEP] token.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token_box` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[SEP]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [PAD] token.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_box` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[PAD]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pad_token_label` (`int`, *optional*, defaults to -100) â€” The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorchâ€™s
    CrossEntropyLoss.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_label` (`int`, *å¯é€‰*, é»˜è®¤ä¸º -100) â€” ç”¨äºå¡«å……æ ‡è®°çš„æ ‡ç­¾ã€‚é»˜è®¤ä¸º-100ï¼Œè¿™æ˜¯PyTorchçš„CrossEntropyLossçš„`ignore_index`ã€‚'
- en: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to only label the first subword, in case word labels are provided.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_label_first_subword` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦ä»…æ ‡è®°ç¬¬ä¸€ä¸ªå­è¯ï¼Œå¦‚æœæä¾›äº†å•è¯æ ‡ç­¾ã€‚'
- en: Construct a â€œfastâ€ LayoutLMv3 tokenizer (backed by HuggingFaceâ€™s *tokenizers*
    library). Based on BPE.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºâ€œå¿«é€Ÿâ€LayoutLMv3æ ‡è®°å™¨ï¼ˆç”±HuggingFaceçš„*tokenizers*åº“æ”¯æŒï¼‰ã€‚åŸºäºBPEã€‚
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ ‡è®°å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `__call__`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py#L224)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py#L224)'
- en: '[PRE10]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå•ä¸ªç¤ºä¾‹çš„å•è¯æˆ–ä¸€æ‰¹ç¤ºä¾‹çš„é—®é¢˜ï¼‰æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆå•è¯æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`text_pair` (`List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” å•è¯çº§åˆ«çš„è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥è¢«å½’ä¸€åŒ–ä¸º0-1000çš„æ¯”ä¾‹ã€‚'
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_labels`ï¼ˆ`List[int]`ã€`List[List[int]]`ï¼Œ*å¯é€‰*ï¼‰ â€” å•è¯çº§æ•´æ•°æ ‡ç­¾ï¼ˆç”¨äºè¯¸å¦‚ FUNSDã€CORD
    ç­‰æ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰ â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ã€`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›äº†å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¡«å……åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³å¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ã€`str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™å°†é€ä¸ªæ ‡è®°è¿›è¡Œæˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œæˆªæ–­ï¼ˆå³å¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰ â€” å¦‚æœè®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œå¹¶ä¸”ä¸ `max_length` ä¸€èµ·ä½¿ç”¨ï¼Œå½“ `return_overflowing_tokens=True`
    æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰çš„
    NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰
    â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰ â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ã€`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`æˆ–`''longest''`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦å¡«å……ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`æˆ–`''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸å¡«å……ï¼ˆå³å¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œ`str`æˆ–[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰
    - æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`æˆ–`''longest_first''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦æˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦æˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦æˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`æˆ–`''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³å¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters. If left unset or set to `None`, this will
    use the predefined model maximum length if a maximum length is required by one
    of the truncation/padding parameters. If the model has no specific maximum input
    length (like XLNet) truncation/padding to a maximum length will be deactivated.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ - æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚'
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰ - å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ - å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºå¯ç”¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šçš„Tensor
    Coresç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰
    - å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºæ ‡è®°å’Œå‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹çš„ä¸»è¦æ–¹æ³•ï¼Œå…·æœ‰å•è¯çº§åˆ«çš„å½’ä¸€åŒ–è¾¹ç•Œæ¡†å’Œå¯é€‰æ ‡ç­¾ã€‚
- en: LayoutLMv3Processor
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3Processor
- en: '### `class transformers.LayoutLMv3Processor`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3Processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/processing_layoutlmv3.py#L27)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/processing_layoutlmv3.py#L27)'
- en: '[PRE11]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_processor` (`LayoutLMv3ImageProcessor`, *optional*) â€” An instance of
    [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor).
    The image processor is a required input.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`ï¼ˆ`LayoutLMv3ImageProcessor`ï¼Œ*å¯é€‰*ï¼‰ - [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)çš„ä¸€ä¸ªå®ä¾‹ã€‚å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` (`LayoutLMv3Tokenizer` or `LayoutLMv3TokenizerFast`, *optional*)
    â€” An instance of [LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)
    or [LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast).
    The tokenizer is a required input.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ`LayoutLMv3Tokenizer`æˆ–`LayoutLMv3TokenizerFast`ï¼Œ*å¯é€‰*ï¼‰- [LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)æˆ–[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)çš„å®ä¾‹ã€‚æ ‡è®°å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a LayoutLMv3 processor which combines a LayoutLMv3 image processor
    and a LayoutLMv3 tokenizer into a single processor.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªLayoutLMv3å¤„ç†å™¨ï¼Œå°†LayoutLMv3å›¾åƒå¤„ç†å™¨å’ŒLayoutLMv3æ ‡è®°å™¨ç»„åˆæˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚
- en: '[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)
    offers all the functionalities you need to prepare data for the model.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv3Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor)æä¾›äº†å‡†å¤‡æ•°æ®ç»™æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰åŠŸèƒ½ã€‚'
- en: It first uses [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)
    to resize and normalize document images, and optionally applies OCR to get words
    and normalized bounding boxes. These are then provided to [LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)
    or [LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast),
    which turns the words and bounding boxes into token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which
    are turned into token-level `labels` for token classification tasks (such as FUNSD,
    CORD).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 'å®ƒé¦–å…ˆä½¿ç”¨[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)æ¥è°ƒæ•´å’Œè§„èŒƒæ–‡æ¡£å›¾åƒï¼Œå¹¶å¯é€‰æ‹©åº”ç”¨OCRä»¥è·å–å•è¯å’Œè§„èŒƒåŒ–çš„è¾¹ç•Œæ¡†ã€‚ç„¶åå°†å®ƒä»¬æä¾›ç»™[LayoutLMv3Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer)æˆ–[LayoutLMv3TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast)ï¼Œå°†å•è¯å’Œè¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§`input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`ã€‚å¯é€‰åœ°ï¼Œå¯ä»¥æä¾›æ•´æ•°`word_labels`ï¼Œè¿™äº›æ ‡ç­¾è¢«è½¬æ¢ä¸ºç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚FUNSDã€CORDï¼‰çš„æ ‡è®°çº§`labels`ã€‚ '
- en: '#### `__call__`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/processing_layoutlmv3.py#L69)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/processing_layoutlmv3.py#L69)'
- en: '[PRE12]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method first forwards the `images` argument to [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__).
    In case [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)
    was initialized with `apply_ocr` set to `True`, it passes the obtained words and
    bounding boxes along with the additional arguments to [**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer.__call__)
    and returns the output, together with resized and normalized `pixel_values`. In
    case [LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)
    was initialized with `apply_ocr` set to `False`, it passes the words (`text`/``text_pair`)
    and `boxes` specified by the user along with the additional arguments to [**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer.__call__)
    and returns the output, together with resized and normalized `pixel_values`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•é¦–å…ˆå°†`images`å‚æ•°è½¬å‘åˆ°[**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚å¦‚æœ[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)åˆå§‹åŒ–æ—¶å°†`apply_ocr`è®¾ç½®ä¸º`True`ï¼Œåˆ™å°†è·å¾—çš„å•è¯å’Œè¾¹ç•Œæ¡†è¿åŒå…¶ä»–å‚æ•°ä¼ é€’ç»™[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer.__call__)å¹¶è¿”å›è¾“å‡ºï¼Œä»¥åŠè°ƒæ•´å¤§å°å’Œè§„èŒƒåŒ–çš„`pixel_values`ã€‚å¦‚æœ[LayoutLMv3ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor)åˆå§‹åŒ–æ—¶å°†`apply_ocr`è®¾ç½®ä¸º`False`ï¼Œåˆ™å°†ç”¨æˆ·æŒ‡å®šçš„å•è¯ï¼ˆ`text`/``text_pair`ï¼‰å’Œ`boxes`è¿åŒå…¶ä»–å‚æ•°ä¼ é€’ç»™[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer.__call__)å¹¶è¿”å›è¾“å‡ºï¼Œä»¥åŠè°ƒæ•´å¤§å°å’Œè§„èŒƒåŒ–çš„`pixel_values`ã€‚
- en: Please refer to the docstring of the above two methods for more information.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šä¿¡æ¯è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: PytorchHide Pytorch content
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorchå†…å®¹
- en: LayoutLMv3Model
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3Model
- en: '### `class transformers.LayoutLMv3Model`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L734)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L734)'
- en: '[PRE13]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)ï¼‰-
    æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare LayoutLMv3 Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„LayoutLMv3æ¨¡å‹å˜æ¢å™¨è¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L827)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L827)'
- en: '[PRE14]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, token_sequence_length, 4)`,
    *optional*) â€” Bounding boxes of each input sequence tokens. Selected in the range
    `[0, config.max_2d_position_embeddings-1]`. Each bounding box should be a normalized
    version in (x0, y0, x1, y1) format, where (x0, y0) corresponds to the position
    of the upper left corner in the bounding box, and (x1, y1) represents the position
    of the lower right corner.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Batch of document images. Each image is divided into patches of shape
    `(num_channels, config.patch_size, config.patch_size)` and the total number of
    patches (=`patch_sequence_length`) equals to `((height / config.patch_size) *
    (width / config.patch_size))`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, token_sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, token_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€”
    æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)
    forward method, overrides the `__call__` special method.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE15]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: LayoutLMv3ForSequenceClassification
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3ForSequenceClassification
- en: '### `class transformers.LayoutLMv3ForSequenceClassification`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1259)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1259)'
- en: '[PRE16]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: LayoutLMv3 Model with a sequence classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for document image classification
    tasks such as the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv3æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰åºåˆ—åˆ†ç±»å¤´éƒ¨ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºæ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå¦‚[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)æ•°æ®é›†ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1277)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1277)'
- en: '[PRE17]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, 4)`ï¼Œ*optional*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰å®šèŒƒå›´ä¸º `[0, config.max_2d_position_embeddings-1]`ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯ (x0,
    y0, x1, y1) æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­ (x0, y0) å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1) è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚'
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Batch of document images. Each image is divided into patches of shape
    `(num_channels, config.patch_size, config.patch_size)` and the total number of
    patches (=`patch_sequence_length`) equals to `((height / config.patch_size) *
    (width / config.patch_size))`.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height,
    width)`) â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚æ¯ä¸ªå›¾åƒè¢«åˆ†æˆå½¢çŠ¶ä¸º `(num_channels, config.patch_size, config.patch_size)`
    çš„è¡¥ä¸ï¼Œå¹¶ä¸”è¡¥ä¸çš„æ€»æ•°ï¼ˆ=`patch_sequence_length`ï¼‰ç­‰äº `((height / config.patch_size) * (width
    / config.patch_size))`ã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ä»£è¡¨æœªè¢« `masked` çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ä»£è¡¨è¢« `masked` çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰å®šåœ¨ `[0, 1]` ä¹‹é—´ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰å®šèŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*optional*)
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„ç‰¹å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)
    â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°† *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ– `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)ï¼‰å’Œè¾“å…¥ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰- åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰- åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LayoutLMv3ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv3ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ æ’­çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: LayoutLMv3ForTokenClassification
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv3ForTokenClassification
- en: '### `class transformers.LayoutLMv3ForTokenClassification`'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv3ForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1016)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1016)'
- en: '[PRE19]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: LayoutLMv3 Model with a token classification head on top (a linear layer on
    top of the final hidden states) e.g. for sequence labeling (information extraction)
    tasks such as [FUNSD](https://guillaumejaume.github.io/FUNSD/), [SROIE](https://rrc.cvc.uab.es/?ch=13),
    [CORD](https://github.com/clovaai/cord) and [Kleister-NDA](https://github.com/applicaai/kleister-nda).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv3æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªæ ‡è®°åˆ†ç±»å¤´ï¼ˆæœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—æ ‡è®°ï¼ˆä¿¡æ¯æå–ï¼‰ä»»åŠ¡çš„[FUNSD](https://guillaumejaume.github.io/FUNSD/)ã€[SROIE](https://rrc.cvc.uab.es/?ch=13)ã€[CORD](https://github.com/clovaai/cord)å’Œ[Kleister-NDA](https://github.com/applicaai/kleister-nda)ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `å‰å‘ä¼ æ’­`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1039)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1039)'
- en: '[PRE20]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æŸ¥çœ‹[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)è·å–è¯¦ç»†ä¿¡æ¯ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Batch of document images. Each image is divided into patches of shape
    `(num_channels, config.patch_size, config.patch_size)` and the total number of
    patches (=`patch_sequence_length`) equals to `((height / config.patch_size) *
    (width / config.patch_size))`.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification loss.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    â€” Classification scores (before SoftMax).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: LayoutLMv3ForQuestionAnswering
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv3ForQuestionAnswering`'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1129)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv3 Model with a span classification head on top for extractive question-answering
    tasks such as [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of
    the text part of the hidden-states output to compute `span start logits` and `span
    end logits`).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py#L1147)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Batch of document images. Each image is divided into patches of shape
    `(num_channels, config.patch_size, config.patch_size)` and the total number of
    patches (=`patch_sequence_length`) equals to `((height / config.patch_size) *
    (width / config.patch_size))`.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    â€” Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€”
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Span-start scores (before SoftMax).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Span-end scores (before SoftMax).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LayoutLMv3ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: TensorFlowHide TensorFlow content
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: TFLayoutLMv3Model
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLayoutLMv3Model`'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1257)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare LayoutLMv3 Model transformer outputting raw hidden-states without any
    specific head on top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1269)'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length,
    4)`, *optional*) â€” Bounding boxes of each input sequence tokens. Selected in the
    range `[0, config.max_2d_position_embeddings-1]`. Each bounding box should be
    a normalized version in (x0, y0, x1, y1) format, where (x0, y0) corresponds to
    the position of the upper left corner in the bounding box, and (x1, y1) represents
    the position of the lower right corner.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    â€” Batch of document images. Each image is divided into patches of shape `(num_channels,
    config.patch_size, config.patch_size)` and the total number of patches (=`patch_sequence_length`)
    equals to `((height / config.patch_size) * (width / config.patch_size))`.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-505
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-506
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) â€” Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert *input_ids* indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLayoutLMv3Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3Model)
    forward method, overrides the `__call__` special method.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: TFLayoutLMv3ForSequenceClassification
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLayoutLMv3ForSequenceClassification`'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1391)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv3 Model with a sequence classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for document image classification
    tasks such as the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1409)'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length,
    4)`, *optional*) â€” Bounding boxes of each input sequence tokens. Selected in the
    range `[0, config.max_2d_position_embeddings-1]`. Each bounding box should be
    a normalized version in (x0, y0, x1, y1) format, where (x0, y0) corresponds to
    the position of the upper left corner in the bounding box, and (x1, y1) represents
    the position of the lower right corner.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    â€” Batch of document images. Each image is divided into patches of shape `(num_channels,
    config.patch_size, config.patch_size)` and the total number of patches (=`patch_sequence_length`)
    equals to `((height / config.patch_size) * (width / config.patch_size))`.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-566
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-570
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-571
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-578
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-579
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) â€” Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert *input_ids* indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-590
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLayoutLMv3ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: TFLayoutLMv3ForTokenClassification
  id: totrans-597
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLayoutLMv3ForTokenClassification`'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1505)'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv3 Model with a token classification head on top (a linear layer on
    top of the final hidden states) e.g. for sequence labeling (information extraction)
    tasks such as [FUNSD](https://guillaumejaume.github.io/FUNSD/), [SROIE](https://rrc.cvc.uab.es/?ch=13),
    [CORD](https://github.com/clovaai/cord) and [Kleister-NDA](https://github.com/applicaai/kleister-nda).
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1534)'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length,
    4)`, *optional*) â€” Bounding boxes of each input sequence tokens. Selected in the
    range `[0, config.max_2d_position_embeddings-1]`. Each bounding box should be
    a normalized version in (x0, y0, x1, y1) format, where (x0, y0) corresponds to
    the position of the upper left corner in the bounding box, and (x1, y1) represents
    the position of the lower right corner.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    â€” Batch of document images. Each image is divided into patches of shape `(num_channels,
    config.patch_size, config.patch_size)` and the total number of patches (=`patch_sequence_length`)
    equals to `((height / config.patch_size) * (width / config.patch_size))`.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-626
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-627
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-631
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-632
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-639
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-640
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) â€” Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert *input_ids* indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of unmasked
    labels, returned when `labels` is provided) â€” Classification loss.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    â€” Classification scores (before SoftMax).'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLayoutLMv3ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-658
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: TFLayoutLMv3ForQuestionAnswering
  id: totrans-659
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLayoutLMv3ForQuestionAnswering`'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1643)'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv3 Model with a span classification head on top for extractive question-answering
    tasks such as [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of
    the text part of the hidden-states output to compute `span start logits` and `span
    end logits`).
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py#L1663)'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-682
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length,
    4)`, *optional*) â€” Bounding boxes of each input sequence tokens. Selected in the
    range `[0, config.max_2d_position_embeddings-1]`. Each bounding box should be
    a normalized version in (x0, y0, x1, y1) format, where (x0, y0) corresponds to
    the position of the upper left corner in the bounding box, and (x1, y1) represents
    the position of the lower right corner.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-685
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    â€” Batch of document images. Each image is divided into patches of shape `(num_channels,
    config.patch_size, config.patch_size)` and the total number of patches (=`patch_sequence_length`)
    equals to `((height / config.patch_size) * (width / config.patch_size))`.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-688
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-689
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-690
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-691
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-693
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-694
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-695
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `sequence_length = token_sequence_length + patch_sequence_length +
    1` where `1` is for [CLS] token. See `pixel_values` for `patch_sequence_length`.
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-699
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-701
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-702
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) â€” Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert *input_ids* indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for position (index) of the start of the labelled span for computing the token
    classification loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for position (index) of the end of the labelled span for computing the token classification
    loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv3Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config))
    and inputs.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `start_positions`
    and `end_positions` are provided) â€” Total span extraction loss is the sum of a
    Cross-Entropy for the start and end positions.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) â€” Span-start
    scores (before SoftMax).'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) â€” Span-end
    scores (before SoftMax).'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLayoutLMv3ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-722
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
