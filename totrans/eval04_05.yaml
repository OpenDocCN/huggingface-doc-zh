- en: A quick tour
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/evaluate/a_quick_tour](https://huggingface.co/docs/evaluate/a_quick_tour)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/a_quick_tour.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/Tip-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/CodeBlock-hf-doc-builder.js">
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Evaluate provides access to a wide range of evaluation tools. It covers a
    range of modalities such as text, computer vision, audio, etc. as well as tools
    to evaluate models or datasets. These tools are split into three categories.
  prefs: []
  type: TYPE_NORMAL
- en: Types of evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different aspects of a typical machine learning pipeline that can
    be evaluated and for each aspect ðŸ¤— Evaluate provides a tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metric**: A metric is used to evaluate a modelâ€™s performance and usually
    involves the modelâ€™s predictions as well as some ground truth labels. You can
    find all integrated metrics at [evaluate-metric](https://huggingface.co/evaluate-metric).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparison**: A comparison is used to compare two models. This can for example
    be done by comparing their predictions to ground truth labels and computing their
    agreement. You can find all integrated comparisons at [evaluate-comparison](https://huggingface.co/evaluate-comparison).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement**: The dataset is as important as the model trained on it. With
    measurements one can investigate a datasetâ€™s properties. You can find all integrated
    measurements at [evaluate-measurement](https://huggingface.co/evaluate-measurement).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these evaluation modules live on Hugging Face Hub as a Space. They
    come with an interactive widget and a documentation card documenting its use and
    limitations. For example [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebd2d536b44a838c1365ef03fe7fa028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each metric, comparison, and measurement is a separate Python module, but for
    using any of them, there is a single entry point: [evaluate.load()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.load)!'
  prefs: []
  type: TYPE_NORMAL
- en: Load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Any metric, comparison, or measurement is loaded with the `evaluate.load` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to make sure you are loading the right type of evaluation (especially
    if there are name clashes) you can explicitly pass the type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Community modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides the modules implemented in ðŸ¤— Evaluate you can also load any community
    module by specifying the repository ID of the metric implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: See the [Creating and Sharing Guide](/docs/evaluate/main/en/creating_and_sharing)
    for information about uploading custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: List available modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With [list_evaluation_modules()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.list_evaluation_modules)
    you can check what modules are available on the hub. You can also filter for a
    specific modules and skip community metrics if you want. You can also see additional
    information such as likes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Module attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All evalution modules come with a range of useful attributes that help to use
    a module stored in a [EvaluationModuleInfo](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModuleInfo)
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '| Attribute | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `description` | A short description of the evaluation module. |'
  prefs: []
  type: TYPE_TB
- en: '| `citation` | A BibTex string for citation when available. |'
  prefs: []
  type: TYPE_TB
- en: '| `features` | A `Features` object defining the input format. |'
  prefs: []
  type: TYPE_TB
- en: '| `inputs_description` | This is equivalent to the modules docstring. |'
  prefs: []
  type: TYPE_TB
- en: '| `homepage` | The homepage of the module. |'
  prefs: []
  type: TYPE_TB
- en: '| `license` | The license of the module. |'
  prefs: []
  type: TYPE_TB
- en: '| `codebase_urls` | Link to the code behind the module. |'
  prefs: []
  type: TYPE_TB
- en: '| `reference_urls` | Additional reference URLs. |'
  prefs: []
  type: TYPE_TB
- en: 'Letâ€™s have a look at a few examples. First, letâ€™s look at the `description`
    attribute of the accuracy metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that it describes how the metric works in theory. If you use this
    metric for your work, especially if it is an academic publication you want to
    reference it properly. For that you can look at the `citation` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can apply a metric or other evaluation module to a use-case, we need
    to know what the input format of the metric is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that features always describe the type of a single input element. In general
    we will add lists of elements so you can always think of a list around the types
    in `features`. Evaluate accepts various input formats (Python lists, NumPy arrays,
    PyTorch tensors, etc.) and converts them to an appropriate format for storage
    and computation.
  prefs: []
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we know how the evaluation module works and what should go in there
    we want to actually use it! When it comes to computing the actual score there
    are two main ways to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: All-in-one
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Incremental
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the incremental approach the necessary inputs are added to the module with
    [EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)
    or [EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)
    and the score is calculated at the end with [EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute).
    Alternatively, one can pass all the inputs at once to `compute()`. Letâ€™s have
    a look at the two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: How to compute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to calculate the score of an evaluation module is by calling
    `compute()` directly with the necessary inputs. Simply pass the inputs as seen
    in `features` to the `compute()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation modules return the results in a dictionary. However, in some instances
    you build up the predictions iteratively or in a distributed fashion in which
    case `add()` or `add_batch()` are useful.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate a single metric or a batch of metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many evaluation pipelines you build the predictions iteratively such as
    in a for-loop. In that case you could store the predictions in a list and at the
    end pass them to `compute()`. With `add()` and `add_batch()` you can circumvent
    the step of storing the predictions separately. If you are only creating single
    predictions at a time you can use `add()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have gathered all predictions you can call `compute()` to compute
    the score based on all stored values. When getting predictions and references
    in batches you can use `add_batch()` which adds a list elements for later processing.
    The rest works as with `add()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is especially useful when you need to get the predictions from your model
    in batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Distributed evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Computing metrics in a distributed environment can be tricky. Metric evaluation
    is executed in separate Python processes, or nodes, on different subsets of a
    dataset. Typically, when a metric score is additive (`f(AuB) = f(A) + f(B)`),
    you can use distributed reduce operations to gather the scores for each subset
    of the dataset. But when a metric is non-additive (`f(AuB) â‰  f(A) + f(B)`), itâ€™s
    not that simple. For example, you canâ€™t take the sum of the [F1](https://huggingface.co/spaces/evaluate-metric/f1)
    scores of each data subset as your **final metric**.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to overcome this issue is to fallback on single process evaluation.
    The metrics are evaluated on a single GPU, which becomes inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Evaluate solves this issue by only computing the final metric on the first
    node. The predictions and references are computed and provided to the metric separately
    for each node. These are temporarily stored in an Apache Arrow table, avoiding
    cluttering the GPU or CPU memory. When you are ready to `compute()` the final
    metric, the first node is able to access the predictions and references stored
    on all the other nodes. Once it has gathered all the predictions and references,
    `compute()` will perform the final metric evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: This solution allows ðŸ¤— Evaluate to perform distributed predictions, which is
    important for evaluation speed in distributed settings. At the same time, you
    can also use complex non-additive metrics without wasting valuable GPU or CPU
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Combining several evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often one wants to not only evaluate a single metric but a range of different
    metrics capturing different aspects of a model. E.g. for classification it is
    usually a good idea to compute F1-score, recall, and precision in addition to
    accuracy to get a better picture of model performance. Naturally, you can load
    a bunch of metrics and call them sequentially. However, a more convenient way
    is to use the [combine()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.combine)
    function to bundle them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `combine` function accepts both the list of names of the metrics as well
    as an instantiated modules. The `compute` call then computes each metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Save and push to the Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saving and sharing evaluation results is an important step. We provide the [evaluate.save()](/docs/evaluate/v0.4.0/en/package_reference/saving_methods#evaluate.save)
    function to easily save metrics results. You can either pass a specific filename
    or a directory. In the latter case, the results are saved in a file with an automatically
    created file name. Besides the directory or file name, the function takes any
    key-value pairs as inputs and stores them in a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of the JSON file look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the specified fields, it also contains useful system information
    for reproducing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides storing the results locally, you should report them on the modelâ€™s
    repository on the Hub. With the [evaluate.push_to_hub()](/docs/evaluate/v0.4.0/en/package_reference/hub_methods#evaluate.push_to_hub)
    function, you can easily report evaluation results to the modelâ€™s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Evaluator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [evaluate.evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    provides automated evaluation and only requires a model, dataset, metric in contrast
    to the metrics in `EvaluationModule`s that require the modelâ€™s predictions. As
    such it is easier to evaluate a model on a dataset with a given metric as the
    inference is handled internally. To make that possible it uses the [pipeline](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.pipeline)
    abstraction from `transformers`. However, you can use your own framework as long
    as it follows the `pipeline` interface.
  prefs: []
  type: TYPE_NORMAL
- en: To make an evaluation with the `evaluator` letâ€™s load a `transformers` pipeline
    (but you can pass your own custom inference class for any framework as long as
    it follows the pipeline call API) with an model trained on IMDb, the IMDb test
    split and the accuracy metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can create an evaluator for text classification and pass the three
    objects to the `compute()` method. With the label mapping `evaluate` provides
    a method to align the pipeline outputs with the label column in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculating the value of the metric alone is often not enough to know if a
    model performs significantly better than another one. With *bootstrapping* `evaluate`
    computes confidence intervals and the standard error which helps estimate how
    stable a score is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The evaluator expects a `"text"` and `"label"` column for the data input. If
    your dataset differs you can provide the columns with the keywords `input_column="text"`
    and `label_column="label"`. Currently only `"text-classification"` is supported
    with more tasks being added in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When comparing several models, sometimes itâ€™s hard to spot the differences in
    their performance simply by looking at their scores. Also often there is not a
    single best model but there are trade-offs between e.g. latency and accuracy as
    larger models might have better performance but are also slower. We are gradually
    adding different visualization approaches, such as plots, to make choosing the
    best model for a use-case easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you have a list of results from multiple models (as dictionaries),
    you can feed them into the `radar_plot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Which lets you visually compare the 4 models and choose the optimal one for
    you, based on one or several metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
  prefs: []
  type: TYPE_IMG
- en: Running evaluation on a suite of tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It can be useful to evaluate models on a variety of different tasks to understand
    their downstream performance. The [EvaluationSuite](evaluation_suite) enables
    evaluation of models on a collection of tasks. Tasks can be constructed as ([evaluator](base_evaluator),
    dataset, metric) tuples and passed to an [EvaluationSuite](evaluation_suite) stored
    on the Hugging Face Hub as a Space, or locally as a Python script. See the [evaluator
    documentation](base_evaluator) for a list of currently supported tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '`EvaluationSuite` scripts can be defined as follows, and supports Python code
    for data preprocessing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation can be run by loading the `EvaluationSuite` and calling `run()` method
    with a model or pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '| accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds
    | task_name |'
  prefs: []
  type: TYPE_TB
- en: '| --: | --: | --: | :-- | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
  prefs: []
  type: TYPE_TB
