- en: A quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/a_quick_tour](https://huggingface.co/docs/evaluate/a_quick_tour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Evaluate provides access to a wide range of evaluation tools. It covers a
    range of modalities such as text, computer vision, audio, etc. as well as tools
    to evaluate models or datasets. These tools are split into three categories.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Types of evaluations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different aspects of a typical machine learning pipeline that can
    be evaluated and for each aspect 🤗 Evaluate provides a tool:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '**Metric**: A metric is used to evaluate a model’s performance and usually
    involves the model’s predictions as well as some ground truth labels. You can
    find all integrated metrics at [evaluate-metric](https://huggingface.co/evaluate-metric).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparison**: A comparison is used to compare two models. This can for example
    be done by comparing their predictions to ground truth labels and computing their
    agreement. You can find all integrated comparisons at [evaluate-comparison](https://huggingface.co/evaluate-comparison).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement**: The dataset is as important as the model trained on it. With
    measurements one can investigate a dataset’s properties. You can find all integrated
    measurements at [evaluate-measurement](https://huggingface.co/evaluate-measurement).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these evaluation modules live on Hugging Face Hub as a Space. They
    come with an interactive widget and a documentation card documenting its use and
    limitations. For example [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebd2d536b44a838c1365ef03fe7fa028.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'Each metric, comparison, and measurement is a separate Python module, but for
    using any of them, there is a single entry point: [evaluate.load()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.load)!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Load
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Any metric, comparison, or measurement is loaded with the `evaluate.load` function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to make sure you are loading the right type of evaluation (especially
    if there are name clashes) you can explicitly pass the type:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Community modules
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides the modules implemented in 🤗 Evaluate you can also load any community
    module by specifying the repository ID of the metric implementation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: See the [Creating and Sharing Guide](/docs/evaluate/main/en/creating_and_sharing)
    for information about uploading custom metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: List available modules
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With [list_evaluation_modules()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.list_evaluation_modules)
    you can check what modules are available on the hub. You can also filter for a
    specific modules and skip community metrics if you want. You can also see additional
    information such as likes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Module attributes
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All evalution modules come with a range of useful attributes that help to use
    a module stored in a [EvaluationModuleInfo](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModuleInfo)
    object.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '| Attribute | Description |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| `description` | A short description of the evaluation module. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| `citation` | A BibTex string for citation when available. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| `features` | A `Features` object defining the input format. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| `inputs_description` | This is equivalent to the modules docstring. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| `homepage` | The homepage of the module. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| `license` | The license of the module. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| `codebase_urls` | Link to the code behind the module. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| `reference_urls` | Additional reference URLs. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: 'Let’s have a look at a few examples. First, let’s look at the `description`
    attribute of the accuracy metric:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see that it describes how the metric works in theory. If you use this
    metric for your work, especially if it is an academic publication you want to
    reference it properly. For that you can look at the `citation` attribute:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Before we can apply a metric or other evaluation module to a use-case, we need
    to know what the input format of the metric is:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that features always describe the type of a single input element. In general
    we will add lists of elements so you can always think of a list around the types
    in `features`. Evaluate accepts various input formats (Python lists, NumPy arrays,
    PyTorch tensors, etc.) and converts them to an appropriate format for storage
    and computation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，features始终描述单个输入元素的类型。通常我们会添加元素列表，因此您可以始终将`features`中的类型想象为列表。Evaluate接受各种输入格式（Python列表、NumPy数组、PyTorch张量等），并将它们转换为适合存储和计算的格式。
- en: Compute
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: 'Now that we know how the evaluation module works and what should go in there
    we want to actually use it! When it comes to computing the actual score there
    are two main ways to do it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道评估模块的工作原理和应该放入其中的内容，我们想要实际使用它！在计算实际得分时，有两种主要方法可以做到：
- en: All-in-one
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一体化
- en: Incremental
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增量
- en: In the incremental approach the necessary inputs are added to the module with
    [EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)
    or [EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)
    and the score is calculated at the end with [EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute).
    Alternatively, one can pass all the inputs at once to `compute()`. Let’s have
    a look at the two approaches.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在增量方法中，必要的输入通过[EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)或[EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)添加到模块中，并且最终得分通过[EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)计算。或者，您可以一次性将所有输入传递给`compute()`。让我们看看这两种方法。
- en: How to compute
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何计算
- en: The simplest way to calculate the score of an evaluation module is by calling
    `compute()` directly with the necessary inputs. Simply pass the inputs as seen
    in `features` to the `compute()` method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 计算评估模块得分的最简单方法是直接调用`compute()`并将必要的输入传递给`compute()`方法中的`features`。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Evaluation modules return the results in a dictionary. However, in some instances
    you build up the predictions iteratively or in a distributed fashion in which
    case `add()` or `add_batch()` are useful.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模块以字典形式返回结果。然而，在某些情况下，您可以迭代地或以分布式方式构建预测，在这种情况下`add()`或`add_batch()`会很有用。
- en: Calculate a single metric or a batch of metrics
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算单个指标或一批指标
- en: 'In many evaluation pipelines you build the predictions iteratively such as
    in a for-loop. In that case you could store the predictions in a list and at the
    end pass them to `compute()`. With `add()` and `add_batch()` you can circumvent
    the step of storing the predictions separately. If you are only creating single
    predictions at a time you can use `add()`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多评估流水线中，您会迭代地构建预测，比如在for循环中。在这种情况下，您可以将预测存储在列表中，并在最后将它们传递给`compute()`。使用`add()`和`add_batch()`可以避免单独存储预测的步骤。如果您一次只创建单个预测，您可以使用`add()`：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once you have gathered all predictions you can call `compute()` to compute
    the score based on all stored values. When getting predictions and references
    in batches you can use `add_batch()` which adds a list elements for later processing.
    The rest works as with `add()`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收集到所有预测，您可以调用`compute()`根据所有存储的值计算得分。当以批量方式获取预测和参考值时，您可以使用`add_batch()`，它为以后处理添加了一个元素列表。其余操作与`add()`相同：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is especially useful when you need to get the predictions from your model
    in batches:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要批量从模型获取预测时，这种方法尤其有用：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Distributed evaluation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式评估
- en: Computing metrics in a distributed environment can be tricky. Metric evaluation
    is executed in separate Python processes, or nodes, on different subsets of a
    dataset. Typically, when a metric score is additive (`f(AuB) = f(A) + f(B)`),
    you can use distributed reduce operations to gather the scores for each subset
    of the dataset. But when a metric is non-additive (`f(AuB) ≠ f(A) + f(B)`), it’s
    not that simple. For example, you can’t take the sum of the [F1](https://huggingface.co/spaces/evaluate-metric/f1)
    scores of each data subset as your **final metric**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中计算指标可能会很棘手。指标评估在不同数据集子集上的单独Python进程或节点中执行。通常，当指标得分是可加的（`f(AuB) = f(A)
    + f(B)`）时，您可以使用分布式reduce操作来收集每个数据集子集的得分。但是当指标是非可加的（`f(AuB) ≠ f(A) + f(B)`）时，情况就不那么简单了。例如，您不能将每个数据子集的[F1](https://huggingface.co/spaces/evaluate-metric/f1)得分求和作为您的**最终指标**。
- en: A common way to overcome this issue is to fallback on single process evaluation.
    The metrics are evaluated on a single GPU, which becomes inefficient.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这个问题的常见方法是回退到单进程评估。指标在单个GPU上评估，这变得低效。
- en: 🤗 Evaluate solves this issue by only computing the final metric on the first
    node. The predictions and references are computed and provided to the metric separately
    for each node. These are temporarily stored in an Apache Arrow table, avoiding
    cluttering the GPU or CPU memory. When you are ready to `compute()` the final
    metric, the first node is able to access the predictions and references stored
    on all the other nodes. Once it has gathered all the predictions and references,
    `compute()` will perform the final metric evaluation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Evaluate通过仅在第一个节点上计算最终指标来解决这个问题。预测和参考值分别为每个节点计算并提供给指标。这些暂时存储在Apache Arrow表中，避免了GPU或CPU内存的混乱。当您准备好`compute()`最终指标时，第一个节点能够访问所有其他节点上存储的预测和参考值。一旦收集到所有预测和参考值，`compute()`将执行最终指标评估。
- en: This solution allows 🤗 Evaluate to perform distributed predictions, which is
    important for evaluation speed in distributed settings. At the same time, you
    can also use complex non-additive metrics without wasting valuable GPU or CPU
    memory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案允许🤗 Evaluate执行分布式预测，这对于在分布式环境中提高评估速度很重要。同时，您还可以使用复杂的非可加指标，而不会浪费宝贵的GPU或CPU内存。
- en: Combining several evaluations
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将多个评估结果合并
- en: 'Often one wants to not only evaluate a single metric but a range of different
    metrics capturing different aspects of a model. E.g. for classification it is
    usually a good idea to compute F1-score, recall, and precision in addition to
    accuracy to get a better picture of model performance. Naturally, you can load
    a bunch of metrics and call them sequentially. However, a more convenient way
    is to use the [combine()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.combine)
    function to bundle them together:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `combine` function accepts both the list of names of the metrics as well
    as an instantiated modules. The `compute` call then computes each metric:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Save and push to the Hub
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saving and sharing evaluation results is an important step. We provide the [evaluate.save()](/docs/evaluate/v0.4.0/en/package_reference/saving_methods#evaluate.save)
    function to easily save metrics results. You can either pass a specific filename
    or a directory. In the latter case, the results are saved in a file with an automatically
    created file name. Besides the directory or file name, the function takes any
    key-value pairs as inputs and stores them in a JSON file.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The content of the JSON file look like the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In addition to the specified fields, it also contains useful system information
    for reproducing the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides storing the results locally, you should report them on the model’s
    repository on the Hub. With the [evaluate.push_to_hub()](/docs/evaluate/v0.4.0/en/package_reference/hub_methods#evaluate.push_to_hub)
    function, you can easily report evaluation results to the model’s repository:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluator
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [evaluate.evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    provides automated evaluation and only requires a model, dataset, metric in contrast
    to the metrics in `EvaluationModule`s that require the model’s predictions. As
    such it is easier to evaluate a model on a dataset with a given metric as the
    inference is handled internally. To make that possible it uses the [pipeline](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.pipeline)
    abstraction from `transformers`. However, you can use your own framework as long
    as it follows the `pipeline` interface.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: To make an evaluation with the `evaluator` let’s load a `transformers` pipeline
    (but you can pass your own custom inference class for any framework as long as
    it follows the pipeline call API) with an model trained on IMDb, the IMDb test
    split and the accuracy metric.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then you can create an evaluator for text classification and pass the three
    objects to the `compute()` method. With the label mapping `evaluate` provides
    a method to align the pipeline outputs with the label column in the dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Calculating the value of the metric alone is often not enough to know if a
    model performs significantly better than another one. With *bootstrapping* `evaluate`
    computes confidence intervals and the standard error which helps estimate how
    stable a score is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The evaluator expects a `"text"` and `"label"` column for the data input. If
    your dataset differs you can provide the columns with the keywords `input_column="text"`
    and `label_column="label"`. Currently only `"text-classification"` is supported
    with more tasks being added in the future.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When comparing several models, sometimes it’s hard to spot the differences in
    their performance simply by looking at their scores. Also often there is not a
    single best model but there are trade-offs between e.g. latency and accuracy as
    larger models might have better performance but are also slower. We are gradually
    adding different visualization approaches, such as plots, to make choosing the
    best model for a use-case easier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you have a list of results from multiple models (as dictionaries),
    you can feed them into the `radar_plot()` function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Which lets you visually compare the 4 models and choose the optimal one for
    you, based on one or several metrics:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让您可以通过可视化比较4个模型，并根据一个或多个指标选择最佳模型：
- en: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
- en: Running evaluation on a suite of tasks
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一系列任务上运行评估
- en: It can be useful to evaluate models on a variety of different tasks to understand
    their downstream performance. The [EvaluationSuite](evaluation_suite) enables
    evaluation of models on a collection of tasks. Tasks can be constructed as ([evaluator](base_evaluator),
    dataset, metric) tuples and passed to an [EvaluationSuite](evaluation_suite) stored
    on the Hugging Face Hub as a Space, or locally as a Python script. See the [evaluator
    documentation](base_evaluator) for a list of currently supported tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种不同任务上评估模型可能很有用，以了解它们的下游性能。[EvaluationSuite](evaluation_suite)使得可以在一组任务上评估模型。任务可以构建为（[evaluator](base_evaluator)、数据集、指标）元组，并传递给存储在Hugging
    Face Hub上的[EvaluationSuite](evaluation_suite)作为一个Space，或者本地作为Python脚本。请参阅[评估器文档](base_evaluator)以获取当前支持的任务列表。
- en: '`EvaluationSuite` scripts can be defined as follows, and supports Python code
    for data preprocessing.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`EvaluationSuite`脚本可以定义如下，并支持用于数据预处理的Python代码。'
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Evaluation can be run by loading the `EvaluationSuite` and calling `run()` method
    with a model or pipeline.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 评估可以通过加载`EvaluationSuite`并调用`run()`方法来运行，传入一个模型或pipeline。
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '| accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds
    | task_name |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 总时间（秒） | 每秒样本数 | 延迟时间（秒） | 任务名称 |'
- en: '| --: | --: | --: | :-- | :-- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | :-- | :-- |'
- en: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
- en: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
