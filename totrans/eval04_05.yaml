- en: A quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速浏览
- en: 'Original text: [https://huggingface.co/docs/evaluate/a_quick_tour](https://huggingface.co/docs/evaluate/a_quick_tour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/evaluate/a_quick_tour](https://huggingface.co/docs/evaluate/a_quick_tour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Evaluate provides access to a wide range of evaluation tools. It covers a
    range of modalities such as text, computer vision, audio, etc. as well as tools
    to evaluate models or datasets. These tools are split into three categories.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Evaluate提供了广泛的评估工具。它涵盖了文本、计算机视觉、音频等多种形式，以及用于评估模型或数据集的工具。这些工具分为三类。
- en: Types of evaluations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估类型
- en: 'There are different aspects of a typical machine learning pipeline that can
    be evaluated and for each aspect 🤗 Evaluate provides a tool:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 典型机器学习流水线的不同方面可以进行评估，对于每个方面🤗 Evaluate都提供了一个工具：
- en: '**Metric**: A metric is used to evaluate a model’s performance and usually
    involves the model’s predictions as well as some ground truth labels. You can
    find all integrated metrics at [evaluate-metric](https://huggingface.co/evaluate-metric).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量**：度量用于评估模型的性能，通常涉及模型的预测以及一些地面真实标签。您可以在[evaluate-metric](https://huggingface.co/evaluate-metric)找到所有集成的度量。'
- en: '**Comparison**: A comparison is used to compare two models. This can for example
    be done by comparing their predictions to ground truth labels and computing their
    agreement. You can find all integrated comparisons at [evaluate-comparison](https://huggingface.co/evaluate-comparison).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较**：比较用于比较两个模型。例如，可以通过将它们的预测与地面真实标签进行比较并计算它们的一致性来进行比较。您可以在[evaluate-comparison](https://huggingface.co/evaluate-comparison)找到所有集成的比较。'
- en: '**Measurement**: The dataset is as important as the model trained on it. With
    measurements one can investigate a dataset’s properties. You can find all integrated
    measurements at [evaluate-measurement](https://huggingface.co/evaluate-measurement).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量**：数据集和训练模型一样重要。通过测量，可以研究数据集的属性。您可以在[evaluate-measurement](https://huggingface.co/evaluate-measurement)找到所有集成的测量。'
- en: 'Each of these evaluation modules live on Hugging Face Hub as a Space. They
    come with an interactive widget and a documentation card documenting its use and
    limitations. For example [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估模块中的每一个都存在于Hugging Face Hub上作为一个Space。它们带有一个交互式小部件和一个文档卡，记录了其使用和限制。例如[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)：
- en: '![](../Images/ebd2d536b44a838c1365ef03fe7fa028.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebd2d536b44a838c1365ef03fe7fa028.png)'
- en: 'Each metric, comparison, and measurement is a separate Python module, but for
    using any of them, there is a single entry point: [evaluate.load()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.load)!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个度量、比较和测量都是一个单独的Python模块，但要使用其中任何一个，都有一个单一的入口点：[evaluate.load()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.load)!
- en: Load
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载
- en: 'Any metric, comparison, or measurement is loaded with the `evaluate.load` function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何度量、比较或测量都可以使用`evaluate.load`函数加载：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to make sure you are loading the right type of evaluation (especially
    if there are name clashes) you can explicitly pass the type:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想确保加载正确类型的评估（特别是如果存在名称冲突），您可以明确传递类型：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Community modules
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社区模块
- en: 'Besides the modules implemented in 🤗 Evaluate you can also load any community
    module by specifying the repository ID of the metric implementation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了🤗 Evaluate中实现的模块外，您还可以通过指定度量实现的存储库ID加载任何社区模块：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: See the [Creating and Sharing Guide](/docs/evaluate/main/en/creating_and_sharing)
    for information about uploading custom metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有关上传自定义度量的信息，请参阅[创建和共享指南](/docs/evaluate/main/en/creating_and_sharing)。
- en: List available modules
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列出可用模块
- en: 'With [list_evaluation_modules()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.list_evaluation_modules)
    you can check what modules are available on the hub. You can also filter for a
    specific modules and skip community metrics if you want. You can also see additional
    information such as likes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[list_evaluation_modules()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.list_evaluation_modules)可以检查Hub上有哪些模块可用。您还可以过滤特定模块并跳过社区度量。您还可以查看额外信息，如喜欢数：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Module attributes
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模块属性
- en: All evalution modules come with a range of useful attributes that help to use
    a module stored in a [EvaluationModuleInfo](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModuleInfo)
    object.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所有评估模块都带有一系列有用的属性，可帮助使用存储在[EvaluationModuleInfo](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModuleInfo)对象中的模块。
- en: '| Attribute | Description |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: 属性 | 描述 |
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `description` | A short description of the evaluation module. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `description` | 评估模块的简要描述。 |'
- en: '| `citation` | A BibTex string for citation when available. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `citation` | 当可用时用于引用的BibTex字符串。 |'
- en: '| `features` | A `Features` object defining the input format. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `features` | 定义输入格式的`Features`对象。 |'
- en: '| `inputs_description` | This is equivalent to the modules docstring. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `inputs_description` | 这相当于模块的文档字符串。 |'
- en: '| `homepage` | The homepage of the module. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `homepage` | 模块的主页。 |'
- en: '| `license` | The license of the module. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `license` | 模块的许可证。 |'
- en: '| `codebase_urls` | Link to the code behind the module. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `codebase_urls` | 模块背后的代码链接。 |'
- en: '| `reference_urls` | Additional reference URLs. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `reference_urls` | 附加参考链接。 |'
- en: 'Let’s have a look at a few examples. First, let’s look at the `description`
    attribute of the accuracy metric:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个例子。首先，让我们看一下准确度度量的`description`属性：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see that it describes how the metric works in theory. If you use this
    metric for your work, especially if it is an academic publication you want to
    reference it properly. For that you can look at the `citation` attribute:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到它描述了度量在理论上的工作原理。如果您在工作中使用此度量，特别是如果它是学术出版物，您希望正确引用它。为此，您可以查看`citation`属性：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Before we can apply a metric or other evaluation module to a use-case, we need
    to know what the input format of the metric is:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以将度量或其他评估模块应用于用例之前，我们需要知道度量的输入格式是什么：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that features always describe the type of a single input element. In general
    we will add lists of elements so you can always think of a list around the types
    in `features`. Evaluate accepts various input formats (Python lists, NumPy arrays,
    PyTorch tensors, etc.) and converts them to an appropriate format for storage
    and computation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，features始终描述单个输入元素的类型。通常我们会添加元素列表，因此您可以始终将`features`中的类型想象为列表。Evaluate接受各种输入格式（Python列表、NumPy数组、PyTorch张量等），并将它们转换为适合存储和计算的格式。
- en: Compute
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: 'Now that we know how the evaluation module works and what should go in there
    we want to actually use it! When it comes to computing the actual score there
    are two main ways to do it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道评估模块的工作原理和应该放入其中的内容，我们想要实际使用它！在计算实际得分时，有两种主要方法可以做到：
- en: All-in-one
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一体化
- en: Incremental
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增量
- en: In the incremental approach the necessary inputs are added to the module with
    [EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)
    or [EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)
    and the score is calculated at the end with [EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute).
    Alternatively, one can pass all the inputs at once to `compute()`. Let’s have
    a look at the two approaches.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在增量方法中，必要的输入通过[EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)或[EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)添加到模块中，并且最终得分通过[EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)计算。或者，您可以一次性将所有输入传递给`compute()`。让我们看看这两种方法。
- en: How to compute
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何计算
- en: The simplest way to calculate the score of an evaluation module is by calling
    `compute()` directly with the necessary inputs. Simply pass the inputs as seen
    in `features` to the `compute()` method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 计算评估模块得分的最简单方法是直接调用`compute()`并将必要的输入传递给`compute()`方法中的`features`。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Evaluation modules return the results in a dictionary. However, in some instances
    you build up the predictions iteratively or in a distributed fashion in which
    case `add()` or `add_batch()` are useful.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模块以字典形式返回结果。然而，在某些情况下，您可以迭代地或以分布式方式构建预测，在这种情况下`add()`或`add_batch()`会很有用。
- en: Calculate a single metric or a batch of metrics
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算单个指标或一批指标
- en: 'In many evaluation pipelines you build the predictions iteratively such as
    in a for-loop. In that case you could store the predictions in a list and at the
    end pass them to `compute()`. With `add()` and `add_batch()` you can circumvent
    the step of storing the predictions separately. If you are only creating single
    predictions at a time you can use `add()`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多评估流水线中，您会迭代地构建预测，比如在for循环中。在这种情况下，您可以将预测存储在列表中，并在最后将它们传递给`compute()`。使用`add()`和`add_batch()`可以避免单独存储预测的步骤。如果您一次只创建单个预测，您可以使用`add()`：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once you have gathered all predictions you can call `compute()` to compute
    the score based on all stored values. When getting predictions and references
    in batches you can use `add_batch()` which adds a list elements for later processing.
    The rest works as with `add()`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收集到所有预测，您可以调用`compute()`根据所有存储的值计算得分。当以批量方式获取预测和参考值时，您可以使用`add_batch()`，它为以后处理添加了一个元素列表。其余操作与`add()`相同：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is especially useful when you need to get the predictions from your model
    in batches:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要批量从模型获取预测时，这种方法尤其有用：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Distributed evaluation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式评估
- en: Computing metrics in a distributed environment can be tricky. Metric evaluation
    is executed in separate Python processes, or nodes, on different subsets of a
    dataset. Typically, when a metric score is additive (`f(AuB) = f(A) + f(B)`),
    you can use distributed reduce operations to gather the scores for each subset
    of the dataset. But when a metric is non-additive (`f(AuB) ≠ f(A) + f(B)`), it’s
    not that simple. For example, you can’t take the sum of the [F1](https://huggingface.co/spaces/evaluate-metric/f1)
    scores of each data subset as your **final metric**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中计算指标可能会很棘手。指标评估在不同数据集子集上的单独Python进程或节点中执行。通常，当指标得分是可加的（`f(AuB) = f(A)
    + f(B)`）时，您可以使用分布式reduce操作来收集每个数据集子集的得分。但是当指标是非可加的（`f(AuB) ≠ f(A) + f(B)`）时，情况就不那么简单了。例如，您不能将每个数据子集的[F1](https://huggingface.co/spaces/evaluate-metric/f1)得分求和作为您的**最终指标**。
- en: A common way to overcome this issue is to fallback on single process evaluation.
    The metrics are evaluated on a single GPU, which becomes inefficient.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这个问题的常见方法是回退到单进程评估。指标在单个GPU上评估，这变得低效。
- en: 🤗 Evaluate solves this issue by only computing the final metric on the first
    node. The predictions and references are computed and provided to the metric separately
    for each node. These are temporarily stored in an Apache Arrow table, avoiding
    cluttering the GPU or CPU memory. When you are ready to `compute()` the final
    metric, the first node is able to access the predictions and references stored
    on all the other nodes. Once it has gathered all the predictions and references,
    `compute()` will perform the final metric evaluation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Evaluate通过仅在第一个节点上计算最终指标来解决这个问题。预测和参考值分别为每个节点计算并提供给指标。这些暂时存储在Apache Arrow表中，避免了GPU或CPU内存的混乱。当您准备好`compute()`最终指标时，第一个节点能够访问所有其他节点上存储的预测和参考值。一旦收集到所有预测和参考值，`compute()`将执行最终指标评估。
- en: This solution allows 🤗 Evaluate to perform distributed predictions, which is
    important for evaluation speed in distributed settings. At the same time, you
    can also use complex non-additive metrics without wasting valuable GPU or CPU
    memory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案允许🤗 Evaluate执行分布式预测，这对于在分布式环境中提高评估速度很重要。同时，您还可以使用复杂的非可加指标，而不会浪费宝贵的GPU或CPU内存。
- en: Combining several evaluations
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将多个评估结果合并
- en: 'Often one wants to not only evaluate a single metric but a range of different
    metrics capturing different aspects of a model. E.g. for classification it is
    usually a good idea to compute F1-score, recall, and precision in addition to
    accuracy to get a better picture of model performance. Naturally, you can load
    a bunch of metrics and call them sequentially. However, a more convenient way
    is to use the [combine()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.combine)
    function to bundle them together:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们不仅想评估单个指标，而且想评估捕捉模型不同方面的一系列不同指标。例如，对于分类，通常计算F1分数、召回率和精确度是一个好主意，以便除了准确度外获得模型性能的更全面的图片。当然，您可以加载一堆指标并依次调用它们。然而，一个更方便的方法是使用[combine()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.combine)函数将它们捆绑在一起：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `combine` function accepts both the list of names of the metrics as well
    as an instantiated modules. The `compute` call then computes each metric:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`combine`函数接受指标名称列表以及实例化的模块。然后`compute`调用计算每个指标：'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Save and push to the Hub
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存并推送到Hub
- en: Saving and sharing evaluation results is an important step. We provide the [evaluate.save()](/docs/evaluate/v0.4.0/en/package_reference/saving_methods#evaluate.save)
    function to easily save metrics results. You can either pass a specific filename
    or a directory. In the latter case, the results are saved in a file with an automatically
    created file name. Besides the directory or file name, the function takes any
    key-value pairs as inputs and stores them in a JSON file.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 保存和分享评估结果是一个重要的步骤。我们提供了[evaluate.save()](/docs/evaluate/v0.4.0/en/package_reference/saving_methods#evaluate.save)函数来轻松保存指标结果。您可以传递一个特定的文件名或目录。在后一种情况下，结果将保存在一个自动创建的文件名的文件中。除了目录或文件名，该函数还接受任何键值对作为输入，并将它们存储在一个JSON文件中。
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The content of the JSON file look like the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件的内容如下：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In addition to the specified fields, it also contains useful system information
    for reproducing the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了指定的字段，它还包含有用的系统信息，以便重现结果。
- en: 'Besides storing the results locally, you should report them on the model’s
    repository on the Hub. With the [evaluate.push_to_hub()](/docs/evaluate/v0.4.0/en/package_reference/hub_methods#evaluate.push_to_hub)
    function, you can easily report evaluation results to the model’s repository:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在本地存储结果，您还应该将它们报告到Hub上的模型存储库。使用[evaluate.push_to_hub()](/docs/evaluate/v0.4.0/en/package_reference/hub_methods#evaluate.push_to_hub)函数，您可以轻松地将评估结果报告到模型存储库：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluator
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估器
- en: The [evaluate.evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    provides automated evaluation and only requires a model, dataset, metric in contrast
    to the metrics in `EvaluationModule`s that require the model’s predictions. As
    such it is easier to evaluate a model on a dataset with a given metric as the
    inference is handled internally. To make that possible it uses the [pipeline](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.pipeline)
    abstraction from `transformers`. However, you can use your own framework as long
    as it follows the `pipeline` interface.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[evaluate.evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)提供了自动化评估，只需要一个模型、数据集和指标，与`EvaluationModule`中需要模型预测的指标相比，更容易评估一个模型在给定指标下的数据集上的表现，因为推理是在内部处理的。为了实现这一点，它使用了`transformers`中的[pipeline](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.pipeline)抽象。然而，只要遵循`pipeline`接口，您可以使用自己的框架。'
- en: To make an evaluation with the `evaluator` let’s load a `transformers` pipeline
    (but you can pass your own custom inference class for any framework as long as
    it follows the pipeline call API) with an model trained on IMDb, the IMDb test
    split and the accuracy metric.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`evaluator`进行评估，让我们加载一个`transformers`管道（但您可以传递自己的自定义推理类，只要它遵循管道调用API），该管道使用在IMDb上训练的模型，IMDb测试拆分和准确度指标。
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then you can create an evaluator for text classification and pass the three
    objects to the `compute()` method. With the label mapping `evaluate` provides
    a method to align the pipeline outputs with the label column in the dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以为文本分类创建一个评估器，并将这三个对象传递给`compute()`方法。通过标签映射，`evaluate`提供了一种方法来将管道输出与数据集中的标签列对齐：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Calculating the value of the metric alone is often not enough to know if a
    model performs significantly better than another one. With *bootstrapping* `evaluate`
    computes confidence intervals and the standard error which helps estimate how
    stable a score is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 仅计算指标的值通常不足以知道一个模型是否比另一个模型表现显著更好。通过*bootstrapping*，`evaluate`计算置信区间和标准误差，这有助于估计得分的稳定性有多高：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The evaluator expects a `"text"` and `"label"` column for the data input. If
    your dataset differs you can provide the columns with the keywords `input_column="text"`
    and `label_column="label"`. Currently only `"text-classification"` is supported
    with more tasks being added in the future.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器期望数据输入中有一个`"text"`和`"label"`列。如果您的数据集不同，您可以使用关键字`input_column="text"`和`label_column="label"`提供列。目前仅支持`"text-classification"`，未来将添加更多任务。
- en: Visualization
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化
- en: When comparing several models, sometimes it’s hard to spot the differences in
    their performance simply by looking at their scores. Also often there is not a
    single best model but there are trade-offs between e.g. latency and accuracy as
    larger models might have better performance but are also slower. We are gradually
    adding different visualization approaches, such as plots, to make choosing the
    best model for a use-case easier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较几个模型时，有时仅通过查看它们的分数很难发现它们性能的差异。通常也不是一个最佳模型，而是在延迟和准确性之间存在权衡，例如更大的模型可能性能更好，但也更慢。我们正在逐步添加不同的可视化方法，例如图表，以便更容易地为用例选择最佳模型。
- en: 'For instance, if you have a list of results from multiple models (as dictionaries),
    you can feed them into the `radar_plot()` function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您有来自多个模型的结果列表（作为字典），您可以将它们输入到`radar_plot()`函数中：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Which lets you visually compare the 4 models and choose the optimal one for
    you, based on one or several metrics:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让您可以通过可视化比较4个模型，并根据一个或多个指标选择最佳模型：
- en: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
- en: Running evaluation on a suite of tasks
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一系列任务上运行评估
- en: It can be useful to evaluate models on a variety of different tasks to understand
    their downstream performance. The [EvaluationSuite](evaluation_suite) enables
    evaluation of models on a collection of tasks. Tasks can be constructed as ([evaluator](base_evaluator),
    dataset, metric) tuples and passed to an [EvaluationSuite](evaluation_suite) stored
    on the Hugging Face Hub as a Space, or locally as a Python script. See the [evaluator
    documentation](base_evaluator) for a list of currently supported tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种不同任务上评估模型可能很有用，以了解它们的下游性能。[EvaluationSuite](evaluation_suite)使得可以在一组任务上评估模型。任务可以构建为（[evaluator](base_evaluator)、数据集、指标）元组，并传递给存储在Hugging
    Face Hub上的[EvaluationSuite](evaluation_suite)作为一个Space，或者本地作为Python脚本。请参阅[评估器文档](base_evaluator)以获取当前支持的任务列表。
- en: '`EvaluationSuite` scripts can be defined as follows, and supports Python code
    for data preprocessing.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`EvaluationSuite`脚本可以定义如下，并支持用于数据预处理的Python代码。'
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Evaluation can be run by loading the `EvaluationSuite` and calling `run()` method
    with a model or pipeline.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 评估可以通过加载`EvaluationSuite`并调用`run()`方法来运行，传入一个模型或pipeline。
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '| accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds
    | task_name |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 总时间（秒） | 每秒样本数 | 延迟时间（秒） | 任务名称 |'
- en: '| --: | --: | --: | :-- | :-- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | :-- | :-- |'
- en: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
- en: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
