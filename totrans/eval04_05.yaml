- en: A quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/a_quick_tour](https://huggingface.co/docs/evaluate/a_quick_tour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Evaluate provides access to a wide range of evaluation tools. It covers a
    range of modalities such as text, computer vision, audio, etc. as well as tools
    to evaluate models or datasets. These tools are split into three categories.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Types of evaluations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different aspects of a typical machine learning pipeline that can
    be evaluated and for each aspect ğŸ¤— Evaluate provides a tool:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '**Metric**: A metric is used to evaluate a modelâ€™s performance and usually
    involves the modelâ€™s predictions as well as some ground truth labels. You can
    find all integrated metrics at [evaluate-metric](https://huggingface.co/evaluate-metric).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparison**: A comparison is used to compare two models. This can for example
    be done by comparing their predictions to ground truth labels and computing their
    agreement. You can find all integrated comparisons at [evaluate-comparison](https://huggingface.co/evaluate-comparison).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement**: The dataset is as important as the model trained on it. With
    measurements one can investigate a datasetâ€™s properties. You can find all integrated
    measurements at [evaluate-measurement](https://huggingface.co/evaluate-measurement).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these evaluation modules live on Hugging Face Hub as a Space. They
    come with an interactive widget and a documentation card documenting its use and
    limitations. For example [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebd2d536b44a838c1365ef03fe7fa028.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'Each metric, comparison, and measurement is a separate Python module, but for
    using any of them, there is a single entry point: [evaluate.load()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.load)!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Load
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Any metric, comparison, or measurement is loaded with the `evaluate.load` function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to make sure you are loading the right type of evaluation (especially
    if there are name clashes) you can explicitly pass the type:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Community modules
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides the modules implemented in ğŸ¤— Evaluate you can also load any community
    module by specifying the repository ID of the metric implementation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: See the [Creating and Sharing Guide](/docs/evaluate/main/en/creating_and_sharing)
    for information about uploading custom metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: List available modules
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With [list_evaluation_modules()](/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.list_evaluation_modules)
    you can check what modules are available on the hub. You can also filter for a
    specific modules and skip community metrics if you want. You can also see additional
    information such as likes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Module attributes
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All evalution modules come with a range of useful attributes that help to use
    a module stored in a [EvaluationModuleInfo](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModuleInfo)
    object.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '| Attribute | Description |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| `description` | A short description of the evaluation module. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| `citation` | A BibTex string for citation when available. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| `features` | A `Features` object defining the input format. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| `inputs_description` | This is equivalent to the modules docstring. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| `homepage` | The homepage of the module. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| `license` | The license of the module. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| `codebase_urls` | Link to the code behind the module. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| `reference_urls` | Additional reference URLs. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: 'Letâ€™s have a look at a few examples. First, letâ€™s look at the `description`
    attribute of the accuracy metric:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see that it describes how the metric works in theory. If you use this
    metric for your work, especially if it is an academic publication you want to
    reference it properly. For that you can look at the `citation` attribute:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Before we can apply a metric or other evaluation module to a use-case, we need
    to know what the input format of the metric is:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that features always describe the type of a single input element. In general
    we will add lists of elements so you can always think of a list around the types
    in `features`. Evaluate accepts various input formats (Python lists, NumPy arrays,
    PyTorch tensors, etc.) and converts them to an appropriate format for storage
    and computation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œfeatureså§‹ç»ˆæè¿°å•ä¸ªè¾“å…¥å…ƒç´ çš„ç±»å‹ã€‚é€šå¸¸æˆ‘ä»¬ä¼šæ·»åŠ å…ƒç´ åˆ—è¡¨ï¼Œå› æ­¤æ‚¨å¯ä»¥å§‹ç»ˆå°†`features`ä¸­çš„ç±»å‹æƒ³è±¡ä¸ºåˆ—è¡¨ã€‚Evaluateæ¥å—å„ç§è¾“å…¥æ ¼å¼ï¼ˆPythonåˆ—è¡¨ã€NumPyæ•°ç»„ã€PyTorchå¼ é‡ç­‰ï¼‰ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºé€‚åˆå­˜å‚¨å’Œè®¡ç®—çš„æ ¼å¼ã€‚
- en: Compute
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¡ç®—
- en: 'Now that we know how the evaluation module works and what should go in there
    we want to actually use it! When it comes to computing the actual score there
    are two main ways to do it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“è¯„ä¼°æ¨¡å—çš„å·¥ä½œåŸç†å’Œåº”è¯¥æ”¾å…¥å…¶ä¸­çš„å†…å®¹ï¼Œæˆ‘ä»¬æƒ³è¦å®é™…ä½¿ç”¨å®ƒï¼åœ¨è®¡ç®—å®é™…å¾—åˆ†æ—¶ï¼Œæœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•å¯ä»¥åšåˆ°ï¼š
- en: All-in-one
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä½“åŒ–
- en: Incremental
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¢é‡
- en: In the incremental approach the necessary inputs are added to the module with
    [EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)
    or [EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)
    and the score is calculated at the end with [EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute).
    Alternatively, one can pass all the inputs at once to `compute()`. Letâ€™s have
    a look at the two approaches.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¢é‡æ–¹æ³•ä¸­ï¼Œå¿…è¦çš„è¾“å…¥é€šè¿‡[EvaluationModule.add()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add)æˆ–[EvaluationModule.add_batch()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch)æ·»åŠ åˆ°æ¨¡å—ä¸­ï¼Œå¹¶ä¸”æœ€ç»ˆå¾—åˆ†é€šè¿‡[EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)è®¡ç®—ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰è¾“å…¥ä¼ é€’ç»™`compute()`ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸¤ç§æ–¹æ³•ã€‚
- en: How to compute
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•è®¡ç®—
- en: The simplest way to calculate the score of an evaluation module is by calling
    `compute()` directly with the necessary inputs. Simply pass the inputs as seen
    in `features` to the `compute()` method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¯„ä¼°æ¨¡å—å¾—åˆ†çš„æœ€ç®€å•æ–¹æ³•æ˜¯ç›´æ¥è°ƒç”¨`compute()`å¹¶å°†å¿…è¦çš„è¾“å…¥ä¼ é€’ç»™`compute()`æ–¹æ³•ä¸­çš„`features`ã€‚
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Evaluation modules return the results in a dictionary. However, in some instances
    you build up the predictions iteratively or in a distributed fashion in which
    case `add()` or `add_batch()` are useful.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¨¡å—ä»¥å­—å…¸å½¢å¼è¿”å›ç»“æœã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥è¿­ä»£åœ°æˆ–ä»¥åˆ†å¸ƒå¼æ–¹å¼æ„å»ºé¢„æµ‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹`add()`æˆ–`add_batch()`ä¼šå¾ˆæœ‰ç”¨ã€‚
- en: Calculate a single metric or a batch of metrics
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—å•ä¸ªæŒ‡æ ‡æˆ–ä¸€æ‰¹æŒ‡æ ‡
- en: 'In many evaluation pipelines you build the predictions iteratively such as
    in a for-loop. In that case you could store the predictions in a list and at the
    end pass them to `compute()`. With `add()` and `add_batch()` you can circumvent
    the step of storing the predictions separately. If you are only creating single
    predictions at a time you can use `add()`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šè¯„ä¼°æµæ°´çº¿ä¸­ï¼Œæ‚¨ä¼šè¿­ä»£åœ°æ„å»ºé¢„æµ‹ï¼Œæ¯”å¦‚åœ¨forå¾ªç¯ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å°†é¢„æµ‹å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ï¼Œå¹¶åœ¨æœ€åå°†å®ƒä»¬ä¼ é€’ç»™`compute()`ã€‚ä½¿ç”¨`add()`å’Œ`add_batch()`å¯ä»¥é¿å…å•ç‹¬å­˜å‚¨é¢„æµ‹çš„æ­¥éª¤ã€‚å¦‚æœæ‚¨ä¸€æ¬¡åªåˆ›å»ºå•ä¸ªé¢„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`add()`ï¼š
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once you have gathered all predictions you can call `compute()` to compute
    the score based on all stored values. When getting predictions and references
    in batches you can use `add_batch()` which adds a list elements for later processing.
    The rest works as with `add()`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ”¶é›†åˆ°æ‰€æœ‰é¢„æµ‹ï¼Œæ‚¨å¯ä»¥è°ƒç”¨`compute()`æ ¹æ®æ‰€æœ‰å­˜å‚¨çš„å€¼è®¡ç®—å¾—åˆ†ã€‚å½“ä»¥æ‰¹é‡æ–¹å¼è·å–é¢„æµ‹å’Œå‚è€ƒå€¼æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`add_batch()`ï¼Œå®ƒä¸ºä»¥åå¤„ç†æ·»åŠ äº†ä¸€ä¸ªå…ƒç´ åˆ—è¡¨ã€‚å…¶ä½™æ“ä½œä¸`add()`ç›¸åŒï¼š
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is especially useful when you need to get the predictions from your model
    in batches:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨éœ€è¦æ‰¹é‡ä»æ¨¡å‹è·å–é¢„æµ‹æ—¶ï¼Œè¿™ç§æ–¹æ³•å°¤å…¶æœ‰ç”¨ï¼š
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Distributed evaluation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è¯„ä¼°
- en: Computing metrics in a distributed environment can be tricky. Metric evaluation
    is executed in separate Python processes, or nodes, on different subsets of a
    dataset. Typically, when a metric score is additive (`f(AuB) = f(A) + f(B)`),
    you can use distributed reduce operations to gather the scores for each subset
    of the dataset. But when a metric is non-additive (`f(AuB) â‰  f(A) + f(B)`), itâ€™s
    not that simple. For example, you canâ€™t take the sum of the [F1](https://huggingface.co/spaces/evaluate-metric/f1)
    scores of each data subset as your **final metric**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®¡ç®—æŒ‡æ ‡å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚æŒ‡æ ‡è¯„ä¼°åœ¨ä¸åŒæ•°æ®é›†å­é›†ä¸Šçš„å•ç‹¬Pythonè¿›ç¨‹æˆ–èŠ‚ç‚¹ä¸­æ‰§è¡Œã€‚é€šå¸¸ï¼Œå½“æŒ‡æ ‡å¾—åˆ†æ˜¯å¯åŠ çš„ï¼ˆ`f(AuB) = f(A)
    + f(B)`ï¼‰æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼reduceæ“ä½œæ¥æ”¶é›†æ¯ä¸ªæ•°æ®é›†å­é›†çš„å¾—åˆ†ã€‚ä½†æ˜¯å½“æŒ‡æ ‡æ˜¯éå¯åŠ çš„ï¼ˆ`f(AuB) â‰  f(A) + f(B)`ï¼‰æ—¶ï¼Œæƒ…å†µå°±ä¸é‚£ä¹ˆç®€å•äº†ã€‚ä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°†æ¯ä¸ªæ•°æ®å­é›†çš„[F1](https://huggingface.co/spaces/evaluate-metric/f1)å¾—åˆ†æ±‚å’Œä½œä¸ºæ‚¨çš„**æœ€ç»ˆæŒ‡æ ‡**ã€‚
- en: A common way to overcome this issue is to fallback on single process evaluation.
    The metrics are evaluated on a single GPU, which becomes inefficient.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å…‹æœè¿™ä¸ªé—®é¢˜çš„å¸¸è§æ–¹æ³•æ˜¯å›é€€åˆ°å•è¿›ç¨‹è¯„ä¼°ã€‚æŒ‡æ ‡åœ¨å•ä¸ªGPUä¸Šè¯„ä¼°ï¼Œè¿™å˜å¾—ä½æ•ˆã€‚
- en: ğŸ¤— Evaluate solves this issue by only computing the final metric on the first
    node. The predictions and references are computed and provided to the metric separately
    for each node. These are temporarily stored in an Apache Arrow table, avoiding
    cluttering the GPU or CPU memory. When you are ready to `compute()` the final
    metric, the first node is able to access the predictions and references stored
    on all the other nodes. Once it has gathered all the predictions and references,
    `compute()` will perform the final metric evaluation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Evaluateé€šè¿‡ä»…åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè®¡ç®—æœ€ç»ˆæŒ‡æ ‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¢„æµ‹å’Œå‚è€ƒå€¼åˆ†åˆ«ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¡ç®—å¹¶æä¾›ç»™æŒ‡æ ‡ã€‚è¿™äº›æš‚æ—¶å­˜å‚¨åœ¨Apache Arrowè¡¨ä¸­ï¼Œé¿å…äº†GPUæˆ–CPUå†…å­˜çš„æ··ä¹±ã€‚å½“æ‚¨å‡†å¤‡å¥½`compute()`æœ€ç»ˆæŒ‡æ ‡æ—¶ï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹èƒ½å¤Ÿè®¿é—®æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸Šå­˜å‚¨çš„é¢„æµ‹å’Œå‚è€ƒå€¼ã€‚ä¸€æ—¦æ”¶é›†åˆ°æ‰€æœ‰é¢„æµ‹å’Œå‚è€ƒå€¼ï¼Œ`compute()`å°†æ‰§è¡Œæœ€ç»ˆæŒ‡æ ‡è¯„ä¼°ã€‚
- en: This solution allows ğŸ¤— Evaluate to perform distributed predictions, which is
    important for evaluation speed in distributed settings. At the same time, you
    can also use complex non-additive metrics without wasting valuable GPU or CPU
    memory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè§£å†³æ–¹æ¡ˆå…è®¸ğŸ¤— Evaluateæ‰§è¡Œåˆ†å¸ƒå¼é¢„æµ‹ï¼Œè¿™å¯¹äºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­æé«˜è¯„ä¼°é€Ÿåº¦å¾ˆé‡è¦ã€‚åŒæ—¶ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨å¤æ‚çš„éå¯åŠ æŒ‡æ ‡ï¼Œè€Œä¸ä¼šæµªè´¹å®è´µçš„GPUæˆ–CPUå†…å­˜ã€‚
- en: Combining several evaluations
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†å¤šä¸ªè¯„ä¼°ç»“æœåˆå¹¶
- en: 'Often one wants to not only evaluate a single metric but a range of different
    metrics capturing different aspects of a model. E.g. for classification it is
    usually a good idea to compute F1-score, recall, and precision in addition to
    accuracy to get a better picture of model performance. Naturally, you can load
    a bunch of metrics and call them sequentially. However, a more convenient way
    is to use the [combine()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.combine)
    function to bundle them together:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `combine` function accepts both the list of names of the metrics as well
    as an instantiated modules. The `compute` call then computes each metric:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Save and push to the Hub
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saving and sharing evaluation results is an important step. We provide the [evaluate.save()](/docs/evaluate/v0.4.0/en/package_reference/saving_methods#evaluate.save)
    function to easily save metrics results. You can either pass a specific filename
    or a directory. In the latter case, the results are saved in a file with an automatically
    created file name. Besides the directory or file name, the function takes any
    key-value pairs as inputs and stores them in a JSON file.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The content of the JSON file look like the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In addition to the specified fields, it also contains useful system information
    for reproducing the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides storing the results locally, you should report them on the modelâ€™s
    repository on the Hub. With the [evaluate.push_to_hub()](/docs/evaluate/v0.4.0/en/package_reference/hub_methods#evaluate.push_to_hub)
    function, you can easily report evaluation results to the modelâ€™s repository:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluator
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [evaluate.evaluator()](/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.evaluator)
    provides automated evaluation and only requires a model, dataset, metric in contrast
    to the metrics in `EvaluationModule`s that require the modelâ€™s predictions. As
    such it is easier to evaluate a model on a dataset with a given metric as the
    inference is handled internally. To make that possible it uses the [pipeline](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.pipeline)
    abstraction from `transformers`. However, you can use your own framework as long
    as it follows the `pipeline` interface.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: To make an evaluation with the `evaluator` letâ€™s load a `transformers` pipeline
    (but you can pass your own custom inference class for any framework as long as
    it follows the pipeline call API) with an model trained on IMDb, the IMDb test
    split and the accuracy metric.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then you can create an evaluator for text classification and pass the three
    objects to the `compute()` method. With the label mapping `evaluate` provides
    a method to align the pipeline outputs with the label column in the dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Calculating the value of the metric alone is often not enough to know if a
    model performs significantly better than another one. With *bootstrapping* `evaluate`
    computes confidence intervals and the standard error which helps estimate how
    stable a score is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The evaluator expects a `"text"` and `"label"` column for the data input. If
    your dataset differs you can provide the columns with the keywords `input_column="text"`
    and `label_column="label"`. Currently only `"text-classification"` is supported
    with more tasks being added in the future.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When comparing several models, sometimes itâ€™s hard to spot the differences in
    their performance simply by looking at their scores. Also often there is not a
    single best model but there are trade-offs between e.g. latency and accuracy as
    larger models might have better performance but are also slower. We are gradually
    adding different visualization approaches, such as plots, to make choosing the
    best model for a use-case easier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you have a list of results from multiple models (as dictionaries),
    you can feed them into the `radar_plot()` function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Which lets you visually compare the 4 models and choose the optimal one for
    you, based on one or several metrics:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æ‚¨å¯ä»¥é€šè¿‡å¯è§†åŒ–æ¯”è¾ƒ4ä¸ªæ¨¡å‹ï¼Œå¹¶æ ¹æ®ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‡æ ‡é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼š
- en: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e9e21f42abd000f112f5e7dfcdd322c.png)'
- en: Running evaluation on a suite of tasks
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šè¿è¡Œè¯„ä¼°
- en: It can be useful to evaluate models on a variety of different tasks to understand
    their downstream performance. The [EvaluationSuite](evaluation_suite) enables
    evaluation of models on a collection of tasks. Tasks can be constructed as ([evaluator](base_evaluator),
    dataset, metric) tuples and passed to an [EvaluationSuite](evaluation_suite) stored
    on the Hugging Face Hub as a Space, or locally as a Python script. See the [evaluator
    documentation](base_evaluator) for a list of currently supported tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å„ç§ä¸åŒä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œä»¥äº†è§£å®ƒä»¬çš„ä¸‹æ¸¸æ€§èƒ½ã€‚[EvaluationSuite](evaluation_suite)ä½¿å¾—å¯ä»¥åœ¨ä¸€ç»„ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹ã€‚ä»»åŠ¡å¯ä»¥æ„å»ºä¸ºï¼ˆ[evaluator](base_evaluator)ã€æ•°æ®é›†ã€æŒ‡æ ‡ï¼‰å…ƒç»„ï¼Œå¹¶ä¼ é€’ç»™å­˜å‚¨åœ¨Hugging
    Face Hubä¸Šçš„[EvaluationSuite](evaluation_suite)ä½œä¸ºä¸€ä¸ªSpaceï¼Œæˆ–è€…æœ¬åœ°ä½œä¸ºPythonè„šæœ¬ã€‚è¯·å‚é˜…[è¯„ä¼°å™¨æ–‡æ¡£](base_evaluator)ä»¥è·å–å½“å‰æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚
- en: '`EvaluationSuite` scripts can be defined as follows, and supports Python code
    for data preprocessing.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`EvaluationSuite`è„šæœ¬å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼Œå¹¶æ”¯æŒç”¨äºæ•°æ®é¢„å¤„ç†çš„Pythonä»£ç ã€‚'
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Evaluation can be run by loading the `EvaluationSuite` and calling `run()` method
    with a model or pipeline.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°å¯ä»¥é€šè¿‡åŠ è½½`EvaluationSuite`å¹¶è°ƒç”¨`run()`æ–¹æ³•æ¥è¿è¡Œï¼Œä¼ å…¥ä¸€ä¸ªæ¨¡å‹æˆ–pipelineã€‚
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '| accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds
    | task_name |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| å‡†ç¡®ç‡ | æ€»æ—¶é—´ï¼ˆç§’ï¼‰ | æ¯ç§’æ ·æœ¬æ•° | å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ | ä»»åŠ¡åç§° |'
- en: '| --: | --: | --: | :-- | :-- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | :-- | :-- |'
- en: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |'
- en: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |'
