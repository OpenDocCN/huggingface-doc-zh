["```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n>>> loss.item()\n3.7837\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n>>> loss.item()\n0.2542\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n>>> import torch\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> # the following 2 hyperparameters are task-specific\n>>> max_source_length = 512\n>>> max_target_length = 128\n\n>>> # Suppose we have the following 2 training examples:\n>>> input_sequence_1 = \"Welcome to NYC\"\n>>> output_sequence_1 = \"Bienvenue \u00e0 NYC\"\n\n>>> input_sequence_2 = \"HuggingFace is a company\"\n>>> output_sequence_2 = \"HuggingFace est une entreprise\"\n\n>>> # encode the inputs\n>>> task_prefix = \"translate English to French: \"\n>>> input_sequences = [input_sequence_1, input_sequence_2]\n\n>>> encoding = tokenizer(\n...     [task_prefix + sequence for sequence in input_sequences],\n...     padding=\"longest\",\n...     max_length=max_source_length,\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n\n>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n\n>>> # encode the targets\n>>> target_encoding = tokenizer(\n...     [output_sequence_1, output_sequence_2],\n...     padding=\"longest\",\n...     max_length=max_target_length,\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n>>> labels = target_encoding.input_ids\n\n>>> # replace padding token id's of the labels by -100 so it's ignored by the loss\n>>> labels[labels == tokenizer.pad_token_id] = -100\n\n>>> # forward pass\n>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n>>> loss.item()\n0.188\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\nDas Haus ist wunderbar.\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> task_prefix = \"translate English to German: \"\n>>> # use different length sentences to test batching\n>>> sentences = [\"The house is wonderful.\", \"I like to work in NYC.\"]\n\n>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n\n>>> output_sequences = model.generate(\n...     input_ids=inputs[\"input_ids\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     do_sample=False,  # disable sampling to test if batching affects output\n... )\n\n>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n\n>>> sequence_ids = model.generate(input_ids)\n>>> sequences = tokenizer.batch_decode(sequence_ids)\n>>> sequences\n['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']\n```", "```py\n( vocab_size = 32128 d_model = 512 d_kv = 64 d_ff = 2048 num_layers = 6 num_decoder_layers = None num_heads = 8 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'relu' is_encoder_decoder = True use_cache = True pad_token_id = 0 eos_token_id = 1 classifier_dropout = 0.0 **kwargs )\n```", "```py\n( vocab_file eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 100 additional_special_tokens = None sp_model_kwargs: Optional = None legacy = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None tokenizer_file = None eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 100 additional_special_tokens = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: T5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, T5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = T5Model.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: T5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n>>> # studies have shown that owning a dog is good for you.\n```", "```py\n( config: T5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, T5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = T5EncoderModel.from_pretrained(\"t5-small\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: T5Config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n( config: T5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = TFT5Model.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"tf\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"tf\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> # training\n>>> inputs = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"tf\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"tf\").input_ids\n>>> outputs = model(inputs, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> inputs = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"tf\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(inputs)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n>>> # studies have shown that owning a dog is good for you\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFT5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = TFT5EncoderModel.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"tf\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids)\n```", "```py\n( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Array = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5Model.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"np\"\n... ).input_ids\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"np\").input_ids\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Array = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None )\n```"]