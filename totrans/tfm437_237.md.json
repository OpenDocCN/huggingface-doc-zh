["```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n>>> loss.item()\n3.7837\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n>>> loss.item()\n0.2542\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n>>> import torch\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> # the following 2 hyperparameters are task-specific\n>>> max_source_length = 512\n>>> max_target_length = 128\n\n>>> # Suppose we have the following 2 training examples:\n>>> input_sequence_1 = \"Welcome to NYC\"\n>>> output_sequence_1 = \"Bienvenue \u00e0 NYC\"\n\n>>> input_sequence_2 = \"HuggingFace is a company\"\n>>> output_sequence_2 = \"HuggingFace est une entreprise\"\n\n>>> # encode the inputs\n>>> task_prefix = \"translate English to French: \"\n>>> input_sequences = [input_sequence_1, input_sequence_2]\n\n>>> encoding = tokenizer(\n...     [task_prefix + sequence for sequence in input_sequences],\n...     padding=\"longest\",\n...     max_length=max_source_length,\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n\n>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n\n>>> # encode the targets\n>>> target_encoding = tokenizer(\n...     [output_sequence_1, output_sequence_2],\n...     padding=\"longest\",\n...     max_length=max_target_length,\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n>>> labels = target_encoding.input_ids\n\n>>> # replace padding token id's of the labels by -100 so it's ignored by the loss\n>>> labels[labels == tokenizer.pad_token_id] = -100\n\n>>> # forward pass\n>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n>>> loss.item()\n0.188\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\nDas Haus ist wunderbar.\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> task_prefix = \"translate English to German: \"\n>>> # use different length sentences to test batching\n>>> sentences = [\"The house is wonderful.\", \"I like to work in NYC.\"]\n\n>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n\n>>> output_sequences = model.generate(\n...     input_ids=inputs[\"input_ids\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     do_sample=False,  # disable sampling to test if batching affects output\n... )\n\n>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']\n```", "```py\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n\n>>> sequence_ids = model.generate(input_ids)\n>>> sequences = tokenizer.batch_decode(sequence_ids)\n>>> sequences\n['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']\n```", "```py\n>>> from transformers import AutoTokenizer, T5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = T5Model.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n>>> # studies have shown that owning a dog is good for you.\n```", "```py\n>>> from transformers import AutoTokenizer, T5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = T5EncoderModel.from_pretrained(\"t5-small\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = TFT5Model.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"tf\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"tf\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> # training\n>>> inputs = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"tf\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"tf\").input_ids\n>>> outputs = model(inputs, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> inputs = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"tf\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(inputs)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n>>> # studies have shown that owning a dog is good for you\n```", "```py\n>>> from transformers import AutoTokenizer, TFT5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = TFT5EncoderModel.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"tf\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5Model.from_pretrained(\"t5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"np\"\n... ).input_ids\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"np\").input_ids\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n>>> text = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]