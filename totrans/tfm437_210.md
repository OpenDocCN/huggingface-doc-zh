# NLLB

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb)

## 更新的分词器行为

**免责声明：** 分词器的默认行为已在 2023 年 4 月修复并更改。之前的版本在目标和源分词序列的末尾都添加了 `[self.eos_token_id, self.cur_lang_code]`。这是错误的，因为 NLLB 论文提到了 (第 48 页，6.1.1\. 模型架构)：

*请注意，我们将源序列前缀与源语言一起使用，而不是像以前的一些作品那样使用目标语言 (Arivazhagan 等人，2019；Johnson 等人，2017)。这主要是因为我们优先考虑在任何一对 200 种语言上优化我们模型的零翻译性能，对监督性能的损失很小。*

先前的行为：

```py
>>> from transformers import NllbTokenizer

>>> tokenizer = NllbTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
>>> tokenizer("How was your day?").input_ids
[13374, 1398, 4260, 4039, 248130, 2, 256047]

>>> # 2: '</s>'
>>> # 256047 : 'eng_Latn'
```

新行为

```py
>>> from transformers import NllbTokenizer

>>> tokenizer = NllbTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
>>> tokenizer("How was your day?").input_ids
[256047, 13374, 1398, 4260, 4039, 248130, 2]
```

可以通过以下方式启用旧行为：

```py
>>> from transformers import NllbTokenizer

>>> tokenizer = NllbTokenizer.from_pretrained("facebook/nllb-200-distilled-600M", legacy_behaviour=True)
```

更多细节，请查看链接的 [PR](https://github.com/huggingface/transformers/pull/22313) 和 [Issue](https://github.com/huggingface/transformers/issues/19943)。

## 概述

NLLB 模型由 Marta R. Costa-jussà、James Cross、Onur Çelebi、Maha Elbayad、Kenneth Heafield、Kevin Heffernan、Elahe Kalbassi、Janice Lam、Daniel Licht、Jean Maillard、Anna Sun、Skyler Wang、Guillaume Wenzek、Al Youngblood、Bapi Akula、Loic Barrault、Gabriel Mejia Gonzalez、Prangthip Hansanti、John Hoffman、Semarley Jarrett、Kaushik Ram Sadagopan、Dirk Rowe、Shannon Spruit、Chau Tran、Pierre Andrews、Necip Fazil Ayan、Shruti Bhosale、Sergey Edunov、Angela Fan、Cynthia Gao、Vedanuj Goswami、Francisco Guzmán、Philipp Koehn、Alexandre Mourachko、Christophe Ropers、Safiyyah Saleem、Holger Schwenk 和 Jeff Wang 在 [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) 中提出。

该论文的摘要如下：

*受到在全球范围内消除语言障碍的目标驱动，机器翻译已经巩固自己作为当今人工智能研究的重点。然而，这些努力已经围绕一小部分语言展开，抛弃了绝大多数主要是低资源语言。要突破 200 种语言障碍并确保安全、高质量的结果，同时考虑伦理因素，需要什么？在《No Language Left Behind》中，我们首先通过与母语者的探索性访谈来定位对低资源语言翻译支持的需求。然后，我们创建了旨在缩小低资源语言和高资源语言之间性能差距的数据集和模型。更具体地说，我们开发了一个基于 Sparsely Gated Mixture of Experts 的条件计算模型，该模型是通过针对低资源语言量身定制的新颖和有效的数据挖掘技术获得的数据进行训练的。我们提出了多种架构和训练改进来对抗在数千个任务上训练时的过拟合。至关重要的是，我们使用人工翻译的基准 Flores-200 评估了超过 40,000 种不同的翻译方向的性能，并结合了一个涵盖 Flores-200 中所有语言的新颖毒性基准来评估翻译的安全性。我们的模型相对于先前的最新技术实现提高了 44% 的 BLEU，为实现通用翻译系统奠定了重要基础。*

此实现包含发布的稠密模型。

**稀疏模型 NLLB-MoE (Mixture of Expert) 现已推出！更多细节请查看 [这里](nllb-moe)**

此模型由 [Lysandre](https://huggingface.co/lysandre) 贡献。作者的代码可以在 [这里](https://github.com/facebookresearch/fairseq/tree/nllb) 找到。

## 使用 NLLB 生成

在生成目标文本时，将`forced_bos_token_id`设置为目标语言ID。以下示例展示了如何使用*facebook/nllb-200-distilled-600M*模型将英语翻译成法语。

请注意，我们使用法语`fra_Latn`的BCP-47代码。在[Flores 200数据集](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)中查看所有BCP-47的列表。

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M")

>>> article = "UN Chief says there is no military solution in Syria"
>>> inputs = tokenizer(article, return_tensors="pt")

>>> translated_tokens = model.generate(
...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id["fra_Latn"], max_length=30
... )
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
Le chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie
```

### 从除英语以外的任何其他语言生成

英语（`eng_Latn`）被设置为默认的翻译源语言。为了指定您想要从其他语言翻译，您应该在分词器初始化的`src_lang`关键字参数中指定BCP-47代码。

查看以下示例，将罗马尼亚语翻译成德语：

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(
...     "facebook/nllb-200-distilled-600M", token=True, src_lang="ron_Latn"
... )
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M", token=True)

>>> article = "Şeful ONU spune că nu există o soluţie militară în Siria"
>>> inputs = tokenizer(article, return_tensors="pt")

>>> translated_tokens = model.generate(
...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id["deu_Latn"], max_length=30
... )
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
UN-Chef sagt, es gibt keine militärische Lösung in Syrien
```

## 资源

+   [翻译任务指南](../tasks/translation)

+   [摘要任务指南](../tasks/summarization)

## NllbTokenizer

### `class transformers.NllbTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb.py#L47)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' tokenizer_file = None src_lang = None tgt_lang = None sp_model_kwargs: Optional = None additional_special_tokens = None legacy_behaviour = False **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `bos_token` (`str`，*可选*，默认为`"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`，*可选*，默认为`"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`，*可选*，默认为`"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如，用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`，*可选*，默认为`"<s>"`) — 用于进行序列分类（对整个序列进行分类而不是每个标记进行分类）时使用的分类器标记。在使用特殊标记构建时，它是序列的第一个标记。

+   `unk_token` (`str`，*可选*，默认为`"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。

+   `pad_token` (`str`，*可选*，默认为`"<pad>"`) — 用于填充的标记，例如，当批处理不同长度的序列时。

+   `mask_token` (`str`，*可选*，默认为`"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `tokenizer_file` (`str`，*可选*) — 要使用的分词器文件的路径，而不是词汇文件。

+   `src_lang` (`str`，*可选*) — 用作翻译源语言的语言。

+   `tgt_lang` (`str`，*可选*) — 用作翻译目标语言的语言。

+   `sp_model_kwargs` (`Dict[str, str]`) — 传递给模型初始化的额外关键字参数。

构建一个NLLB分词器。

改编自[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)和[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。

源语言文档的分词方法是`<tokens> <eos> <语言代码>`，目标语言文档的分词方法是`<语言代码>`

<tokens> <eos>` 用于目标语言文档。

示例：

```py
>>> from transformers import NllbTokenizer

>>> tokenizer = NllbTokenizer.from_pretrained(
...     "facebook/nllb-200-distilled-600M", src_lang="eng_Latn", tgt_lang="fra_Latn"
... )
>>> example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
>>> expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
```

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb.py#L269)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对的ID列表。

返回

`List[int]`

带有适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。NLLB序列具有以下格式，其中`X`表示序列:

+   `input_ids` (用于编码器) `X [eos, src_lang_code]`

+   `decoder_input_ids`: (用于解码器) `X [eos, tgt_lang_code]`

BOS 从不使用。序列对不是预期的用例，但它们将被处理而无需分隔符。

## NllbTokenizerFast

### `class transformers.NllbTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L59)

```py
( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' src_lang = None tgt_lang = None additional_special_tokens = None legacy_behaviour = False **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇表文件的路径。

+   `bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是被设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *可选*, 默认为 `"<mask>"`) — 用于掩码值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `tokenizer_file` (`str`, *可选*) — 要使用的分词器文件的路径，而不是词汇表文件。

+   `src_lang` (`str`, *可选*) — 用作翻译源语言的语言。

+   `tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言。

构建一个“快速”NLLB分词器（由HuggingFace的*tokenizers*库支持）。基于[BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)。

此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

源语言文档的分词方法是`<tokens> <eos> <language code>`，目标语言文档的分词方法是`<tokens> <eos>`。

`<tokens> <eos>` 用于目标语言文档。

示例:

```py
>>> from transformers import NllbTokenizerFast

>>> tokenizer = NllbTokenizerFast.from_pretrained(
...     "facebook/nllb-200-distilled-600M", src_lang="eng_Latn", tgt_lang="fra_Latn"
... )
>>> example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
>>> expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
```

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L212)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对的ID列表。

返回

`List[int]`

带有适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。特殊标记取决于调用set_lang。

NLLB序列具有以下格式，其中`X`表示序列：

+   `input_ids`（用于编码器）`X [eos, src_lang_code]`

+   `decoder_input_ids`：（用于解码器）`X [eos, tgt_lang_code]`

BOS从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L241)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。nllb不使用标记类型id，因此返回一个零的列表。

#### `set_src_lang_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L296)

```py
( src_lang )
```

将特殊标记重置为源语言设置。

+   在传统模式下：无前缀，后缀=[eos, src_lang_code]。

+   在默认模式下：前缀=[src_lang_code]，后缀=[eos]

#### `set_tgt_lang_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L319)

```py
( lang: str )
```

将特殊标记重置为目标语言设置。

+   在传统模式下：无前缀，后缀=[eos, tgt_lang_code]。

+   在默认模式下：前缀=[tgt_lang_code]，后缀=[eos]
