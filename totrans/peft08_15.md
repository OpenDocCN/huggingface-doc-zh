# 自定义模型

> 原文链接：[`huggingface.co/docs/peft/developer_guides/custom_models`](https://huggingface.co/docs/peft/developer_guides/custom_models)

一些微调技术，如提示微调，是特定于语言模型的。这意味着在🤗 PEFT 中，假定正在使用🤗 Transformers 模型。然而，其他微调技术 - 如 LoRA - 不限于特定的模型类型。

在本指南中，我们将看到 LoRA 如何应用于多层感知器，来自[timm](https://huggingface.co/docs/timm/index)库的计算机视觉模型，或者一个新的🤗 Transformers 架构。

## 多层感知器

假设我们想要用 LoRA 微调一个多层感知器。这是定义：

```py
from torch import nn

class MLP(nn.Module):
    def __init__(self, num_units_hidden=2000):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(20, num_units_hidden),
            nn.ReLU(),
            nn.Linear(num_units_hidden, num_units_hidden),
            nn.ReLU(),
            nn.Linear(num_units_hidden, 2),
            nn.LogSoftmax(dim=-1),
        )

    def forward(self, X):
        return self.seq(X)
```

这是一个简单的多层感知器，有一个输入层，一个隐藏层和一个输出层。

对于这个玩具示例，我们选择了一个非常大的隐藏单元数，以突出 PEFT 的效率提升，但这些提升与更现实的示例一致。

这个模型中有一些线性层可以用 LoRA 调整。当使用常见的🤗 Transformers 模型时，PEFT 会知道要将 LoRA 应用于哪些层，但在这种情况下，我们作为用户需要选择层。要确定要调整的层的名称：

```py
print([(n, type(m)) for n, m in MLP().named_modules()])
```

这应该打印：

```py
[('', __main__.MLP),
 ('seq', torch.nn.modules.container.Sequential),
 ('seq.0', torch.nn.modules.linear.Linear),
 ('seq.1', torch.nn.modules.activation.ReLU),
 ('seq.2', torch.nn.modules.linear.Linear),
 ('seq.3', torch.nn.modules.activation.ReLU),
 ('seq.4', torch.nn.modules.linear.Linear),
 ('seq.5', torch.nn.modules.activation.LogSoftmax)]
```

假设我们想将 LoRA 应用于输入层和隐藏层，它们分别是`'seq.0'`和`'seq.2'`。此外，假设我们想要更新输出层而不使用 LoRA，那将是`'seq.4'`。相应的配置将是：

```py
from peft import LoraConfig

config = LoraConfig(
    target_modules=["seq.0", "seq.2"],
    modules_to_save=["seq.4"],
)
```

有了这个，我们可以创建我们的 PEFT 模型并检查训练参数的比例：

```py
from peft import get_peft_model

model = MLP()
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922
```

最后，我们可以使用任何我们喜欢的训练框架，或者编写我们自己的拟合循环，来训练`peft_model`。

要查看完整示例，请查看[此笔记本](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb)。

## timm 模型

[timm](https://huggingface.co/docs/timm/index)库包含大量预训练的计算机视觉模型。这些模型也可以用 PEFT 进行微调。让我们看看这在实践中是如何工作的。

首先确保 timm 已安装在 Python 环境中：

```py
python -m pip install -U timm
```

接下来，我们为图像分类任务加载一个 timm 模型：

```py
import timm

num_classes = ...
model_id = "timm/poolformer_m36.sail_in1k"
model = timm.create_model(model_id, pretrained=True, num_classes=num_classes)
```

再次，我们需要决定要将 LoRA 应用于哪些层。由于 LoRA 支持 2D 卷积层，并且这些层是该模型的主要构建模块，我们应该将 LoRA 应用于 2D 卷积层。为了确定这些层的名称，让我们看看所有的层名称：

```py
print([(n, type(m)) for n, m in model.named_modules()])
```

这将打印一个非常长的列表，我们只显示前几个：

```py
[('', timm.models.metaformer.MetaFormer),
 ('stem', timm.models.metaformer.Stem),
 ('stem.conv', torch.nn.modules.conv.Conv2d),
 ('stem.norm', torch.nn.modules.linear.Identity),
 ('stages', torch.nn.modules.container.Sequential),
 ('stages.0', timm.models.metaformer.MetaFormerStage),
 ('stages.0.downsample', torch.nn.modules.linear.Identity),
 ('stages.0.blocks', torch.nn.modules.container.Sequential),
 ('stages.0.blocks.0', timm.models.metaformer.MetaFormerBlock),
 ('stages.0.blocks.0.norm1', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.0.token_mixer', timm.models.metaformer.Pooling),
 ('stages.0.blocks.0.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),
 ('stages.0.blocks.0.drop_path1', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.layer_scale1', timm.models.metaformer.Scale),
 ('stages.0.blocks.0.res_scale1', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.norm2', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),
 ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv.Conv2d),
 ('stages.0.blocks.0.mlp.act', torch.nn.modules.activation.GELU),
 ('stages.0.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),
 ('stages.0.blocks.0.mlp.norm', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.mlp.fc2', torch.nn.modules.conv.Conv2d),
 ('stages.0.blocks.0.mlp.drop2', torch.nn.modules.dropout.Dropout),
 ('stages.0.blocks.0.drop_path2', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.layer_scale2', timm.models.metaformer.Scale),
 ('stages.0.blocks.0.res_scale2', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.1', timm.models.metaformer.MetaFormerBlock),
 ('stages.0.blocks.1.norm1', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.1.token_mixer', timm.models.metaformer.Pooling),
 ('stages.0.blocks.1.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),
 ...
 ('head.global_pool.flatten', torch.nn.modules.linear.Identity),
 ('head.norm', timm.layers.norm.LayerNorm2d),
 ('head.flatten', torch.nn.modules.flatten.Flatten),
 ('head.drop', torch.nn.modules.linear.Identity),
 ('head.fc', torch.nn.modules.linear.Linear)]
 ]
```

仔细检查后，我们看到 2D 卷积层的名称如`"stages.0.blocks.0.mlp.fc1"`和`"stages.0.blocks.0.mlp.fc2"`。我们如何匹配这些层的名称？您可以编写一个正则表达式来匹配层的名称。对于我们的情况，正则表达式`r".*\.mlp\.fc\d"`应该可以胜任。

此外，与第一个示例一样，我们应该确保输出层，即分类头，在这种情况下也被更新。查看上面打印列表的末尾，我们可以看到它的名称是`'head.fc'`。考虑到这一点，这是我们的 LoRA 配置：

```py
config = LoraConfig(target_modules=r".*\.mlp\.fc\d", modules_to_save=["head.fc"])
```

然后我们只需要通过将基本模型和配置传递给`get_peft_model`来创建 PEFT 模型：

```py
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# prints trainable params: 1,064,454 || all params: 56,467,974 || trainable%: 1.88505789139876
```

这向我们表明，我们只需要训练不到所有参数的 2％，这是一个巨大的效率提升。

要查看完整示例，请查看[此笔记本](https://github.com/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb)。

## 新的 transformers 架构

当发布新的热门 transformers 架构时，我们会尽快将它们添加到 PEFT 中。如果您遇到一个不支持的 transformers 模型，不用担心，只要配置正确，它很可能仍然可以工作。具体来说，您必须识别应该适应的层，并在初始化相应的配置类时正确设置它们，例如`LoraConfig`。以下是一些帮助的提示。

首先，检查现有模型以获取灵感是个好主意。您可以在 PEFT 存储库的[constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)中找到它们。通常，您会发现一个类似的架构使用相同的名称。例如，如果新模型架构是“mistral”模型的变体，并且您想应用 LoRA，您可以看到`TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING`中“mistral”条目包含`["q_proj", "v_proj"]`。这告诉您对于“mistral”模型，LoRA 的`target_modules`应该是`["q_proj", "v_proj"]`：

```py
from peft import LoraConfig, get_peft_model

my_mistral_model = ...
config = LoraConfig(
    target_modules=["q_proj", "v_proj"],
    ...,  # other LoRA arguments
)
peft_model = get_peft_model(my_mistral_model, config)
```

如果这没有帮助，请使用`named_modules`方法检查模型架构中的现有模块，并尝试识别注意力层，特别是关键、查询和值层。这些通常会有名称，如`c_attn`、`query`、`q_proj`等。关键层并不总是适应的，理想情况下，您应该检查包括它是否会导致更好的性能。

此外，线性层是常见的适应目标（例如，在[QLoRA 论文](https://arxiv.org/abs/2305.14314)中，作者建议也对其进行适应）。它们的名称通常会包含字符串`fc`或`dense`。

如果您想将新模型添加到 PEFT，请在[constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)中创建一个条目，并在[存储库](https://github.com/huggingface/peft/pulls)上打开一个拉取请求。不要忘记更新[README](https://github.com/huggingface/peft#models-support-matrix)。

## 验证参数和层

您可以通过几种方式验证是否已正确将 PEFT 方法应用于您的模型。

+   使用 print_trainable_parameters()方法检查可训练参数的比例。如果此数字低于或高于预期，请通过打印模型来检查模型的`repr`。这将显示模型中所有层类型的名称。确保只有预期的目标层被适配层替换。例如，如果 LoRA 应用于`nn.Linear`层，则应只看到使用`lora.Linear`层。

```py
peft_model.print_trainable_parameters()
```

+   您可以查看适应的层的另一种方法是使用`targeted_module_names`属性列出每个已适应模块的名称。

```py
print(peft_model.targeted_module_names)
```
