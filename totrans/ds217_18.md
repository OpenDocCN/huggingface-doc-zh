# æµ

> åŽŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/datasets/stream`](https://huggingface.co/docs/datasets/stream)

æ•°æ®é›†æµå¼ä¼ è¾“ä½¿æ‚¨å¯ä»¥åœ¨ä¸ä¸‹è½½æ•°æ®é›†çš„æƒ…å†µä¸‹ä½¿ç”¨æ•°æ®é›†ã€‚å½“æ‚¨éåŽ†æ•°æ®é›†æ—¶ï¼Œæ•°æ®ä¼šéšä¹‹æµåŠ¨ã€‚è¿™åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ç‰¹åˆ«æœ‰å¸®åŠ©ï¼š

+   æ‚¨ä¸æƒ³ç­‰å¾…æžå¤§çš„æ•°æ®é›†ä¸‹è½½ã€‚

+   æ•°æ®é›†å¤§å°è¶…è¿‡è®¡ç®—æœºä¸Šå¯ç”¨ç£ç›˜ç©ºé—´çš„é‡ã€‚

+   æ‚¨æƒ³å¿«é€ŸæŽ¢ç´¢æ•°æ®é›†çš„ä¸€äº›æ ·æœ¬ã€‚

![](img/3a852aa4a279bb2476adf697aa4b6865.png) ![](img/a71d3ad700730cf461b4f099cd4a8c4b.png)

ä¾‹å¦‚ï¼Œ[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) æ•°æ®é›†çš„è‹±æ–‡éƒ¨åˆ†ä¸º 1.2TBï¼Œä½†æ‚¨å¯ä»¥ç«‹å³ä½¿ç”¨æµå¼ä¼ è¾“ã€‚é€šè¿‡åœ¨ load_dataset()ä¸­è®¾ç½®`streaming=True`æ¥æµå¼ä¼ è¾“æ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)
>>> print(next(iter(dataset)))
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

æ•°æ®é›†æµå¼ä¼ è¾“è¿˜ä½¿æ‚¨å¯ä»¥å¤„ç†ç”±æœ¬åœ°æ–‡ä»¶ç»„æˆçš„æ•°æ®é›†ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•è½¬æ¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“æ‚¨éåŽ†æ•°æ®é›†æ—¶ï¼Œæ•°æ®å°†ä»Žæœ¬åœ°æ–‡ä»¶æµåŠ¨ã€‚è¿™åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ç‰¹åˆ«æœ‰å¸®åŠ©ï¼š

+   æ‚¨ä¸æƒ³ç­‰å¾…æžå¤§çš„æœ¬åœ°æ•°æ®é›†è½¬æ¢ä¸º Arrowã€‚

+   è½¬æ¢åŽçš„æ–‡ä»¶å¤§å°å°†è¶…è¿‡è®¡ç®—æœºä¸Šå¯ç”¨ç£ç›˜ç©ºé—´çš„é‡ã€‚

+   æ‚¨æƒ³å¿«é€ŸæŽ¢ç´¢æ•°æ®é›†çš„ä¸€äº›æ ·æœ¬ã€‚

ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é€šè¿‡æµå¼ä¼ è¾“æœ¬åœ°åŒ…å«æ•°ç™¾ä¸ªåŽ‹ç¼©çš„ JSONL æ–‡ä»¶çš„æ•°æ®é›†ï¼Œå¦‚[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)ï¼š

```py
>>> from datasets import load_dataset
>>> data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}
>>> dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)
>>> print(next(iter(dataset)))
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

ä»¥æµå¼ä¼ è¾“æ¨¡å¼åŠ è½½æ•°æ®é›†ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ç±»åž‹å®žä¾‹ï¼ˆè€Œä¸æ˜¯ç»å…¸çš„ Dataset å¯¹è±¡ï¼‰ï¼Œç§°ä¸º IterableDatasetã€‚è¿™ç§ç‰¹æ®Šç±»åž‹çš„æ•°æ®é›†å…·æœ‰è‡ªå·±çš„ä¸€ç»„å¤„ç†æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

IterableDataset å¯¹äºŽåƒè®­ç»ƒæ¨¡åž‹è¿™æ ·çš„è¿­ä»£ä½œä¸šéžå¸¸æœ‰ç”¨ã€‚æ‚¨ä¸åº”è¯¥å°† IterableDataset ç”¨äºŽéœ€è¦éšæœºè®¿é—®ç¤ºä¾‹çš„ä½œä¸šï¼Œå› ä¸ºæ‚¨å¿…é¡»ä½¿ç”¨ for å¾ªçŽ¯éåŽ†æ•´ä¸ªæ•°æ®é›†ã€‚èŽ·å–å¯è¿­ä»£æ•°æ®é›†ä¸­çš„æœ€åŽä¸€ä¸ªç¤ºä¾‹éœ€è¦æ‚¨éåŽ†æ‰€æœ‰å…ˆå‰çš„ç¤ºä¾‹ã€‚æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Ž IterableDataset æŒ‡å—ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

## ä»Žæ•°æ®é›†è½¬æ¢

å¦‚æžœæ‚¨æœ‰çŽ°æœ‰çš„ Dataset å¯¹è±¡ï¼Œå¯ä»¥ä½¿ç”¨ to_iterable_dataset()å‡½æ•°å°†å…¶è½¬æ¢ä¸º IterableDatasetã€‚è¿™å®žé™…ä¸Šæ¯”åœ¨ load_dataset()ä¸­è®¾ç½®`streaming=True`å‚æ•°æ›´å¿«ï¼Œå› ä¸ºæ•°æ®æ˜¯ä»Žæœ¬åœ°æ–‡ä»¶æµå¼ä¼ è¾“çš„ã€‚

```py
>>> from datasets import load_dataset

# faster ðŸ‡
>>> dataset = load_dataset("food101")
>>> iterable_dataset = dataset.to_iterable_dataset()

# slower ðŸ¢
>>> iterable_dataset = load_dataset("food101", streaming=True)
```

to_iterable_dataset()å‡½æ•°åœ¨å®žä¾‹åŒ– IterableDataset æ—¶æ”¯æŒåˆ†ç‰‡ã€‚å½“å¤„ç†å¤§åž‹æ•°æ®é›†å¹¶ä¸”å¸Œæœ›å¯¹æ•°æ®é›†è¿›è¡Œæ´—ç‰Œæˆ–ä½¿ç”¨ PyTorch DataLoader è¿›è¡Œå¿«é€Ÿå¹¶è¡ŒåŠ è½½æ—¶ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ã€‚

```py
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101")
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=64) # shard the dataset
>>> iterable_dataset = iterable_dataset.shuffle(buffer_size=10_000)  # shuffles the shards order and use a shuffle buffer when you start iterating
dataloader = torch.utils.data.DataLoader(iterable_dataset, num_workers=4)  # assigns 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating
```

## æ´—ç‰Œ

ä¸Žå¸¸è§„ Dataset å¯¹è±¡ä¸€æ ·ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ IterableDataset.shuffle()å¯¹ IterableDataset è¿›è¡Œæ´—ç‰Œã€‚

`buffer_size`å‚æ•°æŽ§åˆ¶ä»Žä¸­éšæœºæŠ½æ ·ç¤ºä¾‹çš„ç¼“å†²åŒºçš„å¤§å°ã€‚å‡è®¾æ‚¨çš„æ•°æ®é›†æœ‰ä¸€ç™¾ä¸‡ä¸ªç¤ºä¾‹ï¼Œå¹¶ä¸”æ‚¨å°†`buffer_size`è®¾ç½®ä¸ºä¸€ä¸‡ã€‚IterableDataset.shuffle()å°†ä»Žç¼“å†²åŒºä¸­çš„å‰ä¸€ä¸‡ä¸ªç¤ºä¾‹ä¸­éšæœºé€‰æ‹©ç¤ºä¾‹ã€‚ç¼“å†²åŒºä¸­çš„é€‰å®šç¤ºä¾‹å°†è¢«æ–°ç¤ºä¾‹æ›¿æ¢ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç¼“å†²åŒºå¤§å°ä¸º 1,000ã€‚

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
>>> shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)
```

IterableDataset.shuffle()è¿˜å°†å¯¹åˆ†ç‰‡çš„é¡ºåºè¿›è¡Œæ´—ç‰Œï¼Œå¦‚æžœæ•°æ®é›†è¢«åˆ†ç‰‡åˆ°å¤šä¸ªæ–‡ä»¶ä¸­ã€‚

## é‡æ–°æ´—ç‰Œ

æœ‰æ—¶æ‚¨å¯èƒ½å¸Œæœ›åœ¨æ¯ä¸ªæ—¶æœŸä¹‹åŽé‡æ–°æ´—ç‰Œæ•°æ®é›†ã€‚è¿™å°†éœ€è¦æ‚¨ä¸ºæ¯ä¸ªæ—¶æœŸè®¾ç½®ä¸åŒçš„ç§å­ã€‚åœ¨æ—¶æœŸä¹‹é—´ä½¿ç”¨`IterableDataset.set_epoch()`å‘Šè¯‰æ•°æ®é›†æ‚¨åœ¨å“ªä¸ªæ—¶æœŸã€‚

æ‚¨çš„ç§å­å®žé™…ä¸Šå˜ä¸ºï¼š`åˆå§‹ç§å­ + å½“å‰æ—¶æœŸ`ã€‚

```py
>>> for epoch in range(epochs):
...     shuffled_dataset.set_epoch(epoch)
...     for example in shuffled_dataset:
...         ...
```

## æ‹†åˆ†æ•°æ®é›†

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ‹†åˆ†æ•°æ®é›†ï¼š

+   IterableDataset.take()è¿”å›žæ•°æ®é›†ä¸­çš„å‰`n`ä¸ªç¤ºä¾‹ï¼š

```py
>>> dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
>>> dataset_head = dataset.take(2)
>>> list(dataset_head)
[{'id': 0, 'text': 'Mtendere Village was...'}, {'id': 1, 'text': 'Lily James cannot fight the music...'}]
```

+   IterableDataset.skip()çœç•¥æ•°æ®é›†ä¸­çš„å‰`n`ä¸ªç¤ºä¾‹ï¼Œå¹¶è¿”å›žå‰©ä½™çš„ç¤ºä¾‹ï¼š

```py
>>> train_dataset = shuffled_dataset.skip(1000)
```

`take`å’Œ`skip`é˜»æ­¢å¯¹`shuffle`çš„æœªæ¥è°ƒç”¨ï¼Œå› ä¸ºå®ƒä»¬é”å®šäº†åˆ†ç‰‡çš„é¡ºåºã€‚åœ¨æ‹†åˆ†æ•°æ®é›†ä¹‹å‰ï¼Œåº”è¯¥å¯¹æ•°æ®é›†è¿›è¡Œ`shuffle`ã€‚

## äº¤é”™

interleave_datasets()å¯ä»¥å°† IterableDataset ä¸Žå…¶ä»–æ•°æ®é›†ç»„åˆã€‚ç»„åˆæ•°æ®é›†è¿”å›žæ¯ä¸ªåŽŸå§‹æ•°æ®é›†çš„äº¤æ›¿ç¤ºä¾‹ã€‚

```py
>>> from datasets import interleave_datasets
>>> en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True, trust_remote_code=True)
>>> fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True, trust_remote_code=True)

>>> multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
>>> list(multilingual_dataset.take(2))
[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': "MÃ©dia de dÃ©bat d'idÃ©es, de culture et de littÃ©rature..."}]
```

ä»Žæ¯ä¸ªåŽŸå§‹æ•°æ®é›†å®šä¹‰é‡‡æ ·æ¦‚çŽ‡ï¼Œä»¥æ›´å¥½åœ°æŽ§åˆ¶å¦‚ä½•å¯¹å®ƒä»¬è¿›è¡Œé‡‡æ ·å’Œç»„åˆã€‚ä½¿ç”¨æ‚¨æœŸæœ›çš„é‡‡æ ·æ¦‚çŽ‡è®¾ç½®`probabilities`å‚æ•°ï¼š

```py
>>> multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
>>> list(multilingual_dataset_with_oversampling.take(2))
[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': 'Lily James cannot fight the music...'}]
```

æœ€ç»ˆæ•°æ®é›†çš„çº¦ 80%ç”±`en_dataset`ç»„æˆï¼Œ20%ç”±`fr_dataset`ç»„æˆã€‚

æ‚¨è¿˜å¯ä»¥æŒ‡å®š`stopping_strategy`ã€‚é»˜è®¤ç­–ç•¥`first_exhausted`æ˜¯ä¸€ç§å­é‡‡æ ·ç­–ç•¥ï¼Œå³æ•°æ®é›†æž„å»ºåœ¨å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†è€—å°½æ ·æœ¬æ—¶åœæ­¢ã€‚æ‚¨å¯ä»¥æŒ‡å®š`stopping_strategy=all_exhausted`æ¥æ‰§è¡Œä¸€ç§è¿‡é‡‡æ ·ç­–ç•¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®é›†æž„å»ºå°†åœ¨æ¯ä¸ªæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬è‡³å°‘æ·»åŠ ä¸€æ¬¡åŽåœæ­¢ã€‚å®žé™…ä¸Šï¼Œè¿™æ„å‘³ç€å¦‚æžœä¸€ä¸ªæ•°æ®é›†è€—å°½ï¼Œå®ƒå°†è¿”å›žåˆ°è¯¥æ•°æ®é›†çš„å¼€å¤´ï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ ‡å‡†ã€‚è¯·æ³¨æ„ï¼Œå¦‚æžœæœªæŒ‡å®šé‡‡æ ·æ¦‚çŽ‡ï¼Œåˆ™æ–°æ•°æ®é›†å°†å…·æœ‰`max_length_datasets*nb_dataset`ä¸ªæ ·æœ¬ã€‚

## é‡å‘½åã€åˆ é™¤å’Œè½¬æ¢

ä»¥ä¸‹æ–¹æ³•å…è®¸æ‚¨ä¿®æ”¹æ•°æ®é›†çš„åˆ—ã€‚è¿™äº›æ–¹æ³•å¯¹äºŽé‡å‘½åæˆ–åˆ é™¤åˆ—ä»¥åŠå°†åˆ—æ›´æ”¹ä¸ºæ–°çš„ç‰¹å¾é›†éžå¸¸æœ‰ç”¨ã€‚

### é‡å‘½å

å½“æ‚¨éœ€è¦åœ¨æ•°æ®é›†ä¸­é‡å‘½ååˆ—æ—¶ï¼Œè¯·ä½¿ç”¨ IterableDataset.rename_column()ã€‚ä¸ŽåŽŸå§‹åˆ—ç›¸å…³çš„ç‰¹å¾å®žé™…ä¸Šæ˜¯ç§»åŠ¨åˆ°æ–°åˆ—åä¸‹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›¿æ¢åŽŸå§‹åˆ—ã€‚

ä½¿ç”¨ IterableDataset.rename_column()æä¾›åŽŸå§‹åˆ—çš„åç§°å’Œæ–°åˆ—åï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('mc4', 'en', streaming=True, split='train', trust_remote_code=True)
>>> dataset = dataset.rename_column("text", "content")
```

### åˆ é™¤

å½“æ‚¨éœ€è¦åˆ é™¤ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—æ—¶ï¼Œè¯·ç»™ IterableDataset.remove_columns()è¦åˆ é™¤çš„åˆ—çš„åç§°ã€‚é€šè¿‡æä¾›åˆ—åç§°çš„åˆ—è¡¨æ¥åˆ é™¤å¤šä¸ªåˆ—ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('mc4', 'en', streaming=True, split='train', trust_remote_code=True)
>>> dataset = dataset.remove_columns('timestamp')
```

### è½¬æ¢

IterableDataset.cast()æ›´æ”¹ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—çš„ç‰¹å¾ç±»åž‹ã€‚æ­¤æ–¹æ³•å°†æ‚¨çš„æ–°`Features`ä½œä¸ºå…¶å‚æ•°ã€‚ä»¥ä¸‹ç¤ºä¾‹ä»£ç æ˜¾ç¤ºäº†å¦‚ä½•æ›´æ”¹`ClassLabel`å’Œ`Value`çš„ç‰¹å¾ç±»åž‹ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('glue', 'mrpc', split='train', streaming=True)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}

>>> from datasets import ClassLabel, Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=['negative', 'positive'])
>>> new_features["idx"] = Value('int64')
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

ä»…å½“åŽŸå§‹ç‰¹å¾ç±»åž‹å’Œæ–°ç‰¹å¾ç±»åž‹å…¼å®¹æ—¶ï¼Œè½¬æ¢æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œå¦‚æžœåŽŸå§‹åˆ—ä»…åŒ…å« 1 å’Œ 0ï¼Œåˆ™å¯ä»¥å°†å…·æœ‰ç‰¹å¾ç±»åž‹`Value('int32')`çš„åˆ—è½¬æ¢ä¸º`Value('bool')`ã€‚

ä½¿ç”¨ IterableDataset.cast_column()æ¥æ›´æ”¹ä¸€ä¸ªåˆ—çš„ç‰¹å¾ç±»åž‹ã€‚å°†åˆ—åå’Œå…¶æ–°ç‰¹å¾ç±»åž‹ä½œä¸ºå‚æ•°ä¼ é€’ï¼š

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}

>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

## æ˜ å°„

ç±»ä¼¼äºŽå¸¸è§„ Dataset çš„ Dataset.map()å‡½æ•°ï¼ŒðŸ¤— Datasets åŠŸèƒ½ IterableDataset.map()ç”¨äºŽå¤„ç† IterableDatasetã€‚å½“ç¤ºä¾‹è¢«æµå¼ä¼ è¾“æ—¶ï¼ŒIterableDataset.map()ä¼šå®žæ—¶åº”ç”¨å¤„ç†ã€‚

å®ƒå…è®¸æ‚¨å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹åº”ç”¨å¤„ç†å‡½æ•°ï¼Œç‹¬ç«‹æˆ–æ‰¹å¤„ç†ã€‚è¯¥å‡½æ•°ç”šè‡³å¯ä»¥åˆ›å»ºæ–°çš„è¡Œå’Œåˆ—ã€‚

ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•å¯¹ IterableDataset è¿›è¡Œæ ‡è®°åŒ–ã€‚è¯¥å‡½æ•°éœ€è¦æŽ¥å—å¹¶è¾“å‡ºä¸€ä¸ª`dict`ï¼š

```py
>>> def add_prefix(example):
...     example['text'] = 'My text: ' + example['text']
...     return example
```

æŽ¥ä¸‹æ¥ï¼Œä½¿ç”¨ IterableDataset.map()å°†æ­¤å‡½æ•°åº”ç”¨äºŽæ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)
>>> updated_dataset = dataset.map(add_prefix)
>>> list(updated_dataset.take(3))
[{'id': 0, 'text': 'My text: Mtendere Village was inspired by...'},
 {'id': 1, 'text': 'My text: Lily James cannot fight the music...'},
 {'id': 2, 'text': 'My text: "I\'d love to help kickstart...'}]
```

è®©æˆ‘ä»¬çœ‹å¦ä¸€ä¸ªä¾‹å­ï¼Œé™¤äº†è¿™æ¬¡æ‚¨å°†ä½¿ç”¨ IterableDataset.map()åˆ é™¤ä¸€åˆ—ã€‚å½“åˆ é™¤åˆ—æ—¶ï¼Œä»…åœ¨å°†ç¤ºä¾‹æä¾›ç»™æ˜ å°„å‡½æ•°åŽæ‰ä¼šåˆ é™¤ã€‚è¿™ä½¿å¾—æ˜ å°„å‡½æ•°å¯ä»¥åœ¨åˆ é™¤ä¹‹å‰ä½¿ç”¨åˆ—çš„å†…å®¹ã€‚

ä½¿ç”¨ IterableDataset.map()ä¸­çš„`remove_columns`å‚æ•°æŒ‡å®šè¦åˆ é™¤çš„åˆ—ï¼š

```py
>>> updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
>>> list(updated_dataset.take(3))
[{'text': 'My text: Mtendere Village was inspired by...'},
 {'text': 'My text: Lily James cannot fight the music...'},
 {'text': 'My text: "I\'d love to help kickstart...'}]
```

### æ‰¹å¤„ç†

IterableDataset.map()è¿˜æ”¯æŒå¯¹ç¤ºä¾‹æ‰¹æ¬¡è¿›è¡Œæ“ä½œã€‚é€šè¿‡è®¾ç½®`batched=True`æ¥æ“ä½œæ‰¹æ¬¡ã€‚é»˜è®¤æ‰¹å¤„ç†å¤§å°ä¸º 1000ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨`batch_size`å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¸ºè®¸å¤šæœ‰è¶£çš„åº”ç”¨æ‰“å¼€äº†å¤§é—¨ï¼Œä¾‹å¦‚æ ‡è®°åŒ–ã€å°†é•¿å¥å­æ‹†åˆ†ä¸ºè¾ƒçŸ­çš„å—ä»¥åŠæ•°æ®å¢žå¼ºã€‚

#### æ ‡è®°åŒ–

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> dataset = load_dataset("mc4", "en", streaming=True, split="train", trust_remote_code=True)
>>> tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
>>> def encode(examples):
...     return tokenizer(examples['text'], truncation=True, padding='max_length')
>>> dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
>>> next(iter(dataset))
{'input_ids': [101, 8466, 1018, 1010, 4029, 2475, 2062, 18558, 3100, 2061, ...,1106, 3739, 102],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1]}
```

æŸ¥çœ‹æ‰¹å¤„ç†æ˜ å°„å¤„ç†æ–‡æ¡£ä¸­çš„å…¶ä»–æ‰¹å¤„ç†ç¤ºä¾‹ã€‚å®ƒä»¬å¯¹å¯è¿­ä»£æ•°æ®é›†çš„å·¥ä½œæ–¹å¼ç›¸åŒã€‚

### è¿‡æ»¤

æ‚¨å¯ä»¥ä½¿ç”¨ Dataset.filter()åŸºäºŽè°“è¯å‡½æ•°è¿‡æ»¤æ•°æ®é›†ä¸­çš„è¡Œã€‚å®ƒè¿”å›žç¬¦åˆæŒ‡å®šæ¡ä»¶çš„è¡Œï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)
>>> start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
>>> next(iter(start_with_ar))
{'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)?...'}
```

Dataset.filter()è¿˜å¯ä»¥é€šè¿‡è®¾ç½®`with_indices=True`æŒ‰ç´¢å¼•è¿›è¡Œè¿‡æ»¤ï¼š

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> list(even_dataset.take(3))
[{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...'},
 {'id': 2, 'text': '"I\'d love to help kickstart continued development! And 0 EUR/month...'},
 {'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...'}]
```

## åœ¨è®­ç»ƒå¾ªçŽ¯ä¸­æµå¼ä¼ è¾“

IterableDataset å¯ä»¥é›†æˆåˆ°è®­ç»ƒå¾ªçŽ¯ä¸­ã€‚é¦–å…ˆï¼Œå¯¹æ•°æ®é›†è¿›è¡Œæ´—ç‰Œï¼š

Pytorch éšè— Pytorch å†…å®¹

```py
>>> seed, buffer_size = 42, 10_000
>>> dataset = dataset.shuffle(seed, buffer_size=buffer_size)
```

æœ€åŽï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„è®­ç»ƒå¾ªçŽ¯å¹¶å¼€å§‹è®­ç»ƒï¼š

```py
>>> import torch
>>> from torch.utils.data import DataLoader
>>> from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
>>> from tqdm import tqdm
>>> dataset = dataset.with_format("torch")
>>> dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
>>> device = 'cuda' if torch.cuda.is_available() else 'cpu' 
>>> model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
>>> model.train().to(device)
>>> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
>>> for epoch in range(3):
...     dataset.set_epoch(epoch)
...     for i, batch in enumerate(tqdm(dataloader, total=5)):
...         if i == 5:
...             break
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs[0]
...         loss.backward()
...         optimizer.step()
...         optimizer.zero_grad()
...         if i % 10 == 0:
...             print(f"loss: {loss}")
```
