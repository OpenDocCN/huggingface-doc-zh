["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/dreambooth\npip install -r requirements.txt\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_dreambooth.py \\\n    --mixed_precision=\"bf16\"\n```", "```py\naccelerate launch train_dreambooth.py \\\n  --snr_gamma=5.0\n```", "```py\naccelerate launch train_dreambooth.py \\\n  --with_prior_preservation \\\n  --prior_loss_weight=1.0 \\\n  --class_data_dir=\"path/to/class/images\" \\\n  --class_prompt=\"text prompt describing class\"\n```", "```py\naccelerate launch train_dreambooth.py \\\n  --train_text_encoder\n```", "```py\nsample_dataset = PromptDataset(args.class_prompt, num_new_images)\nsample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n\nsample_dataloader = accelerator.prepare(sample_dataloader)\npipeline.to(accelerator.device)\n\nfor example in tqdm(\n    sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n):\n    images = pipeline(example[\"prompt\"]).images\n```", "```py\n# Load the tokenizer\nif args.tokenizer_name:\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\nelif args.pretrained_model_name_or_path:\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"tokenizer\",\n        revision=args.revision,\n        use_fast=False,\n    )\n\n# Load scheduler and models\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\ntext_encoder = text_encoder_cls.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n)\n\nif model_has_vae(args):\n    vae = AutoencoderKL.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n    )\nelse:\n    vae = None\n\nunet = UNet2DConditionModel.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n)\n```", "```py\ntrain_dataset = DreamBoothDataset(\n    instance_data_root=args.instance_data_dir,\n    instance_prompt=args.instance_prompt,\n    class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n    class_prompt=args.class_prompt,\n    class_num=args.num_class_images,\n    tokenizer=tokenizer,\n    size=args.resolution,\n    center_crop=args.center_crop,\n    encoder_hidden_states=pre_computed_encoder_hidden_states,\n    class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states,\n    tokenizer_max_length=args.tokenizer_max_length,\n)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=args.train_batch_size,\n    shuffle=True,\n    collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n    num_workers=args.dataloader_num_workers,\n)\n```", "```py\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir,\n    repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```", "```py\n--validation_prompt=\"a photo of a sks dog\"\n--num_validation_images=4\n--validation_steps=100\n```", "```py\npip install bitsandbytes\n```", "```py\naccelerate launch train_dreambooth.py \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n```", "```py\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\nexport INSTANCE_DIR=\"./dog\"\nexport OUTPUT_DIR=\"path_to_saved_model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=400 \\\n  --push_to_hub\n```", "```py\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPTextModel\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\"path/to/model/checkpoint-100/unet\")\n\n# if you have trained with `--args.train_text_encoder` make sure to also load the text encoder\ntext_encoder = CLIPTextModel.from_pretrained(\"path/to/model/checkpoint-100/checkpoint-100/text_encoder\")\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", unet=unet, text_encoder=text_encoder, dtype=torch.float16,\n).to(\"cuda\")\n\nimage = pipeline(\"A photo of sks dog in a bucket\", num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"dog-bucket.png\")\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"path_to_saved_model\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\nimage = pipeline(\"A photo of sks dog in a bucket\", num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"dog-bucket.png\")\n```"]