["```py\npip install -q transformers datasets\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\n>>> model_checkpoint = \"dandelin/vilt-b32-mlm\"\n```", "```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"Graphcore/vqa\", split=\"validation[:200]\")\n>>> dataset\nDataset({\n    features: ['question', 'question_type', 'question_id', 'image_id', 'answer_type', 'label'],\n    num_rows: 200\n})\n```", "```py\n>>> dataset[0]\n{'question': 'Where is he looking?',\n 'question_type': 'none of the above',\n 'question_id': 262148000,\n 'image_id': '/root/.cache/huggingface/datasets/downloads/extracted/ca733e0e000fb2d7a09fbcc94dbfe7b5a30750681d0e965f8e0a23b1c2f98c75/val2014/COCO_val2014_000000262148.jpg',\n 'answer_type': 'other',\n 'label': {'ids': ['at table', 'down', 'skateboard', 'table'],\n  'weights': [0.30000001192092896,\n   1.0,\n   0.30000001192092896,\n   0.30000001192092896]}}\n```", "```py\n>>> dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])\n```", "```py\n>>> from PIL import Image\n\n>>> image = Image.open(dataset[0]['image_id'])\n>>> image\n```", "```py\n>>> import itertools\n\n>>> labels = [item['ids'] for item in dataset['label']]\n>>> flattened_labels = list(itertools.chain(*labels))\n>>> unique_labels = list(set(flattened_labels))\n\n>>> label2id = {label: idx for idx, label in enumerate(unique_labels)}\n>>> id2label = {idx: label for label, idx in label2id.items()} \n```", "```py\n>>> def replace_ids(inputs):\n...   inputs[\"label\"][\"ids\"] = [label2id[x] for x in inputs[\"label\"][\"ids\"]]\n...   return inputs\n\n>>> dataset = dataset.map(replace_ids)\n>>> flat_dataset = dataset.flatten()\n>>> flat_dataset.features\n{'question': Value(dtype='string', id=None),\n 'image_id': Value(dtype='string', id=None),\n 'label.ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n 'label.weights': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}\n```", "```py\n>>> from transformers import ViltProcessor\n\n>>> processor = ViltProcessor.from_pretrained(model_checkpoint)\n```", "```py\n>>> import torch\n\n>>> def preprocess_data(examples):\n...     image_paths = examples['image_id']\n...     images = [Image.open(image_path) for image_path in image_paths]\n...     texts = examples['question']    \n\n...     encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n...     for k, v in encoding.items():\n...           encoding[k] = v.squeeze()\n\n...     targets = []\n\n...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n...         target = torch.zeros(len(id2label))\n\n...         for label, score in zip(labels, scores):\n...             target[label] = score\n\n...         targets.append(target)\n\n...     encoding[\"labels\"] = targets\n\n...     return encoding\n```", "```py\n>>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])\n>>> processed_dataset\nDataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],\n    num_rows: 200\n})\n```", "```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator()\n```", "```py\n>>> from transformers import ViltForQuestionAnswering\n\n>>> model = ViltForQuestionAnswering.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> repo_id = \"MariaK/vilt_finetuned_200\"\n\n>>> training_args = TrainingArguments(\n...     output_dir=repo_id,\n...     per_device_train_batch_size=4,\n...     num_train_epochs=20,\n...     save_steps=200,\n...     logging_steps=50,\n...     learning_rate=5e-5,\n...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n```", "```py\n>>> from transformers import Trainer\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     data_collator=data_collator,\n...     train_dataset=processed_dataset,\n...     tokenizer=processor,\n... )\n```", "```py\n>>> trainer.train() \n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(\"visual-question-answering\", model=\"MariaK/vilt_finetuned_200\")\n```", "```py\n>>> example = dataset[0]\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n>>> print(question)\n>>> pipe(image, question, top_k=1)\n\"Where is he looking?\"\n[{'score': 0.5498199462890625, 'answer': 'down'}]\n```", "```py\n>>> processor = ViltProcessor.from_pretrained(\"MariaK/vilt_finetuned_200\")\n\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n\n>>> # prepare inputs\n>>> inputs = processor(image, question, return_tensors=\"pt\")\n\n>>> model = ViltForQuestionAnswering.from_pretrained(\"MariaK/vilt_finetuned_200\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: down\n```", "```py\n>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n>>> import torch\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)\n```", "```py\n>>> example = dataset[0]\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n```", "```py\n>>> prompt = f\"Question: {question} Answer:\" \n```", "```py\n>>> inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\n\"He is looking at the crowd\" \n```"]