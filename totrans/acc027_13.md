# 示例动物园

> 原始文本: [https://huggingface.co/docs/accelerate/usage_guides/training_zoo](https://huggingface.co/docs/accelerate/usage_guides/training_zoo)

以下是展示 🤗 Accelerate 的教程和脚本的非详尽列表

## 官方 Accelerate 示例:

### 基本示例

这些示例展示了 Accelerate 的基本功能，是一个很好的起点

+   [基本 NLP 示例](https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py)

+   [基本分布式 NLP 示例在 Jupyter Notebook 中](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb)

+   [基本计算机视觉示例](https://github.com/huggingface/accelerate/blob/main/examples/cv_example.py)

+   [基本分布式计算机视觉示例在 Jupyter Notebook 中](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb)

+   [在 Kaggle 中使用 Accelerate](https://www.kaggle.com/code/muellerzr/multi-gpu-and-accelerate)

### 特定功能示例

这些示例展示了 Accelerate 框架提供的特定功能

+   [自动内存感知梯度累积](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/automatic_gradient_accumulation.py)

+   [检查点状态](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/checkpointing.py)

+   [交叉验证](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/cross_validation.py)

+   [DeepSpeed](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/deepspeed_with_config_support.py)

+   [完全分片数据并行](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/fsdp_with_peak_mem_tracking.py)

+   [梯度累积](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/gradient_accumulation.py)

+   [内存感知批量大小查找器](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/memory.py)

+   [度量计算](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/multi_process_metrics.py)

+   [使用跟踪器](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/tracking.py)

+   [使用 Megatron-LM](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/megatron_lm_gpt_pretraining.py)

### 完整示例

这些示例一次展示了 Accelerate 中显示的“特定功能示例”中的所有功能

+   [完整 NLP 示例](https://github.com/huggingface/accelerate/blob/main/examples/complete_nlp_example.py)

+   [完整计算机视觉示例](https://github.com/huggingface/accelerate/blob/main/examples/complete_cv_example.py)

+   [非常完整和可扩展的视觉示例，展示了 SLURM、hydra 和框架的非常可扩展的用法](https://github.com/yuvalkirstain/PickScore)

+   [因果语言模型微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py)

+   [掩码语言模型微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm_no_trainer.py)

+   [语音预训练示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py)

+   [翻译微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation_no_trainer.py)

+   [文本分类微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue_no_trainer.py)

+   [语义分割微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py)

+   [问答微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa_no_trainer.py)

+   [使用 Beam search 进行问题回答微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py)

+   [多项选择问题回答微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/multiple-choice/run_swag_no_trainer.py)

+   [命名实体识别微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner_no_trainer.py)

+   [图像分类微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/run_image_classification_no_trainer.py)

+   [摘要微调示例](https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization_no_trainer.py)

+   [关于如何使用 AWS SageMaker 加速集成的端到端示例](https://github.com/huggingface/notebooks/blob/main/sagemaker/22_accelerate_sagemaker_examples/README.md)

+   [Megatron-LM 用于各种 NLP 任务的示例](https://github.com/pacman100/accelerate-megatron-test)

## 集成示例

这些是与 🤗 加速集成的库的教程：

> 找不到您的集成？提交 PR 进行包含！

### Amphion

+   [使用 Amphion 训练文本到语音模型](https://github.com/open-mmlab/Amphion/blob/main/egs/tts/README.md)

+   [使用 Amphion 训练歌声转换模型](https://github.com/open-mmlab/Amphion/blob/main/egs/svc/README.md)

+   [使用 Amphion 训练 Vocoders](https://github.com/open-mmlab/Amphion/blob/main/egs/vocoder/README.md)

### Catalyst

+   [使用 Catalyst 进行分布式训练教程](https://catalyst-team.github.io/catalyst/tutorials/ddp.html)

### DALLE2-pytorch

+   [DALLE2微调](https://github.com/lucidrains/DALLE2-pytorch#usage)

### 🤗 扩散器

+   [使用扩散器进行文本反演](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion)

+   [使用扩散器训练 DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth)

### fastai

+   [从 Jupyter Notebooks 进行分布式训练与 fastai](https://docs.fast.ai/tutorial.distributed.html)

+   [基本的 fastai 分布式训练示例](https://docs.fast.ai/examples/distributed_app_examples.html)

### GradsFlow

+   [使用 GradsFlow 进行自动图像分类](https://docs.gradsflow.com/en/latest/examples/nbs/01-ImageClassification/)

### imagen-pytorch

+   [微调 Imagen](https://github.com/lucidrains/imagen-pytorch#usage)

### Kornia

+   [使用 Kornia 的 Trainer 进行视觉模型微调](https://kornia.readthedocs.io/en/latest/get-started/training.html)

### PyTorch 加速

+   [PyTorch 加速的快速分布式训练教程](https://pytorch-accelerated.readthedocs.io/en/latest/quickstart.html)

### PyTorch3D

+   [使用 3D 数据进行深度学习](https://pytorch3d.org/tutorials/)

### Stable-Dreamfusion

+   [使用 Stable-Dreamfusion 进行文本转换为 3D 模型的训练](https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF?usp=sharing)

### Tez

+   [使用 Tez 和加速进行叶子病害检测](https://www.kaggle.com/code/abhishek/tez-faster-and-easier-training-for-leaf-detection/notebook)

### trlx

+   [如何使用 trlx 实现情感学习任务](https://github.com/CarperAI/trlx#example-how-to-add-a-task)

### Comfy-UI

+   [使用加速在低 VRAM 环境中使用大型 Stable Diffusion 模型](https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/model_management.py#L291-L296)

## 在科学中

以下是利用 🤗 加速的论文的非详尽列表。

> 找不到您的论文？提交 PR 进行包含！

+   Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy: “Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation”, 2023; [arXiv:2305.01569](http://arxiv.org/abs/2305.01569).

+   Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim: “计划和解决提示：通过大型语言模型改进零样本思维链推理”, 2023; [arXiv:2305.04091](http://arxiv.org/abs/2305.04091).

+   Arthur Câmara, Claudia Hauff: “移动文件的效率研究：将文件移入内存以用于神经IR模型”, 2022; [arXiv:2205.08343](http://arxiv.org/abs/2205.08343).

+   Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, Ce Zhang: “使用单个GPU进行大型语言模型的高吞吐生成推理”, 2023; [arXiv:2303.06865](http://arxiv.org/abs/2303.06865).

+   Peter Melchior, Yan Liang, ChangHoon Hahn, Andy Goulding: “自动编码星系光谱I：架构”, 2022; [arXiv:2211.07890](http://arxiv.org/abs/2211.07890).

+   Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang: “具有软掩码噪声的更便宜更好的扩散语言模型”, 2023; [arXiv:2304.04746](http://arxiv.org/abs/2304.04746).

+   Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa: “Instruct-NeRF2NeRF: 使用说明编辑3D场景”, 2023; [arXiv:2303.12789](http://arxiv.org/abs/2303.12789).

+   Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi: “RealFusion: 从单个图像重建任何对象的360°”, 2023; [arXiv:2302.10663](http://arxiv.org/abs/2302.10663).

+   Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li: “通过人类偏好更好地对齐文本到图像模型”, 2023; [arXiv:2303.14420](http://arxiv.org/abs/2303.14420).

+   Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang: “HuggingGPT: 使用ChatGPT及其HuggingFace朋友解决AI任务”, 2023; [arXiv:2303.17580](http://arxiv.org/abs/2303.17580).

+   Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, Jianshu Chen: “Z-LaVI: 由视觉想象力推动的零样本语言求解器”, 2022; [arXiv:2210.12261](http://arxiv.org/abs/2210.12261).

+   Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho: “如何给扩散模型留后门？”, 2022; [arXiv:2212.05400](http://arxiv.org/abs/2212.05400).

+   Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim: “让2D扩散模型了解3D一致性以实现鲁棒的文本到3D生成”, 2023; [arXiv:2303.07937](http://arxiv.org/abs/2303.07937).

+   Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, Daniel Cohen-Or: “使用文本到图像扩散模型定位对象级形状变化”, 2023; [arXiv:2303.11306](http://arxiv.org/abs/2303.11306).

+   Dídac Surís, Sachit Menon, Carl Vondrick: “ViperGPT: 通过Python执行进行推理的视觉推断”, 2023; [arXiv:2303.08128](http://arxiv.org/abs/2303.08128).

+   Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen: “FateZero: 融合注意力进行零样本基于文本的视频编辑”, 2023; [arXiv:2303.09535](http://arxiv.org/abs/2303.09535).

+   Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi: “NaturalProver: 基于语言模型的数学证明生成”, 2022; [arXiv:2205.12910](http://arxiv.org/abs/2205.12910).

+   Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or: “TEXTure: 文本引导的3D形状纹理”, 2023; [arXiv:2302.01721](http://arxiv.org/abs/2302.01721).

+   Puijin Cheng, Li Lin, Yijin Huang, Huaqing He, Wenhan Luo, Xiaoying Tang: “从退化中学习增强：一种用于眼底图像增强的扩散模型”, 2023; [arXiv:2303.04603](http://arxiv.org/abs/2303.04603).

+   Shun Shao, Yftah Ziser, Shay Cohen: “从神经表示中消除不对齐属性”, 2023; [arXiv:2302.02997](http://arxiv.org/abs/2302.02997).

+   Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, Minjoon Seo: “上下文指导学习”，2023; [arXiv:2302.14691](http://arxiv.org/abs/2302.14691).

+   Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, Anima Anandkumar: “Prismer: 一种具有专家集合的视觉语言模型”，2023; [arXiv:2303.02506](http://arxiv.org/abs/2303.02506).

+   Haoyu Chen, Zhihua Wang, Yang Yang, Qilin Sun, Kede Ma: “学习用于摄影图像的深度色差度量”，2023; [arXiv:2303.14964](http://arxiv.org/abs/2303.14964).

+   Van-Hoang Le, Hongyu Zhang: “基于提示的少样本学习的日志解析”，2023; [arXiv:2302.07435](http://arxiv.org/abs/2302.07435).

+   Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi, Kentaro Inui: “深度神经网络是否捕捉算术推理中的组合性？”，2023; [arXiv:2302.07866](http://arxiv.org/abs/2302.07866).

+   Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu: “行为克隆变压器是神经符号推理者”，2022; [arXiv:2210.07382](http://arxiv.org/abs/2210.07382).

+   Martin Wessel, Tomáš Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, Timo Spinde: “介绍MBIB——第一个媒体偏见识别基准任务和数据集收集”，2023; [arXiv:2304.13148](http://arxiv.org/abs/2304.13148). DOI: [https://dx.doi.org/10.1145/3539618.3591882 10.1145/3539618.3591882].

+   Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or: “Attend-and-Excite: 基于注意力的语义引导用于文本到图像扩散模型”，2023; [arXiv:2301.13826](http://arxiv.org/abs/2301.13826).

+   Marcio Fonseca, Yftah Ziser, Shay B. Cohen: “在长文档的抽象摘要中分解内容和预算决策”，2022; [arXiv:2205.12486](http://arxiv.org/abs/2205.12486).

+   Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or: “TEXTure: 三维形状的文本引导纹理化”，2023; [arXiv:2302.01721](http://arxiv.org/abs/2302.01721).

+   Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia Tsvetkov: “关于文本生成模型评估指标的盲点”，2022; [arXiv:2212.10020](http://arxiv.org/abs/2212.10020).

+   Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham: “上下文检索增强语言模型”，2023; [arXiv:2302.00083](http://arxiv.org/abs/2302.00083).

+   Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P. Xing, Hao Zhang: “MPCFormer: 使用MPC进行快速、高性能和私密的Transformer推断”，2022; [arXiv:2211.01452](http://arxiv.org/abs/2211.01452).

+   Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, Jianfeng Gao: “GODEL: 面向目标导向对话的大规模预训练”，2022; [arXiv:2206.11309](http://arxiv.org/abs/2206.11309).

+   Egil Rønningstad, Erik Velldal, Lilja Øvrelid: “实体级情感分析（ELSA）：一项探索性任务调查”，2023，第29届国际计算语言学会议论文集，2022，6773-6783页; [arXiv:2304.14241](http://arxiv.org/abs/2304.14241).

+   Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine: “自然语言生成的离线RL与隐式语言Q学习”，2022; [arXiv:2206.11871](http://arxiv.org/abs/2206.11871).

+   Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig: “基于执行的评估用于开放领域代码生成”，2022; [arXiv:2212.10481](http://arxiv.org/abs/2212.10481).

+   Minh-Long Luu, Zeyi Huang, Eric P. Xing, Yong Jae Lee, Haohan Wang: “通过随机梯度阈值进行快速显著性引导的混合”，2022; [arXiv:2212.04875](http://arxiv.org/abs/2212.04875).

+   Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng: “MagicMix: 与扩散模型的语义混合”，2022; [arXiv:2210.16056](http://arxiv.org/abs/2210.16056).

+   王雅青，苏布拉塔·穆克吉，刘晓东，高静，艾哈迈德·哈桑·阿瓦达拉，高建峰：“LiST：轻量级提示式自训练使参数高效的少样本学习器”，2021年；[arXiv:2110.06274](http://arxiv.org/abs/2110.06274)。
