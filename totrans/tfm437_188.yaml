- en: Llama2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned Chat
    Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
    by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
    Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan
    Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David
    Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj
    Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,
    Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit
    Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
    Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog,
    Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
    Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan,
    Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,
    Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
    Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom. It is a collection of
    foundation language models ranging from 7B to 70B parameters, with checkpoints
    finetuned for chat application!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Llama2æ¨¡å‹æ˜¯ç”±Hugo Touvronã€Louis Martinã€Kevin Stoneã€Peter Albertã€Amjad Almahairiã€Yasmine
    Babaeiã€Nikolay Bashlykovã€Soumya Batraã€Prajjwal Bhargavaã€Shruti Bhosaleã€Dan Bikelã€Lukas
    Blecherã€Cristian Canton Ferrerã€Moya Chenã€Guillem Cucurullã€David Esiobuã€Jude Fernandesã€Jeremy
    Fuã€Wenyin Fuã€Brian Fullerã€Cynthia Gaoã€Vedanuj Goswamiã€Naman Goyalã€Anthony Hartshornã€Saghar
    Hosseiniã€Rui Houã€Hakan Inanã€Marcin Kardasã€Viktor Kerkez Madian Khabsaã€Isabel Kloumannã€Artem
    Korenevã€Punit Singh Kouraã€Marie-Anne Lachauxã€Thibaut Lavrilã€Jenya Leeã€Diana Liskovichã€Yinghai
    Luã€Yuning Maoã€Xavier Martinetã€Todor Mihaylovã€Pushka rMishraã€Igor Molybogã€Yixin
    Nieã€Andrew Poultonã€Jeremy Reizensteinã€Rashi Rungtaã€Kalyan Saladiã€Alan Scheltenã€Ruan
    Silvaã€Eric Michael Smithã€Ranjan Subramanianã€Xiaoqing EllenTanã€Binh Tangã€Ross Taylorã€Adina
    Williamsã€Jian Xiang Kuanã€Puxin Xuã€Zheng Yanã€Iliyan Zarovã€Yuchen Zhangã€Angela Fanã€Melanie
    Kambadurã€Sharan Narangã€Aurelien Rodriguezã€Robert Stojnicã€Sergey Edunovã€Thomas
    Scialomæå‡ºçš„ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å«ä»7Båˆ°70Bå‚æ•°çš„åŸºç¡€è¯­è¨€æ¨¡å‹çš„é›†åˆï¼Œå…·æœ‰ä¸ºèŠå¤©åº”ç”¨ç¨‹åºè°ƒä¼˜çš„æ£€æŸ¥ç‚¹ï¼
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*In this work, we develop and release Llama 2, a collection of pretrained and
    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
    dialogue use cases. Our models outperform open-source chat models on most benchmarks
    we tested, and based on our human evaluations for helpfulness and safety, may
    be a suitable substitute for closed-source models. We provide a detailed description
    of our approach to fine-tuning and safety improvements of Llama 2-Chat in order
    to enable the community to build on our work and contribute to the responsible
    development of LLMs.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº†Llama 2ï¼Œè¿™æ˜¯ä¸€ç»„é¢„è®­ç»ƒå’Œè°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè§„æ¨¡ä»70äº¿åˆ°700äº¿å‚æ•°ä¸ç­‰ã€‚æˆ‘ä»¬è°ƒä¼˜çš„LLMsï¼Œç§°ä¸ºLlama
    2-Chatï¼Œé’ˆå¯¹å¯¹è¯ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ‘ä»¬æµ‹è¯•çš„å¤§å¤šæ•°åŸºå‡†ä¸Šä¼˜äºå¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„äººç±»è¯„ä¼°ï¼Œå¯¹äºå¸®åŠ©å’Œå®‰å…¨æ€§ï¼Œå¯èƒ½æ˜¯å°é—­æºæ¨¡å‹çš„åˆé€‚æ›¿ä»£å“ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæˆ‘ä»¬å¯¹Llama
    2-Chatè¿›è¡Œè°ƒä¼˜å’Œå®‰å…¨æ”¹è¿›æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œä»¥ä¾¿ä½¿ç¤¾åŒºèƒ½å¤Ÿåœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šæ„å»ºå¹¶ä¿ƒè¿›LLMsçš„è´Ÿè´£ä»»å‘å±•ã€‚*'
- en: Checkout all Llama2 model checkpoints [here](https://huggingface.co/models?search=llama2).
    This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ)
    with contributions from [Lysandre Debut](https://huggingface.co/lysandre). The
    code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox).
    The original code of the authors can be found [here](https://github.com/facebookresearch/llama).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ‰€æœ‰Llama2æ¨¡å‹æ£€æŸ¥ç‚¹[è¿™é‡Œ](https://huggingface.co/models?search=llama2)ã€‚è¯¥æ¨¡å‹ç”±[Arthur
    Zucker](https://huggingface.co/ArthurZ)è´¡çŒ®ï¼Œ[Lysandre Debut](https://huggingface.co/lysandre)ä¹Ÿæœ‰è´¡çŒ®ã€‚Hugging
    Faceä¸­çš„å®ç°ä»£ç åŸºäºGPT-NeoX [è¿™é‡Œ](https://github.com/EleutherAI/gpt-neox)ã€‚ä½œè€…çš„åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/llama)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: The `Llama2` models were trained using `bfloat16`, but the original inference
    uses `float16`. The checkpoints uploaded on the Hub use `torch_dtype = 'float16'`,
    which will be used by the `AutoModel` API to cast the checkpoints from `torch.float32`
    to `torch.float16`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`Llama2`æ¨¡å‹æ˜¯ä½¿ç”¨`bfloat16`è¿›è¡Œè®­ç»ƒçš„ï¼Œä½†åŸå§‹æ¨æ–­ä½¿ç”¨`float16`ã€‚Hubä¸Šä¸Šä¼ çš„æ£€æŸ¥ç‚¹ä½¿ç”¨`torch_dtype=''float16''`ï¼Œ`AutoModel`
    APIå°†ä½¿ç”¨å®ƒå°†æ£€æŸ¥ç‚¹ä»`torch.float32`è½¬æ¢ä¸º`torch.float16`ã€‚'
- en: The `dtype` of the online weights is mostly irrelevant unless you are using
    `torch_dtype="auto"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`. The reason is that the model will first be downloaded
    ( using the `dtype` of the checkpoints online), then it will be casted to the
    default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is
    a `torch_dtype` provided in the config, it will be used.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿æƒé‡çš„`dtype`å¤§å¤šä¸ç›¸å…³ï¼Œé™¤éæ‚¨åœ¨ä½¿ç”¨`model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`åˆå§‹åŒ–æ¨¡å‹æ—¶ä½¿ç”¨`torch_dtype="auto"`ã€‚åŸå› æ˜¯æ¨¡å‹å°†é¦–å…ˆè¢«ä¸‹è½½ï¼ˆä½¿ç”¨åœ¨çº¿æ£€æŸ¥ç‚¹çš„`dtype`ï¼‰ï¼Œç„¶åå°†è¢«è½¬æ¢ä¸º`torch`çš„é»˜è®¤`dtype`ï¼ˆå˜ä¸º`torch.float32`ï¼‰ï¼Œæœ€åï¼Œå¦‚æœé…ç½®ä¸­æä¾›äº†`torch_dtype`ï¼Œåˆ™å°†ä½¿ç”¨å®ƒã€‚
- en: Training the model in `float16` is not recommended and is known to produce `nan`;
    as such, the model should be trained in `bfloat16`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å»ºè®®åœ¨`float16`ä¸­è®­ç»ƒæ¨¡å‹ï¼Œå·²çŸ¥ä¼šäº§ç”Ÿ`nan`ï¼›å› æ­¤ï¼Œæ¨¡å‹åº”è¯¥åœ¨`bfloat16`ä¸­è¿›è¡Œè®­ç»ƒã€‚
- en: 'Tips:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼š
- en: Weights for the Llama2 models can be obtained by filling out [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama2æ¨¡å‹çš„æƒé‡å¯ä»¥é€šè¿‡å¡«å†™[æ­¤è¡¨æ ¼](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)è·å¾—
- en: The architecture is very similar to the first Llama, with the addition of Grouped
    Query Attention (GQA) following this [paper](https://arxiv.org/pdf/2305.13245.pdf)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¶æ„ä¸ç¬¬ä¸€ä¸ªLlamaéå¸¸ç›¸ä¼¼ï¼Œå¢åŠ äº†Grouped Query Attentionï¼ˆGQAï¼‰ï¼Œå‚è€ƒè¿™ç¯‡[è®ºæ–‡](https://arxiv.org/pdf/2305.13245.pdf)
- en: Setting `config.pretraining_tp` to a value different than 1 will activate the
    more accurate but slower computation of the linear layers, which should better
    match the original logits.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°† `config.pretraining_tp` è®¾ç½®ä¸ºä¸ 1 ä¸åŒçš„å€¼å°†æ¿€æ´»çº¿æ€§å±‚çš„æ›´å‡†ç¡®ä½†æ›´æ…¢çš„è®¡ç®—ï¼Œè¿™åº”è¯¥æ›´å¥½åœ°åŒ¹é…åŸå§‹å¯¹æ•°ã€‚
- en: The original model uses `pad_id = -1` which means that there is no padding token.
    We canâ€™t have the same logic, make sure to add a padding token using `tokenizer.add_special_tokens({"pad_token":"<pad>"})`
    and resize the token embedding accordingly. You should also set the `model.config.pad_token_id`.
    The `embed_tokens` layer of the model is initialized with `self.embed_tokens =
    nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`,
    which makes sure that encoding the padding token will output zeros, so passing
    it when initializing is recommended.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸå§‹æ¨¡å‹ä½¿ç”¨ `pad_id = -1`ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰å¡«å……æ ‡è®°ã€‚æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼Œç¡®ä¿ä½¿ç”¨ `tokenizer.add_special_tokens({"pad_token":"<pad>"})`
    æ·»åŠ ä¸€ä¸ªå¡«å……æ ‡è®°ï¼Œå¹¶ç›¸åº”è°ƒæ•´ä»¤ç‰ŒåµŒå…¥ã€‚æ‚¨è¿˜åº”è¯¥è®¾ç½® `model.config.pad_token_id`ã€‚æ¨¡å‹çš„ `embed_tokens` å±‚ä½¿ç”¨ `self.embed_tokens
    = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`
    è¿›è¡Œåˆå§‹åŒ–ï¼Œè¿™ç¡®ä¿äº†å¯¹å¡«å……æ ‡è®°è¿›è¡Œç¼–ç å°†è¾“å‡ºé›¶ï¼Œå› æ­¤åœ¨åˆå§‹åŒ–æ—¶ä¼ é€’å®ƒæ˜¯æ¨èçš„ã€‚
- en: 'After filling out the form and gaining access to the model checkpoints, you
    should be able to use the already converted checkpoints. Otherwise, if you are
    converting your own model, feel free to use the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py).
    The script can be called with the following (example) command:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¡«å†™è¡¨æ ¼å¹¶è·å¾—æ¨¡å‹æ£€æŸ¥ç‚¹è®¿é—®æƒé™åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨å·²è½¬æ¢çš„æ£€æŸ¥ç‚¹ã€‚å¦åˆ™ï¼Œå¦‚æœæ‚¨æ­£åœ¨è½¬æ¢è‡ªå·±çš„æ¨¡å‹ï¼Œè¯·éšæ—¶ä½¿ç”¨ [è½¬æ¢è„šæœ¬](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ï¼ˆç¤ºä¾‹ï¼‰å‘½ä»¤è°ƒç”¨è„šæœ¬ï¼š
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After conversion, the model and tokenizer can be loaded via:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è½¬æ¢åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that executing the script requires enough CPU RAM to host the whole model
    in float16 precision (even if the biggest versions come in several checkpoints
    they each contain a part of each weight of the model, so we need to load them
    all in RAM). For the 75B model, itâ€™s thus 145GB of RAM needed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ‰§è¡Œè„šæœ¬éœ€è¦è¶³å¤Ÿçš„ CPU RAM ä»¥åœ¨ float16 ç²¾åº¦ä¸­æ‰˜ç®¡æ•´ä¸ªæ¨¡å‹ï¼ˆå³ä½¿æœ€å¤§ç‰ˆæœ¬åˆ†ä¸ºå¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œæ¯ä¸ªæ£€æŸ¥ç‚¹éƒ½åŒ…å«æ¨¡å‹çš„æ¯ä¸ªæƒé‡çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å…¨éƒ¨åŠ è½½åˆ°
    RAM ä¸­ï¼‰ã€‚å¯¹äº 75B æ¨¡å‹ï¼Œå› æ­¤éœ€è¦ 145GB çš„ RAMã€‚
- en: The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece).
    One quirk of sentencepiece is that when decoding a sequence, if the first token
    is the start of the word (e.g. â€œBananaâ€), the tokenizer does not prepend the prefix
    space to the string.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA åˆ†è¯å™¨æ˜¯åŸºäº [sentencepiece](https://github.com/google/sentencepiece) çš„ BPE
    æ¨¡å‹ã€‚sentencepiece çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯ï¼Œåœ¨è§£ç åºåˆ—æ—¶ï¼Œå¦‚æœç¬¬ä¸€ä¸ªä»¤ç‰Œæ˜¯å•è¯çš„å¼€å¤´ï¼ˆä¾‹å¦‚â€œBananaâ€ï¼‰ï¼Œåˆ†è¯å™¨ä¸ä¼šåœ¨å­—ç¬¦ä¸²å‰é¢æ·»åŠ å‰ç¼€ç©ºæ ¼ã€‚
- en: When using Flash Attention 2 via `attn_implementation="flash_attention_2"`,
    donâ€™t pass `torch_dtype` to the `from_pretrained` class method and use Automatic
    Mixed-Precision training. When using `Trainer`, it is simply specifying either
    `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`.
    This is required because the Flash Attention only support `fp16` and `bf16` data
    type.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ `attn_implementation="flash_attention_2"` ä½¿ç”¨ Flash Attention 2 æ—¶ï¼Œä¸è¦å°† `torch_dtype`
    ä¼ é€’ç»™ `from_pretrained` ç±»æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒã€‚å½“ä½¿ç”¨ `Trainer` æ—¶ï¼Œåªéœ€å°† `fp16` æˆ– `bf16` æŒ‡å®šä¸º
    `True`ã€‚å¦åˆ™ï¼Œè¯·ç¡®ä¿æ‚¨ä½¿ç”¨ `torch.autocast`ã€‚è¿™æ˜¯å¿…éœ€çš„ï¼Œå› ä¸º Flash Attention ä»…æ”¯æŒ `fp16` å’Œ `bf16`
    æ•°æ®ç±»å‹ã€‚
- en: Resources
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with LLaMA2\. If youâ€™re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and weâ€™ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ LLaMA2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull
    Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: '[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2),
    a blog post about Llama 2 and how to use it with ğŸ¤— Transformers and ğŸ¤— PEFT.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama 2 å·²å‘å¸ƒ - åœ¨ Hugging Face ä¸Šè·å–](https://huggingface.co/blog/llama2)ï¼Œå…³äº Llama
    2 åŠå¦‚ä½•ä¸ ğŸ¤— Transformers å’Œ ğŸ¤— PEFT ä¸€èµ·ä½¿ç”¨çš„åšå®¢æ–‡ç« ã€‚'
- en: '[LLaMA 2 - Every Resource you need](https://www.philschmid.de/llama-2), a compilation
    of relevant resources to learn about LLaMA 2 and how to get started quickly.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLaMA 2 - æ‚¨éœ€è¦çš„æ‰€æœ‰èµ„æº](https://www.philschmid.de/llama-2)ï¼Œä¸€ä¸ªç›¸å…³èµ„æºçš„æ±‡ç¼–ï¼Œç”¨äºäº†è§£ LLaMA
    2 å¹¶å¿«é€Ÿå…¥é—¨ã€‚'
- en: Text Generation
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç”Ÿæˆ
- en: A [notebook](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)
    on how to fine-tune Llama 2 in Google Colab using QLoRA and 4-bit precision. ğŸŒ
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨ Google Colab ä¸­ä½¿ç”¨ QLoRA å’Œ 4 ä½ç²¾åº¦å¯¹ Llama 2 è¿›è¡Œå¾®è°ƒçš„ [ç¬”è®°æœ¬](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)ã€‚ğŸŒ
- en: A [notebook](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing)
    on how to fine-tune the â€œLlama-v2-7b-guanacoâ€ model with 4-bit QLoRA and generate
    Q&A datasets from PDFs. ğŸŒ
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ 4 ä½ QLoRA å¯¹â€œLlama-v2-7b-guanacoâ€æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶ä» PDF ä¸­ç”Ÿæˆé—®ç­”æ•°æ®é›†çš„ [ç¬”è®°æœ¬](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing)ã€‚ğŸŒ
- en: Text Classification
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»
- en: A [notebook](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing)
    on how to fine-tune the Llama 2 model with QLoRa, TRL, and Korean text classification
    dataset. ğŸŒğŸ‡°ğŸ‡·
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ QLoRaã€TRL å’ŒéŸ©æ–‡æ–‡æœ¬åˆ†ç±»æ•°æ®é›†å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ [ç¬”è®°æœ¬](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing)ã€‚ğŸŒğŸ‡°ğŸ‡·
- en: âš—ï¸ Optimization
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: âš—ï¸ ä¼˜åŒ–
- en: '[Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl), a guide
    to using the TRL libraryâ€™s DPO method to fine tune Llama 2 on a specific dataset.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ DPO å¯¹ Llama 2 è¿›è¡Œå¾®è°ƒ](https://huggingface.co/blog/dpo-trl)ï¼Œä¸€ä¸ªæŒ‡å—ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨ TRL
    åº“çš„ DPO æ–¹æ³•å¯¹ç‰¹å®šæ•°æ®é›†ä¸Šçš„ Llama 2 è¿›è¡Œå¾®è°ƒã€‚'
- en: '[Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2),
    a guide to training Llama 2 to generate instructions from inputs, transforming
    the model from instruction-following to instruction-giving.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ‰©å±•æŒ‡å—ï¼šæŒ‡å¯¼è°ƒæ•´ Llama 2](https://www.philschmid.de/instruction-tune-llama-2)ï¼Œä¸€ä¸ªæŒ‡å—ï¼Œç”¨äºè®­ç»ƒ
    Llama 2 ä»è¾“å…¥ç”ŸæˆæŒ‡ä»¤ï¼Œå°†æ¨¡å‹ä»éµå¾ªæŒ‡ä»¤è½¬å˜ä¸ºç»™å‡ºæŒ‡ä»¤ã€‚'
- en: A [notebook](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing)
    on how to fine-tune the Llama 2 model on a personal computer using QLoRa and TRL.
    ğŸŒ
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing)ï¼Œä»‹ç»å¦‚ä½•åœ¨ä¸ªäººè®¡ç®—æœºä¸Šä½¿ç”¨
    QLoRa å’Œ TRL å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: âš¡ï¸ Inference
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: âš¡ï¸ æ¨ç†
- en: A [notebook](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)
    on how to quantize the Llama 2 model using GPTQ from the AutoGPTQ library. ğŸŒ
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨
    AutoGPTQ åº“ä¸­çš„ GPTQ å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚
- en: A [notebook](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing)
    on how to run the Llama 2 Chat Model with 4-bit quantization on a local computer
    or Google Colab. ğŸŒ
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing)ï¼Œä»‹ç»å¦‚ä½•åœ¨æœ¬åœ°è®¡ç®—æœºæˆ–
    Google Colab ä¸Šè¿è¡Œå¸¦æœ‰ 4 ä½é‡åŒ–çš„ Llama 2 Chat Modelã€‚
- en: ğŸš€ Deploy
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ éƒ¨ç½²
- en: '[Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora),
    a complete guide from setup to QLoRA fine-tuning and deployment on Amazon SageMaker.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åœ¨äºšé©¬é€Š SageMaker ä¸Šå¯¹ LLaMA 2 (7-70B) è¿›è¡Œå¾®è°ƒ](https://www.philschmid.de/sagemaker-llama2-qlora)ï¼Œä»è®¾ç½®åˆ°
    QLoRA å¾®è°ƒå’Œéƒ¨ç½²çš„å®Œæ•´æŒ‡å—ã€‚'
- en: '[Deploy Llama 2 7B/13B/70B on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama-llm),
    a guide on using Hugging Faceâ€™s LLM DLC container for secure and scalable deployment.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åœ¨äºšé©¬é€Š SageMaker ä¸Šéƒ¨ç½² Llama 2 7B/13B/70B](https://www.philschmid.de/sagemaker-llama-llm)ï¼Œä½¿ç”¨
    Hugging Face çš„ LLM DLC å®¹å™¨è¿›è¡Œå®‰å…¨å’Œå¯æ‰©å±•éƒ¨ç½²çš„æŒ‡å—ã€‚'
- en: LlamaConfig
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlamaConfig
- en: '### `class transformers.LlamaConfig`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlamaConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/configuration_llama.py#L31)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/configuration_llama.py#L31)'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 32000) â€” Vocabulary size of the
    LLaMA model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 32000) â€” LLaMA æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)
    æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) â€” Dimension of the hidden
    representations.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 4096) â€” éšè—è¡¨ç¤ºçš„ç»´åº¦ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 11008) â€” Dimension of the
    MLP representations.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 11008) â€” MLP è¡¨ç¤ºçš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 32) â€” Number of hidden
    layers in the Transformer decoder.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 32) â€” Transformer è§£ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) â€” Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 32) â€” Transformer è§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚'
- en: '`num_key_value_heads` (`int`, *optional*) â€” This is the number of key_value
    heads that should be used to implement Grouped Query Attention. If `num_key_value_heads=num_attention_heads`,
    the model will use Multi Head Attention (MHA), if `num_key_value_heads=1 the model
    will use Multi Query Attention (MQA) otherwise GQA is used. When converting a
    multi-head checkpoint to a GQA checkpoint, each group key and value head should
    be constructed by meanpooling all the original heads within that group. For more
    details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is
    not specified, will default to` num_attention_heads`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_key_value_heads` (`int`, *å¯é€‰*) â€” è¿™æ˜¯åº”è¯¥ç”¨äºå®ç° Grouped Query Attention çš„ key_value
    heads çš„æ•°é‡ã€‚å¦‚æœ `num_key_value_heads=num_attention_heads`ï¼Œæ¨¡å‹å°†ä½¿ç”¨ Multi Head Attention
    (MHA)ï¼Œå¦‚æœ `num_key_value_heads=1`ï¼Œæ¨¡å‹å°†ä½¿ç”¨ Multi Query Attention (MQA)ï¼Œå¦åˆ™ä½¿ç”¨ GQAã€‚å°†å¤šå¤´æ£€æŸ¥ç‚¹è½¬æ¢ä¸º
    GQA æ£€æŸ¥ç‚¹æ—¶ï¼Œæ¯ä¸ªç»„é”®å’Œå€¼å¤´åº”é€šè¿‡å¯¹è¯¥ç»„ä¸­æ‰€æœ‰åŸå§‹å¤´è¿›è¡Œå‡å€¼æ± åŒ–æ¥æ„å»ºã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ­¤è®ºæ–‡](https://arxiv.org/pdf/2305.13245.pdf)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†é»˜è®¤ä¸º`
    num_attention_heads`ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) â€” The
    non-linear activation function (function or string) in the decoder.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` æˆ– `function`, *å¯é€‰*, é»˜è®¤ä¸º `"silu"`) â€” è§£ç å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 2048) â€” The maximum
    sequence length that this model might ever be used with. Llama 1 supports up to
    2048 tokens, Llama 2 up to 4096, CodeLlama up to 16384.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2048) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚Llama 1
    æ”¯æŒæœ€å¤š 2048 ä¸ªä»¤ç‰Œï¼ŒLlama 2 æ”¯æŒæœ€å¤š 4096 ä¸ªä»¤ç‰Œï¼ŒCodeLlama æ”¯æŒæœ€å¤š 16384 ä¸ªä»¤ç‰Œã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`rms_norm_eps` (`float`, *optional*, defaults to 1e-06) â€” The epsilon used
    by the rms normalization layers.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rms_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-06) â€” rms normalization å±‚ä½¿ç”¨çš„ epsilonã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨ `config.is_decoder=True`
    æ—¶ç›¸å…³ã€‚'
- en: '`pad_token_id` (`int`, *optional*) â€” Padding token id.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *å¯é€‰*) â€” å¡«å……ä»¤ç‰Œ idã€‚'
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) â€” Beginning of stream token
    id.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æµçš„å¼€å§‹ä»¤ç‰Œ idã€‚'
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) â€” End of stream token id.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2) â€” æµçš„ç»“æŸä»¤ç‰Œ idã€‚'
- en: '`pretraining_tp` (`int`, *optional*, defaults to 1) â€” Experimental feature.
    Tensor parallelism rank used during pretraining. Please refer to [this document](https://huggingface.co/docs/transformers/parallelism)
    to understand more about it. This value is necessary to ensure exact reproducibility
    of the pretraining results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretraining_tp` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” å®éªŒæ€§åŠŸèƒ½ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å¼ é‡å¹¶è¡Œæ€§ç­‰çº§ã€‚è¯·å‚è€ƒ[æ­¤æ–‡æ¡£](https://huggingface.co/docs/transformers/parallelism)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ­¤å€¼å¯¹äºç¡®ä¿é¢„è®­ç»ƒç»“æœçš„ç²¾ç¡®å¯é‡ç°æ€§æ˜¯å¿…è¦çš„ã€‚è¯·å‚è€ƒ[æ­¤é—®é¢˜](https://github.com/pytorch/pytorch/issues/76232)ã€‚'
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) â€” Whether to
    tie weight embeddings'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tie_word_embeddings`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ç»‘å®šæƒé‡åµŒå…¥'
- en: '`rope_theta` (`float`, *optional*, defaults to 10000.0) â€” The base period of
    the RoPE embeddings.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rope_theta`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º10000.0ï¼‰â€” RoPEåµŒå…¥çš„åŸºæœ¬å‘¨æœŸã€‚'
- en: '`rope_scaling` (`Dict`, *optional*) â€” Dictionary containing the scaling configuration
    for the RoPE embeddings. Currently supports two scaling strategies: linear and
    dynamic. Their scaling factor must be a float greater than 1\. The expected format
    is `{"type": strategy name, "factor": scaling factor}`. When using this flag,
    donâ€™t update `max_position_embeddings` to the expected new maximum. See the following
    thread for more information on how these scaling strategies behave: [https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/).
    This is an experimental feature, subject to breaking API changes in future versions.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rope_scaling`ï¼ˆ`Dict`ï¼Œ*å¯é€‰*ï¼‰â€” åŒ…å«RoPEåµŒå…¥çš„ç¼©æ”¾é…ç½®çš„å­—å…¸ã€‚ç›®å‰æ”¯æŒä¸¤ç§ç¼©æ”¾ç­–ç•¥ï¼šçº¿æ€§å’ŒåŠ¨æ€ã€‚å®ƒä»¬çš„ç¼©æ”¾å› å­å¿…é¡»æ˜¯å¤§äº1çš„æµ®ç‚¹æ•°ã€‚é¢„æœŸæ ¼å¼ä¸º`{"type":
    ç­–ç•¥åç§°, "factor": ç¼©æ”¾å› å­}`ã€‚ä½¿ç”¨æ­¤æ ‡å¿—æ—¶ï¼Œä¸è¦å°†`max_position_embeddings`æ›´æ–°ä¸ºé¢„æœŸçš„æ–°æœ€å¤§å€¼ã€‚æœ‰å…³è¿™äº›ç¼©æ”¾ç­–ç•¥è¡Œä¸ºçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹ä¸»é¢˜ï¼š[https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿç ´åæ€§APIæ›´æ”¹ã€‚'
- en: '`attention_bias` (`bool`, defaults to `False`, *optional*, defaults to `False`)
    â€” Whether to use a bias in the query, key, value and output projection layers
    during self-attention.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_bias`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” åœ¨è‡ªæ³¨æ„åŠ›æœŸé—´çš„æŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºæŠ•å½±å±‚ä¸­æ˜¯å¦ä½¿ç”¨åç½®ã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: This is the configuration class to store the configuration of a [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel).
    It is used to instantiate an LLaMA model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the LLaMA-7B.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªLLaMAæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLLaMA-7Bçš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: LlamaTokenizer
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlamaTokenizer
- en: '### `class transformers.LlamaTokenizer`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlamaTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L66)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L66)'
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” Path to the vocabulary file.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`ï¼ˆ`str`ï¼‰â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`unk_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<unk>"`)
    â€” The unknown token. A token that is not in the vocabulary cannot be converted
    to an ID and is set to be this token instead.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: '`bos_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<s>"`)
    â€” The beginning of sequence token that was used during pretraining. Can be used
    a sequence classifier token.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<s>"`ï¼‰â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚'
- en: '`eos_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"</s>"`)
    â€” The end of sequence token.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"</s>"`ï¼‰â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚'
- en: '`pad_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    used to make arrays of tokens the same size for batching purpose. Will then be
    ignored by attention mechanisms or loss computation.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿ä»¤ç‰Œæ•°ç»„å¤§å°ç›¸åŒä»¥è¿›è¡Œæ‰¹å¤„ç†çš„ç‰¹æ®Šä»¤ç‰Œã€‚ç„¶åå°†è¢«æ³¨æ„åŠ›æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚'
- en: '`sp_model_kwargs` (`Dict[str, Any]`, `Optional`, *optional*) â€” Will be passed
    to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ`Optional`ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™`SentencePieceProcessor.__init__()`æ–¹æ³•ã€‚[SentencePieceçš„PythonåŒ…è£…å™¨](https://github.com/google/sentencepiece/tree/master/python)å¯ç”¨äºè®¾ç½®ï¼š'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`ï¼šå¯ç”¨å­è¯æ­£åˆ™åŒ–ã€‚'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`ï¼šunigramçš„æŠ½æ ·å‚æ•°ã€‚å¯¹äºBPE-Dropoutæ— æ•ˆã€‚'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`ï¼šä¸æ‰§è¡ŒæŠ½æ ·ã€‚'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`ï¼šä»nbest_sizeç»“æœä¸­æŠ½æ ·ã€‚'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`ï¼šå‡è®¾nbest_sizeä¸ºæ— é™å¤§ï¼Œå¹¶ä½¿ç”¨å‰å‘è¿‡æ»¤å’Œåå‘æŠ½æ ·ç®—æ³•ä»æ‰€æœ‰å‡è®¾ï¼ˆæ ¼å­ï¼‰ä¸­æŠ½æ ·ã€‚'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`ï¼šunigramæŠ½æ ·çš„å¹³æ»‘å‚æ•°ï¼Œä»¥åŠBPE-dropoutåˆå¹¶æ“ä½œçš„dropoutæ¦‚ç‡ã€‚'
- en: '`add_bos_token` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    add an `bos_token` at the start of sequences.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_bos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—å¼€å¤´æ·»åŠ `bos_token`ã€‚'
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to add an `eos_token` at the end of sequences.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_eos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ `eos_token`ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) â€”
    Whether or not to cleanup spaces after decoding, cleanup consists in removing
    potential artifacts like extra spaces.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç åæ¸…é™¤ç©ºæ ¼ï¼Œæ¸…é™¤åŒ…æ‹¬åˆ é™¤é¢å¤–ç©ºæ ¼ç­‰æ½œåœ¨å·¥ä»¶ã€‚'
- en: '`use_default_system_prompt` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the default system prompt for Llama should be used.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_default_system_prompt`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åº”ä½¿ç”¨Llamaçš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚'
- en: '`spaces_between_special_tokens` (`bool`, *optional*, defaults to `False`) â€”
    Whether or not to add spaces between special tokens.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spaces_between_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨ç‰¹æ®Šæ ‡è®°ä¹‹é—´æ·»åŠ ç©ºæ ¼ã€‚'
- en: '`legacy` (`bool`, *optional*) â€” Whether or not the `legacy` behavior of the
    tokenizer should be used. Legacy is before the merge of #24622 and #25224 which
    includes fixes to properly handle tokens that appear after special tokens. A simple
    example:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`legacy`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦åº”ä½¿ç”¨åˆ†è¯å™¨çš„`legacy`è¡Œä¸ºã€‚åœ¨åˆå¹¶ï¼ƒ24622å’Œï¼ƒ25224ä¹‹å‰çš„é—ç•™ç‰ˆæœ¬ä¸­ï¼ŒåŒ…æ‹¬ä¿®å¤æ­£ç¡®å¤„ç†å‡ºç°åœ¨ç‰¹æ®Šæ ‡è®°ä¹‹åçš„æ ‡è®°çš„é—®é¢˜ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š'
- en: '`legacy=True`:'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`legacy=True`ï¼š'
- en: Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default
    padding token is unset as there is no padding token in the original model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªLlamaåˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚é»˜è®¤å¡«å……æ ‡è®°æœªè®¾ç½®ï¼Œå› ä¸ºåŸå§‹æ¨¡å‹ä¸­æ²¡æœ‰å¡«å……æ ‡è®°ã€‚
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L333)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L333)'
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `get_special_tokens_mask`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L344)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L344)'
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” List of IDs.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” IDåˆ—è¡¨ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” Optional second list of IDs for sequence
    pairs.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ªIDåˆ—è¡¨ã€‚'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»ä½¿ç”¨æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°æ ¼å¼åŒ–ã€‚'
- en: Returns
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º[0, 1]ï¼š1è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—IDã€‚åœ¨ä½¿ç”¨åˆ†è¯å™¨çš„`prepare_for_model`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L381)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L381)'
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” List of ids.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” IDåˆ—è¡¨ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” Optional second list of IDs for sequence
    pairs.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ªIDåˆ—è¡¨ã€‚'
- en: Returns
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç»™å®šåºåˆ—çš„[æ ‡è®°ç±»å‹ID](../glossary#token-type-ids)åˆ—è¡¨ã€‚
- en: Creates a mask from the two sequences passed to be used in a sequence-pair classification
    task. An ALBERT
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚ä¸€ä¸ªALBERT
- en: 'sequence pair mask has the following format:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åºåˆ—å¯¹æ©ç çš„æ ¼å¼å¦‚ä¸‹ï¼š
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: if token_ids_1 is None, only returns the first portion of the mask (0s).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœtoken_ids_1ä¸ºNoneï¼Œåˆ™ä»…è¿”å›æ©ç çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ0sï¼‰ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L306)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L306)'
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str`) â€” The directory in which to save the vocabulary.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`ï¼ˆ`str`ï¼‰â€” ä¿å­˜è¯æ±‡è¡¨çš„ç›®å½•ã€‚'
- en: Returns
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Tuple(str)`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple(str)`'
- en: Paths to the files saved.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ã€‚
- en: Save the vocabulary and special tokens file to a directory.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯æ±‡è¡¨å’Œç‰¹æ®Šæ ‡è®°æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ä¸­ã€‚
- en: LlamaTokenizerFast
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlamaTokenizerFast
- en: '### `class transformers.LlamaTokenizerFast`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlamaTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L57)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L57)'
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`, *optional*) â€” [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .model extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” [SentencePiece](https://github.com/google/sentencepiece)æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰.modelæ‰©å±•åï¼‰ï¼Œå…¶ä¸­åŒ…å«å®ä¾‹åŒ–åˆ†è¯å™¨æ‰€éœ€çš„è¯æ±‡è¡¨ã€‚'
- en: '`tokenizer_file` (`str`, *optional*) â€” [tokenizers](https://github.com/huggingface/tokenizers)
    file (generally has a .json extension) that contains everything needed to load
    the tokenizer.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” [tokenizers](https://github.com/huggingface/tokenizers)æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰.jsonæ‰©å±•åï¼‰ï¼Œå…¶ä¸­åŒ…å«åŠ è½½åˆ†è¯å™¨æ‰€éœ€çš„æ‰€æœ‰å†…å®¹ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) â€”
    Whether or not to cleanup spaces after decoding, cleanup consists in removing
    potential artifacts like extra spaces.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç åæ¸…é™¤ç©ºæ ¼ï¼Œæ¸…é™¤åŒ…æ‹¬åˆ é™¤é¢å¤–ç©ºæ ¼ç­‰æ½œåœ¨å·¥ä»¶ã€‚'
- en: '`unk_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<unk>"`)
    â€” The unknown token. A token that is not in the vocabulary cannot be converted
    to an ID and is set to be this token instead.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: '`bos_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<s>"`)
    â€” The beginning of sequence token that was used during pretraining. Can be used
    a sequence classifier token.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<s>"`ï¼‰â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ä»¥ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚'
- en: '`eos_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"</s>"`)
    â€” The end of sequence token.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"</s>"`ï¼‰â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚'
- en: '`add_bos_token` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    add an `bos_token` at the start of sequences.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_bos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—å¼€å¤´æ·»åŠ `bos_token`ã€‚'
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to add an `eos_token` at the end of sequences.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_eos_token`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ `eos_token`ã€‚'
- en: '`use_default_system_prompt` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the default system prompt for Llama should be used.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_default_system_prompt`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨Llamaçš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚'
- en: Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªLlamaåˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚
- en: This uses notably ByteFallback and no normalization.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸»è¦ä½¿ç”¨ByteFallbackå’Œæ— è§„èŒƒåŒ–ã€‚
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you want to change the `bos_token` or the `eos_token`, make sure to specify
    them when initializing the model, or call `tokenizer.update_post_processor()`
    to make sure that the post-processing is correctly done (otherwise the values
    of the first token and final token of an encoded sequence will not be correct).
    For more details, checkout [post-processors] ([https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors))
    documentation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹`bos_token`æˆ–`eos_token`ï¼Œè¯·ç¡®ä¿åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶æŒ‡å®šå®ƒä»¬ï¼Œæˆ–è°ƒç”¨`tokenizer.update_post_processor()`ä»¥ç¡®ä¿åå¤„ç†æ­£ç¡®å®Œæˆï¼ˆå¦åˆ™ç¼–ç åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°å’Œæœ€åä¸€ä¸ªæ ‡è®°çš„å€¼å°†ä¸æ­£ç¡®ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[åå¤„ç†å™¨]ï¼ˆ[https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors)ï¼‰æ–‡æ¡£ã€‚
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L272)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L272)'
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#### `get_special_tokens_mask`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)'
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” List of ids of the first sequence.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªåºåˆ—çš„idåˆ—è¡¨ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” List of ids of the second sequence.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªåºåˆ—çš„idåˆ—è¡¨ã€‚'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: Returns
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: A list of integers in the range [0, 1]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªèŒƒå›´åœ¨[0, 1]å†…çš„æ•´æ•°åˆ—è¡¨
- en: 1 for a special token, 0 for a sequence token.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚
- en: Retrieves sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    or `encode_plus` methods.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—idã€‚åœ¨ä½¿ç”¨tokenizerçš„`prepare_for_model`æˆ–`encode_plus`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” The first tokenized sequence.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” The second tokenized sequence.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚'
- en: Returns
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The token type ids.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°ç±»å‹idã€‚
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹IDã€‚[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹æœ‰ç‰¹æ®Šçš„æ„å»ºæ–¹å¼ï¼Œåº”è¯¥åœ¨å­ç±»ä¸­é‡å†™æ­¤æ–¹æ³•ã€‚
- en: '#### `update_post_processor`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update_post_processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L146)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L146)'
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Updates the underlying post processor with the current `bos_token` and `eos_token`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°åº•å±‚çš„åå¤„ç†å™¨ï¼Œä½¿ç”¨å½“å‰çš„`bos_token`å’Œ`eos_token`ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L190)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L190)'
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: LlamaModel
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlamaModel
- en: '### `class transformers.LlamaModel`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlamaModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L939)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L939)'
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights. config â€” LlamaConfig'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚é…ç½®
    â€” LlamaConfig'
- en: The bare LLaMA Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„LLaMAæ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `LlamaDecoderLayer`
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Transformerè§£ç å™¨ç”±*config.num_hidden_layers*å±‚ç»„æˆã€‚æ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ª`LlamaDecoderLayer`ã€‚
- en: '#### `forward`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L974)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L974)'
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0,
    1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«æ©ç›–çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«æ©ç›–çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¦è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: 1 indicates the head is `not masked`,
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    â€” Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚'
- en: 'Two formats are allowed:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…è®¸ä¸¤ç§æ ¼å¼ï¼š
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” å¯é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: The [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)
    forward method, overrides the `__call__` special method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: LlamaForCausalLM
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlamaForCausalLM
- en: '### `class transformers.LlamaForCausalLM`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlamaForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1106)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1106)'
- en: '[PRE19]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#### `forward`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1136)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1136)'
- en: '[PRE20]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰ â€”
    é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ï¼Œä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«â€œæ©ç›–â€çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: 1 indicates the head is `not masked`,
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç›–â€ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€”
    æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    â€” Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨è§£ç çš„å…ˆå‰é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚'
- en: 'Two formats are allowed:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…è®¸ä¸¤ç§æ ¼å¼ï¼š
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: 'Args â€” labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‚æ•° â€” æ ‡ç­¾ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰ï¼šç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚
- en: Returns
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    and inputs.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`ï¼‰â€”
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼‰'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LlamaForCausalLM](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlamaForCausalLM](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForCausalLM)çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE21]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: LlamaForSequenceClassification
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlamaForSequenceClassification
- en: '### `class transformers.LlamaForSequenceClassification`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LlamaForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1295)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1295)'
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The LLaMa Model transformer with a sequence classification head on top (linear
    layer).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰é¡¶éƒ¨åºåˆ—åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰çš„LLaMaæ¨¡å‹å˜æ¢å™¨ã€‚
- en: '[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)ä½¿ç”¨æœ€åä¸€ä¸ªæ ‡è®°æ¥è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æœæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-2ï¼‰ä¸€æ ·ã€‚'
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå®ƒå¯¹æœ€åä¸€ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤éœ€è¦çŸ¥é“æœ€åä¸€ä¸ªæ ‡è®°çš„ä½ç½®ã€‚å¦‚æœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œåˆ™åœ¨æ¯è¡Œä¸­æ‰¾åˆ°ä¸æ˜¯å¡«å……æ ‡è®°çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚å¦‚æœæœªå®šä¹‰`pad_token_id`ï¼Œåˆ™åœ¨æ‰¹æ¬¡çš„æ¯è¡Œä¸­ç®€å•åœ°å–æœ€åä¸€ä¸ªå€¼ã€‚ç”±äºåœ¨ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶æ— æ³•çŒœæµ‹å¡«å……æ ‡è®°ï¼Œå› æ­¤æ‰§è¡Œç›¸åŒæ“ä½œï¼ˆåœ¨æ‰¹æ¬¡çš„æ¯è¡Œä¸­å–æœ€åä¸€ä¸ªå€¼ï¼‰ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1326)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1326)'
- en: '[PRE23]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›äº†å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0,
    1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: 1 indicates the head is `not masked`,
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    â€” Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰- é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚'
- en: 'Two formats are allowed:'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…è®¸ä¸¤ç§æ ¼å¼ï¼š
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå³é‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™è¯¥æ¨¡å‹çš„è¾“å…¥ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰-
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨
    `[0, ..., config.num_labels - 1]` ä¸­ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ
    `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: The [LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
