- en: Llama2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/166.f3ec5226.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/PipelineTag.44585822.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned Chat
    Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
    by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
    Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan
    Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David
    Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj
    Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,
    Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit
    Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
    Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog,
    Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
    Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan,
    Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,
    Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
    Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom. It is a collection of
    foundation language models ranging from 7B to 70B parameters, with checkpoints
    finetuned for chat application!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this work, we develop and release Llama 2, a collection of pretrained and
    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
    dialogue use cases. Our models outperform open-source chat models on most benchmarks
    we tested, and based on our human evaluations for helpfulness and safety, may
    be a suitable substitute for closed-source models. We provide a detailed description
    of our approach to fine-tuning and safety improvements of Llama 2-Chat in order
    to enable the community to build on our work and contribute to the responsible
    development of LLMs.*'
  prefs: []
  type: TYPE_NORMAL
- en: Checkout all Llama2 model checkpoints [here](https://huggingface.co/models?search=llama2).
    This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ)
    with contributions from [Lysandre Debut](https://huggingface.co/lysandre). The
    code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox).
    The original code of the authors can be found [here](https://github.com/facebookresearch/llama).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Llama2` models were trained using `bfloat16`, but the original inference
    uses `float16`. The checkpoints uploaded on the Hub use `torch_dtype = 'float16'`,
    which will be used by the `AutoModel` API to cast the checkpoints from `torch.float32`
    to `torch.float16`.
  prefs: []
  type: TYPE_NORMAL
- en: The `dtype` of the online weights is mostly irrelevant unless you are using
    `torch_dtype="auto"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`. The reason is that the model will first be downloaded
    ( using the `dtype` of the checkpoints online), then it will be casted to the
    default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is
    a `torch_dtype` provided in the config, it will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model in `float16` is not recommended and is known to produce `nan`;
    as such, the model should be trained in `bfloat16`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tips:'
  prefs: []
  type: TYPE_NORMAL
- en: Weights for the Llama2 models can be obtained by filling out [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture is very similar to the first Llama, with the addition of Grouped
    Query Attention (GQA) following this [paper](https://arxiv.org/pdf/2305.13245.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting `config.pretraining_tp` to a value different than 1 will activate the
    more accurate but slower computation of the linear layers, which should better
    match the original logits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original model uses `pad_id = -1` which means that there is no padding token.
    We can‚Äôt have the same logic, make sure to add a padding token using `tokenizer.add_special_tokens({"pad_token":"<pad>"})`
    and resize the token embedding accordingly. You should also set the `model.config.pad_token_id`.
    The `embed_tokens` layer of the model is initialized with `self.embed_tokens =
    nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`,
    which makes sure that encoding the padding token will output zeros, so passing
    it when initializing is recommended.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After filling out the form and gaining access to the model checkpoints, you
    should be able to use the already converted checkpoints. Otherwise, if you are
    converting your own model, feel free to use the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py).
    The script can be called with the following (example) command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After conversion, the model and tokenizer can be loaded via:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that executing the script requires enough CPU RAM to host the whole model
    in float16 precision (even if the biggest versions come in several checkpoints
    they each contain a part of each weight of the model, so we need to load them
    all in RAM). For the 75B model, it‚Äôs thus 145GB of RAM needed.
  prefs: []
  type: TYPE_NORMAL
- en: The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece).
    One quirk of sentencepiece is that when decoding a sequence, if the first token
    is the start of the word (e.g. ‚ÄúBanana‚Äù), the tokenizer does not prepend the prefix
    space to the string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Flash Attention 2 via `attn_implementation="flash_attention_2"`,
    don‚Äôt pass `torch_dtype` to the `from_pretrained` class method and use Automatic
    Mixed-Precision training. When using `Trainer`, it is simply specifying either
    `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`.
    This is required because the Flash Attention only support `fp16` and `bf16` data
    type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by üåé) resources to
    help you get started with LLaMA2\. If you‚Äôre interested in submitting a resource
    to be included here, please feel free to open a Pull Request and we‚Äôll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: '[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2),
    a blog post about Llama 2 and how to use it with ü§ó Transformers and ü§ó PEFT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLaMA 2 - Every Resource you need](https://www.philschmid.de/llama-2), a compilation
    of relevant resources to learn about LLaMA 2 and how to get started quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text Generation
  prefs: []
  type: TYPE_NORMAL
- en: A [notebook](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)
    on how to fine-tune Llama 2 in Google Colab using QLoRA and 4-bit precision. üåé
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [notebook](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing)
    on how to fine-tune the ‚ÄúLlama-v2-7b-guanaco‚Äù model with 4-bit QLoRA and generate
    Q&A datasets from PDFs. üåé
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text Classification
  prefs: []
  type: TYPE_NORMAL
- en: A [notebook](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing)
    on how to fine-tune the Llama 2 model with QLoRa, TRL, and Korean text classification
    dataset. üåéüá∞üá∑
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚öóÔ∏è Optimization
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl), a guide
    to using the TRL library‚Äôs DPO method to fine tune Llama 2 on a specific dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2),
    a guide to training Llama 2 to generate instructions from inputs, transforming
    the model from instruction-following to instruction-giving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [notebook](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing)
    on how to fine-tune the Llama 2 model on a personal computer using QLoRa and TRL.
    üåé
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ö°Ô∏è Inference
  prefs: []
  type: TYPE_NORMAL
- en: A [notebook](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing)
    on how to quantize the Llama 2 model using GPTQ from the AutoGPTQ library. üåé
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [notebook](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing)
    on how to run the Llama 2 Chat Model with 4-bit quantization on a local computer
    or Google Colab. üåé
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üöÄ Deploy
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora),
    a complete guide from setup to QLoRA fine-tuning and deployment on Amazon SageMaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deploy Llama 2 7B/13B/70B on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama-llm),
    a guide on using Hugging Face‚Äôs LLM DLC container for secure and scalable deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlamaConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/configuration_llama.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_size = 32000 hidden_size = 4096 intermediate_size = 11008 num_hidden_layers
    = 32 num_attention_heads = 32 num_key_value_heads = None hidden_act = 'silu' max_position_embeddings
    = 2048 initializer_range = 0.02 rms_norm_eps = 1e-06 use_cache = True pad_token_id
    = None bos_token_id = 1 eos_token_id = 2 pretraining_tp = 1 tie_word_embeddings
    = False rope_theta = 10000.0 rope_scaling = None attention_bias = False attention_dropout
    = 0.0 **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 32000) ‚Äî Vocabulary size of
    the LLaMA model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 4096) ‚Äî Dimension of the hidden
    representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intermediate_size** (`int`, *optional*, defaults to 11008) ‚Äî Dimension of
    the MLP representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hidden_layers** (`int`, *optional*, defaults to 32) ‚Äî Number of hidden
    layers in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 32) ‚Äî Number of attention
    heads for each attention layer in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_key_value_heads** (`int`, *optional*) ‚Äî This is the number of key_value
    heads that should be used to implement Grouped Query Attention. If `num_key_value_heads=num_attention_heads`,
    the model will use Multi Head Attention (MHA), if `num_key_value_heads=1 the model
    will use Multi Query Attention (MQA) otherwise GQA is used. When converting a
    multi-head checkpoint to a GQA checkpoint, each group key and value head should
    be constructed by meanpooling all the original heads within that group. For more
    details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is
    not specified, will default to` num_attention_heads`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `function`, *optional*, defaults to `"silu"`) ‚Äî The
    non-linear activation function (function or string) in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_position_embeddings** (`int`, *optional*, defaults to 2048) ‚Äî The maximum
    sequence length that this model might ever be used with. Llama 1 supports up to
    2048 tokens, Llama 2 up to 4096, CodeLlama up to 16384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) ‚Äî The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rms_norm_eps** (`float`, *optional*, defaults to 1e-06) ‚Äî The epsilon used
    by the rms normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not the
    model should return the last key/values attentions (not used by all models). Only
    relevant if `config.is_decoder=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_id** (`int`, *optional*) ‚Äî Padding token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token_id** (`int`, *optional*, defaults to 1) ‚Äî Beginning of stream token
    id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token_id** (`int`, *optional*, defaults to 2) ‚Äî End of stream token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pretraining_tp** (`int`, *optional*, defaults to 1) ‚Äî Experimental feature.
    Tensor parallelism rank used during pretraining. Please refer to [this document](https://huggingface.co/docs/transformers/parallelism)
    to understand more about it. This value is necessary to ensure exact reproducibility
    of the pretraining results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tie_word_embeddings** (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    to tie weight embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rope_theta** (`float`, *optional*, defaults to 10000.0) ‚Äî The base period
    of the RoPE embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rope_scaling** (`Dict`, *optional*) ‚Äî Dictionary containing the scaling configuration
    for the RoPE embeddings. Currently supports two scaling strategies: linear and
    dynamic. Their scaling factor must be a float greater than 1\. The expected format
    is `{"type": strategy name, "factor": scaling factor}`. When using this flag,
    don‚Äôt update `max_position_embeddings` to the expected new maximum. See the following
    thread for more information on how these scaling strategies behave: [https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/).
    This is an experimental feature, subject to breaking API changes in future versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_bias** (`bool`, defaults to `False`, *optional*, defaults to `False`)
    ‚Äî Whether to use a bias in the query, key, value and output projection layers
    during self-attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_dropout** (`float`, *optional*, defaults to 0.0) ‚Äî The dropout
    ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel).
    It is used to instantiate an LLaMA model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the LLaMA-7B.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: LlamaTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlamaTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L66)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vocab_file unk_token = ''<unk>'' bos_token = ''<s>'' eos_token = ''</s>''
    pad_token = None sp_model_kwargs: Optional = None add_bos_token = True add_eos_token
    = False clean_up_tokenization_spaces = False use_default_system_prompt = False
    spaces_between_special_tokens = False legacy = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) ‚Äî Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unk_token** (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<unk>"`)
    ‚Äî The unknown token. A token that is not in the vocabulary cannot be converted
    to an ID and is set to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token** (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<s>"`)
    ‚Äî The beginning of sequence token that was used during pretraining. Can be used
    a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token** (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"</s>"`)
    ‚Äî The end of sequence token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str` or `tokenizers.AddedToken`, *optional*) ‚Äî A special token
    used to make arrays of tokens the same size for batching purpose. Will then be
    ignored by attention mechanisms or loss computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model_kwargs** (`Dict[str, Any]`, `Optional`, *optional*) ‚Äî Will be passed
    to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_bos_token** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not
    to add an `bos_token` at the start of sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_eos_token** (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to add an `eos_token` at the end of sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clean_up_tokenization_spaces** (`bool`, *optional*, defaults to `False`)
    ‚Äî Whether or not to cleanup spaces after decoding, cleanup consists in removing
    potential artifacts like extra spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_default_system_prompt** (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not the default system prompt for Llama should be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**spaces_between_special_tokens** (`bool`, *optional*, defaults to `False`)
    ‚Äî Whether or not to add spaces between special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**legacy** (`bool`, *optional*) ‚Äî Whether or not the `legacy` behavior of the
    tokenizer should be used. Legacy is before the merge of #24622 and #25224 which
    includes fixes to properly handle tokens that appear after special tokens. A simple
    example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`legacy=True`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default
    padding token is unset as there is no padding token in the original model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L333)'
  prefs: []
  type: TYPE_NORMAL
- en: ( token_ids_0 token_ids_1 = None )
  prefs: []
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L344)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    bool = False ) ‚Üí `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) ‚Äî List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) ‚Äî Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) ‚Äî
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L381)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) ‚Üí `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) ‚Äî List of ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) ‚Äî Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Creates a mask from the two sequences passed to be used in a sequence-pair classification
    task. An ALBERT
  prefs: []
  type: TYPE_NORMAL
- en: 'sequence pair mask has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: if token_ids_1 is None, only returns the first portion of the mask (0s).
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L306)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory filename_prefix: Optional = None ) ‚Üí `Tuple(str)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**save_directory** (`str`) ‚Äî The directory in which to save the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Tuple(str)`'
  prefs: []
  type: TYPE_NORMAL
- en: Paths to the files saved.
  prefs: []
  type: TYPE_NORMAL
- en: Save the vocabulary and special tokens file to a directory.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaTokenizerFast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlamaTokenizerFast'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_file = None tokenizer_file = None clean_up_tokenization_spaces = False
    unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' add_bos_token = True
    add_eos_token = False use_default_system_prompt = False **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`, *optional*) ‚Äî [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .model extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer_file** (`str`, *optional*) ‚Äî [tokenizers](https://github.com/huggingface/tokenizers)
    file (generally has a .json extension) that contains everything needed to load
    the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clean_up_tokenization_spaces** (`bool`, *optional*, defaults to `False`)
    ‚Äî Whether or not to cleanup spaces after decoding, cleanup consists in removing
    potential artifacts like extra spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unk_token** (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<unk>"`)
    ‚Äî The unknown token. A token that is not in the vocabulary cannot be converted
    to an ID and is set to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token** (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<s>"`)
    ‚Äî The beginning of sequence token that was used during pretraining. Can be used
    a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eos_token** (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"</s>"`)
    ‚Äî The end of sequence token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_bos_token** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not
    to add an `bos_token` at the start of sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_eos_token** (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to add an `eos_token` at the end of sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_default_system_prompt** (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not the default system prompt for Llama should be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: This uses notably ByteFallback and no normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you want to change the `bos_token` or the `eos_token`, make sure to specify
    them when initializing the model, or call `tokenizer.update_post_processor()`
    to make sure that the post-processing is correctly done (otherwise the values
    of the first token and final token of an encoded sequence will not be correct).
    For more details, checkout [post-processors] ([https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors))
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L272)'
  prefs: []
  type: TYPE_NORMAL
- en: ( token_ids_0 token_ids_1 = None )
  prefs: []
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    bool = False ) ‚Üí A list of integers in the range [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) ‚Äî List of ids of the first sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) ‚Äî List of ids of the second sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) ‚Äî
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list of integers in the range [0, 1]
  prefs: []
  type: TYPE_NORMAL
- en: 1 for a special token, 0 for a sequence token.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieves sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    or `encode_plus` methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) ‚Üí `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) ‚Äî The first tokenized sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) ‚Äî The second tokenized sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: The token type ids.
  prefs: []
  type: TYPE_NORMAL
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  prefs: []
  type: TYPE_NORMAL
- en: '#### update_post_processor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L146)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Updates the underlying post processor with the current `bos_token` and `eos_token`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L190)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: str filename_prefix: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: LlamaModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlamaModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L939)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: LlamaConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights. config ‚Äî LlamaConfig'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare LLaMA Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `LlamaDecoderLayer`
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L974)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: LongTensor = None attention_mask: Optional = None position_ids:
    Optional = None past_key_values: Optional = None inputs_embeds: Optional = None
    use_cache: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    ‚Äî Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_key_values** (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    ‚Äî Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don‚Äôt have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) ‚Äî If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaForCausalLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlamaForCausalLM'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1106)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1136)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: LongTensor = None attention_mask: Optional = None position_ids:
    Optional = None past_key_values: Optional = None inputs_embeds: Optional = None
    labels: Optional = None use_cache: Optional = None output_attentions: Optional
    = None output_hidden_states: Optional = None return_dict: Optional = None ) ‚Üí
    [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    ‚Äî Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_key_values** (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    ‚Äî Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don‚Äôt have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) ‚Äî If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Args ‚Äî labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    ‚Äî Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) ‚Äî Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LlamaForCausalLM](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForCausalLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: LlamaForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LlamaForSequenceClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1295)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLaMa Model transformer with a sequence classification head on top (linear
    layer).
  prefs: []
  type: TYPE_NORMAL
- en: '[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.'
  prefs: []
  type: TYPE_NORMAL
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1326)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: LongTensor = None attention_mask: Optional = None position_ids:
    Optional = None past_key_values: Optional = None inputs_embeds: Optional = None
    labels: Optional = None use_cache: Optional = None output_attentions: Optional
    = None output_hidden_states: Optional = None return_dict: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    ‚Äî Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**past_key_values** (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    ‚Äî Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don‚Äôt have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) ‚Äî If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) ‚Äî Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
