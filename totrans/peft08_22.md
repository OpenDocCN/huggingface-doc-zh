# å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/peft/accelerate/fsdp`](https://huggingface.co/docs/peft/accelerate/fsdp)

[å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ](https://pytorch.org/docs/stable/fsdp.html)ï¼ˆFSDPï¼‰æ˜¯ä¸ºäº†åˆ†å¸ƒå¼è®­ç»ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ€å¤šå¯è¾¾ 1T å‚æ•°ã€‚FSDP é€šè¿‡åœ¨æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¹‹é—´åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿˜å¯ä»¥å°†åˆ†ç‰‡æ¨¡å‹å‚æ•°å¸è½½åˆ° CPUã€‚FSDP æä¾›çš„å†…å­˜æ•ˆç‡ä½¿æ‚¨å¯ä»¥å°†è®­ç»ƒæ‰©å±•åˆ°æ›´å¤§çš„æ‰¹æ¬¡æˆ–æ¨¡å‹å¤§å°ã€‚

ç›®å‰ï¼ŒFSDP å¹¶ä¸ä¼šå‡å°‘ GPU å†…å­˜çš„ä½¿ç”¨é‡ï¼Œè€Œå…·æœ‰ CPU å¸è½½çš„ FSDP åœ¨è®­ç»ƒæœŸé—´å®é™…ä¸Šä¼šæ¶ˆè€— 1.65 å€çš„ GPU å†…å­˜ã€‚æ‚¨å¯ä»¥è·Ÿè¸ªè¿™ä¸ª PyTorch [é—®é¢˜](https://github.com/pytorch/pytorch/issues/91165) è·å–ä»»ä½•æ›´æ–°ã€‚

FSDP åœ¨ğŸ¤— Accelerate ä¸­å—åˆ°æ”¯æŒï¼Œæ‚¨å¯ä»¥ä¸ğŸ¤— PEFT ä¸€èµ·ä½¿ç”¨ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å­¦ä¹ å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ FSDP[è®­ç»ƒè„šæœ¬](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py)ã€‚æ‚¨å°†é…ç½®è„šæœ¬ä»¥è®­ç»ƒä¸€ä¸ªå¤§å‹æ¨¡å‹ç”¨äºæ¡ä»¶ç”Ÿæˆã€‚

## é…ç½®

é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥[åˆ›å»ºä¸€ä¸ª FSDP é…ç½®æ–‡ä»¶](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)ä¸ğŸ¤— Accelerate ä¸€èµ·ã€‚ä½¿ç”¨`--config_file`æ ‡å¿—å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç‰¹å®šä½ç½®ï¼Œå¦åˆ™å®ƒå°†ä¿å­˜ä¸ºğŸ¤— Accelerate ç¼“å­˜ä¸­çš„`default_config.yaml`æ–‡ä»¶ã€‚

é…ç½®æ–‡ä»¶ç”¨äºåœ¨å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶è®¾ç½®é»˜è®¤é€‰é¡¹ã€‚

```py
accelerate config --config_file fsdp_config.yaml
```

æ‚¨å°†è¢«é—®åŠæœ‰å…³æ‚¨çš„è®¾ç½®çš„å‡ ä¸ªé—®é¢˜ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ã€‚åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œè¯·ç¡®ä¿å®Œå…¨åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œåˆ©ç”¨ CPU è¿›è¡Œå¸è½½ï¼Œå¹¶æ ¹æ® Transformer å±‚ç±»ååŒ…è£…æ¨¡å‹å±‚ã€‚

```py
`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD
`Offload Params`: Decides Whether to offload parameters and gradients to CPU
`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP 
`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, 
`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...
`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`
`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH
`State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  
```

ä¾‹å¦‚ï¼Œæ‚¨çš„ FSDP é…ç½®æ–‡ä»¶å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
command_file: null
commands: null
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: FSDP
downcast_bf16: 'no'
dynamo_backend: 'NO'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_offload_params: true
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_transformer_layer_cls_to_wrap: T5Block
gpu_ids: null
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config: {}
mixed_precision: 'no'
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_name: null
tpu_zone: null
use_cpu: false
```

## é‡è¦éƒ¨åˆ†

è®©æˆ‘ä»¬æ·±å…¥äº†è§£è®­ç»ƒè„šæœ¬çš„å·¥ä½œåŸç†ã€‚

[`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14) å‡½æ•°ä»åˆå§‹åŒ–ä¸€ä¸ª[Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ç±»å¼€å§‹ï¼Œè¯¥ç±»å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒçš„æ‰€æœ‰å†…å®¹ï¼Œä¾‹å¦‚è‡ªåŠ¨æ£€æµ‹æ‚¨çš„è®­ç»ƒç¯å¢ƒã€‚

ğŸ’¡ éšæ„æ›´æ”¹`main`å‡½æ•°ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†æ ¼å¼ä¸è„šæœ¬ä¸­çš„ä¸åŒï¼Œæ‚¨å¯èƒ½è¿˜éœ€è¦ç¼–å†™è‡ªå·±çš„é¢„å¤„ç†å‡½æ•°ã€‚

è„šæœ¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªä¸æ‚¨æ­£åœ¨ä½¿ç”¨çš„ğŸ¤— PEFT æ–¹æ³•ç›¸å¯¹åº”çš„é…ç½®ã€‚å¯¹äº LoRAï¼Œæ‚¨å°†ä½¿ç”¨ LoraConfig æ¥æŒ‡å®šä»»åŠ¡ç±»å‹ï¼Œä»¥åŠå…¶ä»–ä¸€äº›é‡è¦å‚æ•°ï¼Œå¦‚ä½ç§©çŸ©é˜µçš„ç»´åº¦ã€çŸ©é˜µç¼©æ”¾å› å­å’Œ LoRA å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒçš„ğŸ¤— PEFT æ–¹æ³•ï¼Œè¯·å°†`LoraConfig`æ›¿æ¢ä¸ºé€‚å½“çš„ç±»ã€‚

æ¥ä¸‹æ¥ï¼Œè„šæœ¬ä½¿ç”¨ get_peft_model()å‡½æ•°å°†åŸºç¡€æ¨¡å‹å’Œ`peft_config`åŒ…è£…èµ·æ¥ï¼Œä»¥åˆ›å»ºä¸€ä¸ª PeftModelã€‚

```py
 def main():
+    accelerator = Accelerator()
     model_name_or_path = "t5-base"
     base_path = "temp/data/FinancialPhraseBank-v1.0"
+    peft_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+   model = get_peft_model(model, peft_config)
```

åœ¨æ•´ä¸ªè„šæœ¬ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)å’Œ[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)å‡½æ•°ï¼Œè¿™äº›å‡½æ•°æœ‰åŠ©äºæ§åˆ¶å’ŒåŒæ­¥è¿›ç¨‹çš„æ‰§è¡Œæ—¶é—´ã€‚

å‡†å¤‡å¥½æ•°æ®é›†ï¼Œå¹¶åŠ è½½æ‰€æœ‰å¿…è¦çš„è®­ç»ƒç»„ä»¶åï¼Œè„šæœ¬ä¼šæ£€æŸ¥æ‚¨æ˜¯å¦åœ¨ä½¿ç”¨`fsdp_plugin`ã€‚PyTorch æä¾›äº†ä¸¤ç§åœ¨ FSDP ä¸­åŒ…è£…æ¨¡å‹å±‚çš„æ–¹æ³•ï¼Œè‡ªåŠ¨æˆ–æ‰‹åŠ¨ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯å…è®¸ FSDP è‡ªåŠ¨é€’å½’åŒ…è£…æ¨¡å‹å±‚ï¼Œè€Œæ— éœ€æ›´æ”¹ä»»ä½•å…¶ä»–ä»£ç ã€‚æ‚¨å¯ä»¥é€‰æ‹©æ ¹æ®å±‚åç§°æˆ–å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰æ¥åŒ…è£…æ¨¡å‹å±‚ã€‚åœ¨ FSDP é…ç½®æ–‡ä»¶ä¸­ï¼Œå®ƒä½¿ç”¨`TRANSFORMER_BASED_WRAP`é€‰é¡¹æ¥åŒ…è£…`T5Block`å±‚ã€‚

```py
if getattr(accelerator.state, "fsdp_plugin", None) is not None:
    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)
```

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ğŸ¤— Accelerate çš„[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)å‡½æ•°æ¥å‡†å¤‡æ¨¡å‹ã€æ•°æ®é›†ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è¿›è¡Œè®­ç»ƒã€‚

```py
model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler
)
```

ä»è¿™é‡Œå¼€å§‹ï¼Œè„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ï¼Œå¹¶å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ° Hubã€‚

## è®­ç»ƒ

è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚ä¹‹å‰ï¼Œæ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸º`fsdp_config.yaml`ï¼Œå› æ­¤æ‚¨éœ€è¦é€šè¿‡`--config_file`å‚æ•°ä¼ é€’è·¯å¾„ç»™å¯åŠ¨å™¨ï¼Œå°±åƒè¿™æ ·ï¼š

```py
accelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py
```

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œè„šæœ¬å°†è¿”å›å‡†ç¡®æ€§å¹¶å°†é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚
