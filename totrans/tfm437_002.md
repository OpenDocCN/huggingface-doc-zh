# 🤗 Transformers

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/index`](https://huggingface.co/docs/transformers/v4.37.2/en/index)

[PyTorch](https://pytorch.org/)、[TensorFlow](https://www.tensorflow.org/)和[JAX](https://jax.readthedocs.io/en/latest/)的最先进机器学习。

🤗 Transformers 提供 API 和工具，可以轻松下载和训练最先进的预训练模型。使用预训练模型可以减少计算成本、碳足迹，并节省训练模型所需的时间和资源。这些模型支持不同模态中的常见任务，例如：

📝 **自然语言处理**：文本分类、命名实体识别、问答、语言建模、摘要、翻译、多项选择和文本生成。

🖼️ **计算机视觉**：图像分类、目标检测和分割。

🗣️ **音频**：自动语音识别和音频分类。

🐙 **多模态**：表格问答、光学字符识别、从扫描文档中提取信息、视频分类和视觉问答。

🤗 Transformers 支持 PyTorch、TensorFlow 和 JAX 之间的框架互操作性。这提供了在模型生命周期的每个阶段使用不同框架的灵活性；在一个框架中用三行代码训练模型，然后在另一个框架中加载进行推断。模型还可以导出到类似 ONNX 和 TorchScript 的格式，以在生产环境中部署。

立即加入[Hub](https://huggingface.co/models)、[论坛](https://discuss.huggingface.co/)或[Discord](https://discord.com/invite/JfAtkvEtRb)上不断增长的社区！

## 如果您正在寻找 Hugging Face 团队的定制支持

![HuggingFace 专家加速计划](https://huggingface.co/support)

## 目录

文档分为五个部分：

+   **开始**提供了一个快速浏览库和安装说明，让您快速上手。

+   **教程**是初学者入门的好地方。本节将帮助您获得开始使用库所需的基本技能。

+   **操作指南**向您展示如何实现特定目标，例如对预训练模型进行微调以进行语言建模，或者如何编写和共享自定义模型。

+   **概念指南**提供了更多关于模型、任务和🤗 Transformers 设计理念背后的概念和思想的讨论和解释。

+   **API**描述了所有类和函数：

    +   **主要类**详细介绍了配置、模型、分词器和管道等最重要的类。

    +   **模型**详细介绍了库中实现的每个模型相关的类和函数。

    +   **内部助手**详细介绍了内部使用的实用类和函数。

## 支持的模型和框架

下表表示库中对这些模型的当前支持，它们是否有 Python 分词器（称为“slow”）。由🤗 Tokenizers 库支持的“fast”分词器，它们是否在 Jax（通过 Flax）、PyTorch 和/或 TensorFlow 中有支持。

| 模型 | PyTorch 支持 | TensorFlow 支持 | Flax 支持 |
| :-: | :-: | :-: | :-: |
| ALBERT | ✅ | ✅ | ✅ |
| ALIGN | ✅ | ❌ | ❌ |
| AltCLIP | ✅ | ❌ | ❌ |
| Audio Spectrogram Transformer | ✅ | ❌ | ❌ |
| Autoformer | ✅ | ❌ | ❌ |
| Bark | ✅ | ❌ | ❌ |
| BART | ✅ | ✅ | ✅ |
| BARThez | ✅ | ✅ | ✅ |
| BARTpho | ✅ | ✅ | ✅ |
| BEiT | ✅ | ❌ | ✅ |
| BERT | ✅ | ✅ | ✅ |
| Bert Generation | ✅ | ❌ | ❌ |
| BertJapanese | ✅ | ✅ | ✅ |
| BERTweet | ✅ | ✅ | ✅ |
| BigBird | ✅ | ❌ | ✅ |
| BigBird-Pegasus | ✅ | ❌ | ❌ |
| BioGpt | ✅ | ❌ | ❌ |
| BiT | ✅ | ❌ | ❌ |
| Blenderbot | ✅ | ✅ | ✅ |
| BlenderbotSmall | ✅ | ✅ | ✅ |
| BLIP | ✅ | ✅ | ❌ |
| BLIP-2 | ✅ | ❌ | ❌ |
| BLOOM | ✅ | ❌ | ✅ |
| BORT | ✅ | ✅ | ✅ |
| BridgeTower | ✅ | ❌ | ❌ |
| BROS | ✅ | ❌ | ❌ |
| ByT5 | ✅ | ✅ | ✅ |
| CamemBERT | ✅ | ✅ | ❌ |
| CANINE | ✅ | ❌ | ❌ |
| Chinese-CLIP | ✅ | ❌ | ❌ |
| CLAP | ✅ | ❌ | ❌ |
| CLIP | ✅ | ✅ | ✅ |
| CLIPSeg | ✅ | ❌ | ❌ |
| CLVP | ✅ | ❌ | ❌ |
| CodeGen | ✅ | ❌ | ❌ |
| CodeLlama | ✅ | ❌ | ✅ |
| Conditional DETR | ✅ | ❌ | ❌ |
| ConvBERT | ✅ | ✅ | ❌ |
| ConvNeXT | ✅ | ✅ | ❌ |
| ConvNeXTV2 | ✅ | ✅ | ❌ |
| CPM | ✅ | ✅ | ✅ |
| CPM-Ant | ✅ | ❌ | ❌ |
| CTRL | ✅ | ✅ | ❌ |
| CvT | ✅ | ✅ | ❌ |
| Data2VecAudio | ✅ | ❌ | ❌ |
| Data2VecText | ✅ | ❌ | ❌ |
| Data2VecVision | ✅ | ✅ | ❌ |
| DeBERTa | ✅ | ✅ | ❌ |
| DeBERTa-v2 | ✅ | ✅ | ❌ |
| Decision Transformer | ✅ | ❌ | ❌ |
| Deformable DETR | ✅ | ❌ | ❌ |
| DeiT | ✅ | ✅ | ❌ |
| DePlot | ✅ | ❌ | ❌ |
| DETA | ✅ | ❌ | ❌ |
| DETR | ✅ | ❌ | ❌ |
| DialoGPT | ✅ | ✅ | ✅ |
| DiNAT | ✅ | ❌ | ❌ |
| DINOv2 | ✅ | ❌ | ❌ |
| DistilBERT | ✅ | ✅ | ✅ |
| DiT | ✅ | ❌ | ✅ |
| DonutSwin | ✅ | ❌ | ❌ |
| DPR | ✅ | ✅ | ❌ |
| DPT | ✅ | ❌ | ❌ |
| EfficientFormer | ✅ | ✅ | ❌ |
| EfficientNet | ✅ | ❌ | ❌ |
| ELECTRA | ✅ | ✅ | ✅ |
| EnCodec | ✅ | ❌ | ❌ |
| Encoder decoder | ✅ | ✅ | ✅ |
| ERNIE | ✅ | ❌ | ❌ |
| ErnieM | ✅ | ❌ | ❌ |
| ESM | ✅ | ✅ | ❌ |
| FairSeq Machine-Translation | ✅ | ❌ | ❌ |
| Falcon | ✅ | ❌ | ❌ |
| FastSpeech2Conformer | ✅ | ❌ | ❌ |
| FLAN-T5 | ✅ | ✅ | ✅ |
| FLAN-UL2 | ✅ | ✅ | ✅ |
| FlauBERT | ✅ | ✅ | ❌ |
| FLAVA | ✅ | ❌ | ❌ |
| FNet | ✅ | ❌ | ❌ |
| FocalNet | ✅ | ❌ | ❌ |
| Funnel Transformer | ✅ | ✅ | ❌ |
| Fuyu | ✅ | ❌ | ❌ |
| GIT | ✅ | ❌ | ❌ |
| GLPN | ✅ | ❌ | ❌ |
| GPT Neo | ✅ | ❌ | ✅ |
| GPT NeoX | ✅ | ❌ | ❌ |
| GPT NeoX Japanese | ✅ | ❌ | ❌ |
| GPT-J | ✅ | ✅ | ✅ |
| GPT-Sw3 | ✅ | ✅ | ✅ |
| GPTBigCode | ✅ | ❌ | ❌ |
| GPTSAN-japanese | ✅ | ❌ | ❌ |
| Graphormer | ✅ | ❌ | ❌ |
| GroupViT | ✅ | ✅ | ❌ |
| HerBERT | ✅ | ✅ | ✅ |
| Hubert | ✅ | ✅ | ❌ |
| I-BERT | ✅ | ❌ | ❌ |
| IDEFICS | ✅ | ❌ | ❌ |
| ImageGPT | ✅ | ❌ | ❌ |
| Informer | ✅ | ❌ | ❌ |
| InstructBLIP | ✅ | ❌ | ❌ |
| Jukebox | ✅ | ❌ | ❌ |
| KOSMOS-2 | ✅ | ❌ | ❌ |
| LayoutLM | ✅ | ✅ | ❌ |
| LayoutLMv2 | ✅ | ❌ | ❌ |
| LayoutLMv3 | ✅ | ✅ | ❌ |
| LayoutXLM | ✅ | ❌ | ❌ |
| LED | ✅ | ✅ | ❌ |
| LeViT | ✅ | ❌ | ❌ |
| LiLT | ✅ | ❌ | ❌ |
| LLaMA | ✅ | ❌ | ✅ |
| Llama2 | ✅ | ❌ | ✅ |
| LLaVa | ✅ | ❌ | ❌ |
| Longformer | ✅ | ✅ | ❌ |
| LongT5 | ✅ | ❌ | ✅ |
| LUKE | ✅ | ❌ | ❌ |
| LXMERT | ✅ | ✅ | ❌ |
| M-CTC-T | ✅ | ❌ | ❌ |
| M2M100 | ✅ | ❌ | ❌ |
| MADLAD-400 | ✅ | ✅ | ✅ |
| Marian | ✅ | ✅ | ✅ |
| MarkupLM | ✅ | ❌ | ❌ |
| Mask2Former | ✅ | ❌ | ❌ |
| MaskFormer | ✅ | ❌ | ❌ |
| MatCha | ✅ | ❌ | ❌ |
| mBART | ✅ | ✅ | ✅ |
| mBART-50 | ✅ | ✅ | ✅ |
| MEGA | ✅ | ❌ | ❌ |
| Megatron-BERT | ✅ | ❌ | ❌ |
| Megatron-GPT2 | ✅ | ✅ | ✅ |
| MGP-STR | ✅ | ❌ | ❌ |
| Mistral | ✅ | ❌ | ❌ |
| Mixtral | ✅ | ❌ | ❌ |
| mLUKE | ✅ | ❌ | ❌ |
| MMS | ✅ | ✅ | ✅ |
| MobileBERT | ✅ | ✅ | ❌ |
| MobileNetV1 | ✅ | ❌ | ❌ |
| MobileNetV2 | ✅ | ❌ | ❌ |
| MobileViT | ✅ | ✅ | ❌ |
| MobileViTV2 | ✅ | ❌ | ❌ |
| MPNet | ✅ | ✅ | ❌ |
| MPT | ✅ | ❌ | ❌ |
| MRA | ✅ | ❌ | ❌ |
| MT5 | ✅ | ✅ | ✅ |
| MusicGen | ✅ | ❌ | ❌ |
| MVP | ✅ | ❌ | ❌ |
| NAT | ✅ | ❌ | ❌ |
| Nezha | ✅ | ❌ | ❌ |
| NLLB | ✅ | ❌ | ❌ |
| NLLB-MOE | ✅ | ❌ | ❌ |
| Nougat | ✅ | ✅ | ✅ |
| Nyströmformer | ✅ | ❌ | ❌ |
| OneFormer | ✅ | ❌ | ❌ |
| OpenAI GPT | ✅ | ✅ | ❌ |
| OpenAI GPT-2 | ✅ | ✅ | ✅ |
| OpenLlama | ✅ | ❌ | ❌ |
| OPT | ✅ | ✅ | ✅ |
| OWL-ViT | ✅ | ❌ | ❌ |
| OWLv2 | ✅ | ❌ | ❌ |
| PatchTSMixer | ✅ | ❌ | ❌ |
| PatchTST | ✅ | ❌ | ❌ |
| Pegasus | ✅ | ✅ | ✅ |
| PEGASUS-X | ✅ | ❌ | ❌ |
| Perceiver | ✅ | ❌ | ❌ |
| Persimmon | ✅ | ❌ | ❌ |
| Phi | ✅ | ❌ | ❌ |
| PhoBERT | ✅ | ✅ | ✅ |
| Pix2Struct | ✅ | ❌ | ❌ |
| PLBart | ✅ | ❌ | ❌ |
| PoolFormer | ✅ | ❌ | ❌ |
| Pop2Piano | ✅ | ❌ | ❌ |
| ProphetNet | ✅ | ❌ | ❌ |
| PVT | ✅ | ❌ | ❌ |
| QDQBert | ✅ | ❌ | ❌ |
| Qwen2 | ✅ | ❌ | ❌ |
| RAG | ✅ | ✅ | ❌ |
| REALM | ✅ | ❌ | ❌ |
| Reformer | ✅ | ❌ | ❌ |
| RegNet | ✅ | ✅ | ✅ |
| RemBERT | ✅ | ✅ | ❌ |
| ResNet | ✅ | ✅ | ✅ |
| RetriBERT | ✅ | ❌ | ❌ |
| RoBERTa | ✅ | ✅ | ✅ |
| RoBERTa-PreLayerNorm | ✅ | ✅ | ✅ |
| RoCBert | ✅ | ❌ | ❌ |
| RoFormer | ✅ | ✅ | ✅ |
| RWKV | ✅ | ❌ | ❌ |
| SAM | ✅ | ✅ | ❌ |
| SeamlessM4T | ✅ | ❌ | ❌ |
| SeamlessM4Tv2 | ✅ | ❌ | ❌ |
| SegFormer | ✅ | ✅ | ❌ |
| SEW | ✅ | ❌ | ❌ |
| SEW-D | ✅ | ❌ | ❌ |
| SigLIP | ✅ | ❌ | ❌ |
| Speech Encoder decoder | ✅ | ❌ | ✅ |
| Speech2Text | ✅ | ✅ | ❌ |
| SpeechT5 | ✅ | ❌ | ❌ |
| Splinter | ✅ | ❌ | ❌ |
| SqueezeBERT | ✅ | ❌ | ❌ |
| SwiftFormer | ✅ | ❌ | ❌ |
| Swin Transformer | ✅ | ✅ | ❌ |
| Swin Transformer V2 | ✅ | ❌ | ❌ |
| Swin2SR | ✅ | ❌ | ❌ |
| SwitchTransformers | ✅ | ❌ | ❌ |
| T5 | ✅ | ✅ | ✅ |
| T5v1.1 | ✅ | ✅ | ✅ |
| Table Transformer | ✅ | ❌ | ❌ |
| TAPAS | ✅ | ✅ | ❌ |
| TAPEX | ✅ | ✅ | ✅ |
| Time Series Transformer | ✅ | ❌ | ❌ |
| TimeSformer | ✅ | ❌ | ❌ |
| Trajectory Transformer | ✅ | ❌ | ❌ |
| Transformer-XL | ✅ | ✅ | ❌ |
| TrOCR | ✅ | ❌ | ❌ |
| TVLT | ✅ | ❌ | ❌ |
| TVP | ✅ | ❌ | ❌ |
| UL2 | ✅ | ✅ | ✅ |
| UMT5 | ✅ | ❌ | ❌ |
| UniSpeech | ✅ | ❌ | ❌ |
| UniSpeechSat | ✅ | ❌ | ❌ |
| UnivNet | ✅ | ❌ | ❌ |
| UPerNet | ✅ | ❌ | ❌ |
| VAN | ✅ | ❌ | ❌ |
| VideoMAE | ✅ | ❌ | ❌ |
| ViLT | ✅ | ❌ | ❌ |
| VipLlava | ✅ | ❌ | ❌ |
| Vision Encoder decoder | ✅ | ✅ | ✅ |
| VisionTextDualEncoder | ✅ | ✅ | ✅ |
| VisualBERT | ✅ | ❌ | ❌ |
| ViT | ✅ | ✅ | ✅ |
| ViT Hybrid | ✅ | ❌ | ❌ |
| VitDet | ✅ | ❌ | ❌ |
| ViTMAE | ✅ | ✅ | ❌ |
| ViTMatte | ✅ | ❌ | ❌ |
| ViTMSN | ✅ | ❌ | ❌ |
| VITS | ✅ | ❌ | ❌ |
| ViViT | ✅ | ❌ | ❌ |
| Wav2Vec2 | ✅ | ✅ | ✅ |
| Wav2Vec2-BERT | ✅ | ❌ | ❌ |
| Wav2Vec2-Conformer | ✅ | ❌ | ❌ |
| Wav2Vec2Phoneme | ✅ | ✅ | ✅ |
| WavLM | ✅ | ❌ | ❌ |
| Whisper | ✅ | ✅ | ✅ |
| X-CLIP | ✅ | ❌ | ❌ |
| X-MOD | ✅ | ❌ | ❌ |
| XGLM | ✅ | ✅ | ✅ |
| XLM | ✅ | ✅ | ❌ |
| XLM-ProphetNet | ✅ | ❌ | ❌ |
| XLM-RoBERTa | ✅ | ✅ | ✅ |
| XLM-RoBERTa-XL | ✅ | ❌ | ❌ |
| XLM-V | ✅ | ✅ | ✅ |
| XLNet | ✅ | ✅ | ❌ |
| XLS-R | ✅ | ✅ | ✅ |
| XLSR-Wav2Vec2 | ✅ | ✅ | ✅ |
| YOLOS | ✅ | ❌ | ❌ |
| YOSO | ✅ | ❌ | ❌ |
