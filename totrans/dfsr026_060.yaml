- en: Kandinsky 2.2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/kandinsky](https://huggingface.co/docs/diffusers/training/kandinsky)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This script is experimental, and it‚Äôs easy to overfit and run into issues like
    catastrophic forgetting. Try exploring different hyperparameters to get the best
    results on your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Kandinsky 2.2 is a multilingual text-to-image model capable of producing more
    photorealistic images. The model includes an image prior model for creating image
    embeddings from text prompts, and a decoder model that generates images based
    on the prior model‚Äôs embeddings. That‚Äôs why you‚Äôll find two separate scripts in
    Diffusers for Kandinsky 2.2, one for training the prior model and one for training
    the decoder model. You can train both models separately, but to get the best results,
    you should train both the prior and decoder models.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your GPU, you may need to enable `gradient_checkpointing` (‚ö†Ô∏è not
    supported for the prior model!), `mixed_precision`, and `gradient_accumulation_steps`
    to help fit the model into memory and to speedup training. You can reduce your
    memory-usage even more by enabling memory-efficient attention with [xFormers](../optimization/xformers)
    (version [v0.0.16](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212)
    fails for training on some GPUs so you may need to install a development version
    instead).
  prefs: []
  type: TYPE_NORMAL
- en: This guide explores the [train_text_to_image_prior.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py)
    and the [train_text_to_image_decoder.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py)
    scripts to help you become more familiar with it, and how you can adapt it for
    your own use-case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the scripts, make sure you install the library from source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script you‚Äôre using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ü§ó Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. It‚Äôll automatically configure your training setup based on your
    hardware and environment. Take a look at the ü§ó Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an ü§ó Accelerate environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To setup a default ü§ó Accelerate environment without choosing any configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if your environment doesn‚Äôt support an interactive shell, like a notebook,
    you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections highlight parts of the training scripts that are important
    for understanding how to modify it, but it doesn‚Äôt cover every aspect of the scripts
    in detail. If you‚Äôre interested in learning more, feel free to read through the
    scripts and let us know if you have any questions or concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Script parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training scripts provides many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L190)
    function. The training scripts provides default values for each parameter, such
    as the training batch size and learning rate, but you can also set your own values
    in the training command if you‚Äôd like.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to speedup training with mixed precision using the fp16 format,
    add the `--mixed_precision` parameter to the training command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Most of the parameters are identical to the parameters in the [Text-to-image](text2image#script-parameters)
    training guide, so let‚Äôs get straight to a walkthrough of the Kandinsky training
    scripts!
  prefs: []
  type: TYPE_NORMAL
- en: Min-SNR weighting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can
    help with training by rebalancing the loss to achieve faster convergence. The
    training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR
    is compatible with both prediction types. This weighting strategy is only supported
    by PyTorch and is unavailable in the Flax training script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `--snr_gamma` parameter and set it to the recommended value of 5.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Training script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training script is also similar to the [Text-to-image](text2image#training-script)
    training guide, but it‚Äôs been modified to support training the prior and decoder
    models. This guide focuses on the code that is unique to the Kandinsky 2.2 training
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: prior modeldecoder model
  prefs: []
  type: TYPE_NORMAL
- en: The [`main()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L441)
    function contains the code for preparing the dataset and training the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main differences you‚Äôll notice right away is that the training script
    also loads a [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    - in addition to a scheduler and tokenizer - for preprocessing images and a [CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)
    model for encoding the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Kandinsky uses a [PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)
    to generate the image embeddings, so you‚Äôll want to setup the optimizer to learn
    the prior mode‚Äôs parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the input captions are tokenized, and images are [preprocessed](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L632)
    by the [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the [training loop](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L718)
    converts the input images into latents, adds noise to the image embeddings, and
    makes a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you want to learn more about how the training loop works, check out the [Understanding
    pipelines, models and schedulers](../using-diffusers/write_own_pipeline) tutorial
    which breaks down the basic pattern of the denoising process.
  prefs: []
  type: TYPE_NORMAL
- en: Launch the script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you‚Äôve made all your changes or you‚Äôre okay with the default configuration,
    you‚Äôre ready to launch the training script! üöÄ
  prefs: []
  type: TYPE_NORMAL
- en: You‚Äôll train on the [Pok√©mon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    dataset to generate your own Pok√©mon, but you can also create and train on your
    own dataset by following the [Create a dataset for training](create_dataset) guide.
    Set the environment variable `DATASET_NAME` to the name of the dataset on the
    Hub or if you‚Äôre training on your own files, set the environment variable `TRAIN_DIR`
    to a path to your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre training on more than one GPU, add the `--multi_gpu` parameter to
    the `accelerate launch` command.
  prefs: []
  type: TYPE_NORMAL
- en: To monitor training progress with Weights & Biases, add the `--report_to=wandb`
    parameter to the training command. You‚Äôll also need to add the `--validation_prompt`
    to the training command to keep track of results. This can be really useful for
    debugging the model and viewing intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: prior modeldecoder model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once training is finished, you can use your newly trained model for inference!
  prefs: []
  type: TYPE_NORMAL
- en: prior modeldecoder model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to replace `kandinsky-community/kandinsky-2-2-decoder` with your own
    trained decoder checkpoint!
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Congratulations on training a Kandinsky 2.2 model! To learn more about how
    to use your new model, the following guides may be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the [Kandinsky](../using-diffusers/kandinsky) guide to learn how to use
    it for a variety of different tasks (text-to-image, image-to-image, inpainting,
    interpolation), and how it can be combined with a ControlNet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the [DreamBooth](dreambooth) and [LoRA](lora) training guides to learn
    how to train a personalized Kandinsky model with just a few example images. These
    two training techniques can even be combined!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
