["```py\n- from transformers import AutoModelForSequenceClassification\n+ from optimum.intel.openvino import OVModelForSequenceClassification\n  from transformers import AutoTokenizer, pipeline\n\n  # Download a tokenizer and model from the Hub and convert to OpenVINO format\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n- model = AutoModelForSequenceClassification.from_pretrained(model_id)\n+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n\n  # Run inference!\n  classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n  results = classifier(\"He's a dreadful magician.\")\n```", "```py\n>>> from optimum.onnxruntime import ORTModelForSequenceClassification\n>>> from transformers import AutoTokenizer\n\n>>> model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n>>> save_directory = \"tmp/onnx/\"\n\n>>> # Load a model from transformers and export it to ONNX\n>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n\n>>> # Save the ONNX model and tokenizer\n>>> ort_model.save_pretrained(save_directory)\n>>> tokenizer.save_pretrained(save_directory)\n```", "```py\n>>> from optimum.onnxruntime.configuration import AutoQuantizationConfig\n>>> from optimum.onnxruntime import ORTQuantizer\n\n>>> # Define the quantization methodology\n>>> qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n>>> quantizer = ORTQuantizer.from_pretrained(ort_model)\n\n>>> # Apply dynamic quantization on the model\n>>> quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)\n```", "```py\n>>> from optimum.onnxruntime import ORTModelForSequenceClassification\n>>> from transformers import pipeline, AutoTokenizer\n\n>>> model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"model_quantized.onnx\")\n>>> tokenizer = AutoTokenizer.from_pretrained(save_directory)\n>>> classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n>>> results = classifier(\"I love burritos!\")\n```", "```py\n- from transformers import Trainer, TrainingArguments\n+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n\n  # Download a pretrained model from the Hub\n  model = AutoModelForXxx.from_pretrained(\"bert-base-uncased\")\n\n  # Define the training arguments\n- training_args = TrainingArguments(\n+ training_args = GaudiTrainingArguments(\n      output_dir=\"path/to/save/folder/\",\n+     use_habana=True,\n+     use_lazy_mode=True,\n+     gaudi_config_name=\"Habana/bert-base-uncased\",\n      ...\n  )\n\n  # Initialize the trainer\n- trainer = Trainer(\n+ trainer = GaudiTrainer(\n      model=model,\n      args=training_args,\n      train_dataset=train_dataset,\n      ...\n  )\n\n  # Use Habana Gaudi processor for training!\n  trainer.train()\n```", "```py\n- from transformers import Trainer, TrainingArguments\n+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n\n  # Download a pretrained model from the Hub\n  model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n\n  # Define the training arguments\n- training_args = TrainingArguments(\n+ training_args = ORTTrainingArguments(\n      output_dir=\"path/to/save/folder/\",\n      optim=\"adamw_ort_fused\",\n      ...\n  )\n\n  # Create a ONNX Runtime Trainer\n- trainer = Trainer(\n+ trainer = ORTTrainer(\n      model=model,\n      args=training_args,\n      train_dataset=train_dataset,\n+     feature=\"text-classification\", # The model type to export to ONNX\n      ...\n  )\n\n  # Use ONNX Runtime for training!\n  trainer.train()\n```", "```py\noptimum-cli export onnx --model gpt2 gpt2_onnx/\n```", "```py\noptimum-cli export onnx --help\n```", "```py\n>>> from optimum.bettertransformer import BetterTransformer\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n>>> model = BetterTransformer.transform(model)\n```"]