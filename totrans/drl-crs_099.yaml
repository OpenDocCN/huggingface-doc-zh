- en: Visualize the Clipped Surrogate Objective Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit8/visualize](https://huggingface.co/learn/deep-rl-course/unit8/visualize)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/88.278ee553.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry. **It’s normal if this seems complex to handle right now**. But
    we’re going to see what this Clipped Surrogate Objective Function looks like,
    and this will help you to visualize better what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: '![PPO](../Images/ee86cb6353014f333922753ceeb367f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Table from "Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: We have six different situations. Remember first that we take the minimum between
    the clipped and unclipped objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1 and 2: the ratio is between the range'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In situations 1 and 2, **the clipping does not apply since the ratio is between
    the range**<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">]</mo></mrow>
    <annotation encoding="application/x-tex">[1 - \epsilon, 1 + \epsilon]</annotation></semantics></math>
    [1−ϵ,1+ϵ]
  prefs: []
  type: TYPE_NORMAL
- en: 'In situation 1, we have a positive advantage: the **action is better than the
    average** of all the actions in that state. Therefore, we should encourage our
    current policy to increase the probability of taking that action in that state.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the ratio is between intervals, **we can increase our policy’s probability
    of taking that action at that state.**
  prefs: []
  type: TYPE_NORMAL
- en: 'In situation 2, we have a negative advantage: the action is worse than the
    average of all actions at that state. Therefore, we should discourage our current
    policy from taking that action in that state.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the ratio is between intervals, **we can decrease the probability that
    our policy takes that action at that state.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 3 and 4: the ratio is below the range'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![PPO](../Images/ee86cb6353014f333922753ceeb367f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Table from "Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: If the probability ratio is lower than<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 - \epsilon]</annotation></semantics></math>
    [1−ϵ], the probability of taking that action at that state is much lower than
    with the old policy.
  prefs: []
  type: TYPE_NORMAL
- en: If, like in situation 3, the advantage estimate is positive (A>0), then **you
    want to increase the probability of taking that action at that state.**
  prefs: []
  type: TYPE_NORMAL
- en: But if, like situation 4, the advantage estimate is negative, **we don’t want
    to decrease further** the probability of taking that action at that state. Therefore,
    the gradient is = 0 (since we’re on a flat line), so we don’t update our weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 5 and 6: the ratio is above the range'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![PPO](../Images/ee86cb6353014f333922753ceeb367f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Table from "Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: If the probability ratio is higher than<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 + \epsilon]</annotation></semantics></math>
    [1+ϵ], the probability of taking that action at that state in the current policy
    is **much higher than in the former policy.**
  prefs: []
  type: TYPE_NORMAL
- en: If, like in situation 5, the advantage is positive, **we don’t want to get too
    greedy**. We already have a higher probability of taking that action at that state
    than the former policy. Therefore, the gradient is = 0 (since we’re on a flat
    line), so we don’t update our weights.
  prefs: []
  type: TYPE_NORMAL
- en: If, like in situation 6, the advantage is negative, we want to decrease the
    probability of taking that action at that state.
  prefs: []
  type: TYPE_NORMAL
- en: So if we recap, **we only update the policy with the unclipped objective part**.
    When the minimum is the clipped objective part, we don’t update our policy weights
    since the gradient will equal 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we update our policy only if:'
  prefs: []
  type: TYPE_NORMAL
- en: Our ratio is in the range<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">]</mo></mrow>
    <annotation encoding="application/x-tex">[1 - \epsilon, 1 + \epsilon]</annotation></semantics></math>
    [1−ϵ,1+ϵ]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our ratio is outside the range, but **the advantage leads to getting closer
    to the range**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being below the ratio but the advantage is > 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Being above the ratio but the advantage is < 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You might wonder why, when the minimum is the clipped ratio, the gradient
    is 0.** When the ratio is clipped, the derivative in this case will not be the
    derivative of the<math><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>∗</mo><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">r_t(\theta) * A_t</annotation></semantics></math>
    rt​(θ)∗At​ but the derivative of either<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    stretchy="false">)</mo><mo>∗</mo><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">(1 - \epsilon)* A_t</annotation></semantics></math>(1−ϵ)∗At​
    or the derivative of<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo
    stretchy="false">)</mo><mo>∗</mo><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">(1 + \epsilon)* A_t</annotation></semantics></math>(1+ϵ)∗At​
    which both = 0.'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, thanks to this clipped surrogate objective, **we restrict the
    range that the current policy can vary from the old one.** Because we remove the
    incentive for the probability ratio to move outside of the interval since the
    clip forces the gradient to be zero. If the ratio is ><math><semantics><mrow><mn>1</mn><mo>+</mo><mi>ϵ</mi></mrow>
    <annotation encoding="application/x-tex">1 + \epsilon</annotation></semantics></math>
    1+ϵ or <<math><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow> <annotation
    encoding="application/x-tex">1 - \epsilon</annotation></semantics></math> 1−ϵ
    the gradient will be equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final Clipped Surrogate Objective Loss for PPO Actor-Critic style looks
    like this, it’s a combination of Clipped Surrogate Objective function, Value Loss
    Function and Entropy bonus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PPO objective](../Images/51cc7c8dc865f57c8a43b64c2df40281.png)'
  prefs: []
  type: TYPE_IMG
- en: That was quite complex. Take time to understand these situations by looking
    at the table and the graph. **You must understand why this makes sense.** If you
    want to go deeper, the best resource is the article [“Towards Delivering a Coherent
    Self-Contained Explanation of Proximal Policy Optimization” by Daniel Bick, especially
    part 3.4](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf).
  prefs: []
  type: TYPE_NORMAL
