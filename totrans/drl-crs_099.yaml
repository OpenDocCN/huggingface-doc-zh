- en: Visualize the Clipped Surrogate Objective Function
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/visualize](https://huggingface.co/learn/deep-rl-course/unit8/visualize)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry. **It’s normal if this seems complex to handle right now**. But
    we’re going to see what this Clipped Surrogate Objective Function looks like,
    and this will help you to visualize better what’s going on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![PPO](../Images/ee86cb6353014f333922753ceeb367f0.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: '[Table from "Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We have six different situations. Remember first that we take the minimum between
    the clipped and unclipped objectives.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1 and 2: the ratio is between the range'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In situations 1 and 2, **the clipping does not apply since the ratio is between
    the range**<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">]</mo></mrow>
    <annotation encoding="application/x-tex">[1 - \epsilon, 1 + \epsilon]</annotation></semantics></math>
    [1−ϵ,1+ϵ]
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'In situation 1, we have a positive advantage: the **action is better than the
    average** of all the actions in that state. Therefore, we should encourage our
    current policy to increase the probability of taking that action in that state.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Since the ratio is between intervals, **we can increase our policy’s probability
    of taking that action at that state.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In situation 2, we have a negative advantage: the action is worse than the
    average of all actions at that state. Therefore, we should discourage our current
    policy from taking that action in that state.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Since the ratio is between intervals, **we can decrease the probability that
    our policy takes that action at that state.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 3 and 4: the ratio is below the range'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![PPO](../Images/ee86cb6353014f333922753ceeb367f0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: '[Table from "Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: If the probability ratio is lower than<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 - \epsilon]</annotation></semantics></math>
    [1−ϵ], the probability of taking that action at that state is much lower than
    with the old policy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: If, like in situation 3, the advantage estimate is positive (A>0), then **you
    want to increase the probability of taking that action at that state.**
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: But if, like situation 4, the advantage estimate is negative, **we don’t want
    to decrease further** the probability of taking that action at that state. Therefore,
    the gradient is = 0 (since we’re on a flat line), so we don’t update our weights.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 5 and 6: the ratio is above the range'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![PPO](../Images/ee86cb6353014f333922753ceeb367f0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: '[Table from "Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: If the probability ratio is higher than<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 + \epsilon]</annotation></semantics></math>
    [1+ϵ], the probability of taking that action at that state in the current policy
    is **much higher than in the former policy.**
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: If, like in situation 5, the advantage is positive, **we don’t want to get too
    greedy**. We already have a higher probability of taking that action at that state
    than the former policy. Therefore, the gradient is = 0 (since we’re on a flat
    line), so we don’t update our weights.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: If, like in situation 6, the advantage is negative, we want to decrease the
    probability of taking that action at that state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: So if we recap, **we only update the policy with the unclipped objective part**.
    When the minimum is the clipped objective part, we don’t update our policy weights
    since the gradient will equal 0.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，**我们只更新未剪切的目标部分的策略**。当最小值是剪切的目标部分时，我们不更新我们的策略权重，因为梯度将等于0。
- en: 'So we update our policy only if:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只在以下情况下更新我们的策略：
- en: Our ratio is in the range<math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">]</mo></mrow>
    <annotation encoding="application/x-tex">[1 - \epsilon, 1 + \epsilon]</annotation></semantics></math>
    [1−ϵ,1+ϵ]
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的比率在范围内[1−ϵ,1+ϵ]
- en: Our ratio is outside the range, but **the advantage leads to getting closer
    to the range**
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的比率在范围之外，但**优势导致更接近范围**
- en: Being below the ratio but the advantage is > 0
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在比率之下但优势>0
- en: Being above the ratio but the advantage is < 0
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在比率之上但优势<0
- en: '**You might wonder why, when the minimum is the clipped ratio, the gradient
    is 0.** When the ratio is clipped, the derivative in this case will not be the
    derivative of the<math><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>∗</mo><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">r_t(\theta) * A_t</annotation></semantics></math>
    rt​(θ)∗At​ but the derivative of either<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo
    stretchy="false">)</mo><mo>∗</mo><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">(1 - \epsilon)* A_t</annotation></semantics></math>(1−ϵ)∗At​
    or the derivative of<math><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo
    stretchy="false">)</mo><mo>∗</mo><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">(1 + \epsilon)* A_t</annotation></semantics></math>(1+ϵ)∗At​
    which both = 0.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**你可能会想知道为什么，当最小值是剪切比率时，梯度为0。**当比率被剪切时，在这种情况下的导数将不是rt(θ)∗At，而是(1−ϵ)∗At或(1+ϵ)∗At，两者都等于0。'
- en: To summarize, thanks to this clipped surrogate objective, **we restrict the
    range that the current policy can vary from the old one.** Because we remove the
    incentive for the probability ratio to move outside of the interval since the
    clip forces the gradient to be zero. If the ratio is ><math><semantics><mrow><mn>1</mn><mo>+</mo><mi>ϵ</mi></mrow>
    <annotation encoding="application/x-tex">1 + \epsilon</annotation></semantics></math>
    1+ϵ or <<math><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow> <annotation
    encoding="application/x-tex">1 - \epsilon</annotation></semantics></math> 1−ϵ
    the gradient will be equal to 0.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，由于这个剪切的替代目标，**我们限制了当前策略可以从旧策略变化的范围。**因为我们消除了概率比率移出区间的激励，剪切强制梯度为零。如果比率是1+ϵ或1−ϵ，梯度将等于0。
- en: 'The final Clipped Surrogate Objective Loss for PPO Actor-Critic style looks
    like this, it’s a combination of Clipped Surrogate Objective function, Value Loss
    Function and Entropy bonus:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PPO Actor-Critic风格的最终Clipped Surrogate Objective Loss看起来像这样，它是Clipped Surrogate
    Objective函数、Value Loss函数和熵奖励的组合：
- en: '![PPO objective](../Images/51cc7c8dc865f57c8a43b64c2df40281.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![PPO目标](../Images/51cc7c8dc865f57c8a43b64c2df40281.png)'
- en: That was quite complex. Take time to understand these situations by looking
    at the table and the graph. **You must understand why this makes sense.** If you
    want to go deeper, the best resource is the article [“Towards Delivering a Coherent
    Self-Contained Explanation of Proximal Policy Optimization” by Daniel Bick, especially
    part 3.4](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当复杂。花时间通过查看表格和图形来理解这些情况。**你必须理解为什么这是有道理的。**如果你想深入了解，最好的资源是Daniel Bick的文章[“Towards
    Delivering a Coherent Self-Contained Explanation of Proximal Policy Optimization”，特别是第3.4部分](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)。
