# GPT-J

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptj`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptj)

## 概述

GPT-J 模型是由 Ben Wang 和 Aran Komatsuzaki 在[kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)存储库中发布的。它是在[Pile](https://pile.eleuther.ai/)数据集上训练的类似 GPT-2 的因果语言模型。

此模型由[Stella Biderman](https://huggingface.co/stellaathena)贡献。

## 使用提示

+   要在 float32 中加载[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)，至少需要 2 倍模型大小的 RAM：1 倍用于初始权重，另外 1 倍用于加载检查点。因此，对于 GPT-J，至少需要 48GB RAM 才能加载模型。为了减少 RAM 使用量，有几个选项。`torch_dtype`参数可用于在仅 CUDA 设备上以半精度初始化模型。还有一个存储 fp16 权重的 fp16 分支，可用于进一步最小化 RAM 使用量：

```py
>>> from transformers import GPTJForCausalLM
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained(
...     "EleutherAI/gpt-j-6B",
...     revision="float16",
...     torch_dtype=torch.float16,
... ).to(device)
```

+   该模型应适合在 16GB GPU 上进行推断。对于训练/微调，将需要更多的 GPU RAM。例如，Adam 优化器会复制模型的四份副本：模型、梯度、平均梯度和梯度的平方平均值。因此，即使使用混合精度，梯度更新也是在 fp32 中，至少需要 4 倍模型大小的 GPU 内存。这还不包括激活和数据批次，这将再次需要更多的 GPU RAM。因此，应该探索解决方案，如 DeepSpeed，来训练/微调模型。另一个选项是使用原始代码库在 TPU 上训练/微调模型，然后将模型转换为 Transformers 格式进行推断。有关说明，请参阅[此处](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)。

+   尽管嵌入矩阵的大小为 50400，但 GPT-2 标记器仅使用 50257 个条目。这些额外的代币是为了提高 TPU 的效率而添加的。为了避免嵌入矩阵大小和 vocab 大小之间的不匹配，[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)的标记化器包含 143 个额外的令牌`<|extratoken_1|>, ..., <|extratoken_143|>`，因此`tokenizer`的`vocab_size`也变为 50400。

## 使用示例

generate() 方法可用于使用 GPT-J 模型生成文本。

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

...或者使用 float16 精度：

```py
>>> from transformers import GPTJForCausalLM, AutoTokenizer
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B", torch_dtype=torch.float16).to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

## 资源

一份官方 Hugging Face 和社区资源（由🌎表示），帮助您开始使用 GPT-J。如果您有兴趣提交资源以包含在此处，请随时提出拉取请求，我们将进行审查！资源应该最好展示一些新内容，而不是重复现有资源。

文本生成

+   [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)的描述。

+   一篇关于如何[使用 Hugging Face Transformers 和 Amazon SageMaker 部署 GPT-J 6B 进行推断](https://huggingface.co/blog/gptj-sagemaker)的博客。

+   一篇关于如何[使用 DeepSpeed-Inference 在 GPU 上加速 GPT-J 推断](https://www.philschmid.de/gptj-deepspeed-inference)的博客。

+   一篇介绍[GPT-J-6B：6B 基于 JAX 的 Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)的博客。🌎

+   一个用于[GPT-J-6B 推断演示](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb)的笔记本。🌎

+   另一个演示笔记本：[使用 GPT-J-6B 进行推断](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb)。

+   🤗 Hugging Face 课程的[因果语言建模](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch)章节。

+   GPTJForCausalLM 可通过这个[因果语言建模示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling)、[文本生成示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)进行支持。

+   TFGPTJForCausalLM 由这个[因果语言建模示例脚本](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)支持。

+   FlaxGPTJForCausalLM 由这个[因果语言建模示例脚本](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb)支持。

**文档资源**

+   文本分类任务指南

+   问答任务指南

+   因果语言建模任务指南

## GPTJConfig

### `class transformers.GPTJConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/configuration_gptj.py#L33)

```py
( vocab_size = 50400 n_positions = 2048 n_embd = 4096 n_layer = 28 n_head = 16 rotary_dim = 64 n_inner = None activation_function = 'gelu_new' resid_pdrop = 0.0 embd_pdrop = 0.0 attn_pdrop = 0.0 layer_norm_epsilon = 1e-05 initializer_range = 0.02 use_cache = True bos_token_id = 50256 eos_token_id = 50256 tie_word_embeddings = False **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 50400) — GPT-J 模型的词汇量。定义在调用 GPTJModel 时可以表示的不同标记数量。

+   `n_positions` (`int`, *optional*, 默认为 2048) — 该模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `n_embd` (`int`, *optional*, 默认为 4096) — 嵌入和隐藏状态的维度。

+   `n_layer` (`int`, *optional*, 默认为 28) — Transformer 编码器中的隐藏层数量。

+   `n_head` (`int`, *optional*, 默认为 16) — Transformer 编码器中每个注意力层的注意力头数量。

+   `rotary_dim` (`int`, *optional*, 默认为 64) — 旋转位置嵌入应用的嵌入维度数量。

+   `n_inner` (`int`, *optional*, 默认为 None) — 内部前馈层的维度。`None`将设置为 4 倍的 n_embd。

+   `activation_function` (`str`, *optional*, 默认为`"gelu_new"`) — 激活函数，可在列表`["relu", "silu", "gelu", "tanh", "gelu_new"]`中选择。

+   `resid_pdrop` (`float`, *optional*, 默认为 0.1) — 嵌入、编码器和池化器中所有全连接层的 dropout 概率。

+   `embd_pdrop` (`int`, *optional*, 默认为 0.1) — 嵌入的 dropout 比率。

+   `attn_pdrop` (`float`, *optional*, 默认为 0.1) — 注意力的 dropout 比率。

+   `layer_norm_epsilon` (`float`, *optional*, 默认为 1e-5) — 在层归一化层中使用的 epsilon。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `use_cache` (`bool`, *optional*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

这是一个配置类，用于存储 GPTJModel 的配置。根据指定的参数实例化一个 GPT-J 模型，定义模型架构。使用默认值实例化配置将产生类似于 GPT-J [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)架构的配置。配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import GPTJModel, GPTJConfig

>>> # Initializing a GPT-J 6B configuration
>>> configuration = GPTJConfig()

>>> # Initializing a model from the configuration
>>> model = GPTJModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

PytorchHide Pytorch content

## GPTJModel

### `class transformers.GPTJModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L476)

```py
( config )
```

参数

+   `config` (GPTJConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸 GPT-J 模型变压器，输出原始隐藏状态，没有特定的头部。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L547)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1 表示`未被掩盖`的标记，

    +   0 表示`被掩盖`的标记。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_attention_heads,)`或`(n_layer, num_attention_heads)`，*optional*) — 用于使自注意力模块的特定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1 表示头部`未被掩盖`，

    +   0 表示头部`被掩盖`。 

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_dim)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPast 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPast 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（GPTJConfig）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层输出的隐藏状态序列。

    如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有 2 个额外形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及如果`config.is_encoder_decoder=True`在交叉注意力块中）可用于加速顺序解码的（见`past_key_values`输入）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

GPTJModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默忽略它们。

此示例使用一个随机模型，真实模型都非常庞大。为了获得正确的结果，应该使用 EleutherAI/gpt-j-6B 而不是 hf-internal-testing/tiny-random-gptj。如果加载该检查点时出现内存不足，可以尝试在`from_pretrained`调用中添加`device_map="auto"`。

示例：

```py
>>> from transformers import AutoTokenizer, GPTJModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/tiny-random-gptj")
>>> model = GPTJModel.from_pretrained("hf-internal-testing/tiny-random-gptj")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## GPTJForCausalLM

### `class transformers.GPTJForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L720)

```py
( config )
```

参数

+   `config`（GPTJConfig）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

在顶部具有语言建模头的 GPT-J 模型变换器。

该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L823)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的蒙版。蒙版值选择在`[0, 1]`中：

    +   1 表示“未屏蔽”的标记，

    +   0 表示“已屏蔽”的标记。

    什么是注意力蒙版？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0 对应于*句子 A*标记。

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记的位置索引。在范围`[0, config.n_positions - 1]`中选择。

    什么是位置 ID？

+   `head_mask`（形状为`(num_attention_heads,)`或`(n_layer, num_attention_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部无效的蒙版。蒙版值选择在`[0, 1]`中：

    +   1 表示头部“未屏蔽”，

    +   0 表示头部“已屏蔽”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_dim)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将*input_ids*索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于语言建模的标签。请注意，标签**在模型内部被移位**，即您可以设置`labels = input_ids`。索引选择在`[-100, 0, ..., config.vocab_size]`中，所有设置为`-100`的标签都将被忽略（屏蔽），损失仅计算标签在`[0, ..., config.vocab_size]`中的标签

返回

transformers.modeling_outputs.CausalLMOutputWithPast 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.CausalLMOutputWithPast 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（GPTJConfig）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出 + 每个层的输出）的形状为`(batch_size, sequence_length, hidden_size)`。

    模型在每个层的输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    在注意力 SoftMax 之后的注意力权重，用于计算自注意力头中的加权平均值。

GPTJForCausalLM 的前向方法重写了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例而不是此函数，因为前者会处理运行前后处理步骤，而后者会默默忽略它们。

此示例使用随机模型，真实模型都非常庞大。为了获得正确的结果，应该使用 EleutherAI/gpt-j-6B 而不是 hf-internal-testing/tiny-random-gptj。如果加载该检查点时出现内存不足，可以尝试在`from_pretrained`调用中添加`device_map="auto"`。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, GPTJForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/tiny-random-gptj")
>>> model = GPTJForCausalLM.from_pretrained("hf-internal-testing/tiny-random-gptj")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs, labels=inputs["input_ids"])
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## GPTJForSequenceClassification

### `class transformers.GPTJForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L918)

```py
( config )
```

参数

+   `config` (GPTJConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

带有顶部序列分类头（线性层）的 GPT-J 模型变压器。

GPTJForSequenceClassification 使用最后一个标记进行分类，就像其他因果模型（例如 GPT、GPT-2、GPT-Neo）一样。

由于它对最后一个标记进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，则会找到每行中不是填充标记的最后一个标记。如果未定义`pad_token_id`，则会简单地取每行批次中的最后一个值。当传递`inputs_embeds`而不是`input_ids`时，无法猜测填充标记，因此会执行相同操作（取每行批次中的最后一个值）。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L947)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`范围内：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 段标记索引，指示输入的第一部分和第二部分。索引选在`[0, 1]`范围内：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`内选择。

    什么是位置 ID？

+   `head_mask`（形状为`(num_attention_heads,)`或`(n_layer, num_attention_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`范围内：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_dim)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将*input_ids*索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）- 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（GPTJConfig）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 分类（或回归，如果`config.num_labels==1`）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（或回归，如果`config.num_labels==1`）分数（SoftMax 之前）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量）

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（用于嵌入的输出，如果模型有一个嵌入层，则为一个 + 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

GPTJForSequenceClassification 的前向方法覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。

此示例使用随机模型，真实模型都非常庞大。为了获得正确的结果，您应该使用 EleutherAI/gpt-j-6B 而不是 ydshieh/tiny-random-gptj-for-sequence-classification。如果加载该检查点时出现内存不足，可以尝试在`from_pretrained`调用中添加`device_map="auto"`。

单标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, GPTJForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("ydshieh/tiny-random-gptj-for-sequence-classification")
>>> model = GPTJForSequenceClassification.from_pretrained("ydshieh/tiny-random-gptj-for-sequence-classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = GPTJForSequenceClassification.from_pretrained("ydshieh/tiny-random-gptj-for-sequence-classification", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
```

多标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, GPTJForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("ydshieh/tiny-random-gptj-for-sequence-classification")
>>> model = GPTJForSequenceClassification.from_pretrained("ydshieh/tiny-random-gptj-for-sequence-classification", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = GPTJForSequenceClassification.from_pretrained(
...     "ydshieh/tiny-random-gptj-for-sequence-classification", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

## GPTJForQuestionAnswering

### `class transformers.GPTJForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L1053)

```py
( config )
```

参数

+   `config`（GPTJConfig）— 具有模型所有参数的模型配置类。使用配置文件进行初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

GPT-J 模型变压器，顶部带有用于提取问答任务的跨度分类头，如 SQuAD（在隐藏状态输出的线性层上计算`跨度起始 logits`和`跨度结束 logits`）。

该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_gptj.py#L1074)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`范围内：

    +   1 用于`未被掩码`的标记，

    +   0 用于`masked`的标记。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选定在`[0, 1]`范围内：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`内选定。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_attention_heads,)`或`(n_layer, num_attention_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_dim)`，*optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制权来将*input_ids*索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `start_positions` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算标记分类损失的标记跨度开始位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。序列外的位置不会计入损失的计算。

+   `end_positions` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算标记分类损失的标记跨度结束位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。序列外的位置不会计入损失的计算。

返回

transformers.modeling_outputs.QuestionAnsweringModelOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.QuestionAnsweringModelOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（GPTJConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 总跨度提取损失是起始和结束位置的交叉熵之和。

+   `start_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 跨度开始分数（SoftMax 之前）。

+   `end_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 跨度结束分数（SoftMax 之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出一个，+每个层的输出一个）。

    模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

GPTJForQuestionAnswering 前向方法，覆盖`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默忽略它们。

此示例使用随机模型，因为真实模型都非常庞大。为了获得正确的结果，您应该使用 EleutherAI/gpt-j-6B 而不是 hf-internal-testing/tiny-random-gptj。如果加载该检查点时出现内存不足，可以尝试在`from_pretrained`调用中添加`device_map="auto"`。

示例：

```py
>>> from transformers import AutoTokenizer, GPTJForQuestionAnswering
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/tiny-random-gptj")
>>> model = GPTJForQuestionAnswering.from_pretrained("hf-internal-testing/tiny-random-gptj")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]

>>> # target is "nice puppet"
>>> target_start_index = torch.tensor([14])
>>> target_end_index = torch.tensor([15])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = outputs.loss
```

TensorFlow 隐藏 TensorFlow 内容

## TFGPTJModel

### `class transformers.TFGPTJModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L682)

```py
( config *inputs **kwargs )
```

参数

+   `config`（GPTJConfig） - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 GPT-J 模型变压器输出原始隐藏状态，没有特定的头部。

此模型继承自 TFPreTrainedModel。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取有关一般用法和行为的所有相关信息。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，当将输入传递给模型和层时，Keras 方法更喜欢这种格式。由于这种支持，当使用诸如`model.fit()`之类的方法时，应该会“正常工作” - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在 Keras 方法之外使用第二种格式，比如在使用 Keras `Functional` API 创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个包含与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L691)

```py
( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPast or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`Numpy 数组`或`tf.Tensor`）— `input_ids_length` = 如果`past`为`None`，则为`sequence_length`，否则为`past[0].shape[-2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了`past`，则只应将未计算其过去的输入 ID 作为`input_ids`传递。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `past_key_values`（长度为`config.n_layers`的`List[tf.Tensor]`）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值），如下面的`past`输出所示。可用于加速顺序解码。已将其过去给予该模型的令牌 ID 不应作为输入 ID 传递，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy 数组`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选定的掩码值在`[0, 1]`中：

    +   对于“未被掩盖”的标记，

    +   0 对应于被“掩盖”的标记。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy 数组`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy 数组`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`Numpy 数组`或`tf.Tensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。选定的掩码值在`[0, 1]`中：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。此参数仅在急切模式下使用，在图模式中将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式中将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

+   `use_cache`（`bool`，*可选*，默认为`True`）— 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past`）。在训练期间设置为`False`，在生成期间设置为`True`。

返回

transformers.modeling_tf_outputs.TFBaseModelOutputWithPast 或`tuple(tf.Tensor)`

transformers.modeling_tf_outputs.TFBaseModelOutputWithPast 或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则返回）包含根据配置（GPTJConfig）和输入的各种元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）—模型最后一层的隐藏状态序列。

    如果使用`past_key_values`，则仅输出形状为`(batch_size, 1, hidden_size)`序列的最后一个隐藏状态。

+   `past_key_values`（`List[tf.Tensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`）。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

TFGPTJModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFGPTJModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
>>> model = TFGPTJModel.from_pretrained("EleutherAI/gpt-j-6B")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFGPTJForCausalLM

### `class transformers.TFGPTJForCausalLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L745)

```py
( config *inputs **kwargs )
```

参数

+   `config`（GPTJConfig）—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法加载模型权重。

GPT-J 模型变压器，顶部带有语言建模头。

此模型继承自 TFPreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取与一般用法和行为相关的所有事项。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

第二种格式得到支持的原因是，当将输入传递给模型和层时，Keras 方法更喜欢这种格式。由于这种支持，在使用诸如`model.fit()`之类的方法时，对您来说应该“只需工作” - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在 Keras 方法之外使用第二种格式，比如在使用 Keras`Functional` API 创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个包含与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他 Python 函数一样传递输入！

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L791)

```py
( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutputWithPast or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`Numpy array`或`tf.Tensor`）- 如果`past`为`None`，则`input_ids_length` = `sequence_length`，否则`input_ids_length` = `past[0].shape[-2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了`past`，则只应将尚未计算其过去的输入 ID 作为`input_ids`传递。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `past_key_values`（长度为`config.n_layers`的`List[tf.Tensor]`）- 包含由模型计算的预先计算的隐藏状态（注意力块中的键和值）（请参见下面的`past`输出）。可用于加速顺序解码。已将其过去给定给此模型的标记 ID 不应作为输入 ID 传递，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*）- 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是 token type IDs？

+   `position_ids` (`tf.Tensor`或`Numpy 数组`的形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`Numpy 数组`或`tf.Tensor`的形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部无效的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示头部未被`掩码`，

    +   0 表示头部被`掩码`。

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（某些模块，如 dropout 模块，在训练和评估之间具有不同的行为）。

+   `labels` (`np.ndarray`或`tf.Tensor`的形状为`(batch_size, sequence_length)`，*optional*) — 用于语言建模的标签。请注意，模型内部的标签**已经被移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都将被忽略（掩码），损失仅计算标签在`[0, ..., config.vocab_size]`中的情况。

返回

transformers.modeling_tf_outputs.TFCausalLMOutputWithPast 或`tuple(tf.Tensor)`

一个 transformers.modeling_tf_outputs.TFCausalLMOutputWithPast 或一个`tf.Tensor`的元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（GPTJConfig）和输入的各种元素。

+   `loss` (`tf.Tensor` of shape `(n,)`, *optional*, 其中 n 是非掩码标签的数量，在提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`tf.Tensor`的形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头部的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`List[tf.Tensor]`, *optional*, 当传递`use_cache=True`或当`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`）。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

+   `hidden_states` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    模型在每一层的输出处的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力权重在注意力 softmax 之后。

TFGPTJForCausalLM 的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个函数，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFGPTJForCausalLM
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
>>> model = TFGPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)
>>> logits = outputs.logits
```

## TFGPTJForSequenceClassification

### `class transformers.TFGPTJForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L869)

```py
( config *inputs **kwargs )
```

参数

+   `config`（GPTJConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

GPT-J 模型变压器顶部带有序列分类头（线性层）。

GPTJForSequenceClassification 使用最后一个标记来进行分类，就像其他因果模型（例如 GPT、GPT-2、GPT-Neo）一样。

由于它在最后一个标记上进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，它会找到每行中不是填充标记的最后一个标记。如果没有定义`pad_token_id`，它会简单地取每行批次中的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，它会执行相同的操作（取每行批次中的最后一个值）。

此模型继承自 TFPreTrainedModel。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档，了解与一般用法和行为相关的所有事项。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，当将输入传递给模型和层时，Keras 方法更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在 Keras 方法之外使用第二种格式，比如在使用 Keras`Functional` API 创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：

+   只有`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个包含与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些内容，因为您可以像对待其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L899)

```py
( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutputWithPast or tuple(tf.Tensor)
```

参数

+   `input_ids` (`Numpy array` 或 `tf.Tensor` of shape `(batch_size, input_ids_length)`) — `input_ids_length` = `sequence_length`，如果`past`为`None`，则为`past[0].shape[-2]`（输入过去键值状态的序列长度）。词汇表中输入序列令牌的索引。

    如果使用了`past`，则只有那些尚未计算其过去的输入 ID 应作为`input_ids`传递。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `past_key_values` (`List[tf.Tensor]` of length `config.n_layers`) — 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参见下面的`past`输出）。可用于加速顺序解码。已经计算过其过去的令牌 ID 不应作为输入 ID 传递给此模型。

+   `attention_mask` (`tf.Tensor` 或 `Numpy array` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充令牌索引上执行注意力的掩码。掩码值选定在`[0, 1]`之间：

    +   1 表示未被屏蔽的令牌，

    +   0 表示被屏蔽的令牌。

    什么是注意力掩码？

+   `token_type_ids` (`tf.Tensor` 或 `Numpy array` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选定在`[0, 1]`之间：

    +   0 对应于*句子 A*令牌，

    +   1 对应于*句子 B*令牌。

    什么是令牌类型 ID？

+   `position_ids` (`tf.Tensor` 或 `Numpy array` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列令牌在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`Numpy array` 或 `tf.Tensor` of shape `(num_heads,)` 或 `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1 表示头部未被屏蔽，

    +   0 表示头部被屏蔽。

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *optional*, 默认为 `False`) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

+   `labels` (`np.ndarray` 或 `tf.Tensor`，形状为 `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

transformers.modeling_tf_outputs.TFSequenceClassifierOutputWithPast 或 `tuple(tf.Tensor)`

一个 transformers.modeling_tf_outputs.TFSequenceClassifierOutputWithPast 或一个 `tf.Tensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含各种元素，取决于配置（GPTJConfig）和输入。

+   `loss` (`形状为 `(batch_size, )` 的 `tf.Tensor`, *optional*, 当提供 `labels` 时返回) — 分类（如果 `config.num_labels==1` 则为回归）损失。

+   `logits` (`形状为 `(batch_size, config.num_labels)` 的 `tf.Tensor`) — 分类（如果 `config.num_labels==1` 则为回归）得分（SoftMax 之前）。

+   `past_key_values` (`List[tf.Tensor]`, *optional*, 当传递 `use_cache=True` 或 `config.use_cache=True` 时返回) — 长度为 `config.n_layers` 的 `tf.Tensor` 列表，每个张量的形状为 `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出，一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *optional*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

TFGPTJForSequenceClassification 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFGPTJForSequenceClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
>>> model = TFGPTJForSequenceClassification.from_pretrained("EleutherAI/gpt-j-6B")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

>>> logits = model(**inputs).logits

>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
```

```py
>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = TFGPTJForSequenceClassification.from_pretrained("EleutherAI/gpt-j-6B", num_labels=num_labels)

>>> labels = tf.constant(1)
>>> loss = model(**inputs, labels=labels).loss
```

## TFGPTJForQuestionAnswering

### `class transformers.TFGPTJForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L1003)

```py
( config *inputs **kwargs )
```

参数

+   `config`（GPTJConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

GPT-J 模型变压器在顶部具有用于提取式问答任务（例如 SQuAD）的跨度分类头（在隐藏状态输出顶部的线性层，用于计算`跨度起始对数`和`跨度结束对数`）。

此模型继承自 TFPreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取有关一般用法和行为的所有相关信息。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是 Keras 方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在 Keras 方法之外使用第二种格式，例如在使用 Keras`Functional` API 创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：

+   仅具有`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待任何其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_tf_gptj.py#L1022)

```py
( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`Numpy`数组或`tf.Tensor`）- 如果`past`为`None`，则`input_ids_length`=`sequence_length`，否则`input_ids_length`=`past[0].shape[-2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past`，则只应传递尚未计算其过去的输入 ID 作为`input_ids`。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `past_key_values`（长度为`config.n_layers`的`List[tf.Tensor]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值），如下面的`past`输出所示。可用于加速顺序解码。将过去给定给该模型的标记 ID 不应作为输入 ID 传递，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示`未被掩盖`的标记，

    +   0 表示`被掩盖`的标记。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）- 指示输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids`（`tf.Tensor`或形状为`(batch_size, sequence_length)`的`Numpy`数组，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`Numpy`数组或`tf.Tensor`，*可选*）- 用于使自注意力模块中的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部`未被掩盖`，

    +   0 表示头部`被掩盖`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为关联向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为`True`。

+   `training`（`bool`，*可选*，默认为`False`）- 是否在训练模式下使用模型（某些模块如 dropout 模块在训练和评估之间具有不同的行为）。

+   `start_positions`（形状为`(batch_size,)`的`np.ndarray`或`tf.Tensor`，*可选*）- 用于计算标记分类损失的标记跨度开始位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。

+   `end_positions`（形状为`(batch_size,)`的`np.ndarray`或`tf.Tensor`，*可选*）- 用于计算标记跨度结束位置（索引）的标签的位置。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。

返回

transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput 或`tuple(tf.Tensor)`

一个 transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput 或一个 `tf.Tensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（GPTJConfig）和输入的不同元素。

+   `loss` (`tf.Tensor` of shape `(batch_size, )`, *可选*, 当提供 `start_positions` 和 `end_positions` 时返回) — 总跨度提取损失是起始位置和结束位置的交叉熵之和。

+   `start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-start scores (before SoftMax).

+   `end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-end scores (before SoftMax).

+   `hidden_states` (`tuple(tf.Tensor)`, *可选*, 当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层的输出处的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选*, 当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    在注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

TFGPTJForQuestionAnswering 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在之后调用 `Module` 实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFGPTJForQuestionAnswering
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
>>> model = TFGPTJForQuestionAnswering.from_pretrained("EleutherAI/gpt-j-6B")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="tf")
>>> outputs = model(**inputs)

>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
```

```py
>>> # target is "nice puppet"
>>> target_start_index = tf.constant([14])
>>> target_end_index = tf.constant([15])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = tf.math.reduce_mean(outputs.loss)
```

JAXHide JAX content

## FlaxGPTJModel

### `class transformers.FlaxGPTJModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_flax_gptj.py#L613)

```py
( config: GPTJConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（GPTJConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained() 方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *可选*, 默认为 `jax.numpy.float32`) — 计算的数据类型。可以是 `jax.numpy.float32`，`jax.numpy.float16`（在 GPU 上），以及 `jax.numpy.bfloat16`（在 TPU 上）。

    这可以用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定了 `dtype`，则所有计算将使用给定的 `dtype` 执行。

    `请注意，这仅指定了计算的数据类型，不影响模型参数的数据类型。`

    如果要更改模型参数的数据类型，请参阅 to_fp16() 和 to_bf16()。

裸的 GPTJ 模型变压器输出原始隐藏状态，没有特定的头部。

此模型继承自 FlaxPreTrainedModel。查看超类文档以了解库实现的通用方法，例如下载或保存模型，调整输入嵌入大小，修剪头等。

该模型还是一个 Flax 亚麻[flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规的 Flax 模块，并参考 Flax 文档以获取有关一般用法和行为的所有相关信息。

最后，该模型支持内在的 JAX 特性，例如：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_flax_gptj.py#L435)

```py
( input_ids attention_mask = None position_ids = None params: dict = None past_key_values: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`numpy.ndarray`，形状为`(batch_size, input_ids_length)`) — `input_ids_length` = `sequence_length`。输入序列标记在词汇表中的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`numpy.ndarray`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 代表未被掩盖的标记，

    +   0 代表被掩盖的标记。

    什么是注意力掩码？

+   `position_ids` (`numpy.ndarray`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

+   `past_key_values` (`Dict[str, np.ndarray]`，*optional*，由`init_cache`返回或传递先前的`past_key_values`时返回) — 预先计算的隐藏状态字典（注意力块中的键和值），可用于快速自回归解码。预先计算的键和值隐藏状态的形状为*[batch_size, max_length]*。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

返回

transformers.modeling_flax_outputs.FlaxMaskedLMOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_flax_outputs.FlaxMaskedLMOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（GPTJConfig）和输入的各种元素。

+   `logits` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(jnp.ndarray)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选的*, 当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每层一个）。

    自注意力头中用于计算加权平均值的注意力权重在经过注意力 softmax 后。

`FlaxGPTJPreTrainedModel` 的前向方法，覆盖了 `__call__` 特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxGPTJModel

>>> tokenizer = AutoTokenizer.from_pretrained("gptj")
>>> model = FlaxGPTJModel.from_pretrained("gptj")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## FlaxGPTJForCausalLM

### `class transformers.FlaxGPTJForCausalLM`

[源代码](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_flax_gptj.py#L677)

```py
( config: GPTJConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（GPTJConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *可选的*, 默认为 `jax.numpy.float32`) — 计算的数据类型。可以是 `jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和 `jax.numpy.bfloat16`（在 TPU 上）之一。

    这可用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定了，所有计算将使用给定的 `dtype` 执行。

    `请注意，这仅指定了计算的数据类型，不影响模型参数的数据类型。`

    如果您希望更改模型参数的数据类型，请参阅 to_fp16()和 to_bf16()。

带有语言建模头的 GPTJ 模型变压器。

此模型继承自 FlaxPreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) 子类。将其用作常规的 Flax Module，并参考 Flax 文档以获取与一般用法和行为相关的所有信息。

最后，此模型支持内置的 JAX 功能，例如：

+   [即时编译（JIT）](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[源代码](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptj/modeling_flax_gptj.py#L435)

```py
( input_ids attention_mask = None position_ids = None params: dict = None past_key_values: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`numpy.ndarray`，形状为 `(batch_size, input_ids_length)`） — `input_ids_length` = `sequence_length`。词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示未被“掩盖”的标记，

    +   0 表示被“掩盖”的标记。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

+   `past_key_values`（`Dict[str, np.ndarray]`，*可选*，由`init_cache`返回或传递先前的`past_key_values`时返回）— 预先计算的隐藏状态字典（在注意力块中的键和值）可用于快速自回归解码。预先计算的键和值隐藏状态的形状为*[batch_size, max_length]*。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_flax_outputs.FlaxMaskedLMOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_flax_outputs.FlaxMaskedLMOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包括根据配置（GPTJConfig）和输入的不同元素。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`jnp.ndarray`）— 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    每层模型的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力 softmax 之后的注意力权重。

`FlaxGPTJPreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxGPTJForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("gptj")
>>> model = FlaxGPTJForCausalLM.from_pretrained("gptj")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
>>> outputs = model(**inputs)

>>> # retrieve logts for next token
>>> next_token_logits = outputs.logits[:, -1]
```
