["```py\naccelerate config\n```", "```py\nimport os\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()  # Write a config file\nos._exit(00)  # Restart the notebook\n```", "```py\nimport os, re, torch, PIL\nimport numpy as np\n\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import Compose, RandomResizedCrop, Resize, ToTensor\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom timm import create_model\n```", "```py\nimport os\n\ndata_dir = \"../../images\"\nfnames = os.listdir(data_dir)\nfname = fnames[0]\nprint(fname)\n```", "```py\nbeagle_32.jpg\n```", "```py\nimport re\n\ndef extract_label(fname):\n    stem = fname.split(os.path.sep)[-1]\n    return re.search(r\"^(.*)_\\d+\\.jpg$\", stem).groups()[0]\n```", "```py\nextract_label(fname)\n```", "```py\n\"beagle\"\n```", "```py\nclass PetsDataset(Dataset):\n    def __init__(self, file_names, image_transform=None, label_to_id=None):\n        self.file_names = file_names\n        self.image_transform = image_transform\n        self.label_to_id = label_to_id\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, idx):\n        fname = self.file_names[idx]\n        raw_image = PIL.Image.open(fname)\n        image = raw_image.convert(\"RGB\")\n        if self.image_transform is not None:\n            image = self.image_transform(image)\n        label = extract_label(fname)\n        if self.label_to_id is not None:\n            label = self.label_to_id[label]\n        return {\"image\": image, \"label\": label}\n```", "```py\nfnames = [os.path.join(\"../../images\", fname) for fname in fnames if fname.endswith(\".jpg\")]\n```", "```py\nall_labels = [extract_label(fname) for fname in fnames]\nid_to_label = list(set(all_labels))\nid_to_label.sort()\nlabel_to_id = {lbl: i for i, lbl in enumerate(id_to_label)}\n```", "```py\ndef get_dataloaders(batch_size: int = 64):\n    \"Builds a set of dataloaders with a batch_size\"\n    random_perm = np.random.permutation(len(fnames))\n    cut = int(0.8 * len(fnames))\n    train_split = random_perm[:cut]\n    eval_split = random_perm[cut:]\n\n    # For training a simple RandomResizedCrop will be used\n    train_tfm = Compose([RandomResizedCrop((224, 224), scale=(0.5, 1.0)), ToTensor()])\n    train_dataset = PetsDataset([fnames[i] for i in train_split], image_transform=train_tfm, label_to_id=label_to_id)\n\n    # For evaluation a deterministic Resize will be used\n    eval_tfm = Compose([Resize((224, 224)), ToTensor()])\n    eval_dataset = PetsDataset([fnames[i] for i in eval_split], image_transform=eval_tfm, label_to_id=label_to_id)\n\n    # Instantiate dataloaders\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=4)\n    eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=batch_size * 2, num_workers=4)\n    return train_dataloader, eval_dataloader\n```", "```py\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n```", "```py\ndef training_loop(mixed_precision=\"fp16\", seed: int = 42, batch_size: int = 64):\n    set_seed(seed)\n    accelerator = Accelerator(mixed_precision=mixed_precision)\n```", "```py\n    train_dataloader, eval_dataloader = get_dataloaders(batch_size)\n    model = create_model(\"resnet50d\", pretrained=True, num_classes=len(label_to_id))\n```", "```py\n    for param in model.parameters():\n        param.requires_grad = False\n    for param in model.get_classifier().parameters():\n        param.requires_grad = True\n```", "```py\n    mean = torch.tensor(model.default_cfg[\"mean\"])[None, :, None, None]\n    std = torch.tensor(model.default_cfg[\"std\"])[None, :, None, None]\n```", "```py\n    mean = mean.to(accelerator.device)\n    std = std.to(accelerator.device)\n```", "```py\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)\n    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))\n```", "```py\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n```", "```py\n    for epoch in range(5):\n        model.train()\n        for batch in train_dataloader:\n            inputs = (batch[\"image\"] - mean) / std\n            outputs = model(inputs)\n            loss = torch.nn.functional.cross_entropy(outputs, batch[\"label\"])\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n```", "```py\n        model.eval()\n        accurate = 0\n        num_elems = 0\n```", "```py\n        for batch in eval_dataloader:\n            inputs = (batch[\"image\"] - mean) / std\n            with torch.no_grad():\n                outputs = model(inputs)\n            predictions = outputs.argmax(dim=-1)\n```", "```py\n            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[\"label\"])\n            num_elems += accurate_preds.shape[0]\n            accurate += accurate_preds.long().sum()\n```", "```py\n        eval_metric = accurate.item() / num_elems\n        accelerator.print(f\"epoch {epoch}: {100 * eval_metric:.2f}\")\n```", "```py\ndef training_loop(mixed_precision=\"fp16\", seed: int = 42, batch_size: int = 64):\n    set_seed(seed)\n    # Initialize accelerator\n    accelerator = Accelerator(mixed_precision=mixed_precision)\n    # Build dataloaders\n    train_dataloader, eval_dataloader = get_dataloaders(batch_size)\n\n    # Instantiate the model (you build the model here so that the seed also controls new weight initaliziations)\n    model = create_model(\"resnet50d\", pretrained=True, num_classes=len(label_to_id))\n\n    # Freeze the base model\n    for param in model.parameters():\n        param.requires_grad = False\n    for param in model.get_classifier().parameters():\n        param.requires_grad = True\n\n    # You can normalize the batches of images to be a bit faster\n    mean = torch.tensor(model.default_cfg[\"mean\"])[None, :, None, None]\n    std = torch.tensor(model.default_cfg[\"std\"])[None, :, None, None]\n\n    # To make these constants available on the active device, set it to the accelerator device\n    mean = mean.to(accelerator.device)\n    std = std.to(accelerator.device)\n\n    # Instantiate the optimizer\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)\n\n    # Instantiate the learning rate scheduler\n    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))\n\n    # Prepare everything\n    # There is no specific order to remember, you just need to unpack the objects in the same order you gave them to the\n    # prepare method.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # Now you train the model\n    for epoch in range(5):\n        model.train()\n        for batch in train_dataloader:\n            inputs = (batch[\"image\"] - mean) / std\n            outputs = model(inputs)\n            loss = torch.nn.functional.cross_entropy(outputs, batch[\"label\"])\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        model.eval()\n        accurate = 0\n        num_elems = 0\n        for batch in eval_dataloader:\n            inputs = (batch[\"image\"] - mean) / std\n            with torch.no_grad():\n                outputs = model(inputs)\n            predictions = outputs.argmax(dim=-1)\n            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[\"label\"])\n            num_elems += accurate_preds.shape[0]\n            accurate += accurate_preds.long().sum()\n\n        eval_metric = accurate.item() / num_elems\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}: {100 * eval_metric:.2f}\")\n```", "```py\nfrom accelerate import notebook_launcher\n```", "```py\nargs = (\"fp16\", 42, 64)\nnotebook_launcher(training_loop, args, num_processes=2)\n```", "```py\nnotebook_launcher(training_loop, args, master_addr=\"172.31.43.8\", node_rank=0, num_nodes=2, num_processes=8)\n```", "```py\nnotebook_launcher(training_loop, args, master_addr=\"172.31.43.8\", node_rank=1, num_nodes=2, num_processes=8)\n```", "```py\nmodel = create_model(\"resnet50d\", pretrained=True, num_classes=len(label_to_id))\n\nargs = (model, \"fp16\", 42, 64)\nnotebook_launcher(training_loop, args, num_processes=8)\n```", "```py\nLaunching training on 2 GPUs.\nepoch 0: 88.12\nepoch 1: 91.73\nepoch 2: 92.58\nepoch 3: 93.90\nepoch 4: 94.71\n```"]