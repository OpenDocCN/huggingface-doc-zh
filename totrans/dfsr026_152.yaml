- en: MusicLDM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/musicldm](https://huggingface.co/docs/diffusers/api/pipelines/musicldm)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'MusicLDM was proposed in [MusicLDM: Enhancing Novelty in Text-to-Music Generation
    Using Beat-Synchronous Mixup Strategies](https://huggingface.co/papers/2308.01546)
    by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick,
    Shlomo Dubnov. MusicLDM takes a text prompt as input and predicts the corresponding
    music sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)
    and [AudioLDM](https://huggingface.co/docs/diffusers/api/pipelines/audioldm),
    MusicLDM is a text-to-music *latent diffusion model (LDM)* that learns continuous
    audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)
    latents.
  prefs: []
  type: TYPE_NORMAL
- en: MusicLDM is trained on a corpus of 466 hours of music data. Beat-synchronous
    data augmentation strategies are applied to the music samples, both in the time
    domain and in the latent space. Using beat-synchronous data augmentation strategies
    encourages the model to interpolate between the training samples, but stay within
    the domain of the training data. The result is generated music that is more diverse
    while staying faithful to the corresponding style.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract of the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Diffusion models have shown promising results in cross-modal generation tasks,
    including text-to-image and text-to-audio generation. However, generating music,
    as a special type of audio, presents unique challenges due to limited availability
    of music data and sensitive issues related to copyright and plagiarism. In this
    paper, to tackle these challenges, we first construct a state-of-the-art text-to-music
    model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the
    music domain. We achieve this by retraining the contrastive language-audio pretraining
    model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection
    of music data samples. Then, to address the limitations of training data and to
    avoid plagiarism, we leverage a beat tracking model and propose two different
    mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous
    latent mixup, which recombine training audio directly or via a latent embeddings
    space, respectively. Such mixup strategies encourage the model to interpolate
    between musical training samples and generate new music within the convex hull
    of the training data, making the generated music more diverse while still staying
    faithful to the corresponding style. In addition to popular evaluation metrics,
    we design several new evaluation metrics based on CLAP score to demonstrate that
    our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality
    and novelty of generated music, as well as the correspondence between input text
    and generated music.*'
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline was contributed by [sanchit-gandhi](https://huggingface.co/sanchit-gandhi).
  prefs: []
  type: TYPE_NORMAL
- en: Tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When constructing a prompt, keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive prompt inputs work best; use adjectives to describe the sound (for
    example, “high quality” or “clear”) and make the prompt context specific where
    possible (e.g. “melodic techno with a fast beat and synths” works better than
    “techno”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a *negative prompt* can significantly improve the quality of the generated
    audio. Try using a negative prompt of “low quality, average quality”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During inference:'
  prefs: []
  type: TYPE_NORMAL
- en: The *quality* of the generated audio sample can be controlled by the `num_inference_steps`
    argument; higher steps give higher quality audio at the expense of slower inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple waveforms can be generated in one go: set `num_waveforms_per_prompt`
    to a value greater than 1 to enable. Automatic scoring will be performed between
    the generated waveforms and prompt text, and the audios ranked from best to worst
    accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *length* of the generated audio sample can be controlled by varying the
    `audio_length_in_s` argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: MusicLDMPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.MusicLDMPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel))
    — Frozen text-audio embedding model (`ClapTextModel`), specifically the [laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)
    variant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`PreTrainedTokenizer`) — A [RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor))
    — Feature extractor to compute mel-spectrograms from audio waveforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A `UNet2DConditionModel` to denoise the encoded audio latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded audio
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan))
    — Vocoder of class `SpeechT5HifiGan`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-audio generation using MusicLDM.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L434)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    audio generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_length_in_s` (`int`, *optional*, defaults to 10.24) — The length of
    the generated audio sample in seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 200) — The number of
    denoising steps. More denoising steps usually lead to a higher quality audio at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 2.0) — A higher guidance
    scale value encourages the model to generate audio that is closely linked to the
    text `prompt` at the expense of lower sound quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in audio generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_waveforms_per_prompt` (`int`, *optional*, defaults to 1) — The number
    of waveforms to generate per prompt. If `num_waveforms_per_prompt > 1`, the text
    encoding model is a joint text-audio model ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)),
    and the tokenizer is a `[~transformers.ClapProcessor]`, then automatic scoring
    will be performed between the generated outputs and the input text. This scoring
    ranks the generated waveforms based on their cosine similarity to text input in
    the joint text-audio embedding space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"np"`) — The output format of
    the generated audio. Choose between `"np"` to return a NumPy `np.ndarray` or `"pt"`
    to return a PyTorch `torch.Tensor` object. Set to `"latent"` to return the latent
    diffusion model (LDM) output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated audio.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L125)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_model_cpu_offload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L400)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Offloads all models to CPU using accelerate, reducing memory usage with a low
    impact on performance. Compared to `enable_sequential_cpu_offload`, this method
    moves one whole model at a time to the GPU when its `forward` method is called,
    and the model remains in GPU until the next model runs. Memory savings are lower
    than with `enable_sequential_cpu_offload`, but performance is much better due
    to the iterative execution of the `unet`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L117)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
