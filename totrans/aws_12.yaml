- en: Sentence Transformers on AWS Inferentia with Optimum Neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/optimum-neuron/tutorials/sentence_transformers](https://huggingface.co/docs/optimum-neuron/tutorials/sentence_transformers)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/optimum.neuron/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/start.abfe5599.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/singletons.9144bb03.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/paths.e169ac99.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/app.df8ec0a0.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/index.cdcc3d35.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/0.a52c6f40.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/25.1220aa3e.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/CodeBlock.e3ac94d9.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/Heading.96ce3702.js">
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a notebook version of that tutorial [here](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/sentence-transformers/getting-started.ipynb).*'
  prefs: []
  type: TYPE_NORMAL
- en: This guide explains how to compile, load, and use [Sentence Transformers (SBERT)](https://www.sbert.net/)
    models on AWS Inferentia2 with Optimum Neuron, enabling efficient calculation
    of embeddings. Sentence Transformers are powerful models for generating sentence
    embeddings. You can use this Sentence Transformers to compute sentence / text
    embeddings for more than 100 languages. These embeddings can then be compared
    e.g. with cosine-similarity to find sentences with a similar meaning. This can
    be useful for semantic textual similarity, semantic search, or paraphrase mining.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Currently only text models are supported, we are working on vision support
    for CLIP.*'
  prefs: []
  type: TYPE_NORMAL
- en: Convert Sentence Transformers model to AWS Inferentia2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you need to convert your Sentence Transformers model to a format compatible
    with AWS Inferentia2\. You can compile Sentence Transformers models with Optimum
    Neuron using the `optimum-cli` or `NeuronModelForSentenceTransformers` class.
    Below you will find an example for both approaches. We have to make sure `sentence-transformers`
    is installed. Thats only needed for exporting the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here we will use the `NeuronModelForSentenceTransformers`, which can be used
    to convert any Sntence Transformers model to a format compatible with AWS Inferentia2
    or load already converted models. When exporting models with the `NeuronModelForSentenceTransformers`
    you need to set `export=True` and define the input shape and batch size. The input
    shape is defined by the `sequence_length` and the batch size by `batch_size`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here we will use the `optimum-cli` to convert the model. Similar to the `NeuronModelForSentenceTransformers`
    we need to define our input shape and batch size. The input shape is defined by
    the `sequence_length` and the batch size by `batch_size`. The `optimum-cli` will
    automatically convert the model to a format compatible with AWS Inferentia2 and
    save it to the specified output directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Load compiled Sentence Transformers model and run inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have a compiled Sentence Transformers model, which we either exported
    ourselves or is available on the Hugging Face Hub, we can load it and run inference.
    For loading the model we can use the `NeuronModelForSentenceTransformers` class,
    which is an abstraction layer for the `SentenceTransformer` class. The `NeuronModelForSentenceTransformers`
    class will automatically pad the input to the specified `sequence_length` and
    run inference on AWS Inferentia2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Production Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For deploying these models in a production environment, refer to the [Amazon
    SageMaker Blog](https://www.philschmid.de/inferentia2-embeddings).
  prefs: []
  type: TYPE_NORMAL
