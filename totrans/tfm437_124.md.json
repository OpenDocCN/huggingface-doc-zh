["```py\n# Let's see how to increase the vocabulary of Bert model and tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\nnum_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```", "```py\n# Let's see how to add a new classification token to GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nspecial_tokens_dict = {\"cls_token\": \"<CLS>\"}\n\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\nassert tokenizer.cls_token == \"<CLS>\"\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"my-finetuned-bert\")\n\n# Push the tokenizer to an organization with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n# Let's see how to increase the vocabulary of Bert model and tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\nnum_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```", "```py\n# Let's see how to add a new classification token to GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nspecial_tokens_dict = {\"cls_token\": \"<CLS>\"}\n\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\nassert tokenizer.cls_token == \"<CLS>\"\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"my-finetuned-bert\")\n\n# Push the tokenizer to an organization with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n```"]