["```py\n( **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( new_tokens: Union special_tokens: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n# Let's see how to increase the vocabulary of Bert model and tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\nnum_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```", "```py\n( special_tokens_dict: Dict replace_additional_special_tokens = True ) \u2192 export const metadata = 'undefined';int\n```", "```py\n# Let's see how to add a new classification token to GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nspecial_tokens_dict = {\"cls_token\": \"<CLS>\"}\n\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\nassert tokenizer.cls_token == \"<CLS>\"\n```", "```py\n( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) \u2192 export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"my-finetuned-bert\")\n\n# Push the tokenizer to an organization with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( ids: Union skip_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';str or List[str]\n```", "```py\n( tokens: Union ) \u2192 export const metadata = 'undefined';int or List[int]\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, int]\n```", "```py\n( pair: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( text: str is_split_into_words: bool = False **kwargs ) \u2192 export const metadata = 'undefined';Tuple[str, Dict[str, Any]]\n```", "```py\n( text: str **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( *args **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( new_tokens: Union special_tokens: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n# Let's see how to increase the vocabulary of Bert model and tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\nnum_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```", "```py\n( special_tokens_dict: Dict replace_additional_special_tokens = True ) \u2192 export const metadata = 'undefined';int\n```", "```py\n# Let's see how to add a new classification token to GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nspecial_tokens_dict = {\"cls_token\": \"<CLS>\"}\n\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nprint(\"We have added\", num_added_toks, \"tokens\")\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\nassert tokenizer.cls_token == \"<CLS>\"\n```", "```py\n( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) \u2192 export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"my-finetuned-bert\")\n\n# Push the tokenizer to an organization with the name \"my-finetuned-bert\".\ntokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( ids: Union skip_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';str or List[str]\n```", "```py\n( tokens: Union ) \u2192 export const metadata = 'undefined';int or List[int]\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, int]\n```", "```py\n( pair: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( padding_strategy: PaddingStrategy truncation_strategy: TruncationStrategy max_length: int stride: int pad_to_multiple_of: Optional )\n```", "```py\n( text_iterator vocab_size length = None new_special_tokens = None special_tokens_map = None **kwargs ) \u2192 export const metadata = 'undefined';PreTrainedTokenizerFast\n```", "```py\n( data: Optional = None encoding: Union = None tensor_type: Union = None prepend_batch_axis: bool = False n_sequences: Optional = None )\n```", "```py\n( batch_or_char_index: int char_index: Optional = None sequence_index: int = 0 ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( batch_or_char_index: int char_index: Optional = None sequence_index: int = 0 ) \u2192 export const metadata = 'undefined';int or List[int]\n```", "```py\n( tensor_type: Union = None prepend_batch_axis: bool = False )\n```", "```py\n( batch_index: int = 0 ) \u2192 export const metadata = 'undefined';List[Optional[int]]\n```", "```py\n( device: Union ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( batch_or_token_index: int token_index: Optional = None ) \u2192 export const metadata = 'undefined';CharSpan\n```", "```py\n( batch_or_token_index: int token_index: Optional = None ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( batch_or_token_index: int token_index: Optional = None ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( batch_index: int = 0 ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( batch_index: int = 0 ) \u2192 export const metadata = 'undefined';List[Optional[int]]\n```", "```py\n( batch_or_word_index: int word_index: Optional = None sequence_index: int = 0 ) \u2192 export const metadata = 'undefined';CharSpan or List[CharSpan]\n```", "```py\n( batch_or_word_index: int word_index: Optional = None sequence_index: int = 0 ) \u2192 export const metadata = 'undefined';(TokenSpan, optional)\n```", "```py\n( batch_index: int = 0 ) \u2192 export const metadata = 'undefined';List[Optional[int]]\n```"]