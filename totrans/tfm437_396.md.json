["```py\n( image: ndarray size: Tuple data_format: Union = None input_data_format: Union = None return_numpy: Optional = None ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n( bboxes_center: TensorType )\n```", "```py\n( bboxes_corners: TensorType )\n```", "```py\n( id_map )\n```", "```py\n( image: ndarray mean: Union std: Union data_format: Optional = None input_data_format: Union = None )\n```", "```py\n( image: ndarray padding: Union mode: PaddingMode = <PaddingMode.CONSTANT: 'constant'> constant_values: Union = 0.0 data_format: Union = None input_data_format: Union = None ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n( color )\n```", "```py\n( image: ndarray scale: float data_format: Optional = None dtype: dtype = <class 'numpy.float32'> input_data_format: Union = None ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n( image: ndarray size: Tuple resample: PILImageResampling = None reducing_gap: Optional = None data_format: Optional = None return_numpy: bool = True input_data_format: Union = None ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n( image: Union do_rescale: Optional = None input_data_format: Union = None ) \u2192 export const metadata = 'undefined';PIL.Image.Image\n```", "```py\n( **kwargs )\n```", "```py\n( image_url_or_urls: Union )\n```", "```py\n( image_processor_dict: Dict **kwargs ) \u2192 export const metadata = 'undefined';ImageProcessingMixin\n```", "```py\n( json_file: Union ) \u2192 export const metadata = 'undefined';A image processor of type ImageProcessingMixin\n```", "```py\n( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n# We can't instantiate directly the base class *ImageProcessingMixin* so let's show the examples on a\n# derived class: *CLIPImageProcessor*\nimage_processor = CLIPImageProcessor.from_pretrained(\n    \"openai/clip-vit-base-patch32\"\n)  # Download image_processing_config from huggingface.co and cache.\nimage_processor = CLIPImageProcessor.from_pretrained(\n    \"./test/saved_model/\"\n)  # E.g. image processor (or model) was saved using *save_pretrained('./test/saved_model/')*\nimage_processor = CLIPImageProcessor.from_pretrained(\"./test/saved_model/preprocessor_config.json\")\nimage_processor = CLIPImageProcessor.from_pretrained(\n    \"openai/clip-vit-base-patch32\", do_normalize=False, foo=False\n)\nassert image_processor.do_normalize is False\nimage_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(\n    \"openai/clip-vit-base-patch32\", do_normalize=False, foo=False, return_unused_kwargs=True\n)\nassert image_processor.do_normalize is False\nassert unused_kwargs == {\"foo\": False}\n```", "```py\n( pretrained_model_name_or_path: Union **kwargs ) \u2192 export const metadata = 'undefined';Tuple[Dict, Dict]\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import AutoImageProcessor\n\nimage processor = AutoImageProcessor.from_pretrained(\"bert-base-cased\")\n\n# Push the image processor to your namespace with the name \"my-finetuned-bert\".\nimage processor.push_to_hub(\"my-finetuned-bert\")\n\n# Push the image processor to an organization with the name \"my-finetuned-bert\".\nimage processor.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( auto_class = 'AutoImageProcessor' )\n```", "```py\n( save_directory: Union push_to_hub: bool = False **kwargs )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, Any]\n```", "```py\n( json_file_path: Union )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';str\n```"]