["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor inverse_scheduler: DDIMInverseScheduler requires_safety_checker: bool = True )\n```", "```py\n( image: Union = None target_prompt: Union = None target_negative_prompt: Union = None target_prompt_embeds: Optional = None target_negative_prompt_embeds: Optional = None source_prompt: Union = None source_negative_prompt: Union = None source_prompt_embeds: Optional = None source_negative_prompt_embeds: Optional = None num_maps_per_mask: Optional = 10 mask_encode_strength: Optional = 0.5 mask_thresholding_ratio: Optional = 3.0 num_inference_steps: int = 50 guidance_scale: float = 7.5 generator: Union = None output_type: Optional = 'np' cross_attention_kwargs: Optional = None ) \u2192 export const metadata = 'undefined';List[PIL.Image.Image] or np.array\n```", "```py\n>>> import PIL\n>>> import requests\n>>> import torch\n>>> from io import BytesIO\n\n>>> from diffusers import StableDiffusionDiffEditPipeline\n\n>>> def download_image(url):\n...     response = requests.get(url)\n...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n>>> img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n\n>>> init_image = download_image(img_url).resize((768, 768))\n\n>>> pipe = StableDiffusionDiffEditPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n>>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n>>> pipeline.enable_model_cpu_offload()\n\n>>> mask_prompt = \"A bowl of fruits\"\n>>> prompt = \"A bowl of pears\"\n\n>>> mask_image = pipe.generate_mask(image=init_image, source_prompt=prompt, target_prompt=mask_prompt)\n>>> image_latents = pipe.invert(image=init_image, prompt=mask_prompt).latents\n>>> image = pipe(prompt=prompt, mask_image=mask_image, image_latents=image_latents).images[0]\n```", "```py\n( prompt: Union = None image: Union = None num_inference_steps: int = 50 inpaint_strength: float = 0.8 guidance_scale: float = 7.5 negative_prompt: Union = None generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None decode_latents: bool = False output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None lambda_auto_corr: float = 20.0 lambda_kl: float = 20.0 num_reg_steps: int = 0 num_auto_corr_rolls: int = 5 )\n```", "```py\n>>> import PIL\n>>> import requests\n>>> import torch\n>>> from io import BytesIO\n\n>>> from diffusers import StableDiffusionDiffEditPipeline\n\n>>> def download_image(url):\n...     response = requests.get(url)\n...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n>>> img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n\n>>> init_image = download_image(img_url).resize((768, 768))\n\n>>> pipe = StableDiffusionDiffEditPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n>>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n>>> pipeline.enable_model_cpu_offload()\n\n>>> prompt = \"A bowl of fruits\"\n\n>>> inverted_latents = pipe.invert(image=init_image, prompt=prompt).latents\n```", "```py\n( prompt: Union = None mask_image: Union = None image_latents: Union = None inpaint_strength: Optional = 0.8 num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_ckip: int = None ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import PIL\n>>> import requests\n>>> import torch\n>>> from io import BytesIO\n\n>>> from diffusers import StableDiffusionDiffEditPipeline\n\n>>> def download_image(url):\n...     response = requests.get(url)\n...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n>>> img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n\n>>> init_image = download_image(img_url).resize((768, 768))\n\n>>> pipe = StableDiffusionDiffEditPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n>>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n>>> pipeline.enable_model_cpu_offload()\n\n>>> mask_prompt = \"A bowl of fruits\"\n>>> prompt = \"A bowl of pears\"\n\n>>> mask_image = pipe.generate_mask(image=init_image, source_prompt=prompt, target_prompt=mask_prompt)\n>>> image_latents = pipe.invert(image=init_image, prompt=mask_prompt).latents\n>>> image = pipe(prompt=prompt, mask_image=mask_image, image_latents=image_latents).images[0]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```"]