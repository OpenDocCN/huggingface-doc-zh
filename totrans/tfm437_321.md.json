["```py\nimport torch\nfrom scipy.io.wavfile import write\nfrom datasets import Audio, load_dataset\n\nfrom transformers import UnivNetFeatureExtractor, UnivNetModel\n\nmodel_id_or_path = \"dg845/univnet-dev\"\nmodel = UnivNetModel.from_pretrained(model_id_or_path)\nfeature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)\n\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# Resample the audio to the model and feature extractor's sampling rate.\nds = ds.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n# Pad the end of the converted waveforms to reduce artifacts at the end of the output audio samples.\ninputs = feature_extractor(\n    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], pad_end=True, return_tensors=\"pt\"\n)\n\nwith torch.no_grad():\n    audio = model(**inputs)\n\n# Remove the extra padding at the end of the output.\naudio = feature_extractor.batch_decode(**audio)[0]\n# Convert to wav file\nwrite(\"sample_audio.wav\", feature_extractor.sampling_rate, audio)\n```", "```py\n( model_in_channels = 64 model_hidden_channels = 32 num_mel_bins = 100 resblock_kernel_sizes = [3, 3, 3] resblock_stride_sizes = [8, 8, 4] resblock_dilation_sizes = [[1, 3, 9, 27], [1, 3, 9, 27], [1, 3, 9, 27]] kernel_predictor_num_blocks = 3 kernel_predictor_hidden_channels = 64 kernel_predictor_conv_size = 3 kernel_predictor_dropout = 0.0 initializer_range = 0.01 leaky_relu_slope = 0.2 **kwargs )\n```", "```py\n>>> from transformers import UnivNetModel, UnivNetConfig\n\n>>> # Initializing a Tortoise TTS style configuration\n>>> configuration = UnivNetConfig()\n\n>>> # Initializing a model (with random weights) from the Tortoise TTS style configuration\n>>> model = UnivNetModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( feature_size: int = 1 sampling_rate: int = 24000 padding_value: float = 0.0 do_normalize: bool = False num_mel_bins: int = 100 hop_length: int = 256 win_length: int = 1024 win_function: str = 'hann_window' filter_length: Optional = 1024 max_length_s: int = 10 fmin: float = 0.0 fmax: Optional = None mel_floor: float = 1e-09 center: bool = False compression_factor: float = 1.0 compression_clip_val: float = 1e-05 normalize_min: float = -11.512925148010254 normalize_max: float = 2.3143386840820312 model_in_channels: int = 64 pad_end_length: int = 10 return_attention_mask = True **kwargs )\n```", "```py\n( raw_speech: Union sampling_rate: Optional = None padding: Union = True max_length: Optional = None truncation: bool = True pad_to_multiple_of: Optional = None return_noise: bool = True generator: Optional = None pad_end: bool = False pad_length: Optional = None do_normalize: Optional = None return_attention_mask: Optional = None return_tensors: Union = None )\n```", "```py\n( config: UnivNetConfig )\n```", "```py\n( input_features: FloatTensor noise_sequence: Optional = None padding_mask: Optional = None generator: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.univnet.modeling_univnet.UnivNetModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import UnivNetFeatureExtractor, UnivNetModel\n>>> from datasets import load_dataset, Audio\n\n>>> model = UnivNetModel.from_pretrained(\"dg845/univnet-dev\")\n>>> feature_extractor = UnivNetFeatureExtractor.from_pretrained(\"dg845/univnet-dev\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> # Resample the audio to the feature extractor's sampling rate.\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n>>> inputs = feature_extractor(\n...     ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n... )\n>>> audio = model(**inputs).waveforms\n>>> list(audio.shape)\n[1, 140288]\n```"]