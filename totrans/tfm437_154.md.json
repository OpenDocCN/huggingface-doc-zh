["```py\n( vocab_size = 246534 n_positions = 256 n_embd = 1280 dff = 8192 n_layer = 48 n_head = 16 resid_pdrop = 0.1 embd_pdrop = 0.1 layer_norm_epsilon = 1e-06 initializer_range = 0.02 use_cache = True **kwargs )\n```", "```py\n>>> from transformers import CTRLConfig, CTRLModel\n\n>>> # Initializing a CTRL configuration\n>>> configuration = CTRLConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = CTRLModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file unk_token = '<unk>' **kwargs )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, CTRLModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = CTRLModel.from_pretrained(\"Salesforce/ctrl\")\n\n>>> # CTRL was trained with control codes as the first token\n>>> inputs = tokenizer(\"Opinion My dog is cute\", return_tensors=\"pt\")\n>>> assert inputs[\"input_ids\"][0, 0].item() in tokenizer.control_codes.values()\n\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 5, 1280]\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CTRLLMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = CTRLLMHeadModel.from_pretrained(\"Salesforce/ctrl\")\n\n>>> # CTRL was trained with control codes as the first token\n>>> inputs = tokenizer(\"Wikipedia The llama is\", return_tensors=\"pt\")\n>>> assert inputs[\"input_ids\"][0, 0].item() in tokenizer.control_codes.values()\n\n>>> sequence_ids = model.generate(inputs[\"input_ids\"])\n>>> sequences = tokenizer.batch_decode(sequence_ids)\n>>> sequences\n['Wikipedia The llama is a member of the family Bovidae. It is native to the Andes of Peru,']\n\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> round(outputs.loss.item(), 2)\n9.21\n\n>>> list(outputs.logits.shape)\n[1, 5, 246534]\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CTRLForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = CTRLForSequenceClassification.from_pretrained(\"Salesforce/ctrl\")\n\n>>> # CTRL was trained with control codes as the first token\n>>> inputs = tokenizer(\"Opinion My dog is cute\", return_tensors=\"pt\")\n>>> assert inputs[\"input_ids\"][0, 0].item() in tokenizer.control_codes.values()\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'LABEL_0'\n```", "```py\n>>> import torch\n\n>>> torch.manual_seed(42)\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = CTRLForSequenceClassification.from_pretrained(\"Salesforce/ctrl\", num_labels=num_labels)\n\n>>> labels = torch.tensor(1)\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n0.35\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CTRLForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = CTRLForSequenceClassification.from_pretrained(\n...     \"Salesforce/ctrl\", problem_type=\"multi_label_classification\"\n... )\n\n>>> # CTRL was trained with control codes as the first token\n>>> inputs = tokenizer(\"Opinion My dog is cute\", return_tensors=\"pt\")\n>>> assert inputs[\"input_ids\"][0, 0].item() in tokenizer.control_codes.values()\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'LABEL_0'\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = CTRLForSequenceClassification.from_pretrained(\"Salesforce/ctrl\", num_labels=num_labels)\n\n>>> num_labels = len(model.config.id2label)\n>>> labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(\n...     torch.float\n... )\n>>> loss = model(**inputs, labels=labels).loss\n>>> loss.backward()\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFCTRLModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = TFCTRLModel.from_pretrained(\"Salesforce/ctrl\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFCTRLLMHeadModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = TFCTRLLMHeadModel.from_pretrained(\"Salesforce/ctrl\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFCTRLForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n>>> model = TFCTRLForSequenceClassification.from_pretrained(\"Salesforce/ctrl\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFCTRLForSequenceClassification.from_pretrained(\"Salesforce/ctrl\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```"]