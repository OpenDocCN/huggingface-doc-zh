["```py\n( vocab_size = 30522 type_vocab_size = 2 modality_type_vocab_size = 2 max_position_embeddings = 40 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 384 patch_size = 32 num_channels = 3 qkv_bias = True max_image_length = -1 tie_word_embeddings = False num_images = -1 **kwargs )\n```", "```py\n>>> from transformers import ViLTModel, ViLTConfig\n\n>>> # Initializing a ViLT dandelin/vilt-b32-mlm style configuration\n>>> configuration = ViLTConfig()\n\n>>> # Initializing a model from the dandelin/vilt-b32-mlm style configuration\n>>> model = ViLTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( do_resize: bool = True size: Dict = None size_divisor: int = 32 resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )\n```", "```py\n( images: Union do_resize: Optional = None size: Optional = None size_divisor: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( images text: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None image_token_type_idx: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import ViltProcessor, ViltModel\n>>> from PIL import Image\n>>> import requests\n\n>>> # prepare image and text\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"hello world\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n>>> model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n\n>>> inputs = processor(image, text, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import ViltProcessor, ViltForMaskedLM\n>>> import requests\n>>> from PIL import Image\n>>> import re\n>>> import torch\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"a bunch of [MASK] laying on a [MASK].\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n>>> model = ViltForMaskedLM.from_pretrained(\"dandelin/vilt-b32-mlm\")\n\n>>> # prepare inputs\n>>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> tl = len(re.findall(\"\\[MASK\\]\", text))\n>>> inferred_token = [text]\n\n>>> # gradually fill in the MASK tokens, one by one\n>>> with torch.no_grad():\n...     for i in range(tl):\n...         encoded = processor.tokenizer(inferred_token)\n...         input_ids = torch.tensor(encoded.input_ids)\n...         encoded = encoded[\"input_ids\"][0][1:-1]\n...         outputs = model(input_ids=input_ids, pixel_values=encoding.pixel_values)\n...         mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)\n...         # only take into account text features (minus CLS and SEP token)\n...         mlm_logits = mlm_logits[1 : input_ids.shape[1] - 1, :]\n...         mlm_values, mlm_ids = mlm_logits.softmax(dim=-1).max(dim=-1)\n...         # only take into account text\n...         mlm_values[torch.tensor(encoded) != 103] = 0\n...         select = mlm_values.argmax().item()\n...         encoded[select] = mlm_ids[select].item()\n...         inferred_token = [processor.decode(encoded)]\n\n>>> selected_token = \"\"\n>>> encoded = processor.tokenizer(inferred_token)\n>>> output = processor.decode(encoded.input_ids[0], skip_special_tokens=True)\n>>> print(output)\na bunch of cats laying on a couch.\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import ViltProcessor, ViltForQuestionAnswering\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"How many cats are there?\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n>>> model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n>>> # prepare inputs\n>>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: 2\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.vilt.modeling_vilt.ViltForImagesAndTextClassificationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import ViltProcessor, ViltForImagesAndTextClassification\n>>> import requests\n>>> from PIL import Image\n\n>>> image1 = Image.open(requests.get(\"https://lil.nlp.cornell.edu/nlvr/exs/ex0_0.jpg\", stream=True).raw)\n>>> image2 = Image.open(requests.get(\"https://lil.nlp.cornell.edu/nlvr/exs/ex0_1.jpg\", stream=True).raw)\n>>> text = \"The left image contains twice the number of dogs as the right image.\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-nlvr2\")\n>>> model = ViltForImagesAndTextClassification.from_pretrained(\"dandelin/vilt-b32-finetuned-nlvr2\")\n\n>>> # prepare inputs\n>>> encoding = processor([image1, image2], text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(input_ids=encoding.input_ids, pixel_values=encoding.pixel_values.unsqueeze(0))\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: True\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n>>> model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs.logits[0, :].item()\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```"]