["```py\n>>> from transformers import ViLTModel, ViLTConfig\n\n>>> # Initializing a ViLT dandelin/vilt-b32-mlm style configuration\n>>> configuration = ViLTConfig()\n\n>>> # Initializing a model from the dandelin/vilt-b32-mlm style configuration\n>>> model = ViLTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import ViltProcessor, ViltModel\n>>> from PIL import Image\n>>> import requests\n\n>>> # prepare image and text\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"hello world\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n>>> model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n\n>>> inputs = processor(image, text, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import ViltProcessor, ViltForMaskedLM\n>>> import requests\n>>> from PIL import Image\n>>> import re\n>>> import torch\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"a bunch of [MASK] laying on a [MASK].\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n>>> model = ViltForMaskedLM.from_pretrained(\"dandelin/vilt-b32-mlm\")\n\n>>> # prepare inputs\n>>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> tl = len(re.findall(\"\\[MASK\\]\", text))\n>>> inferred_token = [text]\n\n>>> # gradually fill in the MASK tokens, one by one\n>>> with torch.no_grad():\n...     for i in range(tl):\n...         encoded = processor.tokenizer(inferred_token)\n...         input_ids = torch.tensor(encoded.input_ids)\n...         encoded = encoded[\"input_ids\"][0][1:-1]\n...         outputs = model(input_ids=input_ids, pixel_values=encoding.pixel_values)\n...         mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)\n...         # only take into account text features (minus CLS and SEP token)\n...         mlm_logits = mlm_logits[1 : input_ids.shape[1] - 1, :]\n...         mlm_values, mlm_ids = mlm_logits.softmax(dim=-1).max(dim=-1)\n...         # only take into account text\n...         mlm_values[torch.tensor(encoded) != 103] = 0\n...         select = mlm_values.argmax().item()\n...         encoded[select] = mlm_ids[select].item()\n...         inferred_token = [processor.decode(encoded)]\n\n>>> selected_token = \"\"\n>>> encoded = processor.tokenizer(inferred_token)\n>>> output = processor.decode(encoded.input_ids[0], skip_special_tokens=True)\n>>> print(output)\na bunch of cats laying on a couch.\n```", "```py\n>>> from transformers import ViltProcessor, ViltForQuestionAnswering\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"How many cats are there?\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n>>> model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n>>> # prepare inputs\n>>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: 2\n```", "```py\n>>> from transformers import ViltProcessor, ViltForImagesAndTextClassification\n>>> import requests\n>>> from PIL import Image\n\n>>> image1 = Image.open(requests.get(\"https://lil.nlp.cornell.edu/nlvr/exs/ex0_0.jpg\", stream=True).raw)\n>>> image2 = Image.open(requests.get(\"https://lil.nlp.cornell.edu/nlvr/exs/ex0_1.jpg\", stream=True).raw)\n>>> text = \"The left image contains twice the number of dogs as the right image.\"\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-nlvr2\")\n>>> model = ViltForImagesAndTextClassification.from_pretrained(\"dandelin/vilt-b32-finetuned-nlvr2\")\n\n>>> # prepare inputs\n>>> encoding = processor([image1, image2], text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(input_ids=encoding.input_ids, pixel_values=encoding.pixel_values.unsqueeze(0))\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: True\n```", "```py\n>>> from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n>>> model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs.logits[0, :].item()\n```"]