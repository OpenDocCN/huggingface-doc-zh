- en: Text generation strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/generation_strategies](https://huggingface.co/docs/transformers/v4.37.2/en/generation_strategies)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Text generation is essential to many NLP tasks, such as open-ended text generation,
    summarization, translation, and more. It also plays a role in a variety of mixed-modality
    applications that have text as an output like speech-to-text and vision-to-text.
    Some of the models that can generate text include GPT2, XLNet, OpenAI GPT, CTRL,
    TransformerXL, XLM, Bart, T5, GIT, Whisper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out a few examples that use [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method to produce text outputs for different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Text summarization](./tasks/summarization#inference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image captioning](./model_doc/git#transformers.GitForCausalLM.forward.example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Audio transcription](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the inputs to the generate method depend on the modelâ€™s modality.
    They are returned by the modelâ€™s preprocessor class, such as AutoTokenizer or
    AutoProcessor. If a modelâ€™s preprocessor creates more than one kind of input,
    pass all the inputs to generate(). You can learn more about the individual modelâ€™s
    preprocessor in the corresponding modelâ€™s documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The process of selecting output tokens to generate text is known as decoding,
    and you can customize the decoding strategy that the `generate()` method will
    use. Modifying a decoding strategy does not change the values of any trainable
    parameters. However, it can have a noticeable impact on the quality of the generated
    output. It can help reduce repetition in the text and make it more coherent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide describes:'
  prefs: []
  type: TYPE_NORMAL
- en: default generation configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: common decoding strategies and their main parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saving and sharing custom generation configurations with your fine-tuned model
    on ðŸ¤— Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Default text generation configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A decoding strategy for a model is defined in its generation configuration.
    When using pre-trained models for inference within a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline),
    the models call the `PreTrainedModel.generate()` method that applies a default
    generation configuration under the hood. The default configuration is also used
    when no custom configuration has been saved with the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you load a model explicitly, you can inspect the generation configuration
    that comes with it through `model.generation_config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Printing out the `model.generation_config` reveals only the values that are
    different from the default generation configuration, and does not list any of
    the default values.
  prefs: []
  type: TYPE_NORMAL
- en: The default generation configuration limits the size of the output combined
    with the input prompt to a maximum of 20 tokens to avoid running into resource
    limitations. The default decoding strategy is greedy search, which is the simplest
    decoding strategy that picks a token with the highest probability as the next
    token. For many tasks and small output sizes this works well. However, when used
    to generate longer outputs, greedy search can start producing highly repetitive
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Customize text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can override any `generation_config` by passing the parameters and their
    values directly to the `generate` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if the default decoding strategy mostly works for your task, you can still
    tweak a few things. Some of the commonly adjusted parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_new_tokens`: the maximum number of tokens to generate. In other words,
    the size of the output sequence, not including the tokens in the prompt. As an
    alternative to using the outputâ€™s length as a stopping criteria, you can choose
    to stop generation whenever the full generation exceeds some amount of time. To
    learn more, check [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams`: by specifying a number of beams higher than 1, you are effectively
    switching from greedy search to beam search. This strategy evaluates several hypotheses
    at each time step and eventually chooses the hypothesis that has the overall highest
    probability for the entire sequence. This has the advantage of identifying high-probability
    sequences that start with a lower probability initial tokens and wouldâ€™ve been
    ignored by the greedy search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_sample`: if set to `True`, this parameter enables decoding strategies such
    as multinomial sampling, beam-search multinomial sampling, Top-K sampling and
    Top-p sampling. All these strategies select the next token from the probability
    distribution over the entire vocabulary with various strategy-specific adjustments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_return_sequences`: the number of sequence candidates to return for each
    input. This option is only available for the decoding strategies that support
    multiple sequence candidates, e.g. variations of beam search and sampling. Decoding
    strategies like greedy search and contrastive search return a single output sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save a custom decoding strategy with your model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you would like to share your fine-tuned model with a specific generation
    configuration, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    class instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the decoding strategy parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save your generation configuration with [GenerationConfig.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained),
    making sure to leave its `config_file_name` argument empty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `push_to_hub` to `True` to upload your config to the modelâ€™s repo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can also store several generation configurations in a single directory,
    making use of the `config_file_name` argument in [GenerationConfig.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained).
    You can later instantiate them with [GenerationConfig.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained).
    This is useful if you want to store several generation configurations for a single
    model (e.g. one for creative text generation with sampling, and one for summarization
    with beam search). You must have the right Hub permissions to add configuration
    files to a model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `generate()` supports streaming, through its `streamer` input. The `streamer`
    input is compatible with any instance from a class that has the following methods:
    `put()` and `end()`. Internally, `put()` is used to push new tokens and `end()`
    is used to flag the end of text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: The API for the streamer classes is still under development and may change in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, you can craft your own streaming class for all sorts of purposes!
    We also have basic streaming classes ready for you to use. For example, you can
    use the [TextStreamer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TextStreamer)
    class to stream the output of `generate()` into your screen, one word at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Decoding strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certain combinations of the `generate()` parameters, and ultimately `generation_config`,
    can be used to enable specific decoding strategies. If you are new to this concept,
    we recommend reading [this blog post that illustrates how common decoding strategies
    work](https://huggingface.co/blog/how-to-generate).
  prefs: []
  type: TYPE_NORMAL
- en: Here, weâ€™ll show some of the parameters that control the decoding strategies
    and illustrate how you can use them.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`generate` uses greedy search decoding by default so you donâ€™t have to pass
    any parameters to enable it. This means the parameters `num_beams` is set to 1
    and `do_sample=False`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Contrastive search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The contrastive search decoding strategy was proposed in the 2022 paper [A
    Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417).
    It demonstrates superior results for generating non-repetitive yet coherent long
    outputs. To learn how contrastive search works, check out [this blog post](https://huggingface.co/blog/introducing-csearch).
    The two main parameters that enable and control the behavior of contrastive search
    are `penalty_alpha` and `top_k`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Multinomial sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As opposed to greedy search that always chooses a token with the highest probability
    as the next token, multinomial sampling (also called ancestral sampling) randomly
    selects the next token based on the probability distribution over the entire vocabulary
    given by the model. Every token with a non-zero probability has a chance of being
    selected, thus reducing the risk of repetition.
  prefs: []
  type: TYPE_NORMAL
- en: To enable multinomial sampling set `do_sample=True` and `num_beams=1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Beam-search decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike greedy search, beam-search decoding keeps several hypotheses at each
    time step and eventually chooses the hypothesis that has the overall highest probability
    for the entire sequence. This has the advantage of identifying high-probability
    sequences that start with lower probability initial tokens and wouldâ€™ve been ignored
    by the greedy search.
  prefs: []
  type: TYPE_NORMAL
- en: To enable this decoding strategy, specify the `num_beams` (aka number of hypotheses
    to keep track of) that is greater than 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Beam-search multinomial sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name implies, this decoding strategy combines beam search with multinomial
    sampling. You need to specify the `num_beams` greater than 1, and set `do_sample=True`
    to use this decoding strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Diverse beam search decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The diverse beam search decoding strategy is an extension of the beam search
    strategy that allows for generating a more diverse set of beam sequences to choose
    from. To learn how it works, refer to [Diverse Beam Search: Decoding Diverse Solutions
    from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf). This approach
    has three main parameters: `num_beams`, `num_beam_groups`, and `diversity_penalty`.
    The diversity penalty ensures the outputs are distinct across groups, and beam
    search is used within each group.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This guide illustrates the main parameters that enable various decoding strategies.
    More advanced parameters exist for the `generate` method, which gives you even
    further control over the `generate` methodâ€™s behavior. For the complete list of
    the available parameters, refer to the [API documentation](./main_classes/text_generation.md).
  prefs: []
  type: TYPE_NORMAL
- en: Speculative Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Speculative decoding (also known as assisted decoding) is a modification of
    the decoding strategies above, that uses an assistant model (ideally a much smaller
    one) with the same tokenizer, to generate a few candidate tokens. The main model
    then validates the candidate tokens in a single forward pass, which speeds up
    the decoding process. If `do_sample=True`, then the token validation with resampling
    introduced in the [speculative decoding paper](https://arxiv.org/pdf/2211.17192.pdf)
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, only greedy search and sampling are supported with assisted decoding,
    and assisted decoding doesnâ€™t support batched inputs. To learn more about assisted
    decoding, check [this blog post](https://huggingface.co/blog/assisted-generation).
  prefs: []
  type: TYPE_NORMAL
- en: To enable assisted decoding, set the `assistant_model` argument with a model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When using assisted decoding with sampling methods, you can use the `temperature`
    argument to control the randomness, just like in multinomial sampling. However,
    in assisted decoding, reducing the temperature may help improve the latency.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
