- en: Custom metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/competitions/custom_metric](https://huggingface.co/docs/competitions/custom_metric)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/competitions/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/entry/start.2658c07d.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/chunks/scheduler.b108d059.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/chunks/singletons.620370f2.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/chunks/paths.cc2c2902.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/entry/app.ae6e5a67.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/chunks/index.008de539.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/nodes/0.361b0005.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/nodes/5.414d0e07.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/chunks/CodeBlock.3968c746.js">
    <link rel="modulepreload" href="/docs/competitions/main/en/_app/immutable/chunks/Heading.88bfeb84.js">
  prefs: []
  type: TYPE_NORMAL
- en: In case you don’t settle for the default scikit-learn metrics, you can define
    your own metric.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we expect the organizer to know python.
  prefs: []
  type: TYPE_NORMAL
- en: How to define a custom metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To define a custom metric, change `EVAL_METRIC` in `conf.json` to `custom`.
    You must also make sure that `EVAL_HIGHER_IS_BETTER` is set to `1` or `0` depending
    on whether a higher value of the metric is better or not.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is to create a file `metric.py` in the private competition repo.
    The file should contain a `compute` function that takes competition params as
    input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the part where we check if metric is custom and calculate the metric
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can find the above part in competitions github repo `compute_metrics.py`
  prefs: []
  type: TYPE_NORMAL
- en: '`params` is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You are free to do whatever you want to in the `compute` function. In the end
    it must return a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'public and private scores must be dictionaries! You can also use multiple metrics.
    Example for multiple metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: When using multiple metrics, conf.json must have `SCORING_METRIC` specified
    to rank the participants in the competition.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if I want to use metric2 to rank the participants, I will set `SCORING_METRIC`
    to `metric2` in `conf.json`.
  prefs: []
  type: TYPE_NORMAL
- en: Example of a custom metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Take a careful look at the above code. You can see that we are downloading the
    solution file and the submission file from the dataset repo. We are then calculating
    the metric on the public and private splits of the solution and submission files.
    Finally, we are returning the metric values in a dictionary.
  prefs: []
  type: TYPE_NORMAL
