["```py\npython src/transformers/models/musicgen/convert_musicgen_transformers.py \\\n    --checkpoint small --pytorch_dump_folder /output/path --safe_serialization \n```", "```py\n>>> from transformers import MusicgenForConditionalGeneration\n\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n>>> unconditional_inputs = model.get_unconditional_inputs(num_samples=1)\n\n>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)\n```", "```py\nfrom IPython.display import Audio\n\nsampling_rate = model.config.audio_encoder.sampling_rate\nAudio(audio_values[0].numpy(), rate=sampling_rate)\n```", "```py\n>>> import scipy\n\n>>> sampling_rate = model.config.audio_encoder.sampling_rate\n>>> scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())\n```", "```py\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> inputs = processor(\n...     text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```", "```py\npip install --upgrade pip\npip install datasets[audio]\n```", "```py\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> dataset = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\", streaming=True)\n>>> sample = next(iter(dataset))[\"audio\"]\n\n>>> # take the first half of the audio sample\n>>> sample[\"array\"] = sample[\"array\"][: len(sample[\"array\"]) // 2]\n\n>>> inputs = processor(\n...     audio=sample[\"array\"],\n...     sampling_rate=sample[\"sampling_rate\"],\n...     text=[\"80s blues track with groovy saxophone\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```", "```py\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> dataset = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\", streaming=True)\n>>> sample = next(iter(dataset))[\"audio\"]\n\n>>> # take the first quarter of the audio sample\n>>> sample_1 = sample[\"array\"][: len(sample[\"array\"]) // 4]\n\n>>> # take the first half of the audio sample\n>>> sample_2 = sample[\"array\"][: len(sample[\"array\"]) // 2]\n\n>>> inputs = processor(\n...     audio=[sample_1, sample_2],\n...     sampling_rate=sample[\"sampling_rate\"],\n...     text=[\"80s blues track with groovy saxophone\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n\n>>> # post-process to remove padding from the batched audio\n>>> audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)\n```", "```py\n>>> from transformers import MusicgenForConditionalGeneration\n\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> # inspect the default generation config\n>>> model.generation_config\n\n>>> # increase the guidance scale to 4.0\n>>> model.generation_config.guidance_scale = 4.0\n\n>>> # decrease the max length to 256 tokens\n>>> model.generation_config.max_length = 256\n```", "```py\n>>> from transformers import AutoConfig, MusicgenForCausalLM, MusicgenForConditionalGeneration\n\n>>> # Option 1: get decoder config and pass to `.from_pretrained`\n>>> decoder_config = AutoConfig.from_pretrained(\"facebook/musicgen-small\").decoder\n>>> decoder = MusicgenForCausalLM.from_pretrained(\"facebook/musicgen-small\", **decoder_config)\n\n>>> # Option 2: load the entire composite model, but only return the decoder\n>>> decoder = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").decoder\n```", "```py\n>>> from transformers import (\n...     MusicgenConfig,\n...     MusicgenDecoderConfig,\n...     T5Config,\n...     EncodecConfig,\n...     MusicgenForConditionalGeneration,\n... )\n\n>>> # Initializing text encoder, audio encoder, and decoder model configurations\n>>> text_encoder_config = T5Config()\n>>> audio_encoder_config = EncodecConfig()\n>>> decoder_config = MusicgenDecoderConfig()\n\n>>> configuration = MusicgenConfig.from_sub_models_config(\n...     text_encoder_config, audio_encoder_config, decoder_config\n... )\n\n>>> # Initializing a MusicgenForConditionalGeneration (with random weights) from the facebook/musicgen-small style configuration\n>>> model = MusicgenForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n>>> config_text_encoder = model.config.text_encoder\n>>> config_audio_encoder = model.config.audio_encoder\n>>> config_decoder = model.config.decoder\n\n>>> # Saving the model, including its configuration\n>>> model.save_pretrained(\"musicgen-model\")\n\n>>> # loading model and config from pretrained folder\n>>> musicgen_config = MusicgenConfig.from_pretrained(\"musicgen-model\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"musicgen-model\", config=musicgen_config)\n```", "```py\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n>>> import torch\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> inputs = processor(\n...     text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n\n>>> pad_token_id = model.generation_config.pad_token_id\n>>> decoder_input_ids = (\n...     torch.ones((inputs.input_ids.shape[0] * model.decoder.num_codebooks, 1), dtype=torch.long)\n...     * pad_token_id\n... )\n\n>>> logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits\n>>> logits.shape  # (bsz * num_codebooks, tgt_len, vocab_size)\ntorch.Size([8, 1, 2048])\n```"]