# æ³¨æ„åŠ›å¤„ç†å™¨

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/api/attnprocessor`](https://huggingface.co/docs/diffusers/api/attnprocessor)

æ³¨æ„åŠ›å¤„ç†å™¨æ˜¯ä¸€ä¸ªåº”ç”¨ä¸åŒç±»å‹æ³¨æ„åŠ›æœºåˆ¶çš„ç±»ã€‚

## AttnProcessor

### `class diffusers.models.attention_processor.AttnProcessor`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)

```py
( )
```

æ‰§è¡Œä¸æ³¨æ„åŠ›ç›¸å…³è®¡ç®—çš„é»˜è®¤å¤„ç†å™¨ã€‚

## AttnProcessor2_0

`class diffusers.models.attention_processor.AttnProcessor2_0`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)

```py
( )
```

ç”¨äºå®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å¤„ç†å™¨ï¼ˆå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ PyTorch 2.0ï¼Œåˆ™é»˜è®¤å¯ç”¨ï¼‰ã€‚

## FusedAttnProcessor2_0

### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)

```py
( )
```

å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å¤„ç†å™¨ï¼ˆå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ PyTorch 2.0ï¼Œåˆ™é»˜è®¤å¯ç”¨ï¼‰ã€‚å®ƒä½¿ç”¨èåˆçš„æŠ•å½±å±‚ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

æ­¤ API ç›®å‰å¤„äºğŸ§ªå®éªŒæ€§è´¨ï¼Œå¯èƒ½ä¼šåœ¨æœªæ¥æ›´æ”¹ã€‚

## LoRAAttnProcessor

### `class diffusers.models.attention_processor.LoRAAttnProcessor`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)

```py
( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha: Optional = None **kwargs )
```

å‚æ•°

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚

+   `rank`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 4ï¼‰â€” LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚

+   `network_alpha`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç­‰åŒäº`alpha`ï¼Œä½†å…¶ç”¨æ³•ç‰¹å®šäº Kohyaï¼ˆA1111ï¼‰é£æ ¼çš„ LoRAsã€‚

+   `kwargs`ï¼ˆ`dict`ï¼‰â€” ä¼ é€’ç»™`LoRALinearLayer`å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

ç”¨äºå®ç° LoRA æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ã€‚

## LoRAAttnProcessor2_0

### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)

```py
( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha: Optional = None **kwargs )
```

å‚æ•°

+   `hidden_size`ï¼ˆ`int`ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚

+   `rank`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 4ï¼‰â€” LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚

+   `network_alpha`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç­‰åŒäº`alpha`ï¼Œä½†å…¶ç”¨æ³•ç‰¹å®šäº Kohyaï¼ˆA1111ï¼‰é£æ ¼çš„ LoRAsã€‚

+   `kwargs`ï¼ˆ`dict`ï¼‰â€” ä¼ é€’ç»™`LoRALinearLayer`å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

ç”¨äºä½¿ç”¨ PyTorch 2.0 çš„å†…å­˜é«˜æ•ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å®ç° LoRA æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ã€‚

## CustomDiffusionAttnProcessor

### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)

```py
( train_kv: bool = True train_q_out: bool = True hidden_size: Optional = None cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0 )
```

å‚æ•°

+   `train_kv`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ–‡æœ¬ç‰¹å¾å¯¹åº”çš„é”®å’Œå€¼çŸ©é˜µã€‚

+   `train_q_out`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ½œåœ¨å›¾åƒç‰¹å¾å¯¹åº”çš„æŸ¥è¯¢çŸ©é˜µã€‚

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚

+   `out_bias`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨`train_q_out`ä¸­åŒ…å«åç½®å‚æ•°ã€‚

+   `dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0ï¼‰â€” è¦ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

ç”¨äºå®ç°è‡ªå®šä¹‰æ‰©æ•£æ–¹æ³•çš„æ³¨æ„åŠ›å¤„ç†å™¨ã€‚

## CustomDiffusionAttnProcessor2_0

### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`

[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)

```py
( train_kv: bool = True train_q_out: bool = True hidden_size: Optional = None cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0 )
```

å‚æ•°

+   `train_kv` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ–°è®­ç»ƒå¯¹åº”äºæ–‡æœ¬ç‰¹å¾çš„é”®å’Œå€¼çŸ©é˜µã€‚

+   `train_q_out` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ–°è®­ç»ƒå¯¹åº”äºæ½œåœ¨å›¾åƒç‰¹å¾çš„æŸ¥è¯¢çŸ©é˜µã€‚

+   `hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” `encoder_hidden_states` ä¸­çš„é€šé“æ•°ã€‚

+   `out_bias` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨ `train_q_out` ä¸­åŒ…å«åç½®å‚æ•°ã€‚

+   `dropout` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” è¦ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

ç”¨äºä½¿ç”¨ PyTorch 2.0 çš„å†…å­˜é«˜æ•ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å®ç°è‡ªå®šä¹‰æ‰©æ•£æ–¹æ³•çš„å¤„ç†å™¨ã€‚

## AttnAddedKVProcessor

### `class diffusers.models.attention_processor.AttnAddedKVProcessor`

[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)

```py
( )
```

ç”¨äºæ‰§è¡Œä¸æ–‡æœ¬ç¼–ç å™¨çš„é¢å¤–å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µç›¸å…³çš„æ³¨æ„åŠ›è®¡ç®—çš„å¤„ç†å™¨ã€‚

## AttnAddedKVProcessor2_0

### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`

[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)

```py
( )
```

ç”¨äºæ‰§è¡Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆå¦‚æœä½¿ç”¨ PyTorch 2.0ï¼Œåˆ™é»˜è®¤å¯ç”¨ï¼‰ï¼Œå…·æœ‰é¢å¤–çš„å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µï¼Œç”¨äºæ–‡æœ¬ç¼–ç å™¨ã€‚

## LoRAAttnAddedKVProcessor

### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`

[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)

```py
( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha: Optional = None )
```

å‚æ•°

+   `hidden_size` (`int`, *å¯é€‰*) â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” `encoder_hidden_states` ä¸­çš„é€šé“æ•°ã€‚

+   `rank` (`int`, é»˜è®¤ä¸º 4) â€” LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚

+   `network_alpha` (`int`, *å¯é€‰*) â€” ç­‰åŒäº `alpha`ï¼Œä½†å…¶ä½¿ç”¨æ–¹å¼ç‰¹å®šäº Kohyaï¼ˆA1111ï¼‰é£æ ¼çš„ LoRAsã€‚

+   `kwargs` (`dict`) â€” ä¼ é€’ç»™ `LoRALinearLayer` å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

ç”¨äºå®ç° LoRA æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ï¼Œå…·æœ‰é¢å¤–çš„å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µï¼Œç”¨äºæ–‡æœ¬ç¼–ç å™¨ã€‚

## XFormersAttnProcessor

### `class diffusers.models.attention_processor.XFormersAttnProcessor`

[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op` (`Callable`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” ä½œä¸ºæ³¨æ„åŠ›æ“ä½œç¬¦ä½¿ç”¨çš„åŸºç¡€[æ“ä½œç¬¦](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)ã€‚å»ºè®®è®¾ç½®ä¸º `None`ï¼Œå¹¶å…è®¸ xFormers é€‰æ‹©æœ€ä½³æ“ä½œç¬¦ã€‚

ç”¨äºä½¿ç”¨ xFormers å®ç°å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚

## LoRAXFormersAttnProcessor

### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`

[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)

```py
( hidden_size: int cross_attention_dim: int rank: int = 4 attention_op: Optional = None network_alpha: Optional = None **kwargs )
```

å‚æ•°

+   `hidden_size` (`int`, *å¯é€‰*) â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim` (`int`, *å¯é€‰*) â€” `encoder_hidden_states` ä¸­çš„é€šé“æ•°ã€‚

+   `rank` (`int`, é»˜è®¤ä¸º 4) â€” LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚

+   `attention_op` (`Callable`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” ä½œä¸ºæ³¨æ„åŠ›æ“ä½œç¬¦ä½¿ç”¨çš„åŸºç¡€[æ“ä½œç¬¦](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)ã€‚å»ºè®®è®¾ç½®ä¸º `None`ï¼Œå¹¶å…è®¸ xFormers é€‰æ‹©æœ€ä½³æ“ä½œç¬¦ã€‚

+   `network_alpha` (`int`, *å¯é€‰*) â€” ç­‰åŒäº `alpha`ï¼Œä½†å…¶ä½¿ç”¨æ–¹å¼ç‰¹å®šäº Kohyaï¼ˆA1111ï¼‰é£æ ¼çš„ LoRAsã€‚

+   `kwargs`ï¼ˆ`dict`ï¼‰â€” ä¼ é€’ç»™`LoRALinearLayer`å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

å®ç° LoRA æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ï¼Œä½¿ç”¨ xFormers å®ç°å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›ã€‚

## CustomDiffusionXFormersAttnProcessor

### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)

```py
( train_kv: bool = True train_q_out: bool = False hidden_size: Optional = None cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0 attention_op: Optional = None )
```

å‚æ•°

+   `train_kv`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ–‡æœ¬ç‰¹å¾å¯¹åº”çš„é”®å’Œå€¼çŸ©é˜µã€‚

+   `train_q_out`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ½œåœ¨å›¾åƒç‰¹å¾å¯¹åº”çš„æŸ¥è¯¢çŸ©é˜µã€‚

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚

+   `cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚

+   `out_bias`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨`train_q_out`ä¸­åŒ…å«åç½®å‚æ•°ã€‚

+   `dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0ï¼‰â€” è¦ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `attention_op`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” ç”¨ä½œæ³¨æ„åŠ›æ“ä½œå™¨çš„åŸºç¡€[operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)ã€‚å»ºè®®è®¾ç½®ä¸º`None`ï¼Œå¹¶å…è®¸ xFormers é€‰æ‹©æœ€ä½³æ“ä½œå™¨ã€‚

å®ç°ä½¿ç”¨ xFormers è¿›è¡Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›çš„å¤„ç†å™¨ï¼Œç”¨äºè‡ªå®šä¹‰æ‰©æ•£æ–¹æ³•ã€‚

## SlicedAttnProcessor

### `class diffusers.models.attention_processor.SlicedAttnProcessor`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)

```py
( slice_size: int )
```

å‚æ•°

+   `slice_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è®¡ç®—æ³¨æ„åŠ›çš„æ­¥æ•°ã€‚ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚

å®ç°åˆ‡ç‰‡æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚

## SlicedAttnAddedKVProcessor

### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)

```py
( slice_size )
```

å‚æ•°

+   `slice_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è®¡ç®—æ³¨æ„åŠ›çš„æ­¥æ•°ã€‚ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚

å®ç°å¸¦æœ‰é¢å¤–å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µçš„åˆ‡ç‰‡æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚
