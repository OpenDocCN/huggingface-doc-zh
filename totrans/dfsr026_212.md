# 注意力处理器

> 原始文本：[`huggingface.co/docs/diffusers/api/attnprocessor`](https://huggingface.co/docs/diffusers/api/attnprocessor)

注意力处理器是一个应用不同类型注意力机制的类。

## AttnProcessor

### `class diffusers.models.attention_processor.AttnProcessor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)

```py
( )
```

执行与注意力相关计算的默认处理器。

## AttnProcessor2_0

`class diffusers.models.attention_processor.AttnProcessor2_0`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)

```py
( )
```

用于实现缩放点积注意力的处理器（如果您使用的是 PyTorch 2.0，则默认启用）。

## FusedAttnProcessor2_0

### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)

```py
( )
```

实现缩放点积注意力的处理器（如果您使用的是 PyTorch 2.0，则默认启用）。它使用融合的投影层。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 目前处于🧪实验性质，可能会在未来更改。

## LoRAAttnProcessor

### `class diffusers.models.attention_processor.LoRAAttnProcessor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)

```py
( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha: Optional = None **kwargs )
```

参数

+   `hidden_size`（`int`，*可选*）— 注意力层的隐藏大小。

+   `cross_attention_dim`（`int`，*可选*）— `encoder_hidden_states`中的通道数。

+   `rank`（`int`，默认为 4）— LoRA 更新矩阵的维度。

+   `network_alpha`（`int`，*可选*）— 等同于`alpha`，但其用法特定于 Kohya（A1111）风格的 LoRAs。

+   `kwargs`（`dict`）— 传递给`LoRALinearLayer`层的额外关键字参数。

用于实现 LoRA 注意力机制的处理器。

## LoRAAttnProcessor2_0

### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)

```py
( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha: Optional = None **kwargs )
```

参数

+   `hidden_size`（`int`）— 注意力层的隐藏大小。

+   `cross_attention_dim`（`int`，*可选*）— `encoder_hidden_states`中的通道数。

+   `rank`（`int`，默认为 4）— LoRA 更新矩阵的维度。

+   `network_alpha`（`int`，*可选*）— 等同于`alpha`，但其用法特定于 Kohya（A1111）风格的 LoRAs。

+   `kwargs`（`dict`）— 传递给`LoRALinearLayer`层的额外关键字参数。

用于使用 PyTorch 2.0 的内存高效缩放点积注意力实现 LoRA 注意力机制的处理器。

## CustomDiffusionAttnProcessor

### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)

```py
( train_kv: bool = True train_q_out: bool = True hidden_size: Optional = None cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0 )
```

参数

+   `train_kv`（`bool`，默认为`True`）— 是否新训练与文本特征对应的键和值矩阵。

+   `train_q_out`（`bool`，默认为`True`）— 是否新训练与潜在图像特征对应的查询矩阵。

+   `hidden_size`（`int`，*可选*，默认为`None`）— 注意力层的隐藏大小。

+   `cross_attention_dim`（`int`，*可选*，默认为`None`）— `encoder_hidden_states`中的通道数。

+   `out_bias`（`bool`，默认为`True`）— 是否在`train_q_out`中包含偏置参数。

+   `dropout`（`float`，*可选*，默认为 0.0）— 要使用的丢弃概率。

用于实现自定义扩散方法的注意力处理器。

## CustomDiffusionAttnProcessor2_0

### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)

```py
( train_kv: bool = True train_q_out: bool = True hidden_size: Optional = None cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0 )
```

参数

+   `train_kv` (`bool`, 默认为 `True`) — 是否新训练对应于文本特征的键和值矩阵。

+   `train_q_out` (`bool`, 默认为 `True`) — 是否新训练对应于潜在图像特征的查询矩阵。

+   `hidden_size` (`int`, *可选*, 默认为 `None`) — 注意力层的隐藏大小。

+   `cross_attention_dim` (`int`, *可选*, 默认为 `None`) — `encoder_hidden_states` 中的通道数。

+   `out_bias` (`bool`, 默认为 `True`) — 是否在 `train_q_out` 中包含偏置参数。

+   `dropout` (`float`, *可选*, 默认为 0.0) — 要使用的丢弃概率。

用于使用 PyTorch 2.0 的内存高效缩放点积注意力实现自定义扩散方法的处理器。

## AttnAddedKVProcessor

### `class diffusers.models.attention_processor.AttnAddedKVProcessor`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)

```py
( )
```

用于执行与文本编码器的额外可学习键和值矩阵相关的注意力计算的处理器。

## AttnAddedKVProcessor2_0

### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)

```py
( )
```

用于执行缩放点积注意力（如果使用 PyTorch 2.0，则默认启用），具有额外的可学习键和值矩阵，用于文本编码器。

## LoRAAttnAddedKVProcessor

### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)

```py
( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha: Optional = None )
```

参数

+   `hidden_size` (`int`, *可选*) — 注意力层的隐藏大小。

+   `cross_attention_dim` (`int`, *可选*, 默认为 `None`) — `encoder_hidden_states` 中的通道数。

+   `rank` (`int`, 默认为 4) — LoRA 更新矩阵的维度。

+   `network_alpha` (`int`, *可选*) — 等同于 `alpha`，但其使用方式特定于 Kohya（A1111）风格的 LoRAs。

+   `kwargs` (`dict`) — 传递给 `LoRALinearLayer` 层的额外关键字参数。

用于实现 LoRA 注意力机制的处理器，具有额外的可学习键和值矩阵，用于文本编码器。

## XFormersAttnProcessor

### `class diffusers.models.attention_processor.XFormersAttnProcessor`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *可选*, 默认为 `None`) — 作为注意力操作符使用的基础[操作符](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)。建议设置为 `None`，并允许 xFormers 选择最佳操作符。

用于使用 xFormers 实现内存高效注意力的处理器。

## LoRAXFormersAttnProcessor

### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)

```py
( hidden_size: int cross_attention_dim: int rank: int = 4 attention_op: Optional = None network_alpha: Optional = None **kwargs )
```

参数

+   `hidden_size` (`int`, *可选*) — 注意力层的隐藏大小。

+   `cross_attention_dim` (`int`, *可选*) — `encoder_hidden_states` 中的通道数。

+   `rank` (`int`, 默认为 4) — LoRA 更新矩阵的维度。

+   `attention_op` (`Callable`, *可选*, 默认为 `None`) — 作为注意力操作符使用的基础[操作符](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)。建议设置为 `None`，并允许 xFormers 选择最佳操作符。

+   `network_alpha` (`int`, *可选*) — 等同于 `alpha`，但其使用方式特定于 Kohya（A1111）风格的 LoRAs。

+   `kwargs`（`dict`）— 传递给`LoRALinearLayer`层的额外关键字参数。

实现 LoRA 注意力机制的处理器，使用 xFormers 实现内存高效的注意力。

## CustomDiffusionXFormersAttnProcessor

### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)

```py
( train_kv: bool = True train_q_out: bool = False hidden_size: Optional = None cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0 attention_op: Optional = None )
```

参数

+   `train_kv`（`bool`，默认为`True`）— 是否新训练与文本特征对应的键和值矩阵。

+   `train_q_out`（`bool`，默认为`True`）— 是否新训练与潜在图像特征对应的查询矩阵。

+   `hidden_size`（`int`，*可选*，默认为`None`）— 注意力层的隐藏大小。

+   `cross_attention_dim`（`int`，*可选*，默认为`None`）— `encoder_hidden_states`中的通道数。

+   `out_bias`（`bool`，默认为`True`）— 是否在`train_q_out`中包含偏置参数。

+   `dropout`（`float`，*可选*，默认为 0.0）— 要使用的丢弃概率。

+   `attention_op`（`Callable`，*可选*，默认为`None`）— 用作注意力操作器的基础[operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)。建议设置为`None`，并允许 xFormers 选择最佳操作器。

实现使用 xFormers 进行内存高效注意力的处理器，用于自定义扩散方法。

## SlicedAttnProcessor

### `class diffusers.models.attention_processor.SlicedAttnProcessor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)

```py
( slice_size: int )
```

参数

+   `slice_size`（`int`，*可选*）— 计算注意力的步数。使用`attention_head_dim // slice_size`个切片，`attention_head_dim`必须是`slice_size`的倍数。

实现切片注意力的处理器。

## SlicedAttnAddedKVProcessor

### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)

```py
( slice_size )
```

参数

+   `slice_size`（`int`，*可选*）— 计算注意力的步数。使用`attention_head_dim // slice_size`个切片，`attention_head_dim`必须是`slice_size`的倍数。

实现带有额外可学习键和值矩阵的切片注意力的处理器。
