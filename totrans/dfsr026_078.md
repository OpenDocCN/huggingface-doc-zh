# 减少内存使用

> 原始文本：[https://huggingface.co/docs/diffusers/optimization/memory](https://huggingface.co/docs/diffusers/optimization/memory)

使用扩散模型的障碍是所需的大量内存。为了克服这一挑战，您可以使用几种减少内存的技术，甚至可以在免费或消费级GPU上运行一些最大的模型。有些技术甚至可以结合使用以进一步减少内存使用。

在许多情况下，优化内存或速度会导致另一个方面的性能提高，因此您应该尽可能同时优化两者。本指南侧重于最小化内存使用，但您也可以了解有关如何[加速推断](fp16)的更多信息。

以下结果是从在Nvidia Titan RTX上使用50个DDIM步骤从一个在火星上骑马的宇航员的照片生成单个512x512图像获得的，展示了由于减少内存消耗而可以期望的加速。

|  | 延迟 | 加速 |
| --- | --- | --- |
| 原始 | 9.50秒 | x1 |
| fp16 | 3.61秒 | x2.63 |
| 通道最后 | 3.30秒 | x2.88 |
| 追踪的UNet | 3.21秒 | x2.96 |
| 内存高效注意力 | 2.63秒 | x3.61 |

## 切片的VAE

切片的VAE通过逐个解码潜在的批量图像或包含32张或更多图像的批次来解码大批量图像，从而使有限的VRAM或批次处理更容易。如果您安装了xFormers，您可能还想将其与[enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)结合使用以进一步减少内存使用。

要使用切片的VAE，请在推断之前在您的管道上调用[enable_vae_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.enable_vae_slicing)：

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_vae_slicing()
#pipe.enable_xformers_memory_efficient_attention()
images = pipe([prompt] * 32).images
```

在多图像批次上进行VAE解码可能会带来轻微的性能提升，并且对于单图像批次不应该有性能影响。

## 平铺的VAE

平铺的VAE处理还可以通过将图像分成重叠的瓦片，解码瓦片，然后将输出混合在一起来组成最终图像，从而在有限的VRAM上处理大图像（例如，在8GB的VRAM上生成4k图像）。如果您安装了xFormers，还应该使用[enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)来进一步减少内存使用。

要使用平铺的VAE处理，请在推断之前在您的管道上调用[enable_vae_tiling()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.enable_vae_tiling)：

```py
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")
prompt = "a beautiful landscape photograph"
pipe.enable_vae_tiling()
#pipe.enable_xformers_memory_efficient_attention()

image = pipe([prompt], width=3840, height=2224, num_inference_steps=20).images[0]
```

输出图像存在一些瓦片之间的色调变化，因为瓦片是分开解码的，但您不应该看到瓦片之间的明显缝隙。对于尺寸为512x512或更小的图像，瓦片功能已关闭。

## CPU卸载

将权重卸载到CPU，并仅在执行前向传递时加载它们到GPU上也可以节省内存。通常，这种技术可以将内存消耗减少到不到3GB。

要执行CPU卸载，请在推断之前在您的管道上调用[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload):

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_sequential_cpu_offload()
image = pipe(prompt).images[0]
```

CPU卸载适用于子模块而不是整个模型。这是最小化内存消耗的最佳方法，但由于扩散过程的迭代性质，推断速度要慢得多。管道的UNet组件运行多次（最多`num_inference_steps`次）；每次，不同的UNet子模块根据需要顺序加载和卸载，导致大量的内存传输。

如果您想要优化速度，考虑使用[model offloading](#model-offloading)，因为它速度更快。权衡是您的内存节省量不会那么大。

在使用[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload)时，不要事先将管道移动到CUDA，否则内存消耗的增益将只是微不足道的（有关更多信息，请参见此[问题](https://github.com/huggingface/diffusers/issues/1934)）。

[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload)是一个有状态的操作，它在模型上安装钩子。

## 模型卸载

模型卸载需要🤗 Accelerate版本0.17.0或更高版本。

[Sequential CPU offloading](#cpu-offloading)保留了大量内存，但使推理速度变慢，因为子模块在需要时移动到GPU，并在新模块运行时立即返回到CPU。

完整模型卸载是一种将整个模型移动到GPU的替代方法，而不是处理每个模型的组成*子模块*。与将管道移动到`cuda`相比，对推理时间几乎没有影响，并且仍然提供一些内存节省。

在模型卸载期间，管道的主要组件之一（通常是文本编码器、UNet和VAE）被放置在GPU上，而其他组件则在CPU上等待。像UNet这样运行多次迭代的组件会一直保留在GPU上，直到不再需要为止。

通过在管道上调用[enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)启用模型卸载：

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_model_cpu_offload()
image = pipe(prompt).images[0]
```

为了在调用模型后正确卸载模型，需要运行整个管道，并按照管道的预期顺序调用模型。如果在安装钩子后在管道上下文之外重用模型，请谨慎行事。有关更多信息，请参见[Removing Hooks](https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.hooks.remove_hook_from_module)。

[enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload) 是一个有状态的操作，它在模型上安装钩子，并在管道上安装状态。

## 通道最后的内存格式

通道最后的内存格式是在内存中对NCHW张量进行排序的另一种方式，以保留维度排序。通道最后的张量是按照通道成为最密集维度的方式排序的（以像素为单位存储图像）。由于目前并非所有运算符都支持通道最后的格式，可能会导致性能较差，但您仍应尝试并查看它是否适用于您的模型。

例如，要将管道的UNet设置为使用通道最后的格式：

```py
print(pipe.unet.conv_out.state_dict()["weight"].stride())  # (2880, 9, 3, 1)
pipe.unet.to(memory_format=torch.channels_last)  # in-place operation
print(
    pipe.unet.conv_out.state_dict()["weight"].stride()
)  # (2880, 1, 960, 320) having a stride of 1 for the 2nd dimension proves that it works
```

## 跟踪

跟踪通过模型的示例输入张量，并捕获在其通过模型的层时执行的操作。返回的可执行文件或`ScriptFunction`经过即时编译进行了优化。

要跟踪一个UNet：

```py
import time
import torch
from diffusers import StableDiffusionPipeline
import functools

# torch disable grad
torch.set_grad_enabled(False)

# set variables
n_experiments = 2
unet_runs_per_experiment = 50

# load inputs
def generate_inputs():
    sample = torch.randn((2, 4, 64, 64), device="cuda", dtype=torch.float16)
    timestep = torch.rand(1, device="cuda", dtype=torch.float16) * 999
    encoder_hidden_states = torch.randn((2, 77, 768), device="cuda", dtype=torch.float16)
    return sample, timestep, encoder_hidden_states

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")
unet = pipe.unet
unet.eval()
unet.to(memory_format=torch.channels_last)  # use channels_last memory format
unet.forward = functools.partial(unet.forward, return_dict=False)  # set return_dict=False as default

# warmup
for _ in range(3):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet(*inputs)

# trace
print("tracing..")
unet_traced = torch.jit.trace(unet, inputs)
unet_traced.eval()
print("done tracing")

# warmup and optimize graph
for _ in range(5):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet_traced(*inputs)

# benchmarking
with torch.inference_mode():
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet_traced(*inputs)
        torch.cuda.synchronize()
        print(f"unet traced inference took {time.time() - start_time:.2f} seconds")
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet(*inputs)
        torch.cuda.synchronize()
        print(f"unet inference took {time.time() - start_time:.2f} seconds")

# save the model
unet_traced.save("unet_traced.pt")
```

用跟踪的模型替换管道的`unet`属性：

```py
from diffusers import StableDiffusionPipeline
import torch
from dataclasses import dataclass

@dataclass
class UNet2DConditionOutput:
    sample: torch.FloatTensor

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

# use jitted unet
unet_traced = torch.jit.load("unet_traced.pt")

# del pipe.unet
class TracedUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.in_channels = pipe.unet.config.in_channels
        self.device = pipe.unet.device

    def forward(self, latent_model_input, t, encoder_hidden_states):
        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]
        return UNet2DConditionOutput(sample=sample)

pipe.unet = TracedUNet()

with torch.inference_mode():
    image = pipe([prompt] * 1, num_inference_steps=50).images[0]
```

## 内存高效的注意力

最近关于在注意力块中优化带宽的工作产生了巨大的加速和GPU内存使用量的减少。最新的内存高效注意力类型是[Flash Attention](https://arxiv.org/abs/2205.14135)（您可以在[HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)查看原始代码）。

如果已安装PyTorch >= 2.0，则在启用`xformers`时不应期望推理速度提升。

使用Flash Attention，安装以下内容：

+   PyTorch > 1.12

+   CUDA可用

+   [xFormers](xformers)

然后在管道上调用[enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)：

```py
from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

pipe.enable_xformers_memory_efficient_attention()

with torch.inference_mode():
    sample = pipe("a small cat")

# optional: You can disable it via
# pipe.disable_xformers_memory_efficient_attention()
```

使用`xformers`时的迭代速度应该与PyTorch 2.0的迭代速度相匹配，如[torch2.0](torch2.0)中所述。
