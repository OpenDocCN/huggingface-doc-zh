- en: Autoscaling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动缩放
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/autoscaling](https://huggingface.co/docs/inference-endpoints/autoscaling)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/inference-endpoints/autoscaling](https://huggingface.co/docs/inference-endpoints/autoscaling)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling allows you to dynamically adjust the number of endpoint replicas
    running your models based on traffic and accelerator utilization. By leveraging
    autoscaling, you can seamlessly handle varying workloads while optimizing costs
    and ensuring high availability.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自动缩放允许您根据流量和加速器利用率动态调整运行模型的端点副本数量。通过利用自动缩放，您可以无缝处理不同的工作负载，同时优化成本并确保高可用性。
- en: Scaling Criteria
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放标准
- en: 'The autoscaling process is triggered based on the accelerator’s utilization
    metrics. The criteria for scaling differ depending on the type of accelerator
    being used:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自动缩放过程是基于加速器利用率指标触发的。根据所使用的加速器类型，缩放的标准有所不同：
- en: '**CPU Accelerators**: A new replica is added when the average CPU utilization
    of all replicas reaches 80%.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU 加速器**：当所有副本的平均 CPU 利用率达到 80% 时，将添加一个新副本。'
- en: '**GPU Accelerators**: A new replica is added when the average GPU utilization
    of all replicas over a 2-minute window reaches 80%.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU 加速器**：当所有副本在 2 分钟窗口内的平均 GPU 利用率达到 80% 时，将添加一个新副本。'
- en: It’s important to note that the scaling up process takes place every minute,
    while the scaling down process takes 2 minutes. This frequency ensures a balance
    between responsiveness and stability of the autoscaling system, with a stabilization
    of 300 seconds once scaled up or down.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，扩展过程每分钟进行一次，而缩减过程需要 2 分钟。这种频率确保了自动缩放系统的响应性和稳定性之间的平衡，在扩展或缩减后稳定时间为 300
    秒。
- en: Considerations for Effective Autoscaling
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效自动缩放的考虑因素
- en: 'While autoscaling offers convenient resource management, certain considerations
    should be kept in mind to ensure its effectiveness:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动缩放提供了方便的资源管理，但应该牢记一些考虑因素以确保其有效性：
- en: '**Model Initialization Time**: During the initialization of a new replica,
    the model is downloaded and loaded into memory. If your replicas have a long initialization
    time, autoscaling may not be as effective. This is because the average GPU utilization
    might fall below the threshold during that time, triggering the automatic scaling
    down of your endpoint.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型初始化时间**：在新副本初始化期间，模型将被下载并加载到内存中。如果您的副本初始化时间较长，则自动缩放可能不太有效。这是因为在此期间，平均 GPU
    利用率可能会低于阈值，从而触发端点的自动缩减。'
- en: '**Enterprise Plan Control**: If you have an [enterprise plan](https://huggingface.co/inference-endpoints/enterprise),
    you have full control over the autoscaling definitions. This allows you to customize
    the scaling thresholds, behavior and criteria based on your specific requirements.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**企业计划控制**：如果您拥有[企业计划](https://huggingface.co/inference-endpoints/enterprise)，您可以完全控制自动缩放定义。这使您可以根据特定要求自定义缩放阈值、行为和标准。'
- en: Scaling to 0
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放至 0
- en: Inference Endpoints also supports autoscaling to 0, which means reducing the
    number of replicas to 0 when there is no incoming traffic. This feature is based
    on request patterns rather than accelerator utilization. When an endpoint remains
    idle without receiving any requests for over 15 minutes, the system automatically
    scales down the endpoint to 0 replicas. To enable the feature, go to the Settings
    page and you’ll find a section called “Automatic Scale-to-Zero”.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 推理端点还支持将副本缩放至 0，这意味着在没有传入流量时将副本数量减少至 0。此功能基于请求模式而不是加速器利用率。当端点保持空闲超过 15 分钟没有收到任何请求时，系统会自动将端点缩减至
    0 个副本。要启用此功能，请转到设置页面，您会找到一个名为“自动缩放至零”的部分。
- en: Scaling to 0 replicas helps optimize cost savings by minimizing resource usage
    during periods of inactivity. However, it’s important to be aware that scaling
    to 0 implies a cold start period when the endpoint receives a new request. Additionally,
    the HTTP server will respond with a status code `502 Bad Gateway` while the new
    replica is initializing. Please note that there is currently no queueing system
    in place for incoming requests. Therefore, we recommend developing your own request
    queue client-side with proper error handling to optimize throughput and latency.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将副本缩放至 0 有助于通过在空闲时期最小化资源使用来优化成本节省。然而，重要的是要意识到，将副本缩放至 0 意味着在端点接收到新请求时会有一个冷启动期。此外，在新副本初始化时，HTTP
    服务器将以状态码 `502 Bad Gateway` 响应。请注意，目前没有针对传入请求的排队系统。因此，我们建议在客户端开发自己的请求队列并进行适当的错误处理，以优化吞吐量和延迟。
- en: The duration of the cold start period varies depending on your model’s size.
    It is recommended to consider the potential latency impact when enabling scaling
    to 0 and managing user expectations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动期的持续时间取决于您的模型大小。建议在启用缩放至 0 和管理用户期望时考虑潜在的延迟影响。
