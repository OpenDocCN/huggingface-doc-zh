["```py\n( text_config = None vision_config = None projection_dim = 256 projection_intermediate_dim = 4096 logit_scale_init_value = 2.6592 **kwargs )\n```", "```py\n( text_config: GroupViTTextConfig vision_config: GroupViTVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';GroupViTConfig\n```", "```py\n( vocab_size = 49408 hidden_size = 256 intermediate_size = 1024 num_hidden_layers = 12 num_attention_heads = 4 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 dropout = 0.0 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )\n```", "```py\n>>> from transformers import GroupViTTextConfig, GroupViTTextModel\n\n>>> # Initializing a GroupViTTextModel with nvidia/groupvit-gcc-yfcc style configuration\n>>> configuration = GroupViTTextConfig()\n\n>>> model = GroupViTTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 384 intermediate_size = 1536 depths = [6, 3, 3] num_hidden_layers = 12 num_group_tokens = [64, 8, 0] num_output_groups = [64, 8, 8] num_attention_heads = 6 image_size = 224 patch_size = 16 num_channels = 3 hidden_act = 'gelu' layer_norm_eps = 1e-05 dropout = 0.0 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 assign_eps = 1.0 assign_mlp_ratio = [0.5, 4] **kwargs )\n```", "```py\n>>> from transformers import GroupViTVisionConfig, GroupViTVisionModel\n\n>>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration\n>>> configuration = GroupViTVisionConfig()\n\n>>> model = GroupViTVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: GroupViTConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_segmentation: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.groupvit.modeling_groupvit.GroupViTModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GroupViTModel\n\n>>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import CLIPTokenizer, GroupViTModel\n\n>>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GroupViTModel\n\n>>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config: GroupViTTextConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import CLIPTokenizer, GroupViTTextModel\n\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: GroupViTVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GroupViTVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n( config: GroupViTConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None pixel_values: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None return_loss: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None output_segmentation: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.groupvit.modeling_tf_groupvit.TFGroupViTModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFGroupViTModel\n>>> import tensorflow as tf\n\n>>> model = TFGroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"tf\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';text_features (tf.Tensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import CLIPTokenizer, TFGroupViTModel\n\n>>> model = TFGroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: TFModelInputType | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';image_features (tf.Tensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFGroupViTModel\n\n>>> model = TFGroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config: GroupViTTextConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import CLIPTokenizer, TFGroupViTTextModel\n\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = TFGroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: GroupViTVisionConfig *inputs **kwargs )\n```", "```py\n( pixel_values: TFModelInputType | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFGroupViTVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = TFGroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]