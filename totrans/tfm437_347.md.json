["```py\n>>> from transformers import GroupViTTextConfig, GroupViTTextModel\n\n>>> # Initializing a GroupViTTextModel with nvidia/groupvit-gcc-yfcc style configuration\n>>> configuration = GroupViTTextConfig()\n\n>>> model = GroupViTTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import GroupViTVisionConfig, GroupViTVisionModel\n\n>>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration\n>>> configuration = GroupViTVisionConfig()\n\n>>> model = GroupViTVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GroupViTModel\n\n>>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import CLIPTokenizer, GroupViTModel\n\n>>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GroupViTModel\n\n>>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import CLIPTokenizer, GroupViTTextModel\n\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GroupViTVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFGroupViTModel\n>>> import tensorflow as tf\n\n>>> model = TFGroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"tf\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import CLIPTokenizer, TFGroupViTModel\n\n>>> model = TFGroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFGroupViTModel\n\n>>> model = TFGroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import CLIPTokenizer, TFGroupViTTextModel\n\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = TFGroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFGroupViTVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n>>> model = TFGroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]