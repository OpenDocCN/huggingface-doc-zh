# éŸ³é¢‘åˆ†ç±»

> åŽŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/audio_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/audio_classification)

[https://www.youtube-nocookie.com/embed/KWwzcmG98Ds](https://www.youtube-nocookie.com/embed/KWwzcmG98Ds)

éŸ³é¢‘åˆ†ç±» - å°±åƒæ–‡æœ¬ä¸€æ · - ä»Žè¾“å…¥æ•°æ®ä¸­åˆ†é…ä¸€ä¸ªç±»æ ‡ç­¾è¾“å‡ºã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼Œæ‚¨æœ‰åŽŸå§‹éŸ³é¢‘æ³¢å½¢è€Œä¸æ˜¯æ–‡æœ¬è¾“å…¥ã€‚éŸ³é¢‘åˆ†ç±»çš„ä¸€äº›å®žé™…åº”ç”¨åŒ…æ‹¬è¯†åˆ«è¯´è¯è€…æ„å›¾ã€è¯­è¨€åˆ†ç±»ï¼Œç”šè‡³é€šè¿‡å£°éŸ³è¯†åˆ«åŠ¨ç‰©ç‰©ç§ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1.  åœ¨[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ä¸Šå¯¹[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)è¿›è¡Œå¾®è°ƒï¼Œä»¥åˆ†ç±»è¯´è¯è€…æ„å›¾ã€‚

1.  ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­ã€‚

æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡åž‹æž¶æž„æ”¯æŒï¼š

[éŸ³é¢‘é¢‘è°±å˜æ¢å™¨](../model_doc/audio-spectrogram-transformer)ã€[Data2VecAudio](../model_doc/data2vec-audio)ã€[Hubert](../model_doc/hubert)ã€[SEW](../model_doc/sew)ã€[SEW-D](../model_doc/sew-d)ã€[UniSpeech](../model_doc/unispeech)ã€[UniSpeechSat](../model_doc/unispeech-sat)ã€[Wav2Vec2](../model_doc/wav2vec2)ã€[Wav2Vec2-BERT](../model_doc/wav2vec2-bert)ã€[Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer)ã€[WavLM](../model_doc/wavlm)ã€[Whisper](../model_doc/whisper)

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```py
pip install transformers datasets evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œåˆ†äº«æ‚¨çš„æ¨¡åž‹ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½MInDS-14æ•°æ®é›†

é¦–å…ˆä»ŽðŸ¤—æ•°æ®é›†åº“ä¸­åŠ è½½MInDS-14æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

ä½¿ç”¨[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)æ–¹æ³•å°†æ•°æ®é›†çš„`train`æ‹†åˆ†ä¸ºè¾ƒå°çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®žéªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åŽå†èŠ±æ›´å¤šæ—¶é—´å¤„ç†å®Œæ•´æ•°æ®é›†ã€‚

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

ç„¶åŽæŸ¥çœ‹æ•°æ®é›†ï¼š

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 450
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 113
    })
})
```

è™½ç„¶æ•°æ®é›†åŒ…å«è®¸å¤šæœ‰ç”¨ä¿¡æ¯ï¼Œæ¯”å¦‚`lang_id`å’Œ`english_transcription`ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä¸“æ³¨äºŽ`audio`å’Œ`intent_class`ã€‚ä½¿ç”¨[remove_columns](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.remove_columns)æ–¹æ³•åˆ é™¤å…¶ä»–åˆ—ï¼š

```py
>>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])
```

çŽ°åœ¨çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> minds["train"][0]
{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,
         -0.00024414, -0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 8000},
 'intent_class': 2}
```

æœ‰ä¸¤ä¸ªé¢†åŸŸï¼š

+   `audio`ï¼šå¿…é¡»è°ƒç”¨çš„è¯­éŸ³ä¿¡å·çš„ä¸€ç»´`array`ï¼Œä»¥åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚

+   `intent_class`ï¼šè¡¨ç¤ºè¯´è¯è€…æ„å›¾çš„ç±»åˆ«IDã€‚

ä¸ºäº†è®©æ¨¡åž‹æ›´å®¹æ˜“ä»Žæ ‡ç­¾IDä¸­èŽ·å–æ ‡ç­¾åç§°ï¼Œåˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾åç§°æ˜ å°„åˆ°æ•´æ•°ä»¥åŠåä¹‹çš„å­—å…¸ï¼š

```py
>>> labels = minds["train"].features["intent_class"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

çŽ°åœ¨æ‚¨å¯ä»¥å°†æ ‡ç­¾IDè½¬æ¢ä¸ºæ ‡ç­¾åç§°ï¼š

```py
>>> id2label[str(2)]
'app_error'
```

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½Wav2Vec2ç‰¹å¾æå–å™¨æ¥å¤„ç†éŸ³é¢‘ä¿¡å·ï¼š

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14æ•°æ®é›†çš„é‡‡æ ·çŽ‡ä¸º8000khzï¼ˆæ‚¨å¯ä»¥åœ¨å…¶[æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/PolyAI/minds14)ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ï¼‰ï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦å°†æ•°æ®é›†é‡æ–°é‡‡æ ·ä¸º16000kHzä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡åž‹ï¼š

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,
         -2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 16000},
 'intent_class': 2}
```

çŽ°åœ¨åˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¯¥å‡½æ•°ï¼š

1.  è°ƒç”¨`audio`åˆ—è¿›è¡ŒåŠ è½½ï¼Œå¹¶åœ¨å¿…è¦æ—¶é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚

1.  æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·çŽ‡æ˜¯å¦ä¸Žæ¨¡åž‹é¢„è®­ç»ƒæ—¶çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·çŽ‡åŒ¹é…ã€‚æ‚¨å¯ä»¥åœ¨Wav2Vec2çš„[æ¨¡åž‹å¡ç‰‡](https://huggingface.co/facebook/wav2vec2-base)ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ã€‚

1.  è®¾ç½®æœ€å¤§è¾“å…¥é•¿åº¦ä»¥æ‰¹å¤„ç†æ›´é•¿çš„è¾“å…¥è€Œä¸æˆªæ–­å®ƒä»¬ã€‚

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œè¯·ä½¿ç”¨ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®`batched=True`æ¥åŠ é€Ÿ`map`ï¼Œä»¥ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ã€‚åˆ é™¤æ‚¨ä¸éœ€è¦çš„åˆ—ï¼Œå¹¶å°†`intent_class`é‡å‘½åä¸º`label`ï¼Œå› ä¸ºè¿™æ˜¯æ¨¡åž‹æœŸæœ›çš„åç§°ï¼š

```py
>>> encoded_minds = minds.map(preprocess_function, remove_columns="audio", batched=True)
>>> encoded_minds = encoded_minds.rename_column("intent_class", "label")
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªåº¦é‡é€šå¸¸æœ‰åŠ©äºŽè¯„ä¼°æ¨¡åž‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥é€šè¿‡ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºŽè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)åº¦é‡ï¼ˆæŸ¥çœ‹ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡ï¼‰ï¼š

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

ç„¶åŽåˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™`compute`ä»¥è®¡ç®—å‡†ç¡®æ€§ï¼š

```py
>>> import numpy as np

>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
```

æ‚¨çš„`compute_metrics`å‡½æ•°çŽ°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶å°†è¿”å›žåˆ°å®ƒã€‚

## è®­ç»ƒ

Pytorchéšè— Pytorchå†…å®¹

å¦‚æžœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¾®è°ƒæ¨¡åž‹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[../training#train-with-pytorch-trainer]ï¼

çŽ°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡åž‹äº†ï¼ä½¿ç”¨[AutoModelForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForAudioClassification)åŠ è½½Wav2Vec2ï¼Œä»¥åŠé¢„æœŸæ ‡ç­¾çš„æ•°é‡å’Œæ ‡ç­¾æ˜ å°„ï¼š

```py
>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base", num_labels=num_labels, label2id=label2id, id2label=id2label
... )
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1.  åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡åž‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡åž‹æŽ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•Hugging Faceæ‰èƒ½ä¸Šä¼ æ¨¡åž‹ï¼‰ã€‚åœ¨æ¯ä¸ªæ—¶ä»£ç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è¯„ä¼°å‡†ç¡®æ€§å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚

1.  å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡åž‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚

1.  è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡åž‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model",
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=3e-5,
...     per_device_train_batch_size=32,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=32,
...     num_train_epochs=10,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=feature_extractor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®ŒæˆåŽï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡åž‹å…±äº«åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡åž‹ï¼š

```py
>>> trainer.push_to_hub()
```

è¦äº†è§£å¦‚ä½•ä¸ºéŸ³é¢‘åˆ†ç±»å¾®è°ƒæ¨¡åž‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorchç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)ã€‚

## æŽ¨ç†

çŽ°åœ¨ï¼Œæ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡åž‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡ŒæŽ¨ç†äº†ï¼

åŠ è½½æ‚¨æƒ³è¦è¿›è¡ŒæŽ¨ç†çš„éŸ³é¢‘æ–‡ä»¶ã€‚è®°å¾—é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·çŽ‡ï¼Œä»¥åŒ¹é…æ¨¡åž‹çš„é‡‡æ ·çŽ‡ï¼ˆå¦‚æžœéœ€è¦ï¼‰ï¼

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

å°è¯•ä½¿ç”¨ä¸€ä¸ª[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ¥è¿›è¡ŒæŽ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨å…¶ä¸­ä½¿ç”¨å¾®è°ƒåŽçš„æ¨¡åž‹ã€‚ä½¿ç”¨æ‚¨çš„æ¨¡åž‹å®žä¾‹åŒ–ä¸€ä¸ªç”¨äºŽéŸ³é¢‘åˆ†ç±»çš„`pipeline`ï¼Œå¹¶å°†éŸ³é¢‘æ–‡ä»¶ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
[
    {'score': 0.09766869246959686, 'label': 'cash_deposit'},
    {'score': 0.07998877018690109, 'label': 'app_error'},
    {'score': 0.0781070664525032, 'label': 'joint_account'},
    {'score': 0.07667109370231628, 'label': 'pay_bill'},
    {'score': 0.0755252093076706, 'label': 'balance'}
]
```

å¦‚æžœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æžœï¼š

Pytorchéšè— Pytorchå†…å®¹

åŠ è½½ä¸€ä¸ªç‰¹å¾æå–å™¨æ¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶å°†`input`è¿”å›žä¸ºPyTorchå¼ é‡ï¼š

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡åž‹å¹¶è¿”å›žlogitsï¼š

```py
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

èŽ·å–å…·æœ‰æœ€é«˜æ¦‚çŽ‡çš„ç±»ï¼Œå¹¶ä½¿ç”¨æ¨¡åž‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š

```py
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'cash_deposit'
```
