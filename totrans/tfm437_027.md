# 音频分类

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/audio_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/audio_classification)

[https://www.youtube-nocookie.com/embed/KWwzcmG98Ds](https://www.youtube-nocookie.com/embed/KWwzcmG98Ds)

音频分类 - 就像文本一样 - 从输入数据中分配一个类标签输出。唯一的区别是，您有原始音频波形而不是文本输入。音频分类的一些实际应用包括识别说话者意图、语言分类，甚至通过声音识别动物物种。

本指南将向您展示如何：

1.  在[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)数据集上对[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)进行微调，以分类说话者意图。

1.  使用您微调的模型进行推断。

本教程中所示的任务由以下模型架构支持：

[音频频谱变换器](../model_doc/audio-spectrogram-transformer)、[Data2VecAudio](../model_doc/data2vec-audio)、[Hubert](../model_doc/hubert)、[SEW](../model_doc/sew)、[SEW-D](../model_doc/sew-d)、[UniSpeech](../model_doc/unispeech)、[UniSpeechSat](../model_doc/unispeech-sat)、[Wav2Vec2](../model_doc/wav2vec2)、[Wav2Vec2-BERT](../model_doc/wav2vec2-bert)、[Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer)、[WavLM](../model_doc/wavlm)、[Whisper](../model_doc/whisper)

在开始之前，请确保已安装所有必要的库：

```py
pip install transformers datasets evaluate
```

我们鼓励您登录您的Hugging Face帐户，这样您就可以上传和分享您的模型给社区。在提示时，输入您的令牌以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## 加载MInDS-14数据集

首先从🤗数据集库中加载MInDS-14数据集：

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

使用[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)方法将数据集的`train`拆分为较小的训练集和测试集。这将让您有机会进行实验，并确保一切正常，然后再花更多时间处理完整数据集。

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

然后查看数据集：

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 450
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 113
    })
})
```

虽然数据集包含许多有用信息，比如`lang_id`和`english_transcription`，但在本指南中，您将专注于`audio`和`intent_class`。使用[remove_columns](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.remove_columns)方法删除其他列：

```py
>>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])
```

现在看一个示例：

```py
>>> minds["train"][0]
{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,
         -0.00024414, -0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 8000},
 'intent_class': 2}
```

有两个领域：

+   `audio`：必须调用的语音信号的一维`array`，以加载和重新采样音频文件。

+   `intent_class`：表示说话者意图的类别ID。

为了让模型更容易从标签ID中获取标签名称，创建一个将标签名称映射到整数以及反之的字典：

```py
>>> labels = minds["train"].features["intent_class"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

现在您可以将标签ID转换为标签名称：

```py
>>> id2label[str(2)]
'app_error'
```

## 预处理

下一步是加载Wav2Vec2特征提取器来处理音频信号：

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14数据集的采样率为8000khz（您可以在其[数据集卡片](https://huggingface.co/datasets/PolyAI/minds14)中找到此信息），这意味着您需要将数据集重新采样为16000kHz以使用预训练的Wav2Vec2模型：

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,
         -2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 16000},
 'intent_class': 2}
```

现在创建一个预处理函数，该函数：

1.  调用`audio`列进行加载，并在必要时重新采样音频文件。

1.  检查音频文件的采样率是否与模型预训练时的音频数据的采样率匹配。您可以在Wav2Vec2的[模型卡片](https://huggingface.co/facebook/wav2vec2-base)中找到此信息。

1.  设置最大输入长度以批处理更长的输入而不截断它们。

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
```

要在整个数据集上应用预处理函数，请使用🤗 Datasets [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)函数。您可以通过设置`batched=True`来加速`map`，以一次处理数据集的多个元素。删除您不需要的列，并将`intent_class`重命名为`label`，因为这是模型期望的名称：

```py
>>> encoded_minds = minds.map(preprocess_function, remove_columns="audio", batched=True)
>>> encoded_minds = encoded_minds.rename_column("intent_class", "label")
```

## 评估

在训练过程中包含一个度量通常有助于评估模型的性能。您可以通过🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载一个评估方法。对于这个任务，加载[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)度量（查看🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算度量）：

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

然后创建一个函数，将您的预测和标签传递给`compute`以计算准确性：

```py
>>> import numpy as np

>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
```

您的`compute_metrics`函数现在已经准备就绪，当您设置训练时将返回到它。

## 训练

Pytorch隐藏 Pytorch内容

如果您不熟悉使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)微调模型，请查看这里的基本教程[../training#train-with-pytorch-trainer]！

现在您已经准备好开始训练您的模型了！使用[AutoModelForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForAudioClassification)加载Wav2Vec2，以及预期标签的数量和标签映射：

```py
>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base", num_labels=num_labels, label2id=label2id, id2label=id2label
... )
```

此时，只剩下三个步骤：

1.  在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中定义您的训练超参数。唯一必需的参数是`output_dir`，指定保存模型的位置。通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging Face才能上传模型）。在每个时代结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估准确性并保存训练检查点。

1.  将训练参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，以及模型、数据集、分词器、数据整理器和`compute_metrics`函数。

1.  调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model",
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=3e-5,
...     per_device_train_batch_size=32,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=32,
...     num_train_epochs=10,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=feature_extractor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，这样每个人都可以使用您的模型：

```py
>>> trainer.push_to_hub()
```

要了解如何为音频分类微调模型的更深入示例，请查看相应的[PyTorch笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)。

## 推理

现在，您已经微调了一个模型，可以用它进行推理了！

加载您想要进行推理的音频文件。记得重新采样音频文件的采样率，以匹配模型的采样率（如果需要）！

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

尝试使用一个[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)来进行推理的最简单方法是在其中使用微调后的模型。使用您的模型实例化一个用于音频分类的`pipeline`，并将音频文件传递给它：

```py
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
[
    {'score': 0.09766869246959686, 'label': 'cash_deposit'},
    {'score': 0.07998877018690109, 'label': 'app_error'},
    {'score': 0.0781070664525032, 'label': 'joint_account'},
    {'score': 0.07667109370231628, 'label': 'pay_bill'},
    {'score': 0.0755252093076706, 'label': 'balance'}
]
```

如果您愿意，也可以手动复制`pipeline`的结果：

Pytorch隐藏 Pytorch内容

加载一个特征提取器来预处理音频文件，并将`input`返回为PyTorch张量：

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

将您的输入传递给模型并返回logits：

```py
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

获取具有最高概率的类，并使用模型的`id2label`映射将其转换为标签：

```py
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'cash_deposit'
```
