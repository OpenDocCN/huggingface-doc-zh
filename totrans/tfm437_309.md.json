["```py\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\n\nmodel_id = \"facebook/mms-1b-all\"\ntarget_lang = \"fra\"\n\nprocessor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True)\n```", "```py\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([314]) in the model instantiated\n- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([314, 1280]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```", "```py\nfrom transformers import pipeline\n\nmodel_id = \"facebook/mms-1b-all\"\ntarget_lang = \"fra\"\n\npipe = pipeline(model=model_id, model_kwargs={\"target_lang\": \"fra\", \"ignore_mismatched_sizes\": True})\n```", "```py\nfrom datasets import load_dataset, Audio\n\n# English\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n\n# French\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"fr\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nfr_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n```", "```py\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\nimport torch\n\nmodel_id = \"facebook/mms-1b-all\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id)\n```", "```py\ninputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n# 'joe keton disapproved of films and buster also had reservations about the media'\n```", "```py\nprocessor.tokenizer.set_target_lang(\"fra\")\nmodel.load_adapter(\"fra\")\n\ninputs = processor(fr_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n# \"ce dernier est vol\u00e9 tout au long de l'histoire romaine\"\n```", "```py\nprocessor.tokenizer.vocab.keys()\n```", "```py\npip install --upgrade transformers accelerate\n```", "```py\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n\ninputs = tokenizer(text=\"Hello - my dog is cute\", return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\n\nwith torch.no_grad():\n   outputs = model(**inputs)\n\nwaveform = outputs.waveform[0]\n```", "```py\nimport scipy\n\nscipy.io.wavfile.write(\"synthesized_speech.wav\", rate=model.config.sampling_rate, data=waveform)\n```", "```py\nfrom IPython.display import Audio\n\nAudio(waveform, rate=model.config.sampling_rate)\n```", "```py\nfrom transformers import VitsTokenizer\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nprint(tokenizer.is_uroman)\n```", "```py\ngit clone https://github.com/isi-nlp/uroman.git\ncd uroman\nexport UROMAN=$(pwd)\n```", "```py\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\nimport os\nimport subprocess\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-kor\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-kor\")\n\ndef uromanize(input_string, uroman_path):\n    \"\"\"Convert non-Roman strings to Roman using the `uroman` perl package.\"\"\"\n    script_path = os.path.join(uroman_path, \"bin\", \"uroman.pl\")\n\n    command = [\"perl\", script_path]\n\n    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    # Execute the perl command\n    stdout, stderr = process.communicate(input=input_string.encode())\n\n    if process.returncode != 0:\n        raise ValueError(f\"Error {process.returncode}: {stderr.decode()}\")\n\n    # Return the output as a string and skip the new-line character at the end\n    return stdout.decode()[:-1]\n\ntext = \"\uc774\ubd10 \ubb34\uc2a8 \uc77c\uc774\uc57c\"\nuromaized_text = uromanize(text, uroman_path=os.environ[\"UROMAN\"])\n\ninputs = tokenizer(text=uromaized_text, return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\nwith torch.no_grad():\n   outputs = model(inputs[\"input_ids\"])\n\nwaveform = outputs.waveform[0]\n```", "```py\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n\ninputs = tokenizer(text=\"Hello - my dog is cute\", return_tensors=\"pt\")\n\n# make deterministic\nset_seed(555)  \n\n# make speech faster and more noisy\nmodel.speaking_rate = 1.5\nmodel.noise_scale = 0.8\n\nwith torch.no_grad():\n   outputs = model(**inputs)\n```", "```py\npip install torch accelerate datasets[audio]\npip install --upgrade transformers\n```", "```py\nfrom datasets import load_dataset, Audio\n\n# English\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n\n# Arabic\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ar\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nar_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n```", "```py\nfrom transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\nimport torch\n\nmodel_id = \"facebook/mms-lid-126\"\n\nprocessor = AutoFeatureExtractor.from_pretrained(model_id)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n```", "```py\n# English\ninputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\nlang_id = torch.argmax(outputs, dim=-1)[0].item()\ndetected_lang = model.config.id2label[lang_id]\n# 'eng'\n\n# Arabic\ninputs = processor(ar_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\nlang_id = torch.argmax(outputs, dim=-1)[0].item()\ndetected_lang = model.config.id2label[lang_id]\n# 'ara'\n```", "```py\nprocessor.id2label.values()\n```"]