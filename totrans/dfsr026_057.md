# 无条件图像生成

> 原始文本：[https://huggingface.co/docs/diffusers/training/unconditional_training](https://huggingface.co/docs/diffusers/training/unconditional_training)

无条件图像生成模型在训练过程中不受文本或图像的限制。它只生成类似于其训练数据分布的图像。

本指南将探索[train_unconditional.py](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py)训练脚本，帮助您熟悉它，以及如何为您自己的用例进行调整。

在运行脚本之前，请确保您从源代码安装了库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

然后导航到包含训练脚本的示例文件夹，并安装所需的依赖项：

```py
cd examples/unconditional_image_generation
pip install -r requirements.txt
```

🤗 Accelerate是一个帮助您在多个GPU/TPU上训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多信息。

初始化一个🤗 Accelerate环境：

```py
accelerate config
```

要设置一个默认的🤗 Accelerate环境而不选择任何配置：

```py
accelerate config default
```

或者如果您的环境不支持像笔记本这样的交互式shell，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看[创建用于训练的数据集](create_dataset)指南，了解如何创建一个适用于训练脚本的数据集。

## 脚本参数

以下部分突出显示了训练脚本中重要的部分，以便了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py)，并告诉我们如果您有任何问题或疑虑。

训练脚本提供了许多参数，帮助您定制您的训练运行。所有参数及其描述都在[`parse_args()`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L55)函数中找到。它为每个参数提供了默认值，如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。

例如，要使用bf16格式加速混合精度训练，请在训练命令中添加`--mixed_precision`参数：

```py
accelerate launch train_unconditional.py \
  --mixed_precision="bf16"
```

一些基本和重要的要指定的参数包括：

+   `--dataset_name`：Hub上的数据集名称或要训练的本地路径数据集

+   `--output_dir`：保存训练模型的位置

+   `--push_to_hub`：是否将训练好的模型推送到Hub

+   `--checkpointing_steps`：在模型训练过程中保存检查点的频率；如果训练被中断，您可以通过在训练命令中添加`--resume_from_checkpoint`来从该检查点继续训练

带上您的数据集，让训练脚本处理其他一切！

## 训练脚本

用于预处理数据集和训练循环的代码位于[`main()`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L275)函数中。如果您需要调整训练脚本，这就是您需要进行更改的地方。

如果您没有提供模型配置，`train_unconditional`脚本将[初始化一个`UNet2DModel`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L356)。如果您愿意，您可以在这里配置UNet：

```py
model = UNet2DModel(
    sample_size=args.resolution,
    in_channels=3,
    out_channels=3,
    layers_per_block=2,
    block_out_channels=(128, 128, 256, 256, 512, 512),
    down_block_types=(
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",
        "AttnUpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)
```

接下来，脚本初始化一个[调度器](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L418) 和[优化器](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L429)：

```py
# Initialize the scheduler
accepts_prediction_type = "prediction_type" in set(inspect.signature(DDPMScheduler.__init__).parameters.keys())
if accepts_prediction_type:
    noise_scheduler = DDPMScheduler(
        num_train_timesteps=args.ddpm_num_steps,
        beta_schedule=args.ddpm_beta_schedule,
        prediction_type=args.prediction_type,
    )
else:
    noise_scheduler = DDPMScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule)

# Initialize the optimizer
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

然后，它[加载数据集](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L451) ，您可以指定如何[预处理](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L455) 它：

```py
dataset = load_dataset("imagefolder", data_dir=args.train_data_dir, cache_dir=args.cache_dir, split="train")

augmentations = transforms.Compose(
    [
        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),
        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ]
)
```

最后，[训练循环](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L540) 处理其他所有事项，如向图像添加噪声，预测噪声残差，计算损失，保存指定步骤的检查点，并保存并推送模型到 Hub。如果您想了解训练循环的工作原理，请查看[理解管道、模型和调度器](../using-diffusers/write_own_pipeline) 教程，该教程详细解释了去噪过程的基本模式。

## 启动脚本

一旦您完成所有更改或对默认配置满意，您就可以启动训练脚本！🚀

完整的训练运行在4xV100 GPU上需要2小时。

单GPU多GPU

```py
accelerate launch train_unconditional.py \
  --dataset_name="huggan/flowers-102-categories" \
  --output_dir="ddpm-ema-flowers-64" \
  --mixed_precision="fp16" \
  --push_to_hub
```

训练脚本会在您的存储库中创建并保存一个检查点文件。现在您可以加载和使用训练好的模型进行推断：

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("anton-l/ddpm-butterflies-128").to("cuda")
image = pipeline().images[0]
```
