["```py\npip install -U flash-attn --no-build-isolation\n```", "```py\n>>> import torch\n>>> from transformers import OPTForCausalLM, GPT2Tokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n>>> tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n\n>>> prompt = (\"A chat between a curious human and the Statue of Liberty.\\n\\nHuman: What is your name?\\nStatue: I am the \"\n              \"Statue of Liberty.\\nHuman: Where do you live?\\nStatue: New York City.\\nHuman: How long have you lived \"\n              \"there?\")\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n>>> tokenizer.batch_decode(generated_ids)[0]\n'</s>A chat between a curious human and the Statue of Liberty.\\n\\nHuman: What is your name?\\nStatue: I am the Statue of Liberty.\\nHuman: Where do you live?\\nStatue: New York City.\\nHuman: How long have you lived there?\\nStatue: I have lived here for about a year.\\nHuman: What is your favorite place to eat?\\nStatue: I love'\n```", "```py\n( vocab_size = 50272 hidden_size = 768 num_hidden_layers = 12 ffn_dim = 3072 max_position_embeddings = 2048 do_layer_norm_before = True _remove_final_layer_norm = False word_embed_proj_dim = None dropout = 0.1 attention_dropout = 0.0 num_attention_heads = 12 activation_function = 'relu' layerdrop = 0.0 init_std = 0.02 use_cache = True pad_token_id = 1 bos_token_id = 2 eos_token_id = 2 enable_bias = True layer_norm_elementwise_affine = True **kwargs )\n```", "```py\n>>> from transformers import OPTConfig, OPTModel\n\n>>> # Initializing a OPT facebook/opt-large style configuration\n>>> configuration = OPTConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/opt-large style configuration\n>>> model = OPTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: OPTConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, OPTModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n>>> model = OPTModel.from_pretrained(\"facebook/opt-350m\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, OPTForCausalLM\n\n>>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\n>>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious. I'm just a little bit of a weirdo.\"\n```", "```py\n( config: OPTConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OPTForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ArthurZ/opt-350m-dummy-sc\")\n>>> model = OPTForSequenceClassification.from_pretrained(\"ArthurZ/opt-350m-dummy-sc\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'LABEL_0'\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = OPTForSequenceClassification.from_pretrained(\"ArthurZ/opt-350m-dummy-sc\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n1.71\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OPTForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ArthurZ/opt-350m-dummy-sc\")\n>>> model = OPTForSequenceClassification.from_pretrained(\"ArthurZ/opt-350m-dummy-sc\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = OPTForSequenceClassification.from_pretrained(\n...     \"ArthurZ/opt-350m-dummy-sc\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config: OPTConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, OPTForQuestionAnswering\n>>> import torch\n\n>>> torch.manual_seed(4)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\n>>> # note: we are loading a OPTForQuestionAnswering from the hub here,\n>>> # so the head will be randomly initialized, hence the predictions will be random\n>>> model = OPTForQuestionAnswering.from_pretrained(\"facebook/opt-350m\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> answer_offset = len(tokenizer(question)[0])\n\n>>> predict_answer_tokens = inputs.input_ids[\n...     0, answer_offset + answer_start_index : answer_offset + answer_end_index + 1\n... ]\n>>> predicted = tokenizer.decode(predict_answer_tokens)\n>>> predicted\n' a nice puppet'\n```", "```py\n( config: OPTConfig **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFOPTModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n>>> model = TFOPTModel.from_pretrained(\"facebook/opt-350m\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: OPTConfig **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFOPTForCausalLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n>>> model = TFOPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n( config: OPTConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None params: dict = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None dropout_rng: PRNGKey = None deterministic: bool = True ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxOPTModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n>>> model = FlaxOPTModel.from_pretrained(\"facebook/opt-350m\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: OPTConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None params: dict = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None dropout_rng: PRNGKey = None deterministic: bool = True ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxOPTForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n>>> model = FlaxOPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```"]