# 模型

> 原文：[https://huggingface.co/docs/trl/models](https://huggingface.co/docs/trl/models)

通过`AutoModelForCausalLMWithValueHead`类，TRL支持transformers中的所有解码器模型架构，如GPT-2、OPT和GPT-Neo。此外，使用`AutoModelForSeq2SeqLMWithValueHead`，您可以使用编码器-解码器架构，如T5。TRL还需要参考模型，这是训练的模型的冻结副本。使用`create_reference_model`，您可以轻松创建一个冻结副本，并在两个模型之间共享层以节省内存。

## PreTrainedModelWrapper

### `class trl.PreTrainedModelWrapper`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L59)

```py
( pretrained_model = None score_module = None supports_rm_adapter = False rm_adapter_name = None **kwargs )
```

一个围绕(`transformers.PreTrainedModel`)的包装类，以便与(`~transformers.PreTrained`)类兼容，以保留(`~transformers.PreTrainedModel`)类的一些属性和方法。

#### `add_and_load_reward_modeling_adapter`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L440)

```py
( pretrained_model adapter_model_id adapter_name = 'reward_model_adapter' token = None )
```

添加和加载奖励建模适配器。只有当模型是`PeftModel`并且您已经使用`reward_modeling_adapter_id`参数初始化了模型，指向奖励建模适配器的id时，才能使用此方法。最新的还需要包含得分头以产生奖励。

#### `compute_reward_score`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L571)

```py
( input_ids attention_mask = None **kwargs )
```

计算给定输入的奖励分数。该方法首先要启用适配器，然后计算奖励分数。之后，模型禁用奖励建模适配器，并再次启用默认的ppo适配器。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L107)

```py
( pretrained_model_name_or_path *model_args **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `transformers.PreTrainedModel`) — 预训练模型的路径或名称。

+   `*model_args` (`list`, *可选*)) — 传递给基础模型的`from_pretrained`方法的额外位置参数。

+   *`*kwargs` (`dict`, *可选*) — 传递给基础模型的`from_pretrained`方法的额外关键字参数。我们还预处理kwargs以提取特定于`transformers.PreTrainedModel`类和特定于trl模型的参数。kwargs还支持来自`peft`库的`prepare_model_for_kbit_training`参数。

从`transformers`的预训练模型实例化一个新模型。预训练模型是使用`transformers.PreTrainedModel`类的`from_pretrained`方法加载的。特定于`transformers.PreTrainedModel`类的参数通过此方法传递，并从`kwargs`参数中过滤出来。

#### `post_init`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L563)

```py
( *args **kwargs )
```

后初始化方法。此方法在模型实例化并从检查点加载后调用。它可用于执行其他操作，如加载state_dict。

#### `push_to_hub`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L512)

```py
( *args **kwargs )
```

参数

+   `*args` (`list`, *可选*) — 传递给基础模型的`push_to_hub`方法的位置参数。

+   *`*kwargs` (`dict`, *可选*) — 传递给基础模型的`push_to_hub`方法的关键字参数。

将预训练模型推送到hub。此方法是`transformers.PreTrainedModel.push_to_hub`的包装器。有关更多信息，请参阅`transformers.PreTrainedModel.push_to_hub`的文档。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L528)

```py
( *args **kwargs )
```

参数

+   `*args` (`list`, *可选*) — 传递给基础模型的 `save_pretrained` 方法的位置参数。

+   *`*kwargs`（`dict`，*可选*）— 传递给底层模型的`save_pretrained`方法的关键字参数。

将预训练模型保存到目录中。此方法是`transformers.PreTrainedModel.save_pretrained`的包装器。请参考`transformers.PreTrainedModel.save_pretrained`的文档以获取更多信息。

#### `state_dict`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L557)

```py
( *args **kwargs )
```

返回预训练模型的state_dict。

## AutoModelForCausalLMWithValueHead

### `class trl.AutoModelForCausalLMWithValueHead`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L61)

```py
( pretrained_model **kwargs )
```

一个自回归模型，除了语言模型头之外还有一个值头。此类继承自`~trl.PreTrainedModelWrapper`，并包装了一个`transformers.PreTrainedModel`类。包装器类支持经典函数，如`from_pretrained`，`push_to_hub`和`generate`。要调用包装模型的方法，只需操作此类的`pretrained_model`属性。

类属性：

+   `transformers_parent_class`（`transformers.PreTrainedModel`）— 包装模型的父类。对于此类，应设置为`transformers.AutoModelForCausalLM`。

+   `lm_head_namings`（`tuple`）— 用于识别包装模型的语言模型头的字符串元组。对于此类，设置为`("lm_head", "embed_out")`，但可以在将来更改为其他模型

+   `supported_args`（`tuple`）— 用于识别`ValueHead`类支持的参数的字符串元组。目前支持的参数为：

    +   `summary_dropout_prob`（`float`，`可选`，默认为`None`）— `ValueHead`类的丢弃概率。

    +   `v_head_initializer_range`（`float`，`可选`，默认为`0.2`）— 如果选择了特定的初始化策略，则`ValueHead`的初始化器范围。

    +   `v_head_init_strategy`（`str`，`可选`，默认为`None`）— `ValueHead`的初始化策略。目前支持的策略有：

        +   `None`— 使用随机分布初始化`ValueHead`的权重。这是默认策略。

        +   `“normal”`— 使用正态分布初始化`ValueHead`的权重。

#### `__init__`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L96)

```py
( pretrained_model **kwargs )
```

参数

+   `pretrained_model`（`transformers.PreTrainedModel`）— 要包装的模型。它应该是一个因果语言模型，如GPT2。或者任何映射到`AutoModelForCausalLM`类内部的模型。

+   `kwargs`（`dict`，`可选`）— 传递给`ValueHead`类的额外关键字参数。

初始化模型。

#### `forward`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L140)

```py
( input_ids = None past_key_values = None attention_mask = None **kwargs )
```

参数

+   `input_ids`（形状为*(batch_size, sequence_length)*的*torch.LongTensor*）— 词汇表中输入序列令牌的索引。

+   `past_key_values`（*tuple(tuple(torch.FloatTensor))*，*可选*）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值，如*past_key_values*输入）以加速顺序解码。

+   `attention_mask`（*torch.FloatTensor*，形状为*(batch_size, sequence_length)*，*可选*）— 用于避免在填充令牌索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于`未被掩码`的令牌为1，

    +   对于`被掩码`的令牌为0。

+   `kwargs`（*dict*，*可选*）— 传递给包装模型的额外关键字参数。

对包装模型执行前向传递，并返回值头的logits。

#### `generate`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L190)

```py
( *args **kwargs )
```

参数

+   `*args`（`list`，*可选*）— 传递给包装模型的`generate`方法的位置参数。

+   *`*kwargs` (`dict`, *optional*) — 传递给包装模型的`generate`方法的关键字参数。

包装模型的`generate`方法的简单包装器。有关支持的参数的更多信息，请参阅包装模型的[`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)方法。

#### `_init_weights`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L117)

```py
( **kwargs )
```

参数

+   *`*kwargs` (`dict`, `optional`) — 附加的关键字参数，传递给`ValueHead`类。这些参数可以包含`v_head_init_strategy`参数以及`v_head_initializer_range`参数。

初始化值头的权重。默认的初始化策略是随机的。用户可以通过在调用`.from_pretrained`时传递`v_head_init_strategy`参数来传递不同的初始化策略。支持的策略有：

+   `normal`: 使用正态分布初始化权重。

## AutoModelForSeq2SeqLMWithValueHead

### `class trl.AutoModelForSeq2SeqLMWithValueHead`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L264)

```py
( pretrained_model **kwargs )
```

参数

+   `pretrained_model` (`transformers.PreTrainedModel`) — 要包装的模型。它应该是一个因果语言模型，如GPT2。或者映射到`AutoModelForSeq2SeqLM`类内部的任何模型。kwargs — 传递给`ValueHead`类的附加关键字参数。

除了语言模型头之外还带有值头的seq2seq模型。这个类继承自`~trl.PreTrainedModelWrapper`，并包装了一个`transformers.PreTrainedModel`类。包装器类支持经典功能，如`from_pretrained`和`push_to_hub`，还提供一些额外的功能，如`generate`。

#### `__init__`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L287)

```py
( pretrained_model **kwargs )
```

#### `forward`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L395)

```py
( input_ids = None past_key_values = None attention_mask = None **kwargs )
```

#### `generate`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L425)

```py
( *args **kwargs )
```

我们在包装的模型上调用`generate`。

#### `_init_weights`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L381)

```py
( **kwargs )
```

我们初始化值头的权重。

## create_reference_model

#### `trl.create_reference_model`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L602)

```py
( model: PreTrainedModelWrapper num_shared_layers: int = None pattern: str = None )
```

参数

+   `model` (`PreTrainedModelWrapper`) — 要复制的模型。

+   `num_shared_layers` (`int`, *optional*) — 在两个模型之间共享并保持冻结的初始层的数量。

+   `pattern` (`str`, *optional*) — 使用字符串模式选择共享层（例如，“transformer.h.{layer}”用于GPT2），如果需要自定义模式，则可以在这里传递。

创建模型的静态参考副本。请注意，模型将处于`.eval()`模式。

返回`PreTrainedModelWrapper`
