["```py\n( include_buffers: bool = None )\n```", "```py\nimport torch.nn as nn\nfrom accelerate import init_empty_weights\n\n# Initialize a model with 100 billions parameters in no time and without using any RAM.\nwith init_empty_weights():\n    tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n```", "```py\n( model: Module execution_device: Optional = None offload_buffers: bool = False state_dict: Optional = None preload_module_classes: Optional = None )\n```", "```py\n( model: Module execution_device: Union = None prev_module_hook: Optional = None )\n```", "```py\nmodel_1, hook_1 = cpu_offload_with_hook(model_1, cuda_device)\nmodel_2, hook_2 = cpu_offload_with_hook(model_2, cuda_device, prev_module_hook=hook_1)\nmodel_3, hook_3 = cpu_offload_with_hook(model_3, cuda_device, prev_module_hook=hook_2)\n\nhid_1 = model_1(input)\nfor i in range(50):\n    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.\n    hid_2 = model_2(hid_1)\n# model2 is offloaded to the CPU just before this forward.\nhid_3 = model_3(hid_3)\n\n# For model3, you need to manually call the hook offload method.\nhook_3.offload()\n```", "```py\n( model: Module offload_dir: Union execution_device: Optional = None offload_buffers: bool = False preload_module_classes: Optional = None )\n```", "```py\n( model: Module device_map: Dict main_device: Optional = None state_dict: Optional = None offload_dir: Union = None offload_index: Optional = None offload_buffers: bool = False skip_keys: Union = None preload_module_classes: Optional = None force_hooks: bool = False )\n```", "```py\n( model: Module checkpoint: Union device_map: Union = None max_memory: Optional = None no_split_module_classes: Optional = None offload_folder: Union = None offload_buffers: bool = False dtype: Union = None offload_state_dict: Optional = None skip_keys: Union = None preload_module_classes: Optional = None force_hooks: bool = False )\n```", "```py\n>>> from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n>>> from huggingface_hub import hf_hub_download\n>>> from transformers import AutoConfig, AutoModelForCausalLM\n\n>>> # Download the Weights\n>>> checkpoint = \"EleutherAI/gpt-j-6B\"\n>>> weights_location = hf_hub_download(checkpoint, \"pytorch_model.bin\")\n\n>>> # Create a model and initialize it with empty weights\n>>> config = AutoConfig.from_pretrained(checkpoint)\n>>> with init_empty_weights():\n...     model = AutoModelForCausalLM.from_config(config)\n\n>>> # Load the checkpoint and dispatch it to the right devices\n>>> model = load_checkpoint_and_dispatch(\n...     model, weights_location, device_map=\"auto\", no_split_module_classes=[\"GPTJBlock\"]\n... )\n```", "```py\n( model: Module checkpoint: Union device_map: Optional = None offload_folder: Union = None dtype: Union = None offload_state_dict: bool = False offload_buffers: bool = False keep_in_fp32_modules: List = None offload_8bit_bnb: bool = False )\n```", "```py\n( model: Module max_memory: Optional = None no_split_module_classes: Optional = None dtype: Union = None special_dtypes: Optional = None verbose: bool = False clean_result: bool = True )\n```", "```py\n( )\n```", "```py\n( module )\n```", "```py\n( module )\n```", "```py\n( module output ) \u2192 export const metadata = 'undefined';Any\n```", "```py\n( module *args **kwargs ) \u2192 export const metadata = 'undefined';Tuple[Tuple[Any], Dict[Str, Any]]\n```", "```py\n( execution_device: Union = None offload: bool = False io_same_device: bool = False weights_map: Optional = None offload_buffers: bool = False place_submodules: bool = False skip_keys: Union = None tied_params_map: Optional = None )\n```", "```py\n( *hooks )\n```", "```py\n( module: Module hook: ModelHook append: bool = False ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```", "```py\n( module: Module execution_device: Union skip_keys: Union = None preload_module_classes: Optional = None tied_params_map: Optional = None )\n```", "```py\n( module: Module execution_device: Optional = None offload: bool = False weights_map: Optional = None offload_buffers: bool = False module_name: str = '' skip_keys: Union = None preload_module_classes: Optional = None tied_params_map: Optional = None )\n```", "```py\n( module: Module execution_device: Union = None offload: Union = False weights_map: Mapping = None offload_buffers: bool = False module_name: str = '' skip_keys: Union = None preload_module_classes: Optional = None tied_params_map: Optional = None )\n```", "```py\n( module: Module recurse = False ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```", "```py\n( module: Module )\n```"]