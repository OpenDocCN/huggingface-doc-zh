- en: Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/48.0e3a65df.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.optimization` module provides:'
  prefs: []
  type: TYPE_NORMAL
- en: an optimizer with weight decay fixed that can be used to fine-tuned models,
    and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'several schedules in the form of schedule objects that inherit from `_LRSchedule`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a gradient accumulation class to accumulate the gradients of multiple batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdamW (PyTorch)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.AdamW'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L396)'
  prefs: []
  type: TYPE_NORMAL
- en: '( params: Iterable lr: float = 0.001 betas: Tuple = (0.9, 0.999) eps: float
    = 1e-06 weight_decay: float = 0.0 correct_bias: bool = True no_deprecation_warning:
    bool = False )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**params** (`Iterable[nn.parameter.Parameter]`) — Iterable of parameters to
    optimize or dictionaries defining parameter groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lr** (`float`, *optional*, defaults to 0.001) — The learning rate to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**betas** (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`) —
    Adam’s betas parameters (b1, b2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eps** (`float`, *optional*, defaults to 1e-06) — Adam’s epsilon for numerical
    stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weight_decay** (`float`, *optional*, defaults to 0.0) — Decoupled weight
    decay to apply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**correct_bias** (`bool`, *optional*, defaults to `True`) — Whether or not
    to correct bias in Adam (for instance, in Bert TF repository they use `False`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**no_deprecation_warning** (`bool`, *optional*, defaults to `False`) — A flag
    used to disable the deprecation warning (set to `True` to disable the warning).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements Adam algorithm with weight decay fix as introduced in [Decoupled
    Weight Decay Regularization](https://arxiv.org/abs/1711.05101).
  prefs: []
  type: TYPE_NORMAL
- en: '#### step'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L447)'
  prefs: []
  type: TYPE_NORMAL
- en: '( closure: Callable = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (`Callable`, *optional*) — A closure that reevaluates the model
    and returns the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: AdaFactor (PyTorch)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Adafactor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L510)'
  prefs: []
  type: TYPE_NORMAL
- en: ( params lr = None eps = (1e-30, 0.001) clip_threshold = 1.0 decay_rate = -0.8
    beta1 = None weight_decay = 0.0 scale_parameter = True relative_step = True warmup_init
    = False )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**params** (`Iterable[nn.parameter.Parameter]`) — Iterable of parameters to
    optimize or dictionaries defining parameter groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lr** (`float`, *optional*) — The external learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eps** (`Tuple[float, float]`, *optional*, defaults to `(1e-30, 0.001)`) —
    Regularization constants for square gradient and parameter scale respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clip_threshold** (`float`, *optional*, defaults to 1.0) — Threshold of root
    mean square of final gradient update'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decay_rate** (`float`, *optional*, defaults to -0.8) — Coefficient used to
    compute running averages of square'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**beta1** (`float`, *optional*) — Coefficient used for computing running averages
    of gradient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weight_decay** (`float`, *optional*, defaults to 0.0) — Weight decay (L2
    penalty)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scale_parameter** (`bool`, *optional*, defaults to `True`) — If True, learning
    rate is scaled by root mean square'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**relative_step** (`bool`, *optional*, defaults to `True`) — If True, time-dependent
    learning rate is computed instead of external learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**warmup_init** (`bool`, *optional*, defaults to `False`) — Time-dependent
    learning rate computation depends on whether warm-up initialization is being used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AdaFactor pytorch implementation can be used as a drop in replacement for Adam
    original fairseq code: [https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py](https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper: *Adafactor: Adaptive Learning Rates with Sublinear Memory Cost* [https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235)
    Note that this optimizer internally adjusts the learning rate depending on the
    `scale_parameter`, `relative_step` and `warmup_init` options. To use a manual
    (external) learning rate schedule you should set `scale_parameter=False` and `relative_step=False`.'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation handles low-precision (FP16, bfloat) values, but we have
    not thoroughly tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommended T5 finetuning settings ([https://discuss.huggingface.co/t/t5-finetuning-tips/684/3](https://discuss.huggingface.co/t/t5-finetuning-tips/684/3)):'
  prefs: []
  type: TYPE_NORMAL
- en: Training without LR warmup or clip_threshold is not recommended.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use scheduled LR warm-up to fixed LR
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: use clip_threshold=1.0 ([https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable relative updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use scale_parameter=False
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional optimizer operations like gradient clipping should not be used alongside
    Adafactor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Others reported the following combination to work well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When using `lr=None` with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    you will most likely need to use `AdafactorSchedule`
  prefs: []
  type: TYPE_NORMAL
- en: 'scheduler as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#### step'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L656)'
  prefs: []
  type: TYPE_NORMAL
- en: ( closure = None )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (callable, optional) — A closure that reevaluates the model and
    returns the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step
  prefs: []
  type: TYPE_NORMAL
- en: AdamWeightDecay (TensorFlow)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.AdamWeightDecay'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L172)'
  prefs: []
  type: TYPE_NORMAL
- en: '( learning_rate: Union = 0.001 beta_1: float = 0.9 beta_2: float = 0.999 epsilon:
    float = 1e-07 amsgrad: bool = False weight_decay_rate: float = 0.0 include_in_weight_decay:
    Optional = None exclude_from_weight_decay: Optional = None name: str = ''AdamWeightDecay''
    **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**learning_rate** (`Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]`,
    *optional*, defaults to 0.001) — The learning rate to use or a schedule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**beta_1** (`float`, *optional*, defaults to 0.9) — The beta1 parameter in
    Adam, which is the exponential decay rate for the 1st momentum estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**beta_2** (`float`, *optional*, defaults to 0.999) — The beta2 parameter in
    Adam, which is the exponential decay rate for the 2nd momentum estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**epsilon** (`float`, *optional*, defaults to 1e-07) — The epsilon parameter
    in Adam, which is a small constant for numerical stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**amsgrad** (`bool`, *optional*, defaults to `False`) — Whether to apply AMSGrad
    variant of this algorithm or not, see [On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weight_decay_rate** (`float`, *optional*, defaults to 0.0) — The weight decay
    to apply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**include_in_weight_decay** (`List[str]`, *optional*) — List of the parameter
    names (or re patterns) to apply weight decay to. If none is passed, weight decay
    is applied to all parameters by default (unless they are in `exclude_from_weight_decay`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**exclude_from_weight_decay** (`List[str]`, *optional*) — List of the parameter
    names (or re patterns) to exclude from applying weight decay to. If a `include_in_weight_decay`
    is passed, the names in it will supersede this list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**name** (`str`, *optional*, defaults to `"AdamWeightDecay"`) — Optional name
    for the operations created when applying gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict[str, Any]`, *optional*) — Keyword arguments. Allowed to be
    {`clipnorm`, `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by norm;
    `clipvalue` is clip gradients by value, `decay` is included for backward compatibility
    to allow time inverse decay of learning rate. `lr` is included for backward compatibility,
    recommended to use `learning_rate` instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding
    the square of the weights to the loss function is *not* the correct way of using
    L2 regularization/weight decay with Adam, since that will interact with the m
    and v parameters in strange ways as shown in [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).
  prefs: []
  type: TYPE_NORMAL
- en: Instead we want to decay the weights in a manner that doesn’t interact with
    the m/v parameters. This is equivalent to adding the square of the weights to
    the loss with plain (non-momentum) SGD.
  prefs: []
  type: TYPE_NORMAL
- en: '#### from_config'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L229)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Creates an optimizer from its config with WarmUp custom object.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.create_optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L88)'
  prefs: []
  type: TYPE_NORMAL
- en: '( init_lr: float num_train_steps: int num_warmup_steps: int min_lr_ratio: float
    = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float =
    1e-08 adam_clipnorm: Optional = None adam_global_clipnorm: Optional = None weight_decay_rate:
    float = 0.0 power: float = 1.0 include_in_weight_decay: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**init_lr** (`float`) — The desired learning rate at the end of the warmup
    phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_train_steps** (`int`) — The total number of training steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of warmup steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_lr_ratio** (`float`, *optional*, defaults to 0) — The final learning
    rate at the end of the linear decay will be `init_lr * min_lr_ratio`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**adam_beta1** (`float`, *optional*, defaults to 0.9) — The beta1 to use in
    Adam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**adam_beta2** (`float`, *optional*, defaults to 0.999) — The beta2 to use
    in Adam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**adam_epsilon** (`float`, *optional*, defaults to 1e-8) — The epsilon to use
    in Adam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**adam_clipnorm** (`float`, *optional*, defaults to `None`) — If not `None`,
    clip the gradient norm for each weight tensor to this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**adam_global_clipnorm** (`float`, *optional*, defaults to `None`) — If not
    `None`, clip gradient norm to this value. When using this argument, the norm is
    computed over all weight tensors, as if they were concatenated into a single vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weight_decay_rate** (`float`, *optional*, defaults to 0) — The weight decay
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**power** (`float`, *optional*, defaults to 1.0) — The power to use for PolynomialDecay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**include_in_weight_decay** (`List[str]`, *optional*) — List of the parameter
    names (or re patterns) to apply weight decay to. If none is passed, weight decay
    is applied to all parameters except bias and layer norm parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates an optimizer with a learning rate schedule using a warmup phase followed
    by a linear decay.
  prefs: []
  type: TYPE_NORMAL
- en: Schedules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning Rate Schedules (Pytorch)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class transformers.SchedulerType'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L394)'
  prefs: []
  type: TYPE_NORMAL
- en: ( value names = None module = None qualname = None type = None start = 1 )
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.get_scheduler'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L338)'
  prefs: []
  type: TYPE_NORMAL
- en: '( name: Union optimizer: Optimizer num_warmup_steps: Optional = None num_training_steps:
    Optional = None scheduler_specific_kwargs: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**name** (`str` or `SchedulerType`) — The name of the scheduler to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optimizer** (`torch.optim.Optimizer`) — The optimizer that will be used during
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`, *optional*) — The number of warmup steps to do.
    This is not required by all schedulers (hence the argument being optional), the
    function will raise an error if it’s unset and the scheduler type requires it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_training_steps** (`int“, *optional*) — The number of training steps to
    do. This is not required by all schedulers (hence the argument being optional),
    the function will raise an error if it’s unset and the scheduler type requires
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler_specific_kwargs** (`dict`, *optional*) — Extra parameters for schedulers
    such as cosine with restarts. Mismatched scheduler types and scheduler parameters
    will cause the scheduler function to raise a TypeError.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unified API to get any scheduler from its name.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.get_constant_schedule'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer: Optimizer last_epoch: int = -1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with a constant learning rate, using the learning rate set
    in optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.get_constant_schedule_with_warmup'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L80)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer: Optimizer num_warmup_steps: int last_epoch: int = -1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of steps for the warmup phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with a constant learning rate preceded by a warmup period
    during which the learning rate increases linearly between 0 and the initial lr
    set in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72024b1b3539399498dd9d47bf49625e.png) #### transformers.get_cosine_schedule_with_warmup'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L143)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer: Optimizer num_warmup_steps: int num_training_steps: int num_cycles:
    float = 0.5 last_epoch: int = -1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of steps for the warmup phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_training_steps** (`int`) — The total number of training steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_cycles** (`float`, *optional*, defaults to 0.5) — The number of waves
    in the cosine schedule (the defaults is to just decrease from the max value to
    0 following a half-cosine).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with a learning rate that decreases following the values of
    the cosine function between the initial lr set in the optimizer to 0, after a
    warmup period during which it increases linearly between 0 and the initial lr
    set in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f52d964e94bc88eb74e955db94a8a49.png) #### transformers.get_cosine_with_hard_restarts_schedule_with_warmup'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L188)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer: Optimizer num_warmup_steps: int num_training_steps: int num_cycles:
    int = 1 last_epoch: int = -1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of steps for the warmup phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_training_steps** (`int`) — The total number of training steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_cycles** (`int`, *optional*, defaults to 1) — The number of hard restarts
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with a learning rate that decreases following the values of
    the cosine function between the initial lr set in the optimizer to 0, with several
    hard restarts, after a warmup period during which it increases linearly between
    0 and the initial lr set in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a747d841c70030341cbfbcf4078836b.png) #### transformers.get_linear_schedule_with_warmup'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L107)'
  prefs: []
  type: TYPE_NORMAL
- en: ( optimizer num_warmup_steps num_training_steps last_epoch = -1 )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of steps for the warmup phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_training_steps** (`int`) — The total number of training steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with a learning rate that decreases linearly from the initial
    lr set in the optimizer to 0, after a warmup period during which it increases
    linearly from 0 to the initial lr set in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e1bcf23c9d854d0c98549eec5391c06.png) #### transformers.get_polynomial_decay_schedule_with_warmup'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L242)'
  prefs: []
  type: TYPE_NORMAL
- en: ( optimizer num_warmup_steps num_training_steps lr_end = 1e-07 power = 1.0 last_epoch
    = -1 )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of steps for the warmup phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_training_steps** (`int`) — The total number of training steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lr_end** (`float`, *optional*, defaults to 1e-7) — The end LR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**power** (`float`, *optional*, defaults to 1.0) — Power factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with a learning rate that decreases as a polynomial decay
    from the initial lr set in the optimizer to end lr defined by *lr_end*, after
    a warmup period during which it increases linearly from 0 to the initial lr set
    in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: *power* defaults to 1.0 as in the fairseq implementation, which in turn
    is based on the original BERT implementation at [https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37](https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.get_inverse_sqrt_schedule'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L296)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer: Optimizer num_warmup_steps: int timescale: int = None last_epoch:
    int = -1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_warmup_steps** (`int`) — The number of steps for the warmup phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timescale** (`int`, *optional*, defaults to `num_warmup_steps`) — Time scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**last_epoch** (`int`, *optional*, defaults to -1) — The index of the last
    epoch when resuming training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a schedule with an inverse square-root learning rate, from the initial
    lr set in the optimizer, after a warmup period which increases lr linearly from
    0 to the initial lr set in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Warmup (TensorFlow)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class transformers.WarmUp'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: '( initial_learning_rate: float decay_schedule_fn: Callable warmup_steps: int
    power: float = 1.0 name: str = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**initial_learning_rate** (`float`) — The initial learning rate for the schedule
    after the warmup (so this will be the learning rate at the end of the warmup).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decay_schedule_fn** (`Callable`) — The schedule function to apply after the
    warmup for the rest of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**warmup_steps** (`int`) — The number of steps for the warmup part of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**power** (`float`, *optional*, defaults to 1.0) — The power to use for the
    polynomial warmup (defaults is a linear warmup).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**name** (`str`, *optional*) — Optional name prefix for the returned tensors
    during the schedule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies a warmup schedule on a given learning rate decay schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GradientAccumulator (TensorFlow)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class transformers.GradientAccumulator'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L302)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Gradient accumulation utility. When used with a distribution strategy, the accumulator
    should be called in a replica context. Gradients will be accumulated locally on
    each replica and without synchronization. Users should then call `.gradients`,
    scale the gradients if required, and pass the result to `apply_gradients`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### reset'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L364)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Resets the accumulated gradients on the current replica.
  prefs: []
  type: TYPE_NORMAL
