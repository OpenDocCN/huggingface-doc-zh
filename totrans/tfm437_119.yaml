- en: Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.optimization` module provides:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '`.optimization`模块提供：'
- en: an optimizer with weight decay fixed that can be used to fine-tuned models,
    and
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有固定权重衰减的优化器，可用于微调模型，以及
- en: 'several schedules in the form of schedule objects that inherit from `_LRSchedule`:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_LRSchedule`继承的形式有几个调度对象：'
- en: a gradient accumulation class to accumulate the gradients of multiple batches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个梯度累积类，用于累积多个批次的梯度
- en: AdamW (PyTorch)
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdamW（PyTorch）
- en: '### `class transformers.AdamW`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AdamW`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L396)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L396)'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (`Iterable[nn.parameter.Parameter]`) — Iterable of parameters to optimize
    or dictionaries defining parameter groups.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（`Iterable[nn.parameter.Parameter]`）— 要优化的参数的可迭代对象或定义参数组的字典。'
- en: '`lr` (`float`, *optional*, defaults to 0.001) — The learning rate to use.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（`float`，*可选*，默认为0.001）— 要使用的学习率。'
- en: '`betas` (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`) — Adam’s
    betas parameters (b1, b2).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（`Tuple[float,float]`，*可选*，默认为`(0.9, 0.999)`）— Adam的betas参数（b1，b2）。'
- en: '`eps` (`float`, *optional*, defaults to 1e-06) — Adam’s epsilon for numerical
    stability.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（`float`，*可选*，默认为1e-06）— Adam的数值稳定性epsilon。'
- en: '`weight_decay` (`float`, *optional*, defaults to 0.0) — Decoupled weight decay
    to apply.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（`float`，*可选*，默认为0.0）— 要应用的解耦权重衰减。'
- en: '`correct_bias` (`bool`, *optional*, defaults to `True`) — Whether or not to
    correct bias in Adam (for instance, in Bert TF repository they use `False`).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`correct_bias`（`bool`，*可选*，默认为`True`）— 是否在Adam中校正偏差（例如，在Bert TF存储库中，它们使用`False`）。'
- en: '`no_deprecation_warning` (`bool`, *optional*, defaults to `False`) — A flag
    used to disable the deprecation warning (set to `True` to disable the warning).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_deprecation_warning`（`bool`，*可选*，默认为`False`）— 用于禁用弃用警告的标志（设置为`True`以禁用警告）。'
- en: Implements Adam algorithm with weight decay fix as introduced in [Decoupled
    Weight Decay Regularization](https://arxiv.org/abs/1711.05101).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了带有权重衰减修复的Adam算法，该算法在[解耦权重衰减正则化](https://arxiv.org/abs/1711.05101)中引入。
- en: '#### `step`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L447)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L447)'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (`Callable`, *optional*) — A closure that reevaluates the model and
    returns the loss.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（`Callable`，*可选*）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: AdaFactor (PyTorch)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaFactor（PyTorch）
- en: '### `class transformers.Adafactor`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Adafactor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L510)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L510)'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (`Iterable[nn.parameter.Parameter]`) — Iterable of parameters to optimize
    or dictionaries defining parameter groups.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（`Iterable[nn.parameter.Parameter]`）— 要优化的参数的可迭代对象或定义参数组的字典。'
- en: '`lr` (`float`, *optional*) — The external learning rate.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（`float`，*可选*）— 外部学习率。'
- en: '`eps` (`Tuple[float, float]`, *optional*, defaults to `(1e-30, 0.001)`) — Regularization
    constants for square gradient and parameter scale respectively'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（`Tuple[float, float]`，*可选*，默认为`(1e-30, 0.001)`）— 平方梯度和参数比例的正则化常数'
- en: '`clip_threshold` (`float`, *optional*, defaults to 1.0) — Threshold of root
    mean square of final gradient update'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_threshold`（`float`，*可选*，默认为1.0）— 最终梯度更新的均方根阈值'
- en: '`decay_rate` (`float`, *optional*, defaults to -0.8) — Coefficient used to
    compute running averages of square'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decay_rate`（`float`，*可选*，默认为-0.8）— 用于计算平方运行平均值的系数'
- en: '`beta1` (`float`, *optional*) — Coefficient used for computing running averages
    of gradient'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta1`（`float`，*可选*）— 用于计算梯度的运行平均值的系数'
- en: '`weight_decay` (`float`, *optional*, defaults to 0.0) — Weight decay (L2 penalty)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（`float`，*可选*，默认为0.0）— 权重衰减（L2惩罚）'
- en: '`scale_parameter` (`bool`, *optional*, defaults to `True`) — If True, learning
    rate is scaled by root mean square'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_parameter`（`bool`，*可选*，默认为`True`）— 如果为True，则学习率将按均方根缩放'
- en: '`relative_step` (`bool`, *optional*, defaults to `True`) — If True, time-dependent
    learning rate is computed instead of external learning rate'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_step`（`bool`，*可选*，默认为`True`）— 如果为True，则计算时间相关的学习率，而不是外部学习率'
- en: '`warmup_init` (`bool`, *optional*, defaults to `False`) — Time-dependent learning
    rate computation depends on whether warm-up initialization is being used'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_init`（`bool`，*可选*，默认为`False`）— 时间相关的学习率计算取决于是否使用了热身初始化'
- en: 'AdaFactor pytorch implementation can be used as a drop in replacement for Adam
    original fairseq code: [https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py](https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: AdaFactor的PyTorch实现可用作Adam原始fairseq代码的替代品：[https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py](https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py)
- en: 'Paper: *Adafactor: Adaptive Learning Rates with Sublinear Memory Cost* [https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235)
    Note that this optimizer internally adjusts the learning rate depending on the
    `scale_parameter`, `relative_step` and `warmup_init` options. To use a manual
    (external) learning rate schedule you should set `scale_parameter=False` and `relative_step=False`.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 论文：*Adafactor：自适应学习率与亚线性内存成本* [https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235)
    请注意，此优化器根据`scale_parameter`、`relative_step`和`warmup_init`选项内部调整学习率。要使用手动（外部）学习率调度，您应将`scale_parameter=False`和`relative_step=False`。
- en: This implementation handles low-precision (FP16, bfloat) values, but we have
    not thoroughly tested.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现处理低精度（FP16，bfloat）值，但我们尚未进行彻底测试。
- en: 'Recommended T5 finetuning settings ([https://discuss.huggingface.co/t/t5-finetuning-tips/684/3](https://discuss.huggingface.co/t/t5-finetuning-tips/684/3)):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的T5微调设置（[https://discuss.huggingface.co/t/t5-finetuning-tips/684/3](https://discuss.huggingface.co/t/t5-finetuning-tips/684/3)）：
- en: Training without LR warmup or clip_threshold is not recommended.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不建议在没有LR热身或clip_threshold的情况下进行训练。
- en: use scheduled LR warm-up to fixed LR
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用计划的LR热身到固定的LR
- en: use clip_threshold=1.0 ([https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235))
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用clip_threshold=1.0 ([https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235))
- en: Disable relative updates
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用相对更新
- en: Use scale_parameter=False
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scale_parameter=False
- en: Additional optimizer operations like gradient clipping should not be used alongside
    Adafactor
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不应该在Adafactor旁边使用额外的优化器操作，如梯度裁剪。
- en: 'Example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Others reported the following combination to work well:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其他人报告以下组合效果很好：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When using `lr=None` with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    you will most likely need to use `AdafactorSchedule`
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`lr=None`与[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)时，您很可能需要使用`AdafactorSchedule`
- en: 'scheduler as following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器如下：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Usage:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `step`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `步骤`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L656)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L656)'
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`闭包`（可调用，可选）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤
- en: AdamWeightDecay (TensorFlow)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdamWeightDecay（TensorFlow）
- en: '### `class transformers.AdamWeightDecay`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AdamWeightDecay`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L172)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L172)'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`learning_rate` (`Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]`,
    *optional*, defaults to 0.001) — The learning rate to use or a schedule.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`（`Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]`，*可选*，默认为0.001）—
    要使用的学习率或计划。'
- en: '`beta_1` (`float`, *optional*, defaults to 0.9) — The beta1 parameter in Adam,
    which is the exponential decay rate for the 1st momentum estimates.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta_1`（`float`，*可选*，默认为0.9）— Adam中的beta1参数，即第1动量估计的指数衰减率。'
- en: '`beta_2` (`float`, *optional*, defaults to 0.999) — The beta2 parameter in
    Adam, which is the exponential decay rate for the 2nd momentum estimates.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta_2`（`float`，*可选*，默认为0.999）— Adam中的beta2参数，即第2动量估计的指数衰减率。'
- en: '`epsilon` (`float`, *optional*, defaults to 1e-07) — The epsilon parameter
    in Adam, which is a small constant for numerical stability.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`（`float`，*可选*，默认为1e-07）— Adam中的epsilon参数，这是用于数值稳定性的小常数。'
- en: '`amsgrad` (`bool`, *optional*, defaults to `False`) — Whether to apply AMSGrad
    variant of this algorithm or not, see [On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amsgrad`（`bool`，*可选*，默认为`False`）— 是否应用AMSGrad变体的算法，参见[关于Adam及其更多的收敛性](https://arxiv.org/abs/1904.09237)。'
- en: '`weight_decay_rate` (`float`, *optional*, defaults to 0.0) — The weight decay
    to apply.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay_rate`（`float`，*可选*，默认为0.0）— 要应用的权重衰减。'
- en: '`include_in_weight_decay` (`List[str]`, *optional*) — List of the parameter
    names (or re patterns) to apply weight decay to. If none is passed, weight decay
    is applied to all parameters by default (unless they are in `exclude_from_weight_decay`).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_in_weight_decay`（`List[str]`，*可选*）— 要应用权重衰减的参数名称（或re模式）的列表。如果没有传递，则默认情况下将权重衰减应用于所有参数（除非它们在`exclude_from_weight_decay`中）。'
- en: '`exclude_from_weight_decay` (`List[str]`, *optional*) — List of the parameter
    names (or re patterns) to exclude from applying weight decay to. If a `include_in_weight_decay`
    is passed, the names in it will supersede this list.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_from_weight_decay`（`List[str]`，*可选*）— 要排除不应用权重衰减的参数名称（或re模式）的列表。如果传递了`include_in_weight_decay`，则其中的名称将取代此列表。'
- en: '`name` (`str`, *optional*, defaults to `"AdamWeightDecay"`) — Optional name
    for the operations created when applying gradients.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`（`str`，*可选*，默认为`"AdamWeightDecay"`）— 应用梯度时创建的操作的可选名称。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Keyword arguments. Allowed to be
    {`clipnorm`, `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by norm;
    `clipvalue` is clip gradients by value, `decay` is included for backward compatibility
    to allow time inverse decay of learning rate. `lr` is included for backward compatibility,
    recommended to use `learning_rate` instead.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）— 关键字参数。允许为{`clipnorm`，`clipvalue`，`lr`，`decay`}。`clipnorm`是按范数裁剪梯度；`clipvalue`是按值裁剪梯度，`decay`包含了向后兼容性，允许学习率的时间反转衰减。`lr`包含了向后兼容性，建议使用`learning_rate`代替。'
- en: Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding
    the square of the weights to the loss function is *not* the correct way of using
    L2 regularization/weight decay with Adam, since that will interact with the m
    and v parameters in strange ways as shown in [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Adam启用L2权重衰减和梯度的全局范数裁剪。只是将权重的平方添加到损失函数中*不是*使用Adam进行L2正则化/权重衰减的正确方式，因为这将以奇怪的方式与m和v参数交互，如[解耦权重衰减正则化](https://arxiv.org/abs/1711.05101)所示。
- en: Instead we want to decay the weights in a manner that doesn’t interact with
    the m/v parameters. This is equivalent to adding the square of the weights to
    the loss with plain (non-momentum) SGD.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们希望以一种不会与m/v参数交互的方式衰减权重。这相当于使用纯（非动量）SGD将权重的平方添加到损失中。
- en: '#### `from_config`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L229)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L229)'
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Creates an optimizer from its config with WarmUp custom object.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其配置创建具有WarmUp自定义对象的优化器。
- en: '#### `transformers.create_optimizer`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.create_optimizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L88)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L88)'
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`init_lr` (`float`) — The desired learning rate at the end of the warmup phase.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_lr`（`float`）—热身阶段结束时的期望学习率。'
- en: '`num_train_steps` (`int`) — The total number of training steps.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_steps`（`int`）—训练步骤的总数。'
- en: '`num_warmup_steps` (`int`) — The number of warmup steps.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps`（`int`）—热身步骤的数量。'
- en: '`min_lr_ratio` (`float`, *optional*, defaults to 0) — The final learning rate
    at the end of the linear decay will be `init_lr * min_lr_ratio`.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_lr_ratio`（`float`，*可选*，默认为0）—线性衰减结束时的最终学习率将为`init_lr * min_lr_ratio`。'
- en: '`adam_beta1` (`float`, *optional*, defaults to 0.9) — The beta1 to use in Adam.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta1`（`float`，*可选*，默认为0.9）—Adam中使用的beta1。'
- en: '`adam_beta2` (`float`, *optional*, defaults to 0.999) — The beta2 to use in
    Adam.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_beta2`（`float`，*可选*，默认为0.999）—Adam中使用的beta2。'
- en: '`adam_epsilon` (`float`, *optional*, defaults to 1e-8) — The epsilon to use
    in Adam.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_epsilon`（`float`，*可选*，默认为1e-8）—Adam中使用的epsilon。'
- en: '`adam_clipnorm` (`float`, *optional*, defaults to `None`) — If not `None`,
    clip the gradient norm for each weight tensor to this value.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_clipnorm`（`float`，*可选*，默认为`None`）—如果不是`None`，则将每个权重张量的梯度范数剪裁为此值。'
- en: '`adam_global_clipnorm` (`float`, *optional*, defaults to `None`) — If not `None`,
    clip gradient norm to this value. When using this argument, the norm is computed
    over all weight tensors, as if they were concatenated into a single vector.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam_global_clipnorm`（`float`，*可选*，默认为`None`）—如果不是`None`，则将梯度范数剪裁为此值。使用此参数时，规范是在所有权重张量上计算的，就好像它们被连接成一个单一向量。'
- en: '`weight_decay_rate` (`float`, *optional*, defaults to 0) — The weight decay
    to use.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay_rate`（`float`，*可选*，默认为0）—要使用的权重衰减。'
- en: '`power` (`float`, *optional*, defaults to 1.0) — The power to use for PolynomialDecay.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`power`（`float`，*可选*，默认为1.0）—用于PolynomialDecay的幂。'
- en: '`include_in_weight_decay` (`List[str]`, *optional*) — List of the parameter
    names (or re patterns) to apply weight decay to. If none is passed, weight decay
    is applied to all parameters except bias and layer norm parameters.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_in_weight_decay`（`List[str]`，*可选*）—要应用权重衰减的参数名称（或re模式）的列表。如果未传递任何内容，则将权重衰减应用于除偏置和层归一化参数之外的所有参数。'
- en: Creates an optimizer with a learning rate schedule using a warmup phase followed
    by a linear decay.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个使用热身阶段后跟线性衰减的学习率时间表的优化器。
- en: Schedules
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间表
- en: Learning Rate Schedules (Pytorch)
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率时间表（Pytorch）
- en: '### `class transformers.SchedulerType`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SchedulerType`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L394)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L394)'
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: An enumeration.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '#### `transformers.get_scheduler`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.get_scheduler`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L338)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L338)'
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`name` (`str` or `SchedulerType`) — The name of the scheduler to use.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`（`str`或`SchedulerType`）—要使用的调度程序的名称。'
- en: '`optimizer` (`torch.optim.Optimizer`) — The optimizer that will be used during
    training.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`（`torch.optim.Optimizer`）—训练期间将使用的优化器。'
- en: '`num_warmup_steps` (`int`, *optional*) — The number of warmup steps to do.
    This is not required by all schedulers (hence the argument being optional), the
    function will raise an error if it’s unset and the scheduler type requires it.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps`（`int`，*可选*）—要执行的热身步骤数。并非所有调度程序都需要（因此参数是可选的），如果未设置并且调度程序类型需要，则函数将引发错误。'
- en: '`num_training_steps` (`int“, *optional*) — The number of training steps to
    do. This is not required by all schedulers (hence the argument being optional),
    the function will raise an error if it’s unset and the scheduler type requires
    it.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps`（`int`，*可选*）—要执行的训练步骤数。并非所有调度程序都需要（因此参数是可选的），如果未设置并且调度程序类型需要，则函数将引发错误。'
- en: '`scheduler_specific_kwargs` (`dict`, *optional*) — Extra parameters for schedulers
    such as cosine with restarts. Mismatched scheduler types and scheduler parameters
    will cause the scheduler function to raise a TypeError.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler_specific_kwargs`（`dict`，*可选*）—用于诸如带重启的余弦等调度程序的额外参数。不匹配的调度程序类型和调度程序参数将导致调度程序函数引发TypeError。'
- en: Unified API to get any scheduler from its name.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从其名称获取任何调度程序的统一API。
- en: '#### `transformers.get_constant_schedule`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.get_constant_schedule`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L39)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L39)'
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`（`~torch.optim.Optimizer`）—要为其调度学习率的优化器。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch`（`int`，*可选*，默认为-1）—恢复训练时的最后一个时期的索引。'
- en: Create a schedule with a constant learning rate, using the learning rate set
    in optimizer.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优化器中设置的学习率创建一个具有恒定学习率的时间表。
- en: '#### `transformers.get_constant_schedule_with_warmup`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.get_constant_schedule_with_warmup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L80)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L80)'
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`（`~torch.optim.Optimizer`）—要为其调度学习率的优化器。'
- en: '`num_warmup_steps` (`int`) — The number of steps for the warmup phase.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps`（`int`）—热身阶段的步数。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch`（`int`，*可选*，默认为-1）—恢复训练时的最后一个时期的索引。'
- en: Create a schedule with a constant learning rate preceded by a warmup period
    during which the learning rate increases linearly between 0 and the initial lr
    set in the optimizer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有恒定学习率的时间表，在此期间学习率在0和优化器中设置的初始lr之间线性增加的热身期之前。
- en: '![](../Images/72024b1b3539399498dd9d47bf49625e.png) #### `transformers.get_cosine_schedule_with_warmup`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/72024b1b3539399498dd9d47bf49625e.png) #### `transformers.get_cosine_schedule_with_warmup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L143)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L143)'
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。'
- en: '`num_warmup_steps` (`int`) — The number of steps for the warmup phase.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps` (`int`) — 热身阶段的步数。'
- en: '`num_training_steps` (`int`) — The total number of training steps.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps` (`int`) — 总训练步数。'
- en: '`num_cycles` (`float`, *optional*, defaults to 0.5) — The number of waves in
    the cosine schedule (the defaults is to just decrease from the max value to 0
    following a half-cosine).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_cycles` (`float`, *optional*, defaults to 0.5) — 余弦计划中波数的数量（默认值是从最大值到0按照半余弦减少）。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。'
- en: Create a schedule with a learning rate that decreases following the values of
    the cosine function between the initial lr set in the optimizer to 0, after a
    warmup period during which it increases linearly between 0 and the initial lr
    set in the optimizer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个学习率随余弦函数值下降的计划，从优化器中设置的初始lr到0，经过一个热身阶段，在此期间学习率线性增加从0到优化器中设置的初始lr。
- en: '![](../Images/8f52d964e94bc88eb74e955db94a8a49.png) #### `transformers.get_cosine_with_hard_restarts_schedule_with_warmup`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/8f52d964e94bc88eb74e955db94a8a49.png) #### `transformers.get_cosine_with_hard_restarts_schedule_with_warmup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L188)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L188)'
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。'
- en: '`num_warmup_steps` (`int`) — The number of steps for the warmup phase.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps` (`int`) — 热身阶段的步数。'
- en: '`num_training_steps` (`int`) — The total number of training steps.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps` (`int`) — 总训练步数。'
- en: '`num_cycles` (`int`, *optional*, defaults to 1) — The number of hard restarts
    to use.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_cycles` (`int`, *optional*, defaults to 1) — 要使用的硬重启次数。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。'
- en: Create a schedule with a learning rate that decreases following the values of
    the cosine function between the initial lr set in the optimizer to 0, with several
    hard restarts, after a warmup period during which it increases linearly between
    0 and the initial lr set in the optimizer.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个学习率随余弦函数值下降的计划，从优化器中设置的初始lr到0，经过几次硬重启，在此期间学习率线性增加从0到优化器中设置的初始lr。
- en: '![](../Images/0a747d841c70030341cbfbcf4078836b.png) #### `transformers.get_linear_schedule_with_warmup`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0a747d841c70030341cbfbcf4078836b.png) #### `transformers.get_linear_schedule_with_warmup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L107)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L107)'
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。'
- en: '`num_warmup_steps` (`int`) — The number of steps for the warmup phase.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps` (`int`) — 热身阶段的步数。'
- en: '`num_training_steps` (`int`) — The total number of training steps.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps` (`int`) — 总训练步数。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。'
- en: Create a schedule with a learning rate that decreases linearly from the initial
    lr set in the optimizer to 0, after a warmup period during which it increases
    linearly from 0 to the initial lr set in the optimizer.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个学习率从优化器中设置的初始lr线性下降到0的计划，在此期间学习率从0线性增加到优化器中设置的初始lr。
- en: '![](../Images/9e1bcf23c9d854d0c98549eec5391c06.png) #### `transformers.get_polynomial_decay_schedule_with_warmup`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/9e1bcf23c9d854d0c98549eec5391c06.png) #### `transformers.get_polynomial_decay_schedule_with_warmup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L242)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L242)'
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。'
- en: '`num_warmup_steps` (`int`) — The number of steps for the warmup phase.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps` (`int`) — 热身阶段的步数。'
- en: '`num_training_steps` (`int`) — The total number of training steps.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_training_steps` (`int`) — 总训练步数。'
- en: '`lr_end` (`float`, *optional*, defaults to 1e-7) — The end LR.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_end` (`float`, *optional*, defaults to 1e-7) — 最终LR。'
- en: '`power` (`float`, *optional*, defaults to 1.0) — Power factor.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`power` (`float`, *optional*, defaults to 1.0) — 功率因子。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。'
- en: Create a schedule with a learning rate that decreases as a polynomial decay
    from the initial lr set in the optimizer to end lr defined by *lr_end*, after
    a warmup period during which it increases linearly from 0 to the initial lr set
    in the optimizer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个学习率从优化器中设置的初始lr按多项式衰减到由*lr_end*定义的最终lr的计划，在此期间学习率从0线性增加到优化器中设置的初始lr。
- en: 'Note: *power* defaults to 1.0 as in the fairseq implementation, which in turn
    is based on the original BERT implementation at [https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37](https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：*power* 默认为1.0，与fairseq实现相同，fairseq实现又基于原始BERT实现 [https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37](https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37)
- en: '#### `transformers.get_inverse_sqrt_schedule`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.get_inverse_sqrt_schedule`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L296)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L296)'
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`~torch.optim.Optimizer`) — The optimizer for which to schedule
    the learning rate.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。'
- en: '`num_warmup_steps` (`int`) — The number of steps for the warmup phase.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_warmup_steps` (`int`) — 热身阶段的步数。'
- en: '`timescale` (`int`, *optional*, defaults to `num_warmup_steps`) — Time scale.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timescale` (`int`, *可选*, 默认为 `num_warmup_steps`) — 时间尺度。'
- en: '`last_epoch` (`int`, *optional*, defaults to -1) — The index of the last epoch
    when resuming training.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_epoch` (`int`, *可选*, 默认为 -1) — 恢复训练时的最后一个时代的索引。'
- en: Create a schedule with an inverse square-root learning rate, from the initial
    lr set in the optimizer, after a warmup period which increases lr linearly from
    0 to the initial lr set in the optimizer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有反平方根学习率的调度，从优化器中设置的初始lr开始，在一个热身期间之后，该期间将使lr从0线性增加到优化器中设置的初始lr。
- en: Warmup (TensorFlow)
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Warmup（TensorFlow）
- en: '### `class transformers.WarmUp`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '### `类 transformers.WarmUp`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L30)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L30)'
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`initial_learning_rate` (`float`) — The initial learning rate for the schedule
    after the warmup (so this will be the learning rate at the end of the warmup).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_learning_rate` (`float`) — 热身后调度的初始学习率（这将是热身结束时的学习率）。'
- en: '`decay_schedule_fn` (`Callable`) — The schedule function to apply after the
    warmup for the rest of training.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decay_schedule_fn` (`Callable`) — 在热身后应用于剩余训练的调度函数。'
- en: '`warmup_steps` (`int`) — The number of steps for the warmup part of training.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_steps` (`int`) — 训练中热身阶段的步数。'
- en: '`power` (`float`, *optional*, defaults to 1.0) — The power to use for the polynomial
    warmup (defaults is a linear warmup).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`power` (`float`, *可选*, 默认为 1.0) — 用于多项式热身的幂（默认为线性热身）。'
- en: '`name` (`str`, *optional*) — Optional name prefix for the returned tensors
    during the schedule.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`, *可选*) — 调度期间返回张量的可选名称前缀。'
- en: Applies a warmup schedule on a given learning rate decay schedule.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定学习率衰减调度应用热身调度。
- en: Gradient Strategies
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度策略
- en: GradientAccumulator (TensorFlow)
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GradientAccumulator（TensorFlow）
- en: '### `class transformers.GradientAccumulator`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '### `类 transformers.GradientAccumulator`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L302)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L302)'
- en: '[PRE21]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Gradient accumulation utility. When used with a distribution strategy, the accumulator
    should be called in a replica context. Gradients will be accumulated locally on
    each replica and without synchronization. Users should then call `.gradients`,
    scale the gradients if required, and pass the result to `apply_gradients`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积实用程序。与分布策略一起使用时，应在副本上下文中调用累加器。梯度将在每个副本上本地累积，无需同步。然后用户应调用`.gradients`，根据需要缩放梯度，并将结果传递给`apply_gradients`。
- en: '#### `reset`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `重置`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L364)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L364)'
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Resets the accumulated gradients on the current replica.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 重置当前副本上累积的梯度。
