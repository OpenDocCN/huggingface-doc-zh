["```py\nfrom transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer\n\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"fr\")\n\nsrc_text = \"Life is like a box of chocolates.\"\ntgt_text = \"La vie est comme une bo\u00eete de chocolat.\"\n\nmodel_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n\nloss = model(**model_inputs).loss  # forward pass\n```", "```py\n>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\n>>> hi_text = \"\u091c\u0940\u0935\u0928 \u090f\u0915 \u091a\u0949\u0915\u0932\u0947\u091f \u092c\u0949\u0915\u094d\u0938 \u0915\u0940 \u0924\u0930\u0939 \u0939\u0948\u0964\"\n>>> chinese_text = \"\u751f\u6d3b\u5c31\u50cf\u4e00\u76d2\u5de7\u514b\u529b\u3002\"\n\n>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n>>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n>>> # translate Hindi to French\n>>> tokenizer.src_lang = \"hi\"\n>>> encoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\n>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\"La vie est comme une bo\u00eete de chocolat.\"\n\n>>> # translate Chinese to English\n>>> tokenizer.src_lang = \"zh\"\n>>> encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\"Life is like a box of chocolate.\"\n```", "```py\n( vocab_size = 128112 max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.05 decoder_layerdrop = 0.05 use_cache = True is_encoder_decoder = True activation_function = 'relu' d_model = 1024 dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 2 scale_embedding = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import M2M100Config, M2M100Model\n\n>>> # Initializing a M2M100 facebook/m2m100_418M style configuration\n>>> configuration = M2M100Config()\n\n>>> # Initializing a model (with random weights) from the facebook/m2m100_418M style configuration\n>>> model = M2M100Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file spm_file src_lang = None tgt_lang = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' pad_token = '<pad>' unk_token = '<unk>' language_codes = 'm2m100' sp_model_kwargs: Optional = None num_madeup_words = 8 **kwargs )\n```", "```py\n>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\n>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n>>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"ro\")\n>>> src_text = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> tgt_text = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n>>> outputs = model(**model_inputs)  # should work\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( config: M2M100Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, M2M100Model\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n>>> model = M2M100Model.from_pretrained(\"facebook/m2m100_418M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: M2M100Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, M2M100ForConditionalGeneration\n\n>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n>>> text_to_translate = \"Life is like a box of chocolates\"\n>>> model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n\n>>> # translate to French\n>>> gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n>>> print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))\n```"]