["```py\n>>> from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments\n\n>>> args = PyTorchBenchmarkArguments(models=[\"bert-base-uncased\"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512])\n>>> benchmark = PyTorchBenchmark(args)\n```", "```py\n>>> from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments\n\n>>> args = TensorFlowBenchmarkArguments(\n...     models=[\"bert-base-uncased\"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512]\n... )\n>>> benchmark = TensorFlowBenchmark(args)\n```", "```py\npython examples/pytorch/benchmarking/run_benchmark.py --help\n```", "```py\n>>> results = benchmark.run()\n>>> print(results)\n====================       INFERENCE - SPEED - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length     Time in s                  \n--------------------------------------------------------------------------------\nbert-base-uncased          8               8             0.006     \nbert-base-uncased          8               32            0.006     \nbert-base-uncased          8              128            0.018     \nbert-base-uncased          8              512            0.088     \n--------------------------------------------------------------------------------\n\n====================      INFERENCE - MEMORY - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length    Memory in MB \n--------------------------------------------------------------------------------\nbert-base-uncased          8               8             1227\nbert-base-uncased          8               32            1281\nbert-base-uncased          8              128            1307\nbert-base-uncased          8              512            1539\n--------------------------------------------------------------------------------\n\n====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_version: 2.11.0\n- framework: PyTorch\n- use_torchscript: False\n- framework_version: 1.4.0\n- python_version: 3.6.10\n- system: Linux\n- cpu: x86_64\n- architecture: 64bit\n- date: 2020-06-29\n- time: 08:58:43.371351\n- fp16: False\n- use_multiprocessing: True\n- only_pretrain_model: False\n- cpu_ram_mb: 32088\n- use_gpu: True\n- num_gpus: 1\n- gpu: TITAN RTX\n- gpu_ram_mb: 24217\n- gpu_power_watts: 280.0\n- gpu_performance_state: 2\n- use_tpu: False\n```", "```py\npython examples/tensorflow/benchmarking/run_benchmark_tf.py --help\n```", "```py\n>>> results = benchmark.run()\n>>> print(results)\n>>> results = benchmark.run()\n>>> print(results)\n====================       INFERENCE - SPEED - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length     Time in s                  \n--------------------------------------------------------------------------------\nbert-base-uncased          8               8             0.005\nbert-base-uncased          8               32            0.008\nbert-base-uncased          8              128            0.022\nbert-base-uncased          8              512            0.105\n--------------------------------------------------------------------------------\n\n====================      INFERENCE - MEMORY - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length    Memory in MB \n--------------------------------------------------------------------------------\nbert-base-uncased          8               8             1330\nbert-base-uncased          8               32            1330\nbert-base-uncased          8              128            1330\nbert-base-uncased          8              512            1770\n--------------------------------------------------------------------------------\n\n====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_version: 2.11.0\n- framework: Tensorflow\n- use_xla: False\n- framework_version: 2.2.0\n- python_version: 3.6.10\n- system: Linux\n- cpu: x86_64\n- architecture: 64bit\n- date: 2020-06-29\n- time: 09:26:35.617317\n- fp16: False\n- use_multiprocessing: True\n- only_pretrain_model: False\n- cpu_ram_mb: 32088\n- use_gpu: True\n- num_gpus: 1\n- gpu: TITAN RTX\n- gpu_ram_mb: 24217\n- gpu_power_watts: 280.0\n- gpu_performance_state: 2\n- use_tpu: False\n```", "```py\n>>> from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments, BertConfig\n\n>>> args = PyTorchBenchmarkArguments(\n...     models=[\"bert-base\", \"bert-384-hid\", \"bert-6-lay\"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512]\n... )\n>>> config_base = BertConfig()\n>>> config_384_hid = BertConfig(hidden_size=384)\n>>> config_6_lay = BertConfig(num_hidden_layers=6)\n\n>>> benchmark = PyTorchBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\n>>> benchmark.run()\n====================       INFERENCE - SPEED - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length       Time in s                  \n--------------------------------------------------------------------------------\nbert-base                  8              128            0.006\nbert-base                  8              512            0.006\nbert-base                  8              128            0.018     \nbert-base                  8              512            0.088     \nbert-384-hid              8               8             0.006     \nbert-384-hid              8               32            0.006     \nbert-384-hid              8              128            0.011     \nbert-384-hid              8              512            0.054     \nbert-6-lay                 8               8             0.003     \nbert-6-lay                 8               32            0.004     \nbert-6-lay                 8              128            0.009     \nbert-6-lay                 8              512            0.044\n--------------------------------------------------------------------------------\n\n====================      INFERENCE - MEMORY - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length      Memory in MB \n--------------------------------------------------------------------------------\nbert-base                  8               8             1277\nbert-base                  8               32            1281\nbert-base                  8              128            1307     \nbert-base                  8              512            1539     \nbert-384-hid              8               8             1005     \nbert-384-hid              8               32            1027     \nbert-384-hid              8              128            1035     \nbert-384-hid              8              512            1255     \nbert-6-lay                 8               8             1097     \nbert-6-lay                 8               32            1101     \nbert-6-lay                 8              128            1127     \nbert-6-lay                 8              512            1359\n--------------------------------------------------------------------------------\n\n====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_version: 2.11.0\n- framework: PyTorch\n- use_torchscript: False\n- framework_version: 1.4.0\n- python_version: 3.6.10\n- system: Linux\n- cpu: x86_64\n- architecture: 64bit\n- date: 2020-06-29\n- time: 09:35:25.143267\n- fp16: False\n- use_multiprocessing: True\n- only_pretrain_model: False\n- cpu_ram_mb: 32088\n- use_gpu: True\n- num_gpus: 1\n- gpu: TITAN RTX\n- gpu_ram_mb: 24217\n- gpu_power_watts: 280.0\n- gpu_performance_state: 2\n- use_tpu: False\n```", "```py\n>>> from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments, BertConfig\n\n>>> args = TensorFlowBenchmarkArguments(\n...     models=[\"bert-base\", \"bert-384-hid\", \"bert-6-lay\"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512]\n... )\n>>> config_base = BertConfig()\n>>> config_384_hid = BertConfig(hidden_size=384)\n>>> config_6_lay = BertConfig(num_hidden_layers=6)\n\n>>> benchmark = TensorFlowBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\n>>> benchmark.run()\n====================       INFERENCE - SPEED - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length       Time in s                  \n--------------------------------------------------------------------------------\nbert-base                  8               8             0.005\nbert-base                  8               32            0.008\nbert-base                  8              128            0.022\nbert-base                  8              512            0.106\nbert-384-hid              8               8             0.005\nbert-384-hid              8               32            0.007\nbert-384-hid              8              128            0.018\nbert-384-hid              8              512            0.064\nbert-6-lay                 8               8             0.002\nbert-6-lay                 8               32            0.003\nbert-6-lay                 8              128            0.0011\nbert-6-lay                 8              512            0.074\n--------------------------------------------------------------------------------\n\n====================      INFERENCE - MEMORY - RESULT       ====================\n--------------------------------------------------------------------------------\nModel Name             Batch Size     Seq Length      Memory in MB \n--------------------------------------------------------------------------------\nbert-base                  8               8             1330\nbert-base                  8               32            1330\nbert-base                  8              128            1330\nbert-base                  8              512            1770\nbert-384-hid              8               8             1330\nbert-384-hid              8               32            1330\nbert-384-hid              8              128            1330\nbert-384-hid              8              512            1540\nbert-6-lay                 8               8             1330\nbert-6-lay                 8               32            1330\nbert-6-lay                 8              128            1330\nbert-6-lay                 8              512            1540\n--------------------------------------------------------------------------------\n\n====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_version: 2.11.0\n- framework: Tensorflow\n- use_xla: False\n- framework_version: 2.2.0\n- python_version: 3.6.10\n- system: Linux\n- cpu: x86_64\n- architecture: 64bit\n- date: 2020-06-29\n- time: 09:38:15.487125\n- fp16: False\n- use_multiprocessing: True\n- only_pretrain_model: False\n- cpu_ram_mb: 32088\n- use_gpu: True\n- num_gpus: 1\n- gpu: TITAN RTX\n- gpu_ram_mb: 24217\n- gpu_power_watts: 280.0\n- gpu_performance_state: 2\n- use_tpu: False\n```"]