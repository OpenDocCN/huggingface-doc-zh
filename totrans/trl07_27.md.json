["```py\nmodel_name = \"huggyllama/llama-7b\"\nrm_adapter_id = \"trl-lib/llama-7b-hh-rm-adapter\"\n\n# PPO adapter\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    model_name,\n    peft_config=lora_config,\n    reward_adapter=rm_adapter_id,\n)\n\n...\ntrainer = PPOTrainer(\n    model=model,\n    ...\n)\n\n...\n```", "```py\nrewards = trainer.model.compute_reward_score(**inputs)\n```", "```py\nadapter_name_policy_1 = \"policy_1\"\nrewards = trainer.model.compute_reward_score(**inputs, ppo_adapter_name=adapter_name_policy_1)\n...\n```", "```py\nmodel_name = \"llama-7b\"\nrm_adapter_id = \"trl-lib/llama-7b-hh-rm-adapter\"\n\n# PPO adapter\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    model_name,\n    peft_config=lora_config,\n    reward_adapter=rm_adapter_id,\n    load_in_8bit=True,\n)\n\n...\ntrainer = PPOTrainer(\n    model=model,\n    ...\n)\n...\n```"]