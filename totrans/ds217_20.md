# 与PyTorch一起使用

> 原始文本：[https://huggingface.co/docs/datasets/use_with_pytorch](https://huggingface.co/docs/datasets/use_with_pytorch)

本文是关于如何在PyTorch中使用`datasets`的快速介绍，特别关注如何从我们的数据集中获取`torch.Tensor`对象，以及如何使用PyTorch的`DataLoader`和Hugging Face的`Dataset`以获得最佳性能。

## 数据集格式

默认情况下，数据集返回常规的Python对象：整数、浮点数、字符串、列表等。

要获取PyTorch张量，可以使用[Dataset.with_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_format)将数据集的格式设置为`pytorch`：

```py
>>> from datasets import Dataset
>>> data = [[1, 2],[3, 4]]
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': tensor([1, 2])}
>>> ds[:2]
{'data': tensor([[1, 2],
         [3, 4]])}
```

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象是Arrow表的包装器，允许从数据集中的数组快速零拷贝读取到PyTorch张量中。

要将数据加载为GPU上的张量，请指定`device`参数：

```py
>>> import torch
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>>> ds = ds.with_format("torch", device=device)
>>> ds[0]
{'data': tensor([1, 2], device='cuda:0')}
```

## N维数组

如果您的数据集由N维数组组成，则默认情况下它们被视为嵌套列表。特别是，PyTorch格式化的数据集输出嵌套列表而不是单个张量：

```py
>>> from datasets import Dataset
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': [tensor([1, 2]), tensor([3, 4])]}
```

要获取单个张量，必须显式使用`Array`特征类型并指定张量的形状：

```py
>>> from datasets import Dataset, Features, Array2D
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
>>> features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
>>> ds = Dataset.from_dict({"data": data}, features=features)
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': tensor([[1, 2],
         [3, 4]])}
>>> ds[:2]
{'data': tensor([[[1, 2],
          [3, 4]],

         [[5, 6],
          [7, 8]]])}
```

## 其他特征类型

[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)数据会被正确转换为张量：

```py
>>> from datasets import Dataset, Features, ClassLabel
>>> labels = [0, 0, 1]
>>> features = Features({"label": ClassLabel(names=["negative", "positive"])})
>>> ds = Dataset.from_dict({"label": labels}, features=features) 
>>> ds = ds.with_format("torch")  
>>> ds[:3]
{'label': tensor([0, 0, 1])}
```

字符串和二进制对象保持不变，因为PyTorch仅支持数字。

[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)和[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)特征类型也受支持。

要使用[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)特征类型，您需要安装`vision`额外功能，如`pip install datasets[vision]`。

```py
>>> from datasets import Dataset, Features, Audio, Image
>>> images = ["path/to/image.png"] * 10
>>> features = Features({"image": Image()})
>>> ds = Dataset.from_dict({"image": images}, features=features) 
>>> ds = ds.with_format("torch")
>>> ds[0]["image"].shape
torch.Size([512, 512, 4])
>>> ds[0]
{'image': tensor([[[255, 215, 106, 255],
         [255, 215, 106, 255],
         ...,
         [255, 255, 255, 255],
         [255, 255, 255, 255]]], dtype=torch.uint8)}
>>> ds[:2]["image"].shape
torch.Size([2, 512, 512, 4])
>>> ds[:2]
{'image': tensor([[[[255, 215, 106, 255],
          [255, 215, 106, 255],
          ...,
          [255, 255, 255, 255],
          [255, 255, 255, 255]]]], dtype=torch.uint8)}
```

要使用[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)特征类型，您需要安装`audio`额外功能，如`pip install datasets[audio]`。

```py
>>> from datasets import Dataset, Features, Audio, Image
>>> audio = ["path/to/audio.wav"] * 10
>>> features = Features({"audio": Audio()})
>>> ds = Dataset.from_dict({"audio": audio}, features=features) 
>>> ds = ds.with_format("torch")  
>>> ds[0]["audio"]["array"]
tensor([ 6.1035e-05,  1.5259e-05,  1.6785e-04,  ..., -1.5259e-05,
        -1.5259e-05,  1.5259e-05])
>>> ds[0]["audio"]["sampling_rate"]
tensor(44100)
```

## 数据加载

与`torch.utils.data.Dataset`对象一样，[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)可以直接传递给PyTorch的`DataLoader`：

```py
>>> import numpy as np
>>> from datasets import Dataset 
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(16)
>>> label = np.random.randint(0, 2, size=16)
>>> ds = Dataset.from_dict({"data": data, "label": label}).with_format("torch")
>>> dataloader = DataLoader(ds, batch_size=4)
>>> for batch in dataloader:
...     print(batch)                                                                                            
{'data': tensor([0.0047, 0.4979, 0.6726, 0.8105]), 'label': tensor([0, 1, 0, 1])}
{'data': tensor([0.4832, 0.2723, 0.4259, 0.2224]), 'label': tensor([0, 0, 0, 0])}
{'data': tensor([0.5837, 0.3444, 0.4658, 0.6417]), 'label': tensor([0, 1, 0, 0])}
{'data': tensor([0.7022, 0.1225, 0.7228, 0.8259]), 'label': tensor([1, 1, 1, 1])}
```

### 优化数据加载

有几种方法可以提高数据加载速度，这可以节省您的时间，特别是如果您正在处理大型数据集。PyTorch提供了并行化数据加载、检索批次索引而不是单独检索以及流式迭代数据集而无需将其下载到磁盘上的功能。

#### 使用多个工作进程

您可以使用PyTorch的`DataLoader`的`num_workers`参数并行加载数据，并获得更高的吞吐量。

在幕后，`DataLoader`启动`num_workers`个进程。每个进程重新加载传递给`DataLoader`的数据集，并用于查询示例。在工作进程内重新加载数据集不会填满您的RAM，因为它只是从磁盘再次内存映射数据集。

```py
>>> import numpy as np
>>> from datasets import Dataset, load_from_disk
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(10_000)
>>> Dataset.from_dict({"data": data}).save_to_disk("my_dataset")
>>> ds = load_from_disk("my_dataset").with_format("torch")
>>> dataloader = DataLoader(ds, batch_size=32, num_workers=4)
```

### 流式数据

通过将其加载为[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)来流式传输数据集。这允许您逐步迭代远程数据集，而无需将其下载到磁盘上或本地数据文件上。在[选择常规数据集或可迭代数据集](./about_mapstyle_vs_iterable)指南中了解更多关于哪种类型的数据集最适合您的用例。

从`datasets`中的可迭代数据集继承自`torch.utils.data.IterableDataset`，因此您可以将其传递给`torch.utils.data.DataLoader`：

```py
>>> import numpy as np
>>> from datasets import Dataset, load_dataset
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(10_000)
>>> Dataset.from_dict({"data": data}).push_to_hub("<username>/my_dataset")  # Upload to the Hugging Face Hub
>>> my_iterable_dataset = load_dataset("<username>/my_dataset", streaming=True, split="train")
>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32)
```

如果数据集分为多个分片（即如果数据集由多个数据文件组成），则可以使用`num_workers`并行流式传输：

```py
>>> my_iterable_dataset = load_dataset("deepmind/code_contests", streaming=True, split="train")
>>> my_iterable_dataset.n_shards
39
>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32, num_workers=4)
```

在这种情况下，每个工作进程都会获得要从中流式传输的分片列表的子集。

### 分布式

要在训练节点之间分割数据集，您可以使用[datasets.distributed.split_dataset_by_node()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.distributed.split_dataset_by_node)：

```py
import os
from datasets.distributed import split_dataset_by_node

ds = split_dataset_by_node(ds, rank=int(os.environ["RANK"]), world_size=int(os.environ["WORLD_SIZE"]))
```

这适用于映射样式数据集和可迭代数据集。数据集在大小为`world_size`的节点池中为排名为`rank`的节点分割。

对于映射样式数据集：

每个节点被分配一块数据，例如，排名为0的节点被给予数据集的第一块。

对于可迭代数据集：

如果数据集的分片数是`world_size`的因子（即如果`dataset.n_shards % world_size == 0`），那么分片将均匀分配给节点，这是最优化的。否则，每个节点保留`world_size`中的一个示例，跳过其他示例。

这也可以与`torch.utils.data.DataLoader`结合使用，如果您希望每个节点使用多个工作进程来加载数据。
