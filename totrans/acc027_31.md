# 处理推断中的大型模型

> 原始文本：[`huggingface.co/docs/accelerate/concept_guides/big_model_inference`](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference)

在 PyTorch 中加载预训练模型时，通常的工作流程如下：

```py
import torch

my_model = ModelClass(...)
state_dict = torch.load(checkpoint_file)
my_model.load_state_dict(state_dict)
```

用简单的英语来说，这些步骤是：

1.  创建具有随机初始化权重的模型

1.  从磁盘加载模型权重（通常称为状态字典的字典）

1.  将这些权重加载到模型中

虽然这对于常规大小的模型非常有效，但是当我们处理一个巨大的模型时，这个工作流程有一些明显的局限性：在步骤 1 中，我们在 RAM 中加载模型的完整版本，并花费一些时间随机初始化权重（这些权重将在步骤 3 中被丢弃）。在步骤 2 中，我们在 RAM 中加载另一个模型的完整版本，其中包含预训练的权重。如果您加载一个具有 60 亿参数的模型，这意味着您将需要每个模型副本 24GB 的 RAM，因此总共需要 48GB（其中一半用于以 FP16 加载模型）。

这个 API 是相当新的，仍处于实验阶段。虽然我们努力提供一个稳定的 API，但未来可能会更改公共 API 的一些小部分。

## 流程如何工作：快速概述

[`www.youtube-nocookie.com/embed/MWCSGj9jEAo`](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)

## 流程如何工作：使用代码

### 实例化一个空模型

🤗 Accelerate 引入的第一个工具是一个上下文管理器 init_empty_weights()，它可以帮助您初始化一个模型，而不使用任何 RAM，以便可以对任何大小的模型执行步骤 1。这是它的工作原理：

```py
from accelerate import init_empty_weights

with init_empty_weights():
    my_model = ModelClass(...)
```

例如：

```py
with init_empty_weights():
    model = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])
```

使用超过 100B 参数初始化一个空模型。在幕后，这依赖于 PyTorch 1.9 中引入的元设备。在上下文管理器下的初始化过程中，每次创建参数时，它都会立即移动到该设备。

您不能直接将像这样初始化的模型移动到 CPU 或另一个设备，因为它没有任何数据。使用这种空模型进行前向传递很可能会失败，因为并非所有操作都支持在元设备上进行。

### 分片检查点

可能您的模型太大，即使单个副本也无法放入 RAM。这并不意味着它不能被加载：如果您有一个或多个 GPU，这意味着有更多内存可用于存储您的模型。在这种情况下，最好将您的检查点分成几个更小的文件，我们称之为检查点分片。

🤗 Accelerate 将处理分片检查点，只要您遵循以下格式：您的检查点应该在一个文件夹中，其中包含几个包含部分状态字典的文件，并且应该有一个以 JSON 格式的索引，其中包含将参数名称映射到包含其权重的文件的字典。您可以使用 save_model() 轻松地将模型分片。例如，我们可以有一个包含的文件夹：

```py
first_state_dict.bin
index.json
second_state_dict.bin
```

index.json 是以下文件：

```py
{
  "linear1.weight": "first_state_dict.bin",
  "linear1.bias": "first_state_dict.bin",
  "linear2.weight": "second_state_dict.bin",
  "linear2.bias": "second_state_dict.bin"
}
```

和 `first_state_dict.bin` 包含 `"linear1.weight"` 和 `"linear1.bias"` 的权重，`second_state_dict.bin` 包含 `"linear2.weight"` 和 `"linear2.bias"` 的权重

### 加载权重

🤗 Accelerate 引入的第二个工具是一个函数 load_checkpoint_and_dispatch()，它将允许您在空模型中加载检查点。这支持完整检查点（包含整个状态字典的单个文件）以及分片检查点。它还将自动将这些权重分配到您可用的设备上（GPU、CPU RAM），因此如果您加载一个分片检查点，最大的 RAM 使用量将是最大分片的大小。

如果您想使用 🤗 Transformers 模型进行大型模型推断，请查看这个[文档](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading)。

这是我们如何使用这个加载 [GPT2-1.5B](https://huggingface.co/marcsun13/gpt2-xl-linear-sharded) 模型。

让我们下载这个模型的分片版本。

```py
pip install huggingface_hub
```

```py
from huggingface_hub import snapshot_download
checkpoint = "marcsun13/gpt2-xl-linear-sharded"
weights_location = snapshot_download(repo_id=checkpoint)
```

为了初始化模型，我们将使用 minGPT 库。

```py
git clone https://github.com/karpathy/minGPT.git
pip install minGPT/
```

```py
from accelerate import init_empty_weights
from mingpt.model import GPT

model_config = GPT.get_default_config()
model_config.model_type = 'gpt2-xl'
model_config.vocab_size = 50257
model_config.block_size = 1024

with init_empty_weights():
    model = GPT(model_config)
```

然后，加载我们刚下载的检查点：

```py
from accelerate import load_checkpoint_and_dispatch

model = load_checkpoint_and_dispatch(
    model, checkpoint=weights_location, device_map="auto", no_split_module_classes=['Block']
)
```

通过传递 `device_map="auto"`，我们告诉 🤗 Accelerate 根据可用资源自动确定将模型的每一层放在哪里：

+   首先，我们使用 GPU 上可用的最大空间

+   如果我们仍然需要空间，我们将剩余的权重存储在 CPU 上

+   如果没有足够的 RAM，我们将剩余的权重存储在硬盘上作为内存映射张量

#### 不要分割模块类

此参数将指示一些名称为 "Block" 的模块不应该在不同设备之间分割。您应该在这里设置包括某种残差连接的所有块。

#### 设备映射

您可以通过访问您的模型的 `hf_device_map` 属性来查看 🤗 Accelerate 选择的 `device_map`：

```py
model.hf_device_map
```

```py
{'transformer.wte': 0,
 'transformer.wpe': 0,
 'transformer.drop': 0,
 'transformer.h.0': 0,
 ...
 'transformer.h.21': 0, 
 'transformer.h.22': 1, 
 'transformer.h.23': 1, 
 'transformer.h.24': 1,
 ...
 'transformer.h.47': 1, 
 'transformer.ln_f': 1, 
 'lm_head': 1}
```

完全可以为层创建自己的设备映射，指定要使用的 GPU 设备（一个数字）、"cpu" 或 "disk"，然后传入：

```py
device_map = {
    "transformer.wte": "cpu",
    "transformer.wpe": 0,
    "transformer.drop": "cpu",
    "transformer.h.0": "disk"
}

model = load_checkpoint_and_dispatch(
    model, checkpoint=weights_location, device_map=device_map
)

```

### 运行模型

现在我们已经完成了这一步，我们的模型分布在几个设备上，也许还有硬盘。但它仍然可以像常规的 PyTorch 模型一样使用：

```py
from mingpt.bpe import BPETokenizer
tokenizer = BPETokenizer()
inputs = tokenizer("Hello, my name is").to(0)

outputs = model.generate(x1, max_new_tokens=10, do_sample=False)[0]
tokenizer.decode(outputs.cpu().squeeze())
```

在幕后，🤗 Accelerate 为模型添加了钩子，以便：

+   在每一层，输入都被放在正确的设备上（所以即使您的模型分布在几个 GPU 上，它也可以工作）

+   对于卸载到 CPU 上的权重，在前向传递之前将它们放在 GPU 上，然后在后面清理

+   对于卸载到硬盘上的权重，在前向传递之前将它们加载到 RAM 中，然后在后面清理

这样，即使模型不适合在一个 GPU 或 CPU RAM 上运行推断，也可以运行您的模型！

这只支持模型的推断，不支持训练。大部分计算发生在 `torch.no_grad()` 上下文管理器中，以避免用中间激活消耗一些 GPU 内存。

### 设计设备映射

您可以让 🤗 Accelerate 处理设备映射计算，方法是将 `device_map` 设置为支持的选项之一（"auto"、"balanced"、"balanced_low_0"、"sequential"），或者如果您想更多地控制每个层应该放在哪里，可以自己创建一个。

您可以在元设备上的模型上推导出模型的所有大小（从而计算出 `device_map`）。

当没有足够的 GPU 内存来容纳整个模型时，所有选项都会产生相同的结果（即将可以放在 GPU 上的所有内容放在 GPU 上，然后将权重卸载到 CPU 上，甚至在没有足够的 RAM 时也可以放在磁盘上）。

当您拥有比模型大小更多的 GPU 内存时，每个选项之间的区别如下：

+   "auto" 和 "balanced" 将模型均匀分割在所有可用的 GPU 上，使您可以使用大于 1 的批量大小。

+   "balanced_low_0" 将模型均匀分割在除第一个之外的所有 GPU 上，并且只将不适合其他 GPU 的内容放在 GPU 0 上。当您需要使用 GPU 0 处理输出时，比如使用 Transformers 模型的 `generate` 函数时，这个选项非常适用

+   "sequential" 将尽可能多地放在 GPU 0 上，然后移动到 GPU 1 等等（如果不需要，就不会使用最后的 GPU）。

"auto" 和 "balanced" 选项目前产生相同的结果，但是如果我们找到更合理的策略，"auto" 的行为可能会在将来发生变化，而 "balanced" 将保持稳定。

首先请注意，您可以使用`max_memory`参数（在 infer_auto_device_map()和所有使用它的函数中可用）限制每个 GPU 上使用的内存。设置`max_memory`时，应传递一个包含 GPU 标识符（例如`0`、`1`等）和`"cpu"`键的字典，用于 CPU 卸载的最大 RAM。值可以是整数（以字节为单位）或表示带单位的数字的字符串，例如`"10GiB"`或`"10GB"`。

以下是一个示例，在这个示例中，我们不希望在两个 GPU 上使用超过 10GiB，并且模型权重的 CPU RAM 不超过 30GiB：

```py
from accelerate import infer_auto_device_map

device_map = infer_auto_device_map(my_model, max_memory={0: "10GiB", 1: "10GiB", "cpu": "30GiB"})
```

当 PyTorch 中发生第一次分配时，它会加载大约 1-2GB 的 CUDA 内核，具体取决于 GPU。因此，您始终比 GPU 的实际大小可用内存少。要查看实际使用了多少内存，请执行`torch.ones(1).cuda()`并查看内存使用情况。

因此，当您使用`max_memory`创建内存映射时，请确保根据可用内存进行相应调整，以避免内存不足错误。

此外，如果您对输出执行一些额外的操作，而不将其放回 CPU（例如在 Transformers 的`generate`方法中），并且将输入放在 GPU 上，则该 GPU 将消耗比其他 GPU 更多的内存（Accelerate 始终将输出放回到输入设备）。因此，如果您想要优化最大批处理大小，并且您有许多 GPU，请给第一个 GPU 分配更少的内存。例如，在 8x80 A100 设置中使用 BLOOM-176B，接近理想的映射是：

```py
max_memory = {0: "30GIB", 1: "46GIB", 2: "46GIB", 3: "46GIB", 4: "46GIB", 5: "46GIB", 6: "46GIB", 7: "46GIB"}
```

正如您所看到的，我们给剩下的 7 个 GPU 分配了比 GPU 0 多约 50%的内存。

如果您选择完全自己设计`device_map`，它应该是一个字典，其中键是您模型的模块名称，值是有效的设备标识符（例如 GPU 的整数）或`"cpu"`表示 CPU 卸载，`"disk"`表示磁盘卸载。键需要覆盖整个模型，然后您可以根据需要定义设备映射：例如，如果您的模型有两个块（假设是`block1`和`block2`），每个块包含三个线性层（假设是`linear1`、`linear2`和`linear3`），一个有效的设备映射可以是：

```py
device_map = {"block1": 0, "block2": 1}
```

另一个有效的示例可能是：

```py
device_map = {"block1": 0, "block2.linear1": 0, "block2.linear2": 1, "block2.linear3": 1}
```

另一方面，这个映射是无效的，因为它没有涵盖模型的每个参数：

```py
device_map = {"block1": 0, "block2.linear1": 1, "block2.linear2": 1}
```

为了最有效地使用，请确保您的设备映射以顺序方式将参数放在 GPU 上（例如，不要将第一个权重放在 GPU 0 上，然后将权重放在 GPU 1 上，最后将权重放回 GPU 0），以避免在 GPU 之间进行多次数据传输。

## 仅 CPU 卸载

如果您想要在 CPU 上卸载模型，可以使用 cpu_offload()。结果，模型的所有参数将被卸载，模型的状态字典只会保留一份副本。在前向传递过程中，参数将从该状态字典中提取出来，并放在执行设备上，并在需要时传递，然后再次卸载。

```py
cpu_offload(model, execution_device)
```

您还可以使用 cpu_offload_with_hook()。此函数将模型卸载到 CPU，并在执行时将其放回执行设备。与 cpu_offload()的区别在于，模型在前向传递后仍保留在执行设备上，并且仅在调用返回的`hook`的`offload`方法时再次卸载。此外，cpu_offload_with_hook()更高效但节省的内存较少。它适用于在循环中运行模型的管道：

```py
model_1, hook_1 = cpu_offload_with_hook(model_1, execution_device)
model_2, hook_2 = cpu_offload_with_hook(model_2, execution_device, prev_module_hook=hook_1)
model_3, hook_3 = cpu_offload_with_hook(model_3, execution_device, prev_module_hook=hook_2)

hid_1 = model_1(input)
for i in range(50):
    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.
    hid_2 = model_2(hid_1)
# model2 is offloaded to the CPU just before this forward.
hid_3 = model_3(hid_3)

# For model3, you need to manually call the hook offload method.
hook_3.offload()
```

## 仅磁盘卸载

要执行磁盘卸载，您可以使用 disk_offload()。结果，模型的所有参数将作为内存映射数组卸载到给定文件夹中。在前向传递期间，参数将从该文件夹访问，并在需要时放在传递的执行设备上，然后再次卸载。

```py
disk_offload(model, offload_dir, execution_device)
```

## 限制和进一步发展

我们意识到 API 中目前的限制：

+   infer_auto_device_map()（或在 load_checkpoint_and_dispatch()中使用`device_map="auto"`）会尝试在执行时最大化它看到的可用 GPU 和 CPU RAM。虽然 PyTorch 非常擅长有效管理 GPU RAM（并在不需要时将其归还），但对于 Python 和 CPU RAM 来说并非完全如此。因此，自动计算的设备映射可能对 CPU 太过密集。如果由于 RAM 不足而导致崩溃，请将一些模块移动到磁盘设备。

+   infer_auto_device_map()（或在 load_checkpoint_and_dispatch()中使用`device_map="auto"`）属性会按顺序分配设备（以避免来回移动），因此如果您的第一层比您拥有的 GPU 大小还要大，它最终会将所有内容放在 CPU/磁盘上。

+   load_checkpoint_and_dispatch()和 load_checkpoint_in_model()目前不会对您的状态字典与模型的正确性进行任何检查（这将在未来版本中修复），因此如果尝试加载具有不匹配或缺失键的检查点，则可能会出现一些奇怪的错误。

+   当您的模型在多个 GPU 上分割时使用的模型并行性是天真的并且没有优化，这意味着只有一个 GPU 在给定时间工作，而另一个处于空闲状态。

+   当权重被转移到 CPU/硬盘上时，没有预取（但我们将在未来版本中解决这个问题），这意味着权重在需要时才会放在 GPU 上，而不是之前。

+   如果您运行的硬件之间的磁盘和 CPU 之间没有快速通信（如 NVMes），硬盘卸载可能会非常慢。
