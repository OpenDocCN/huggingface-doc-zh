["```py\nimport torch\nfrom diffusers import (\n    EulerDiscreteScheduler,\n    MotionAdapter,\n    PIAPipeline,\n)\nfrom diffusers.utils import export_to_gif, load_image\n\nadapter = MotionAdapter.from_pretrained(\"openmmlab/PIA-condition-adapter\")\npipe = PIAPipeline.from_pretrained(\"SG161222/Realistic_Vision_V6.0_B1_noVAE\", motion_adapter=adapter, torch_dtype=torch.float16)\n\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\npipe.enable_vae_slicing()\n\nimage = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png?download=true\"\n)\nimage = image.resize((512, 512))\nprompt = \"cat in a field\"\nnegative_prompt = \"wrong white balance, dark, sketches,worst quality,low quality\"\n\ngenerator = torch.Generator(\"cpu\").manual_seed(0)\noutput = pipe(image=image, prompt=prompt, generator=generator)\nframes = output.frames[0]\nexport_to_gif(frames, \"pia-animation.gif\")\n```", "```py\nimport torch\nfrom diffusers import (\n    DDIMScheduler,\n    MotionAdapter,\n    PIAPipeline,\n)\nfrom diffusers.utils import export_to_gif, load_image\n\nadapter = MotionAdapter.from_pretrained(\"openmmlab/PIA-condition-adapter\")\npipe = PIAPipeline.from_pretrained(\"SG161222/Realistic_Vision_V6.0_B1_noVAE\", motion_adapter=adapter)\n\n# enable FreeInit\n# Refer to the enable_free_init documentation for a full list of configurable parameters\npipe.enable_free_init(method=\"butterworth\", use_fast_sampling=True)\n\n# Memory saving options\npipe.enable_model_cpu_offload()\npipe.enable_vae_slicing()\n\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\nimage = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png?download=true\"\n)\nimage = image.resize((512, 512))\nprompt = \"cat in a hat\"\nnegative_prompt = \"wrong white balance, dark, sketches,worst quality,low quality\"\n\ngenerator = torch.Generator(\"cpu\").manual_seed(0)\n\noutput = pipe(image=image, prompt=prompt, generator=generator)\nframes = output.frames[0]\nexport_to_gif(frames, \"pia-freeinit-animation.gif\")\n```", "```py\n>>> import torch\n>>> from diffusers import (\n...     EulerDiscreteScheduler,\n...     MotionAdapter,\n...     PIAPipeline,\n... )\n>>> from diffusers.utils import export_to_gif, load_image\n>>> adapter = MotionAdapter.from_pretrained(\"../checkpoints/pia-diffusers\")\n>>> pipe = PIAPipeline.from_pretrained(\"SG161222/Realistic_Vision_V6.0_B1_noVAE\", motion_adapter=adapter)\n>>> pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n>>> image = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png?download=true\"\n... )\n>>> image = image.resize((512, 512))\n>>> prompt = \"cat in a hat\"\n>>> negative_prompt = \"wrong white balance, dark, sketches,worst quality,low quality, deformed, distorted, disfigured, bad eyes, wrong lips,weird mouth, bad teeth, mutated hands and fingers, bad anatomy,wrong anatomy, amputation, extra limb, missing limb, floating,limbs, disconnected limbs, mutation, ugly, disgusting, bad_pictures, negative_hand-neg\"\n>>> generator = torch.Generator(\"cpu\").manual_seed(0)\n>>> output = pipe(image=image, prompt=prompt, negative_prompt=negative_prompt, generator=generator)\n>>> frames = output.frames[0]\n>>> export_to_gif(frames, \"pia-animation.gif\")\n```"]