- en: Training on TPUs with 🤗 Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/concept_guides/training_tpu](https://huggingface.co/docs/accelerate/concept_guides/training_tpu)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Training on TPUs can be slightly different from training on multi-gpu, even
    with 🤗 Accelerate. This guide aims to show you where you should be careful and
    why, as well as the best practices in general.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Training in a Notebook
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main carepoint when training on TPUs comes from the [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher).
    As mentioned in the [notebook tutorial](../usage_guides/notebook), you need to
    restructure your training code into a function that can get passed to the [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)
    function and be careful about not declaring any tensors on the GPU.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: While on a TPU that last part is not as important, a critical part to understand
    is that when you launch code from a notebook you do so through a process called
    **forking**. When launching from the command-line, you perform **spawning**, where
    a python process is not currently running and you *spawn* a new process in. Since
    your Jupyter notebook is already utilizing a python process, you need to *fork*
    a new process from it to launch your code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Where this becomes important is in regard to declaring your model. On forked
    TPU processes, it is recommended that you instantiate your model *once* and pass
    this into your training function. This is different than training on GPUs where
    you create `n` models that have their gradients synced and back-propagated at
    certain moments. Instead, one model instance is shared between all the nodes and
    it is passed back and forth. This is important especially when training on low-resource
    TPUs such as those provided in Kaggle kernels or on Google Colaboratory.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example of a training function passed to the [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)
    if training on CPUs or GPUs:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This code snippet is based off the one from the `simple_nlp_example` notebook
    found [here](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb)
    with slight modifications for the sake of simplicity
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `notebook_launcher` will default to 8 processes if 🤗 Accelerate has been
    configured for a TPU
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use this example and declare the model *inside* the training loop, then
    on a low-resource system you will potentially see an error like:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This error is *extremely* cryptic but the basic explanation is you ran out
    of system RAM. You can avoid this entirely by reconfiguring the training function
    to accept a single `model` argument, and declare it in an outside cell:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And finally calling the training function with:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The above workaround is only needed when launching a TPU instance from a Jupyter
    Notebook on a low-resource server such as Google Colaboratory or Kaggle. If using
    a script or launching on a much beefier server declaring the model beforehand
    is not needed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Mixed Precision and Global Variables
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the [mixed precision tutorial](../usage_guides/mixed_precision),
    🤗 Accelerate supports fp16 and bf16, both of which can be used on TPUs. That being
    said, ideally `bf16` should be utilized as it is extremely efficient to use.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: There are two “layers” when using `bf16` and 🤗 Accelerate on TPUs, at the base
    level and at the operation level.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'At the base level, this is enabled when passing `mixed_precision="bf16"` to
    `Accelerator`, such as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By default, this will cast `torch.float` and `torch.double` to `bfloat16` on
    TPUs. The specific configuration being set is an environmental variable of `XLA_USE_BF16`
    is set to `1`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: There is a further configuration you can perform which is setting the `XLA_DOWNCAST_BF16`
    environmental variable. If set to `1`, then `torch.float` is `bfloat16` and `torch.double`
    is `float32`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'This is performed in the `Accelerator` object when passing `downcast_bf16=True`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在传递`downcast_bf16=True`时在`Accelerator`对象中执行的。
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using downcasting instead of bf16 everywhere is good for when you are trying
    to calculate metrics, log values, and more where raw bf16 tensors would be unusable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算指标、记录值等需要使用原始bf16张量时，使用downcasting而不是bf16是很好的选择。
- en: Training Times on TPUs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU上的训练时间
- en: As you launch your script, you may notice that training seems exceptionally
    slow at first. This is because TPUs first run through a few batches of data to
    see how much memory to allocate before finally utilizing this configured memory
    allocation extremely efficiently.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动脚本时，您可能会注意到训练一开始似乎异常缓慢。这是因为TPU首先运行几批数据，以查看需要分配多少内存，然后才能极其高效地利用这个配置的内存分配。
- en: If you notice that your evaluation code to calculate the metrics of your model
    takes longer due to a larger batch size being used, it is recommended to keep
    the batch size the same as the training data if it is too slow. Otherwise the
    memory will reallocate to this new batch size after the first few iterations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您注意到评估代码计算模型指标所需的时间更长，这是因为使用了更大的批量大小，建议如果速度太慢，则保持批量大小与训练数据相同。否则，在前几次迭代之后，内存将重新分配到这个新的批量大小。
- en: Just because the memory is allocated does not mean it will be used or that the
    batch size will increase when going back to your training dataloader.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为内存被分配并不意味着它会被使用，或者当返回到训练数据加载器时批量大小会增加。
