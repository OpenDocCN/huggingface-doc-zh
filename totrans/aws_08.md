# åœ¨AWS Trainiumä¸Šä¸ºæ–‡æœ¬åˆ†ç±»å¾®è°ƒBERT

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert)

*è¿™ä¸ªæ•™ç¨‹æœ‰ä¸€ä¸ªç¬”è®°æœ¬ç‰ˆæœ¬[åœ¨è¿™é‡Œ](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/notebook.ipynb)*ã€‚

è¿™ä¸ªæ•™ç¨‹å°†å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨[AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls)å’ŒHugging Face Transformersã€‚å®ƒå°†æ¶µç›–å¦‚ä½•åœ¨AWSä¸Šè®¾ç½®Trainiumå®ä¾‹ï¼ŒåŠ è½½å’Œå¾®è°ƒä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†ç±»çš„transformersæ¨¡å‹

æ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š

1.  [è®¾ç½®AWSç¯å¢ƒ](#1-setup-aws-environment)

1.  [åŠ è½½å’Œå¤„ç†æ•°æ®é›†](#2-load-and-process-the-dataset)

1.  [ä½¿ç”¨Hugging Face Transformerså’ŒOptimum Neuronå¾®è°ƒBERT](#3-fine-tune-bert-using-hugging-face-transformers)

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰ä¸€ä¸ª[Hugging Faceè´¦æˆ·](https://huggingface.co/join)ä»¥ä¿å­˜å·¥ä»¶å’Œå®éªŒã€‚

## å¿«é€Ÿä»‹ç»ï¼šAWS Trainium

[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/)æ˜¯ä¸“ä¸ºæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰è®­ç»ƒå·¥ä½œè´Ÿè½½è€Œæ„å»ºçš„EC2ã€‚Trainiumæ˜¯[AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls)çš„ç»§ä»»è€…ï¼Œä¸“æ³¨äºé«˜æ€§èƒ½è®­ç»ƒå·¥ä½œè´Ÿè½½ï¼Œå£°ç§°ä¸å¯æ¯”è¾ƒçš„åŸºäºGPUçš„å®ä¾‹ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬èŠ‚çœé«˜è¾¾50%ã€‚

Trainiumå·²ç»é’ˆå¯¹è®­ç»ƒè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæ¨èæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚è¯¥åŠ é€Ÿå™¨æ”¯æŒå¹¿æ³›çš„æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬FP32ã€TF32ã€BF16ã€FP16ã€UINT8å’Œå¯é…ç½®çš„FP8ã€‚

æœ€å¤§çš„Trainiumå®ä¾‹ï¼Œ`trn1.32xlarge`æ‹¥æœ‰è¶…è¿‡500GBçš„å†…å­˜ï¼Œä½¿å¾—åœ¨å•ä¸ªå®ä¾‹ä¸Šè½»æ¾å¾®è°ƒçº¦10Bå‚æ•°æ¨¡å‹å˜å¾—å®¹æ˜“ã€‚ä¸‹é¢æ˜¯å¯ç”¨å®ä¾‹ç±»å‹çš„æ¦‚è¿°ã€‚æ›´å¤šç»†èŠ‚[åœ¨è¿™é‡Œ](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details)ï¼š

| å®ä¾‹å¤§å° | åŠ é€Ÿå™¨ | åŠ é€Ÿå™¨å†…å­˜ | vCPU | CPUå†…å­˜ | æ¯å°æ—¶ä»·æ ¼ |
| --- | --- | --- | --- | --- | --- |
| trn1.2xlarge | 1 | 32 | 8 | 32 | $1.34 |
| trn1.32xlarge | 16 | 512 | 128 | 512 | $21.50 |
| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | $24.78 |

* * *

ç°åœ¨æˆ‘ä»¬çŸ¥é“Trainiumæä¾›äº†ä»€ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€

*æ³¨æ„ï¼šè¿™ä¸ªæ•™ç¨‹æ˜¯åœ¨ä¸€ä¸ªtrn1.2xlargeçš„AWS EC2å®ä¾‹ä¸Šåˆ›å»ºçš„ã€‚*

## 1\. è®¾ç½®AWSç¯å¢ƒ

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨AWSä¸Šä½¿ç”¨`trn1.2xlarge`å®ä¾‹ï¼ŒåŒ…æ‹¬1ä¸ªåŠ é€Ÿå™¨ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªNeuron Coreså’Œ[Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2)ã€‚

è¿™ç¯‡åšæ–‡ä¸è¯¦ç»†ä»‹ç»å¦‚ä½•åˆ›å»ºå®ä¾‹ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æˆ‘çš„ä¹‹å‰å…³äº[â€œä¸ºHugging Face Transformersè®¾ç½®AWS Trainiumâ€](https://www.philschmid.de/setup-aws-trainium)çš„åšå®¢ï¼Œå…¶ä¸­åŒ…æ‹¬å…³äºè®¾ç½®ç¯å¢ƒçš„é€æ­¥æŒ‡å—ã€‚

ä¸€æ—¦å®ä¾‹å¯åŠ¨è¿è¡Œï¼Œæˆ‘ä»¬å¯ä»¥sshè¿›å…¥å®ƒã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æƒ³åœ¨ç»ˆç«¯å†…å¼€å‘ï¼Œè€Œæ˜¯æƒ³ä½¿ç”¨ä¸€ä¸ª`Jupyter`ç¯å¢ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥å‡†å¤‡æ•°æ®é›†å’Œå¯åŠ¨è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨`ssh`å‘½ä»¤ä¸­æ·»åŠ ä¸€ä¸ªç”¨äºè½¬å‘çš„ç«¯å£ï¼Œè¿™å°†æŠŠæˆ‘ä»¬çš„æœ¬åœ°ä¸»æœºæµé‡éš§é“åˆ°Trainiumå®ä¾‹ã€‚

```py
PUBLIC_DNS="" # IP address, e.g. ec2-3-80-....
KEY_PATH="" # local path to key, e.g. ssh/trn.pem

ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯åŠ¨æˆ‘ä»¬çš„**`jupyter`**æœåŠ¡å™¨ã€‚

```py
python -m notebook --allow-root --port=8080
```

æ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªç†Ÿæ‚‰çš„**`jupyter`**è¾“å‡ºï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæŒ‡å‘ç¬”è®°æœ¬çš„URLã€‚

**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**

æˆ‘ä»¬å¯ä»¥ç‚¹å‡»å®ƒï¼Œåœ¨æˆ‘ä»¬çš„æœ¬åœ°æµè§ˆå™¨ä¸­æ‰“å¼€ä¸€ä¸ª**`jupyter`**ç¯å¢ƒã€‚

![jupyter.webp](../Images/f3e7326719a8cc7f67122b89fb3e1dc1.png)

æˆ‘ä»¬å°†ä»…ä½¿ç”¨Jupyterç¯å¢ƒå‡†å¤‡æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`torchrun`åœ¨ä¸¤ä¸ªNeuron Coresä¸Šå¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬å¹¶å¼€å§‹å§ã€‚

## 2\. åŠ è½½å’Œå¤„ç†æ•°æ®é›†

æˆ‘ä»¬æ­£åœ¨å¯¹[æƒ…æ„Ÿ](https://huggingface.co/datasets/philschmid/emotion)æ•°æ®é›†ä¸Šçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¿æŒç¤ºä¾‹ç®€å•ã€‚`emotion`æ˜¯ä¸€ä¸ªåŒ…å«å…­ç§åŸºæœ¬æƒ…ç»ªï¼ˆæ„¤æ€’ã€ææƒ§ã€å–œæ‚¦ã€çˆ±ã€æ‚²ä¼¤å’ŒæƒŠè®¶ï¼‰çš„è‹±æ–‡Twitteræ¶ˆæ¯æ•°æ®é›†ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨[ğŸ¤— Datasets](https://huggingface.co/docs/datasets/index)åº“ä¸­çš„`load_dataset()`æ–¹æ³•åŠ è½½`emotion`ã€‚

```py
from datasets import load_dataset

# Dataset id from huggingface.co/dataset
dataset_id = "philschmid/emotion"

# Load raw dataset
raw_dataset = load_dataset(dataset_id)

print(f"Train dataset size: {len(raw_dataset['train'])}")
print(f"Test dataset size: {len(raw_dataset['test'])}")

# Train dataset size: 16000
# Test dataset size: 2000
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†çš„ç¤ºä¾‹ã€‚

```py
from random import randrange

random_id = randrange(len(raw_dataset['train']))
raw_dataset['train'][random_id]
# {'text': 'i feel isolated and alone in my trade', 'label': 0}
```

æˆ‘ä»¬å¿…é¡»å°†æˆ‘ä»¬çš„â€œè‡ªç„¶è¯­è¨€â€è½¬æ¢ä¸ºæ ‡è®°IDä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªåˆ†è¯å™¨å®Œæˆçš„ï¼Œå®ƒå¯¹è¾“å…¥è¿›è¡Œåˆ†è¯ï¼ˆåŒ…æ‹¬å°†æ ‡è®°è½¬æ¢ä¸ºé¢„è®­ç»ƒè¯æ±‡è¡¨ä¸­å¯¹åº”çš„IDï¼‰ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[Hugging Faceè¯¾ç¨‹](https://huggingface.co/course/chapter1/1)ä¸­çš„[ç¬¬6ç« ](https://huggingface.co/course/chapter6/1?fw=pt)ã€‚

æˆ‘ä»¬çš„NeuronåŠ é€Ÿå™¨æœŸæœ›è¾“å…¥å…·æœ‰å›ºå®šçš„å½¢çŠ¶ã€‚æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æˆªæ–­æˆ–å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ã€‚

```py
from transformers import AutoTokenizer
import os
# Model id to load the tokenizer
model_id = "bert-base-uncased"
save_dataset_path = "lm_dataset"
# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Tokenize helper function
def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True,return_tensors="pt")

# Tokenize dataset
raw_dataset =  raw_dataset.rename_column("label", "labels") # to match Trainer
tokenized_dataset = raw_dataset.map(tokenize, batched=True, remove_columns=["text"])
tokenized_dataset = tokenized_dataset.with_format("torch")

# save dataset to disk
tokenized_dataset["train"].save_to_disk(os.path.join(save_dataset_path,"train"))
tokenized_dataset["test"].save_to_disk(os.path.join(save_dataset_path,"eval"))
```

## 3. ä½¿ç”¨Hugging Face Transformerså¾®è°ƒBERT

é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨ä¼šä½¿ç”¨[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)å’Œ[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)æ¥å¾®è°ƒåŸºäºPyTorchçš„transformeræ¨¡å‹ã€‚

ä½†æ˜¯ï¼Œä¸AWSä¸€èµ·ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª[NeuronTrainer](https://huggingface.co/docs/optimum-neuron/package_reference/trainer)ï¼Œä»¥æé«˜åœ¨Trainiumæˆ–Inferentia2å®ä¾‹ä¸Šè®­ç»ƒæ—¶çš„æ€§èƒ½ã€ç¨³å¥æ€§å’Œå®‰å…¨æ€§ã€‚`NeuronTrainer`è¿˜é…å¤‡äº†ä¸€ä¸ª[æ¨¡å‹ç¼“å­˜](https://www.notion.so/Getting-started-with-AWS-Trainium-and-Hugging-Face-Transformers-8428c72556194aed9c393de101229dcf)ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨Hugging Face Hubä¸­çš„é¢„ç¼–è¯‘æ¨¡å‹å’Œé…ç½®ï¼Œè·³è¿‡è®­ç»ƒå¼€å§‹æ—¶éœ€è¦çš„ç¼–è¯‘æ­¥éª¤ã€‚è¿™å¯ä»¥å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­çº¦3å€ã€‚

`NeuronTrainer`æ˜¯`optimum-neuron`åº“çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥ä½œä¸º`Trainer`çš„ä¸€å¯¹ä¸€æ›¿ä»£å“ä½¿ç”¨ã€‚æ‚¨åªéœ€è°ƒæ•´è®­ç»ƒè„šæœ¬ä¸­çš„å¯¼å…¥å³å¯ã€‚

```py
- from transformers import Trainer, TrainingArguments
+ from optimum.neuron import NeuronTrainer as Trainer
+ from optimum.neuron import NeuronTrainingArguments as TrainingArguments
```

æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªç®€å•çš„[train.py](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/scripts/train.py)è®­ç»ƒè„šæœ¬ï¼ŒåŸºäº[â€œGetting started with Pytorch 2.0 and Hugging Face Transformersâ€](https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer)åšå®¢æ–‡ç« ï¼Œä½¿ç”¨`NeuronTrainer`ã€‚ä»¥ä¸‹æ˜¯æ‘˜å½•

```py
from transformers import TrainingArguments
from optimum.neuron import NeuronTrainer as Trainer

def parse_args():
	...

def training_function(args):

    # load dataset from disk and tokenizer
    train_dataset = load_from_disk(os.path.join(args.dataset_path, "train"))
		...

    # Download the model from huggingface.co/models
    model = AutoModelForSequenceClassification.from_pretrained(
        args.model_id, num_labels=num_labels, label2id=label2id, id2label=id2label
    )

    training_args = TrainingArguments(
			...
    )

    # Create Trainer instance
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
    )

    # Start training
    trainer.train()
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`wget`å‘½ä»¤å°†è®­ç»ƒè„šæœ¬åŠ è½½åˆ°æˆ‘ä»¬çš„ç¯å¢ƒä¸­ï¼Œä¹Ÿå¯ä»¥ä»[è¿™é‡Œ](https://github.com/huggingface/optimum-neuron/blob/notebooks/text-classification/scripts/train.py)æ‰‹åŠ¨å¤åˆ¶åˆ°ç¬”è®°æœ¬ä¸­ã€‚

```py
!wget https://raw.githubusercontent.com/huggingface/optimum-neuron/main/notebooks/text-classification/scripts/train.py
```

æˆ‘ä»¬å°†ä½¿ç”¨`torchrun`åœ¨ä¸¤ä¸ªç¥ç»å…ƒæ ¸å¿ƒä¸Šå¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚`torchrun`æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œå¯ä»¥è‡ªåŠ¨å°†PyTorchæ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ªåŠ é€Ÿå™¨ä¸Šã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¶…å‚æ•°æ—è¾¹ä¼ é€’åŠ é€Ÿå™¨æ•°é‡ä½œä¸º`nproc_per_node`å‚æ•°ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```py
!torchrun --nproc_per_node=2 train.py \
 --model_id bert-base-uncased \
 --dataset_path lm_dataset \
 --lr 5e-5 \
 --per_device_train_batch_size 16 \
 --bf16 True \
 --epochs 3
```

***æ³¨æ„**ï¼šå¦‚æœæ‚¨çœ‹åˆ°ç³Ÿç³•çš„å‡†ç¡®ç‡ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æš‚æ—¶åœç”¨`bf16`ã€‚*

ç»è¿‡9åˆ†é’Ÿçš„è®­ç»ƒï¼Œè·å¾—äº†å‡ºè‰²çš„`0.914`çš„f1åˆ†æ•°ã€‚

```py
***** train metrics *****
  epoch                    =        3.0
  train_runtime            =    0:08:30
  train_samples            =      16000
  train_samples_per_second =     96.337

***** eval metrics *****
  eval_f1                  =      0.914
  eval_runtime             =    0:00:08
```

æœ€åï¼Œç»ˆæ­¢EC2å®ä¾‹ä»¥é¿å…ä¸å¿…è¦çš„è´¹ç”¨ã€‚ä»æ€§ä»·æ¯”æ¥çœ‹ï¼Œæˆ‘ä»¬çš„è®­ç»ƒåªèŠ±è´¹äº†**`20ct`**ï¼ˆ**`1.34$/h * 0.15h = 0.20$`**ï¼‰
