- en: Megatron-LM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Megatron-LM
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/megatron_lm](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/usage_guides/megatron_lm](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) enables training large
    transformer language models at scale. It provides efficient tensor, pipeline and
    sequence based model parallelism for pre-training transformer based Language Models
    such as [GPT](https://arxiv.org/abs/2005.14165) (Decoder Only), [BERT](https://arxiv.org/pdf/1810.04805.pdf)
    (Encoder Only) and [T5](https://arxiv.org/abs/1910.10683) (Encoder-Decoder). For
    detailed information and how things work behind the scene please refer the github
    [repo](https://github.com/NVIDIA/Megatron-LM).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)ä½¿å¾—å¯ä»¥åœ¨è§„æ¨¡ä¸Šè®­ç»ƒå¤§å‹transformerè¯­è¨€æ¨¡å‹ã€‚å®ƒä¸ºé¢„è®­ç»ƒåŸºäºtransformerçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚[GPT](https://arxiv.org/abs/2005.14165)ï¼ˆä»…è§£ç å™¨ï¼‰ã€[BERT](https://arxiv.org/pdf/1810.04805.pdf)ï¼ˆä»…ç¼–ç å™¨ï¼‰å’Œ[T5](https://arxiv.org/abs/1910.10683)ï¼ˆç¼–ç å™¨-è§£ç å™¨ï¼‰ï¼‰æä¾›äº†é«˜æ•ˆçš„å¼ é‡ã€ç®¡é“å’ŒåŸºäºåºåˆ—çš„æ¨¡å‹å¹¶è¡Œæ€§ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ä»¥åŠå¹•åå·¥ä½œåŸç†ï¼Œè¯·å‚è€ƒgithub[repo](https://github.com/NVIDIA/Megatron-LM)ã€‚'
- en: What is integrated?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯é›†æˆçš„ï¼Ÿ
- en: 'Accelerate integrates following feature of Megatron-LM to enable large scale
    pre-training/finetuning of BERT (Encoder), GPT (Decoder) or T5 models (Encoder
    and Decoder):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Accelerateé›†æˆäº†Megatron-LMçš„ä»¥ä¸‹åŠŸèƒ½ï¼Œä»¥å®ç°BERTï¼ˆç¼–ç å™¨ï¼‰ã€GPTï¼ˆè§£ç å™¨ï¼‰æˆ–T5æ¨¡å‹ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰çš„å¤§è§„æ¨¡é¢„è®­ç»ƒ/å¾®è°ƒï¼š
- en: 'a. **Tensor Parallelism (TP)**: Reduces memory footprint without much additional
    communication on intra-node ranks. Each tensor is split into multiple chunks with
    each shard residing on separate GPU. At each step, the same mini-batch of data
    is processed independently and in parallel by each shard followed by syncing across
    all GPUs (`all-reduce` operation). In a simple transformer layer, this leads to
    2 `all-reduces` in the forward path and 2 in the backward path. For more details,
    please refer research paper [Megatron-LM: Training Multi-Billion Parameter Language
    Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf) and this
    section of ğŸ¤— blogpost [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'a. **å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰**ï¼šå‡å°‘å†…éƒ¨èŠ‚ç‚¹æ’åä¹‹é—´çš„é€šä¿¡ï¼Œå‡å°‘å†…å­˜å ç”¨ã€‚æ¯ä¸ªå¼ é‡è¢«åˆ†å‰²æˆå¤šä¸ªå—ï¼Œæ¯ä¸ªå—éƒ½é©»ç•™åœ¨ä¸åŒçš„GPUä¸Šã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç›¸åŒçš„æ•°æ®å°æ‰¹é‡ç”±æ¯ä¸ªå—ç‹¬ç«‹å¹¶å¹¶è¡Œå¤„ç†ï¼Œç„¶åé€šè¿‡æ‰€æœ‰GPUè¿›è¡ŒåŒæ­¥ï¼ˆ`all-reduce`æ“ä½œï¼‰ã€‚åœ¨ç®€å•çš„transformerå±‚ä¸­ï¼Œè¿™å¯¼è‡´å‰å‘è·¯å¾„ä¸­æœ‰2ä¸ª`all-reduces`ï¼Œåå‘è·¯å¾„ä¸­æœ‰2ä¸ª`all-reduces`ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒç ”ç©¶è®ºæ–‡[Megatron-LM:
    Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)å’ŒğŸ¤—åšå®¢æ–‡ç« [BLOOM
    TrainingèƒŒåçš„æŠ€æœ¯](https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism)çš„è¿™ä¸€éƒ¨åˆ†ã€‚'
- en: 'b. **Pipeline Parallelism (PP)**: Reduces memory footprint and enables large
    scale training via inter-node parallelization. Reduces the bubble of naive PP
    via PipeDream-Flush schedule/1F1B schedule and Interleaved 1F1B schedule. Layers
    are distributed uniformly across PP stages. For example, if a model has `24` layers
    and we have `4` GPUs for pipeline parallelism, each GPU will have `6` layers (24/4).
    For more details on schedules to reduce the idle time of PP, please refer to the
    research paper [Efficient Large-Scale Language Model Training on GPU Clusters
    Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf) and this section of ğŸ¤—
    blogpost [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: b. **ç®¡é“å¹¶è¡Œï¼ˆPPï¼‰**ï¼šé€šè¿‡èŠ‚ç‚¹é—´å¹¶è¡ŒåŒ–å‡å°‘å†…å­˜å ç”¨å¹¶å®ç°å¤§è§„æ¨¡è®­ç»ƒã€‚é€šè¿‡PipeDream-Flushè°ƒåº¦/1F1Bè°ƒåº¦å’Œäº¤æ›¿1F1Bè°ƒåº¦å‡å°‘äº†å¤©çœŸPPçš„æ°”æ³¡ã€‚å±‚åœ¨PPé˜¶æ®µå‡åŒ€åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰`24`å±‚ï¼Œæˆ‘ä»¬æœ‰`4`ä¸ªGPUç”¨äºç®¡é“å¹¶è¡Œï¼Œæ¯ä¸ªGPUå°†æœ‰`6`å±‚ï¼ˆ24/4ï¼‰ã€‚æœ‰å…³å‡å°‘PPç©ºé—²æ—¶é—´çš„è°ƒåº¦çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒç ”ç©¶è®ºæ–‡[Efficient
    Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)å’ŒğŸ¤—åšå®¢æ–‡ç« [BLOOM
    TrainingèƒŒåçš„æŠ€æœ¯](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism)çš„è¿™ä¸€éƒ¨åˆ†ã€‚
- en: 'c. **Sequence Parallelism (SP)**: Reduces memory footprint without any additional
    communication. Only applicable when using TP. It reduces activation memory required
    as it prevents the same copies to be on the tensor parallel ranks post `all-reduce`
    by replacing then with `reduce-scatter` and `no-op` operation would be replaced
    by `all-gather`. As `all-reduce = reduce-scatter + all-gather`, this saves a ton
    of activation memory at no added communication cost. To put it simply, it shards
    the outputs of each transformer layer along sequence dimension, e.g., if the sequence
    length is `1024` and the TP size is `4`, each GPU will have `256` tokens (1024/4)
    for each sample. This increases the batch size that can be supported for training.
    For more details, please refer to the research paper [Reducing Activation Recomputation
    in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: c. **åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰**ï¼šåœ¨ä¸å¢åŠ ä»»ä½•é¢å¤–é€šä¿¡çš„æƒ…å†µä¸‹å‡å°‘å†…å­˜å ç”¨ã€‚ä»…åœ¨ä½¿ç”¨TPæ—¶é€‚ç”¨ã€‚å®ƒå‡å°‘äº†æ¿€æ´»å†…å­˜çš„éœ€æ±‚ï¼Œå› ä¸ºå®ƒé€šè¿‡åœ¨`all-reduce`åæ›¿æ¢ç›¸åŒçš„å‰¯æœ¬åˆ°å¼ é‡å¹¶è¡Œæ’åä¸Šæ¥é˜²æ­¢è¿™ç§æƒ…å†µï¼Œé€šè¿‡`reduce-scatter`æ›¿æ¢`no-op`æ“ä½œï¼Œ`all-reduce
    = reduce-scatter + all-gather`ï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå¤§é‡æ¿€æ´»å†…å­˜è€Œä¸å¢åŠ é€šä¿¡æˆæœ¬ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒæ²¿ç€åºåˆ—ç»´åº¦åˆ†ç‰‡æ¯ä¸ªtransformerå±‚çš„è¾“å‡ºï¼Œä¾‹å¦‚ï¼Œå¦‚æœåºåˆ—é•¿åº¦ä¸º`1024`ï¼ŒTPå¤§å°ä¸º`4`ï¼Œæ¯ä¸ªGPUå°†æœ‰`256`ä¸ªæ ‡è®°ï¼ˆ1024/4ï¼‰ç”¨äºæ¯ä¸ªæ ·æœ¬ã€‚è¿™å¢åŠ äº†æ”¯æŒè®­ç»ƒçš„æ‰¹é‡å¤§å°ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒç ”ç©¶è®ºæ–‡[Reducing
    Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf)ã€‚
- en: 'd. **Data Parallelism (DP)** via Distributed Optimizer: Reduces the memory
    footprint by sharding optimizer states and gradients across DP ranks (versus the
    traditional method of replicating the optimizer state across data parallel ranks).
    For example, when using Adam optimizer with mixed-precision training, each parameter
    accounts for 12 bytes of memory. This gets distributed equally across the GPUs,
    i.e., each parameter would account for 3 bytes (12/4) if we have 4 GPUs. For more
    details, please refer the research paper [ZeRO: Memory Optimizations Toward Training
    Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf) and following
    section of ğŸ¤— blog [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'd. **æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰** é€šè¿‡åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ï¼šé€šè¿‡åœ¨ DP æ’åä¹‹é—´åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ¥å‡å°‘å†…å­˜å ç”¨é‡ï¼ˆä¸ä¼ ç»Ÿæ–¹æ³•åœ¨æ•°æ®å¹¶è¡Œæ’åä¹‹é—´å¤åˆ¶ä¼˜åŒ–å™¨çŠ¶æ€ç›¸æ¯”ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒçš„
    Adam ä¼˜åŒ–å™¨æ—¶ï¼Œæ¯ä¸ªå‚æ•°å ç”¨ 12 å­—èŠ‚çš„å†…å­˜ã€‚è¿™äº›å†…å­˜å‡åŒ€åˆ†å¸ƒåœ¨ GPU ä¸Šï¼Œå³å¦‚æœæœ‰ 4 ä¸ª GPUï¼Œåˆ™æ¯ä¸ªå‚æ•°å°†å ç”¨ 3 å­—èŠ‚ï¼ˆ12/4ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç ”ç©¶è®ºæ–‡
    [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
    å’Œ ğŸ¤— åšå®¢ [BLOOM Training èƒŒåçš„æŠ€æœ¯](https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism)
    çš„ä»¥ä¸‹éƒ¨åˆ†ã€‚'
- en: 'e. **Selective Activation Recomputation**: Reduces the memory footprint of
    activations significantly via smart activation checkpointing. It doesnâ€™t store
    activations occupying large memory while being fast to recompute thereby achieving
    great tradeoff between memory and recomputation. For example, for GPT-3, this
    leads to 70% reduction in required memory for activations at the expense of only
    2.7% FLOPs overhead for recomputation of activations. For more details, please
    refer to the research paper [Reducing Activation Recomputation in Large Transformer
    Models](https://arxiv.org/pdf/2205.05198.pdf).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: e. **é€‰æ‹©æ€§æ¿€æ´»é‡è®¡ç®—**ï¼šé€šè¿‡æ™ºèƒ½æ¿€æ´»æ£€æŸ¥ç‚¹å‡å°‘æ¿€æ´»çš„å†…å­˜å ç”¨ã€‚å®ƒä¸ä¼šå­˜å‚¨å ç”¨å¤§é‡å†…å­˜çš„æ¿€æ´»ï¼ŒåŒæ—¶é‡æ–°è®¡ç®—é€Ÿåº¦å¿«ï¼Œä»è€Œåœ¨å†…å­˜å’Œé‡æ–°è®¡ç®—ä¹‹é—´å–å¾—å¾ˆå¥½çš„æŠ˜è¡·ã€‚ä¾‹å¦‚ï¼Œå¯¹äº
    GPT-3ï¼Œè¿™å¯¼è‡´æ¿€æ´»æ‰€éœ€å†…å­˜å‡å°‘äº† 70%ï¼Œè€Œä»…ä»¥ 2.7% çš„ FLOPs å¼€é”€é‡æ–°è®¡ç®—æ¿€æ´»ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç ”ç©¶è®ºæ–‡ [Reducing Activation
    Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf)ã€‚
- en: 'f. **Fused Kernels**: Fused Softmax, Mixed Precision Fused Layer Norm and Fused
    gradient accumulation to weight gradient computation of linear layer. PyTorch
    JIT compiled Fused GeLU and Fused Bias+Dropout+Residual addition.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: f. **èåˆå†…æ ¸**ï¼šèåˆ Softmaxã€æ··åˆç²¾åº¦èåˆå±‚å½’ä¸€åŒ–å’Œèåˆæ¢¯åº¦ç´¯ç§¯ä»¥æƒé‡æ¢¯åº¦è®¡ç®—çº¿æ€§å±‚ã€‚PyTorch JIT ç¼–è¯‘çš„èåˆ GeLU å’Œèåˆ
    Bias+Dropout+Residual additionã€‚
- en: 'g. **Support for Indexed datasets**: Efficient binary format of datasets for
    large scale training. Support for the `mmap`, `cached` index file and the `lazy`
    loader format.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: g. **æ”¯æŒç´¢å¼•æ•°æ®é›†**ï¼šç”¨äºå¤§è§„æ¨¡è®­ç»ƒçš„æ•°æ®é›†çš„é«˜æ•ˆäºŒè¿›åˆ¶æ ¼å¼ã€‚æ”¯æŒ `mmap`ã€`cached` ç´¢å¼•æ–‡ä»¶å’Œ `lazy` åŠ è½½å™¨æ ¼å¼ã€‚
- en: 'h. **Checkpoint reshaping and interoperability**: Utility for reshaping Megatron-LM
    checkpoints of variable tensor and pipeline parallel sizes to the beloved ğŸ¤— Transformers
    sharded checkpoints as it has great support with plethora of tools such as ğŸ¤— Accelerate
    Big Model Inference, Megatron-DeepSpeed Inference etc. Support is also available
    for converting ğŸ¤— Transformers sharded checkpoints to Megatron-LM checkpoint of
    variable tensor and pipeline parallel sizes for large scale training.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: h. **æ£€æŸ¥ç‚¹é‡å¡‘å’Œäº’æ“ä½œæ€§**ï¼šç”¨äºå°†å˜é‡å¼ é‡å’Œç®¡é“å¹¶è¡Œå¤§å°çš„ Megatron-LM æ£€æŸ¥ç‚¹é‡å¡‘ä¸ºå¤‡å—å–œçˆ±çš„ ğŸ¤— Transformers åˆ†ç‰‡æ£€æŸ¥ç‚¹çš„å®ç”¨ç¨‹åºï¼Œå› ä¸ºå®ƒä¸ä¼—å¤šå·¥å…·ï¼ˆå¦‚
    ğŸ¤— Accelerate Big Model Inferenceã€Megatron-DeepSpeed Inference ç­‰ï¼‰å…·æœ‰è‰¯å¥½çš„æ”¯æŒã€‚è¿˜æ”¯æŒå°† ğŸ¤—
    Transformers åˆ†ç‰‡æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºå˜é‡å¼ é‡å’Œç®¡é“å¹¶è¡Œå¤§å°çš„ Megatron-LM æ£€æŸ¥ç‚¹ï¼Œç”¨äºå¤§è§„æ¨¡è®­ç»ƒã€‚
- en: Pre-Requisites
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ˆå†³æ¡ä»¶
- en: You will need to install the latest pytorch, cuda, nccl, and NVIDIA [APEX](https://github.com/NVIDIA/apex#quick-start)
    releases and the nltk library. See [documentation](https://github.com/NVIDIA/Megatron-LM#setup)
    for more details. Another way to setup the environment is to pull an NVIDIA PyTorch
    Container that comes with all the required installations from NGC.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦å®‰è£…æœ€æ–°çš„ pytorchã€cudaã€nccl å’Œ NVIDIA [APEX](https://github.com/NVIDIA/apex#quick-start)
    ç‰ˆæœ¬ä»¥åŠ nltk åº“ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://github.com/NVIDIA/Megatron-LM#setup)ã€‚å¦ä¸€ç§è®¾ç½®ç¯å¢ƒçš„æ–¹æ³•æ˜¯æ‹‰å–ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰€éœ€å®‰è£…çš„
    NVIDIA PyTorch å®¹å™¨ã€‚
- en: 'Below is a step-by-step method to set up the conda environment:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è®¾ç½® conda ç¯å¢ƒçš„é€æ­¥æ–¹æ³•ï¼š
- en: Create a virtual environment
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Assuming that the machine has CUDA 11.3 installed, installing the corresponding
    PyTorch GPU Version
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡è®¾æœºå™¨å·²å®‰è£… CUDA 11.3ï¼Œå®‰è£…ç›¸åº”çš„ PyTorch GPU ç‰ˆæœ¬
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install Nvidia APEX
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£… Nvidia APEX
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Installing Megatron-LM
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£… Megatron-LM
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Accelerate Megatron-LM Plugin
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ é€Ÿ Megatron-LM æ’ä»¶
- en: 'Important features are directly supported via the `accelerate config` command.
    An example of the corresponding questions for using Megatron-LM features is shown
    below:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦åŠŸèƒ½é€šè¿‡ `accelerate config` å‘½ä»¤ç›´æ¥æ”¯æŒã€‚ä¸‹é¢æ˜¾ç¤ºäº†ä½¿ç”¨ Megatron-LM åŠŸèƒ½çš„ç›¸åº”é—®é¢˜çš„ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The resulting config is shown below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœé…ç½®å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will take the example of GPT pre-training. The minimal changes required
    to the official `run_clm_no_trainer.py` to use Megatron-LM are as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»¥ GPT é¢„è®­ç»ƒä¸ºä¾‹ã€‚è¦ä½¿ç”¨ Megatron-LMï¼Œå¯¹å®˜æ–¹çš„ `run_clm_no_trainer.py` è¿›è¡Œçš„æœ€å°æ›´æ”¹å¦‚ä¸‹ï¼š
- en: 'As Megatron-LM uses its own implementation of Optimizer, the corresponding
    scheduler compatible with it needs to be used. As such, support for only the Megatron-LMâ€™s
    scheduler is present. User will need to create `accelerate.utils.MegatronLMDummyScheduler`.
    Example is given below:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”±äº Megatron-LM ä½¿ç”¨è‡ªå·±çš„ä¼˜åŒ–å™¨å®ç°ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ä¸ä¹‹å…¼å®¹çš„ç›¸åº”è°ƒåº¦å™¨ã€‚å› æ­¤ï¼Œä»…æ”¯æŒ Megatron-LM çš„è°ƒåº¦å™¨ã€‚ç”¨æˆ·éœ€è¦åˆ›å»º `accelerate.utils.MegatronLMDummyScheduler`ã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Getting the details of the total batch size now needs to be cognization of
    tensor and pipeline parallel sizes. Example of getting the effective total batch
    size is shown below:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨éœ€è¦äº†è§£æ€»æ‰¹é‡å¤§å°çš„ç»†èŠ‚ï¼Œéœ€è¦è®¤çŸ¥å¼ é‡å’Œç®¡é“å¹¶è¡Œå¤§å°ã€‚è·å–æœ‰æ•ˆæ€»æ‰¹é‡å¤§å°çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When using Megatron-LM, the losses are already averaged across the data parallel
    group
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ Megatron-LM æ—¶ï¼ŒæŸå¤±å·²ç»åœ¨æ•°æ®å¹¶è¡Œç»„ä¸­è¿›è¡Œäº†å¹³å‡
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For Megatron-LM, we need to save the model using `accelerator.save_state`
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äº Megatron-LMï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `accelerator.save_state` æ¥ä¿å­˜æ¨¡å‹
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Thatâ€™s it! We are good to go ğŸš€. Please find the example script in the examples
    folder at the path `accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py`.
    Letâ€™s run it for `gpt-large` model architecture using 4 A100-80GB GPUs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·äº†ï¼æˆ‘ä»¬å‡†å¤‡å¥½å‡ºå‘äº†ğŸš€ã€‚è¯·åœ¨è·¯å¾„`accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py`çš„ç¤ºä¾‹è„šæœ¬ä¸­æ‰¾åˆ°ç¤ºä¾‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨4ä¸ªA100-80GB
    GPUæ¥è¿è¡Œ`gpt-large`æ¨¡å‹æ¶æ„ã€‚
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Below are some important excerpts from the output logs:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¾“å‡ºæ—¥å¿—ä¸­çš„ä¸€äº›é‡è¦æ‘˜å½•ï¼š
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There are a large number of other options/features that one can set using `accelerate.utils.MegatronLMPlugin`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰è®¸å¤šå…¶ä»–é€‰é¡¹/åŠŸèƒ½å¯ä»¥ä½¿ç”¨`accelerate.utils.MegatronLMPlugin`è®¾ç½®ã€‚
- en: Advanced features to leverage writing custom train step and Megatron-LM Indexed
    Datasets
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤å’ŒMegatron-LMç´¢å¼•æ•°æ®é›†çš„é«˜çº§åŠŸèƒ½
- en: For leveraging more features, please go through below details.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åˆ©ç”¨æ›´å¤šåŠŸèƒ½ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ã€‚
- en: Below is an example of changes required to customize the Train Step while using
    Megatron-LM. You will implement the `accelerate.utils.AbstractTrainStep` or inherit
    from their corresponding children `accelerate.utils.GPTTrainStep`, `accelerate.utils.BertTrainStep`
    or `accelerate.utils.T5TrainStep`.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨ä½¿ç”¨Megatron-LMæ—¶è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤æ‰€éœ€çš„æ›´æ”¹ç¤ºä¾‹ã€‚æ‚¨å°†å®ç°`accelerate.utils.AbstractTrainStep`æˆ–ç»§æ‰¿è‡ªå…¶ç›¸åº”çš„å­ç±»`accelerate.utils.GPTTrainStep`ã€`accelerate.utils.BertTrainStep`æˆ–`accelerate.utils.T5TrainStep`ã€‚
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For using the Megatron-LM datasets, a few more changes are required. Dataloaders
    for these datasets are available only on rank 0 of each tensor parallel group.
    As such, there are rank where dataloader wonâ€™t be available and this requires
    tweaks to the training loop. Being able to do all this shows how flexible and
    extensible ğŸ¤— Accelerate is. The changes required are as follows.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨Megatron-LMæ•°æ®é›†ï¼Œéœ€è¦è¿›è¡Œä¸€äº›æ›´æ”¹ã€‚è¿™äº›æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨ä»…åœ¨æ¯ä¸ªå¼ é‡å¹¶è¡Œç»„çš„ç­‰çº§0ä¸Šå¯ç”¨ã€‚å› æ­¤ï¼Œå­˜åœ¨æ•°æ®åŠ è½½å™¨ä¸å¯ç”¨çš„ç­‰çº§ï¼Œè¿™éœ€è¦å¯¹è®­ç»ƒå¾ªç¯è¿›è¡Œè°ƒæ•´ã€‚èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹æ˜¾ç¤ºäº†ğŸ¤—
    Accelerateæœ‰å¤šä¹ˆçµæ´»å’Œå¯æ‰©å±•ã€‚æ‰€éœ€çš„æ›´æ”¹å¦‚ä¸‹ã€‚
- en: a. For Megatron-LM indexed datasets, we need to use `MegatronLMDummyDataLoader`
    and pass the required dataset args to it such as `data_path`, `seq_length` etc.
    See [here](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L804)
    for the list of available args.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: a. å¯¹äºMegatron-LMç´¢å¼•æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`MegatronLMDummyDataLoader`å¹¶å°†æ‰€éœ€çš„æ•°æ®é›†å‚æ•°ä¼ é€’ç»™å®ƒï¼Œä¾‹å¦‚`data_path`ã€`seq_length`ç­‰ã€‚è¯·å‚é˜…[æ­¤å¤„](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L804)ä»¥è·å–å¯ç”¨å‚æ•°çš„åˆ—è¡¨ã€‚
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: b. `megatron_dataloader` is repeated 3 times to get training, validation and
    test dataloaders as per the `args.splits_string` proportions
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: b. `megatron_dataloader`é‡å¤3æ¬¡ï¼Œä»¥æ ¹æ®`args.splits_string`æ¯”ä¾‹è·å–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: c. Changes to training and evaluation loops as dataloader is only available
    on tensor parallel ranks 0 So, we need to iterate only if the dataloader isnâ€™t
    `None` else provide empty dict As such, we loop using `while` loop and break when
    `completed_steps` is equal to `args.max_train_steps` This is similar to the Megatron-LM
    setup wherein user has to provide `max_train_steps` when using Megaton-LM indexed
    datasets. This displays how flexible and extensible ğŸ¤— Accelerate is.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: c. ç”±äºæ•°æ®åŠ è½½å™¨ä»…åœ¨å¼ é‡å¹¶è¡Œç­‰çº§0ä¸Šå¯ç”¨ï¼Œå› æ­¤éœ€è¦å¯¹è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯è¿›è¡Œæ›´æ”¹ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦åœ¨æ•°æ®åŠ è½½å™¨ä¸æ˜¯`None`æ—¶è¿­ä»£ï¼Œå¦åˆ™æä¾›ç©ºå­—å…¸ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨`while`å¾ªç¯è¿›è¡Œå¾ªç¯ï¼Œå¹¶åœ¨`completed_steps`ç­‰äº`args.max_train_steps`æ—¶ä¸­æ–­ã€‚è¿™ç±»ä¼¼äºMegatron-LMè®¾ç½®ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨Megaton-LMç´¢å¼•æ•°æ®é›†æ—¶å¿…é¡»æä¾›`max_train_steps`ã€‚è¿™æ˜¾ç¤ºäº†ğŸ¤—
    Accelerateæœ‰å¤šä¹ˆçµæ´»å’Œå¯æ‰©å±•ã€‚
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Utility for Checkpoint reshaping and interoperability
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨äºæ£€æŸ¥ç‚¹é‡å¡‘å’Œäº’æ“ä½œæ€§çš„å®ç”¨ç¨‹åº
- en: The scripts for these are present in ğŸ¤— Transformers library under respective
    models. Currently, it is available for GPT model [checkpoint_reshaping_and_interoperability.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py)
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™äº›è„šæœ¬ä½äºğŸ¤— Transformersåº“ä¸­çš„ç›¸åº”æ¨¡å‹ä¸‹ã€‚ç›®å‰ï¼Œå®ƒé€‚ç”¨äºGPTæ¨¡å‹[checkpoint_reshaping_and_interoperability.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py)
- en: Below is an example of conversion of checkpoint from Megatron-LM to universal
    ğŸ¤— Transformers sharded checkpoint.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å°†æ£€æŸ¥ç‚¹ä»Megatron-LMè½¬æ¢ä¸ºé€šç”¨ğŸ¤— Transformersåˆ†ç‰‡æ£€æŸ¥ç‚¹çš„ç¤ºä¾‹ã€‚
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Conversion of checkpoint from transformers to megatron with `tp_size=2`, `pp_size=2`
    and `dp_size=2`.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†transformersçš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºmegatronï¼Œä½¿ç”¨`tp_size=2`ï¼Œ`pp_size=2`å’Œ`dp_size=2`ã€‚
- en: '[PRE17]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Megatron-LM GPT models support returning logits and megatron_generate function
    for text generation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Megatron-LM GPTæ¨¡å‹æ”¯æŒè¿”å›å¯¹æ•°å’Œç”¨äºæ–‡æœ¬ç”Ÿæˆçš„megatron_generateå‡½æ•°
- en: Returning logits require setting `require_logits=True` in MegatronLMPlugin as
    shown below. These would be available on the in the last stage of pipeline.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿”å›å¯¹æ•°éœ€è¦åœ¨MegatronLMPluginä¸­è®¾ç½®`require_logits=True`ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚è¿™äº›å°†åœ¨ç®¡é“çš„æœ€åé˜¶æ®µå¯ç”¨ã€‚
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`megatron_generate` method for Megatron-LM GPT model: This will use Tensor
    and Pipeline Parallelism to complete generations for a batch of inputs when using
    greedy with/without top_k/top_p sampling and for individual prompt inputs when
    using beam search decoding. Only a subset of features of transformers generate
    is supported. This will help in using large models via tensor and pipeline parallelism
    for generation (already does key-value caching and uses fused kernels by default).
    This requires data parallel size to be 1, sequence parallelism and activation
    checkpointing to be disabled. It also requires specifying path to tokenizerâ€™s
    vocab file and merges file. Below example shows how to configure and use `megatron_generate`
    method for Megatron-LM GPT model.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Megatron-LM GPTæ¨¡å‹çš„`megatron_generate`æ–¹æ³•ï¼šå½“ä½¿ç”¨è´ªå©ªé‡‡æ ·æˆ–ä¸å¸¦top_k/top_pé‡‡æ ·æ—¶ï¼Œå°†ä½¿ç”¨å¼ é‡å’Œç®¡é“å¹¶è¡Œæ€§æ¥å®Œæˆä¸€æ‰¹è¾“å…¥çš„ç”Ÿæˆï¼Œä»¥åŠå½“ä½¿ç”¨æ³¢æŸæœç´¢è§£ç æ—¶ï¼Œç”¨äºå•ä¸ªæç¤ºè¾“å…¥çš„ç”Ÿæˆã€‚ä»…æ”¯æŒtransformers
    generateçš„ä¸€éƒ¨åˆ†åŠŸèƒ½ã€‚è¿™å°†æœ‰åŠ©äºé€šè¿‡å¼ é‡å’Œç®¡é“å¹¶è¡Œæ€§ä½¿ç”¨å¤§å‹æ¨¡å‹è¿›è¡Œç”Ÿæˆï¼ˆé»˜è®¤æƒ…å†µä¸‹å·²ç»è¿›è¡Œäº†é”®å€¼ç¼“å­˜å¹¶ä½¿ç”¨èåˆå†…æ ¸ï¼‰ã€‚è¿™éœ€è¦å°†æ•°æ®å¹¶è¡Œå¤§å°è®¾ç½®ä¸º1ï¼Œç¦ç”¨åºåˆ—å¹¶è¡Œæ€§å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ã€‚è¿˜éœ€è¦æŒ‡å®šåˆ†è¯å™¨çš„è¯æ±‡æ–‡ä»¶å’Œåˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºäº†å¦‚ä½•é…ç½®å’Œä½¿ç”¨Megatron-LM
    GPTæ¨¡å‹çš„`megatron_generate`æ–¹æ³•ã€‚
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: An end-to-end example of using `megatron_generate` method for Megatron-LM GPT
    model is available at [megatron_gpt2_generation.py](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/inference/megatron_gpt2_generation.py)
    with config file [megatron_lm_gpt_generate_config.yaml](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/Configs/megatron_lm_gpt_generate_config.yaml).
    The bash script with accelerate launch command is available at [megatron_lm_gpt_generate.sh](https://github.com/pacman100/accelerate-megatron-test/blob/main/megatron_lm_gpt_generate.sh).
    The output logs of the script are available at [megatron_lm_gpt_generate.log](https://github.com/pacman100/accelerate-megatron-test/blob/main/output_logs/megatron_lm_gpt_generate.log).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰å…³åœ¨ Megatron-LM GPT æ¨¡å‹ä¸­ä½¿ç”¨ `megatron_generate` æ–¹æ³•çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œè¯·å‚é˜… [megatron_gpt2_generation.py](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/inference/megatron_gpt2_generation.py)ï¼Œé…ç½®æ–‡ä»¶ä¸º
    [megatron_lm_gpt_generate_config.yaml](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/Configs/megatron_lm_gpt_generate_config.yaml)ã€‚å¸¦æœ‰
    accelerate launch å‘½ä»¤çš„ bash è„šæœ¬ä½äº [megatron_lm_gpt_generate.sh](https://github.com/pacman100/accelerate-megatron-test/blob/main/megatron_lm_gpt_generate.sh)ã€‚è„šæœ¬çš„è¾“å‡ºæ—¥å¿—ä½äº
    [megatron_lm_gpt_generate.log](https://github.com/pacman100/accelerate-megatron-test/blob/main/output_logs/megatron_lm_gpt_generate.log)ã€‚
- en: Support for ROPE and ALiBi Positional embeddings and Multi-Query Attention
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¯æŒ ROPE å’Œ ALiBi ä½ç½®åµŒå…¥ä»¥åŠ Multi-Query Attention
- en: For ROPE/ALiBi attention, pass `position_embedding_type` with `("absolute" |
    "rotary" | "alibi")` to `MegatronLMPlugin` as shown below.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äº ROPE/ALiBi æ³¨æ„åŠ›ï¼Œå°† `position_embedding_type` ä¸ `("absolute" | "rotary" | "alibi")`
    ä¼ é€’ç»™ `MegatronLMPlugin`ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '[PRE20]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For Multi-Query Attention, pass `attention_head_type` with `("multihead" | "multiquery")`
    to `MegatronLMPlugin` as shown below.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äº Multi-Query Attentionï¼Œå°† `attention_head_type` ä¸ `("multihead" | "multiquery")`
    ä¼ é€’ç»™ `MegatronLMPlugin`ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '[PRE21]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Caveats
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ³¨æ„äº‹é¡¹
- en: Supports Transformers GPT2, Megatron-BERT and T5 models. This covers Decoder
    only, Encode only and Encoder-Decoder model classes.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ”¯æŒ Transformers GPT2ã€Megatron-BERT å’Œ T5 æ¨¡å‹ã€‚è¿™æ¶µç›–äº†ä»…è§£ç å™¨ã€ä»…ç¼–ç å™¨å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç±»ã€‚
- en: Only loss is returned from model forward pass as there is quite complex interplay
    of pipeline, tensor and data parallelsim behind the scenes. The `model(**batch_data)`
    call return loss(es) averaged across the data parallel ranks. This is fine for
    most cases wherein pre-training jobs are run using Megatron-LM features and you
    can easily compute the `perplexity` using the loss. For GPT model, returning logits
    in addition to loss(es) is supported. These logits arenâ€™t gathered across data
    parallel ranks. Use `accelerator.utils.gather_across_data_parallel_groups` to
    gather logits across data parallel ranks. These logits along with labels can be
    used for computing various performance metrics.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”±äºç®¡é“ã€å¼ é‡å’Œæ•°æ®å¹¶è¡ŒèƒŒåå­˜åœ¨ç›¸å½“å¤æ‚çš„ç›¸äº’ä½œç”¨ï¼Œå› æ­¤ä»…ä»æ¨¡å‹å‰å‘ä¼ é€’ä¸­è¿”å›æŸå¤±ã€‚`model(**batch_data)` è°ƒç”¨è¿”å›è·¨æ•°æ®å¹¶è¡Œç­‰çº§å¹³å‡çš„æŸå¤±ã€‚è¿™å¯¹äºå¤§å¤šæ•°æƒ…å†µæ˜¯å¯ä»¥çš„ï¼Œå…¶ä¸­ä½¿ç”¨
    Megatron-LM åŠŸèƒ½è¿è¡Œé¢„è®­ç»ƒä½œä¸šï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾ä½¿ç”¨æŸå¤±è®¡ç®— `perplexity`ã€‚å¯¹äº GPT æ¨¡å‹ï¼Œé™¤äº†æŸå¤±ä¹‹å¤–è¿˜æ”¯æŒè¿”å› logitsã€‚è¿™äº›
    logits ä¸ä¼šåœ¨æ•°æ®å¹¶è¡Œç­‰çº§ä¹‹é—´æ”¶é›†ã€‚ä½¿ç”¨ `accelerator.utils.gather_across_data_parallel_groups`
    æ¥æ”¶é›†è·¨æ•°æ®å¹¶è¡Œç­‰çº§çš„ logitsã€‚è¿™äº› logits ä»¥åŠæ ‡ç­¾å¯ç”¨äºè®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡ã€‚
- en: The main process is the last rank as the losses/logits are available in the
    last stage of pipeline. `accelerator.is_main_process` and `accelerator.is_local_main_process`
    return `True` for last rank when using Megatron-LM integration.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸»è¿›ç¨‹æ˜¯æœ€åä¸€ä¸ªç­‰çº§ï¼Œå› ä¸ºæŸå¤±/Logits åœ¨ç®¡é“çš„æœ€åé˜¶æ®µå¯ç”¨ã€‚åœ¨ä½¿ç”¨ Megatron-LM é›†æˆæ—¶ï¼Œ`accelerator.is_main_process`
    å’Œ `accelerator.is_local_main_process` åœ¨æœ€åä¸€ä¸ªç­‰çº§è¿”å› `True`ã€‚
- en: In `accelerator.prepare` call, a Megatron-LM model corresponding to a given
    Transformers model is created with random weights. Please use `accelerator.load_state`
    to load the Megatron-LM checkpoint with matching TP, PP and DP partitions.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ `accelerator.prepare` è°ƒç”¨ä¸­ï¼Œå°†ä¸ºç»™å®šçš„ Transformers æ¨¡å‹åˆ›å»ºä¸€ä¸ªå…·æœ‰éšæœºæƒé‡çš„ Megatron-LM æ¨¡å‹ã€‚è¯·ä½¿ç”¨
    `accelerator.load_state` åŠ è½½å…·æœ‰åŒ¹é… TPã€PP å’Œ DP åˆ†åŒºçš„ Megatron-LM æ£€æŸ¥ç‚¹ã€‚
- en: Currently, checkpoint reshaping and interoperability support is only available
    for GPT. Soon it will be extended to BERT and T5.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œæ£€æŸ¥ç‚¹é‡å¡‘å’Œäº’æ“ä½œæ€§æ”¯æŒä»…é€‚ç”¨äº GPTã€‚å¾ˆå¿«å°†æ‰©å±•åˆ° BERT å’Œ T5ã€‚
- en: '`gradient_accumulation_steps` needs to be 1\. When using Megatron-LM, micro
    batches in pipeline parallelism setting is synonymous with gradient accumulation.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` éœ€è¦ä¸º 1ã€‚åœ¨ä½¿ç”¨ Megatron-LM æ—¶ï¼Œç®¡é“å¹¶è¡Œè®¾ç½®ä¸­çš„å¾®æ‰¹æ¬¡ç­‰åŒäºæ¢¯åº¦ç´¯ç§¯ã€‚'
- en: When using Megatron-LM, use `accelerator.save_state` and `accelerator.load_state`
    for saving and loading checkpoints.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ Megatron-LM æ—¶ï¼Œè¯·ä½¿ç”¨ `accelerator.save_state` å’Œ `accelerator.load_state` æ¥ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹ã€‚
- en: Below are the mapping from Megatron-LM model architectures to the the equivalent
    ğŸ¤— transformers model architectures. Only these ğŸ¤— transformers model architectures
    are supported.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä» Megatron-LM æ¨¡å‹æ¶æ„åˆ°ç­‰æ•ˆ ğŸ¤— transformers æ¨¡å‹æ¶æ„çš„æ˜ å°„ã€‚ä»…æ”¯æŒè¿™äº› ğŸ¤— transformers æ¨¡å‹æ¶æ„ã€‚
- en: 'a. Megatron-LM [BertModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py)
    : ğŸ¤— transformers models with `megatron-bert` in configâ€™s model type, e.g., [MegatronBERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: a. Megatron-LM [BertModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py)ï¼šğŸ¤—
    transformers æ¨¡å‹ä¸­é…ç½®ä¸º `megatron-bert` çš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ [MegatronBERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)
- en: 'b. Megatron-LM [GPTModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py)
    : ğŸ¤— transformers models with `gpt2` in configâ€™s model type, e.g., [OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: b. Megatron-LM [GPTModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py)ï¼šğŸ¤—
    transformers æ¨¡å‹ä¸­é…ç½®ä¸º `gpt2` çš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ [OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
- en: 'c. Megatron-LM [T5Model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py)
    : ğŸ¤— transformers models with `t5` in configâ€™s model type, e.g., [T5](https://huggingface.co/docs/transformers/model_doc/t5)
    and [MT5](https://huggingface.co/docs/transformers/model_doc/mt5)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: c. Megatron-LM [T5Model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py)ï¼šğŸ¤—
    transformers æ¨¡å‹ä¸­é…ç½®ä¸º `t5` çš„æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ [T5](https://huggingface.co/docs/transformers/model_doc/t5)
    å’Œ [MT5](https://huggingface.co/docs/transformers/model_doc/mt5)
