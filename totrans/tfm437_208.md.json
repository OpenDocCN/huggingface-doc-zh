["```py\n>>> from transformers import MvpTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n>>> model_with_prompt = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp-summarization\")\n\n>>> inputs = tokenizer(\n...     \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Why You Shouldn't Quit Your Job\"]\n\n>>> generated_ids = model_with_prompt.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Don't do it if these are your reasons\"]\n```", "```py\n>>> from transformers import MvpTokenizerFast, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizerFast.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n>>> model_with_mtl = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n\n>>> inputs = tokenizer(\n...     \"Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Stan Lee created the character of Iron Man, a fictional superhero appearing in American comic']\n\n>>> generated_ids = model_with_mtl.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Iron Man is a fictional superhero appearing in American comic books published by Marvel Comics.']\n```", "```py\n>>> from transformers import MvpForConditionalGeneration\n\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\", use_prompt=True)\n>>> # the number of trainable parameters (full tuning)\n>>> sum(p.numel() for p in model.parameters() if p.requires_grad)\n468116832\n\n>>> # lightweight tuning with randomly initialized prompts\n>>> model.set_lightweight_tuning()\n>>> # the number of trainable parameters (lightweight tuning)\n>>> sum(p.numel() for p in model.parameters() if p.requires_grad)\n61823328\n\n>>> # lightweight tuning with task-specific prompts\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n>>> model.set_lightweight_tuning()\n>>> # original lightweight Prefix-tuning\n>>> model = MvpForConditionalGeneration.from_pretrained(\"facebook/bart-large\", use_prompt=True)\n>>> model.set_lightweight_tuning()\n```", "```py\n( vocab_size = 50267 max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 activation_function = 'gelu' d_model = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 classifier_dropout = 0.0 scale_embedding = False use_cache = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 is_encoder_decoder = True decoder_start_token_id = 2 forced_eos_token_id = 2 use_prompt = False prompt_length = 100 prompt_mid_dim = 800 **kwargs )\n```", "```py\n>>> from transformers import MvpConfig, MvpModel\n\n>>> # Initializing a MVP RUCAIBox/mvp style configuration\n>>> configuration = MvpConfig()\n\n>>> # Initializing a model (with random weights) from the RUCAIBox/mvp style configuration\n>>> model = MvpModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False **kwargs )\n```", "```py\n>>> from transformers import MvpTokenizer\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False trim_offsets = True **kwargs )\n```", "```py\n>>> from transformers import MvpTokenizerFast\n\n>>> tokenizer = MvpTokenizerFast.from_pretrained(\"RUCAIBox/mvp\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: MvpConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MvpModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpModel.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: MvpConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\n...     \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n...     return_tensors=\"pt\",\n... )\n>>> labels = tokenizer(\"Bad Reasons To Quit Your Job\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> loss = model(**inputs, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> with torch.no_grad():\n...     generated_ids = model.generate(**inputs)\n\n>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n```", "```py\n( config: MvpConfig **kwargs )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MvpForSequenceClassification\n\n>>> num_labels = 2  # for example, this is a binary classification task\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForSequenceClassification.from_pretrained(\"RUCAIBox/mvp\", num_labels=num_labels)\n\n>>> inputs = tokenizer(\"Classify: Hello, my dog is cute\", return_tensors=\"pt\")\n>>> labels = torch.tensor(1)  # the real label for inputs\n\n>>> loss = model(**inputs, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax()\n```", "```py\n( config )\n```", "```py\n( input_ids: Tensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MvpForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForQuestionAnswering.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\n...     \"Answer the following question: Who was Jim Henson? [SEP] Jim Henson was a nice puppet\",\n...     return_tensors=\"pt\",\n... )\n>>> target_start_index = torch.tensor([18])\n>>> target_end_index = torch.tensor([19])\n\n>>> loss = model(**inputs, start_positions=target_start_index, end_positions=target_end_index).loss\n>>> loss.backward()\n```", "```py\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> predict_answer = tokenizer.decode(predict_answer_tokens)\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MvpForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForCausalLM.from_pretrained(\"RUCAIBox/mvp\", add_cross_attention=False)\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 8, 50267]\n```"]