["```py\n>>> from transformers import MvpTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n>>> model_with_prompt = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp-summarization\")\n\n>>> inputs = tokenizer(\n...     \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Why You Shouldn't Quit Your Job\"]\n\n>>> generated_ids = model_with_prompt.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Don't do it if these are your reasons\"]\n```", "```py\n>>> from transformers import MvpTokenizerFast, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizerFast.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n>>> model_with_mtl = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n\n>>> inputs = tokenizer(\n...     \"Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Stan Lee created the character of Iron Man, a fictional superhero appearing in American comic']\n\n>>> generated_ids = model_with_mtl.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Iron Man is a fictional superhero appearing in American comic books published by Marvel Comics.']\n```", "```py\n>>> from transformers import MvpForConditionalGeneration\n\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\", use_prompt=True)\n>>> # the number of trainable parameters (full tuning)\n>>> sum(p.numel() for p in model.parameters() if p.requires_grad)\n468116832\n\n>>> # lightweight tuning with randomly initialized prompts\n>>> model.set_lightweight_tuning()\n>>> # the number of trainable parameters (lightweight tuning)\n>>> sum(p.numel() for p in model.parameters() if p.requires_grad)\n61823328\n\n>>> # lightweight tuning with task-specific prompts\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n>>> model.set_lightweight_tuning()\n>>> # original lightweight Prefix-tuning\n>>> model = MvpForConditionalGeneration.from_pretrained(\"facebook/bart-large\", use_prompt=True)\n>>> model.set_lightweight_tuning()\n```", "```py\n>>> from transformers import MvpConfig, MvpModel\n\n>>> # Initializing a MVP RUCAIBox/mvp style configuration\n>>> configuration = MvpConfig()\n\n>>> # Initializing a model (with random weights) from the RUCAIBox/mvp style configuration\n>>> model = MvpModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import MvpTokenizer\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n>>> from transformers import MvpTokenizerFast\n\n>>> tokenizer = MvpTokenizerFast.from_pretrained(\"RUCAIBox/mvp\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n>>> from transformers import AutoTokenizer, MvpModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpModel.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\n...     \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n...     return_tensors=\"pt\",\n... )\n>>> labels = tokenizer(\"Bad Reasons To Quit Your Job\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> loss = model(**inputs, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> with torch.no_grad():\n...     generated_ids = model.generate(**inputs)\n\n>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MvpForSequenceClassification\n\n>>> num_labels = 2  # for example, this is a binary classification task\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForSequenceClassification.from_pretrained(\"RUCAIBox/mvp\", num_labels=num_labels)\n\n>>> inputs = tokenizer(\"Classify: Hello, my dog is cute\", return_tensors=\"pt\")\n>>> labels = torch.tensor(1)  # the real label for inputs\n\n>>> loss = model(**inputs, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax()\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MvpForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForQuestionAnswering.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\n...     \"Answer the following question: Who was Jim Henson? [SEP] Jim Henson was a nice puppet\",\n...     return_tensors=\"pt\",\n... )\n>>> target_start_index = torch.tensor([18])\n>>> target_end_index = torch.tensor([19])\n\n>>> loss = model(**inputs, start_positions=target_start_index, end_positions=target_end_index).loss\n>>> loss.backward()\n```", "```py\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> predict_answer = tokenizer.decode(predict_answer_tokens)\n```", "```py\n>>> from transformers import AutoTokenizer, MvpForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForCausalLM.from_pretrained(\"RUCAIBox/mvp\", add_cross_attention=False)\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 8, 50267]\n```"]