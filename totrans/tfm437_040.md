# å›¾åƒå­—å¹•

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning)

å›¾åƒå­—å¹•æ˜¯é¢„æµ‹ç»™å®šå›¾åƒçš„å­—å¹•çš„ä»»åŠ¡ã€‚å®ƒçš„å¸¸è§å®é™…åº”ç”¨åŒ…æ‹¬å¸®åŠ©è§†è§‰éšœç¢äººå£«ï¼Œå¸®åŠ©ä»–ä»¬åœ¨ä¸åŒæƒ…å†µä¸‹å¯¼èˆªã€‚å› æ­¤ï¼Œå›¾åƒå­—å¹•é€šè¿‡å‘äººä»¬æè¿°å›¾åƒæ¥å¸®åŠ©æé«˜äººä»¬å¯¹å†…å®¹çš„å¯è®¿é—®æ€§ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

+   å¾®è°ƒå›¾åƒå­—å¹•æ¨¡å‹ã€‚

+   ç”¨äºæ¨ç†çš„å¾®è°ƒæ¨¡å‹ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```py
pip install transformers datasets evaluate -q
pip install jiwer -q
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
from huggingface_hub import notebook_login

notebook_login()
```

## åŠ è½½Pokemon BLIPå­—å¹•æ•°æ®é›†

ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“åŠ è½½ä¸€ä¸ªç”±{å›¾åƒ-æ ‡é¢˜}å¯¹ç»„æˆçš„æ•°æ®é›†ã€‚è¦åœ¨PyTorchä¸­åˆ›å»ºè‡ªå·±çš„å›¾åƒå­—å¹•æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥å‚è€ƒ[æ­¤ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb)ã€‚

```py
from datasets import load_dataset

ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds
```

```py
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
```

æ•°æ®é›†æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œ`å›¾åƒ`å’Œ`æ–‡æœ¬`ã€‚

è®¸å¤šå›¾åƒå­—å¹•æ•°æ®é›†åŒ…å«æ¯ä¸ªå›¾åƒçš„å¤šä¸ªå­—å¹•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªå¸¸è§çš„ç­–ç•¥æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨å¯ç”¨çš„å­—å¹•ä¸­éšæœºæŠ½å–ä¸€ä¸ªå­—å¹•ã€‚

ä½¿ç”¨[~datasets.Dataset.train_test_split]æ–¹æ³•å°†æ•°æ®é›†çš„è®­ç»ƒé›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

è®©æˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬ã€‚

```py
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np

def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```

![æ ·æœ¬è®­ç»ƒå›¾åƒ](../Images/465bc374aa742ae3c24da1893b1ca2b5.png)

## é¢„å¤„ç†æ•°æ®é›†

ç”±äºæ•°æ®é›†å…·æœ‰ä¸¤ç§æ¨¡æ€ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰ï¼Œé¢„å¤„ç†æµæ°´çº¿å°†é¢„å¤„ç†å›¾åƒå’Œæ ‡é¢˜ã€‚

ä¸ºæ­¤ï¼ŒåŠ è½½ä¸æ‚¨å³å°†å¾®è°ƒçš„æ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ç±»ã€‚

```py
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

å¤„ç†å™¨å°†åœ¨å†…éƒ¨é¢„å¤„ç†å›¾åƒï¼ˆåŒ…æ‹¬è°ƒæ•´å¤§å°å’Œåƒç´ ç¼©æ”¾ï¼‰å¹¶å¯¹æ ‡é¢˜è¿›è¡Œæ ‡è®°ã€‚

```py
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs

train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

æœ‰äº†å‡†å¤‡å¥½çš„æ•°æ®é›†ï¼Œæ‚¨ç°åœ¨å¯ä»¥ä¸ºå¾®è°ƒè®¾ç½®æ¨¡å‹ã€‚

## åŠ è½½åŸºç¡€æ¨¡å‹

å°†[â€œmicrosoft/git-baseâ€](https://huggingface.co/microsoft/git-base)åŠ è½½åˆ°[`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM)å¯¹è±¡ä¸­ã€‚

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

## è¯„ä¼°

å›¾åƒå­—å¹•æ¨¡å‹é€šå¸¸ä½¿ç”¨[Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge)æˆ–[Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer)è¿›è¡Œè¯„ä¼°ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨Word Error Rate (WER)ã€‚

æˆ‘ä»¬ä½¿ç”¨ğŸ¤—è¯„ä¼°åº“æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æœ‰å…³WERçš„æ½œåœ¨é™åˆ¶å’Œå…¶ä»–æ³¨æ„äº‹é¡¹ï¼Œè¯·å‚è€ƒ[æ­¤æŒ‡å—](https://huggingface.co/spaces/evaluate-metric/wer)ã€‚

```py
from evaluate import load
import torch

wer = load("wer")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

## è®­ç»ƒï¼

ç°åœ¨ï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹å¾®è°ƒæ¨¡å‹äº†ã€‚æ‚¨å°†ä½¿ç”¨ğŸ¤—[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ¥è¿›è¡Œæ­¤æ“ä½œã€‚

é¦–å…ˆï¼Œä½¿ç”¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å®šä¹‰è®­ç»ƒå‚æ•°ã€‚

```py
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

ç„¶åå°†å®ƒä»¬ä¸æ•°æ®é›†å’Œæ¨¡å‹ä¸€èµ·ä¼ é€’ç»™ğŸ¤— Trainerã€‚

```py
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

è¦å¼€å§‹è®­ç»ƒï¼Œåªéœ€åœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¯¹è±¡ä¸Šè°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)ã€‚

```py
trainer.train()
```

æ‚¨åº”è¯¥çœ‹åˆ°éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè®­ç»ƒæŸå¤±å¹³ç¨³ä¸‹é™ã€‚

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
trainer.push_to_hub()
```

## æ¨ç†

ä»`test_ds`ä¸­å–ä¸€ä¸ªæ ·æœ¬å›¾åƒæ¥æµ‹è¯•æ¨¡å‹ã€‚

```py
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

![æµ‹è¯•å›¾ç‰‡](../Images/8106832a19bcb32bb4f42e5d637f0640.png)ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚

```py
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

è°ƒç”¨`generate`å¹¶è§£ç é¢„æµ‹ã€‚

```py
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```

```py
a drawing of a pink and blue pokemon
```

çœ‹èµ·æ¥å¾®è°ƒçš„æ¨¡å‹ç”Ÿæˆäº†ä¸€ä¸ªç›¸å½“ä¸é”™çš„å­—å¹•ï¼
