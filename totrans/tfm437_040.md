# 图像字幕

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_captioning)

图像字幕是预测给定图像的字幕的任务。它的常见实际应用包括帮助视觉障碍人士，帮助他们在不同情况下导航。因此，图像字幕通过向人们描述图像来帮助提高人们对内容的可访问性。

本指南将向您展示如何：

+   微调图像字幕模型。

+   用于推理的微调模型。

在开始之前，请确保您已安装所有必要的库：

```py
pip install transformers datasets evaluate -q
pip install jiwer -q
```

我们鼓励您登录您的Hugging Face账户，这样您就可以上传并与社区分享您的模型。在提示时，输入您的令牌以登录：

```py
from huggingface_hub import notebook_login

notebook_login()
```

## 加载Pokemon BLIP字幕数据集

使用🤗数据集库加载一个由{图像-标题}对组成的数据集。要在PyTorch中创建自己的图像字幕数据集，您可以参考[此笔记本](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb)。

```py
from datasets import load_dataset

ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds
```

```py
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
```

数据集有两个特征，`图像`和`文本`。

许多图像字幕数据集包含每个图像的多个字幕。在这种情况下，一个常见的策略是在训练过程中在可用的字幕中随机抽取一个字幕。

使用[~datasets.Dataset.train_test_split]方法将数据集的训练集拆分为训练集和测试集：

```py
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

让我们从训练集中可视化几个样本。

```py
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np

def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```

![样本训练图像](../Images/465bc374aa742ae3c24da1893b1ca2b5.png)

## 预处理数据集

由于数据集具有两种模态（图像和文本），预处理流水线将预处理图像和标题。

为此，加载与您即将微调的模型相关联的处理器类。

```py
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

处理器将在内部预处理图像（包括调整大小和像素缩放）并对标题进行标记。

```py
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs

train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

有了准备好的数据集，您现在可以为微调设置模型。

## 加载基础模型

将[“microsoft/git-base”](https://huggingface.co/microsoft/git-base)加载到[`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM)对象中。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

## 评估

图像字幕模型通常使用[Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge)或[Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer)进行评估。在本指南中，您将使用Word Error Rate (WER)。

我们使用🤗评估库来做到这一点。有关WER的潜在限制和其他注意事项，请参考[此指南](https://huggingface.co/spaces/evaluate-metric/wer)。

```py
from evaluate import load
import torch

wer = load("wer")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

## 训练！

现在，您已经准备好开始微调模型了。您将使用🤗[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)来进行此操作。

首先，使用[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)定义训练参数。

```py
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

然后将它们与数据集和模型一起传递给🤗 Trainer。

```py
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

要开始训练，只需在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对象上调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)。

```py
trainer.train()
```

您应该看到随着训练的进行，训练损失平稳下降。

一旦训练完成，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，以便每个人都可以使用您的模型：

```py
trainer.push_to_hub()
```

## 推理

从`test_ds`中取一个样本图像来测试模型。

```py
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

![测试图片](../Images/8106832a19bcb32bb4f42e5d637f0640.png)为模型准备图像。

```py
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

调用`generate`并解码预测。

```py
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```

```py
a drawing of a pink and blue pokemon
```

看起来微调的模型生成了一个相当不错的字幕！
