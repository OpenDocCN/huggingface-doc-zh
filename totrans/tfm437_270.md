# GLPN

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/glpn](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/glpn)

è¿™æ˜¯ä¸€ä¸ªæœ€è¿‘æ¨å‡ºçš„æ¨¡å‹ï¼Œå› æ­¤ API å°šæœªç»è¿‡å¹¿æ³›æµ‹è¯•ã€‚æœªæ¥å¯èƒ½ä¼šæœ‰ä¸€äº›é”™è¯¯æˆ–è½»å¾®çš„ç ´åæ€§æ›´æ”¹éœ€è¦ä¿®å¤ã€‚å¦‚æœæ‚¨å‘ç°å¼‚å¸¸ï¼Œè¯·æäº¤ [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title)ã€‚

## æ¦‚è¿°

GLPN æ¨¡å‹ç”± Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim åœ¨ [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) ä¸­æå‡ºã€‚GLPN å°† [SegFormer](segformer) çš„åˆ†å±‚æ··åˆå˜å‹å™¨ä¸è½»é‡çº§è§£ç å™¨ç»“åˆï¼Œç”¨äºå•çœ¼æ·±åº¦ä¼°è®¡ã€‚æ‰€æå‡ºçš„è§£ç å™¨æ˜¾ç¤ºå‡ºæ¯”å…ˆå‰æå‡ºçš„è§£ç å™¨æ›´å¥½çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—å¤æ‚åº¦æ˜æ˜¾è¾ƒä½ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*ä»å•ä¸ªå›¾åƒè¿›è¡Œæ·±åº¦ä¼°è®¡æ˜¯ä¸€ä¸ªé‡è¦çš„ä»»åŠ¡ï¼Œå¯ä»¥åº”ç”¨äºè®¡ç®—æœºè§†è§‰çš„å„ä¸ªé¢†åŸŸï¼Œå¹¶éšç€å·ç§¯ç¥ç»ç½‘ç»œçš„å‘å±•è€Œè¿…é€Ÿå¢é•¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»“æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œç”¨äºå•çœ¼æ·±åº¦ä¼°è®¡ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç½‘ç»œçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬éƒ¨ç½²äº†ä¸€ä¸ªåˆ†å±‚å˜å‹å™¨ç¼–ç å™¨æ¥æ•è·å’Œä¼ è¾¾å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§ä½†åŠŸèƒ½å¼ºå¤§çš„è§£ç å™¨ï¼Œä»¥ç”Ÿæˆä¼°è®¡çš„æ·±åº¦å›¾ï¼ŒåŒæ—¶è€ƒè™‘å±€éƒ¨è¿æ¥æ€§ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„é€‰æ‹©æ€§ç‰¹å¾èåˆæ¨¡å—åœ¨å¤šå°ºåº¦å±€éƒ¨ç‰¹å¾å’Œå…¨å±€è§£ç æµä¹‹é—´æ„å»ºè¿æ¥è·¯å¾„ï¼Œç½‘ç»œå¯ä»¥æ•´åˆä¸¤ç§è¡¨ç¤ºå¹¶æ¢å¤ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„è§£ç å™¨æ˜¾ç¤ºå‡ºæ¯”å…ˆå‰æå‡ºçš„è§£ç å™¨æ›´å¥½çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—å¤æ‚åº¦æ˜æ˜¾è¾ƒä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ·±åº¦ä¼°è®¡ä¸­çš„ä¸€ä¸ªé‡è¦è§‚å¯Ÿç»“æœæ¥æ”¹è¿›æ·±åº¦ç‰¹å®šçš„å¢å¼ºæ–¹æ³•ï¼Œä»¥å¢å¼ºæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç½‘ç»œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ·±åº¦æ•°æ®é›† NYU Depth V2 ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·²è¿›è¡Œäº†å¤§é‡å®éªŒæ¥éªŒè¯å’Œå±•ç¤ºæ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ¯”å…¶ä»–æ¯”è¾ƒæ¨¡å‹æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚*

![å›¾ç¤º](../Images/ff344ea07d21e963e3c93f56069bfcb3.png) æ–¹æ³•æ¦‚è¿°ã€‚æ‘˜è‡ª [åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2201.07436)ã€‚

æ­¤æ¨¡å‹ç”± [nielsr](https://huggingface.co/nielsr) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨ [æ­¤å¤„](https://github.com/vinvino02/GLPDepth) æ‰¾åˆ°ã€‚

## èµ„æº

ä¸€ç³»åˆ—å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰èµ„æºï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ GLPNã€‚

+   [GLPNForDepthEstimation](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNForDepthEstimation) çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨ [æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GLPN) æ‰¾åˆ°ã€‚

+   [å•çœ¼æ·±åº¦ä¼°è®¡ä»»åŠ¡æŒ‡å—](../tasks/monocular_depth_estimation)

## GLPNConfig

### `class transformers.GLPNConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/configuration_glpn.py#L29)

```py
( num_channels = 3 num_encoder_blocks = 4 depths = [2, 2, 2, 2] sr_ratios = [8, 4, 2, 1] hidden_sizes = [32, 64, 160, 256] patch_sizes = [7, 3, 3, 3] strides = [4, 2, 2, 2] num_attention_heads = [1, 2, 5, 8] mlp_ratios = [4, 4, 4, 4] hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 drop_path_rate = 0.1 layer_norm_eps = 1e-06 decoder_hidden_size = 64 max_depth = 10 head_in_index = -1 **kwargs )
```

å‚æ•°

+   `num_channels` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3) â€” è¾“å…¥é€šé“æ•°ã€‚

+   `num_encoder_blocks` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 4) â€” ç¼–ç å™¨å—çš„æ•°é‡ï¼ˆå³ Mix Transformer ç¼–ç å™¨ä¸­çš„é˜¶æ®µæ•°ï¼‰ã€‚

+   `depths` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[2, 2, 2, 2]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¸­çš„å±‚æ•°ã€‚

+   `sr_ratios` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[8, 4, 2, 1]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¸­çš„åºåˆ—ç¼©å‡æ¯”ç‡ã€‚

+   `hidden_sizes` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[32, 64, 160, 256]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„ç»´åº¦ã€‚

+   `patch_sizes` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[7, 3, 3, 3]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¹‹å‰çš„è¡¥ä¸å¤§å°ã€‚

+   `strides` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[4, 2, 2, 2]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¹‹å‰çš„æ­¥å¹…ã€‚

+   `num_attention_heads` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[1, 2, 5, 8]`) â€” æ¯ä¸ª Transformer ç¼–ç å™¨å—ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `mlp_ratios` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[4, 4, 4, 4]`) â€” ç¼–ç å™¨å—ä¸­ Mix FFN éšè—å±‚å¤§å°ä¸è¾“å…¥å±‚å¤§å°çš„æ¯”ç‡ã€‚

+   `hidden_act` (`str` æˆ– `function`, *å¯é€‰*, é»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `drop_path_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” ç”¨äºéšæœºæ·±åº¦çš„ä¸¢å¤±æ¦‚ç‡ï¼Œç”¨äº Transformer ç¼–ç å™¨å—ä¸­ã€‚

+   `layer_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-06) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `decoder_hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 64) â€” è§£ç å™¨çš„ç»´åº¦ã€‚

+   `max_depth` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 10) â€” è§£ç å™¨çš„æœ€å¤§æ·±åº¦ã€‚

+   `head_in_index` (`int`, *å¯é€‰*, é»˜è®¤ä¸º -1) â€” ç”¨äºåœ¨å¤´éƒ¨ä¸­ä½¿ç”¨çš„ç‰¹å¾çš„ç´¢å¼•ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ [GLPNModel](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNModel) é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª GLPN æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº GLPN [vinvino02/glpn-kitti](https://huggingface.co/vinvino02/glpn-kitti) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) å¹¶å¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import GLPNModel, GLPNConfig

>>> # Initializing a GLPN vinvino02/glpn-kitti style configuration
>>> configuration = GLPNConfig()

>>> # Initializing a model from the vinvino02/glpn-kitti style configuration
>>> model = GLPNModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## GLPNFeatureExtractor

### `class transformers.GLPNFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/feature_extraction_glpn.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## GLPNImageProcessor

### `class transformers.GLPNImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/image_processing_glpn.py#L40)

```py
( do_resize: bool = True size_divisor: int = 32 resample = <Resampling.BILINEAR: 2> do_rescale: bool = True **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸ï¼Œå°†å®ƒä»¬å‘ä¸‹èˆå…¥åˆ°æœ€æ¥è¿‘ `size_divisor` çš„å€æ•°ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `do_resize` è¦†ç›–ã€‚

+   `size_divisor` (`int`, *å¯é€‰*, é»˜è®¤ä¸º32) â€” å½“ `do_resize` ä¸º `True` æ—¶ï¼Œå›¾åƒå°†è¢«è°ƒæ•´å¤§å°ï¼Œä½¿å…¶é«˜åº¦å’Œå®½åº¦å‘ä¸‹èˆå…¥åˆ°æœ€æ¥è¿‘ `size_divisor` çš„å€æ•°ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `size_divisor` è¦†ç›–ã€‚

+   `resample` (`PIL.Image` é‡é‡‡æ ·æ»¤æ³¢å™¨, *å¯é€‰*, é»˜è®¤ä¸º `Resampling.BILINEAR`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `resample` è¦†ç›–ã€‚

+   `do_rescale` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åº”ç”¨ç¼©æ”¾å› å­ï¼ˆä½¿åƒç´ å€¼åœ¨ 0 å’Œ 1 ä¹‹é—´æµ®åŠ¨ï¼‰ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `do_rescale` è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ª GLPN å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/image_processing_glpn.py#L124)

```py
( images: Union do_resize: Optional = None size_divisor: Optional = None resample = None do_rescale: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images`ï¼ˆ`PIL.Image.Image` æˆ– `TensorType` æˆ– `List[np.ndarray]` æˆ– `List[TensorType]`ï¼‰â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä» 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_normalize=False`ã€‚

+   `do_resize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_resize`ï¼‰â€” æ˜¯å¦è°ƒæ•´è¾“å…¥å¤§å°ï¼Œä½¿å¾—ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ç»´åº¦æ˜¯ `size_divisor` çš„å€æ•°ã€‚

+   `size_divisor`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.size_divisor`ï¼‰â€” å½“ `do_resize` ä¸º `True` æ—¶ï¼Œå›¾åƒè¢«è°ƒæ•´å¤§å°ï¼Œä½¿å…¶é«˜åº¦å’Œå®½åº¦å‘ä¸‹èˆå…¥åˆ°æœ€æ¥è¿‘çš„ `size_divisor` çš„å€æ•°ã€‚

+   `resample`ï¼ˆ`PIL.Image` é‡é‡‡æ ·æ»¤æ³¢å™¨ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.resample`ï¼‰â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™ä½¿ç”¨çš„ `PIL.Image` é‡é‡‡æ ·æ»¤æ³¢å™¨ï¼Œä¾‹å¦‚ `PILImageResampling.BILINEAR`ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_rescale`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_rescale`ï¼‰â€” æ˜¯å¦åº”ç”¨ç¼©æ”¾å› å­ï¼ˆä½¿åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼‰ã€‚

+   `return_tensors`ï¼ˆ`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*ï¼‰â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `None`: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`: è¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹å¤„ç†ã€‚

+   `data_format`ï¼ˆ`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `ChannelDimension.FIRST`ï¼‰â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `ChannelDimension.FIRST`: å›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

+   `input_data_format`ï¼ˆ`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒæ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   `"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

é¢„å¤„ç†ç»™å®šçš„å›¾åƒã€‚

## GLPNModel

### `class transformers.GLPNModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/modeling_glpn.py#L479)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[GLPNConfig](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ GLPN ç¼–ç å™¨ï¼ˆMix-Transformerï¼‰è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/modeling_glpn.py#L503)

```py
( pixel_values: FloatTensor output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)` çš„ `torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [GLPNImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–`tuple(torch.FloatTensor)`

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[GLPNConfig](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ª+æ¯ä¸ªå±‚çš„è¾“å‡ºä¸€ä¸ªï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[GLPNModel](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, GLPNModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("vinvino02/glpn-kitti")
>>> model = GLPNModel.from_pretrained("vinvino02/glpn-kitti")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 512, 15, 20]
```

## GLPNForDepthEstimation

### `class transformers.GLPNForDepthEstimation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/modeling_glpn.py#L680)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[GLPNConfig](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

GLPNæ¨¡å‹å˜æ¢å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è½»é‡çº§æ·±åº¦ä¼°è®¡å¤´ï¼Œä¾‹å¦‚é€‚ç”¨äºKITTIã€NYUv2ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

å‰è¿›

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/glpn/modeling_glpn.py#L695)

```py
( pixel_values: FloatTensor labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.DepthEstimatorOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰- åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[GLPNImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `output_attentions`ï¼ˆ*å¯é€‰*ï¼Œå¸ƒå°”å€¼ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ*å¯é€‰*ï¼Œå¸ƒå°”å€¼ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ*å¯é€‰*ï¼Œå¸ƒå°”å€¼ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®æ·±åº¦ä¼°è®¡å›¾ã€‚

è¿”å›

[transformers.modeling_outputs.DepthEstimatorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.DepthEstimatorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[GLPNConfig](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰- åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `predicted_depth`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.FloatTensor`ï¼‰- æ¯ä¸ªåƒç´ çš„é¢„æµ‹æ·±åº¦ã€‚

+   `hidden_states`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[GLPNForDepthEstimation](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNForDepthEstimation)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, GLPNForDepthEstimation
>>> import torch
>>> import numpy as np
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("vinvino02/glpn-kitti")
>>> model = GLPNForDepthEstimation.from_pretrained("vinvino02/glpn-kitti")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     predicted_depth = outputs.predicted_depth

>>> # interpolate to original size
>>> prediction = torch.nn.functional.interpolate(
...     predicted_depth.unsqueeze(1),
...     size=image.size[::-1],
...     mode="bicubic",
...     align_corners=False,
... )

>>> # visualize the prediction
>>> output = prediction.squeeze().cpu().numpy()
>>> formatted = (output * 255 / np.max(output)).astype("uint8")
>>> depth = Image.fromarray(formatted)
```
