# ç›®æ ‡æ£€æµ‹

> åŽŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/object_detection](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/object_detection)

ç›®æ ‡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œç”¨äºŽæ£€æµ‹å›¾åƒä¸­çš„å®žä¾‹ï¼ˆå¦‚äººç±»ã€å»ºç­‘ç‰©æˆ–æ±½è½¦ï¼‰ã€‚ç›®æ ‡æ£€æµ‹æ¨¡åž‹æŽ¥æ”¶å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ£€æµ‹åˆ°çš„å¯¹è±¡çš„è¾¹ç•Œæ¡†çš„åæ ‡å’Œç›¸å…³æ ‡ç­¾ã€‚ä¸€å¹…å›¾åƒå¯ä»¥åŒ…å«å¤šä¸ªå¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡éƒ½æœ‰è‡ªå·±çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œå®ƒå¯ä»¥æœ‰ä¸€è¾†æ±½è½¦å’Œä¸€åº§å»ºç­‘ç‰©ï¼‰ï¼Œæ¯ä¸ªå¯¹è±¡å¯ä»¥å‡ºçŽ°åœ¨å›¾åƒçš„ä¸åŒéƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒå¯ä»¥æœ‰å‡ è¾†æ±½è½¦ï¼‰ã€‚è¿™ä¸ªä»»åŠ¡é€šå¸¸ç”¨äºŽè‡ªåŠ¨é©¾é©¶ï¼Œç”¨äºŽæ£€æµ‹è¡Œäººã€é“è·¯æ ‡å¿—å’Œäº¤é€šç¯ç­‰ã€‚å…¶ä»–åº”ç”¨åŒ…æ‹¬åœ¨å›¾åƒä¸­è®¡æ•°å¯¹è±¡ã€å›¾åƒæœç´¢ç­‰ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š

1.  å¯¹[DETR](https://huggingface.co/docs/transformers/model_doc/detr)è¿›è¡Œå¾®è°ƒï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å·ç§¯ä¸»å¹²ä¸Žç¼–ç å™¨-è§£ç å™¨Transformerç»“åˆçš„æ¨¡åž‹ï¼Œåœ¨[CPPE-5](https://huggingface.co/datasets/cppe-5)æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

1.  ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­ã€‚

æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡åž‹æž¶æž„æ”¯æŒï¼š

[æ¡ä»¶DETR](../model_doc/conditional_detr), [å¯å˜DETR](../model_doc/deformable_detr), [DETA](../model_doc/deta), [DETR](../model_doc/detr), [è¡¨æ ¼Transformer](../model_doc/table-transformer), [YOLOS](../model_doc/yolos)

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```py
pip install -q datasets transformers evaluate timm albumentations
```

æ‚¨å°†ä½¿ç”¨ðŸ¤—æ•°æ®é›†ä»ŽHugging Face HubåŠ è½½æ•°æ®é›†ï¼ŒðŸ¤—è½¬æ¢å™¨æ¥è®­ç»ƒæ‚¨çš„æ¨¡åž‹ï¼Œå¹¶ä½¿ç”¨`albumentations`æ¥å¢žå¼ºæ•°æ®ã€‚ç›®å‰éœ€è¦ä½¿ç”¨`timm`æ¥åŠ è½½DETRæ¨¡åž‹çš„å·ç§¯ä¸»å¹²ã€‚

æˆ‘ä»¬é¼“åŠ±æ‚¨ä¸Žç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡åž‹ã€‚ç™»å½•åˆ°æ‚¨çš„Hugging Faceå¸æˆ·å¹¶å°†å…¶ä¸Šä¼ åˆ°Hubã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½CPPE-5æ•°æ®é›†

[CPPE-5æ•°æ®é›†](https://huggingface.co/datasets/cppe-5)åŒ…å«å¸¦æœ‰æ³¨é‡Šçš„å›¾åƒï¼Œç”¨äºŽè¯†åˆ«COVID-19å¤§æµè¡ŒèƒŒæ™¯ä¸‹çš„åŒ»ç–—ä¸ªäººé˜²æŠ¤è£…å¤‡ï¼ˆPPEï¼‰ã€‚

é¦–å…ˆåŠ è½½æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> cppe5 = load_dataset("cppe-5")
>>> cppe5
DatasetDict({
    train: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 29
    })
})
```

æ‚¨å°†çœ‹åˆ°è¿™ä¸ªæ•°æ®é›†å·²ç»å¸¦æœ‰ä¸€ä¸ªåŒ…å«1000å¼ å›¾åƒçš„è®­ç»ƒé›†å’Œä¸€ä¸ªåŒ…å«29å¼ å›¾åƒçš„æµ‹è¯•é›†ã€‚

ç†Ÿæ‚‰æ•°æ®ï¼ŒæŽ¢ç´¢ç¤ºä¾‹çš„å¤–è§‚ã€‚

```py
>>> cppe5["train"][0]
{'image_id': 15,
 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,
 'width': 943,
 'height': 663,
 'objects': {'id': [114, 115, 116, 117],
  'area': [3796, 1596, 152768, 81002],
  'bbox': [[302.0, 109.0, 73.0, 52.0],
   [810.0, 100.0, 57.0, 28.0],
   [160.0, 31.0, 248.0, 616.0],
   [741.0, 68.0, 202.0, 401.0]],
  'category': [4, 4, 0, 0]}}
```

æ•°æ®é›†ä¸­çš„ç¤ºä¾‹å…·æœ‰ä»¥ä¸‹å­—æ®µï¼š

+   `image_id`ï¼šç¤ºä¾‹å›¾åƒid

+   `image`ï¼šåŒ…å«å›¾åƒçš„`PIL.Image.Image`å¯¹è±¡

+   `width`ï¼šå›¾åƒçš„å®½åº¦

+   `height`ï¼šå›¾åƒçš„é«˜åº¦

+   `objects`ï¼šåŒ…å«å›¾åƒä¸­å¯¹è±¡çš„è¾¹ç•Œæ¡†å…ƒæ•°æ®çš„å­—å…¸ï¼š

    +   `id`ï¼šæ³¨é‡Šid

    +   `area`ï¼šè¾¹ç•Œæ¡†çš„é¢ç§¯

    +   `bbox`ï¼šå¯¹è±¡çš„è¾¹ç•Œæ¡†ï¼ˆä»¥[COCOæ ¼å¼](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco)ï¼‰

    +   `category`ï¼šå¯¹è±¡çš„ç±»åˆ«ï¼Œå¯èƒ½çš„å€¼åŒ…æ‹¬`é˜²æŠ¤æœï¼ˆ0ï¼‰`ã€`é¢ç½©ï¼ˆ1ï¼‰`ã€`æ‰‹å¥—ï¼ˆ2ï¼‰`ã€`æŠ¤ç›®é•œï¼ˆ3ï¼‰`å’Œ`å£ç½©ï¼ˆ4ï¼‰`

æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°`bbox`å­—æ®µéµå¾ªCOCOæ ¼å¼ï¼Œè¿™æ˜¯DETRæ¨¡åž‹æœŸæœ›çš„æ ¼å¼ã€‚ç„¶è€Œï¼Œ`objects`å†…éƒ¨å­—æ®µçš„åˆ†ç»„ä¸ŽDETRæ‰€éœ€çš„æ³¨é‡Šæ ¼å¼ä¸åŒã€‚åœ¨ä½¿ç”¨æ­¤æ•°æ®è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œæ‚¨éœ€è¦åº”ç”¨ä¸€äº›é¢„å¤„ç†è½¬æ¢ã€‚

ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ï¼Œå¯è§†åŒ–æ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ã€‚

```py
>>> import numpy as np
>>> import os
>>> from PIL import Image, ImageDraw

>>> image = cppe5["train"][0]["image"]
>>> annotations = cppe5["train"][0]["objects"]
>>> draw = ImageDraw.Draw(image)

>>> categories = cppe5["train"].features["objects"].feature["category"].names

>>> id2label = {index: x for index, x in enumerate(categories, start=0)}
>>> label2id = {v: k for k, v in id2label.items()}

>>> for i in range(len(annotations["id"])):
...     box = annotations["bbox"][i]
...     class_idx = annotations["category"][i]
...     x, y, w, h = tuple(box)
...     # Check if coordinates are normalized or not
...     if max(box) > 1.0:
...         # Coordinates are un-normalized, no need to re-scale them
...         x1, y1 = int(x), int(y)
...         x2, y2 = int(x + w), int(y + h)
...     else:
...         # Coordinates are normalized, re-scale them
...         x1 = int(x * width)
...         y1 = int(y * height)
...         x2 = int((x + w) * width)
...         y2 = int((y + h) * height)
...     draw.rectangle((x, y, x + w, y + h), outline="red", width=1)
...     draw.text((x, y), id2label[class_idx], fill="white")

>>> image
```

![CPPE-5å›¾åƒç¤ºä¾‹](../Images/dc3ce9ec7d52953b62801d6b8b583148.png)

è¦å¯è§†åŒ–å¸¦æœ‰å…³è”æ ‡ç­¾çš„è¾¹ç•Œæ¡†ï¼Œæ‚¨å¯ä»¥ä»Žæ•°æ®é›†çš„å…ƒæ•°æ®ä¸­èŽ·å–æ ‡ç­¾ï¼Œç‰¹åˆ«æ˜¯`category`å­—æ®µã€‚æ‚¨è¿˜éœ€è¦åˆ›å»ºæ˜ å°„æ ‡ç­¾idåˆ°æ ‡ç­¾ç±»åˆ«ï¼ˆ`id2label`ï¼‰ä»¥åŠåå‘æ˜ å°„ï¼ˆ`label2id`ï¼‰çš„å­—å…¸ã€‚åœ¨è®¾ç½®æ¨¡åž‹æ—¶ï¼Œæ‚¨å¯ä»¥ç¨åŽä½¿ç”¨å®ƒä»¬ã€‚åŒ…æ‹¬è¿™äº›æ˜ å°„å°†ä½¿æ‚¨çš„æ¨¡åž‹åœ¨Hugging Face Hubä¸Šå…±äº«æ—¶å¯ä»¥è¢«å…¶ä»–äººé‡å¤ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œä¸Šè¿°ä»£ç ä¸­ç»˜åˆ¶è¾¹ç•Œæ¡†çš„éƒ¨åˆ†å‡å®šå®ƒæ˜¯ä»¥`XYWH`ï¼ˆxï¼Œyåæ ‡å’Œæ¡†çš„å®½åº¦å’Œé«˜åº¦ï¼‰æ ¼å¼ã€‚å¯¹äºŽå…¶ä»–æ ¼å¼å¦‚`(x1ï¼Œy1ï¼Œx2ï¼Œy2)`å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚

ä½œä¸ºç†Ÿæ‚‰æ•°æ®çš„æœ€åŽä¸€æ­¥ï¼ŒæŽ¢ç´¢å¯èƒ½å­˜åœ¨çš„é—®é¢˜ã€‚ç›®æ ‡æ£€æµ‹æ•°æ®é›†çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯è¾¹ç•Œæ¡†â€œæ‹‰ä¼¸â€åˆ°å›¾åƒè¾¹ç¼˜ä¹‹å¤–ã€‚è¿™ç§â€œå¤±æŽ§â€çš„è¾¹ç•Œæ¡†å¯èƒ½ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å‘é”™è¯¯ï¼Œåº”åœ¨æ­¤é˜¶æ®µåŠ ä»¥è§£å†³ã€‚åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­æœ‰ä¸€äº›ç¤ºä¾‹å­˜åœ¨è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†ç®€åŒ–æœ¬æŒ‡å—ä¸­çš„æ“ä½œï¼Œæˆ‘ä»¬å°†è¿™äº›å›¾åƒä»Žæ•°æ®ä¸­åˆ é™¤ã€‚

```py
>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]
>>> keep = [i for i in range(len(cppe5["train"])) if i not in remove_idx]
>>> cppe5["train"] = cppe5["train"].select(keep)
```

## é¢„å¤„ç†æ•°æ®

è¦å¾®è°ƒæ¨¡åž‹ï¼Œæ‚¨å¿…é¡»é¢„å¤„ç†æ‚¨è®¡åˆ’ä½¿ç”¨çš„æ•°æ®ï¼Œä»¥ç²¾ç¡®åŒ¹é…é¢„è®­ç»ƒæ¨¡åž‹ä½¿ç”¨çš„æ–¹æ³•ã€‚[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è´Ÿè´£å¤„ç†å›¾åƒæ•°æ®ä»¥åˆ›å»º`pixel_values`ï¼Œ`pixel_mask`å’Œ`labels`ï¼Œä¾›DETRæ¨¡åž‹è®­ç»ƒã€‚å›¾åƒå¤„ç†å™¨å…·æœ‰ä¸€äº›å±žæ€§ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒï¼š

+   `image_mean = [0.485, 0.456, 0.406 ]`

+   `image_std = [0.229, 0.224, 0.225]`

è¿™äº›æ˜¯ç”¨äºŽåœ¨æ¨¡åž‹é¢„è®­ç»ƒæœŸé—´å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚åœ¨è¿›è¡ŒæŽ¨ç†æˆ–å¾®è°ƒé¢„è®­ç»ƒå›¾åƒæ¨¡åž‹æ—¶ï¼Œè¿™äº›å€¼è‡³å…³é‡è¦ã€‚

ä»Žè¦å¾®è°ƒçš„æ¨¡åž‹ç›¸åŒçš„æ£€æŸ¥ç‚¹å®žä¾‹åŒ–å›¾åƒå¤„ç†å™¨ã€‚

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "facebook/detr-resnet-50"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

åœ¨å°†å›¾åƒä¼ é€’ç»™`image_processor`ä¹‹å‰ï¼Œå¯¹æ•°æ®é›†åº”ç”¨ä¸¤ä¸ªé¢„å¤„ç†è½¬æ¢ï¼š

+   å¢žå¼ºå›¾åƒ

+   é‡æ–°æ ¼å¼åŒ–æ³¨é‡Šä»¥æ»¡è¶³DETRçš„æœŸæœ›

é¦–å…ˆï¼Œä¸ºäº†ç¡®ä¿æ¨¡åž‹ä¸ä¼šåœ¨è®­ç»ƒæ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ•°æ®å¢žå¼ºåº“è¿›è¡Œå›¾åƒå¢žå¼ºã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨[Albumentations](https://albumentations.ai/docs/)...æ­¤åº“ç¡®ä¿è½¬æ¢å½±å“å›¾åƒå¹¶ç›¸åº”æ›´æ–°è¾¹ç•Œæ¡†ã€‚ðŸ¤—æ•°æ®é›†åº“æ–‡æ¡£æœ‰ä¸€ä¸ªè¯¦ç»†çš„[å…³äºŽå¦‚ä½•ä¸ºç›®æ ‡æ£€æµ‹å¢žå¼ºå›¾åƒçš„æŒ‡å—](https://huggingface.co/docs/datasets/object_detection)ï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ã€‚åœ¨è¿™é‡Œåº”ç”¨ç›¸åŒçš„æ–¹æ³•ï¼Œå°†æ¯ä¸ªå›¾åƒè°ƒæ•´ä¸º(480, 480)ï¼Œæ°´å¹³ç¿»è½¬å¹¶å¢žåŠ äº®åº¦ï¼š

```py
>>> import albumentations
>>> import numpy as np
>>> import torch

>>> transform = albumentations.Compose(
...     [
...         albumentations.Resize(480, 480),
...         albumentations.HorizontalFlip(p=1.0),
...         albumentations.RandomBrightnessContrast(p=1.0),
...     ],
...     bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
... )
```

`image_processor`æœŸæœ›æ³¨é‡Šé‡‡ç”¨ä»¥ä¸‹æ ¼å¼ï¼š`{'image_id': int, 'annotations': List[Dict]}`ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸æ˜¯ä¸€ä¸ªCOCOå¯¹è±¡æ³¨é‡Šã€‚è®©æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥ä¸ºå•ä¸ªç¤ºä¾‹é‡æ–°æ ¼å¼åŒ–æ³¨é‡Šï¼š

```py
>>> def formatted_anns(image_id, category, area, bbox):
...     annotations = []
...     for i in range(0, len(category)):
...         new_ann = {
...             "image_id": image_id,
...             "category_id": category[i],
...             "isCrowd": 0,
...             "area": area[i],
...             "bbox": list(bbox[i]),
...         }
...         annotations.append(new_ann)

...     return annotations
```

çŽ°åœ¨ï¼Œæ‚¨å¯ä»¥å°†å›¾åƒå’Œæ³¨é‡Šè½¬æ¢ç»„åˆåœ¨ä¸€èµ·ï¼Œç”¨äºŽä¸€æ‰¹ç¤ºä¾‹ï¼š

```py
>>> # transforming a batch
>>> def transform_aug_ann(examples):
...     image_ids = examples["image_id"]
...     images, bboxes, area, categories = [], [], [], []
...     for image, objects in zip(examples["image"], examples["objects"]):
...         image = np.array(image.convert("RGB"))[:, :, ::-1]
...         out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

...         area.append(objects["area"])
...         images.append(out["image"])
...         bboxes.append(out["bboxes"])
...         categories.append(out["category"])

...     targets = [
...         {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
...     ]

...     return image_processor(images=images, annotations=targets, return_tensors="pt")
```

ä½¿ç”¨ðŸ¤—æ•°æ®é›†çš„[with_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.with_transform)æ–¹æ³•å°†æ­¤é¢„å¤„ç†å‡½æ•°åº”ç”¨äºŽæ•´ä¸ªæ•°æ®é›†ã€‚æ­¤æ–¹æ³•åœ¨åŠ è½½æ•°æ®é›†å…ƒç´ æ—¶åŠ¨æ€åº”ç”¨è½¬æ¢ã€‚

æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥æ£€æŸ¥æ•°æ®é›†ç»è¿‡è½¬æ¢åŽçš„ç¤ºä¾‹æ˜¯ä»€ä¹ˆæ ·å­ã€‚æ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªå¸¦æœ‰`pixel_values`çš„å¼ é‡ï¼Œä¸€ä¸ªå¸¦æœ‰`pixel_mask`çš„å¼ é‡å’Œ`labels`ã€‚

```py
>>> cppe5["train"] = cppe5["train"].with_transform(transform_aug_ann)
>>> cppe5["train"][15]
{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],
          ...,
          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],

         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],
          ...,
          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],

         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],
          ...,
          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}
```

æ‚¨å·²æˆåŠŸå¢žå¼ºäº†å•ä¸ªå›¾åƒå¹¶å‡†å¤‡å¥½å®ƒä»¬çš„æ³¨é‡Šã€‚ç„¶è€Œï¼Œé¢„å¤„ç†è¿˜æ²¡æœ‰å®Œæˆã€‚åœ¨æœ€åŽä¸€æ­¥ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„`collate_fn`æ¥å°†å›¾åƒæ‰¹é‡å¤„ç†åœ¨ä¸€èµ·ã€‚å°†å›¾åƒï¼ˆçŽ°åœ¨æ˜¯`pixel_values`ï¼‰å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç›¸åº”çš„`pixel_mask`æ¥æŒ‡ç¤ºå“ªäº›åƒç´ æ˜¯çœŸå®žçš„ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ˜¯å¡«å……çš„ï¼ˆ0ï¼‰ã€‚

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## è®­ç»ƒDETRæ¨¡åž‹

åœ¨å‰å‡ èŠ‚ä¸­ï¼Œæ‚¨å·²ç»å®Œæˆäº†å¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œï¼ŒçŽ°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„æ¨¡åž‹äº†ï¼å³ä½¿åœ¨è°ƒæ•´å¤§å°åŽï¼Œæ­¤æ•°æ®é›†ä¸­çš„å›¾åƒä»ç„¶ç›¸å½“å¤§ã€‚è¿™æ„å‘³ç€å¾®è°ƒæ­¤æ¨¡åž‹å°†éœ€è¦è‡³å°‘ä¸€ä¸ªGPUã€‚

è®­ç»ƒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1.  ä½¿ç”¨ä¸Žé¢„å¤„ç†ä¸­ç›¸åŒçš„æ£€æŸ¥ç‚¹åŠ è½½æ¨¡åž‹[AutoModelForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForObjectDetection)ã€‚

1.  åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚

1.  å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡åž‹ã€æ•°æ®é›†ã€å›¾åƒå¤„ç†å™¨å’Œæ•°æ®æ•´ç†å™¨ã€‚

1.  è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡åž‹ã€‚

åœ¨ä»Žç”¨äºŽé¢„å¤„ç†çš„ç›¸åŒæ£€æŸ¥ç‚¹åŠ è½½æ¨¡åž‹æ—¶ï¼Œè¯·è®°ä½ä¼ é€’æ‚¨ä»Žæ•°æ®é›†å…ƒæ•°æ®ä¸­åˆ›å»ºçš„`label2id`å’Œ`id2label`æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŒ‡å®š`ignore_mismatched_sizes=True`ä»¥ç”¨æ–°çš„æ›¿æ¢çŽ°æœ‰çš„åˆ†ç±»å¤´ã€‚

```py
>>> from transformers import AutoModelForObjectDetection

>>> model = AutoModelForObjectDetection.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
...     ignore_mismatched_sizes=True,
... )
```

åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­ä½¿ç”¨`output_dir`æŒ‡å®šä¿å­˜æ¨¡åž‹çš„ä½ç½®ï¼Œç„¶åŽæ ¹æ®éœ€è¦é…ç½®è¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™å°†åˆ é™¤å›¾åƒåˆ—ã€‚æ²¡æœ‰å›¾åƒåˆ—ï¼Œæ‚¨æ— æ³•åˆ›å»º`pixel_values`ã€‚å› æ­¤ï¼Œå°†`remove_unused_columns`è®¾ç½®ä¸º`False`ã€‚å¦‚æžœå¸Œæœ›é€šè¿‡å°†å…¶æŽ¨é€åˆ°Hubæ¥å…±äº«æ‚¨çš„æ¨¡åž‹ï¼Œè¯·å°†`push_to_hub`è®¾ç½®ä¸º`True`ï¼ˆæ‚¨å¿…é¡»ç™»å½•åˆ°Hugging Faceæ‰èƒ½ä¸Šä¼ æ‚¨çš„æ¨¡åž‹ï¼‰ã€‚

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(
...     output_dir="detr-resnet-50_finetuned_cppe5",
...     per_device_train_batch_size=8,
...     num_train_epochs=10,
...     fp16=True,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=1e-5,
...     weight_decay=1e-4,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

æœ€åŽï¼Œå°†æ‰€æœ‰å†…å®¹æ±‡æ€»ï¼Œå¹¶è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)ï¼š

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=collate_fn,
...     train_dataset=cppe5["train"],
...     tokenizer=image_processor,
... )

>>> trainer.train()
```

å¦‚æžœåœ¨`training_args`ä¸­å°†`push_to_hub`è®¾ç½®ä¸º`True`ï¼Œåˆ™è®­ç»ƒæ£€æŸ¥ç‚¹å°†è¢«æŽ¨é€åˆ°Hugging Face Hubã€‚åœ¨è®­ç»ƒå®ŒæˆåŽï¼Œé€šè¿‡è°ƒç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æœ€ç»ˆæ¨¡åž‹ä¹ŸæŽ¨é€åˆ°Hubã€‚

```py
>>> trainer.push_to_hub()
```

## è¯„ä¼°

ç›®æ ‡æ£€æµ‹æ¨¡åž‹é€šå¸¸ä½¿ç”¨ä¸€ç»„[COCOé£Žæ ¼æŒ‡æ ‡](https://cocodataset.org/#detection-eval)è¿›è¡Œè¯„ä¼°ã€‚æ‚¨å¯ä»¥ä½¿ç”¨çŽ°æœ‰çš„æŒ‡æ ‡å®žçŽ°ä¹‹ä¸€ï¼Œä½†åœ¨è¿™é‡Œï¼Œæ‚¨å°†ä½¿ç”¨æ¥è‡ª`torchvision`çš„æŒ‡æ ‡æ¥è¯„ä¼°æŽ¨é€åˆ°Hubçš„æœ€ç»ˆæ¨¡åž‹ã€‚

è¦ä½¿ç”¨`torchvision`è¯„ä¼°å™¨ï¼Œæ‚¨éœ€è¦å‡†å¤‡ä¸€ä¸ªçœŸå®žçš„COCOæ•°æ®é›†ã€‚æž„å»ºCOCOæ•°æ®é›†çš„APIè¦æ±‚æ•°æ®ä»¥ç‰¹å®šæ ¼å¼å­˜å‚¨ï¼Œå› æ­¤æ‚¨éœ€è¦é¦–å…ˆå°†å›¾åƒå’Œæ³¨é‡Šä¿å­˜åˆ°ç£ç›˜ä¸Šã€‚å°±åƒæ‚¨ä¸ºè®­ç»ƒå‡†å¤‡æ•°æ®æ—¶ä¸€æ ·ï¼Œæ¥è‡ª`cppe5["test"]`çš„æ³¨é‡Šéœ€è¦è¿›è¡Œæ ¼å¼åŒ–ã€‚ä½†æ˜¯ï¼Œå›¾åƒåº”ä¿æŒåŽŸæ ·ã€‚

è¯„ä¼°æ­¥éª¤éœ€è¦ä¸€äº›å·¥ä½œï¼Œä½†å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚é¦–å…ˆï¼Œå‡†å¤‡`cppe5["test"]`é›†ï¼šæ ¼å¼åŒ–æ³¨é‡Šå¹¶å°†æ•°æ®ä¿å­˜åˆ°ç£ç›˜ä¸Šã€‚

```py
>>> import json

>>> # format annotations the same as for training, no need for data augmentation
>>> def val_formatted_anns(image_id, objects):
...     annotations = []
...     for i in range(0, len(objects["id"])):
...         new_ann = {
...             "id": objects["id"][i],
...             "category_id": objects["category"][i],
...             "iscrowd": 0,
...             "image_id": image_id,
...             "area": objects["area"][i],
...             "bbox": objects["bbox"][i],
...         }
...         annotations.append(new_ann)

...     return annotations

>>> # Save images and annotations into the files torchvision.datasets.CocoDetection expects
>>> def save_cppe5_annotation_file_images(cppe5):
...     output_json = {}
...     path_output_cppe5 = f"{os.getcwd()}/cppe5/"

...     if not os.path.exists(path_output_cppe5):
...         os.makedirs(path_output_cppe5)

...     path_anno = os.path.join(path_output_cppe5, "cppe5_ann.json")
...     categories_json = [{"supercategory": "none", "id": id, "name": id2label[id]} for id in id2label]
...     output_json["images"] = []
...     output_json["annotations"] = []
...     for example in cppe5:
...         ann = val_formatted_anns(example["image_id"], example["objects"])
...         output_json["images"].append(
...             {
...                 "id": example["image_id"],
...                 "width": example["image"].width,
...                 "height": example["image"].height,
...                 "file_name": f"{example['image_id']}.png",
...             }
...         )
...         output_json["annotations"].extend(ann)
...     output_json["categories"] = categories_json

...     with open(path_anno, "w") as file:
...         json.dump(output_json, file, ensure_ascii=False, indent=4)

...     for im, img_id in zip(cppe5["image"], cppe5["image_id"]):
...         path_img = os.path.join(path_output_cppe5, f"{img_id}.png")
...         im.save(path_img)

...     return path_output_cppe5, path_anno
```

æŽ¥ä¸‹æ¥ï¼Œå‡†å¤‡ä¸€ä¸ªå¯ä»¥ä¸Ž`cocoevaluator`ä¸€èµ·ä½¿ç”¨çš„`CocoDetection`ç±»çš„å®žä¾‹ã€‚

```py
>>> import torchvision

>>> class CocoDetection(torchvision.datasets.CocoDetection):
...     def __init__(self, img_folder, image_processor, ann_file):
...         super().__init__(img_folder, ann_file)
...         self.image_processor = image_processor

...     def __getitem__(self, idx):
...         # read in PIL image and target in COCO format
...         img, target = super(CocoDetection, self).__getitem__(idx)

...         # preprocess image and target: converting target to DETR format,
...         # resizing + normalization of both image and target)
...         image_id = self.ids[idx]
...         target = {"image_id": image_id, "annotations": target}
...         encoding = self.image_processor(images=img, annotations=target, return_tensors="pt")
...         pixel_values = encoding["pixel_values"].squeeze()  # remove batch dimension
...         target = encoding["labels"][0]  # remove batch dimension

...         return {"pixel_values": pixel_values, "labels": target}

>>> im_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")

>>> path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5["test"])
>>> test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)
```

æœ€åŽï¼ŒåŠ è½½æŒ‡æ ‡å¹¶è¿è¡Œè¯„ä¼°ã€‚

```py
>>> import evaluate
>>> from tqdm import tqdm

>>> model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
>>> module = evaluate.load("ybelkada/cocoevaluate", coco=test_ds_coco_format.coco)
>>> val_dataloader = torch.utils.data.DataLoader(
...     test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn
... )

>>> with torch.no_grad():
...     for idx, batch in enumerate(tqdm(val_dataloader)):
...         pixel_values = batch["pixel_values"]
...         pixel_mask = batch["pixel_mask"]

...         labels = [
...             {k: v for k, v in t.items()} for t in batch["labels"]
...         ]  # these are in DETR format, resized + normalized

...         # forward pass
...         outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)

...         orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
...         results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to Pascal VOC format (xmin, ymin, xmax, ymax)

...         module.add(prediction=results, reference=labels)
...         del batch

>>> results = module.compute()
>>> print(results)
Accumulating evaluation results...
DONE (t=0.08s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590
```

é€šè¿‡è°ƒæ•´[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­çš„è¶…å‚æ•°ï¼Œè¿™äº›ç»“æžœå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„ã€‚è¯•ä¸€è¯•å§ï¼

## æŽ¨æ–­

çŽ°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªDETRæ¨¡åž‹ï¼Œå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ°Hugging Face Hubï¼Œæ‚¨å¯ä»¥å°†å…¶ç”¨äºŽæŽ¨æ–­ã€‚å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨æ‚¨çš„æ¨¡åž‹å®žä¾‹åŒ–ä¸€ä¸ªç”¨äºŽç›®æ ‡æ£€æµ‹çš„æµæ°´çº¿ï¼Œå¹¶å°†å›¾åƒä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline
>>> import requests

>>> url = "https://i.imgur.com/2lnWoly.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> obj_detector = pipeline("object-detection", model="devonho/detr-resnet-50_finetuned_cppe5")
>>> obj_detector(image)
```

å¦‚æžœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶æµæ°´çº¿çš„ç»“æžœï¼š

```py
>>> image_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
>>> model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")

>>> with torch.no_grad():
...     inputs = image_processor(images=image, return_tensors="pt")
...     outputs = model(**inputs)
...     target_sizes = torch.tensor([image.size[::-1]])
...     results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected Coverall with confidence 0.566 at location [1215.32, 147.38, 4401.81, 3227.08]
Detected Mask with confidence 0.584 at location [2449.06, 823.19, 3256.43, 1413.9]
```

è®©æˆ‘ä»¬ç»˜åˆ¶ç»“æžœï¼š

```py
>>> draw = ImageDraw.Draw(image)

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     x, y, x2, y2 = tuple(box)
...     draw.rectangle((x, y, x2, y2), outline="red", width=1)
...     draw.text((x, y), model.config.id2label[label.item()], fill="white")

>>> image
```

åœ¨ä¸€å¼ æ–°å›¾ç‰‡ä¸Šçš„ç›®æ ‡æ£€æµ‹ç»“æžœ
