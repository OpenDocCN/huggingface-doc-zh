["```py\nPUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\nKEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n\nssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n```", "```py\npython -m notebook --allow-root --port=8080\n```", "```py\nfrom datasets import load_dataset\n\n# Dataset id from huggingface.co/dataset\ndataset_id = \"philschmid/emotion\"\n\n# Load raw dataset\nraw_dataset = load_dataset(dataset_id)\n\nprint(f\"Train dataset size: {len(raw_dataset['train'])}\")\nprint(f\"Test dataset size: {len(raw_dataset['test'])}\")\n\n# Train dataset size: 16000\n# Test dataset size: 2000\n```", "```py\nfrom random import randrange\n\nrandom_id = randrange(len(raw_dataset['train']))\nraw_dataset['train'][random_id]\n# {'text': 'i feel isolated and alone in my trade', 'label': 0}\n```", "```py\nfrom transformers import AutoTokenizer\nimport os\n# Model id to load the tokenizer\nmodel_id = \"bert-base-uncased\"\nsave_dataset_path = \"lm_dataset\"\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Tokenize helper function\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding='max_length', truncation=True,return_tensors=\"pt\")\n\n# Tokenize dataset\nraw_dataset =  raw_dataset.rename_column(\"label\", \"labels\") # to match Trainer\ntokenized_dataset = raw_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\ntokenized_dataset = tokenized_dataset.with_format(\"torch\")\n\n# save dataset to disk\ntokenized_dataset[\"train\"].save_to_disk(os.path.join(save_dataset_path,\"train\"))\ntokenized_dataset[\"test\"].save_to_disk(os.path.join(save_dataset_path,\"eval\"))\n```", "```py\n- from transformers import Trainer, TrainingArguments\n+ from optimum.neuron import NeuronTrainer as Trainer\n+ from optimum.neuron import NeuronTrainingArguments as TrainingArguments\n```", "```py\nfrom transformers import TrainingArguments\nfrom optimum.neuron import NeuronTrainer as Trainer\n\ndef parse_args():\n\t...\n\ndef training_function(args):\n\n    # load dataset from disk and tokenizer\n    train_dataset = load_from_disk(os.path.join(args.dataset_path, \"train\"))\n\t\t...\n\n    # Download the model from huggingface.co/models\n    model = AutoModelForSequenceClassification.from_pretrained(\n        args.model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n    )\n\n    training_args = TrainingArguments(\n\t\t\t...\n    )\n\n    # Create Trainer instance\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    # Start training\n    trainer.train()\n```", "```py\n!wget https://raw.githubusercontent.com/huggingface/optimum-neuron/main/notebooks/text-classification/scripts/train.py\n```", "```py\n!torchrun --nproc_per_node=2 train.py \\\n --model_id bert-base-uncased \\\n --dataset_path lm_dataset \\\n --lr 5e-5 \\\n --per_device_train_batch_size 16 \\\n --bf16 True \\\n --epochs 3\n```", "```py\n***** train metrics *****\n  epoch                    =        3.0\n  train_runtime            =    0:08:30\n  train_samples            =      16000\n  train_samples_per_second =     96.337\n\n***** eval metrics *****\n  eval_f1                  =      0.914\n  eval_runtime             =    0:00:08\n```"]