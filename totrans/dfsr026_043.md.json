["```py\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate\n```", "```py\nsource_prompt = \"a bowl of fruits\"\ntarget_prompt = \"a bowl of pears\"\n```", "```py\nimport torch\nfrom diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n\npipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\",\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    use_safetensors=True,\n)\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\npipeline.enable_model_cpu_offload()\npipeline.enable_vae_slicing()\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\n\nimg_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\nraw_image = load_image(img_url).resize((768, 768))\nraw_image\n```", "```py\nfrom PIL import Image\n\nsource_prompt = \"a bowl of fruits\"\ntarget_prompt = \"a basket of pears\"\nmask_image = pipeline.generate_mask(\n    image=raw_image,\n    source_prompt=source_prompt,\n    target_prompt=target_prompt,\n)\nImage.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))\n```", "```py\ninv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents\n```", "```py\noutput_image = pipeline(\n    prompt=target_prompt,\n    mask_image=mask_image,\n    image_latents=inv_latents,\n    negative_prompt=source_prompt,\n).images[0]\nmask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))\nmake_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)\n```", "```py\nimport torch\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n```", "```py\nsource_concept = \"bowl\"\ntarget_concept = \"basket\"\n\nsource_text = f\"Provide a caption for images containing a {source_concept}. \"\n\"The captions should be in English and should be no longer than 150 characters.\"\n\ntarget_text = f\"Provide a caption for images containing a {target_concept}. \"\n\"The captions should be in English and should be no longer than 150 characters.\"\n```", "```py\n@torch.no_grad()\ndef generate_prompts(input_prompt):\n    input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n    outputs = model.generate(\n        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nsource_prompts = generate_prompts(source_text)\ntarget_prompts = generate_prompts(target_text)\nprint(source_prompts)\nprint(target_prompts)\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionDiffEditPipeline\n\npipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\npipeline.enable_vae_slicing()\n\n@torch.no_grad()\ndef embed_prompts(sentences, tokenizer, text_encoder, device=\"cuda\"):\n    embeddings = []\n    for sent in sentences:\n        text_inputs = tokenizer(\n            sent,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]\n        embeddings.append(prompt_embeds)\n    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)\n\nsource_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)\ntarget_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)\n```", "```py\n  from diffusers import DDIMInverseScheduler, DDIMScheduler\n  from diffusers.utils import load_image, make_image_grid\n  from PIL import Image\n\n  pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n  pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n\n  img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n  raw_image = load_image(img_url).resize((768, 768))\n\n  mask_image = pipeline.generate_mask(\n      image=raw_image,\n-     source_prompt=source_prompt,\n-     target_prompt=target_prompt,\n+     source_prompt_embeds=source_embeds,\n+     target_prompt_embeds=target_embeds,\n  )\n\n  inv_latents = pipeline.invert(\n-     prompt=source_prompt,\n+     prompt_embeds=source_embeds,\n      image=raw_image,\n  ).latents\n\n  output_image = pipeline(\n      mask_image=mask_image,\n      image_latents=inv_latents,\n-     prompt=target_prompt,\n-     negative_prompt=source_prompt,\n+     prompt_embeds=target_embeds,\n+     negative_prompt_embeds=source_embeds,\n  ).images[0]\n  mask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\")\n  make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)\n```", "```py\nimport torch\nfrom transformers import BlipForConditionalGeneration, BlipProcessor\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n```", "```py\n@torch.no_grad()\ndef generate_caption(images, caption_generator, caption_processor):\n    text = \"a photograph of\"\n\n    inputs = caption_processor(images, text, return_tensors=\"pt\").to(device=\"cuda\", dtype=caption_generator.dtype)\n    caption_generator.to(\"cuda\")\n    outputs = caption_generator.generate(**inputs, max_new_tokens=128)\n\n    # offload caption generator\n    caption_generator.to(\"cpu\")\n\n    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n    return caption\n```", "```py\nfrom diffusers.utils import load_image\n\nimg_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\nraw_image = load_image(img_url).resize((768, 768))\ncaption = generate_caption(raw_image, model, processor)\n```"]