# CLIPSeg

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clipseg](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clipseg)

## 概述

CLIPSeg模型是由Timo Lüddecke和Alexander Ecker在[使用文本和图像提示进行图像分割](https://arxiv.org/abs/2112.10003)中提出的。CLIPSeg在冻结的[CLIP](clip)模型之上添加了一个最小的解码器，用于零样本和一样本图像分割。

论文摘要如下：

*图像分割通常通过训练一个固定对象类别的模型来解决。随后合并额外类别或更复杂查询是昂贵的，因为需要在包含这些表达的数据集上重新训练模型。在这里，我们提出了一个系统，可以根据测试时的任意提示生成图像分割。提示可以是文本或图像。这种方法使我们能够为三种常见的分割任务创建一个统一的模型（仅训练一次），这些任务具有不同的挑战：指代表达分割、零样本分割和一样本分割。我们基于CLIP模型作为骨干，扩展了一个基于transformer的解码器，实现了密集预测。在扩展的PhraseCut数据集上训练后，我们的系统可以根据自由文本提示或表达查询的附加图像为图像生成二进制分割图。我们详细分析了不同变体的基于图像的提示。这种新颖的混合输入不仅可以动态适应上述三种分割任务，还可以适应任何可以制定文本或图像查询的二进制分割任务。最后，我们发现我们的系统能够很好地适应涉及功能或属性的广义查询*

![绘图](../Images/b4846ade27ce9551fbfd181099756635.png) CLIPSeg概述。取自[原始论文。](https://arxiv.org/abs/2112.10003)

这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。原始代码可以在[这里](https://github.com/timojl/clipseg)找到。

## 使用提示

+   [CLIPSegForImageSegmentation](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation)在[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)之上添加了一个解码器。后者与[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)相同。

+   [CLIPSegForImageSegmentation](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation)可以根据测试时的任意提示生成图像分割。提示可以是文本（作为`input_ids`提供给模型）或图像（作为`conditional_pixel_values`提供给模型）。还可以提供自定义的条件嵌入（作为`conditional_embeddings`提供给模型）。

## 资源

列出了官方Hugging Face和社区（由🌎表示）的资源，以帮助您开始使用CLIPSeg。如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新东西，而不是重复现有资源。

图像分割

+   一个展示[使用CLIPSeg进行零样本图像分割的笔记本](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb)。

## CLIPSegConfig

### `class transformers.CLIPSegConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/configuration_clipseg.py#L248)

```py
( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 extract_layers = [3, 6, 9] reduce_dim = 64 decoder_num_attention_heads = 4 decoder_attention_dropout = 0.0 decoder_hidden_act = 'quick_gelu' decoder_intermediate_size = 2048 conditional_layer = 0 use_complex_transposed_convolution = False **kwargs )
```

参数

+   `text_config`（`dict`，*可选*）—用于初始化[CLIPSegTextConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegTextConfig)的配置选项字典。

+   `vision_config` (`dict`，*可选*) — 用于初始化[CLIPSegVisionConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegVisionConfig)的配置选项的字典。

+   `projection_dim` (`int`，*可选*，默认为512) — 文本和视觉投影层的维度。

+   `logit_scale_init_value` (`float`，*可选*，默认为2.6592) — *logit_scale*参数的初始值。默认值根据原始CLIPSeg实现使用。

+   `extract_layers` (`List[int]`，*可选*，默认为`[3, 6, 9]`) — 在通过CLIP的冻结视觉骨干传递查询图像时要提取的层。

+   `reduce_dim` (`int`，*可选*，默认为64) — 用于减少CLIP视觉嵌入的维度。

+   `decoder_num_attention_heads` (`int`，*可选*，默认为4) — CLIPSeg解码器中的注意力头数。

+   `decoder_attention_dropout` (`float`，*可选*，默认为0.0) — 注意力概率的丢弃比率。

+   `decoder_hidden_act` (`str`或`function`，*可选*，默认为`"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。

+   `decoder_intermediate_size` (`int`，*可选*，默认为2048) — Transformer解码器中“中间”（即前馈）层的维度。

+   `conditional_layer` (`int`，*可选*，默认为0) — 使用Transformer编码器的层，其激活将与条件嵌入使用FiLM（Feature-wise Linear Modulation）组合。如果为0，则使用最后一层。

+   `use_complex_transposed_convolution` (`bool`，*可选*，默认为`False`) — 是否在解码器中使用更复杂的转置卷积，从而实现更精细的分割。

+   `kwargs`（*可选*） — 关键字参数的字典。

[CLIPSegConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegConfig)是用于存储[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)配置的配置类。它用于根据指定的参数实例化一个CLIPSeg模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生与CLIPSeg [CIDAS/clipseg-rd64](https://huggingface.co/CIDAS/clipseg-rd64)架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import CLIPSegConfig, CLIPSegModel

>>> # Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration
>>> configuration = CLIPSegConfig()

>>> # Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration
>>> model = CLIPSegModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig

>>> # Initializing a CLIPSegText and CLIPSegVision configuration
>>> config_text = CLIPSegTextConfig()
>>> config_vision = CLIPSegVisionConfig()

>>> config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/configuration_clipseg.py#L423)

```py
( text_config: CLIPSegTextConfig vision_config: CLIPSegVisionConfig **kwargs ) → export const metadata = 'undefined';CLIPSegConfig
```

返回值

[CLIPSegConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegConfig)

配置对象的一个实例

从clipseg文本模型配置和clipseg视觉模型配置实例化一个[CLIPSegConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegConfig)（或派生类）。

## CLIPSegTextConfig

### `class transformers.CLIPSegTextConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/configuration_clipseg.py#L31)

```py
( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )
```

参数

+   `vocab_size` (`int`，*可选*，默认为49408) — CLIPSeg文本模型的词汇量。定义了在调用[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)时可以由`inputs_ids`表示的不同标记数量。

+   `hidden_size` (`int`，*可选*，默认为512) — 编码器层和池化器层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 2048) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中隐藏层的数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 8) — Transformer编码器中每个注意力层的注意力头数。

+   `max_position_embeddings` (`int`, *optional*, defaults to 77) — 此模型可能使用的最大序列长度。通常将其设置为一个较大的值以防万一（例如512、1024或2048）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢弃比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

+   `pad_token_id` (`int`, *optional*, defaults to 1) — 填充标记id。

+   `bos_token_id` (`int`, *optional*, defaults to 49406) — 流的开始标记id。

+   `eos_token_id` (`int`, *optional*, defaults to 49407) — 流的结束标记id。

这是一个配置类，用于存储[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)的配置。根据指定的参数实例化一个CLIPSeg模型，定义模型架构。使用默认值实例化配置将产生与CLIPSeg [CIDAS/clipseg-rd64](https://huggingface.co/CIDAS/clipseg-rd64)架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import CLIPSegTextConfig, CLIPSegTextModel

>>> # Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration
>>> configuration = CLIPSegTextConfig()

>>> # Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration
>>> model = CLIPSegTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## CLIPSegVisionConfig

### `class transformers.CLIPSegVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/configuration_clipseg.py#L143)

```py
( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中隐藏层的数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 32) — 每个patch的大小（分辨率）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。 

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢弃比率。

+   `initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *可选*, 默认为1.0) — 用于初始化所有权重矩阵的因子（应保持为1，内部用于初始化测试）。

这是用于存储[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)配置的配置类。它用于根据指定的参数实例化一个CLIPSeg模型，定义模型架构。使用默认值实例化配置将产生与CLIPSeg [CIDAS/clipseg-rd64](https://huggingface.co/CIDAS/clipseg-rd64)架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例:

```py
>>> from transformers import CLIPSegVisionConfig, CLIPSegVisionModel

>>> # Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration
>>> configuration = CLIPSegVisionConfig()

>>> # Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration
>>> model = CLIPSegVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## CLIPSegProcessor

### `class transformers.CLIPSegProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/processing_clipseg.py#L25)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor` ([ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor), *可选*) — 图像处理器是必需的输入。

+   `tokenizer` ([CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast), *可选*) — 标记器是必需的输入。

构建一个CLIPSeg处理器，将CLIPSeg图像处理器和CLIP标记器包装成一个单一处理器。

[CLIPSegProcessor](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegProcessor)提供了[ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)和[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)的所有功能。查看`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegProcessor.decode)以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/processing_clipseg.py#L134)

```py
( *args **kwargs )
```

此方法将其所有参数转发到CLIPTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/processing_clipseg.py#L141)

```py
( *args **kwargs )
```

此方法将其所有参数转发到CLIPTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。

## CLIPSegModel

### `class transformers.CLIPSegModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L924)

```py
( config: CLIPSegConfig )
```

参数

+   `config` ([CLIPSegConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

此模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L1056)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clipseg.modeling_clipseg.CLIPSegOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*) — 避免在填充标记索引上执行注意力的蒙版。蒙版值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`, *optional*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.clipseg.modeling_clipseg.CLIPSegOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clipseg.modeling_clipseg.CLIPSegOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（`<class 'transformers.models.clipseg.configuration_clipseg.CLIPSegConfig'>`）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当`return_loss`为`True`时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image:(torch.FloatTensor`，形状为`(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(torch.FloatTensor`，形状为`(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于[CLIPSegTextModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegTextModel)的汇聚输出获得的文本嵌入。

+   `image_embeds(torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于[CLIPSegVisionModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel)的汇聚输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [CLIPSegTextModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegTextModel)的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` [CLIPSegVisionModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel)的输出。

[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPSegModel

>>> processor = AutoProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
>>> model = CLIPSegModel.from_pretrained("CIDAS/clipseg-rd64-refined")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L960)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被掩盖的标记为1，

    +   对于被掩盖的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

文本特征（`torch.FloatTensor`，形状为`(batch_size, output_dim)`）

通过将投影层应用于[CLIPSegTextModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegTextModel)的汇聚输出获得的文本嵌入。

[CLIPSegModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegModel)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, CLIPSegModel

>>> tokenizer = AutoTokenizer.from_pretrained("CIDAS/clipseg-rd64-refined")
>>> model = CLIPSegModel.from_pretrained("CIDAS/clipseg-rd64-refined")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L1007)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values`（`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`）— 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。 

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

image_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）

通过将投影层应用于[CLIPSegVisionModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel)的池化输出获得的图像嵌入。

CLIPSegModel的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPSegModel

>>> processor = AutoProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
>>> model = CLIPSegModel.from_pretrained("CIDAS/clipseg-rd64-refined")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> image_features = model.get_image_features(**inputs)
```

## CLIPSegTextModel

### `class transformers.CLIPSegTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L757)

```py
( config: CLIPSegTextConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L774)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中选择：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为 `(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这将返回经过线性层和双曲正切激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层，则为嵌入输出的输出 + 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    注意力权重经过注意力 softmax 后，用于计算自注意力头中的加权平均值。

[CLIPSegTextModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegTextModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, CLIPSegTextModel

>>> tokenizer = AutoTokenizer.from_pretrained("CIDAS/clipseg-rd64-refined")
>>> model = CLIPSegTextModel.from_pretrained("CIDAS/clipseg-rd64-refined")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## CLIPSegVisionModel

### `class transformers.CLIPSegVisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L872)

```py
( config: CLIPSegVisionConfig )
```

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L885)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False` 或 `return_dict=False`）包含根据配置 (`<class 'transformers.models.clipseg.configuration_clipseg.CLIPSegVisionConfig'>`) 和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后，序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- `torch.FloatTensor`的元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）的形状为`(batch_size, sequence_length, hidden_size)`。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- `torch.FloatTensor`的元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    用于计算自注意力头中加权平均值的注意力softmax后的注意力权重。

[CLIPSegVisionModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPSegVisionModel

>>> processor = AutoProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
>>> model = CLIPSegVisionModel.from_pretrained("CIDAS/clipseg-rd64-refined")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```

## CLIPSegForImageSegmentation

### `class transformers.CLIPSegForImageSegmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L1307)

```py
( config: CLIPSegConfig )
```

参数

+   `config`（[CLIPSegConfig](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

CLIPSeg模型在顶部使用基于Transformer的解码器进行零样本和一样本图像分割。

这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clipseg/modeling_clipseg.py#L1358)

```py
( input_ids: Optional = None pixel_values: Optional = None conditional_pixel_values: Optional = None conditional_embeddings: Optional = None attention_mask: Optional = None position_ids: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   1表示未被`masked`的标记。

    +   0表示被`masked`的标记。

    [注意力蒙版是什么？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [位置ID是什么？](../glossary#position-ids)

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参见[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss`（`bool`，*可选*）— 是否返回对比损失。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包含根据配置（`<class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'>`）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当`return_loss`为`True`时返回）— 图像文本相似性的对比损失。…

+   `vision_model_output`（`BaseModelOutputWithPooling`）— [CLIPSegVisionModel](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel)的输出。

[CLIPSegForImageSegmentation](/docs/transformers/v4.37.2/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, CLIPSegForImageSegmentation
>>> from PIL import Image
>>> import requests

>>> processor = AutoProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
>>> model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = ["a cat", "a remote", "a blanket"]
>>> inputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors="pt")

>>> outputs = model(**inputs)

>>> logits = outputs.logits
>>> print(logits.shape)
torch.Size([3, 352, 352])
```
