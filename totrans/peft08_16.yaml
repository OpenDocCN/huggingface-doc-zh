- en: Adapter injection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适配器注入
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/low_level_api](https://huggingface.co/docs/peft/developer_guides/low_level_api)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/developer_guides/low_level_api](https://huggingface.co/docs/peft/developer_guides/low_level_api)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: With PEFT, you can inject trainable adapters into any `torch` module which allows
    you to use adapter methods without relying on the modeling classes in PEFT. Currently,
    PEFT supports injecting [LoRA](../conceptual_guides/adapter#low-rank-adaptation-lora),
    [AdaLoRA](../conceptual_guides/adapter#adaptive-low-rank-adaptation-adalora),
    and [IA3](../conceptual_guides/ia3) into models because for these adapters, inplace
    modification of the model is sufficient for finetuning it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PEFT，您可以将可训练的适配器注入到任何`torch`模块中，这使您可以在不依赖PEFT中的建模类的情况下使用适配器方法。目前，PEFT支持将[LoRA](../conceptual_guides/adapter#low-rank-adaptation-lora)、[AdaLoRA](../conceptual_guides/adapter#adaptive-low-rank-adaptation-adalora)和[IA3](../conceptual_guides/ia3)注入到模型中，因为对于这些适配器，对模型的原地修改足以进行微调。
- en: Check the table below to see when you should inject adapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 查看下表，了解何时应该注入适配器。
- en: '| Pros | Cons |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 缺点 |'
- en: '| --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| the model is modified inplace, keeping all the original attributes and methods
    | manually write the `from_pretrained` and `save_pretrained` utility functions
    from Hugging Face to save and load adapters |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 模型在原地修改，保留所有原始属性和方法 | 手动编写`from_pretrained`和`save_pretrained`实用函数，从Hugging
    Face保存和加载适配器 |'
- en: '| works for any `torch` module and modality | doesn’t work with any of the
    utility methods provided by `PeftModel` such as disabling and merging adapters
    |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 适用于任何`torch`模块和模态 | 不能与`PeftModel`提供的任何实用方法一起使用，如禁用和合并适配器 |'
- en: To perform the adapter injection, use the [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    method. This method takes 3 arguments, the PEFT config, the model, and an optional
    adapter name. You can also attach multiple adapters to the model if you call [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    multiple times with different adapter names.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行适配器注入，请使用 [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    方法。此方法接受3个参数，PEFT配置、模型和一个可选的适配器名称。如果使用不同的适配器名称多次调用 [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)，还可以将多个适配器附加到模型上。
- en: 'For example, to inject LoRA adapters into the `linear` submodule of the `DummyModel`
    module:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将LoRA适配器注入到`DummyModel`模块的`linear`子模块中：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Print the model to see that the adapters have been correctly injected.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 打印模型以查看适配器是否已正确注入。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To only save the adapter, use the [get_peft_model_state_dict()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model_state_dict)
    function:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要仅保存适配器，请使用 [get_peft_model_state_dict()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model_state_dict)
    函数：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Otherwise, `model.state_dict()` returns the full state dict of the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，`model.state_dict()`将返回模型的完整状态字典。
