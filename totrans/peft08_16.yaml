- en: Adapter injection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/developer_guides/low_level_api](https://huggingface.co/docs/peft/developer_guides/low_level_api)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/10.24e12d1e.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
  prefs: []
  type: TYPE_NORMAL
- en: With PEFT, you can inject trainable adapters into any `torch` module which allows
    you to use adapter methods without relying on the modeling classes in PEFT. Currently,
    PEFT supports injecting [LoRA](../conceptual_guides/adapter#low-rank-adaptation-lora),
    [AdaLoRA](../conceptual_guides/adapter#adaptive-low-rank-adaptation-adalora),
    and [IA3](../conceptual_guides/ia3) into models because for these adapters, inplace
    modification of the model is sufficient for finetuning it.
  prefs: []
  type: TYPE_NORMAL
- en: Check the table below to see when you should inject adapters.
  prefs: []
  type: TYPE_NORMAL
- en: '| Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| the model is modified inplace, keeping all the original attributes and methods
    | manually write the `from_pretrained` and `save_pretrained` utility functions
    from Hugging Face to save and load adapters |'
  prefs: []
  type: TYPE_TB
- en: '| works for any `torch` module and modality | doesn’t work with any of the
    utility methods provided by `PeftModel` such as disabling and merging adapters
    |'
  prefs: []
  type: TYPE_TB
- en: To perform the adapter injection, use the [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    method. This method takes 3 arguments, the PEFT config, the model, and an optional
    adapter name. You can also attach multiple adapters to the model if you call [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    multiple times with different adapter names.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to inject LoRA adapters into the `linear` submodule of the `DummyModel`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Print the model to see that the adapters have been correctly injected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To only save the adapter, use the [get_peft_model_state_dict()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model_state_dict)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Otherwise, `model.state_dict()` returns the full state dict of the model.
  prefs: []
  type: TYPE_NORMAL
