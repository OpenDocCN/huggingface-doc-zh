- en: Adapter injection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/low_level_api](https://huggingface.co/docs/peft/developer_guides/low_level_api)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: With PEFT, you can inject trainable adapters into any `torch` module which allows
    you to use adapter methods without relying on the modeling classes in PEFT. Currently,
    PEFT supports injecting [LoRA](../conceptual_guides/adapter#low-rank-adaptation-lora),
    [AdaLoRA](../conceptual_guides/adapter#adaptive-low-rank-adaptation-adalora),
    and [IA3](../conceptual_guides/ia3) into models because for these adapters, inplace
    modification of the model is sufficient for finetuning it.
  prefs: []
  type: TYPE_NORMAL
- en: Check the table below to see when you should inject adapters.
  prefs: []
  type: TYPE_NORMAL
- en: '| Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| the model is modified inplace, keeping all the original attributes and methods
    | manually write the `from_pretrained` and `save_pretrained` utility functions
    from Hugging Face to save and load adapters |'
  prefs: []
  type: TYPE_TB
- en: '| works for any `torch` module and modality | doesnâ€™t work with any of the
    utility methods provided by `PeftModel` such as disabling and merging adapters
    |'
  prefs: []
  type: TYPE_TB
- en: To perform the adapter injection, use the [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    method. This method takes 3 arguments, the PEFT config, the model, and an optional
    adapter name. You can also attach multiple adapters to the model if you call [inject_adapter_in_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.inject_adapter_in_model)
    multiple times with different adapter names.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to inject LoRA adapters into the `linear` submodule of the `DummyModel`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Print the model to see that the adapters have been correctly injected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To only save the adapter, use the [get_peft_model_state_dict()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model_state_dict)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Otherwise, `model.state_dict()` returns the full state dict of the model.
  prefs: []
  type: TYPE_NORMAL
