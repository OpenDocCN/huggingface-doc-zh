["```py\n( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) \u2192 export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline\n>>> import torch\n\n>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\")\n>>> pipe_prior.to(\"cuda\")\n>>> prompt = \"red cat, 4k photo\"\n>>> image_emb, negative_image_emb = pipe_prior(prompt).to_tuple()\n\n>>> pipe = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\")\n>>> pipe.to(\"cuda\")\n>>> image = pipe(\n...     image_embeds=image_emb,\n...     negative_image_embeds=negative_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=50,\n... ).images\n>>> image[0].save(\"cat.png\")\n```", "```py\n( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) \u2192 export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n>>> from diffusers.utils import load_image\n>>> import PIL\n>>> import torch\n>>> from torchvision import transforms\n\n>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n... )\n>>> pipe_prior.to(\"cuda\")\n>>> img1 = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/cat.png\"\n... )\n>>> img2 = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/starry_night.jpeg\"\n... )\n>>> images_texts = [\"a cat\", img1, img2]\n>>> weights = [0.3, 0.3, 0.4]\n>>> out = pipe_prior.interpolate(images_texts, weights)\n>>> pipe = KandinskyV22Pipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n... )\n>>> pipe.to(\"cuda\")\n>>> image = pipe(\n...     image_embeds=out.image_embeds,\n...     negative_image_embeds=out.negative_image_embeds,\n...     height=768,\n...     width=768,\n...     num_inference_steps=50,\n... ).images[0]\n>>> image.save(\"starry_cat.png\")\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )\n```", "```py\n( image_embeds: Union negative_image_embeds: Union height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline\n>>> import torch\n\n>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\")\n>>> pipe_prior.to(\"cuda\")\n>>> prompt = \"red cat, 4k photo\"\n>>> out = pipe_prior(prompt)\n>>> image_emb = out.image_embeds\n>>> zero_image_emb = out.negative_image_embeds\n>>> pipe = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\")\n>>> pipe.to(\"cuda\")\n>>> image = pipe(\n...     image_embeds=image_emb,\n...     negative_image_embeds=zero_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=50,\n... ).images\n>>> image[0].save(\"cat.png\")\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k\"\n\nimage = pipe(prompt=prompt, num_inference_steps=25).images[0]\n```", "```py\n( gpu_id = 0 )\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )\n```", "```py\n( image_embeds: Union negative_image_embeds: Union hint: FloatTensor height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union image: Union strength: float = 0.3 negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True ) \u2192 export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorEmb2EmbPipeline\n>>> import torch\n\n>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n... )\n>>> pipe_prior.to(\"cuda\")\n\n>>> prompt = \"red cat, 4k photo\"\n>>> img = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/cat.png\"\n... )\n>>> image_emb, nagative_image_emb = pipe_prior(prompt, image=img, strength=0.2).to_tuple()\n\n>>> pipe = KandinskyPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-2-decoder, torch_dtype=torch.float16\"\n... )\n>>> pipe.to(\"cuda\")\n\n>>> image = pipe(\n...     image_embeds=image_emb,\n...     negative_image_embeds=negative_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=100,\n... ).images\n\n>>> image[0].save(\"cat.png\")\n```", "```py\n( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) \u2192 export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22Pipeline\n>>> from diffusers.utils import load_image\n>>> import PIL\n\n>>> import torch\n>>> from torchvision import transforms\n\n>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n... )\n>>> pipe_prior.to(\"cuda\")\n\n>>> img1 = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/cat.png\"\n... )\n\n>>> img2 = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/starry_night.jpeg\"\n... )\n\n>>> images_texts = [\"a cat\", img1, img2]\n>>> weights = [0.3, 0.3, 0.4]\n>>> image_emb, zero_image_emb = pipe_prior.interpolate(images_texts, weights)\n\n>>> pipe = KandinskyV22Pipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n... )\n>>> pipe.to(\"cuda\")\n\n>>> image = pipe(\n...     image_embeds=image_emb,\n...     negative_image_embeds=zero_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=150,\n... ).images[0]\n\n>>> image.save(\"starry_cat.png\")\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )\n```", "```py\n( image_embeds: Union image: Union negative_image_embeds: Union height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 strength: float = 0.3 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 strength: float = 0.3 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport os\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\nimage.thumbnail((768, 768))\n\nimage = pipe(prompt=prompt, image=original_image, num_inference_steps=25).images[0]\n```", "```py\n( gpu_id = 0 )\n```", "```py\n( gpu_id = 0 )\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )\n```", "```py\n( image_embeds: Union image: Union negative_image_embeds: Union hint: FloatTensor height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 strength: float = 0.3 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )\n```", "```py\n( image_embeds: Union image: Union mask_image: Union negative_image_embeds: Union height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union image: Union mask_image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nmask = np.zeros((768, 768), dtype=np.float32)\n# Let's mask out an area above the cat's head\nmask[:250, 250:-250] = 1\n\nimage = pipe(prompt=prompt, image=original_image, mask_image=mask, num_inference_steps=25).images[0]\n```", "```py\n( gpu_id = 0 )\n```"]