# 受控生成

> 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation](https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation)

控制扩散模型生成的输出长期以来一直是社区追求的目标，现在是一个活跃的研究课题。在许多流行的扩散模型中，输入（包括图像和文本提示）的微小变化可能会极大地改变输出。在理想的世界中，我们希望能够控制语义是如何被保留和改变的。

大多数保留语义的示例都归结为能够准确地将输入的变化映射到输出的变化。即在提示中给主语添加形容词会保留整个图像，只修改已更改的主语。或者，特定主题的图像变化会保留主题的姿势。

此外，我们希望影响生成图像的质量，超出语义保留的范围。即一般来说，我们希望输出具有良好的质量，符合特定风格，或者是逼真的。

我们将记录一些技术，`diffusers`支持控制扩散模型的生成。许多是尖端研究，可能非常微妙。如果有什么需要澄清的地方或者您有建议，请毫不犹豫地在[论坛](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)上开启讨论或者在[GitHub问题](https://github.com/huggingface/diffusers/issues)上提出。

我们提供了一个高层次的解释，说明如何控制生成，以及技术的片段。有关技术的更深入解释，始终可以从链接到的流水线中找到原始论文，这些是最好的资源。

根据用例，应相应选择技术。在许多情况下，这些技术可以结合使用。例如，可以将文本反转与SEGA结合使用，以为使用文本反转生成的输出提供更多语义指导。

除非另有说明，这些都是适用于现有模型且不需要自己权重的技术。

1.  [InstructPix2Pix](#instruct-pix2pix)

1.  [Pix2Pix Zero](#pix2pix-zero)

1.  [关注和激励](#attend-and-excite)

1.  [语义引导](#semantic-guidance-sega)

1.  [自注意力引导](#self-attention-guidance-sag)

1.  [深度到图像](#depth2image)

1.  [多扩散全景](#multidiffusion-panorama)

1.  [梦想展台](#dreambooth)

1.  [文本反转](#textual-inversion)

1.  [ControlNet](#controlnet)

1.  [提示加权](#prompt-weighting)

1.  [自定义扩散](#custom-diffusion)

1.  [模型编辑](#model-editing)

1.  [DiffEdit](#diffedit)

1.  [T2I适配器](#t2i-adapter)

1.  [FABRIC](#fabric)

为方便起见，我们提供了一个表格，用于表示哪些方法仅用于推断，哪些需要微调/训练。

| **方法** | **仅推断** | **需要训练/微调** | **评论** |
| :-: | :-: | :-: | :-: |

| [InstructPix2Pix](#instruct-pix2pix) | ✅ | ❌ | 还可以进行微调以获得更好的效果

特定性能

编辑说明。|

| [Pix2Pix Zero](#pix2pix-zero) | ✅ | ❌ |  |
| --- | --- | --- | --- |
| [关注和激励](#attend-and-excite) | ✅ | ❌ |  |
| [语义引导](#semantic-guidance-sega) | ✅ | ❌ |  |
| [自注意力引导](#self-attention-guidance-sag) | ✅ | ❌ |  |
| [深度到图像](#depth2image) | ✅ | ❌ |  |
| [多扩散全景](#multidiffusion-panorama) | ✅ | ❌ |  |
| [梦想展台](#dreambooth) | ❌ | ✅ |  |
| [文本反转](#textual-inversion) | ❌ | ✅ |  |

| [ControlNet](#controlnet) | ✅ | ❌ | 可以在ControlNet上进行训练/微调

自定义条件。|

| [提示加权](#prompt-weighting) | ✅ | ❌ |  |
| --- | --- | --- | --- |
| [自定义扩散](#custom-diffusion) | ❌ | ✅ |  |
| [模型编辑](#model-editing) | ✅ | ❌ |  |
| [DiffEdit](#diffedit) | ✅ | ❌ |  |
| [T2I适配器](#t2i-adapter) | ✅ | ❌ |  |
| [面料](#fabric) | ✅ | ❌ |  |

## InstructPix2Pix

[论文](https://arxiv.org/abs/2211.09800)

[InstructPix2Pix](../api/pipelines/pix2pix)是从Stable Diffusion微调而来，以支持编辑输入图像。它以图像和描述编辑的提示作为输入，并输出编辑后的图像。InstructPix2Pix已经明确训练得很好，可以很好地处理类似[InstructGPT](https://openai.com/blog/instruction-following/)的提示。

## Pix2Pix Zero

[论文](https://arxiv.org/abs/2302.03027)

[Pix2Pix Zero](../api/pipelines/pix2pix_zero)允许修改图像，使一个概念或主题转换为另一个概念，同时保留一般图像语义。

去噪过程是从一个概念嵌入引导到另一个概念嵌入。在去噪过程中，中间潜变量被优化，以将注意力图推向参考注意力图。参考注意力图来自输入图像的去噪过程，并用于鼓励语义保留。

Pix2Pix Zero既可以用于编辑合成图像，也可以用于真实图像。

+   要编辑合成图像，首先根据标题生成一幅图像。接下来，为将要编辑的概念和新目标概念生成图像标题。我们可以使用像[Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)这样的模型来实现这一目的。然后，通过文本编码器为源概念和目标概念创建“平均”提示嵌入。最后，使用pix2pix-zero算法来编辑合成图像。

+   要编辑真实图像，首先使用像[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)这样的模型生成图像标题。然后，将DDIM反演应用于提示和图像，生成“反向”潜变量。与之前类似，为源概念和目标概念创建“平均”提示嵌入，最后使用pix2pix-zero算法结合“反向”潜变量来编辑图像。

Pix2Pix Zero是第一个允许“零-shot”图像编辑的模型。这意味着该模型可以在消费级GPU上不到一分钟内编辑一幅图像，如[此处所示](../api/pipelines/pix2pix_zero#usage-example)。

如上所述，Pix2Pix Zero包括优化潜变量（而不是UNet、VAE或文本编码器）以引导生成朝向特定概念。这意味着整体流程可能需要比标准的[StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img)更多的内存。

像InstructPix2Pix和Pix2Pix Zero这样的方法之间的一个重要区别是前者涉及微调预训练权重，而后者不涉及。这意味着您可以将Pix2Pix Zero应用于任何可用的Stable Diffusion模型。

## Attend and Excite

[论文](https://arxiv.org/abs/2301.13826)

[Attend and Excite](../api/pipelines/attend_and_excite)允许在最终图像中忠实地呈现提示中的主题。

输入一组令牌索引，对应于提示中需要出现在图像中的主题。在去噪过程中，每个令牌索引都保证至少有一个图像块的最小注意力阈值。在去噪过程中，中间潜变量被迭代优化，以加强最被忽视主题令牌的注意力，直到所有主题令牌的注意力阈值都被通过。

与Pix2Pix Zero类似，Attend and Excite也涉及一个微优化循环（不触及预训练权重），并且可能需要比通常的[StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img)更多的内存。

## 语义引导（SEGA）

[论文](https://arxiv.org/abs/2301.12247)

[SEGA](../api/pipelines/semantic_stable_diffusion)允许在图像中应用或移除一个或多个概念。概念的强度也可以被控制。例如，微笑概念可以用来逐步增加或减少肖像的微笑。

类似于无分类器指导通过空提示输入提供指导，SEGA在概念提示上提供指导。可以同时应用多个这些概念提示。每个概念提示可以根据指导是积极还是消极地应用来添加或删除其概念。

与Pix2Pix Zero或Attend and Excite不同，SEGA直接与扩散过程互动，而不是执行任何显式的基于梯度的优化。

## 自注意力指导（SAG）

[论文](https://arxiv.org/abs/2210.00939)

[自注意力指导](../api/pipelines/self_attention_guidance)提高图像的总体质量。

SAG从不受高频细节条件的预测提供指导，到完全受条件的图像。高频细节从UNet自注意力图中提取出来。

## Depth2Image

[项目](https://huggingface.co/stabilityai/stable-diffusion-2-depth)

[Depth2Image](../api/pipelines/stable_diffusion/depth2img)是从Stable Diffusion微调的，以更好地保留文本引导图像变化的语义。

它以原始图像的单眼深度估计为条件。

## MultiDiffusion全景

[论文](https://arxiv.org/abs/2302.08113)

[MultiDiffusion全景](../api/pipelines/panorama)定义了一个新的在预训练扩散模型上的生成过程。这个过程将多个扩散生成方法绑定在一起，可以方便地应用于生成高质量和多样化的图像。结果符合用户提供的控制，如期望的宽高比（例如全景），以及空间引导信号，从紧密的分割蒙版到边界框。MultiDiffusion全景允许以任意宽高比（例如全景）生成高质量的图像。

## 微调您自己的模型

除了预训练模型外，Diffusers还有用于在用户提供的数据上微调模型的训练脚本。

## DreamBooth

[项目](https://dreambooth.github.io/)

[DreamBooth](../training/dreambooth)微调模型以教导它一个新主题。即一些人物的图片可以用来以不同风格生成那个人的图片。

## 文本反转

[论文](https://arxiv.org/abs/2208.01618)

[文本反转](../training/text_inversion)微调模型以教导它一个新概念。即一些艺术风格的图片可以用来生成那种风格的图片。

## ControlNet

[论文](https://arxiv.org/abs/2302.05543)

[ControlNet](../api/pipelines/controlnet)是一个辅助网络，它添加了额外的条件。有8个经典的预训练ControlNets，训练了不同的条件，如边缘检测、涂鸦、深度图和语义分割。

## 提示加权

[提示加权](../using-diffusers/weighted_prompts)是一种简单的技术，它在文本输入的某些部分上放置更多的注意权重。

## 自定义扩散

[论文](https://arxiv.org/abs/2212.04488)

[自定义扩散](../training/custom_diffusion)仅微调预训练的文本到图像扩散模型的交叉注意力图。它还允许进行文本反转。它通过设计支持多概念训练。与DreamBooth和文本反转一样，自定义扩散也用于教导预训练的文本到图像扩散模型关于新概念，以生成涉及感兴趣概念的输出。

## 模型编辑

[论文](https://arxiv.org/abs/2303.08084)

[text-to-image模型编辑流程](../api/pipelines/model_editing)有助于减轻预训练的文本到图像扩散模型可能对输入提示中存在的主题做出的一些不正确的隐含假设。例如，如果您提示Stable Diffusion生成“一束玫瑰”的图像，生成的图像中的玫瑰更有可能是红色的。这个流程帮助您改变这种假设。

## DiffEdit

[论文](https://arxiv.org/abs/2210.11427)

[DiffEdit](../api/pipelines/diffedit) 允许在保留原始输入图像尽可能不变的同时，对输入图像和输入提示进行语义编辑。

## T2I-Adapter

[Paper](https://arxiv.org/abs/2302.08453)

[T2I-Adapter](../api/pipelines/stable_diffusion/adapter) 是一个辅助网络，添加了额外的条件。有8个经典的预训练适配器，针对不同的条件进行训练，如边缘检测、素描、深度图和语义分割。

## Fabric

[Paper](https://arxiv.org/abs/2307.10159)

[Fabric](https://github.com/huggingface/diffusers/tree/442017ccc877279bcf24fbe92f92d3d0def191b6/examples/community#stable-diffusion-fabric-pipeline) 是一种无需训练的方法，适用于广泛流行的扩散模型，利用最常用架构中存在的自注意力层来使扩散过程依赖于一组反馈图像。
