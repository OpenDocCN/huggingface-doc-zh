# KOSMOS-2

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/kosmos-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/kosmos-2)

## 概述

KOSMOS-2模型是由Zhiliang Peng、Wenhui Wang、Li Dong、Yaru Hao、Shaohan Huang、Shuming Ma、Furu Wei在[Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824)中提出的。

KOSMOS-2是基于Transformer的因果语言模型，通过在基于网络规模的图像文本对数据集[GRIT](https://huggingface.co/datasets/zzliang/GRIT)上进行下一个单词预测任务进行训练。数据集中边界框的空间坐标被转换为位置标记序列，这些标记被附加到它们各自的实体文本跨度上（例如，`a snowman`后跟`<patch_index_0044><patch_index_0863>`）。数据格式类似于将图像中的对象区域与相应标题中的文本跨度连接起来的“超链接”。

论文摘要如下：

*我们介绍了Kosmos-2，一个多模态大型语言模型（MLLM），使其能够感知对象描述（例如，边界框）并将文本与视觉世界联系起来。具体来说，我们将引用表达式表示为Markdown中的链接，即“[文本跨度](边界框)”，其中对象描述是位置标记序列。与多模态语料库一起，我们构建了大规模的基于图像文本对的数据（称为GrIT）来训练模型。除了MLLM的现有功能（例如，感知一般模态，遵循指令以及执行上下文学习）之外，Kosmos-2还将接地能力整合到下游应用中。我们在广泛的任务上评估了Kosmos-2，包括（i）多模态接地，例如引用表达理解和短语接地，（ii）多模态引用，例如引用表达生成，（iii）感知语言任务，以及（iv）语言理解和生成。这项工作为实体AI的发展奠定了基础，并为语言、多模态感知、行动和世界建模的大融合提供了启示，这是通往人工通用智能的关键一步。代码和预训练模型可在[https://aka.ms/kosmos-2](https://aka.ms/kosmos-2)上获得。*

![drawing](../Images/3603c868c64eefee06e6ec0e9aa1dd1d.png) KOSMOS-2可以处理的任务概述。摘自[原始论文](https://arxiv.org/abs/2306.14824)。

## 示例

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Kosmos2ForConditionalGeneration

>>> model = Kosmos2ForConditionalGeneration.from_pretrained("microsoft/kosmos-2-patch14-224")
>>> processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")

>>> url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> prompt = "<grounding> An image of"

>>> inputs = processor(text=prompt, images=image, return_tensors="pt")

>>> generated_ids = model.generate(
...     pixel_values=inputs["pixel_values"],
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     image_embeds=None,
...     image_embeds_position_mask=inputs["image_embeds_position_mask"],
...     use_cache=True,
...     max_new_tokens=64,
... )
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)
>>> processed_text
'<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.'

>>> caption, entities = processor.post_process_generation(generated_text)
>>> caption
'An image of a snowman warming himself by a fire.'

>>> entities
[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]
```

这个模型是由[Yih-Dar SHIEH](https://huggingface.co/ydshieh)贡献的。原始代码可以在[这里找到](https://github.com/microsoft/unilm/tree/master/kosmos-2)。

## Kosmos2Config

### `class transformers.Kosmos2Config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/configuration_kosmos2.py#L244)

```py
( text_config = None vision_config = None latent_query_num = 64 **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化`Kosmos2TextConfig`的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化`Kosmos2VisionConfig`的配置选项字典。

+   `latent_query_num` (`int`, *optional*, 默认为64) — 代表文本解码器组件中使用的图像特征的潜在查询标记数量。

+   `kwargs` (*optional*) — 关键字参数字典。

这是一个配置类，用于存储[Kosmos2Model](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2Model)的配置。根据指定的参数实例化KOSMOS-2模型，定义模型架构。使用默认值实例化配置将产生类似于KOSMOS-2 [microsoft/kosmos-2-patch14-224](https://huggingface.co/microsoft/kosmos-2-patch14-224) 架构的配置。

示例：

```py
>>> from transformers import Kosmos2Config, Kosmos2Model

>>> # Initializing a Kosmos-2 kosmos-2-patch14-224 style configuration
>>> configuration = Kosmos2Config()

>>> # Initializing a model (with random weights) from the kosmos-2-patch14-224 style configuration
>>> model = Kosmos2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Kosmos2ImageProcessor

## Kosmos2Processor

### `class transformers.Kosmos2Processor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/processing_kosmos2.py#L38)

```py
( image_processor tokenizer num_patch_index_tokens = 1024 )
```

参数

+   `image_processor` (`CLIPImageProcessor`) — [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)的一个实例。图像处理器是必需的输入。

+   `tokenizer` (`XLMRobertaTokenizerFast`) — [‘XLMRobertaTokenizerFast`]的一个实例。分词器是必需的输入。

+   `num_patch_index_tokens` (`int`, *可选*, 默认为1024) — 代表补丁索引的标记数量。

构建一个KOSMOS-2处理器，将KOSMOS-2图像处理器和KOSMOS-2分词器包装成一个单一处理器。

[Kosmos2Processor](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2Processor)提供了[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)的所有功能，以及[XLMRobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast)的一些功能。查看[**call**()](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2Processor.__call__)和`decode()`的文档字符串以获取更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/processing_kosmos2.py#L105)

```py
( images: Union = None text: Union = None bboxes: Union = None num_image_tokens: Optional = 64 first_image_token_id: Optional = None add_special_tokens: bool = True add_eos_token: bool = False padding: Union = False truncation: Union = None max_length: Optional = None pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

参数

+   `bboxes` (`Union[List[Tuple[int]], List[Tuple[float]], List[List[Tuple[int]]], List[List[Tuple[float]]]`, *可选*) — 与`texts`相关联的边界框。

+   `num_image_tokens` (`int`, 默认为64`) — 用于标记存储图像信息的占位符的(连续)位置数量。这应该与您正在使用的`Kosmos2Config`实例中的`latent_query_num`相同。

+   `first_image_token_id` (`int`, *可选*) — 用于保留存储图像信息的子序列的第一个位置的标记id。如果未设置，将默认为`self.tokenizer.unk_token_id + 1`。

+   `add_eos_token` (`bool`, 默认为`False`) — 当`add_special_tokens=True`时，是否包含`EOS`标记id在编码中。

此方法使用[CLIPImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)方法为模型准备图像，并使用[XLMRobertaTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)为模型准备文本。

请参考上述两种方法的文档字符串以获取更多信息。

本文档的其余部分显示了特定于`Kosmos2Processor`的参数。

## Kosmos2Model

### `class transformers.Kosmos2Model`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/modeling_kosmos2.py#L1735)

```py
( config: Kosmos2Config )
```

参数

+   `config` ([Kosmos2Config](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2Config)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

KOSMOS-2模型用于生成文本和图像特征。该模型由一个视觉编码器和一个语言模型组成。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[什么是注意力掩码？](../glossary#attention-mask)

```py
( pixel_values: Optional = None input_ids: Optional = None image_embeds_position_mask: Optional = None attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None image_embeds: Optional = None inputs_embeds: Optional = None position_ids: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.kosmos2.modeling_kosmos2.Kosmos2ModelOutput or tuple(torch.FloatTensor)
```

可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下会忽略填充。

    0 表示头部被掩盖。

    1 表示头部未被掩盖，

+   `image_embeds` — (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*): 在 `Kosmos2ImageToTextProjection` 输出的隐藏状态序列。

    +   1 表示放置图像特征的位置，

    +   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]` 之间：

    +   1 表示未被掩盖的标记，

    +   0 表示被掩盖的标记。

    [什么是位置 ID？](../glossary#position-ids)

+   [什么是输入 ID？](../glossary#input-ids)

    +   `image_embeds_position_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于指示在序列中插入图像特征的位置的掩码。掩码值选在 `[0, 1]` 之间：

    +   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为 `config.n_layers`，每个元组包含 4 个形状为 `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` 的张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

+   [<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/modeling_kosmos2.py#L1761)

    如果使用了 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（即没有将其过去的键值状态提供给此模型的那些）的形状为 `(batch_size, 1)`，而不是所有 `decoder_input_ids` 的形状为 `(batch_size, sequence_length)`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选在 `[0, 1]` 之间：

    0 表示不适用于图像特征的位置（即文本标记）。

+   `use_cache` (`bool`, *optional*) — 如果设置为 `True`，将返回 `past_key_values` 键值状态，可用于加速解码（请参阅 `past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量中的 `hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

`transformers.models.kosmos2.modeling_kosmos2.Kosmos2ModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.kosmos2.modeling_kosmos2.Kosmos2ModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.kosmos2.configuration_kosmos2.Kosmos2Config'>`）和输入不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`） — 模型最后一层的隐藏状态序列。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `image_embeds`（形状为`(batch_size, latent_query_num, hidden_size)`的`torch.FloatTensor`，*可选*） — `Kosmos2ImageToTextProjection`输出处的隐藏状态序列。

+   `projection_attentions`（`tuple(torch.FloatTensor)`，*可选*） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    由`Kosmos2ImageToTextProjection`给出的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `vision_model_output(BaseModelOutputWithPooling,` *可选*) — `Kosmos2VisionModel`的输出。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中，如果`config.is_encoder_decoder=True`，还可以使用）可用于加速顺序解码。

[Kosmos2Model](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2Model)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Kosmos2Model

>>> model = Kosmos2Model.from_pretrained("microsoft/kosmos-2-patch14-224")
>>> processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")

>>> url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> text = (
...     "<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863>"
...     "</object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911>"
...     "</object>"
... )

>>> inputs = processor(text=text, images=image, return_tensors="pt", add_eos_token=True)

>>> last_hidden_state = model(
...     pixel_values=inputs["pixel_values"],
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     image_embeds_position_mask=inputs["image_embeds_position_mask"],
... ).last_hidden_state
>>> list(last_hidden_state.shape)
[1, 91, 2048]
```

## Kosmos2ForConditionalGeneration

### `class transformers.Kosmos2ForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/modeling_kosmos2.py#L1866)

```py
( config: Kosmos2Config )
```

参数

+   `config` ([Kosmos2Config](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2Config)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

KOSMOS-2模型用于生成文本和边界框，给定一张图片。该模型由视觉编码器和语言模型组成。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/kosmos2/modeling_kosmos2.py#L1901)

```py
( pixel_values: Optional = None input_ids: Optional = None image_embeds_position_mask: Optional = None attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None image_embeds: Optional = None inputs_embeds: Optional = None position_ids: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.kosmos2.modeling_kosmos2.Kosmos2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。详细信息请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    索引可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取。详细信息请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `image_embeds_position_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于指示在序列中插入图像特征的位置的蒙版。蒙版值选择在`[0, 1]`之间：

    +   1表示放置图像特征的位置，

    +   0表示不用于图像特征的位置（即文本标记）。

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的蒙版。蒙版值选择在`[0, 1]`之间：

    +   1表示未被蒙版的标记，

    +   0表示被蒙版的标记。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部失效的蒙版。蒙版值选择在`[0, 1]`之间：

    +   1表示头部未被蒙版，

    +   0表示头部被蒙版。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组包含形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的4个张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用`past_key_values`，用户可以选择仅输入最后一个`decoder_input_ids`（这些没有将其过去的键值状态提供给该模型）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。image_embeds — (`torch.FloatTensor`，形状为`(batch_size, latent_query_num, hidden_size)`，*可选*）：`Kosmos2ImageToTextProjection`输出的隐藏状态序列。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算从左到右的语言建模损失（下一个词预测）的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

返回

`transformers.models.kosmos2.modeling_kosmos2.Kosmos2ForConditionalGenerationModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.kosmos2.modeling_kosmos2.Kosmos2ForConditionalGenerationModelOutput`或一个`torch.FloatTensor`元组（如果传入`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.kosmos2.configuration_kosmos2.Kosmos2Config'>`）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头部的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传入`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传入`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `image_embeds` (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*) — 在`Kosmos2ImageToTextProjection`输出的隐藏状态序列。

+   `projection_attentions` (`tuple(torch.FloatTensor)`, *optional*) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    由`Kosmos2ImageToTextProjection`给出的注意力权重，在注意力softmax后，用于计算自注意力头中的加权平均值。

+   `vision_model_output(BaseModelOutputWithPooling,` *可选*) - `Kosmos2VisionModel`的输出。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，如果`config.is_encoder_decoder=True`还包括交叉注意力块中的键和值），可以用来加速顺序解码（参见`past_key_values`输入）。

[Kosmos2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Kosmos2ForConditionalGeneration

>>> model = Kosmos2ForConditionalGeneration.from_pretrained("microsoft/kosmos-2-patch14-224")
>>> processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")

>>> url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> prompt = "<grounding> An image of"

>>> inputs = processor(text=prompt, images=image, return_tensors="pt")

>>> generated_ids = model.generate(
...     pixel_values=inputs["pixel_values"],
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     image_embeds=None,
...     image_embeds_position_mask=inputs["image_embeds_position_mask"],
...     use_cache=True,
...     max_new_tokens=64,
... )
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)
>>> processed_text
'<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.'

>>> caption, entities = processor.post_process_generation(generated_text)
>>> caption
'An image of a snowman warming himself by a fire.'

>>> entities
[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]
```
