- en: FlauBERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FlauBERT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flaubert](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flaubert)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flaubert](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flaubert)
- en: '[![Models](../Images/439b6a26627a4eebe5deef3d06882621.png)](https://huggingface.co/models?filter=flaubert)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/flaubert_small_cased)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![模型](../Images/439b6a26627a4eebe5deef3d06882621.png)](https://huggingface.co/models?filter=flaubert)
    [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/flaubert_small_cased)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The FlauBERT model was proposed in the paper [FlauBERT: Unsupervised Language
    Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le et
    al. It’s a transformer model pretrained using a masked language modeling (MLM)
    objective (like BERT).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: FlauBERT模型是由Hang Le等人在论文[FlauBERT:法语无监督语言模型预训练](https://arxiv.org/abs/1912.05372)中提出的。它是使用掩码语言建模（MLM）目标（如BERT）预训练的Transformer模型。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Language models have become a key step to achieve state-of-the art results
    in many different Natural Language Processing (NLP) tasks. Leveraging the huge
    amount of unlabeled texts nowadays available, they provide an efficient way to
    pre-train continuous word representations that can be fine-tuned for a downstream
    task, along with their contextualization at the sentence level. This has been
    widely demonstrated for English using contextualized representations (Dai and
    Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin
    et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT,
    a model learned on a very large and heterogeneous French corpus. Models of different
    sizes are trained using the new CNRS (French National Centre for Scientific Research)
    Jean Zay supercomputer. We apply our French language models to diverse NLP tasks
    (text classification, paraphrasing, natural language inference, parsing, word
    sense disambiguation) and show that most of the time they outperform other pretraining
    approaches. Different versions of FlauBERT as well as a unified evaluation protocol
    for the downstream tasks, called FLUE (French Language Understanding Evaluation),
    are shared to the research community for further reproducible experiments in French
    NLP.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*语言模型已成为在许多不同自然语言处理（NLP）任务中实现最先进结果的关键步骤。利用当今大量可用的未标记文本，它们提供了一种有效的方式来预训练连续的词表示，可以在下游任务中进行微调，以及在句子级别上对其进行上下文化。这已经在英语中广泛证明了使用上下文化表示（Dai和Le，2015;
    Peters等，2018; Howard和Ruder，2018; Radford等，2018; Devlin等，2019; Yang等，2019b）。在本文中，我们介绍并分享FlauBERT，这是一个在非常大型和异构的法语语料库上学习的模型。使用新的法国国家科学研究中心（CNRS）Jean
    Zay超级计算机训练不同大小的模型。我们将我们的法语语言模型应用于各种NLP任务（文本分类、释义、自然语言推理、解析、词义消歧），并展示大多数时候它们优于其他预训练方法。FlauBERT的不同版本以及用于下游任务的统一评估协议，称为FLUE（法语语言理解评估），已经共享给研究社区，以进行进一步可重现的法语NLP实验。*'
- en: This model was contributed by [formiel](https://huggingface.co/formiel). The
    original code can be found [here](https://github.com/getalp/Flaubert).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[formiel](https://huggingface.co/formiel)贡献。原始代码可以在[这里](https://github.com/getalp/Flaubert)找到。
- en: 'Tips:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: Like RoBERTa, without the sentence ordering prediction (so just trained on the
    MLM objective).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与RoBERTa一样，没有句子排序预测（只是在MLM目标上训练）。
- en: Resources
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标记分类任务指南](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掩码语言建模任务指南](../tasks/masked_language_modeling)'
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多选任务指南](../tasks/multiple_choice)'
- en: FlaubertConfig
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertConfig
- en: '### `class transformers.FlaubertConfig`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/configuration_flaubert.py#L34)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/configuration_flaubert.py#L34)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pre_norm` (`bool`, *optional*, defaults to `False`) — Whether to apply the
    layer normalization before or after the feed forward layer following the attention
    in each layer (Vaswani et al., Tensor2Tensor for Neural Machine Translation. 2018)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_norm`（`bool`，*可选*，默认为`False`）— 是否在每层的注意力后应用层归一化之前还是之后应用前馈层（Vaswani等人，Tensor2Tensor用于神经机器翻译。2018）'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.0) — Probability to drop layers
    during training (Fan et al., Reducing Transformer Depth on Demand with Structured
    Dropout. ICLR 2020)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop`（`float`，*可选*，默认为0.0）— 训练期间丢弃层的概率（Fan等人，使用结构化丢弃减少Transformer深度的需求。ICLR
    2020）'
- en: '`vocab_size` (`int`, *optional*, defaults to 30145) — Vocabulary size of the
    FlauBERT model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [FlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertModel)
    or [TFFlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertModel).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为30145）— FlauBERT模型的词汇量。定义了在调用[FlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertModel)或[TFFlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertModel)时可以表示的不同令牌数量。'
- en: '`emb_dim` (`int`, *optional*, defaults to 2048) — Dimensionality of the encoder
    layers and the pooler layer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_dim`（`int`，*可选*，默认为2048）— 编码器层和池化层的维度。'
- en: '`n_layer` (`int`, *optional*, defaults to 12) — Number of hidden layers in
    the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer`（`int`，*可选*，默认为12）— Transformer编码器中的隐藏层数量。'
- en: '`n_head` (`int`, *optional*, defaults to 16) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head`（`int`，*可选*，默认为16）— Transformer编码器中每个注意力层的注意力头数。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`（`float`，*可选*，默认为0.1）— 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention mechanism'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout`（`float`，*可选*，默认为0.1）— 注意机制的丢弃概率。'
- en: '`gelu_activation` (`bool`, *optional*, defaults to `True`) — Whether or not
    to use a *gelu* activation instead of *relu*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gelu_activation`（`bool`，*可选*，默认为`True`）— 是否使用*gelu*激活而不是*relu*。'
- en: '`sinusoidal_embeddings` (`bool`, *optional*, defaults to `False`) — Whether
    or not to use sinusoidal positional embeddings instead of absolute positional
    embeddings.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sinusoidal_embeddings`（`bool`，*可选*，默认为`False`）— 是否使用正弦位置嵌入而不是绝对位置嵌入。'
- en: '`causal` (`bool`, *optional*, defaults to `False`) — Whether or not the model
    should behave in a causal manner. Causal models use a triangular attention mask
    in order to only attend to the left-side context instead if a bidirectional context.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`causal`（`bool`，*可选*，默认为`False`) — 模型是否应该以因果方式行为。因果模型使用三角形注意掩码，以便只关注左侧上下文而不是双向上下文。'
- en: '`asm` (`bool`, *optional*, defaults to `False`) — Whether or not to use an
    adaptive log softmax projection layer instead of a linear layer for the prediction
    layer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`asm`（`bool`，*可选*，默认为`False`）— 是否使用自适应对数softmax投影层而不是线性层进行预测。'
- en: '`n_langs` (`int`, *optional*, defaults to 1) — The number of languages the
    model handles. Set to 1 for monolingual models.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_langs`（`int`，*可选*，默认为1）— 模型处理的语言数量。对于单语模型，设置为1。'
- en: '`use_lang_emb` (`bool`, *optional*, defaults to `True`) — Whether to use language
    embeddings. Some models use additional language embeddings, see [the multilingual
    models page](http://huggingface.co/transformers/multilingual.html#xlm-language-embeddings)
    for information on how to use them.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_lang_emb`（`bool`，*可选*，默认为`True`）— 是否使用语言嵌入。一些模型使用额外的语言嵌入，请参阅[多语言模型页面](http://huggingface.co/transformers/multilingual.html#xlm-language-embeddings)以获取有关如何使用它们的信息。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings`（`int`，*可选*，默认为512）— 该模型可能会使用的最大序列长度。通常设置为一个较大的值以防万一（例如512或1024或2048）。'
- en: '`embed_init_std` (`float`, *optional*, defaults to 2048^-0.5) — The standard
    deviation of the truncated_normal_initializer for initializing the embedding matrices.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_init_std`（`float`，*可选*，默认为2048^-0.5）— 用于初始化嵌入矩阵的截断正态初始化器的标准差。'
- en: '`init_std` (`int`, *optional*, defaults to 50257) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices except
    the embedding matrices.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std`（`int`，*可选*，默认为50257）— 用于初始化除嵌入矩阵之外的所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps`（`float`，*可选*，默认为1e-12）— 层归一化层使用的epsilon。'
- en: '`bos_index` (`int`, *optional*, defaults to 0) — The index of the beginning
    of sentence token in the vocabulary.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_index`（`int`，*可选*，默认为0）— 词汇表中句子开头标记的索引。'
- en: '`eos_index` (`int`, *optional*, defaults to 1) — The index of the end of sentence
    token in the vocabulary.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_index`（`int`，*可选*，默认为1）— 词汇表中句子结束标记的索引。'
- en: '`pad_index` (`int`, *optional*, defaults to 2) — The index of the padding token
    in the vocabulary.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_index`（`int`，*可选*，默认为2）— 词汇表中填充标记的索引。'
- en: '`unk_index` (`int`, *optional*, defaults to 3) — The index of the unknown token
    in the vocabulary.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_index`（`int`，*可选*，默认为3）— 词汇表中未知标记的索引。'
- en: '`mask_index` (`int`, *optional*, defaults to 5) — The index of the masking
    token in the vocabulary.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_index`（`int`，*可选*，默认为5）— 词汇表中掩码标记的索引。'
- en: '`is_encoder(bool,` *optional*, defaults to `True`) — Whether or not the initialized
    model should be a transformer encoder or decoder as seen in Vaswani et al.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder(bool,` *可选*，默认为`True`) — 初始化模型是否应该是Transformer编码器或解码器，如Vaswani等人所见。'
- en: '`summary_type` (`string`, *optional*, defaults to “first”) — Argument used
    when doing sequence summary. Used in the sequence classification and multiple
    choice models.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type`（`string`，*可选*，默认为“first”）— 在进行序列摘要时使用的参数。用于序列分类和多选模型。'
- en: 'Has to be one of the following options:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 必须是以下选项之一：
- en: '`"last"`: Take the last token hidden state (like XLNet).'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"last"`: 获取最后一个标记的隐藏状态（类似XLNet）。'
- en: '`"first"`: Take the first token hidden state (like BERT).'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"first"`: 获取第一个标记的隐藏状态（类似BERT）。'
- en: '`"mean"`: Take the mean of all tokens hidden states.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mean"`: 获取所有标记的隐藏状态的平均值。'
- en: '`"cls_index"`: Supply a Tensor of classification token position (like GPT/GPT-2).'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cls_index"`: 提供一个分类标记位置的张量（类似GPT/GPT-2）。'
- en: '`"attn"`: Not implemented now, use multi-head attention.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"attn"`: 目前未实现，使用多头注意力。'
- en: '`summary_use_proj` (`bool`, *optional*, defaults to `True`) — Argument used
    when doing sequence summary. Used in the sequence classification and multiple
    choice models.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_use_proj`（`bool`，*可选*，默认为`True`) — 在进行序列摘要时使用的参数。用于序列分类和多选模型。'
- en: Whether or not to add a projection after the vector extraction.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否在向量提取后添加投影。
- en: '`summary_activation` (`str`, *optional*) — Argument used when doing sequence
    summary. Used in the sequence classification and multiple choice models.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_activation`（`str`，*可选*）— 在进行序列摘要时使用的参数。用于序列分类和多选模型。'
- en: Pass `"tanh"` for a tanh activation to the output, any other value will result
    in no activation.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将输出传递给“tanh”以获得tanh激活，任何其他值将导致无激活。
- en: '`summary_proj_to_labels` (`bool`, *optional*, defaults to `True`) — Used in
    the sequence classification and multiple choice models.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_proj_to_labels`（`bool`，*可选*，默认为`True`）— 用于序列分类和多选模型。'
- en: Whether the projection outputs should have `config.num_labels` or `config.hidden_size`
    classes.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 投影输出应具有`config.num_labels`或`config.hidden_size`类。
- en: '`summary_first_dropout` (`float`, *optional*, defaults to 0.1) — Used in the
    sequence classification and multiple choice models.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_first_dropout` (`float`, *optional*, 默认为0.1) — 用于序列分类和多选模型。'
- en: The dropout ratio to be used after the projection and activation.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在投影和激活后使用的丢弃比率。
- en: '`start_n_top` (`int`, *optional*, defaults to 5) — Used in the SQuAD evaluation
    script.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_n_top` (`int`, *optional*, 默认为5) — 用于SQuAD评估脚本。'
- en: '`end_n_top` (`int`, *optional*, defaults to 5) — Used in the SQuAD evaluation
    script.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_n_top` (`int`, *optional*, 默认为5) — 用于SQuAD评估脚本。'
- en: '`mask_token_id` (`int`, *optional*, defaults to 0) — Model agnostic parameter
    to identify masked tokens when generating text in an MLM context.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token_id` (`int`, *optional*, 默认为0) — 用于在MLM上下文中生成文本时识别掩码标记的与模型无关的参数。'
- en: '`lang_id` (`int`, *optional*, defaults to 1) — The ID of the language used
    by the model. This parameter is used when generating text in a given language.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lang_id` (`int`, *optional*, 默认为1) — 模型使用的语言ID。在生成特定语言的文本时使用此参数。'
- en: This is the configuration class to store the configuration of a [FlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertModel)
    or a [TFFlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertModel).
    It is used to instantiate a FlauBERT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the FlauBERT [flaubert/flaubert_base_uncased](https://huggingface.co/flaubert/flaubert_base_uncased)
    architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 [FlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertModel)
    或 [TFFlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertModel)
    配置的配置类。它用于根据指定的参数实例化一个 FlauBERT 模型，定义模型架构。使用默认值实例化配置将产生类似于 FlauBERT [flaubert/flaubert_base_uncased](https://huggingface.co/flaubert/flaubert_base_uncased)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: FlaubertTokenizer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertTokenizer
- en: '### `class transformers.FlaubertTokenizer`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L165)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L165)'
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Vocabulary file.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件。'
- en: '`merges_file` (`str`) — Merges file.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件。'
- en: '`do_lowercase` (`bool`, *optional*, defaults to `False`) — Controls lower casing.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lowercase` (`bool`, *optional*, 默认为 `False`) — 控制小写。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是 `cls_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, 默认为 `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于问答的文本和问题。还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`cls_token` (`str`, *optional*, defaults to `"</s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, 默认为 `"</s>"`) — 用于进行序列分类时使用的分类器标记（对整个序列进行分类而不是每个标记进行分类）。在使用特殊标记构建时，它是序列的第一个标记。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<special1>"`) — The token used
    for masking values. This is the token used when training this model with masked
    language modeling. This is the token which the model will try to predict.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, 默认为 `"<special1>"`) — 用于掩码值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `[''<special0>'',
    ''<special1>'', ''<special2>'', ''<special3>'', ''<special4>'', ''<special5>'',
    ''<special6>'', ''<special7>'', ''<special8>'', ''<special9>'']`) — List of additional
    special tokens.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *optional*, 默认为 `[''<special0>'',
    ''<special1>'', ''<special2>'', ''<special3>'', ''<special4>'', ''<special5>'',
    ''<special6>'', ''<special7>'', ''<special8>'', ''<special9>'']`) — 附加特殊标记的列表。'
- en: '`lang2id` (`Dict[str, int]`, *optional*) — Dictionary mapping languages string
    identifiers to their IDs.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lang2id` (`Dict[str, int]`, *optional*) — 将语言字符串标识符映射到其ID的字典。'
- en: '`id2lang` (`Dict[int, str]`, *optional*) — Dictionary mapping language IDs
    to their string identifiers.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id2lang` (`Dict[int, str]`, *optional*) — 将语言ID映射到其字符串标识符的字典。'
- en: 'Construct a Flaubert tokenizer. Based on Byte-Pair Encoding. The tokenization
    process is the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Flaubert 分词器。基于字节对编码。分词过程如下：
- en: Moses preprocessing and tokenization.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摩西预处理和标记化。
- en: Normalizing all inputs text.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化所有输入文本。
- en: The arguments `special_tokens` and the function `set_special_tokens`, can be
    used to add additional symbols (like ”`classify`”) to a vocabulary.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数`special_tokens`和函数`set_special_tokens`可用于向词汇表添加额外的符号（如“`classify`”）。
- en: The argument `do_lowercase` controls lower casing (automatically set for pretrained
    vocabularies).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数`do_lowercase`控制小写（对于预训练词汇表会自动设置）。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此标记器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L476)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L476)'
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`，*可选*) — 第二个序列对应的ID列表。'
- en: Returns
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLM sequence has the following
    format:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，为序列分类任务从序列或序列对构建模型输入。一个XLM序列的格式如下：
- en: 'single sequence: `<s> X </s>`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`<s> X </s>`
- en: 'pair of sequences: `<s> A </s> B </s>`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`<s> A </s> B </s>`
- en: '#### `convert_tokens_to_string`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L470)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L470)'
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Converts a sequence of tokens (string) in a single string.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L533)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L533)'
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`，*可选*) — 第二个序列对应的ID列表。'
- en: Returns
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定的序列，列出[token类型ID](../glossary#token-type-ids)的列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. An XLM sequence
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。一个XLM序列
- en: 'pair mask has the following format:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 配对掩码的格式如下：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。
- en: '#### `get_special_tokens_mask`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L504)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/tokenization_flaubert.py#L504)'
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`，*可选*) — 第二个序列对应的ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`，*可选*，默认为`False`) — 标记列表是否已经使用特殊标记格式化为模型。'
- en: Returns
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：特殊标记为1，序列标记为0。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用标记器的`prepare_for_model`方法添加特殊标记时调用此方法。
- en: PytorchHide Pytorch content
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: FlaubertModel
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertModel
- en: '### `class transformers.FlaubertModel`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L380)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L380)'
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `forward`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L473)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L473)'
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的蒙版。蒙版值选择在 `[0, 1]` 中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被 `掩盖` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被 `掩盖` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[注意力蒙版是什么？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 段标记索引，用于指示输入的第一部分和第二部分。索引选择在 `[0, 1]` 中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[令牌类型 ID 是什么？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[位置 ID 是什么？](../glossary#position-ids)'
- en: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Length
    of each sentence that can be used to avoid performing attention on padding token
    indices. You can also use `attention_mask` for the same result (see above), kept
    here for compatibility. Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用
    `attention_mask` 来获得相同的结果（请参见上文），这里保留是为了兼容性。索引选择在 `[0, ..., input_ids.size(-1)]`：'
- en: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — Dictionary strings to
    `torch.FloatTensor` that contains precomputed hidden-states (key and values in
    the attention blocks) as computed by the model (see `cache` output below). Can
    be used to speed up sequential decoding. The dictionary object will be modified
    in-place during the forward pass to add newly computed hidden-states.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, torch.FloatTensor]`，*可选*) — 包含预计算的隐藏状态（由模型计算的注意力块中的键和值）的字符串到
    `torch.FloatTensor` 的字典。可用于加速顺序解码。在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块的选定头部失效的蒙版。蒙版值选择在 `[0, 1]` 中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `掩盖`，
- en: 0 indicates the head is `masked`.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `掩盖`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，您可以直接传递一个嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个
    `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含各种元素，具体取决于配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层的输出，则为嵌入的输出 + 每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—形状为`(batch_size,
    num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [FlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertModel)
    forward method, overrides the `__call__` special method.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: FlaubertWithLMHeadModel
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertWithLMHeadModel
- en: '### `class transformers.FlaubertWithLMHeadModel`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertWithLMHeadModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L645)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L645)'
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）—模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Flaubert Model transformer with a language modeling head on top (linear
    layer with weights tied to the input embeddings).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Flaubert模型变压器，顶部带有语言建模头（线性层，权重与输入嵌入绑定）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L683)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L683)'
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）—词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[输入ID是什么？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*）—避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0,
    1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“掩盖”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩盖”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[注意力掩码是什么？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—段标记索引，指示输入的第一部分和第二部分。索引选择在`[0,
    1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[令牌类型ID是什么？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—输入序列标记的每个位置的位置嵌入的索引。在范围`[0,
    config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[位置ID是什么？](../glossary#position-ids)'
- en: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Length
    of each sentence that can be used to avoid performing attention on padding token
    indices. You can also use `attention_mask` for the same result (see above), kept
    here for compatibility. Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 每个句子的长度，可用于避免在填充令牌索引上执行注意力。您也可以使用`attention_mask`获得相同的结果（请参见上文），这里保留以保持兼容性。在`[0,
    ..., input_ids.size(-1)]`中选择的索引：'
- en: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — Dictionary strings to
    `torch.FloatTensor` that contains precomputed hidden-states (key and values in
    the attention blocks) as computed by the model (see `cache` output below). Can
    be used to speed up sequential decoding. The dictionary object will be modified
    in-place during the forward pass to add newly computed hidden-states.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, torch.FloatTensor]`，*optional*) — 包含预先计算的隐藏状态（由模型计算的注意力块中的键和值）的字符串到`torch.FloatTensor`的字典。可用于加速顺序解码。在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩码。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于语言建模的标签。请注意，模型内部会对标签进行**偏移**，即您可以设置`labels = input_ids`。在`[-100, 0, ..., config.vocab_size]`中选择索引。所有设置为`-100`的标签将被忽略（掩码），损失仅计算在`[0,
    ..., config.vocab_size]`中的标签。'
- en: Returns
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出
    + 每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层输出的模型的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [FlaubertWithLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertWithLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel)
    的前向方法重写了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: FlaubertForSequenceClassification
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertForSequenceClassification
- en: '### `class transformers.FlaubertForSequenceClassification`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L743)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L743)'
- en: '[PRE13]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: Flaubert Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部有一个用于序列分类/回归的 Flaubert 模型（在汇总输出的顶部有一个线性层），例如用于 GLUE 任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L763)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L763)'
- en: '[PRE14]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`） — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被掩码的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被掩码的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 段标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于一个 *句子 A* 的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于一个 *句子 B* 的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Length
    of each sentence that can be used to avoid performing attention on padding token
    indices. You can also use `attention_mask` for the same result (see above), kept
    here for compatibility. Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用
    `attention_mask` 获得相同的结果（见上文），这里保留是为了兼容性。索引在 `[0, ..., input_ids.size(-1)]` 中选择：'
- en: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — Dictionary strings to
    `torch.FloatTensor` that contains precomputed hidden-states (key and values in
    the attention blocks) as computed by the model (see `cache` output below). Can
    be used to speed up sequential decoding. The dictionary object will be modified
    in-place during the forward pass to add newly computed hidden-states.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache`（`Dict[str, torch.FloatTensor]`，*可选*）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值）的字符串到`torch.FloatTensor`的字典对象（请参见下面的`cache`输出）。可用于加速顺序解码。在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩盖。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）- 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则包括根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入而异的各种元素。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 分类（或如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（或如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（用于嵌入层的输出和每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [FlaubertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类的示例：
- en: '[PRE15]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Example of multi-label classification:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类的示例：
- en: '[PRE16]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: FlaubertForMultipleChoice
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertForMultipleChoice
- en: '### `class transformers.FlaubertForMultipleChoice`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertForMultipleChoice`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1202)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1202)'
- en: '[PRE17]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）—
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Flaubert Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Flaubert模型在顶部具有多选分类头（池化输出上的线性层和softmax），例如用于RocStories/SWAG任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1221)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1221)'
- en: '[PRE18]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被掩盖的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被掩盖的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`之间。'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Length
    of each sentence that can be used to avoid performing attention on padding token
    indices. You can also use `attention_mask` for the same result (see above), kept
    here for compatibility. Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用`attention_mask`获得相同的结果（见上文），这里保留以保持兼容性。索引选择在`[0,
    ..., input_ids.size(-1)]`之间：'
- en: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — Dictionary strings to
    `torch.FloatTensor` that contains precomputed hidden-states (key and values in
    the attention blocks) as computed by the model (see `cache` output below). Can
    be used to speed up sequential decoding. The dictionary object will be modified
    in-place during the forward pass to add newly computed hidden-states.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache`（`Dict[str, torch.FloatTensor]`，*可选*）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值）的字符串到`torch.FloatTensor`的字典。可用于加速顺序解码。在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices-1]` where `num_choices` is the size of the second dimension of
    the input tensors. (See `input_ids` above)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算多项选择分类损失的标签。索引应在`[0,
    ..., num_choices-1]`范围内，其中`num_choices`是输入张量的第二维的大小。（参见上面的`input_ids`）'
- en: Returns
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.MultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.MultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.MultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput)或者一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者`config.return_dict=False`时）包含不同的元素，取决于配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为*(1,)*的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`）— *num_choices*是输入张量的第二维。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类得分（SoftMax之前）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [FlaubertForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: FlaubertForTokenClassification
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertForTokenClassification
- en: '### `class transformers.FlaubertForTokenClassification`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L846)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L846)'
- en: '[PRE20]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Flaubert Model with a token classification head on top (a linear layer on top
    of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Flaubert模型，顶部带有一个标记分类头（隐藏状态输出的顶部线性层），例如用于命名实体识别（NER）任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L866)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L866)'
- en: '[PRE21]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Length
    of each sentence that can be used to avoid performing attention on padding token
    indices. You can also use `attention_mask` for the same result (see above), kept
    here for compatibility. Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用`attention_mask`获得相同的结果（见上文），这里保留以保持兼容性。索引选择在`[0,
    ..., input_ids.size(-1)]`中：'
- en: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — Dictionary strings to
    `torch.FloatTensor` that contains precomputed hidden-states (key and values in
    the attention blocks) as computed by the model (see `cache` output below). Can
    be used to speed up sequential decoding. The dictionary object will be modified
    in-place during the forward pass to add newly computed hidden-states.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache`（`Dict[str, torch.FloatTensor]`，*可选*）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值）的`torch.FloatTensor`字符串字典（请参见下面的`cache`输出）。可用于加速顺序解码。在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 indicates the head is `not masked`,
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部“未屏蔽”，
- en: 0 indicates the head is `masked`.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部“已屏蔽”。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为关联向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算标记分类损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`中。'
- en: Returns
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）—
    分类分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及可选的初始嵌入输出的隐藏状态。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [FlaubertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是调用此函数，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: FlaubertForQuestionAnsweringSimple
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertForQuestionAnsweringSimple
- en: '### `class transformers.FlaubertForQuestionAnsweringSimple`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertForQuestionAnsweringSimple`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L931)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L931)'
- en: '[PRE23]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: Flaubert Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: Flaubert 模型在顶部具有一个用于提取式问答任务（如 SQuAD）的跨度分类头（在隐藏状态输出的顶部有一个线性层，用于计算 `span start
    logits` 和 `span end logits`）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L949)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L949)'
- en: '[PRE24]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]`:'
- en: 1 for tokens that are `not masked`,
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被遮蔽的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被遮蔽的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引选择在 `[0, 1]`:'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings -
    1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Length
    of each sentence that can be used to avoid performing attention on padding token
    indices. You can also use `attention_mask` for the same result (see above), kept
    here for compatibility. Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用
    `attention_mask` 来获得相同的结果（见上文），这里保留是为了兼容性。所选索引范围为 `[0, ..., input_ids.size(-1)]`:'
- en: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — Dictionary strings to
    `torch.FloatTensor` that contains precomputed hidden-states (key and values in
    the attention blocks) as computed by the model (see `cache` output below). Can
    be used to speed up sequential decoding. The dictionary object will be modified
    in-place during the forward pass to add newly computed hidden-states.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, torch.FloatTensor]`, *optional*) — 包含由模型计算的预计算隐藏状态（关键和值在注意力块中）的
    `torch.FloatTensor` 字典字符串（请参见下面的 `cache` 输出）。可用于加速顺序解码。在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在 `[0, 1]`:'
- en: 1 indicates the head is `not masked`,
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被遮蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算标记跨度起始位置的位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算标记跨度结束位置的位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内计算损失。'
- en: Returns
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 总跨度提取损失是起始位置和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 跨度起始分数（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 跨度结束分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [FlaubertForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple)
    forward method, overrides the `__call__` special method.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: FlaubertForQuestionAnswering
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaubertForQuestionAnswering
- en: '### `class transformers.FlaubertForQuestionAnswering`'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaubertForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1088)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1088)'
- en: '[PRE26]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#### `forward`'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1098)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_flaubert.py#L1098)'
- en: '[PRE27]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Returns
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)):
    Model configuration class with all the parameters of the model. Initializing with
    a config file does not load the weights associated with the model, only the configuration.
    Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))：模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看
    [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法来加载模型权重。'
- en: The [FlaubertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaubertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: Base class for outputs of question answering models using a `SquadHead`.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`SquadHead`的问答模型输出的基类。
- en: 'Example:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 例子：
- en: '[PRE28]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: TensorFlowHide TensorFlow content
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: TFFlaubertModel
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFFlaubertModel
- en: '### `class transformers.TFFlaubertModel`'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFFlaubertModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L243)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L243)'
- en: '[PRE29]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法来加载模型权重。'
- en: The bare Flaubert Model transformer outputting raw hidden-states without any
    specific head on top.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Flaubert模型变压器，输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个 [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入都作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入都作为第一个位置参数的列表、元组或字典。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，Keras 方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用 `model.fit()` 等方法时，您应该可以“只需工作”
    - 只需以 `model.fit()` 支持的任何格式传递您的输入和标签！但是，如果您想在 Keras 方法之外使用第二种格式，例如在使用 Keras `Functional`
    API 创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含 `input_ids` 的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度不同的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])` 或
    `model([input_ids, attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个包含与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些内容，因为您可以像对待任何其他
    Python 函数一样传递输入！
- en: '#### `call`'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L252)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L252)'
- en: '[PRE30]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`Numpy array` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`)
    — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。 有关详细信息，请参阅 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    和 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`Numpy array` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。 选择在 `[0, 1]` 中的掩码值：'
- en: '`1` for tokens that are `not masked`,'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被屏蔽的标记，为 `1`，
- en: '`0` for tokens that are `masked`.'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被屏蔽的标记，为 `0`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — A parallel sequence of tokens to be used to indicate the language
    of each token in the input. Indices are languages ids which can be obtained from
    the language names by using two conversion mappings provided in the configuration
    of the model (only provided for multilingual models). More precisely, the *language
    name to language id* mapping is in `model.config.lang2id` (which is a dictionary
    string to int) and the *language id to language name* mapping is in `model.config.id2lang`
    (dictionary int to string).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langs` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于指示输入中每个标记的语言的并行标记序列。 索引是语言 ID，可以通过模型配置中提供的两个转换映射从语言名称中获取（仅适用于多语言模型）。 更确切地说，*语言名称到语言
    ID* 映射在 `model.config.lang2id` 中（这是一个字符串到整数的字典），*语言 ID 到语言名称* 映射在 `model.config.id2lang`
    中（整数到字符串的字典）。'
- en: See usage examples detailed in the [multilingual documentation](../multilingual).
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅[多语言文档](../multilingual)中详细的使用示例。
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 段标记索引，指示输入的第一部分和第二部分。 索引在 `[0, 1]` 中选择：'
- en: '`0` corresponds to a *sentence A* token,'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 对应于 *句子 A* 标记，'
- en: '`1` corresponds to a *sentence B* token.'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 对应于 *句子 B* 标记。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 每个输入序列标记在位置嵌入中的位置的索引。 选择在范围 `[0, config.max_position_embeddings - 1]` 中。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — Length of each sentence that can be used to avoid performing attention on padding
    token indices. You can also use *attention_mask* for the same result (see above),
    kept here for compatibility Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`tf.Tensor`或`Numpy array`，形状为`(batch_size,)`，*可选*) — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用*attention_mask*获得相同的结果（见上文），这里保留以保持兼容性。所选索引在`[0,
    ..., input_ids.size(-1)]`中：'
- en: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — Dictionary string to `tf.FloatTensor`
    that contains precomputed hidden states (key and values in the attention blocks)
    as computed by the model (see `cache` output below). Can be used to speed up sequential
    decoding.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, tf.Tensor]`，*可选*) — 包含由模型计算的预计算隐藏状态（注意力块中的键和值）的字符串到`tf.FloatTensor`的字典。可用于加速顺序解码。'
- en: The dictionary object will be modified in-place during the forward pass to add
    newly computed hidden-states.
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字典对象将在前向传递期间就地修改以添加新计算的隐藏状态。
- en: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`Numpy array`或形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*可选*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: '`1` indicates the head is `not masked`,'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`表示头部未被`掩码`，'
- en: '`0` indicates the head is `masked`.'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`表示头部被`掩码`。'
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert `input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*)
    — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`，*可选*，默认为`False`) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`)
    — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFFlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertModel)
    forward method, overrides the `__call__` special method.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFFlaubertModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE31]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: TFFlaubertWithLMHeadModel
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFFlaubertWithLMHeadModel
- en: '### `class transformers.TFFlaubertWithLMHeadModel`'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFFlaubertWithLMHeadModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L819)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L819)'
- en: '[PRE32]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）—模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Flaubert Model transformer with a language modeling head on top (linear
    layer with weights tied to the input embeddings).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: Flaubert模型变压器，顶部带有语言建模头（线性层，其权重与输入嵌入绑定）。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典的第一个位置参数。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，在使用`model.fit()`等方法时，应该“只需工作” -
    只需传递您的输入和标签，以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个张量，其中只有`input_ids`而没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个与文档字符串中给定的顺序相对应的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L855)'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L855)'
- en: '[PRE33]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`）—词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入ID是什么？
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`范围内：'
- en: '`1` for tokens that are `not masked`,'
  id: totrans-526
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`表示未被屏蔽的标记，'
- en: '`0` for tokens that are `masked`.'
  id: totrans-527
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`表示被屏蔽的标记。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意掩码是什么？
- en: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — A parallel sequence of tokens to be used to indicate the language
    of each token in the input. Indices are languages ids which can be obtained from
    the language names by using two conversion mappings provided in the configuration
    of the model (only provided for multilingual models). More precisely, the *language
    name to language id* mapping is in `model.config.lang2id` (which is a dictionary
    string to int) and the *language id to language name* mapping is in `model.config.id2lang`
    (dictionary int to string).'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langs`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）— 用于指示输入中每个标记的语言的并行标记序列。语言ID可以通过模型配置中提供的两个转换映射从语言名称中获取（仅适用于多语言模型）。更准确地说，*语言名称到语言ID*映射在`model.config.lang2id`中（这是一个字符串到整数的字典），*语言ID到语言名称*映射在`model.config.id2lang`中（整数到字符串的字典）。'
- en: See usage examples detailed in the [multilingual documentation](../multilingual).
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在[多语言文档](../multilingual)中详细说明了用法示例。
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）—
    指示输入的第一部分和第二部分的段标记索引。索引选定在`[0, 1]`范围内：'
- en: '`0` corresponds to a *sentence A* token,'
  id: totrans-532
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`对应于*句子A*的标记，'
- en: '`1` corresponds to a *sentence B* token.'
  id: totrans-533
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`对应于*句子B*的标记。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令牌类型ID是什么？
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选定范围在`[0, config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置ID是什么？
- en: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — Length of each sentence that can be used to avoid performing attention on padding
    token indices. You can also use *attention_mask* for the same result (see above),
    kept here for compatibility Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths`（形状为`(batch_size,)`的`tf.Tensor`或`Numpy`数组，*可选*）— 每个句子的长度，可用于避免在填充标记索引上执行注意力。您还可以使用*attention_mask*获得相同的结果（请参见上文），这里保留以确保兼容性。选定的索引在`[0,
    ..., input_ids.size(-1)]`内：'
- en: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — Dictionary string to `tf.FloatTensor`
    that contains precomputed hidden states (key and values in the attention blocks)
    as computed by the model (see `cache` output below). Can be used to speed up sequential
    decoding.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache`（`Dict[str, tf.Tensor]`，*可选*）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值）的字符串到`tf.FloatTensor`的字典。可用于加速顺序解码。'
- en: The dictionary object will be modified in-place during the forward pass to add
    newly computed hidden-states.
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。
- en: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`Numpy`数组或`tf.Tensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：'
- en: '`1` indicates the head is `not masked`,'
  id: totrans-541
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`表示头部未被屏蔽，'
- en: '`0` indicates the head is `masked`.'
  id: totrans-542
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`表示被屏蔽的头部。'
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert `input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更好地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`）- 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModelOutput`
    or `tuple(tf.Tensor)`'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModelOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModelOutput`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModelOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）-
    语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFFlaubertWithLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFFlaubertWithLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE34]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: TFFlaubertForSequenceClassification
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFFlaubertForSequenceClassification
- en: '### `class transformers.TFFlaubertForSequenceClassification`'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFFlaubertForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L915)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L915)'
- en: '[PRE35]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）-
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Flaubert Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: Flaubert模型在顶部具有序列分类/回归头（在池化输出的顶部有一个线性层），例如用于GLUE任务。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有事项。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，您应该可以“轻松”地工作
    - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个按照文档字符串中给定的顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些问题，因为您可以像将输入传递给任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L931)'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L931)'
- en: '[PRE36]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`Numpy数组`或`tf.Tensor`） - 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入ID是什么？
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`Numpy数组`或`tf.Tensor`，*可选*）
    - 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: '`1` for tokens that are `not masked`,'
  id: totrans-585
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为`1`，
- en: '`0` for tokens that are `masked`.'
  id: totrans-586
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的标记为`0`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力掩码是什么？
- en: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — A parallel sequence of tokens to be used to indicate the language
    of each token in the input. Indices are languages ids which can be obtained from
    the language names by using two conversion mappings provided in the configuration
    of the model (only provided for multilingual models). More precisely, the *language
    name to language id* mapping is in `model.config.lang2id` (which is a dictionary
    string to int) and the *language id to language name* mapping is in `model.config.id2lang`
    (dictionary int to string).'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langs`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy数组`，*可选*） - 用于指示输入中每个标记的语言的并行标记序列。索引是语言ID，可以通过模型配置中提供的两个转换映射从语言名称中获取（仅适用于多语言模型）。更确切地说，*语言名称到语言ID*映射在`model.config.lang2id`中（这是一个字符串到整数的字典），*语言ID到语言名称*映射在`model.config.id2lang`中（整数到字符串的字典）。'
- en: See usage examples detailed in the [multilingual documentation](../multilingual).
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅[多语言文档](../multilingual)中详细的使用示例。
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy数组`，*可选*）
    - 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: '`0` corresponds to a *sentence A* token,'
  id: totrans-591
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`对应于*句子A*标记，'
- en: '`1` corresponds to a *sentence B* token.'
  id: totrans-592
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`对应于*句子B*标记。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令牌类型ID是什么？
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy数组`，*可选*）
    - 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置ID是什么？
- en: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — Length of each sentence that can be used to avoid performing attention on padding
    token indices. You can also use *attention_mask* for the same result (see above),
    kept here for compatibility Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`tf.Tensor`或形状为`(batch_size,)`的`Numpy`数组，*optional*) — 每个句子的长度，可用于避免在填充令牌索引上执行注意力。您也可以使用*attention_mask*获得相同的结果（请参见上文），这里保留以确保兼容性。在`[0,
    ..., input_ids.size(-1)]`中选择的索引：'
- en: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — Dictionary string to `tf.FloatTensor`
    that contains precomputed hidden states (key and values in the attention blocks)
    as computed by the model (see `cache` output below). Can be used to speed up sequential
    decoding.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — 包含预计算的隐藏状态（由模型计算的注意力块中的键和值）的字符串到`tf.FloatTensor`的字典。可用于加速顺序解码。'
- en: The dictionary object will be modified in-place during the forward pass to add
    newly computed hidden-states.
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。
- en: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`Numpy`数组或形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*optional*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: '`1` indicates the head is `not masked`,'
  id: totrans-600
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`表示头部是`not masked`，'
- en: '`0` indicates the head is `masked`.'
  id: totrans-601
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`表示头部是`masked`。'
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert `input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（某些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for computing
    the sequence classification/regression loss. Indices should be in `[0, ..., config.num_labels
    - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square
    loss), If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor`，形状为`(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`中。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入。
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`，形状为`(batch_size, )`，*optional*，当提供`labels`时返回） — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层输出的模型隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力softmax之后的注意力权重。
- en: The [TFFlaubertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFFlaubertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: TFFlaubertForMultipleChoice
  id: totrans-622
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFFlaubertForMultipleChoice
- en: '### `class transformers.TFFlaubertForMultipleChoice`'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFFlaubertForMultipleChoice`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1202)'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1202)'
- en: '[PRE39]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Flaubert Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有多选分类头的Flaubert模型（池化输出上方的线性层和softmax），例如用于RocStories/SWAG任务。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（如PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需传递您的输入和标签，以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，您可以使用三种可能性来收集所有输入张量放在第一个位置参数中：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个`input_ids`张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1240)'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1240)'
- en: '[PRE40]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-644
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: '`1` for tokens that are `not masked`,'
  id: totrans-647
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被屏蔽的标记，`1`，
- en: '`0` for tokens that are `masked`.'
  id: totrans-648
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被屏蔽的标记，`0`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — A parallel sequence of tokens to be used to indicate the language
    of each token in the input. Indices are languages ids which can be obtained from
    the language names by using two conversion mappings provided in the configuration
    of the model (only provided for multilingual models). More precisely, the *language
    name to language id* mapping is in `model.config.lang2id` (which is a dictionary
    string to int) and the *language id to language name* mapping is in `model.config.id2lang`
    (dictionary int to string).'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于指示输入中每个标记的语言的并行标记序列。索引是语言ID，可以通过模型配置中提供的两个转换映射从语言名称中获取（仅适用于多语言模型）。更准确地说，*语言名称到语言ID*映射在`model.config.lang2id`中（这是一个字符串到整数的字典），*语言ID到语言名称*映射在`model.config.id2lang`中（整数到字符串的字典）。'
- en: See usage examples detailed in the [multilingual documentation](../multilingual).
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅[多语言文档](../multilingual)中详细的使用示例。
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: '`0` corresponds to a *sentence A* token,'
  id: totrans-653
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`对应于*句子A*标记，'
- en: '`1` corresponds to a *sentence B* token.'
  id: totrans-654
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`对应于*句子B*标记。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-655
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — Length of each sentence that can be used to avoid performing attention on padding
    token indices. You can also use *attention_mask* for the same result (see above),
    kept here for compatibility Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用*attention_mask*获得相同的结果（请参见上文），这里保留以保持兼容性。选择的索引在`[0,
    ..., input_ids.size(-1)]`中：'
- en: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — Dictionary string to `tf.FloatTensor`
    that contains precomputed hidden states (key and values in the attention blocks)
    as computed by the model (see `cache` output below). Can be used to speed up sequential
    decoding.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — 包含预先计算的隐藏状态（键和值在注意力块中）的字符串到`tf.FloatTensor`的字典，由模型计算（参见下面的`cache`输出）。可用于加速顺序解码。'
- en: The dictionary object will be modified in-place during the forward pass to add
    newly computed hidden-states.
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。
- en: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: '`1` indicates the head is `not masked`,'
  id: totrans-662
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`表示头部未被屏蔽，'
- en: '`0` indicates the head is `masked`.'
  id: totrans-663
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`表示头部被屏蔽。'
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert `input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型内部的嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量中的`attentions`。此参数仅在急切模式下可用，在图模式中将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`） — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape *(batch_size, )*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为*(batch_size, )*的`tf.Tensor`，*可选*，当提供`labels`时返回） — 分类损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_choices)`的`tf.Tensor`） — *num_choices*是输入张量的第二维度。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SoftMax之前的分类分数。
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-676
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个层的模型的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFFlaubertForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFFlaubertForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE41]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: TFFlaubertForTokenClassification
  id: totrans-683
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFFlaubertForTokenClassification
- en: '### `class transformers.TFFlaubertForTokenClassification`'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFFlaubertForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1109)'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1109)'
- en: '[PRE42]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Flaubert Model with a token classification head on top (a linear layer on top
    of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有标记分类头的Flaubert模型（隐藏状态输出顶部的线性层），例如用于命名实体识别（NER）任务。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。请查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用`input_ids`一个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个变长列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些内容，因为您可以像对待其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1129)'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1129)'
- en: '[PRE43]'
  id: totrans-702
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-706
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：'
- en: '`1` for tokens that are `not masked`,'
  id: totrans-708
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，使用`1`。
- en: '`0` for tokens that are `masked`.'
  id: totrans-709
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，使用`0`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — A parallel sequence of tokens to be used to indicate the language
    of each token in the input. Indices are languages ids which can be obtained from
    the language names by using two conversion mappings provided in the configuration
    of the model (only provided for multilingual models). More precisely, the *language
    name to language id* mapping is in `model.config.lang2id` (which is a dictionary
    string to int) and the *language id to language name* mapping is in `model.config.id2lang`
    (dictionary int to string).'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langs`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）- 用于指示输入中每个标记的语言的并行标记序列。索引是语言ID，可以通过模型配置中提供的两个转换映射从语言名称中获取（仅适用于多语言模型）。更准确地说，*语言名称到语言ID*映射在`model.config.lang2id`中（这是一个字符串到整数的字典），*语言ID到语言名称*映射在`model.config.id2lang`中（整数到字符串的字典）。'
- en: See usage examples detailed in the [multilingual documentation](../multilingual).
  id: totrans-712
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅[多语言文档](../multilingual)中详细的用法示例。
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`tf.Tensor` 或 `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — 指示输入的第一部分和第二部分的段标记索引。索引选择在 `[0, 1]` 范围内：'
- en: '`0` corresponds to a *sentence A* token,'
  id: totrans-714
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 对应于 *句子 A* 标记，'
- en: '`1` corresponds to a *sentence B* token.'
  id: totrans-715
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 对应于 *句子 B* 标记。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`tf.Tensor` 或 `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings -
    1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — Length of each sentence that can be used to avoid performing attention on padding
    token indices. You can also use *attention_mask* for the same result (see above),
    kept here for compatibility Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths` (`tf.Tensor` 或 `Numpy array` of shape `(batch_size,)`, *optional*)
    — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您还可以使用 *attention_mask* 获得相同的结果（见上文），这里保留了兼容性索引选择在
    `[0, ..., input_ids.size(-1)]` 范围内：'
- en: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — Dictionary string to `tf.FloatTensor`
    that contains precomputed hidden states (key and values in the attention blocks)
    as computed by the model (see `cache` output below). Can be used to speed up sequential
    decoding.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — 包含预先计算的隐藏状态（注意力块中的键和值）的字符串到
    `tf.FloatTensor` 的字典，由模型计算（请参见下面的 `cache` 输出）。可用于加速顺序解码。'
- en: The dictionary object will be modified in-place during the forward pass to add
    newly computed hidden-states.
  id: totrans-721
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字典对象将在前向传递过程中被就地修改，以添加新计算的隐藏状态。
- en: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`Numpy array` 或 `tf.Tensor` of shape `(num_heads,)` 或 `(num_layers,
    num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在 `[0, 1]` 范围内：'
- en: '`1` indicates the head is `not masked`,'
  id: totrans-723
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 表示头部未被屏蔽，'
- en: '`0` indicates the head is `masked`.'
  id: totrans-724
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 表示头部被屏蔽。'
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert `input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递 `input_ids`。如果您希望更多地控制如何将 `input_ids` 索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的
    `attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的
    `hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *optional*, 默认为 `False`) — 是否在训练模式下使用模型（一些模块，如 dropout
    模块，在训练和评估之间有不同的行为）。'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算标记分类损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。'
- en: Returns
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)
    或 `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput)
    或 `tf.Tensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of unmasked
    labels, returned when `labels` is provided) — Classification loss.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, 其中 n 是未屏蔽标签的数量，在提供 `labels`
    时返回) — 分类损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-739
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFFlaubertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-744
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: TFFlaubertForQuestionAnsweringSimple
  id: totrans-745
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFFlaubertForQuestionAnsweringSimple`'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1005)'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flaubert Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layer on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个带有与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些内容，因为您可以像对待任何其他
    Python 函数一样传递输入！
- en: '#### `call`'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1022)'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flaubert/modeling_tf_flaubert.py#L1022)'
- en: '[PRE47]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`Numpy array`或`tf.Tensor`） —
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-767
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    和 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`Numpy array`或`tf.Tensor`，*可选*）
    — 用于避免在填充标记索引上执行注意力。掩码值在`[0, 1]`中选择：'
- en: '`1` for tokens that are `not masked`,'
  id: totrans-770
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 表示未被遮蔽的标记，'
- en: '`0` for tokens that are `masked`.'
  id: totrans-771
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 表示被遮蔽的标记。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-772
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`langs` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — A parallel sequence of tokens to be used to indicate the language
    of each token in the input. Indices are languages ids which can be obtained from
    the language names by using two conversion mappings provided in the configuration
    of the model (only provided for multilingual models). More precisely, the *language
    name to language id* mapping is in `model.config.lang2id` (which is a dictionary
    string to int) and the *language id to language name* mapping is in `model.config.id2lang`
    (dictionary int to string).'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langs`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*）
    — 用于指示输入中每个标记的语言的并行标记序列。索引是语言 ID，可以通过模型配置中提供的两个转换映射从语言名称中获取（仅适用于多语言模型）。更准确地说，*语言名称到语言
    ID* 映射在 `model.config.lang2id` 中（这是一个字符串到整数的字典），*语言 ID 到语言名称* 映射在 `model.config.id2lang`
    中（整数到字符串的字典）。'
- en: See usage examples detailed in the [multilingual documentation](../multilingual).
  id: totrans-774
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅[多语言文档](../multilingual)中详细的使用示例。
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*）
    — 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: '`0` corresponds to a *sentence A* token,'
  id: totrans-776
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 对应于 *句子 A* 标记，'
- en: '`1` corresponds to a *sentence B* token.'
  id: totrans-777
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 对应于 *句子 B* 标记。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-778
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*）
    — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-780
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`lengths` (`tf.Tensor` or `Numpy array` of shape `(batch_size,)`, *optional*)
    — Length of each sentence that can be used to avoid performing attention on padding
    token indices. You can also use *attention_mask* for the same result (see above),
    kept here for compatibility Indices selected in `[0, ..., input_ids.size(-1)]`:'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lengths`（形状为`(batch_size,)`的`tf.Tensor`或`Numpy array`，*可选*） — 每个句子的长度，可用于避免在填充标记索引上执行注意力。您也可以使用
    *attention_mask* 获得相同的结果（见上文），这里保留以保持兼容性。在`[0, ..., input_ids.size(-1)]`中选择的索引：'
- en: '`cache` (`Dict[str, tf.Tensor]`, *optional*) — Dictionary string to `tf.FloatTensor`
    that contains precomputed hidden states (key and values in the attention blocks)
    as computed by the model (see `cache` output below). Can be used to speed up sequential
    decoding.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache`（`Dict[str, tf.Tensor]`，*可选*） — 包含模型计算的预计算隐藏状态（注意力块中的键和值）的字符串到 `tf.FloatTensor`
    的字典。可用于加速顺序解码。'
- en: The dictionary object will be modified in-place during the forward pass to add
    newly computed hidden-states.
  id: totrans-783
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前向传递期间，字典对象将被就地修改以添加新计算的隐藏状态。
- en: '`head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`Numpy array`或`tf.Tensor`，*可选*）
    — 用于使自注意力模块中选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: '`1` indicates the head is `not masked`,'
  id: totrans-785
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 表示头部未被遮蔽，'
- en: '`0` indicates the head is `masked`.'
  id: totrans-786
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 表示头部被遮蔽。'
- en: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_ids` you can choose to directly
    pass an embedded representation. This is useful if you want more control over
    how to convert `input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`,
    *optional*) — 可选，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式中将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式中将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式中该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（一些模块，如dropout模块，在训练和评估之间有不同的行为）。'
- en: '`start_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the start of the labelled span for computing the token
    classification loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — 用于计算标记分类损失的标记（索引）起始位置的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会计入损失计算。'
- en: '`end_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the end of the labelled span for computing the token classification
    loss. Positions are clamped to the length of the sequence (`sequence_length`).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — 用于计算标记分类损失的标记（索引）结束位置的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会计入损失计算。'
- en: Returns
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig))
    and inputs.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[FlaubertConfig](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.FlaubertConfig)）和输入的不同元素。
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `start_positions`
    and `end_positions` are provided) — Total span extraction loss is the sum of a
    Cross-Entropy for the start and end positions.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, 当提供`start_positions`和`end_positions`时返回)
    — 总跨度提取损失是起始位置和结束位置的交叉熵之和。'
- en: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-start
    scores (before SoftMax).'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 跨度起始分数（SoftMax之前）。'
- en: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-end
    scores (before SoftMax).'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 跨度结束分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-801
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每一层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-803
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFFlaubertForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple)
    forward method, overrides the `__call__` special method.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFFlaubertForQuestionAnsweringSimple](/docs/transformers/v4.37.2/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE48]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-808
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
