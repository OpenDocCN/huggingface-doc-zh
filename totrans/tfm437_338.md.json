["```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n\n>>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> # Squirtle, Bulbasaur, Charmander, Pikachu in English\n>>> texts = [\"\u6770\u5c3c\u9f9f\", \"\u5999\u86d9\u79cd\u5b50\", \"\u5c0f\u706b\u9f99\", \"\u76ae\u5361\u4e18\"]\n\n>>> # compute image feature\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_features = model.get_image_features(**inputs)\n>>> image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n\n>>> # compute text features\n>>> inputs = processor(text=texts, padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n>>> text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n\n>>> # compute image-text similarity scores\n>>> inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # probs: [[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]]\n```", "```py\n( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 **kwargs )\n```", "```py\n>>> from transformers import ChineseCLIPConfig, ChineseCLIPModel\n\n>>> # Initializing a ChineseCLIPConfig with OFA-Sys/chinese-clip-vit-base-patch16 style configuration\n>>> configuration = ChineseCLIPConfig()\n\n>>> # Initializing a ChineseCLIPModel (with random weights) from the OFA-Sys/chinese-clip-vit-base-patch16 style configuration\n>>> model = ChineseCLIPModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a ChineseCLIPConfig from a ChineseCLIPTextConfig and a ChineseCLIPVisionConfig\n\n>>> # Initializing a ChineseCLIPTextConfig and ChineseCLIPVisionConfig configuration\n>>> config_text = ChineseCLIPTextConfig()\n>>> config_vision = ChineseCLIPVisionConfig()\n\n>>> config = ChineseCLIPConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: ChineseCLIPTextConfig vision_config: ChineseCLIPVisionConfig **kwargs )\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 initializer_factor = 1.0 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True **kwargs )\n```", "```py\n>>> from transformers import ChineseCLIPTextConfig, ChineseCLIPTextModel\n\n>>> # Initializing a ChineseCLIPTextConfig with OFA-Sys/chinese-clip-vit-base-patch16 style configuration\n>>> configuration = ChineseCLIPTextConfig()\n\n>>> # Initializing a ChineseCLIPTextModel (with random weights) from the OFA-Sys/chinese-clip-vit-base-patch16 style configuration\n>>> model = ChineseCLIPTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import ChineseCLIPVisionConfig, ChineseCLIPVisionModel\n\n>>> # Initializing a ChineseCLIPVisionConfig with OFA-Sys/chinese-clip-vit-base-patch16 style configuration\n>>> configuration = ChineseCLIPVisionConfig()\n\n>>> # Initializing a ChineseCLIPVisionModel (with random weights) from the OFA-Sys/chinese-clip-vit-base-patch16 style configuration\n>>> model = ChineseCLIPVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_convert_rgb: bool = True **kwargs )\n```", "```py\n( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: int = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_convert_rgb: bool = None return_tensors: Union = None data_format: Optional = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: ChineseCLIPConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.chinese_clip.modeling_chinese_clip.ChineseCLIPOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, ChineseCLIPModel\n\n>>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> processor = AutoProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"\u6770\u5c3c\u9f9f\", \"\u5999\u86d9\u79cd\u5b50\", \"\u5c0f\u706b\u9f99\", \"\u76ae\u5361\u4e18\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoTokenizer, ChineseCLIPModel\n\n>>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> inputs = tokenizer([\"\u6770\u5c3c\u9f9f\", \"\u5999\u86d9\u79cd\u5b50\", \"\u5c0f\u706b\u9f99\", \"\u76ae\u5361\u4e18\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n>>> text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, ChineseCLIPModel\n\n>>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> processor = AutoProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n>>> image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ChineseCLIPTextModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> model = ChineseCLIPTextModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: ChineseCLIPVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import CLIPProcessor, ChineseCLIPVisionModel\n\n>>> model = ChineseCLIPVisionModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> processor = CLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]