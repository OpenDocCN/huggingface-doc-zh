# 推理API

> 原始文本：[https://huggingface.co/docs/accelerate/package_reference/inference](https://huggingface.co/docs/accelerate/package_reference/inference)

这些文档涉及[PiPPy](https://github.com/PyTorch/PiPPy)集成。

`accelerate.prepare_pippy`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/inference.py#L111)

```py
( model split_points: Union = 'auto' no_split_module_classes: Optional = None example_args: Optional = () example_kwargs: Optional = None num_chunks: Optional = None gather_output: Optional = False )
```

参数

+   `model`（`torch.nn.Module`）- 我们要为管道并行推理拆分的模型

+   `split_points`（`str`或`List[str]`，默认为'auto'）- 如何生成分割点并在每个GPU上分块模型。'auto'将根据任何模型找到最佳平衡的分割。否则应该是要按照分割的模型中的层名称列表。

+   `no_split_module_classes`（`List[str]`）- 我们不希望拆分的层的类名列表。

+   `example_args`（模型输入的元组）- 该模型预期使用基于顺序的输入。如果可能的话，建议使用这种方法。

+   `example_kwargs`（模型输入的字典）- 该模型预期使用基于字典的输入。这是一个*非常*限制性的结构，需要在*所有*推理调用中存在相同的键。除非先前条件对所有情况都成立，否则不建议使用。

+   `num_chunks`（`int`，默认为可用GPU的数量）- 管道将具有的不同阶段的数量。默认情况下，它将为每个GPU分配一个块，但可以进行调整和调试。一般来说，应该有num_chunks >= num_gpus。

+   `gather_output`（`bool`，默认为`False`）- 如果为`True`，则来自最后一个GPU（保存真实输出的GPU）的输出将发送到所有GPU。

为管道并行推理包装`model`。
