# 配置

> 原始文本：[https://huggingface.co/docs/peft/package_reference/config](https://huggingface.co/docs/peft/package_reference/config)

`PeftConfigMixin`是用于存储[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)适配器配置的基本配置类，而[PromptLearningConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PromptLearningConfig)是用于软提示方法（p-tuning、prefix tuning和prompt tuning）的基本配置类。这些基本类包含了从Hub保存和加载模型配置的方法，指定要使用的PEFT方法、要执行的任务类型以及模型配置（如层数和注意力头数）。

## PeftConfigMixin

### `class peft.config.PeftConfigMixin`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L26)

```py
( peft_type: Optional = None auto_mapping: Optional = None )
```

参数

+   `peft_type`（Union[`~peft.utils.config.PeftType`，`str`]）— 要使用的Peft方法的类型。

这是PEFT适配器模型的基本配置类。它包含所有PEFT适配器模型共有的方法。此类继承自[PushToHubMixin](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.utils.PushToHubMixin)，其中包含将模型推送到Hub的方法。`save_pretrained`方法将在目录中保存适配器模型的配置。`from_pretrained`方法将从目录中加载适配器模型的配置。

#### `from_json_file`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L153)

```py
( path_json_file: str **kwargs )
```

参数

+   `path_json_file`（`str`）— json文件的路径。

从json文件加载配置文件。

#### `from_peft_type`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L82)

```py
( **kwargs )
```

参数

+   `kwargs`（配置关键字参数）— 传递给配置初始化的关键字参数。

此方法从一组kwargs中加载适配器模型的配置。

适当的配置类型由`peft_type`参数确定。如果未提供`peft_type`，则实例化调用类类型。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L120)

```py
( pretrained_model_name_or_path: str subfolder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`）— 配置保存的目录或Hub存储库id。

+   `kwargs`（额外的关键字参数，*可选*）— 传递给子类初始化的额外关键字参数。

此方法从目录中加载适配器模型的配置。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L49)

```py
( save_directory: str **kwargs )
```

参数

+   `save_directory`（`str`）— 配置将被保存的目录。

+   `kwargs`（额外的关键字参数，*可选*）— 传递给[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。

此方法将适配器模型的配置保存在目录中。

#### `to_dict`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L43)

```py
( )
```

将适配器模型的配置作为字典返回。

## PeftConfig

### `class peft.PeftConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L221)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False )
```

参数

+   `peft_type`（Union[`~peft.utils.config.PeftType`，`str`]）— 要使用的Peft方法的类型。

+   `task_type`（Union[`~peft.utils.config.TaskType`，`str`]）— 要执行的任务类型。

+   `inference_mode`（`bool`，默认为`False`）— 是否在推理模式下使用Peft模型。

这是用于存储[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)配置的基本配置类。

## PromptLearningConfig

### `class peft.PromptLearningConfig`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L241)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False num_virtual_tokens: int = None token_dim: int = None num_transformer_submodules: Optional = None num_attention_heads: Optional = None num_layers: Optional = None )
```

参数

+   `num_virtual_tokens` (`int`) — 要使用的虚拟标记数量。

+   `token_dim` (`int`) — 基础变换器模型的隐藏嵌入维度。

+   `num_transformer_submodules` (`int`) — 基础变换器模型中的变换器子模块数量。

+   `num_attention_heads` (`int`) — 基础变换器模型中的注意力头数量。

+   `num_layers` (`int`) — 基础变换器模型中的层数。

这是用于存储`PrefixTuning`、[PromptEncoder](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoder)或`PromptTuning`配置的基础配置类。
