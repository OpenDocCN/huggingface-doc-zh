- en: Accelerate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/15.f0a1f8b9.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Accelerate is a library that enables the same PyTorch code to be run across
    any distributed configuration by adding just four lines of code! In short, training
    and inference at scale made simple, efficient and adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Built on `torch_xla` and `torch.distributed`, ðŸ¤— Accelerate takes care of the
    heavy lifting, so you donâ€™t have to write any custom code to adapt to these platforms.
    Convert existing codebases to utilize [DeepSpeed](usage_guides/deepspeed), perform
    [fully sharded data parallelism](usage_guides/fsdp), and have automatic support
    for mixed-precision training!
  prefs: []
  type: TYPE_NORMAL
- en: To get a better idea of this process, make sure to check out the [Tutorials](basic_tutorials/overview)!
  prefs: []
  type: TYPE_NORMAL
- en: 'This code can then be launched on any system through Accelerateâ€™s CLI interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Tutorials'
  prefs: []
  type: TYPE_NORMAL
- en: Learn the basics and become familiar with using ðŸ¤— Accelerate. Start here if
    you are using ðŸ¤— Accelerate for the first time!](./basic_tutorials/overview) [How-to
    guides
  prefs: []
  type: TYPE_NORMAL
- en: Practical guides to help you achieve a specific goal. Take a look at these guides
    to learn how to use ðŸ¤— Accelerate to solve real-world problems.](./usage_guides/explore)
    [Conceptual guides
  prefs: []
  type: TYPE_NORMAL
- en: High-level explanations for building a better understanding of important topics
    such as avoiding subtle nuances and pitfalls in distributed training and DeepSpeed.](./concept_guides/gradient_synchronization)
    [Reference
  prefs: []
  type: TYPE_NORMAL
- en: Technical descriptions of how ðŸ¤— Accelerate classes and methods work.](./package_reference/accelerator)
  prefs: []
  type: TYPE_NORMAL
