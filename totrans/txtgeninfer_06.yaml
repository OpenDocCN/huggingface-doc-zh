- en: Messages API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-generation-inference/messages_api](https://huggingface.co/docs/text-generation-inference/messages_api)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-generation-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/start.96d64f85.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/scheduler.9680c161.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/singletons.5632daf5.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.9d57cde4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/paths.5eca520f.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/app.48a2a24c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.38d74ee1.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/0.c01ff294.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/16.e1b5faf3.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/CodeBlock.1371964c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Heading.74c51a96.js">
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference (TGI) now supports the Messages API, which is fully
    compatible with the OpenAI Chat Completion API. This feature is available starting
    from version 1.4.0\. You can use OpenAI’s client libraries or third-party libraries
    expecting OpenAI schema to interact with TGI’s Messages API. Below are some examples
    of how to utilize this compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The Messages API is supported from TGI version 1.4.0 and above. Ensure
    you are using a compatible version to access this feature.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Making a Request](#making-a-request)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Streaming](#streaming)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Synchronous](#synchronous)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hugging Face Inference Endpoints](#hugging-face-inference-endpoints)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cloud Providers](#cloud-providers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Amazon SageMaker](#amazon-sagemaker)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a Request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can make a request to TGI’s Messages API using `curl`. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also use OpenAI’s Python client library to make a streaming request.
    Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Synchronous
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you prefer to make a synchronous request, you can do so like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Hugging Face Inference Endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Messages API is integrated with [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).
  prefs: []
  type: TYPE_NORMAL
- en: 'Every endpoint that uses “Text Generation Inference” with an LLM, which has
    a chat template can now be used. Below is an example of how to use IE with TGI
    using OpenAI’s Python client library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Make sure to replace `base_url` with your endpoint URL and to include
    `v1/` at the end of the URL. The `api_key` should be replaced with your Hugging
    Face API key.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Cloud Providers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TGI can be deployed on various cloud providers for scalable and robust text
    generation. One such provider is Amazon SageMaker, which has recently added support
    for TGI. Here’s how you can deploy TGI on Amazon SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To enable the Messages API in Amazon SageMaker you need to set the environment
    variable `MESSAGES_API_ENABLED=true`.
  prefs: []
  type: TYPE_NORMAL
- en: This will modify the `/invocations` route to accept Messages dictonaries consisting
    out of role and content. See the example below on how to deploy Llama with the
    new Messages API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
