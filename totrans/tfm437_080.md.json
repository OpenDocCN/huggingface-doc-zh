["```py\npip install flash-attn --no-build-isolation\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n\nmodel_id = \"tiiuae/falcon-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.bfloat16, \n    attn_implementation=\"flash_attention_2\",\n)\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n\nmodel_id = \"tiiuae/falcon-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# load in 8bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    load_in_8bit=True,\n    attn_implementation=\"flash_attention_2\",\n)\n\n# load in 4bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    load_in_4bit=True,\n    attn_implementation=\"flash_attention_2\",\n)\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16).to(\"cuda\")\n# convert the model to BetterTransformer\nmodel.to_bettertransformer()\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    outputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```", "```py\nRuntimeError: No available kernel. Aborting execution.\n\n# install PyTorch nightly\npip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n```", "```py\nmodel = model.to_bettertransformer()\n```", "```py\nmodel = model.reverse_bettertransformer()\nmodel.save_pretrained(\"saved_model\")\n```", "```py\n# these versions support 8-bit and 4-bit\npip install bitsandbytes>=0.39.0 accelerate>=0.20.0\n\n# install Transformers\npip install transformers\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"bigscience/bloom-2b5\"\nmodel_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n```", "```py\nmax_memory_mapping = {0: \"600MB\", 1: \"1GB\"}\nmodel_name = \"bigscience/bloom-3b\"\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    model_name, device_map=\"auto\", load_in_4bit=True, max_memory=max_memory_mapping\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"bigscience/bloom-2b5\"\nmodel_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"bigscience/bloom-2b5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n\nprompt = \"Hello, my llama is cute\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ngenerated_ids = model.generate(**inputs)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n```", "```py\nmax_memory_mapping = {0: \"1GB\", 1: \"2GB\"}\nmodel_name = \"bigscience/bloom-3b\"\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_name, device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping\n)\n```", "```py\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\n\nort_model = ORTModelForSequenceClassification.from_pretrained(\n  \"distilbert-base-uncased-finetuned-sst-2-english\",\n  export=True,\n  provider=\"CUDAExecutionProvider\",\n)\n```", "```py\nfrom optimum.pipelines import pipeline\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\npipeline = pipeline(task=\"text-classification\", model=ort_model, tokenizer=tokenizer, device=\"cuda:0\")\nresult = pipeline(\"Both the music and visual were astounding, not to mention the actors performance.\")\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# load model in 4-bit\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=quantization_config)\n\n# enable BetterTransformer\nmodel = model.to_bettertransformer()\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# enable FlashAttention\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    outputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```"]