- en: ðŸ¤— Optimum Neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/index](https://huggingface.co/docs/optimum-neuron/index)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS
    Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls)
    and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls).
    It provides a set of tools enabling easy model loading, training and inference
    on single- and multi-Accelerator settings for different downstream tasks. The
    list of officially validated models and tasks is available [here](https://huggingface.co/docs/optimum-neuron/package_reference/configuration#supported-architectures).
  prefs: []
  type: TYPE_NORMAL
- en: '[Tutorials'
  prefs: []
  type: TYPE_NORMAL
- en: Learn the basics and become familiar with training & deploying transformers
    on AWS Trainium and AWS Inferentia. Start here if you are using ðŸ¤— Optimum Neuron
    for the first time!](./tutorials/overview) [How-to guides
  prefs: []
  type: TYPE_NORMAL
- en: Practical guides to help you achieve a specific goal. Take a look at these guides
    to learn how to use ðŸ¤— Optimum Neuron to solve real-world problems.](./guides/overview)
    [Reference
  prefs: []
  type: TYPE_NORMAL
- en: Technical descriptions of how the classes and methods of ðŸ¤— Optimum Neuron work.](./package_reference/trainer)
  prefs: []
  type: TYPE_NORMAL
