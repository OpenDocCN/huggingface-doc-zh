- en: Low Precision Training Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½ç²¾åº¦è®­ç»ƒæ–¹æ³•
- en: 'Original text: [https://huggingface.co/docs/accelerate/concept_guides/low_precision_training](https://huggingface.co/docs/accelerate/concept_guides/low_precision_training)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/concept_guides/low_precision_training](https://huggingface.co/docs/accelerate/concept_guides/low_precision_training)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The release of new kinds of hardware led to the emergence of new training paradigms
    that better utilize them. Currently, this is in the form of training in 8-bit
    precision using packages such as [TransformersEngine](https://github.com/NVIDIA/TransformerEngine)
    (TE) or [MS-AMP](https://github.com/Azure/MS-AMP/tree/main).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°å‹ç¡¬ä»¶çš„å‘å¸ƒå¯¼è‡´äº†æ–°çš„è®­ç»ƒèŒƒå¼çš„å‡ºç°ï¼Œæ›´å¥½åœ°åˆ©ç”¨å®ƒä»¬ã€‚ç›®å‰ï¼Œè¿™ä»¥ä½¿ç”¨è¯¸å¦‚ [TransformersEngine](https://github.com/NVIDIA/TransformerEngine)ï¼ˆTEï¼‰æˆ–
    [MS-AMP](https://github.com/Azure/MS-AMP/tree/main) ç­‰è½¯ä»¶åŒ…è¿›è¡Œ 8 ä½ç²¾åº¦è®­ç»ƒçš„å½¢å¼å­˜åœ¨ã€‚
- en: For an introduction to the topics discussed today, we recommend reviewing the
    [low-precision usage guide](../usage_guides/low_precision_training.md) as this
    documentation will reference it regularly.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»Šå¤©è®¨è®ºçš„ä¸»é¢˜çš„ä»‹ç»ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹ [ä½ç²¾åº¦ä½¿ç”¨æŒ‡å—](../usage_guides/low_precision_training.md)ï¼Œå› ä¸ºæœ¬æ–‡æ¡£å°†ç»å¸¸å¼•ç”¨å®ƒã€‚
- en: A Quick Chart
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå›¾è¡¨
- en: 'Below is a quick chart from the MS-AMP documentation showing the different
    bit-precisions for each solution during training:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ MS-AMP æ–‡æ¡£ä¸­æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªè§£å†³æ–¹æ¡ˆçš„ä¸åŒä½ç²¾åº¦çš„å¿«é€Ÿå›¾è¡¨ï¼š
- en: '| Optimization Level | Computation(GEMM) | Comm | Weight | Master Weight |
    Weight Gradient | Optimizer States |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| ä¼˜åŒ–çº§åˆ« | è®¡ç®—ï¼ˆGEMMï¼‰ | é€šä¿¡ | æƒé‡ | ä¸»æƒé‡ | æƒé‡æ¢¯åº¦ | ä¼˜åŒ–å™¨çŠ¶æ€ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 AMP | FP16 | FP32 | FP32 | N/A | FP32 | FP32+FP32 |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| FP16 AMP | FP16 | FP32 | FP32 | N/A | FP32 | FP32+FP32 |'
- en: '| Nvidia TE | FP8 | FP32 | FP32 | N/A | FP32 | FP32+FP32 |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Nvidia TE | FP8 | FP32 | FP32 | N/A | FP32 | FP32+FP32 |'
- en: '| MS-AMP O1 | FP8 | FP8 | FP16 | N/A | FP8 | FP32+FP32 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| MS-AMP O1 | FP8 | FP8 | FP16 | N/A | FP8 | FP32+FP32 |'
- en: '| MS-AMP O2 | FP8 | FP8 | FP16 | N/A | FP8 | FP8+FP16 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| MS-AMP O2 | FP8 | FP8 | FP16 | N/A | FP8 | FP8+FP16 |'
- en: '| MS-AMP O3 | FP8 | FP8 | FP8 | FP16 | FP8 | FP8+FP16 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| MS-AMP O3 | FP8 | FP8 | FP8 | FP16 | FP8 | FP8+FP16 |'
- en: TransformersEngine
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransformersEngine
- en: '`TransformersEngine` is the first solution to trying to train in 8-bit floating
    point. It works by using drop-in replacement layers for certain ones in a model
    that utilize their FP8-engine to reduce the number of bits (such as 32 to 8) without
    degrading the final accuracy of the model.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransformersEngine` æ˜¯å°è¯•åœ¨ 8 ä½æµ®ç‚¹æ•°ä¸­è¿›è¡Œè®­ç»ƒçš„ç¬¬ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡ä½¿ç”¨æ¨¡å‹ä¸­æŸäº›å±‚çš„æ›¿æ¢å±‚æ¥å·¥ä½œï¼Œè¿™äº›æ›¿æ¢å±‚åˆ©ç”¨å®ƒä»¬çš„
    FP8 å¼•æ“æ¥å‡å°‘ä½æ•°ï¼ˆä¾‹å¦‚ä» 32 åˆ° 8ï¼‰ï¼Œè€Œä¸ä¼šé™ä½æ¨¡å‹çš„æœ€ç»ˆå‡†ç¡®æ€§ã€‚'
- en: 'Specifically, ğŸ¤— Accelerate will find and replace the following layers with
    `TransformersEngine` versions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼ŒğŸ¤— Accelerate å°†ä½¿ç”¨ `TransformersEngine` ç‰ˆæœ¬æ‰¾åˆ°å¹¶æ›¿æ¢ä»¥ä¸‹å±‚ï¼š
- en: '`nn.LayerNorm` for `te.LayerNorm`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.LayerNorm` æ›¿æ¢ä¸º `te.LayerNorm`'
- en: '`nn.Linear` for `te.Linear`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Linear` æ›¿æ¢ä¸º `te.Linear`'
- en: As a result we wind up with a model that has most of its layers in BF16, while
    some layers are in FP8 reducing some of the memory.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†å±‚éƒ½æ˜¯ BF16ï¼Œè€Œä¸€äº›å±‚æ˜¯ FP8ï¼Œä»è€Œå‡å°‘äº†ä¸€äº›å†…å­˜ã€‚
- en: Anecdotally, we have noticed that performance gains donâ€™t really start showing
    when using `TransformerEngine` until a large majority of the layers in the model
    are made up of those two layers to replace. As a result, only larger models have
    shown performance improvements when the number of parameters is around and upwards
    of a few billion.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ®æˆ‘ä»¬è§‚å¯Ÿï¼Œå½“æ¨¡å‹ä¸­çš„å¤§å¤šæ•°å±‚ç”±è¿™ä¸¤ä¸ªå±‚ç»„æˆä»¥è¿›è¡Œæ›¿æ¢æ—¶ï¼Œä½¿ç”¨ `TransformerEngine` æ‰ä¼šå¼€å§‹æ˜¾ç¤ºæ€§èƒ½æå‡ã€‚å› æ­¤ï¼Œåªæœ‰è¾ƒå¤§çš„æ¨¡å‹åœ¨å‚æ•°æ•°é‡çº¦ä¸ºå‡ åäº¿åŠä»¥ä¸Šæ—¶æ‰æ˜¾ç¤ºå‡ºæ€§èƒ½æ”¹è¿›ã€‚
- en: 'The `TransformerEngine` can receive many different arguments that customize
    how it performs FP8 calculations and what they do. A full list of the arguments
    is available below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransformerEngine` å¯ä»¥æ¥æ”¶è®¸å¤šä¸åŒçš„å‚æ•°ï¼Œå®šåˆ¶å®ƒå¦‚ä½•æ‰§è¡Œ FP8 è®¡ç®—ä»¥åŠå®ƒä»¬çš„åŠŸèƒ½ã€‚ä¸‹é¢æ˜¯å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼š'
- en: '`margin`: The margin to use for the gradient scaling.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`margin`ï¼šç”¨äºæ¢¯åº¦ç¼©æ”¾çš„è¾¹è·ã€‚'
- en: '`interval`: The interval to use for how often the scaling factor is recomputed.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interval`ï¼šç”¨äºé‡æ–°è®¡ç®—ç¼©æ”¾å› å­çš„é—´éš”ã€‚'
- en: '`fp8_format``: The format to use for the FP8 recipe. Must be one of` E4M3`or`HYBRID`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp8_format`ï¼šç”¨äº FP8 é…æ–¹çš„æ ¼å¼ã€‚å¿…é¡»æ˜¯`E4M3`æˆ–`HYBRID`ä¹‹ä¸€ã€‚'
- en: '`amax_history_len`: The length of the history to use for the scaling factor
    computation'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amax_history_len`ï¼šç”¨äºç¼©æ”¾å› å­è®¡ç®—çš„å†å²é•¿åº¦'
- en: '`amax_compute_algo`: The algorithm to use for the scaling factor computation.
    Must be one of `max` or `most_recent`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amax_compute_algo`ï¼šç”¨äºç¼©æ”¾å› å­è®¡ç®—çš„ç®—æ³•ã€‚å¿…é¡»æ˜¯`max`æˆ–`most_recent`ä¹‹ä¸€ã€‚'
- en: '`override_linear_precision`: Whether or not to execute `fprop`, `dgrad`, and
    `wgrad` GEMMS in higher precision.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`override_linear_precision`ï¼šæ˜¯å¦åœ¨æ›´é«˜ç²¾åº¦ä¸­æ‰§è¡Œ `fprop`ã€`dgrad` å’Œ `wgrad` GEMMSã€‚'
- en: You can customize each of these as part of [utils.FP8RecipeKwargs](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.FP8RecipeKwargs)
    to help optimize performance of your models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å°†è¿™äº›å‚æ•°ä½œä¸º [utils.FP8RecipeKwargs](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.FP8RecipeKwargs)
    çš„ä¸€éƒ¨åˆ†è¿›è¡Œå®šåˆ¶ï¼Œä»¥å¸®åŠ©ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: If we notice in the chart mentioned earlier, TE simply casts the computation
    layers into FP8, while everything else is in FP32\. As a result this winds up
    utilizing the most memory but does so with the benefit of guaranteeing the least
    amount of loss in end accuracy during training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ³¨æ„åˆ°å‰é¢æåˆ°çš„å›¾è¡¨ï¼ŒTE ç®€å•åœ°å°†è®¡ç®—å±‚è½¬æ¢ä¸º FP8ï¼Œè€Œå…¶ä»–æ‰€æœ‰å†…å®¹éƒ½æ˜¯ FP32ã€‚å› æ­¤ï¼Œè¿™æœ€ç»ˆåˆ©ç”¨äº†å¤§éƒ¨åˆ†å†…å­˜ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿è¯äº†æœ€å°‘çš„æœ€ç»ˆå‡†ç¡®æ€§æŸå¤±ã€‚
- en: MS-AMP
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MS-AMP
- en: MS-AMP takes a different approach to `TransformersEngine` by providing three
    different optimization levels to convert more operations in FP8 or FP16.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MS-AMP é€šè¿‡æä¾›ä¸‰ç§ä¸åŒçš„ä¼˜åŒ–çº§åˆ«æ¥é‡‡ç”¨ä¸åŒçš„æ–¹æ³•æ¥ä½¿ç”¨ `TransformersEngine`ï¼Œä»¥å°†æ›´å¤šæ“ä½œè½¬æ¢ä¸º FP8 æˆ– FP16ã€‚
- en: The base optimization level (`O1`), passes communications of the weights (such
    as in DDP) in FP8, stores the weights of the model in FP16, and leaves the optimizer
    states in FP32\. The main benefit of this optimization level is that we can reduce
    the communication bandwidth by essentially half. Additionally, more GPU memory
    is saved due to 1/2 of everything being cast in FP8, and the weights being cast
    to FP16\. Notably, both the optimizer states remain in FP32.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¼˜åŒ–çº§åˆ«ï¼ˆ`O1`ï¼‰å°†æƒé‡çš„é€šä¿¡ï¼ˆå¦‚åœ¨DDPä¸­ï¼‰ä¼ é€’ä¸ºFP8ï¼Œåœ¨FP16ä¸­å­˜å‚¨æ¨¡å‹çš„æƒé‡ï¼Œå¹¶å°†ä¼˜åŒ–å™¨çŠ¶æ€ä¿ç•™åœ¨FP32ä¸­ã€‚è¿™ç§ä¼˜åŒ–çº§åˆ«çš„ä¸»è¦å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥å°†é€šä¿¡å¸¦å®½å‡å°‘ä¸€åŠã€‚æ­¤å¤–ï¼Œç”±äºä¸€åŠçš„å†…å®¹éƒ½æ˜¯FP8ï¼Œæƒé‡è¢«è½¬æ¢ä¸ºFP16ï¼Œå› æ­¤å¯ä»¥èŠ‚çœæ›´å¤šçš„GPUå†…å­˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ä»ä¿ç•™åœ¨FP32ä¸­ã€‚
- en: The second optimization level (`O2`) improves upon this by also reducing the
    precision of the optimizer states. One is in FP8 while the other is in FP16\.
    Generally itâ€™s been shown that this will only provide a net-gain of no degraded
    end accuracy, increased training speed, and reduced memory as now every state
    is either in FP16 or FP8.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªä¼˜åŒ–çº§åˆ«ï¼ˆ`O2`ï¼‰é€šè¿‡é™ä½ä¼˜åŒ–å™¨çŠ¶æ€çš„ç²¾åº¦æ¥æ”¹è¿›ã€‚ä¸€ä¸ªæ˜¯FP8ï¼Œå¦ä¸€ä¸ªæ˜¯FP16ã€‚é€šå¸¸å·²ç»è¡¨æ˜ï¼Œè¿™åªä¼šæä¾›ä¸€ä¸ªå‡€å¢ç›Šï¼Œä¸ä¼šé™ä½æœ€ç»ˆçš„å‡†ç¡®æ€§ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶å‡å°‘å†…å­˜ï¼Œå› ä¸ºç°åœ¨æ¯ä¸ªçŠ¶æ€éƒ½æ˜¯FP16æˆ–FP8ã€‚
- en: Finally, MS-AMP has a third optimization level (`O3`) which helps during DDP
    scenarios such as DeepSpeed. The weights of the model in memory are fully cast
    to FP8, and the master weights are now stored in FP16\. This fully reduces memory
    by the highest factor as now not only is almost everything in FP8, only two states
    are left in FP16\. Currently, only DeepSpeed versions up through 0.9.2 are supported,
    so this capability is not included in the ğŸ¤— Accelerate integration
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒMS-AMPæœ‰ç¬¬ä¸‰ä¸ªä¼˜åŒ–çº§åˆ«ï¼ˆ`O3`ï¼‰ï¼Œåœ¨DDPåœºæ™¯ï¼ˆå¦‚DeepSpeedï¼‰ä¸­æœ‰æ‰€å¸®åŠ©ã€‚æ¨¡å‹çš„æƒé‡åœ¨å†…å­˜ä¸­å®Œå…¨è½¬æ¢ä¸ºFP8ï¼Œä¸»æƒé‡ç°åœ¨å­˜å‚¨åœ¨FP16ä¸­ã€‚è¿™æ ·å¯ä»¥é€šè¿‡æœ€é«˜å› å­å®Œå…¨å‡å°‘å†…å­˜ï¼Œå› ä¸ºç°åœ¨å‡ ä¹æ‰€æœ‰å†…å®¹éƒ½æ˜¯FP8ï¼Œåªæœ‰ä¸¤ä¸ªçŠ¶æ€ç•™åœ¨FP16ä¸­ã€‚ç›®å‰ï¼Œåªæ”¯æŒDeepSpeedç‰ˆæœ¬0.9.2åŠä»¥ä¸‹ï¼Œå› æ­¤è¿™ç§èƒ½åŠ›ä¸åŒ…æ‹¬åœ¨ğŸ¤—
    Accelerateé›†æˆä¸­ã€‚
- en: Combining the two
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“åˆä¸¤è€…
- en: More experiments need to be performed but itâ€™s been noted that combining both
    MS-AMP and TransformersEngine can lead to the highest throughput by relying on
    NVIDIAâ€™s optimized FP8 operators and utilizing how MS-AMP reduces the memory overhead.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦è¿›è¡Œæ›´å¤šå®éªŒï¼Œä½†å·²ç»æ³¨æ„åˆ°ï¼Œç»“åˆMS-AMPå’ŒTransformersEngineå¯ä»¥ä¾é NVIDIAä¼˜åŒ–çš„FP8è¿ç®—ç¬¦ä»¥åŠåˆ©ç”¨MS-AMPå‡å°‘å†…å­˜å¼€é”€æ¥å®ç°æœ€é«˜ååé‡ã€‚
