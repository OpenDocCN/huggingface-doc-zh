# SigLIP

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/siglip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/siglip)

## 概述

SigLIP模型是由Xiaohua Zhai、Basil Mustafa、Alexander Kolesnikov、Lucas Beyer在[用于语言图像预训练的Sigmoid Loss](https://arxiv.org/abs/2303.15343)中提出的。SigLIP建议用简单的成对Sigmoid损失替换[CLIP](clip)中使用的损失函数。这导致在ImageNet的零样本分类准确性方面表现更好。

论文摘要如下：

*我们提出了一种简单的成对Sigmoid损失用于语言-图像预训练（SigLIP）。与标准的具有softmax归一化的对比学习不同，Sigmoid损失仅在图像-文本对上操作，不需要全局查看成对相似性以进行归一化。Sigmoid损失同时允许进一步扩大批处理大小，同时在较小的批处理大小下表现更好。结合锁定图像调整，仅使用四个TPUv4芯片，我们训练了一个在两天内实现了84.5% ImageNet零样本准确性的SigLiT模型。批处理大小与损失的解耦进一步使我们能够研究示例与对之间的影响以及负到正的比率。最后，我们将批处理大小推到极限，高达一百万，并发现随着批处理大小的增长，好处迅速减少，32k的更合理的批处理大小已经足够。*

## 使用提示

+   SigLIP的使用类似于[CLIP](clip)。主要区别在于训练损失，它不需要查看批处理中所有图像和文本的成对相似性的全局视图。需要将sigmoid激活函数应用于logits，而不是softmax。

+   目前不支持训练。如果你想要微调SigLIP或从头开始训练，请参考来自[OpenCLIP](https://github.com/mlfoundations/open_clip/blob/73ad04ae7fb93ede1c02dc9040a828634cb1edf1/src/open_clip/loss.py#L307)的损失函数，该函数利用了各种`torch.distributed`实用程序。

+   当使用独立的[SiglipTokenizer](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTokenizer)或[SiglipProcessor](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipProcessor)时，请确保传递`padding="max_length"`，因为模型是这样训练的。

![drawing](../Images/c4663b93ec2c7f896c4d1127ca7ac46f.png) SigLIP评估结果与CLIP进行比较。摘自[原始论文](https://arxiv.org/abs/2303.15343)。

这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。原始代码可以在[这里](https://github.com/google-research/big_vision/tree/main)找到。

## 使用示例

有两种主要方法可以使用SigLIP：一种是使用管道API，它为您抽象了所有复杂性，另一种是自己使用`SiglipModel`类。

### Pipeline API

该流程允许在几行代码中使用模型：

```py
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> # load pipe
>>> image_classifier = pipeline(task="zero-shot-image-classification", model="google/siglip-base-patch16-224")

>>> # load image
>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # inference
>>> outputs = image_classifier(image, candidate_labels=["2 cats", "a plane", "a remote"])
>>> outputs = [{"score": round(output["score"], 4), "label": output["label"] } for output in outputs]
>>> print(outputs)
[{'score': 0.1979, 'label': '2 cats'}, {'score': 0.0, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]
```

### 自己使用模型

如果你想自己进行预处理和后处理，以下是如何操作的：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AutoModel
>>> import torch

>>> model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
>>> processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> texts = ["a photo of 2 cats", "a photo of 2 dogs"]
>>> # important: we pass `padding=max_length` since the model was trained with this
>>> inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits_per_image = outputs.logits_per_image
>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities
>>> print(f"{probs[0][0]:.1%} that image 0 is '{texts[0]}'")
31.9% that image 0 is 'a photo of 2 cats'
```

## SiglipConfig

### `class transformers.SiglipConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/configuration_siglip.py#L232)

```py
( text_config = None vision_config = None **kwargs )
```

参数

+   `text_config`（`dict`，*可选*）—用于初始化[SiglipTextConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTextConfig)的配置选项字典。

+   `vision_config`（`dict`，*可选*）—用于初始化[SiglipVisionConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipVisionConfig)的配置选项字典。

+   `kwargs`（*可选*）—关键字参数字典。

[SiglipConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipConfig)是用于存储[SiglipModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipModel)配置的配置类。根据指定的参数实例化一个Siglip模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生类似于Siglip [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import SiglipConfig, SiglipModel

>>> # Initializing a SiglipConfig with google/siglip-base-patch16-224 style configuration
>>> configuration = SiglipConfig()

>>> # Initializing a SiglipModel (with random weights) from the google/siglip-base-patch16-224 style configuration
>>> model = SiglipModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a SiglipConfig from a SiglipTextConfig and a SiglipVisionConfig
>>> from transformers import SiglipTextConfig, SiglipVisionConfig

>>> # Initializing a SiglipText and SiglipVision configuration
>>> config_text = SiglipTextConfig()
>>> config_vision = SiglipVisionConfig()

>>> config = SiglipConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/configuration_siglip.py#L292)

```py
( text_config: SiglipTextConfig vision_config: SiglipVisionConfig **kwargs ) → export const metadata = 'undefined';SiglipConfig
```

返回

[SiglipConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipConfig)

配置对象的一个实例

从siglip文本模型配置和siglip视觉模型配置实例化一个[SiglipConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipConfig)（或派生类）。

## SiglipTextConfig

### `class transformers.SiglipTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/configuration_siglip.py#L31)

```py
( vocab_size = 32000 hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 max_position_embeddings = 64 hidden_act = 'gelu_pytorch_tanh' layer_norm_eps = 1e-06 attention_dropout = 0.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 32000) — Siglip文本模型的词汇表大小。定义在调用[SiglipModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipModel)时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。

+   `max_position_embeddings` (`int`, *optional*, defaults to 64) — 该模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu_pytorch_tanh"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"` `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `pad_token_id` (`int`, *optional*, defaults to 1) — 词汇表中填充标记的id。

+   `bos_token_id` (`int`, *optional*, defaults to 49406) — 词汇表中序列开始标记的id。

+   `eos_token_id` (`int`, *optional*, defaults to 49407) — 词汇表中序列结束标记的id。

这是一个配置类，用于存储[SiglipTextModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTextModel)的配置。根据指定的参数实例化一个Siglip文本编码器，定义模型架构。使用默认值实例化配置将产生类似于Siglip [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224)架构的文本编码器配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例:

```py
>>> from transformers import SiglipTextConfig, SiglipTextModel

>>> # Initializing a SiglipTextConfig with google/siglip-base-patch16-224 style configuration
>>> configuration = SiglipTextConfig()

>>> # Initializing a SiglipTextModel (with random weights) from the google/siglip-base-patch16-224 style configuration
>>> model = SiglipTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## SiglipVisionConfig

### `class transformers.SiglipVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/configuration_siglip.py#L136)

```py
( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 16 hidden_act = 'gelu_pytorch_tanh' layer_norm_eps = 1e-06 attention_dropout = 0.0 **kwargs )
```

参数

+   `hidden_size` (`int`, *可选*, 默认为 768) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *可选*, 默认为 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为 12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*, 默认为 12) — Transformer编码器中每个注意力层的注意力头数量。

+   `num_channels` (`int`, *可选*, 默认为 3) — 输入图像中的通道数。

+   `image_size` (`int`, *可选*, 默认为 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *可选*, 默认为 16) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str` 或 `function`, *可选*, 默认为 `"gelu_pytorch_tanh"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`、`"quick_gelu"`。

+   `layer_norm_eps` (`float`, *可选*, 默认为 1e-06) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *可选*, 默认为 0.0) — 注意力概率的丢弃比率。

这是用于存储[SiglipVisionModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipVisionModel)配置的配置类。根据指定的参数实例化Siglip视觉编码器，定义模型架构。使用默认值实例化配置将产生类似于Siglip [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224)架构的视觉编码器的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例:

```py
>>> from transformers import SiglipVisionConfig, SiglipVisionModel

>>> # Initializing a SiglipVisionConfig with google/siglip-base-patch16-224 style configuration
>>> configuration = SiglipVisionConfig()

>>> # Initializing a SiglipVisionModel (with random weights) from the google/siglip-base-patch16-224 style configuration
>>> model = SiglipVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## SiglipTokenizer

### `class transformers.SiglipTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/tokenization_siglip.py#L53)

```py
( vocab_file eos_token = '</s>' unk_token = '<unk>' pad_token = '</s>' additional_special_tokens = None sp_model_kwargs: Optional = None model_max_length = 64 do_lower_case = True **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含实例化分词器所需词汇的[SentencePiece](https://github.com/google/sentencepiece)文件（通常具有*.spm*扩展名）。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为 `"</s>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `additional_special_tokens` (`List[str]`, *可选*) — 分词器使用的额外特殊标记。

+   `sp_model_kwargs` (`dict`, *可选*) — 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`: 启用子词正则化。

    +   `nbest_size`: 对unigram的采样参数。对于BPE-Dropout无效。

        +   `nbest_size = {0,1}`: 不执行采样。

        +   `nbest_size > 1`：从 nbest_size 结果中采样。

        +   `nbest_size < 0`：假设 nbest_size 为无限，并使用前向过滤和后向采样算法从所有假设（格）中采样。

    +   `alpha`：unigram 采样的平滑参数，以及 BPE-dropout 合并操作的丢弃概率。

+   `model_max_length` (`int`, *optional*, 默认为 64) — 模型输入的最大长度（标记数）。

+   `do_lower_case` (`bool`, *optional*, 默认为 `True`) — 在标记化时是否将输入转换为小写。

构建一个 Siglip 分词器。基于 [SentencePiece](https://github.com/google/sentencepiece)。

此分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/tokenization_siglip.py#L237)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

具有适当特殊标记的[输入 ID](../glossary#input-ids)列表。

通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。序列的格式如下：

+   单个序列：`X </s>`

+   序列对：`A </s> B </s>`

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/tokenization_siglip.py#L173)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — 标记列表是否已经为模型格式化了特殊标记。

返回

`List[int]`

一个整数列表，范围为 [0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 id。在使用分词器的 `prepare_for_model` 方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/tokenization_siglip.py#L214)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

零列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。T5 不使用标记类型 id，因此返回一个零列表。

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/tokenization_siglip.py#L374)

```py
( save_directory: str filename_prefix: Optional = None )
```

## SiglipImageProcessor

### `class transformers.SiglipImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/image_processing_siglip.py#L46)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

参数

+   `do_resize` (`bool`, *optional*, 默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的 `size`。可以被 `preprocess` 方法中的 `do_resize` 覆盖。

+   `size` (`Dict[str, int]` *optional*, 默认为 `{"height" -- 224, "width": 224}`)：调整大小后的图像尺寸。可以被 `preprocess` 方法中的 `size` 覆盖。

+   `resample` (`PILImageResampling`, *optional*, 默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则要使用的重采样滤波器。可以被 `preprocess` 方法中的 `resample` 覆盖。

+   `do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否按照指定的比例 `rescale_factor` 重新缩放图像。可以被 `preprocess` 方法中的 `do_rescale` 覆盖。

+   `rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新缩放图像，则使用的缩放因子。可以被 `preprocess` 方法中的 `rescale_factor` 覆盖。

+   `do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否按指定的均值和标准差对图像进行归一化。可以被 `preprocess` 方法中的 `do_normalize` 覆盖。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `[0.5, 0.5, 0.5]`) — 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `[0.5, 0.5, 0.5]`) — 如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

构建 SigLIP 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/image_processing_siglip.py#L104)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Optional = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个图像或图像批处理，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 调整大小后的图像尺寸。

+   `resample` (`int`, *可选*, 默认为 `self.resample`) — 如果调整图像大小，则使用的重采样滤波器。这可以是枚举 `PILImageResampling` 中的一个。仅在 `do_resize` 设置为 `True` 时生效。

+   `do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否重新缩放图像。

+   `rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则重新缩放图像的重新缩放因子。

+   `do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 用于归一化的图像均值。仅在 `do_normalize` 设置为 `True` 时生效。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_std`) — 用于归一化的图像标准差。仅在 `do_normalize` 设置为 `True` 时生效。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一:

    +   未设置: 返回 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`: 返回类型为 `tf.Tensor` 的批处理。

    +   `TensorType.PYTORCH` 或 `'pt'`: 返回类型为 `torch.Tensor` 的批处理。

    +   `TensorType.NUMPY` 或 `'np'`: 返回类型为 `np.ndarray` 的批处理。

    +   `TensorType.JAX` 或 `'jax'`: 返回类型为 `jax.numpy.ndarray` 的批处理。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一:

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   未设置: 使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像推断通道维度格式。可以是以下之一:

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像以 (高度, 宽度) 格式。

预处理图像或图像批处理。

## SiglipProcessor

### `class transformers.SiglipProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/processing_siglip.py#L28)

```py
( image_processor tokenizer )
```

参数

+   `image_processor` ([SiglipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipImageProcessor)) — 图像处理器是必需的输入。

+   `tokenizer` ([SiglipTokenizer](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTokenizer)) — Tokenizer是必需的输入。

构建一个Siglip处理器，将Siglip图像处理器和Siglip标记器包装成一个处理器。

[SiglipProcessor](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipProcessor)提供了[SiglipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipImageProcessor)和[SiglipTokenizer](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTokenizer)的所有功能。查看`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipProcessor.decode)以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/processing_siglip.py#L131)

```py
( *args **kwargs )
```

这个方法将所有参数转发给SiglipTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/processing_siglip.py#L124)

```py
( *args **kwargs )
```

这个方法将所有参数转发给SiglipTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。

## SiglipModel

### `class transformers.SiglipModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L965)

```py
( config: SiglipConfig )
```

参数

+   `config` ([SiglipConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般使用和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L1095)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.siglip.modeling_siglip.SiglipOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示未被掩码的标记，

    +   0表示被掩码的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.siglip.modeling_siglip.SiglipOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.siglip.modeling_siglip.SiglipOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.siglip.configuration_siglip.SiglipConfig'>`）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当`return_loss`为`True`时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image:(torch.FloatTensor`，形状为`(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(torch.FloatTensor`，形状为`(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于[SiglipTextModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTextModel)的汇聚输出获得的文本嵌入。

+   `image_embeds(torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于[SiglipVisionModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipVisionModel)的汇聚输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [SiglipTextModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTextModel)的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` [SiglipVisionModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipVisionModel)的输出。

[SiglipModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AutoModel
>>> import torch

>>> model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
>>> processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> texts = ["a photo of 2 cats", "a photo of 2 dogs"]
>>> # important: we pass `padding=max_length` since the model was trained with this
>>> inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits_per_image = outputs.logits_per_image
>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities
>>> print(f"{probs[0][0]:.1%} that image 0 is '{texts[0]}'")
31.9% that image 0 is 'a photo of 2 cats'
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L996)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，如果提供，将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于`未被掩盖`的标记为1，

    +   对于`被掩盖`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

text_features (`torch.FloatTensor`，形状为`(batch_size, output_dim`)

通过将投影层应用于[SiGLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTextModel)的汇总输出获得的文本嵌入。

[SiglipModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前处理和后处理步骤，而后者会默默地忽略它们。

例如：

```py
>>> from transformers import AutoTokenizer, AutoModel
>>> import torch

>>> model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
>>> tokenizer = AutoTokenizer.from_pretrained("google/siglip-base-patch16-224")

>>> # important: make sure to set padding="max_length" as that's how the model was trained
>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding="max_length", return_tensors="pt")
>>> with torch.no_grad():
...     text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L1045)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下，如果提供，将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

image_features (`torch.FloatTensor`，形状为`(batch_size, output_dim`)

通过将投影层应用于[SiGLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipVisionModel)的汇总输出获得的图像嵌入。

[SiglipModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AutoModel
>>> import torch

>>> model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
>>> processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     image_features = model.get_image_features(**inputs)
```

## SiglipTextModel

### `class transformers.SiglipTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L764)

```py
( config: SiglipTextConfig )
```

参数

+   `config`（[SiglipConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

SigLIP的文本模型，没有任何头部或顶部的投影。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L785)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.siglip.configuration_siglip.SiglipTextConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列的输出。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过辅助预训练任务中用于处理的各层后，序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选的*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型每层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[SiglipTextModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipTextModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, SiglipTextModel

>>> model = SiglipTextModel.from_pretrained("google/siglip-base-patch16-224")
>>> tokenizer = AutoTokenizer.from_pretrained("google/siglip-base-patch16-224")

>>> # important: make sure to set padding="max_length" as that's how the model was trained
>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding="max_length", return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## SiglipVisionModel

### `class transformers.SiglipVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L905)

```py
( config: SiglipVisionConfig )
```

参数

+   `config`（[SiglipConfig](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipConfig)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

SigLIP中的视觉模型，没有顶部的头部或投影。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/siglip/modeling_siglip.py#L924)

```py
( pixel_values output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling) 或 `tuple(torch.FloatTensor)`

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（`<class 'transformers.models.siglip.configuration_siglip.SiglipVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为 `(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这将返回经过线性层和双曲正切激活函数处理后的分类标记。线性层的权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层，则为嵌入输出的一个 + 每层的输出一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

[SiglipVisionModel](/docs/transformers/v4.37.2/en/model_doc/siglip#transformers.SiglipVisionModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, SiglipVisionModel

>>> model = SiglipVisionModel.from_pretrained("google/siglip-base-patch16-224")
>>> processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled features
```
