- en: Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter any issue when using PEFT, please check the following list
    of common issues and their solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Examples donâ€™t work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Examples often rely on the most recent package versions, so please ensure theyâ€™re
    up-to-date. In particular, check the following package versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`peft`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accelerate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, you can update the package version by running this command inside
    your Python environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing PEFT from source is useful for keeping up with the latest developments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'ValueError: Attempting to unscale FP16 gradients'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This error probably occurred because the model was loaded with `torch_dtype=torch.float16`
    and then used in an automatic mixed precision (AMP) context, e.g. by setting `fp16=True`
    in the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class from ðŸ¤— Transformers. The reason is that when using AMP, trainable weights
    should never use fp16\. To make this work without loading the whole model in fp32,
    add the following to your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the [cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)
    function to correctly cast the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Bad results from a loaded PEFT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There can be several reasons for getting a poor result from a loaded PEFT model
    which are listed below. If youâ€™re still unable to troubleshoot the problem, see
    if anyone else had a similar [issue](https://github.com/huggingface/peft/issues)
    on GitHub, and if you canâ€™t find any, open a new issue.
  prefs: []
  type: TYPE_NORMAL
- en: When opening an issue, it helps a lot if you provide a minimal code example
    that reproduces the issue. Also, please report if the loaded model performs at
    the same level as the model did before fine-tuning, if it performs at a random
    level, or if it is only slightly worse than expected. This information helps us
    identify the problem more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Random deviations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If your model outputs are not exactly the same as previous runs, there could
    be an issue with random elements. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: please ensure it is in `.eval()` mode, which is important, for instance, if
    the model uses dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: if you use [generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    on a language model, there could be random sampling, so obtaining the same result
    requires setting a random seed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: if you used quantization and merged the weights, small deviations are expected
    due to rounding errors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Incorrectly loaded model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please ensure that you load the model correctly. A common error is trying to
    load a *trained* model with [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    which is incorrect. Instead, the loading code should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Randomly initialized layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For some tasks, it is important to correctly configure `modules_to_save` in
    the config to account for randomly initialized layers.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, this is necessary if you use LoRA to fine-tune a language model
    for sequence classification because ðŸ¤— Transformers adds a randomly initialized
    classification head on top of the model. If you do not add this layer to `modules_to_save`,
    the classification head wonâ€™t be saved. The next time you load the model, youâ€™ll
    get a *different* randomly initialized classification head, resulting in completely
    different results.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT tries to correctly guess the `modules_to_save` if you provide the `task_type`
    argument in the config. This should work for transformers models that follow the
    standard naming scheme. It is always a good idea to double check though because
    we canâ€™t guarantee all models follow the naming scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you load a transformers model that has randomly initialized layers, you
    should see a warning along the lines of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The mentioned layers should be added to `modules_to_save` in the config to avoid
    the described problem.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the vocabulary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For many language fine-tuning tasks, extending the modelâ€™s vocabulary is necessary
    since new tokens are being introduced. This requires extending the embedding layer
    to account for the new tokens and also storing the embedding layer in addition
    to the adapter weights when saving the adapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the embedding layer by adding it to the `target_modules` of the config.
    The embedding layer name must follow the standard naming scheme from Transformers.
    For example, the Mistral config could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once added to `target_modules`, PEFT automatically stores the embedding layer
    when saving the adapter if the model has the [get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)
    and [get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings).
    This is generally the case for Transformers models.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the modelâ€™s embedding layer doesnâ€™t follow the Transformerâ€™s naming scheme,
    you can still save it by manually passing `save_embedding_layers=True` when saving
    the adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For inference, load the base model first and resize it the same way you did
    before you trained the model. After youâ€™ve resized the base model, you can load
    the PEFT checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: For a complete example, please check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb).
  prefs: []
  type: TYPE_NORMAL
