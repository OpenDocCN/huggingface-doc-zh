- en: Troubleshooting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter any issue when using PEFT, please check the following list
    of common issues and their solutions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在使用PEFT时遇到任何问题，请检查以下常见问题及其解决方案列表。
- en: Examples don’t work
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例不起作用
- en: 'Examples often rely on the most recent package versions, so please ensure they’re
    up-to-date. In particular, check the following package versions:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 示例通常依赖于最新的软件包版本，因此请确保它们是最新的。特别是，请检查以下软件包版本：
- en: '`peft`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft`'
- en: '`transformers`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers`'
- en: '`accelerate`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accelerate`'
- en: '`torch`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`'
- en: 'In general, you can update the package version by running this command inside
    your Python environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您可以通过在Python环境中运行以下命令来更新软件包版本：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Installing PEFT from source is useful for keeping up with the latest developments:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从源代码安装PEFT对于跟上最新发展是有用的：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'ValueError: Attempting to unscale FP16 gradients'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ValueError: 尝试对FP16梯度进行反缩放'
- en: 'This error probably occurred because the model was loaded with `torch_dtype=torch.float16`
    and then used in an automatic mixed precision (AMP) context, e.g. by setting `fp16=True`
    in the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class from 🤗 Transformers. The reason is that when using AMP, trainable weights
    should never use fp16\. To make this work without loading the whole model in fp32,
    add the following to your code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误可能是因为模型加载时使用了`torch_dtype=torch.float16`，然后在自动混合精度（AMP）环境中使用，例如通过在🤗 Transformers的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类中设置`fp16=True`。原因是在使用AMP时，可训练权重不应使用fp16。为了使其在不加载整个模型到fp32的情况下工作，请将以下内容添加到您的代码中：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, you can use the [cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)
    function to correctly cast the weights:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用[cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)函数正确转换权重：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Bad results from a loaded PEFT model
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从加载的PEFT模型中获得糟糕的结果
- en: There can be several reasons for getting a poor result from a loaded PEFT model
    which are listed below. If you’re still unable to troubleshoot the problem, see
    if anyone else had a similar [issue](https://github.com/huggingface/peft/issues)
    on GitHub, and if you can’t find any, open a new issue.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从加载的PEFT模型中获得糟糕结果可能有几个原因，列举如下。如果您仍然无法解决问题，请查看GitHub上是否有其他人遇到类似的[问题](https://github.com/huggingface/peft/issues)，如果找不到，请提出新问题。
- en: When opening an issue, it helps a lot if you provide a minimal code example
    that reproduces the issue. Also, please report if the loaded model performs at
    the same level as the model did before fine-tuning, if it performs at a random
    level, or if it is only slightly worse than expected. This information helps us
    identify the problem more quickly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出问题时，如果您提供一个能够重现问题的最小代码示例，将会很有帮助。另外，请报告加载的模型是否与微调前的模型表现相同，是否表现在随机水平上，或者是否只比预期稍差。这些信息有助于我们更快地识别问题。
- en: Random deviations
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机偏差
- en: 'If your model outputs are not exactly the same as previous runs, there could
    be an issue with random elements. For example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型输出与之前的运行结果不完全相同，可能存在随机元素的问题。例如：
- en: please ensure it is in `.eval()` mode, which is important, for instance, if
    the model uses dropout
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请确保处于`.eval()`模式，这很重要，例如，如果模型使用了dropout
- en: if you use [generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    on a language model, there could be random sampling, so obtaining the same result
    requires setting a random seed
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在语言模型上使用[generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)，可能会进行随机抽样，因此要获得相同的结果，需要设置一个随机种子
- en: if you used quantization and merged the weights, small deviations are expected
    due to rounding errors
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用了量化并合并了权重，由于四舍五入误差，可能会出现小的偏差。
- en: Incorrectly loaded model
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误加载的模型
- en: 'Please ensure that you load the model correctly. A common error is trying to
    load a *trained* model with [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    which is incorrect. Instead, the loading code should look like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保正确加载模型。一个常见错误是尝试使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)加载*训练过的*模型，这是不正确的。相反，加载代码应该如下所示：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Randomly initialized layers
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机初始化的层
- en: For some tasks, it is important to correctly configure `modules_to_save` in
    the config to account for randomly initialized layers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些任务，正确配置`config`中的`modules_to_save`非常重要，以考虑随机初始化的层。
- en: As an example, this is necessary if you use LoRA to fine-tune a language model
    for sequence classification because 🤗 Transformers adds a randomly initialized
    classification head on top of the model. If you do not add this layer to `modules_to_save`,
    the classification head won’t be saved. The next time you load the model, you’ll
    get a *different* randomly initialized classification head, resulting in completely
    different results.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您使用LoRA对语言模型进行微调以进行序列分类，因为🤗 Transformers在模型顶部添加了一个随机初始化的分类头。如果您没有将此层添加到`modules_to_save`中，分类头将不会被保存。下次加载模型时，您将获得一个*不同的*随机初始化的分类头，导致完全不同的结果。
- en: PEFT tries to correctly guess the `modules_to_save` if you provide the `task_type`
    argument in the config. This should work for transformers models that follow the
    standard naming scheme. It is always a good idea to double check though because
    we can’t guarantee all models follow the naming scheme.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在配置中提供了`task_type`参数，PEFT会尝试正确猜测`modules_to_save`。这对于遵循标准命名方案的transformers模型应该有效。不过最好还是仔细检查一下，因为我们无法保证所有模型都遵循命名方案。
- en: 'When you load a transformers model that has randomly initialized layers, you
    should see a warning along the lines of:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当您加载一个具有随机初始化层的transformers模型时，您应该看到类似以下的警告：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The mentioned layers should be added to `modules_to_save` in the config to avoid
    the described problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 应将提到的层添加到配置中的`modules_to_save`中，以避免所描述的问题。
- en: Extending the vocabulary
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展词汇表
- en: For many language fine-tuning tasks, extending the model’s vocabulary is necessary
    since new tokens are being introduced. This requires extending the embedding layer
    to account for the new tokens and also storing the embedding layer in addition
    to the adapter weights when saving the adapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多语言微调任务，需要扩展模型的词汇表，因为引入了新的标记。这需要扩展嵌入层以考虑新的标记，并在保存适配器时将嵌入层存储在适配器权重之外。
- en: 'Save the embedding layer by adding it to the `target_modules` of the config.
    The embedding layer name must follow the standard naming scheme from Transformers.
    For example, the Mistral config could look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将嵌入层添加到配置的`target_modules`中保存嵌入层。嵌入层的名称必须遵循Transformers的标准命名方案。例如，Mistral配置可能如下所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once added to `target_modules`, PEFT automatically stores the embedding layer
    when saving the adapter if the model has the [get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)
    and [get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings).
    This is generally the case for Transformers models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加到`target_modules`，PEFT在保存适配器时会自动存储嵌入层，如果模型具有[get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)和[get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings)。这通常适用于Transformers模型。
- en: 'If the model’s embedding layer doesn’t follow the Transformer’s naming scheme,
    you can still save it by manually passing `save_embedding_layers=True` when saving
    the adapter:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的嵌入层不遵循Transformer的命名方案，仍可以通过在保存适配器时手动传递`save_embedding_layers=True`来保存它。
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For inference, load the base model first and resize it the same way you did
    before you trained the model. After you’ve resized the base model, you can load
    the PEFT checkpoint.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，首先加载基础模型并以与训练模型之前相同的方式调整大小。调整大小后，可以加载PEFT检查点。
- en: For a complete example, please check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有关完整示例，请查看[此笔记本](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb)。
