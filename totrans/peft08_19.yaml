- en: Troubleshooting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•…éšœæ’é™¤
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter any issue when using PEFT, please check the following list
    of common issues and their solutions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨ä½¿ç”¨PEFTæ—¶é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆåˆ—è¡¨ã€‚
- en: Examples donâ€™t work
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ä¸èµ·ä½œç”¨
- en: 'Examples often rely on the most recent package versions, so please ensure theyâ€™re
    up-to-date. In particular, check the following package versions:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹é€šå¸¸ä¾èµ–äºæœ€æ–°çš„è½¯ä»¶åŒ…ç‰ˆæœ¬ï¼Œå› æ­¤è¯·ç¡®ä¿å®ƒä»¬æ˜¯æœ€æ–°çš„ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹è½¯ä»¶åŒ…ç‰ˆæœ¬ï¼š
- en: '`peft`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft`'
- en: '`transformers`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers`'
- en: '`accelerate`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accelerate`'
- en: '`torch`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`'
- en: 'In general, you can update the package version by running this command inside
    your Python environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨Pythonç¯å¢ƒä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ›´æ–°è½¯ä»¶åŒ…ç‰ˆæœ¬ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Installing PEFT from source is useful for keeping up with the latest developments:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æºä»£ç å®‰è£…PEFTå¯¹äºè·Ÿä¸Šæœ€æ–°å‘å±•æ˜¯æœ‰ç”¨çš„ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'ValueError: Attempting to unscale FP16 gradients'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ValueError: å°è¯•å¯¹FP16æ¢¯åº¦è¿›è¡Œåç¼©æ”¾'
- en: 'This error probably occurred because the model was loaded with `torch_dtype=torch.float16`
    and then used in an automatic mixed precision (AMP) context, e.g. by setting `fp16=True`
    in the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class from ğŸ¤— Transformers. The reason is that when using AMP, trainable weights
    should never use fp16\. To make this work without loading the whole model in fp32,
    add the following to your code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé”™è¯¯å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹åŠ è½½æ—¶ä½¿ç”¨äº†`torch_dtype=torch.float16`ï¼Œç„¶ååœ¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚é€šè¿‡åœ¨ğŸ¤— Transformersçš„[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç±»ä¸­è®¾ç½®`fp16=True`ã€‚åŸå› æ˜¯åœ¨ä½¿ç”¨AMPæ—¶ï¼Œå¯è®­ç»ƒæƒé‡ä¸åº”ä½¿ç”¨fp16ã€‚ä¸ºäº†ä½¿å…¶åœ¨ä¸åŠ è½½æ•´ä¸ªæ¨¡å‹åˆ°fp32çš„æƒ…å†µä¸‹å·¥ä½œï¼Œè¯·å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­ï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, you can use the [cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)
    function to correctly cast the weights:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)å‡½æ•°æ­£ç¡®è½¬æ¢æƒé‡ï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Bad results from a loaded PEFT model
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»åŠ è½½çš„PEFTæ¨¡å‹ä¸­è·å¾—ç³Ÿç³•çš„ç»“æœ
- en: There can be several reasons for getting a poor result from a loaded PEFT model
    which are listed below. If youâ€™re still unable to troubleshoot the problem, see
    if anyone else had a similar [issue](https://github.com/huggingface/peft/issues)
    on GitHub, and if you canâ€™t find any, open a new issue.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä»åŠ è½½çš„PEFTæ¨¡å‹ä¸­è·å¾—ç³Ÿç³•ç»“æœå¯èƒ½æœ‰å‡ ä¸ªåŸå› ï¼Œåˆ—ä¸¾å¦‚ä¸‹ã€‚å¦‚æœæ‚¨ä»ç„¶æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·æŸ¥çœ‹GitHubä¸Šæ˜¯å¦æœ‰å…¶ä»–äººé‡åˆ°ç±»ä¼¼çš„[é—®é¢˜](https://github.com/huggingface/peft/issues)ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯·æå‡ºæ–°é—®é¢˜ã€‚
- en: When opening an issue, it helps a lot if you provide a minimal code example
    that reproduces the issue. Also, please report if the loaded model performs at
    the same level as the model did before fine-tuning, if it performs at a random
    level, or if it is only slightly worse than expected. This information helps us
    identify the problem more quickly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æå‡ºé—®é¢˜æ—¶ï¼Œå¦‚æœæ‚¨æä¾›ä¸€ä¸ªèƒ½å¤Ÿé‡ç°é—®é¢˜çš„æœ€å°ä»£ç ç¤ºä¾‹ï¼Œå°†ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚å¦å¤–ï¼Œè¯·æŠ¥å‘ŠåŠ è½½çš„æ¨¡å‹æ˜¯å¦ä¸å¾®è°ƒå‰çš„æ¨¡å‹è¡¨ç°ç›¸åŒï¼Œæ˜¯å¦è¡¨ç°åœ¨éšæœºæ°´å¹³ä¸Šï¼Œæˆ–è€…æ˜¯å¦åªæ¯”é¢„æœŸç¨å·®ã€‚è¿™äº›ä¿¡æ¯æœ‰åŠ©äºæˆ‘ä»¬æ›´å¿«åœ°è¯†åˆ«é—®é¢˜ã€‚
- en: Random deviations
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: éšæœºåå·®
- en: 'If your model outputs are not exactly the same as previous runs, there could
    be an issue with random elements. For example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ¨¡å‹è¾“å‡ºä¸ä¹‹å‰çš„è¿è¡Œç»“æœä¸å®Œå…¨ç›¸åŒï¼Œå¯èƒ½å­˜åœ¨éšæœºå…ƒç´ çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼š
- en: please ensure it is in `.eval()` mode, which is important, for instance, if
    the model uses dropout
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯·ç¡®ä¿å¤„äº`.eval()`æ¨¡å¼ï¼Œè¿™å¾ˆé‡è¦ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ¨¡å‹ä½¿ç”¨äº†dropout
- en: if you use [generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    on a language model, there could be random sampling, so obtaining the same result
    requires setting a random seed
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨è¯­è¨€æ¨¡å‹ä¸Šä½¿ç”¨[generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ï¼Œå¯èƒ½ä¼šè¿›è¡ŒéšæœºæŠ½æ ·ï¼Œå› æ­¤è¦è·å¾—ç›¸åŒçš„ç»“æœï¼Œéœ€è¦è®¾ç½®ä¸€ä¸ªéšæœºç§å­
- en: if you used quantization and merged the weights, small deviations are expected
    due to rounding errors
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨äº†é‡åŒ–å¹¶åˆå¹¶äº†æƒé‡ï¼Œç”±äºå››èˆäº”å…¥è¯¯å·®ï¼Œå¯èƒ½ä¼šå‡ºç°å°çš„åå·®ã€‚
- en: Incorrectly loaded model
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é”™è¯¯åŠ è½½çš„æ¨¡å‹
- en: 'Please ensure that you load the model correctly. A common error is trying to
    load a *trained* model with [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    which is incorrect. Instead, the loading code should look like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ç¡®ä¿æ­£ç¡®åŠ è½½æ¨¡å‹ã€‚ä¸€ä¸ªå¸¸è§é”™è¯¯æ˜¯å°è¯•ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)åŠ è½½*è®­ç»ƒè¿‡çš„*æ¨¡å‹ï¼Œè¿™æ˜¯ä¸æ­£ç¡®çš„ã€‚ç›¸åï¼ŒåŠ è½½ä»£ç åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Randomly initialized layers
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: éšæœºåˆå§‹åŒ–çš„å±‚
- en: For some tasks, it is important to correctly configure `modules_to_save` in
    the config to account for randomly initialized layers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›ä»»åŠ¡ï¼Œæ­£ç¡®é…ç½®`config`ä¸­çš„`modules_to_save`éå¸¸é‡è¦ï¼Œä»¥è€ƒè™‘éšæœºåˆå§‹åŒ–çš„å±‚ã€‚
- en: As an example, this is necessary if you use LoRA to fine-tune a language model
    for sequence classification because ğŸ¤— Transformers adds a randomly initialized
    classification head on top of the model. If you do not add this layer to `modules_to_save`,
    the classification head wonâ€™t be saved. The next time you load the model, youâ€™ll
    get a *different* randomly initialized classification head, resulting in completely
    different results.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨LoRAå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œåºåˆ—åˆ†ç±»ï¼Œå› ä¸ºğŸ¤— Transformersåœ¨æ¨¡å‹é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´ã€‚å¦‚æœæ‚¨æ²¡æœ‰å°†æ­¤å±‚æ·»åŠ åˆ°`modules_to_save`ä¸­ï¼Œåˆ†ç±»å¤´å°†ä¸ä¼šè¢«ä¿å­˜ã€‚ä¸‹æ¬¡åŠ è½½æ¨¡å‹æ—¶ï¼Œæ‚¨å°†è·å¾—ä¸€ä¸ª*ä¸åŒçš„*éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´ï¼Œå¯¼è‡´å®Œå…¨ä¸åŒçš„ç»“æœã€‚
- en: PEFT tries to correctly guess the `modules_to_save` if you provide the `task_type`
    argument in the config. This should work for transformers models that follow the
    standard naming scheme. It is always a good idea to double check though because
    we canâ€™t guarantee all models follow the naming scheme.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨é…ç½®ä¸­æä¾›äº†`task_type`å‚æ•°ï¼ŒPEFTä¼šå°è¯•æ­£ç¡®çŒœæµ‹`modules_to_save`ã€‚è¿™å¯¹äºéµå¾ªæ ‡å‡†å‘½åæ–¹æ¡ˆçš„transformersæ¨¡å‹åº”è¯¥æœ‰æ•ˆã€‚ä¸è¿‡æœ€å¥½è¿˜æ˜¯ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•ä¿è¯æ‰€æœ‰æ¨¡å‹éƒ½éµå¾ªå‘½åæ–¹æ¡ˆã€‚
- en: 'When you load a transformers model that has randomly initialized layers, you
    should see a warning along the lines of:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨åŠ è½½ä¸€ä¸ªå…·æœ‰éšæœºåˆå§‹åŒ–å±‚çš„transformersæ¨¡å‹æ—¶ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è­¦å‘Šï¼š
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The mentioned layers should be added to `modules_to_save` in the config to avoid
    the described problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åº”å°†æåˆ°çš„å±‚æ·»åŠ åˆ°é…ç½®ä¸­çš„`modules_to_save`ä¸­ï¼Œä»¥é¿å…æ‰€æè¿°çš„é—®é¢˜ã€‚
- en: Extending the vocabulary
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰©å±•è¯æ±‡è¡¨
- en: For many language fine-tuning tasks, extending the modelâ€™s vocabulary is necessary
    since new tokens are being introduced. This requires extending the embedding layer
    to account for the new tokens and also storing the embedding layer in addition
    to the adapter weights when saving the adapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®¸å¤šè¯­è¨€å¾®è°ƒä»»åŠ¡ï¼Œéœ€è¦æ‰©å±•æ¨¡å‹çš„è¯æ±‡è¡¨ï¼Œå› ä¸ºå¼•å…¥äº†æ–°çš„æ ‡è®°ã€‚è¿™éœ€è¦æ‰©å±•åµŒå…¥å±‚ä»¥è€ƒè™‘æ–°çš„æ ‡è®°ï¼Œå¹¶åœ¨ä¿å­˜é€‚é…å™¨æ—¶å°†åµŒå…¥å±‚å­˜å‚¨åœ¨é€‚é…å™¨æƒé‡ä¹‹å¤–ã€‚
- en: 'Save the embedding layer by adding it to the `target_modules` of the config.
    The embedding layer name must follow the standard naming scheme from Transformers.
    For example, the Mistral config could look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†åµŒå…¥å±‚æ·»åŠ åˆ°é…ç½®çš„`target_modules`ä¸­ä¿å­˜åµŒå…¥å±‚ã€‚åµŒå…¥å±‚çš„åç§°å¿…é¡»éµå¾ªTransformersçš„æ ‡å‡†å‘½åæ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼ŒMistralé…ç½®å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once added to `target_modules`, PEFT automatically stores the embedding layer
    when saving the adapter if the model has the [get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)
    and [get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings).
    This is generally the case for Transformers models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ·»åŠ åˆ°`target_modules`ï¼ŒPEFTåœ¨ä¿å­˜é€‚é…å™¨æ—¶ä¼šè‡ªåŠ¨å­˜å‚¨åµŒå…¥å±‚ï¼Œå¦‚æœæ¨¡å‹å…·æœ‰[get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)å’Œ[get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings)ã€‚è¿™é€šå¸¸é€‚ç”¨äºTransformersæ¨¡å‹ã€‚
- en: 'If the modelâ€™s embedding layer doesnâ€™t follow the Transformerâ€™s naming scheme,
    you can still save it by manually passing `save_embedding_layers=True` when saving
    the adapter:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„åµŒå…¥å±‚ä¸éµå¾ªTransformerçš„å‘½åæ–¹æ¡ˆï¼Œä»å¯ä»¥é€šè¿‡åœ¨ä¿å­˜é€‚é…å™¨æ—¶æ‰‹åŠ¨ä¼ é€’`save_embedding_layers=True`æ¥ä¿å­˜å®ƒã€‚
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For inference, load the base model first and resize it the same way you did
    before you trained the model. After youâ€™ve resized the base model, you can load
    the PEFT checkpoint.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¨ç†ï¼Œé¦–å…ˆåŠ è½½åŸºç¡€æ¨¡å‹å¹¶ä»¥ä¸è®­ç»ƒæ¨¡å‹ä¹‹å‰ç›¸åŒçš„æ–¹å¼è°ƒæ•´å¤§å°ã€‚è°ƒæ•´å¤§å°åï¼Œå¯ä»¥åŠ è½½PEFTæ£€æŸ¥ç‚¹ã€‚
- en: For a complete example, please check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb)ã€‚
