# LoHa

> 原文链接：[https://huggingface.co/docs/peft/package_reference/loha](https://huggingface.co/docs/peft/package_reference/loha)

低秩哈达玛积（[LoHa](https://huggingface.co/papers/2108.06098)），类似于 LoRA，但它使用更多低秩矩阵来近似大权重矩阵，并将它们与哈达玛积结合。这种方法比 LoRA 更加参数高效，并实现了可比较的性能。

论文摘要：

*在这项工作中，我们提出了一种通信高效的参数化方法 FedPara，用于克服频繁模型上传和下载的负担。我们的方法重新参数化层的权重参数，使用低秩权重后跟哈达玛积。与传统的低秩参数化相比，我们的 FedPara 方法不受低秩约束，因此具有更大的容量。这种特性使得在需要 3 到 10 倍较低的通信成本的情况下实现可比较的性能成为可能，而这是传统低秩方法无法实现的。我们的方法的效率可以通过与其他高效的 FL 优化器结合来进一步提高。此外，我们将我们的方法扩展到个性化 FL 应用 pFedPara，将参数分为全局和本地参数。我们展示 pFedPara 在比其他个性化 FL 方法少三倍以上参数的情况下表现更好*。

## LoHaConfig

### `class peft.LoHaConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/config.py#L22)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False rank_pattern: Optional[dict] = <factory> alpha_pattern: Optional[dict] = <factory> r: int = 8 alpha: int = 8 rank_dropout: float = 0.0 module_dropout: float = 0.0 use_effective_conv2d: bool = False target_modules: Union = None init_weights: bool = True layers_to_transform: Union = None layers_pattern: Optional = None modules_to_save: Optional = None )
```

参数

+   `r` (`int`) — LoHa 秩。

+   `alpha` (`int`) — LoHa 缩放的 alpha 参数。

+   `rank_dropout` (`float`) — 在训练期间对秩维度的丢弃概率。

+   `module_dropout` (`float`) — 在训练期间禁用 LoHa 模块的丢弃概率。

+   `use_effective_conv2d` (`bool`) — 使用参数有效分解对于 ksize > 1 的 Conv2d（来自 FedPara 论文的“Proposition 3”）。

+   `target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个，只有指定名称的模块将被替换。当传递一个字符串时，将执行正则表达式匹配。当传递一个字符串列表时，将执行精确匹配或检查模块名称是否以任何传递的字符串结尾。如果指定为 'all-linear'，则选择所有线性/Conv1D 模块，不包括输出层。如果未指定，将根据模型架构选择模块。如果不知道架构，将引发错误 — 在这种情况下，您应该手动指定目标模块。

+   `init_weights` (`bool`) — 是否对适配器权重进行初始化。默认为 `True`，不建议传递 `False`。

+   `layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递一个整数列表，它将适用于在此列表中指定的层索引的适配器。如果传递一个整数，它将在该索引处的层上应用转换。

+   `layers_pattern` (`str`) — 层模式名称，仅在 `layers_to_transform` 不同于 `None` 时使用。

+   `rank_pattern` (`dict`) — 从层名称或正则表达式到与 `r` 指定的默认秩不同的秩的映射。

+   `alpha_pattern` (`dict`) — 从层名称或正则表达式到与 `alpha` 指定的默认 alpha 不同的 alphas 的映射。

+   `modules_to_save` (`Optional[List[str]]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。

这是用于存储 [LoHaModel](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaModel) 配置的类。 

## LoHaModel

### `class peft.LoHaModel`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/model.py#L27)

```py
( model config adapter_name ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model`（`torch.nn.Module`）— 适配器调谐层将附加到的模型。

+   `config`（[LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig)）— LoHa模型的配置。

+   `adapter_name`（`str`）— 适配器的名称，默认为`"default"`。

返回

`torch.nn.Module`

LoHa模型。

从预训练模型创建低秩哈达玛积模型。该方法部分描述在[https://arxiv.org/abs/2108.06098](https://arxiv.org/abs/2108.06098)。当前实现大量借鉴自[https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py](https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py)

示例：

```py
>>> from diffusers import StableDiffusionPipeline
>>> from peft import LoHaModel, LoHaConfig

>>> config_te = LoHaConfig(
...     r=8,
...     lora_alpha=32,
...     target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"],
...     rank_dropout=0.0,
...     module_dropout=0.0,
...     init_weights=True,
... )
>>> config_unet = LoHaConfig(
...     r=8,
...     lora_alpha=32,
...     target_modules=[
...         "proj_in",
...         "proj_out",
...         "to_k",
...         "to_q",
...         "to_v",
...         "to_out.0",
...         "ff.net.0.proj",
...         "ff.net.2",
...     ],
...     rank_dropout=0.0,
...     module_dropout=0.0,
...     init_weights=True,
...     use_effective_conv2d=True,
... )

>>> model = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> model.text_encoder = LoHaModel(model.text_encoder, config_te, "default")
>>> model.unet = LoHaModel(model.unet, config_unet, "default")
```

**属性**：

+   `model`（`~torch.nn.Module`）— 要适应的模型。

+   `peft_config`（[LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig)）：LoHa模型的配置。
