["```py\n( vocab_size = 513 n_positions = 1024 n_embd = 512 n_layer = 24 n_head = 8 n_inner = None activation_function = 'quick_gelu' resid_pdrop = 0.1 embd_pdrop = 0.1 attn_pdrop = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 scale_attn_weights = True use_cache = True tie_word_embeddings = False scale_attn_by_inverse_layer_idx = False reorder_and_upcast_attn = False **kwargs )\n```", "```py\n>>> from transformers import ImageGPTConfig, ImageGPTModel\n\n>>> # Initializing a ImageGPT configuration\n>>> configuration = ImageGPTConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = ImageGPTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( clusters: Union = None do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_normalize: bool = True do_color_quantize: bool = True **kwargs )\n```", "```py\n( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_normalize: bool = None do_color_quantize: Optional = None clusters: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config: ImageGPTConfig )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, ImageGPTModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"openai/imagegpt-small\")\n>>> model = ImageGPTModel.from_pretrained(\"openai/imagegpt-small\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: ImageGPTConfig )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, ImageGPTForCausalImageModeling\n>>> import torch\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"openai/imagegpt-small\")\n>>> model = ImageGPTForCausalImageModeling.from_pretrained(\"openai/imagegpt-small\")\n>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n>>> model.to(device)\n>>> # unconditional generation of 8 images\n>>> batch_size = 4\n>>> context = torch.full((batch_size, 1), model.config.vocab_size - 1)  # initialize with SOS token\n>>> context = context.to(device)\n>>> output = model.generate(\n...     input_ids=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40\n... )\n\n>>> clusters = image_processor.clusters\n>>> height = image_processor.size[\"height\"]\n>>> width = image_processor.size[\"width\"]\n\n>>> samples = output[:, 1:].cpu().detach().numpy()\n>>> samples_img = [\n...     np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [height, width, 3]).astype(np.uint8) for s in samples\n... ]  # convert color cluster tokens back to pixels\n>>> f, axes = plt.subplots(1, batch_size, dpi=300)\n\n>>> for img, ax in zip(samples_img, axes):\n...     ax.axis(\"off\")\n...     ax.imshow(img)\n```", "```py\n( config: ImageGPTConfig )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, ImageGPTForImageClassification\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"openai/imagegpt-small\")\n>>> model = ImageGPTForImageClassification.from_pretrained(\"openai/imagegpt-small\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```"]