["```py\n>>> from transformers import BlipConfig, BlipModel\n\n>>> # Initializing a BlipConfig with Salesforce/blip-vqa-base style configuration\n>>> configuration = BlipConfig()\n\n>>> # Initializing a BlipPModel (with random weights) from the Salesforce/blip-vqa-base style configuration\n>>> model = BlipModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a BlipConfig from a BlipTextConfig and a BlipVisionConfig\n\n>>> # Initializing a BLIPText and BLIPVision configuration\n>>> config_text = BlipTextConfig()\n>>> config_vision = BlipVisionConfig()\n\n>>> config = BlipConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n>>> from transformers import BlipTextConfig, BlipTextModel\n\n>>> # Initializing a BlipTextConfig with Salesforce/blip-vqa-base style configuration\n>>> configuration = BlipTextConfig()\n\n>>> # Initializing a BlipTextModel (with random weights) from the Salesforce/blip-vqa-base style configuration\n>>> model = BlipTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import BlipVisionConfig, BlipVisionModel\n\n>>> # Initializing a BlipVisionConfig with Salesforce/blip-vqa-base style configuration\n>>> configuration = BlipVisionConfig()\n\n>>> # Initializing a BlipVisionModel (with random weights) from the Salesforce/blip-vqa-base style configuration\n>>> model = BlipVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipModel\n\n>>> model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoProcessor, BlipModel\n\n>>> model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipModel\n\n>>> model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"A picture of\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipForImageTextRetrieval\n\n>>> model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"an image of a cat\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipForQuestionAnswering\n\n>>> model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # training\n>>> text = \"How many cats are in the picture?\"\n>>> label = \"2\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n>>> labels = processor(text=label, return_tensors=\"pt\").input_ids\n\n>>> inputs[\"labels\"] = labels\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n>>> loss.backward()\n\n>>> # inference\n>>> text = \"How many cats are in the picture?\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(processor.decode(outputs[0], skip_special_tokens=True))\n2\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipModel\n\n>>> model = TFBlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"tf\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoProcessor, TFBlipModel\n\n>>> model = TFBlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipModel\n\n>>> model = TFBlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> model = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"A picture of\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipForImageTextRetrieval\n\n>>> model = TFBlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"an image of a cat\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipForQuestionAnswering\n\n>>> model = TFBlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # training\n>>> text = \"How many cats are in the picture?\"\n>>> label = \"2\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n>>> labels = processor(text=label, return_tensors=\"tf\").input_ids\n\n>>> inputs[\"labels\"] = labels\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n\n>>> # inference\n>>> text = \"How many cats are in the picture?\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n>>> outputs = model.generate(**inputs)\n>>> print(processor.decode(outputs[0], skip_special_tokens=True))\n2\n```"]