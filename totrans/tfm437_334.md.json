["```py\n( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 image_text_hidden_size = 256 **kwargs )\n```", "```py\n>>> from transformers import BlipConfig, BlipModel\n\n>>> # Initializing a BlipConfig with Salesforce/blip-vqa-base style configuration\n>>> configuration = BlipConfig()\n\n>>> # Initializing a BlipPModel (with random weights) from the Salesforce/blip-vqa-base style configuration\n>>> model = BlipModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a BlipConfig from a BlipTextConfig and a BlipVisionConfig\n\n>>> # Initializing a BLIPText and BLIPVision configuration\n>>> config_text = BlipTextConfig()\n>>> config_vision = BlipVisionConfig()\n\n>>> config = BlipConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: BlipTextConfig vision_config: BlipVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';BlipConfig\n```", "```py\n( vocab_size = 30524 hidden_size = 768 encoder_hidden_size = 768 intermediate_size = 3072 projection_dim = 768 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 512 hidden_act = 'gelu' layer_norm_eps = 1e-12 hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 bos_token_id = 30522 eos_token_id = 2 pad_token_id = 0 sep_token_id = 102 is_decoder = True use_cache = True **kwargs )\n```", "```py\n>>> from transformers import BlipTextConfig, BlipTextModel\n\n>>> # Initializing a BlipTextConfig with Salesforce/blip-vqa-base style configuration\n>>> configuration = BlipTextConfig()\n\n>>> # Initializing a BlipTextModel (with random weights) from the Salesforce/blip-vqa-base style configuration\n>>> model = BlipTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 image_size = 384 patch_size = 16 hidden_act = 'gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 1e-10 **kwargs )\n```", "```py\n>>> from transformers import BlipVisionConfig, BlipVisionModel\n\n>>> # Initializing a BlipVisionConfig with Salesforce/blip-vqa-base style configuration\n>>> configuration = BlipVisionConfig()\n\n>>> # Initializing a BlipVisionModel (with random weights) from the Salesforce/blip-vqa-base style configuration\n>>> model = BlipVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_convert_rgb: bool = True **kwargs )\n```", "```py\n( images: Union do_resize: Optional = None size: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None do_convert_rgb: bool = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config: BlipConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipModel\n\n>>> model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoProcessor, BlipModel\n\n>>> model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipModel\n\n>>> model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None is_decoder: Optional = False )\n```", "```py\n( config: BlipVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n( config: BlipConfig )\n```", "```py\n( pixel_values: FloatTensor input_ids: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipForConditionalGenerationModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"A picture of\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n```", "```py\n( config: BlipConfig )\n```", "```py\n( input_ids: LongTensor pixel_values: FloatTensor use_itm_head: Optional = True attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipTextVisionModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipForImageTextRetrieval\n\n>>> model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"an image of a cat\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```", "```py\n( config: BlipConfig )\n```", "```py\n( input_ids: LongTensor pixel_values: FloatTensor decoder_input_ids: Optional = None decoder_attention_mask: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipTextVisionModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, BlipForQuestionAnswering\n\n>>> model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # training\n>>> text = \"How many cats are in the picture?\"\n>>> label = \"2\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n>>> labels = processor(text=label, return_tensors=\"pt\").input_ids\n\n>>> inputs[\"labels\"] = labels\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n>>> loss.backward()\n\n>>> # inference\n>>> text = \"How many cats are in the picture?\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(processor.decode(outputs[0], skip_special_tokens=True))\n2\n```", "```py\n( config: BlipConfig *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None pixel_values: tf.Tensor | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None return_loss: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipOutput or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipModel\n\n>>> model = TFBlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"tf\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None return_dict: Optional[bool] = None ) \u2192 export const metadata = 'undefined';text_features (tf.Tensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoProcessor, TFBlipModel\n\n>>> model = TFBlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: tf.Tensor | None = None return_dict: Optional[bool] = None ) \u2192 export const metadata = 'undefined';image_features (tf.Tensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipModel\n\n>>> model = TFBlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config add_pooling_layer = True name = None **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None encoder_embeds: tf.Tensor | None = None encoder_hidden_states: tf.Tensor | None = None encoder_attention_mask: tf.Tensor | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None is_decoder: bool = False training: bool = False )\n```", "```py\n( config: BlipVisionConfig *args **kwargs )\n```", "```py\n( pixel_values: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = None ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)\n```", "```py\n( config: BlipConfig *args **kwargs )\n```", "```py\n( pixel_values: tf.Tensor input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None labels: tf.Tensor | None = None return_dict: Optional[bool] = None training: Optional[bool] = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipForConditionalGenerationModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n>>> model = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"A picture of\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n```", "```py\n( config: BlipConfig *args **kwargs )\n```", "```py\n( input_ids: tf.Tensor pixel_values: tf.Tensor | None = None use_itm_head: Optional[bool] = True attention_mask: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipImageTextMatchingModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipForImageTextRetrieval\n\n>>> model = TFBlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"an image of a cat\"\n\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n```", "```py\n( config: BlipConfig *args **kwargs )\n```", "```py\n( input_ids: tf.Tensor pixel_values: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None attention_mask: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None labels: tf.Tensor | None = None return_dict: Optional[bool] = None training: Optional[bool] = None ) \u2192 export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipTextVisionModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFBlipForQuestionAnswering\n\n>>> model = TFBlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # training\n>>> text = \"How many cats are in the picture?\"\n>>> label = \"2\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n>>> labels = processor(text=label, return_tensors=\"tf\").input_ids\n\n>>> inputs[\"labels\"] = labels\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n\n>>> # inference\n>>> text = \"How many cats are in the picture?\"\n>>> inputs = processor(images=image, text=text, return_tensors=\"tf\")\n>>> outputs = model.generate(**inputs)\n>>> print(processor.decode(outputs[0], skip_special_tokens=True))\n2\n```"]