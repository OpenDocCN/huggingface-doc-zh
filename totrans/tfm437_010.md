# ä½¿ç”¨è„šæœ¬è¿›è¡Œè®­ç»ƒ

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)

é™¤äº†ğŸ¤— Transformersçš„[notebooks](./noteboks/README)ä¹‹å¤–ï¼Œè¿˜æœ‰ç¤ºä¾‹è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ã€[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)æˆ–[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚

æ‚¨è¿˜ä¼šå‘ç°æˆ‘ä»¬åœ¨[ç ”ç©¶é¡¹ç›®](https://github.com/huggingface/transformers/tree/main/examples/research_projects)å’Œ[é—ç•™ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ä¸­ä½¿ç”¨çš„è„šæœ¬ï¼Œè¿™äº›è„šæœ¬å¤§å¤šæ˜¯ç¤¾åŒºè´¡çŒ®çš„ã€‚è¿™äº›è„šæœ¬ç›®å‰æ²¡æœ‰å¾—åˆ°ç§¯æç»´æŠ¤ï¼Œå¹¶ä¸”éœ€è¦ç‰¹å®šç‰ˆæœ¬çš„ğŸ¤— Transformersï¼Œè¿™å¾ˆå¯èƒ½ä¸åº“çš„æœ€æ–°ç‰ˆæœ¬ä¸å…¼å®¹ã€‚

ç¤ºä¾‹è„šæœ¬ä¸æ˜¯æœŸæœ›åœ¨æ¯ä¸ªé—®é¢˜ä¸Šç«‹å³è¿è¡Œï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´è„šæœ¬ä»¥é€‚åº”æ‚¨è¦è§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†å¸®åŠ©æ‚¨ï¼Œå¤§å¤šæ•°è„šæœ¬å®Œå…¨æš´éœ²äº†æ•°æ®é¢„å¤„ç†çš„æ–¹å¼ï¼Œå…è®¸æ‚¨æ ¹æ®éœ€è¦è¿›è¡Œç¼–è¾‘ä»¥é€‚åº”æ‚¨çš„ç”¨ä¾‹ã€‚

å¯¹äºæ‚¨æƒ³åœ¨ç¤ºä¾‹è„šæœ¬ä¸­å®ç°çš„ä»»ä½•åŠŸèƒ½ï¼Œè¯·åœ¨æäº¤æ‹‰å–è¯·æ±‚ä¹‹å‰åœ¨[è®ºå›](https://discuss.huggingface.co/)æˆ–[é—®é¢˜](https://github.com/huggingface/transformers/issues)ä¸­è®¨è®ºã€‚è™½ç„¶æˆ‘ä»¬æ¬¢è¿é”™è¯¯ä¿®å¤ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸å¤ªå¯èƒ½åˆå¹¶å¢åŠ æ›´å¤šåŠŸèƒ½ä½†ç‰ºç‰²å¯è¯»æ€§çš„æ‹‰å–è¯·æ±‚ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åœ¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)å’Œ[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)ä¸­è¿è¡Œä¸€ä¸ªç¤ºä¾‹æ‘˜è¦è®­ç»ƒè„šæœ¬ã€‚é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰ç¤ºä¾‹éƒ½é¢„è®¡èƒ½å¤Ÿåœ¨ä¸¤ä¸ªæ¡†æ¶ä¸­è¿è¡Œã€‚

## è®¾ç½®

è¦æˆåŠŸè¿è¡Œç¤ºä¾‹è„šæœ¬çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæ‚¨å¿…é¡»åœ¨æ–°çš„è™šæ‹Ÿç¯å¢ƒä¸­**ä»æºä»£ç å®‰è£…ğŸ¤— Transformers**ï¼š

```py
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

å¯¹äºæ—§ç‰ˆæœ¬çš„ç¤ºä¾‹è„šæœ¬ï¼Œè¯·ç‚¹å‡»ä¸‹é¢çš„åˆ‡æ¢ï¼š

<details data-svelte-h="svelte-145d76l"><summary>æ—§ç‰ˆæœ¬ğŸ¤— Transformersçš„ç¤ºä¾‹</summary>

+   [v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)

+   [v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)

+   [v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)

+   [v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)

+   [v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)

+   [v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)

+   [v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)

+   [v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)

+   [v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)

+   [v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)

+   [v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)

+   [v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)

+   [v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)

+   [v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)

+   [v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)

+   [v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)

+   [v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)

+   [v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)

+   [v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)

+   [v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)

+   [v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)

+   [v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)

+   [v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)

+   [v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)

+   [v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)

+   [v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)

+   [v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>

Then switch your current clone of ğŸ¤— Transformers to a specific version, like v3.5.1 for example:

```py
git checkout tags/v3.5.1
```

After youâ€™ve setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:

```py
pip install -r requirements.txt
```

## Run a script

PytorchHide Pytorch content

The example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.

```py
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlowHide TensorFlow content

The example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset using Keras on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.

```py
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

## Distributed training and mixed precision

The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) supports distributed training and mixed precision, which means you can also use it in a script. To enable both of these features:

+   Add the `fp16` argument to enable mixed precision.

+   Set the number of GPUs to use with the `nproc_per_node` argument.

```py
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) for distributed training, and you donâ€™t need to add any additional arguments to the training script. The TensorFlow script will use multiple GPUs by default if they are available.

## Run a script on a TPU

PytorchHide Pytorch content

Tensor Processing Units (TPUs) are specifically designed to accelerate performance. PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores` argument to set the number of TPU cores you want to use.

```py
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlowHide TensorFlow content

Tensor Processing Units (TPUs) are specifically designed to accelerate performance. TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu` argument.

```py
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

## Run a script with ğŸ¤— Accelerate

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library that offers a unified method for training a model on several types of setups (CPU-only, multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training loop. Make sure you have ğŸ¤— Accelerate installed if you donâ€™t already have it:

> Note: As Accelerate is rapidly developing, the git version of accelerate must be installed to run the scripts
> 
> ```py
> pip install git+https://github.com/huggingface/accelerate
> ```

ä½ éœ€è¦ä½¿ç”¨`run_summarization_no_trainer.py`è„šæœ¬ï¼Œè€Œä¸æ˜¯`run_summarization.py`è„šæœ¬ã€‚ğŸ¤— åŠ é€Ÿæ”¯æŒçš„è„šæœ¬å°†åœ¨æ–‡ä»¶å¤¹ä¸­æœ‰ä¸€ä¸ª`task_no_trainer.py`æ–‡ä»¶ã€‚é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºå¹¶ä¿å­˜ä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼š

```py
accelerate config
```

æµ‹è¯•ä½ çš„è®¾ç½®ä»¥ç¡®ä¿é…ç½®æ­£ç¡®ï¼š

```py
accelerate test
```

ç°åœ¨ä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒäº†ï¼š

```py
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

æ‘˜è¦è„šæœ¬æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œåªè¦å®ƒä»¬æ˜¯CSVæˆ–JSON Lineæ–‡ä»¶ã€‚å½“ä½ ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†æ—¶ï¼Œä½ éœ€è¦æŒ‡å®šå‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼š

+   `train_file`å’Œ`validation_file`æŒ‡å®šäº†ä½ çš„è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `text_column`æ˜¯è¦æ€»ç»“çš„è¾“å…¥æ–‡æœ¬ã€‚

+   `summary_column`æ˜¯è¦è¾“å‡ºçš„ç›®æ ‡æ–‡æœ¬ã€‚

ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†çš„æ‘˜è¦è„šæœ¬å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## æµ‹è¯•ä¸€ä¸ªè„šæœ¬

åœ¨æ‰¿è¯ºå®Œæ•´æ•°æ®é›†ä¹‹å‰ï¼Œæœ€å¥½å…ˆåœ¨è¾ƒå°‘æ•°é‡çš„æ•°æ®é›†ç¤ºä¾‹ä¸Šè¿è¡Œä½ çš„è„šæœ¬ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œã€‚ä½¿ç”¨ä»¥ä¸‹å‚æ•°å°†æ•°æ®é›†æˆªæ–­ä¸ºæœ€å¤§æ ·æœ¬æ•°ï¼š

+   `max_train_samples`

+   `max_eval_samples`

+   `max_predict_samples`

```py
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

å¹¶éæ‰€æœ‰ç¤ºä¾‹è„šæœ¬éƒ½æ”¯æŒ`max_predict_samples`å‚æ•°ã€‚å¦‚æœä½ ä¸ç¡®å®šä½ çš„è„šæœ¬æ˜¯å¦æ”¯æŒè¿™ä¸ªå‚æ•°ï¼Œæ·»åŠ `-h`å‚æ•°è¿›è¡Œæ£€æŸ¥ï¼š

```py
examples/pytorch/summarization/run_summarization.py -h
```

## ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

å¦ä¸€ä¸ªæœ‰ç”¨çš„é€‰é¡¹æ˜¯ä»å…ˆå‰çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚è¿™å°†ç¡®ä¿ä½ å¯ä»¥åœ¨ä¸­æ–­è®­ç»ƒåç»§ç»­è¿›è¡Œï¼Œè€Œä¸å¿…é‡æ–°å¼€å§‹ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚

ç¬¬ä¸€ç§æ–¹æ³•ä½¿ç”¨`output_dir previous_output_dir`å‚æ•°ä»`output_dir`ä¸­å­˜å‚¨çš„æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥åˆ é™¤`overwrite_output_dir`ï¼š

```py
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨`resume_from_checkpoint path_to_specific_checkpoint`å‚æ•°ä»ç‰¹å®šæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹æ¢å¤è®­ç»ƒã€‚

```py
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## åˆ†äº«ä½ çš„æ¨¡å‹

æ‰€æœ‰è„šæœ¬éƒ½å¯ä»¥å°†ä½ çš„æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ°[æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ã€‚ç¡®ä¿åœ¨å¼€å§‹ä¹‹å‰å·²ç»ç™»å½•åˆ°Hugging Faceï¼š

```py
huggingface-cli login
```

ç„¶ååœ¨è„šæœ¬ä¸­æ·»åŠ `push_to_hub`å‚æ•°ã€‚è¿™ä¸ªå‚æ•°å°†åˆ›å»ºä¸€ä¸ªå­˜å‚¨åº“ï¼Œå…¶ä¸­åŒ…å«ä½ çš„Hugging Faceç”¨æˆ·åå’Œ`output_dir`ä¸­æŒ‡å®šçš„æ–‡ä»¶å¤¹åç§°ã€‚

ç»™ä½ çš„å­˜å‚¨åº“èµ·ä¸€ä¸ªç‰¹å®šçš„åç§°ï¼Œä½¿ç”¨`push_to_hub_model_id`å‚æ•°æ·»åŠ å®ƒã€‚å­˜å‚¨åº“å°†è‡ªåŠ¨åˆ—åœ¨ä½ çš„å‘½åç©ºé—´ä¸‹ã€‚

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä¸Šä¼ å…·æœ‰ç‰¹å®šå­˜å‚¨åº“åç§°çš„æ¨¡å‹ï¼š

```py
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
