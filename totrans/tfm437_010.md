# 使用脚本进行训练

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)

除了🤗 Transformers的[notebooks](./noteboks/README)之外，还有示例脚本演示如何使用[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)、[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)或[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)训练模型的方法。

您还会发现我们在[研究项目](https://github.com/huggingface/transformers/tree/main/examples/research_projects)和[遗留示例](https://github.com/huggingface/transformers/tree/main/examples/legacy)中使用的脚本，这些脚本大多是社区贡献的。这些脚本目前没有得到积极维护，并且需要特定版本的🤗 Transformers，这很可能与库的最新版本不兼容。

示例脚本不是期望在每个问题上立即运行，您可能需要调整脚本以适应您要解决的问题。为了帮助您，大多数脚本完全暴露了数据预处理的方式，允许您根据需要进行编辑以适应您的用例。

对于您想在示例脚本中实现的任何功能，请在提交拉取请求之前在[论坛](https://discuss.huggingface.co/)或[问题](https://github.com/huggingface/transformers/issues)中讨论。虽然我们欢迎错误修复，但是我们不太可能合并增加更多功能但牺牲可读性的拉取请求。

本指南将向您展示如何在[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)和[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)中运行一个示例摘要训练脚本。除非另有说明，所有示例都预计能够在两个框架中运行。

## 设置

要成功运行示例脚本的最新版本，您必须在新的虚拟环境中**从源代码安装🤗 Transformers**：

```py
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

对于旧版本的示例脚本，请点击下面的切换：

<details data-svelte-h="svelte-145d76l"><summary>旧版本🤗 Transformers的示例</summary>

+   [v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)

+   [v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)

+   [v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)

+   [v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)

+   [v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)

+   [v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)

+   [v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)

+   [v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)

+   [v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)

+   [v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)

+   [v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)

+   [v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)

+   [v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)

+   [v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)

+   [v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)

+   [v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)

+   [v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)

+   [v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)

+   [v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)

+   [v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)

+   [v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)

+   [v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)

+   [v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)

+   [v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)

+   [v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)

+   [v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)

+   [v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>

Then switch your current clone of 🤗 Transformers to a specific version, like v3.5.1 for example:

```py
git checkout tags/v3.5.1
```

After you’ve setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:

```py
pip install -r requirements.txt
```

## Run a script

PytorchHide Pytorch content

The example script downloads and preprocesses a dataset from the 🤗 [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.

```py
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlowHide TensorFlow content

The example script downloads and preprocesses a dataset from the 🤗 [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset using Keras on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.

```py
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

## Distributed training and mixed precision

The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) supports distributed training and mixed precision, which means you can also use it in a script. To enable both of these features:

+   Add the `fp16` argument to enable mixed precision.

+   Set the number of GPUs to use with the `nproc_per_node` argument.

```py
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) for distributed training, and you don’t need to add any additional arguments to the training script. The TensorFlow script will use multiple GPUs by default if they are available.

## Run a script on a TPU

PytorchHide Pytorch content

Tensor Processing Units (TPUs) are specifically designed to accelerate performance. PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores` argument to set the number of TPU cores you want to use.

```py
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlowHide TensorFlow content

Tensor Processing Units (TPUs) are specifically designed to accelerate performance. TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu` argument.

```py
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

## Run a script with 🤗 Accelerate

🤗 [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library that offers a unified method for training a model on several types of setups (CPU-only, multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training loop. Make sure you have 🤗 Accelerate installed if you don’t already have it:

> Note: As Accelerate is rapidly developing, the git version of accelerate must be installed to run the scripts
> 
> ```py
> pip install git+https://github.com/huggingface/accelerate
> ```

你需要使用`run_summarization_no_trainer.py`脚本，而不是`run_summarization.py`脚本。🤗 加速支持的脚本将在文件夹中有一个`task_no_trainer.py`文件。首先运行以下命令创建并保存一个配置文件：

```py
accelerate config
```

测试你的设置以确保配置正确：

```py
accelerate test
```

现在你已经准备好开始训练了：

```py
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## 使用自定义数据集

摘要脚本支持自定义数据集，只要它们是CSV或JSON Line文件。当你使用自己的数据集时，你需要指定几个额外的参数：

+   `train_file`和`validation_file`指定了你的训练和验证文件的路径。

+   `text_column`是要总结的输入文本。

+   `summary_column`是要输出的目标文本。

使用自定义数据集的摘要脚本将如下所示：

```py
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## 测试一个脚本

在承诺完整数据集之前，最好先在较少数量的数据集示例上运行你的脚本，以确保一切按预期工作。使用以下参数将数据集截断为最大样本数：

+   `max_train_samples`

+   `max_eval_samples`

+   `max_predict_samples`

```py
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

并非所有示例脚本都支持`max_predict_samples`参数。如果你不确定你的脚本是否支持这个参数，添加`-h`参数进行检查：

```py
examples/pytorch/summarization/run_summarization.py -h
```

## 从检查点恢复训练

另一个有用的选项是从先前的检查点恢复训练。这将确保你可以在中断训练后继续进行，而不必重新开始。有两种方法可以从检查点恢复训练。

第一种方法使用`output_dir previous_output_dir`参数从`output_dir`中存储的最新检查点恢复训练。在这种情况下，你应该删除`overwrite_output_dir`：

```py
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

第二种方法使用`resume_from_checkpoint path_to_specific_checkpoint`参数从特定检查点文件夹恢复训练。

```py
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## 分享你的模型

所有脚本都可以将你的最终模型上传到[模型中心](https://huggingface.co/models)。确保在开始之前已经登录到Hugging Face：

```py
huggingface-cli login
```

然后在脚本中添加`push_to_hub`参数。这个参数将创建一个存储库，其中包含你的Hugging Face用户名和`output_dir`中指定的文件夹名称。

给你的存储库起一个特定的名称，使用`push_to_hub_model_id`参数添加它。存储库将自动列在你的命名空间下。

以下示例展示了如何上传具有特定存储库名称的模型：

```py
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
