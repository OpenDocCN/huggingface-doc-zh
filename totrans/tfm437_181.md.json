["```py\n>>> from transformers import AutoModel, AutoTokenizer\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").cuda()\n>>> x_tok = tokenizer(\"\u306f\u3001\", prefix_text=\"\u7e54\u7530\u4fe1\u9577\", return_tensors=\"pt\")\n>>> torch.manual_seed(0)\n>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)\n>>> tokenizer.decode(gen_tok[0])\n'\u7e54\u7530\u4fe1\u9577\u306f\u30012004\u5e74\u306b\u300e\u6226\u56fdBASARA\u300f\u306e\u305f\u3081\u306b\u3001\u8c4a\u81e3\u79c0\u5409'\n```", "```py\n( vocab_size = 36000 max_position_embeddings = 1280 d_model = 1024 d_ff = 8192 d_ext = 4096 d_spout = 128 num_switch_layers = 10 num_ext_layers = 0 num_heads = 16 num_experts = 16 expert_capacity = 128 dropout_rate = 0.0 layer_norm_epsilon = 1e-05 router_bias = False router_jitter_noise = 0.0 router_dtype = 'float32' router_ignore_padding_tokens = False output_hidden_states = False output_attentions = False initializer_factor = 0.002 output_router_logits = False use_cache = True separator_token_id = 35998 pad_token_id = 35995 eos_token_id = 35999 **kwargs )\n```", "```py\n( vocab_file emoji_file unk_token = '<|nottoken|>' pad_token = '<|separator|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' sep_token = '<|segmenter|>' do_clean_text = False **kwargs )\n```", "```py\n>>> from transformers import GPTSanJapaneseTokenizer\n\n>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> # You can confirm both \u6176\u5fdc and \u6176\u61c9 are encoded to 17750\n>>> tokenizer(\"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\u5b9f\u306f\u6176\u5fdc(\u6176\u61c9)\u5927\u5b66\u51fa\u8eab\")[\"input_ids\"]\n[35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]\n\n>>> # Both \u6176\u5fdc and \u6176\u61c9 are decoded to \u6176\u5fdc\n>>> tokenizer.decode(tokenizer(\"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\u5b9f\u306f\u6176\u5fdc(\u6176\u61c9)\u5927\u5b66\u51fa\u8eab\")[\"input_ids\"])\n'\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\u5b9f\u306f\u6176\u5fdc(\u6176\u5fdc)\u5927\u5b66\u51fa\u8eab'\n```", "```py\n>>> from transformers import GPTSanJapaneseTokenizer\n\n>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> tokenizer(\"\u5b9f\u306f\u6176\u5fdc(\u6176\u61c9)\u5927\u5b66\u51fa\u8eab\", prefix_text=\"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\")[\"input_ids\"]\n[35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]\n\n>>> # Mask for Prefix-LM inputs\n>>> tokenizer(\"\u5b9f\u306f\u6176\u5fdc(\u6176\u61c9)\u5927\u5b66\u51fa\u8eab\", prefix_text=\"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\")[\"token_type_ids\"]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n```", "```py\n>>> from transformers import GPTSanJapaneseTokenizer\n\n>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> tokenizer([[\"\u6b66\u7530\u4fe1\u7384\", \"\u306f\u3001\"], [\"\u7e54\u7530\u4fe1\u9577\", \"\u306e\u914d\u4e0b\u306e\u3001\"]], padding=True)[\"input_ids\"]\n[[35993, 8640, 25948, 35998, 30647, 35675, 35999, 35999], [35993, 10382, 9868, 35998, 30646, 9459, 30646, 35675]]\n\n>>> # Mask for Prefix-LM inputs\n>>> tokenizer([[\"\u6b66\u7530\u4fe1\u7384\", \"\u306f\u3001\"], [\"\u7e54\u7530\u4fe1\u9577\", \"\u306e\u914d\u4e0b\u306e\u3001\"]], padding=True)[\"token_type_ids\"]\n[[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0]]\n\n>>> # Mask for padding\n>>> tokenizer([[\"\u6b66\u7530\u4fe1\u7384\", \"\u306f\u3001\"], [\"\u7e54\u7530\u4fe1\u9577\", \"\u306e\u914d\u4e0b\u306e\u3001\"]], padding=True)[\"attention_mask\"]\n[[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None )\n```", "```py\n>>> from transformers import GPTSanJapaneseTokenizer\n\n>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\uff71\uff72\uff73\uff74\")\n>>> # input_ids:      | SOT | SEG | \uff71 | \uff72 | \uff73 | \uff74 |\n>>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\n\n>>> x_token = tokenizer(\"\", prefix_text=\"\uff71\uff72\uff73\uff74\")\n>>> # input_ids:      | SOT | \uff71 | \uff72 | \uff73 | \uff74 | SEG |\n>>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\n\n>>> x_token = tokenizer(\"\uff73\uff74\", prefix_text=\"\uff71\uff72\")\n>>> # input_ids:      | SOT | \uff71 | \uff72 | SEG | \uff73 | \uff74 |\n>>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\n```", "```py\n( config: GPTSanJapaneseConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None spout: Optional = None past_key_values: Optional = None head_mask: Optional = None use_cache: Optional = False inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None output_router_logits: Optional = None num_precontext: Optional = None )\n```", "```py\n( config: GPTSanJapaneseConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None spout: Optional = None past_key_values: Optional = None head_mask: Optional = None use_cache: Optional = False inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None output_router_logits: Optional = None labels: Optional = None )\n```", "```py\n>>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\", return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> gen_token = model.generate(input_ids, max_new_tokens=50)\n>>> tokenizer.decode(gen_token[0])\n\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\u653f\u6cbb\u30fb\u8ecd\u4e8b\u306e\u4e2d\u67a2\u307e\u3067\u638c\u63e1\u3057\u305f\u653f\u6cbb\u5bb6\u3067\u3042\u308a\u3001\u65e5\u672c\u53f2\u4e0a\u985e\u3092\u898b\u306a\u3044\u9a5a\u7570\u7684\u306a\u8ecd\u4e8b\u4fb5\u653b\u3092\u7d9a\u3051...\"\n```", "```py\n>>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\", prefix_text=\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\", return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> token_type_ids = x_token.token_type_ids.to(device)\n>>> gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n>>> tokenizer.decode(gen_token[0])\n\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\u653f\u6cbb\u30fb\u5916\u4ea4\u3067\u6570\u3005\u306e\u6226\u679c\u3092\u4e0a\u3052\u308b\u304c\u30011568\u5e74\u304b\u3089\u306f\u3001\u3044\u308f\u3086\u308b\u672c\u80fd\u5bfa\u306e\u5909\u3067\u7d30\u5ddd\u6674\u5143\u306b\u6697\u6bba\u3055\u308c\u308b...\"\n```", "```py\n>>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> masked_sentence = \"\u6b66\u7530\u4fe1\u7384\u306f\u3001<|inputmask|>\u6642\u4ee3\u30d5\u30a1\u30f3\u306a\u3089\u305c\u3072\u62bc\u3055\u3048<|inputmask|>\u304d\u305f\u3044\u540d\u5c06\u306e\u4e00\u4eba\u3002\"\n>>> x_token = tokenizer(\"\", prefix_text=masked_sentence, return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> token_type_ids = x_token.token_type_ids.to(device)\n>>> out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n>>> out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-1)\n>>> tokenizer.decode(out_mlm_token[0])\n\"\u6b66\u7530\u4fe1\u7384\u306f\u3001\u6226\u56fd\u6642\u4ee3\u30d5\u30a1\u30f3\u306a\u3089\u305c\u3072\u62bc\u3055\u3048\u3066\u304a\u304d\u305f\u3044\u540d\u5c06\u306e\u4e00\u4eba\u3002\"\n\n>>> tokenizer.decode(out_lm_token[0][input_ids.shape[1] :])\n\"\u6b66\u7530\u6c0f\u306e\u4e09\u4ee3\u306b\u6e21\u3063\u305f\u6b66\u7530\u5bb6\u306e\u3072\u3068\u308a\\n\u7532\u6590\u5e02\u306b\u4f4f\u3080\u3001\u65e5\u672c\u53f2\u4e0a\u6700\u5927\u306e\u6226\u56fd\u5927\u540d\u3002...\"\n```"]