["```py\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\ninputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\ngeneration_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n```", "```py\ngeneration_output[:2]\n```", "```py\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> inputs = processor(\n...     text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n\n>>> inputs = tokenizer(\"Alice: I love cats. What do you love?\\nBob:\", return_tensors=\"pt\")\n\n>>> # With greedy decoding, we see Bob repeating Alice's opinion. If Bob was a chatbot, it would be a poor one.\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nAlice: I love cats. What do you love?\nBob: I love cats. What do you\n\n>>> # With this logits processor, we can prevent Bob from repeating Alice's opinion.\n>>> outputs = model.generate(**inputs, encoder_no_repeat_ngram_size=2)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nAlice: I love cats. What do you love?\nBob: My cats are very cute.\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n\n>>> inputs = tokenizer([\"Alice and Bob. The third member's name was\"], return_tensors=\"pt\")\n>>> gen_out = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nAlice and Bob. The third member's name was not mentioned.\n\n>>> # With the `encoder_repetition_penalty` argument we can trigger this logits processor in `generate`, which can\n>>> # promote the use of prompt tokens (\"Bob\" in this example)\n>>> gen_out = model.generate(**inputs, encoder_repetition_penalty=1.2)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nAlice and Bob. The third member's name was Bob. The third member's name was Bob.\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> set_seed(0)\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n>>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n\n>>> # With sampling, the output is unexpected -- sometimes too unexpected.\n>>> outputs = model.generate(**inputs, do_sample=True)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: 1, 2, 0, 2, 2. 2, 2, 2, 2\n\n>>> # With epsilon sampling, the output gets restricted to high-probability tokens. Note that this is similar to\n>>> # Top P sampling, which restricts tokens based on their cumulative probability.\n>>> # Pro tip: The paper recomends using `epsilon_cutoff` values between 3e-4 and 9e-4\n>>> outputs = model.generate(**inputs, do_sample=True, epsilon_cutoff=0.1)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> set_seed(0)\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n>>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n\n>>> # With sampling, the output is unexpected -- sometimes too unexpected.\n>>> outputs = model.generate(**inputs, do_sample=True)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: 1, 2, 0, 2, 2. 2, 2, 2, 2\n\n>>> # With eta sampling, the output gets restricted to high-probability tokens. You can see it as a dynamic form of\n>>> # epsilon sampling that adapts its cutoff probability based on the entropy (high entropy = lower cutoff).\n>>> # Pro tip: The paper recomends using `eta_cutoff` values between 3e-4 to 4e-3\n>>> outputs = model.generate(**inputs, do_sample=True, eta_cutoff=0.1)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n>>> text = \"Just wanted to let you know, I\"\n>>> inputs = tokenizer(text, return_tensors=\"pt\")\n\n>>> # Let's consider that we want short sentences, so we limit `max_length=30`. However, we observe that the answer\n>>> # tends to end abruptly.\n>>> set_seed(1)\n>>> outputs = model.generate(**inputs, do_sample=True, temperature=0.9, max_length=30, pad_token_id=50256)\n>>> print(tokenizer.batch_decode(outputs)[0])\nJust wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was\npublished in 2010. Although\n\n>>> # To promote the appearance of the EOS token at the right time, we add the `exponential_decay_length_penalty =\n>>> # (start_index, decay_factor)`. Instead of cutting at max_tokens, the output comes to an end before and usually\n>>> # with more meaning. What happens is that starting from `start_index` the EOS token score will be increased\n>>> # by `decay_factor` exponentially. However, if you set a high decay factor, you may also end up with abruptly\n>>> # ending sequences.\n>>> set_seed(1)\n>>> outputs = model.generate(\n...     **inputs,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=30,\n...     pad_token_id=50256,\n...     exponential_decay_length_penalty=(15, 1.6),\n... )\n>>> print(tokenizer.batch_decode(outputs)[0])\nJust wanted to let you know, I received a link to an ebook, the book How To Start A Social Network\nwhich<|endoftext|>\n\n>>> # With a small decay factor, you will have a higher chance of getting a meaningful sequence.\n>>> set_seed(1)\n>>> outputs = model.generate(\n...     **inputs,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=30,\n...     pad_token_id=50256,\n...     exponential_decay_length_penalty=(15, 1.01),\n... )\n>>> print(tokenizer.batch_decode(outputs)[0])\nJust wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was\npublished in 2010.<|endoftext|>\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n\n>>> inputs = tokenizer(\"Translate from English to German: I love cats.\", return_tensors=\"pt\")\n\n>>> # By default, it continues generating according to the model's logits\n>>> outputs = model.generate(**inputs, max_new_tokens=10)\n>>> print(tokenizer.batch_decode(outputs)[0])\n<pad> Ich liebe Kitty.</s>\n\n>>> # We can use `forced_bos_token_id` to force the start of generation with an encoder-decoder model\n>>> # (including forcing it to end straight away with an EOS token)\n>>> outputs = model.generate(**inputs, max_new_tokens=10, forced_bos_token_id=tokenizer.eos_token_id)\n>>> print(tokenizer.batch_decode(outputs)[0])\n<pad></s>\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n>>> inputs = tokenizer(\"A sequence: 1, 2, 3\", return_tensors=\"pt\")\n\n>>> # By default, it continues generating according to the model's logits\n>>> outputs = model.generate(**inputs, max_new_tokens=10)\n>>> print(tokenizer.batch_decode(outputs)[0])\nA sequence: 1, 2, 3, 4, 5, 6, 7, 8\n\n>>> # `forced_eos_token_id` ensures the generation ends with a EOS token\n>>> outputs = model.generate(**inputs, max_new_tokens=10, forced_eos_token_id=tokenizer.eos_token_id)\n>>> print(tokenizer.batch_decode(outputs)[0])\nA sequence: 1, 2, 3, 4, 5, 6, 7,<|endoftext|>\n```", "```py\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n\n>>> # This Whisper model forces the generation to start with `50362` at the first position by default, i.e.\n>>> # `\"forced_decoder_ids\": [[1, 50362]]`. This means all other tokens are masked out.\n>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n>>> print(\n...     all(outputs.scores[0][0, i] == float(\"-inf\") for i in range(processor.tokenizer.vocab_size) if i != 50362)\n... )\nTrue\n>>> print(outputs.scores[0][0, 50362])\ntensor(0.)\n\n>>> # If we disable `forced_decoder_ids`, we stop seeing that effect\n>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, forced_decoder_ids=None)\n>>> print(\n...     all(outputs.scores[0][0, i] == float(\"-inf\") for i in range(processor.tokenizer.vocab_size) if i != 50362)\n... )\nFalse\n>>> print(outputs.scores[0][0, 50362])\ntensor(19.3140)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> # Initialize the model and tokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n>>> # A long text about the solar system\n>>> text = (\n...     \"The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, \"\n...     \"either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight \"\n...     \"planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System \"\n...     \"bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant \"\n...     \"interstellar molecular cloud.\"\n... )\n>>> inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\")\n\n>>> # Generate diverse summary\n>>> outputs_diverse = model.generate(\n...     **inputs,\n...     num_beam_groups=2,\n...     diversity_penalty=10.0,\n...     max_length=100,\n...     num_beams=4,\n...     num_return_sequences=2,\n... )\n>>> summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)\n\n>>> # Generate non-diverse summary\n>>> outputs_non_diverse = model.generate(\n...     **inputs,\n...     max_length=100,\n...     num_beams=4,\n...     num_return_sequences=2,\n... )\n>>> summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)\n\n>>> # With `diversity_penalty`, the resulting beams are much more diverse\n>>> print(summary_non_diverse)\n['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',\n'the Solar System formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.']\n\n>>> print(summaries_diverse)\n['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',\n'the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets. the rest of the objects are smaller objects, such as the five dwarf planets and small solar system bodies.']\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n>>> inputs = tokenizer(\"A sequence: 1, 2, 3\", return_tensors=\"pt\")\n\n>>> # By default, the scores are not normalized -- the sum of their exponentials is NOT a normalized probability\n>>> # distribution, summing to 1\n>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n>>> print(torch.sum(torch.exp(outputs.scores[-1])))\ntensor(816.3250)\n\n>>> # Normalizing them may have a positive impact on beam methods, or when using the scores on your application\n>>> outputs = model.generate(**inputs, renormalize_logits=True, return_dict_in_generate=True, output_scores=True)\n>>> print(torch.sum(torch.exp(outputs.scores[-1])))\ntensor(1.0000)\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n\n>>> inputs = tokenizer(\"A number:\", return_tensors=\"pt\")\n>>> gen_out = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nA number: one\n\n>>> # setting `min_length` to a value smaller than the uncontrolled output length has no impact\n>>> gen_out = model.generate(**inputs, min_length=3)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nA number: one\n\n>>> # setting a larger `min_length` will force the model to generate beyond its natural ending point, which is not\n>>> # necessarily incorrect\n>>> gen_out = model.generate(**inputs, min_length=10)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nA number: one thousand, nine hundred and ninety-four\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n\n>>> inputs = tokenizer([\"A number:\"], return_tensors=\"pt\")\n>>> gen_out = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nA number: one\n\n>>> # setting `min_new_tokens` will force the model to generate beyond its natural ending point, which is not\n>>> # necessarily incorrect\n>>> gen_out = model.generate(**inputs, min_new_tokens=2)\n>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\nA number: one thousand\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> inputs = tokenizer([\"In a word, the cake is a\"], return_tensors=\"pt\")\n\n>>> output_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n>>> print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])\nIn a word, the cake is a bit of a mess.\n\n>>> # Now let's take the bad words out. Please note that the tokenizer is initialized differently\n>>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"gpt2\", add_prefix_space=True)\n\n>>> def get_tokens_as_list(word_list):\n...     \"Converts a sequence of words into a list of tokens\"\n...     tokens_list = []\n...     for word in word_list:\n...         tokenized_word = tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]\n...         tokens_list.append(tokenized_word)\n...     return tokens_list\n\n>>> bad_words_ids = get_tokens_as_list(word_list=[\"mess\"])\n>>> output_ids = model.generate(\n...     inputs[\"input_ids\"], max_new_tokens=5, bad_words_ids=bad_words_ids, pad_token_id=tokenizer.eos_token_id\n... )\n>>> print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])\nIn a word, the cake is a bit of a surprise.\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n>>> inputs = tokenizer([\"Today I\"], return_tensors=\"pt\")\n\n>>> output = model.generate(**inputs)\n>>> print(tokenizer.decode(output[0], skip_special_tokens=True))\nToday I\u2019m not sure if I\u2019m going to be able to do it.\n\n>>> # Now let's add ngram size using `no_repeat_ngram_size`. This stops the repetitions (\"I\u2019m\") in the output.\n>>> output = model.generate(**inputs, no_repeat_ngram_size=2)\n>>> print(tokenizer.decode(output[0], skip_special_tokens=True))\nToday I\u2019m not sure if I can get a better understanding of the nature of this issue\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n\n>>> inputs = tokenizer(\"Alice and Bob\", return_tensors=\"pt\")\n\n>>> # By default, it continues generating according to the model's logits\n>>> outputs = model.generate(**inputs, max_new_tokens=5)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nAlice and Bob are friends\n\n>>> # We can contrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.\n>>> # For instance, we can force an entire entity to be generated when its beginning is detected.\n>>> entity =  tokenizer(\" Bob Marley\", return_tensors=\"pt\").input_ids[0]  # 3 tokens\n>>> def prefix_allowed_tokens_fn(batch_id, input_ids):\n...     '''\n...     Attempts to generate 'Bob Marley' when 'Bob' is detected.\n...     In this case, `batch_id` is not used, but you can set rules for each batch member.\n...     '''\n...     if input_ids[-1] == entity[0]:\n...         return entity[1]\n...     elif input_ids[-2] == entity[0] and input_ids[-1] == entity[1]:\n...         return entity[2]\n...     return list(range(tokenizer.vocab_size))  # If no match, allow all tokens\n\n>>> outputs = model.generate(**inputs, max_new_tokens=5, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nAlice and Bob Marley\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> # Initializing the model and tokenizer for it\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n>>> inputs = tokenizer([\"I'm not going to\"], return_tensors=\"pt\")\n\n>>> # This shows a normal generate without any specific parameters\n>>> summary_ids = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\nI'm not going to be able to do that. I'm going to be able to do that\n\n>>> # This generates a penalty for repeated tokens\n>>> penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n>>> print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])\nI'm not going to be able to do that. I'll just have to go out and play\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> inputs = tokenizer([\"The full name of Donald is Donald\"], return_tensors=\"pt\")\n\n>>> summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4)\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\nThe full name of Donald is Donald J. Trump Jr\n\n>>> # Now let's control generation through a bias. Please note that the tokenizer is initialized differently!\n>>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"gpt2\", add_prefix_space=True)\n\n>>> def get_tokens_as_tuple(word):\n...     return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])\n\n>>> # If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\n>>> sequence_bias = {get_tokens_as_tuple(\"Trump\"): -10.0}\n>>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, sequence_bias=sequence_bias)\n>>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\nThe full name of Donald is Donald J. Donald,\n\n>>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n>>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\nThe full name of Donald is Donald Rumsfeld,\n\n>>> # We can also add a positive bias to nudge the model towards specific tokens or continuations\n>>> sequence_bias = {get_tokens_as_tuple(\"Donald Duck\"): 10.0}\n>>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n>>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\nThe full name of Donald is Donald Duck.\n```", "```py\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n\n>>> # Whisper has `begin_suppress_tokens` set by default (= `[220, 50256]`). 50256 is the EOS token, so this means\n>>> # it can't generate and EOS token in the first iteration, but it can in the others.\n>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n>>> print(outputs.scores[1][0, 50256])  # 1 (and not 0) is the first freely generated token\ntensor(-inf)\n>>> print(outputs.scores[-1][0, 50256])  # in other places we can see some probability mass for EOS\ntensor(29.9010)\n\n>>> # If we disable `begin_suppress_tokens`, we can generate EOS in the first iteration.\n>>> outputs = model.generate(\n...     **inputs, return_dict_in_generate=True, output_scores=True, begin_suppress_tokens=None\n... )\n>>> print(outputs.scores[1][0, 50256])\ntensor(11.2027)\n```", "```py\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n\n>>> # Whisper has a long list of suppressed tokens. For instance, in this case, the token 1 is suppressed by default.\n>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n>>> print(outputs.scores[1][0, 1])  # 1 (and not 0) is the first freely generated token\ntensor(-inf)\n\n>>> # If we disable `suppress_tokens`, we can generate it.\n>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, suppress_tokens=None)\n>>> print(outputs.scores[1][0, 1])\ntensor(5.7738)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> set_seed(0)  # for reproducibility\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> model.config.pad_token_id = model.config.eos_token_id\n>>> inputs = tokenizer([\"Hugging Face Company is\"], return_tensors=\"pt\")\n\n>>> # With temperature=1.0, the default, we consistently get random outputs due to random sampling.\n>>> generate_kwargs = {\"max_new_tokens\": 10, \"do_sample\": True, \"temperature\": 1.0, \"num_return_sequences\": 2}\n>>> outputs = model.generate(**inputs, **generate_kwargs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Hugging Face Company is a joint venture between GEO Group, one of',\n'Hugging Face Company is not an exact science \u2013 but what we believe does']\n\n>>> # However, with temperature close to 0, it approximates greedy decoding strategies (invariant)\n>>> generate_kwargs[\"temperature\"] = 0.0001\n>>> outputs = model.generate(**inputs, **generate_kwargs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Hugging Face Company is a company that has been around for over 20 years',\n'Hugging Face Company is a company that has been around for over 20 years']\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> set_seed(0)\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n>>> inputs = tokenizer(\"A sequence: A, B, C, D\", return_tensors=\"pt\")\n\n>>> # With sampling, the output is unexpected -- sometimes too unexpected.\n>>> outputs = model.generate(**inputs, do_sample=True)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: A, B, C, D, G, H, I. A, M\n\n>>> # With `top_k` sampling, the output gets restricted the k most likely tokens.\n>>> # Pro tip: In practice, LLMs use `top_k` in the 5-50 range.\n>>> outputs = model.generate(**inputs, do_sample=True, top_k=2)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: A, B, C, D, E, F, G, H, I\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> set_seed(0)\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n>>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n\n>>> # With sampling, the output is unexpected -- sometimes too unexpected.\n>>> outputs = model.generate(**inputs, do_sample=True)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: 1, 2, 0, 2, 2. 2, 2, 2, 2\n\n>>> # With `top_p` sampling, the output gets restricted to high-probability tokens.\n>>> # Pro tip: In practice, LLMs use `top_p` in the 0.9-0.95 range.\n>>> outputs = model.generate(**inputs, do_sample=True, top_p=0.1)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\nA sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n\n>>> inputs = tokenizer(\"1, 2, 3\", return_tensors=\"pt\")\n\n>>> # We can see that greedy decoding produces a sequence of numbers\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n\n>>> # For this particular seed, we can see that sampling produces nearly the same low-information (= low entropy)\n>>> # sequence\n>>> set_seed(18)\n>>> outputs = model.generate(**inputs, do_sample=True)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n1, 2, 3, 4, 5, 6, 7, 8, 9 and 10\n\n>>> # With `typical_p` set, the most obvious sequence is no longer produced, which may be good for your problem\n>>> set_seed(18)\n>>> outputs = model.generate(\n...     **inputs, do_sample=True, typical_p=0.1, return_dict_in_generate=True, output_scores=True\n... )\n>>> print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0])\n1, 2, 3 and 5\n\n>>> # We can see that the token corresponding to \"4\" (token 934) in the second position, the most likely token\n>>> # as seen with greedy decoding, was entirely blocked out\n>>> print(outputs.scores[1][0, 934])\ntensor(-inf)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> inputs = tokenizer([\"Today, a dragon flew over Paris, France,\"], return_tensors=\"pt\")\n>>> out = model.generate(inputs[\"input_ids\"], guidance_scale=1.5)\n>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n'Today, a dragon flew over Paris, France, killing at least 50 people and injuring more than 100'\n\n>>> # with a negative prompt\n>>> neg_inputs = tokenizer([\"A very happy event happened,\"], return_tensors=\"pt\")\n>>> out = model.generate(inputs[\"input_ids\"], guidance_scale=2, negative_prompt_ids=neg_inputs[\"input_ids\"])\n>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n'Today, a dragon flew over Paris, France, killing at least 130 people. French media reported that'\n\n>>> # with a positive prompt\n>>> neg_inputs = tokenizer([\"A very happy event happened,\"], return_tensors=\"pt\")\n>>> out = model.generate(inputs[\"input_ids\"], guidance_scale=0, negative_prompt_ids=neg_inputs[\"input_ids\"])\n>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n\"Today, a dragon flew over Paris, France, and I'm very happy to be here. I\"\n```", "```py\n>>> import torch\n>>> from transformers import AutoProcessor, WhisperForConditionalGeneration, GenerationConfig\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = processor(ds[3][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n\n>>> #Displaying timestamps\n>>> generated_ids = model.generate(inputs=input_features, return_timestamps=True)\n>>> transcription = processor.batch_decode(generated_ids, decode_with_timestamps=True)[0]\n>>> print(\"Transcription:\", transcription)\nTranscription: <|startoftranscript|><|0.00|> He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can<|6.44|><|6.44|> discover in it but little of rocky Ithaca.<|9.44|><|endoftext|>\n\n>>> #No timestamps & change EOS:\n>>> #This allows the user to select a specific token to terminate the sequence on, in this case it's the word \"can\"(460)\n>>> model.generation_config.eos_token_id = 460\n>>> generated_ids = model.generate(inputs=input_features,return_timestamps=False)\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(\"Transcription:\", transcription)\nTranscription:  He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can\n```", "```py\ncompleted = False\nwhile not completed:\n    _, completed = constraint.update(constraint.advance())\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\n>>> tok = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n>>> streamer = TextStreamer(tok)\n\n>>> # Despite returning the usual output, the streamer will also print the generated text to stdout.\n>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\nAn increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n>>> from threading import Thread\n\n>>> tok = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n>>> streamer = TextIteratorStreamer(tok)\n\n>>> # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n>>> generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n>>> thread = Thread(target=model.generate, kwargs=generation_kwargs)\n>>> thread.start()\n>>> generated_text = \"\"\n>>> for new_text in streamer:\n...     generated_text += new_text\n>>> generated_text\n'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'\n```"]