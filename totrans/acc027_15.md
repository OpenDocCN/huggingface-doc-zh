# 了解一个模型有多大可以适合您的机器

> 原文链接：[`huggingface.co/docs/accelerate/usage_guides/model_size_estimator`](https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator)

在探索潜在模型在您的机器上是否适合内存（例如将模型加载到 CUDA）时，一个非常困难的方面是知道一个模型有多大，才能适合您当前的显卡。

为了帮助缓解这一问题，🤗 Accelerate 通过`accelerate estimate-memory`提供了一个 CLI 界面。本教程将帮助您了解如何使用它，预期结果，并在最后链接到🤗 Hub 上托管的交互式演示，甚至让您直接发布这些结果到模型仓库！

目前我们支持搜索可以在`timm`和`transformers`中使用的模型。

此 API 将在`meta`设备上将模型加载到内存中，因此我们实际上并没有下载和加载模型的全部权重到内存中，也不需要。因此，可以完全放心地测量 80 亿参数模型（甚至更多），而不必担心您的 CPU 是否能够处理！

## Gradio 演示

以下是与上述描述相关的一些 gradio 演示。第一个是官方的 Hugging Face 内存估算空间，直接利用 Accelerate：

[`hf-accelerate-model-memory-usage.hf.space?__theme=light`](https://hf-accelerate-model-memory-usage.hf.space?__theme=light)

[`hf-accelerate-model-memory-usage.hf.space?__theme=dark`](https://hf-accelerate-model-memory-usage.hf.space?__theme=dark)

社区成员进一步发展了这个想法，使您可以直接筛选模型，看看是否可以在给定 GPU 约束和 LoRA 配置下运行特定的 LLM。要尝试，请查看[此处](https://huggingface.co/spaces/Vokturz/can-it-run-llm)以获取更多详细信息。

## 命令

在使用`accelerate estimate-memory`时，您需要传入要使用的模型的名称，可能是该模型使用的框架（如果无法自动找到），以及您希望模型加载的数据类型。

例如，这是如何计算`bert-base-cased`的内存占用量的方法：

```py
accelerate estimate-memory bert-base-cased
```

这将下载`bert-based-cased`的`config.json`，将模型加载到`meta`设备上，并报告它将使用多少空间：

加载`bert-base-cased`的内存使用情况：

| 数据类型 | 最大层 | 总大小 | 使用 Adam 进行训练 |
| --- | --- | --- | --- |
| float32 | 84.95 MB | 418.18 MB | 1.61 GB |
| float16 | 42.47 MB | 206.59 MB | 826.36 MB |
| int8 | 21.24 MB | 103.29 MB | 413.18 MB |
| int4 | 10.62 MB | 51.65 MB | 206.59 MB |

默认情况下，它将返回所有支持的数据类型（从`int4`到`float32`），但如果您对特定数据类型感兴趣，可以进行筛选。

### 特定库

如果无法自动确定源库（就像在`bert-base-cased`的情况下），则可以传入库名称。

```py
accelerate estimate-memory HuggingFaceM4/idefics-80b-instruct --library_name transformers
```

加载`HuggingFaceM4/idefics-80b-instruct`的内存使用情况：

| 数据类型 | 最大层 | 总大小 | 使用 Adam 进行训练 |
| --- | --- | --- | --- |
| float32 | 3.02 GB | 297.12 GB | 1.16 TB |
| float16 | 1.51 GB | 148.56 GB | 594.24 GB |
| int8 | 772.52 MB | 74.28 GB | 297.12 GB |
| int4 | 386.26 MB | 37.14 GB | 148.56 GB |

```py
accelerate estimate-memory timm/resnet50.a1_in1k --library_name timm
```

加载`timm/resnet50.a1_in1k`的内存使用情况：

| 数据类型 | 最大层 | 总大小 | 使用 Adam 进行训练 |
| --- | --- | --- | --- |
| float32 | 9.0 MB | 97.7 MB | 390.78 MB |
| float16 | 4.5 MB | 48.85 MB | 195.39 MB |
| int8 | 2.25 MB | 24.42 MB | 97.7 MB |
| int4 | 1.12 MB | 12.21 MB | 48.85 MB |

### 特定数据类型

如前所述，虽然默认情况下我们返回`int4`到`float32`，但任何数据类型都可以使用，包括`float32`、`float16`、`int8`和`int4`。

为此，在指定`--dtypes`后传入它们：

```py
accelerate estimate-memory bert-base-cased --dtypes float32 float16
```

加载`bert-base-cased`的内存使用情况：

| 数据类型 | 最大层 | 总大小 | 使用 Adam 进行训练 |
| --- | --- | --- | --- |
| float32 | 84.95 MB | 413.18 MB | 1.61 GB |
| float16 | 42.47 MB | 206.59 MB | 826.36 MB |

## 这个计算器的注意事项

这个计算器将告诉您加载模型所需的内存量，*而不是*执行推断。

这个计算准确度在实际值的几个百分点内，因此它非常好地展示了需要多少内存。例如，完整精度下加载`bert-base-cased`实际上需要`413.68 MB`，而计算器估计为`413.18 MB`。

在执行推断时，您可以期望额外增加高达 20%，这是由[EleutherAI](https://blog.eleuther.ai/transformer-math/)发现的。我们将进行研究，以找到这些值更准确的估计，并在完成后更新这个计算器。
