# ControlNet

> 原始文本：[https://huggingface.co/docs/diffusers/api/models/controlnet](https://huggingface.co/docs/diffusers/api/models/controlnet)

ControlNet 模型是由 Lvmin Zhang、Anyi Rao、Maneesh Agrawala 在 [Adding Conditional Control to Text-to-Image Diffusion Models](https://huggingface.co/papers/2302.05543) 中引入的。通过在模型上添加额外的输入，如边缘图、深度图、分割图和姿势检测的关键点，它为文本到图像生成提供了更高程度的控制。

该论文的摘要是：

*我们提出了 ControlNet，这是一种神经网络架构，用于向大型、预训练的文本到图像扩散模型添加空间条件控制。ControlNet 锁定了生产就绪的大型扩散模型，并重复使用它们深度和稳健的编码层，这些层经过数十亿张图像的预训练，作为学习多样化条件控制的强大支撑。神经架构连接了“零卷积”（从零初始化的卷积层），逐渐增加参数，确保没有有害噪音会影响微调。我们使用 Stable Diffusion 测试了各种条件控制，例如边缘、深度、分割、人体姿势等，使用单个或多个条件，有或没有提示。我们展示了 ControlNet 的训练在小型（<50k）和大型（>1m）数据集上都很稳健。广泛的结果表明，ControlNet 可能促进更广泛的应用，以控制图像扩散模型。*

## 从原始格式加载

默认情况下，应该使用 [from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained) 加载 [ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)，但也可以使用 `FromOriginalControlnetMixin.from_single_file` 从原始格式加载，如下所示：

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

url = "https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11p_sd15_canny.pth"  # can also be a local path
controlnet = ControlNetModel.from_single_file(url)

url = "https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.safetensors"  # can also be a local path
pipe = StableDiffusionControlNetPipeline.from_single_file(url, controlnet=controlnet)
```

## ControlNetModel

### `class diffusers.ControlNetModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L111)

```py
( in_channels: int = 4 conditioning_channels: int = 3 flip_sin_to_cos: bool = True freq_shift: int = 0 down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D') mid_block_type: Optional = 'UNetMidBlock2DCrossAttn' only_cross_attention: Union = False block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: int = 2 downsample_padding: int = 1 mid_block_scale_factor: float = 1 act_fn: str = 'silu' norm_num_groups: Optional = 32 norm_eps: float = 1e-05 cross_attention_dim: int = 1280 transformer_layers_per_block: Union = 1 encoder_hid_dim: Optional = None encoder_hid_dim_type: Optional = None attention_head_dim: Union = 8 num_attention_heads: Union = None use_linear_projection: bool = False class_embed_type: Optional = None addition_embed_type: Optional = None addition_time_embed_dim: Optional = None num_class_embeds: Optional = None upcast_attention: bool = False resnet_time_scale_shift: str = 'default' projection_class_embeddings_input_dim: Optional = None controlnet_conditioning_channel_order: str = 'rgb' conditioning_embedding_out_channels: Optional = (16, 32, 96, 256) global_pool_conditions: bool = False addition_embed_type_num_heads: int = 64 )
```

参数

+   `in_channels` (`int`, 默认为 4) — 输入样本中的通道数。

+   `flip_sin_to_cos` (`bool`, 默认为 `True`) — 是否在时间嵌入中将 sin 翻转为 cos。

+   `freq_shift` (`int`, 默认为 0) — 应用于时间嵌入的频率偏移。

+   `down_block_types` (`tuple[str]`, 默认为 `("CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D")`) — 要使用的下采样块的元组。

+   `only_cross_attention` (`Union[bool, Tuple[bool]]`, 默认为 `False`) —

+   `block_out_channels` (`tuple[int]`, 默认为 `(320, 640, 1280, 1280)`) — 每个块的输出通道的元组。

+   `layers_per_block` (`int`, 默认为 2) — 每个块的层数。

+   `downsample_padding` (`int`, 默认为 1) — 用于下采样卷积的填充。

+   `mid_block_scale_factor` (`float`, 默认为 1) — 用于中间块的比例因子。

+   `act_fn` (`str`, 默认为“silu”) — 要使用的激活函数。

+   `norm_num_groups` (`int`, *可选*, 默认为 32) — 用于归一化的组数。如果为 None，则在后处理中跳过归一化和激活层。

+   `norm_eps` (`float`, 默认为 1e-5) — 用于归一化的 epsilon。

+   `cross_attention_dim` (`int`, 默认为 1280) — 交叉注意力特征的维度。

+   `transformer_layers_per_block` (`int` 或 `Tuple[int]`, *可选*, 默认为 1) — 类型为 `BasicTransformerBlock` 的 transformer 块的数量。仅适用于 `~models.unet_2d_blocks.CrossAttnDownBlock2D`, `~models.unet_2d_blocks.CrossAttnUpBlock2D`, `~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`。

+   `encoder_hid_dim` (`int`, *可选*, 默认为 None) — 如果定义了 `encoder_hid_dim_type`，则将从 `encoder_hid_dim` 维度投影到 `cross_attention_dim`。

+   `encoder_hid_dim_type` (`str`, *optional*, defaults to `None`) — 如果给定，`encoder_hidden_states`和可能的其他嵌入将根据`encoder_hid_dim_type`下投影到文本嵌入的维度`cross_attention`。

+   `attention_head_dim` (`Union[int, Tuple[int]]`, defaults to 8) — 注意力头的维度。

+   `use_linear_projection` (`bool`, defaults to `False`) —

+   `class_embed_type` (`str`, *optional*, defaults to `None`) — 要使用的类嵌入类型，最终将与时间嵌入相加。选择从None、`"timestep"`、`"identity"`、`"projection"`或`"simple_projection"`。

+   `addition_embed_type` (`str`, *optional*, defaults to `None`) — 配置一个可选的嵌入，将与时间嵌入相加。选择从`None`或“text”。“text”将使用`TextTimeEmbedding`层。

+   `num_class_embeds` (`int`, *optional*, defaults to 0) — 可学习嵌入矩阵的输入维度，将被投影到`time_embed_dim`，当执行类条件化时，`class_embed_type`等于`None`。

+   `upcast_attention` (`bool`, defaults to `False`) —

+   `resnet_time_scale_shift` (`str`, defaults to `"default"`) — ResNet块的时间尺度偏移配置（参见`ResnetBlock2D`）。选择从`default`或`scale_shift`。

+   `projection_class_embeddings_input_dim` (`int`, *optional*, defaults to `None`) — 当`class_embed_type="projection"`时，`class_labels`输入的维度。在`class_embed_type="projection"`时需要。

+   `controlnet_conditioning_channel_order` (`str`, defaults to `"rgb"`) — 条件图像的通道顺序。如果是`bgr`，将转换为`rgb`。 

+   `conditioning_embedding_out_channels` (`tuple[int]`, *optional*, defaults to `(16, 32, 96, 256)`) — `conditioning_embedding`层中每个块的输出通道的元组。

+   `global_pool_conditions` (`bool`, defaults to `False`) — TODO(Patrick) - 未使用的参数。

+   `addition_embed_type_num_heads` (`int`, defaults to 64) — 用于`TextTimeEmbedding`层的头数。

一个ControlNet模型。

#### `forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L663)

```py
( sample: FloatTensor timestep: Union encoder_hidden_states: Tensor controlnet_cond: FloatTensor conditioning_scale: float = 1.0 class_labels: Optional = None timestep_cond: Optional = None attention_mask: Optional = None added_cond_kwargs: Optional = None cross_attention_kwargs: Optional = None guess_mode: bool = False return_dict: bool = True ) → export const metadata = 'undefined';ControlNetOutput or tuple
```

参数

+   `sample` (`torch.FloatTensor`) — 嘈杂的输入张量。

+   `timestep` (`Union[torch.Tensor, float, int]`) — 对输入进行去噪的时间步数。

+   `encoder_hidden_states` (`torch.Tensor`) — 编码器的隐藏状态。

+   `controlnet_cond` (`torch.FloatTensor`) — 形状为`(batch_size, sequence_length, hidden_size)`的条件输入张量。

+   `conditioning_scale` (`float`, defaults to `1.0`) — ControlNet输出的比例因子。

+   `class_labels` (`torch.Tensor`, *optional*, defaults to `None`) — 可选的类标签，用于条件化。它们的嵌入将与时间步嵌入相加。

+   `timestep_cond` (`torch.Tensor`, *optional*, defaults to `None`) — 时间步的额外条件嵌入。如果提供，这些嵌入将与通过`self.time_embedding`层传递的时间步嵌入相加以获得最终的时间步嵌入。

+   `attention_mask` (`torch.Tensor`, *optional*, defaults to `None`) — 形状为`(batch, key_tokens)`的注意力掩码应用于`encoder_hidden_states`。如果为`1`，则保留掩码，否则如果为`0`，则丢弃。掩码将转换为偏置，将大的负值添加到对应于“丢弃”标记的注意力分数。

+   `added_cond_kwargs` (`dict`) — Stable Diffusion XL UNet的额外条件。

+   `cross_attention_kwargs` (`dict[str]`, *optional*, defaults to `None`) — 如果指定，则传递给`AttnProcessor`的kwargs字典。

+   `guess_mode` (`bool`, defaults to `False`) — 在此模式下，ControlNet编码器尽最大努力识别输入内容，即使您删除所有提示。建议使用介于3.0和5.0之间的`guidance_scale`。

+   `return_dict`（`bool`，默认为`True`）— 是否返回一个[ControlNetOutput](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.models.controlnet.ControlNetOutput)而不是一个普通的元组。

返回

[ControlNetOutput](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.models.controlnet.ControlNetOutput) **或** `tuple`

如果`return_dict`为`True`，则返回一个[ControlNetOutput](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.models.controlnet.ControlNetOutput)，否则返回一个元组，其中第一个元素是样本张量。

[ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)的前向方法。

#### `from_unet`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L444)

```py
( unet: UNet2DConditionModel controlnet_conditioning_channel_order: str = 'rgb' conditioning_embedding_out_channels: Optional = (16, 32, 96, 256) load_weights_from_unet: bool = True conditioning_channels: int = 3 )
```

参数

+   `unet`（`UNet2DConditionModel`）— 要复制到[ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)的UNet模型权重。在适用的情况下，还会复制所有配置选项。

从[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)实例化一个[ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)。

#### `set_attention_slice`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L594)

```py
( slice_size: Union )
```

参数

+   `slice_size`（`str`或`int`或`list(int)`，*可选*，默认为`"auto"`）— 当为`"auto"`时，注意力头的输入减半，因此注意力在两个步骤中计算。如果为`"max"`，通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。

启用此选项时，注意力模块将输入张量分片以在多个步骤中计算注意力。这对于节省一些内存以换取速度略微降低很有用。

#### `set_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L543)

```py
( processor: Union )
```

参数

+   `processor`（`AttentionProcessor`的`dict`或仅`AttentionProcessor`）— 实例化的处理器类或将设置为**所有**`Attention`层的处理器的处理器类字典。

    如果`processor`是一个字典，则键需要定义相应的交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。

设置用于计算注意力的注意力处理器。

#### `set_default_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L578)

```py
( )
```

禁用自定义注意力处理器并设置默认的注意力实现。

## ControlNetOutput

### `class diffusers.models.controlnet.ControlNetOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet.py#L46)

```py
( down_block_res_samples: Tuple mid_block_res_sample: Tensor )
```

参数

+   `down_block_res_samples`（`tuple[torch.Tensor]`）— 不同分辨率的每个下采样块的下采样激活的元组。每个张量的形状应为`(batch_size, channel * resolution, height //resolution, width // resolution)`。输出可用于对原始UNet的下采样激活进行条件化。

+   `mid_down_block_re_sample`（`torch.Tensor`）— 中间块（最低采样分辨率）的激活。每个张量的形状应为`(batch_size, channel * lowest_resolution, height // lowest_resolution, width // lowest_resolution)`。输出可用于对原始UNet的中间块激活进行条件化。

[ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)的输出。

## FlaxControlNetModel

### `class diffusers.FlaxControlNetModel`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet_flax.py#L103)

```py
( sample_size: int = 32 in_channels: int = 4 down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D') only_cross_attention: Union = False block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: int = 2 attention_head_dim: Union = 8 num_attention_heads: Union = None cross_attention_dim: int = 1280 dropout: float = 0.0 use_linear_projection: bool = False dtype: dtype = <class 'jax.numpy.float32'> flip_sin_to_cos: bool = True freq_shift: int = 0 controlnet_conditioning_channel_order: str = 'rgb' conditioning_embedding_out_channels: Tuple = (16, 32, 96, 256) parent: Union = <flax.linen.module._Sentinel object at 0x7fbd557629e0> name: Optional = None )
```

参数

+   `sample_size` (`int`, *optional*) — 输入样本的大小。

+   `in_channels` (`int`, *optional*, 默认为4) — 输入样本中的通道数。

+   `down_block_types` (`Tuple[str]`, *optional*, 默认为 `("FlaxCrossAttnDownBlock2D", "FlaxCrossAttnDownBlock2D", "FlaxCrossAttnDownBlock2D", "FlaxDownBlock2D")`) — 要使用的下采样块的元组。

+   `block_out_channels` (`Tuple[int]`, *optional*, 默认为 `(320, 640, 1280, 1280)`) — 每个块的输出通道的元组。

+   `layers_per_block` (`int`, *optional*, 默认为2) — 每个块的层数。

+   `attention_head_dim` (`int` 或 `Tuple[int]`, *optional*, 默认为8) — 注意力头的维度。

+   `num_attention_heads` (`int` 或 `Tuple[int]`, *optional*) — 注意力头的数量。

+   `cross_attention_dim` (`int`, *optional*, 默认为768) — 交叉注意力特征的维度。

+   `dropout` (`float`, *optional*, 默认为0) — 下、上和瓶颈块的丢弃概率。

+   `flip_sin_to_cos` (`bool`, *optional*, 默认为 `True`) — 是否在时间嵌入中将 sin 翻转为 cos。

+   `freq_shift` (`int`, *optional*, 默认为0) — 应用于时间嵌入的频率偏移。

+   `controlnet_conditioning_channel_order` (`str`, *optional*, 默认为 `rgb`) — 条件图像的通道顺序。如果是 `bgr`，将转换为 `rgb`。

+   `conditioning_embedding_out_channels` (`tuple`, *optional*, 默认为 `(16, 32, 96, 256)`) — `conditioning_embedding` 层中每个块的输出通道的元组。

一个 ControlNet 模型。

这个模型继承自 [FlaxModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.FlaxModelMixin)。检查超类文档以获取所有模型实现的通用方法（如下载或保存）。

这个模型也是一个 Flax Linen [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#module) 子类。将其用作常规的 Flax Linen 模块，并参考 Flax 文档以获取有关其一般用法和行为的所有事项。

支持以下内在的 JAX 特性：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

## FlaxControlNetOutput

### `class diffusers.models.controlnet_flax.FlaxControlNetOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/controlnet_flax.py#L33)

```py
( down_block_res_samples: Array mid_block_res_sample: Array )
```

参数

+   `down_block_res_samples` (`jnp.ndarray`) —

+   `mid_block_res_sample` (`jnp.ndarray`) —

[FlaxControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.FlaxControlNetModel) 的输出。

#### `replace`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个新对象，用新值替换指定字段。
