- en: Scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/timm/training_script](https://huggingface.co/docs/timm/training_script)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/timm/v0.9.12/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/entry/start.c775fc75.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/scheduler.85c25b89.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/singletons.602dd09e.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/paths.2e0ff118.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/entry/app.8203bc6a.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/index.c9837788.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/nodes/0.d2b01217.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/nodes/76.dc7e45d2.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/Tip.6bd863a0.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/CodeBlock.52fa569e.js">
    <link rel="modulepreload" href="/docs/timm/v0.9.12/en/_app/immutable/chunks/Heading.3097d2ed.js">
  prefs: []
  type: TYPE_NORMAL
- en: A train, validation, inference, and checkpoint cleaning script included in the
    github root folder. Scripts are not currently packaged in the pip release.
  prefs: []
  type: TYPE_NORMAL
- en: The training and validation scripts evolved from early versions of the [PyTorch
    Imagenet Examples](https://github.com/pytorch/examples). I have added significant
    functionality over time, including CUDA specific performance enhancements based
    on [NVIDIA’s APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples).
  prefs: []
  type: TYPE_NORMAL
- en: Training Script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The variety of training args is large and not all combinations of options (or
    even options) have been fully tested. For the training dataset folder, specify
    the folder to the base that contains a `train` and `validation` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process
    per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead
    of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3\. --apex-amp will
    force use of APEX components if they are installed.
  prefs: []
  type: TYPE_NORMAL
- en: Validation / Inference Scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Validation and inference scripts are similar in usage. One outputs metrics on
    a validation set and the other outputs topk class ids in a csv. Specify the folder
    containing validation images, not the base as in training script.
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate with the model’s pretrained weights (if they exist):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To run inference from a checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Training Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These params are for dual Titan RTX cards with NVIDIA Apex installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This params are for dual Titan RTX cards with NVIDIA Apex installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: SE-ResNeXt-26-D and SE-ResNeXt-26-T
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These hparams (or similar) work well for a wide range of ResNet architecture,
    generally a good idea to increase the epoch # as the model size increases… ie
    approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR
    proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti
    cards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training of this model started with the same command line as EfficientNet-B2
    w/ RA above. After almost three weeks of training the process crashed. The results
    weren’t looking amazing so I resumed the training several times with tweaks to
    a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing
    looked great. I ended up averaging the best checkpoints from all restarts. The
    result is mediocre at default res/crop but oddly performs much better with a full
    image test crop of 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Michael Klachko](https://github.com/michaelklachko) achieved these results
    with the command line for B2 adapted for larger batch size, with the recommended
    B0 dropout rate of 0.2.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39
    top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically
    better ImageNet validation result than my first good AugMix training of 78.99\.
    However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch,
    etc. Unlike my first AugMix runs, I’ve enabled SplitBatchNorm, disabled random
    erasing on the clean split, and cranked up random erasing prob on the 2 augmented
    paths.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trained by [Andrew Lavin](https://github.com/andravin) with 8 V100 cards. Model
    EMA was not used, final checkpoint is the average of 8 best checkpoints during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely
    101\. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using
    a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd
    line below are tuned for 8 GPU training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
