# Data2Vec

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec)

## æ¦‚è¿°

Data2Vecæ¨¡å‹æ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael Auliåœ¨[æ•°æ®2vec: è¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€ä¸­çš„è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚Data2Vecæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„è‡ªç›‘ç£å­¦ä¹  - æ–‡æœ¬ã€éŸ³é¢‘å’Œå›¾åƒã€‚é‡è¦çš„æ˜¯ï¼Œé¢„è®­ç»ƒçš„é¢„æµ‹ç›®æ ‡æ˜¯è¾“å…¥çš„ä¸Šä¸‹æ–‡åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç‰¹å®šäºæ¨¡æ€çš„ã€ä¸Šä¸‹æ–‡æ— å…³çš„ç›®æ ‡ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*å°½ç®¡è‡ªç›‘ç£å­¦ä¹ çš„ä¸€èˆ¬æ€æƒ³åœ¨å„ç§æ¨¡æ€ä¹‹é—´æ˜¯ç›¸åŒçš„ï¼Œä½†å®é™…çš„ç®—æ³•å’Œç›®æ ‡å·®å¼‚å¾ˆå¤§ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é’ˆå¯¹å•ä¸€æ¨¡æ€å¼€å‘çš„ã€‚ä¸ºäº†è®©æˆ‘ä»¬æ›´æ¥è¿‘äºä¸€èˆ¬çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†data2vecï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„å­¦ä¹ æ–¹æ³•æ¥å¤„ç†è¯­éŸ³ã€NLPæˆ–è®¡ç®—æœºè§†è§‰ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäºè¾“å…¥æ•°æ®çš„é®è”½è§†å›¾æ¥é¢„æµ‹å®Œæ•´è¾“å…¥æ•°æ®çš„æ½œåœ¨è¡¨ç¤ºï¼Œä½¿ç”¨æ ‡å‡†çš„Transformeræ¶æ„è¿›è¡Œè‡ªè’¸é¦è®¾ç½®ã€‚data2vecä¸æ˜¯é¢„æµ‹ç‰¹å®šäºæ¨¡æ€çš„ç›®æ ‡ï¼Œæ¯”å¦‚å•è¯ã€è§†è§‰æ ‡è®°æˆ–äººç±»è¯­éŸ³å•å…ƒï¼Œè€Œæ˜¯é¢„æµ‹åŒ…å«æ¥è‡ªæ•´ä¸ªè¾“å…¥çš„ä¿¡æ¯çš„ä¸Šä¸‹æ–‡åŒ–æ½œåœ¨è¡¨ç¤ºã€‚å¯¹è¯­éŸ³è¯†åˆ«ã€å›¾åƒåˆ†ç±»å’Œè‡ªç„¶è¯­è¨€ç†è§£çš„ä¸»è¦åŸºå‡†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¸»æµæ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆ–å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨[www.github.com/pytorch/fairseq/tree/master/examples/data2vec](http://www.github.com/pytorch/fairseq/tree/master/examples/data2vec)ä¸Šæ‰¾åˆ°ã€‚*

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[edugp](https://huggingface.co/edugp)å’Œ[patrickvonplaten](https://huggingface.co/patrickvonplaten)è´¡çŒ®çš„ã€‚[sayakpaul](https://github.com/sayakpaul)å’Œ[Rocketknight1](https://github.com/Rocketknight1)ä¸ºTensorFlowä¸­çš„è§†è§‰è´¡çŒ®äº†Data2Vecã€‚

åŸå§‹ä»£ç ï¼ˆç”¨äºNLPå’Œè¯­éŸ³ï¼‰å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/pytorch/fairseq/tree/main/examples/data2vec)æ‰¾åˆ°ã€‚è§†è§‰çš„åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/data2vec_vision/tree/main/beit)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   Data2VecAudioã€Data2VecTextå’ŒData2VecVisionéƒ½æ˜¯ä½¿ç”¨ç›¸åŒçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒçš„ã€‚

+   å¯¹äºData2VecAudioï¼Œé¢„å¤„ç†ä¸[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)ç›¸åŒï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€‚

+   å¯¹äºData2VecTextï¼Œé¢„å¤„ç†ä¸[RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)ç›¸åŒï¼ŒåŒ…æ‹¬æ ‡è®°åŒ–ã€‚

+   å¯¹äºData2VecVisionï¼Œé¢„å¤„ç†ä¸[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)ç›¸åŒï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€‚

## èµ„æº

å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨Data2Vecã€‚

å›¾åƒåˆ†ç±»

+   [Data2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification)ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚

+   è¦åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒ[TFData2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification)ï¼Œè¯·å‚é˜…[æ­¤ç¬”è®°æœ¬](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb)ã€‚

**Data2VecTextæ–‡æ¡£èµ„æº**

+   [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)

+   [æ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)

+   [é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)

+   [å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)

+   [æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/masked_language_modeling)

+   [å¤šé¡¹é€‰æ‹©ä»»åŠ¡æŒ‡å—](../tasks/multiple_choice)

**Data2VecAudioæ–‡æ¡£èµ„æº**

+   [éŸ³é¢‘åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/audio_classification)

+   [è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æŒ‡å—](../tasks/asr)

**Data2VecVisionæ–‡æ¡£èµ„æº**

+   [å›¾åƒåˆ†ç±»](../tasks/image_classification)

+   [è¯­ä¹‰åˆ†å‰²](../tasks/semantic_segmentation)

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## Data2VecTextConfig

### `class transformers.Data2VecTextConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_text.py#L31)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True classifier_dropout = None **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º30522) â€” DATA2VECæ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨`Data2VecModel`æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º3072) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str`æˆ–`Callable`, *å¯é€‰*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚

+   `type_vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º2) â€” åœ¨è°ƒç”¨`Data2VecModel`æ—¶ä¼ é€’çš„`token_type_ids`çš„è¯æ±‡é‡ã€‚

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `position_embedding_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹©`"absolute"`ä¹‹ä¸€ã€‚æœ‰å…³`"relative_key"`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨ç›¸å¯¹ä½ç½®è¡¨ç¤ºçš„è‡ªæ³¨æ„åŠ›ï¼ˆShawç­‰äººï¼‰](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³`"relative_key_query"`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨æ›´å¥½çš„ç›¸å¯¹ä½ç½®åµŒå…¥æ”¹è¿›Transformeræ¨¡å‹ï¼ˆHuangç­‰äººï¼‰](https://arxiv.org/abs/2009.13658)ä¸­çš„*æ–¹æ³•4*ã€‚

+   `is_decoder` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ¨¡å‹æ˜¯å¦ç”¨ä½œè§£ç å™¨ã€‚å¦‚æœä¸º`False`ï¼Œåˆ™æ¨¡å‹ç”¨ä½œç¼–ç å™¨ã€‚

+   `use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨`config.is_decoder=True`æ—¶ç›¸å…³ã€‚

+   `classifier_dropout` (`float`, *optional*) â€” åˆ†ç±»å¤´çš„ dropout æ¯”ç‡ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel) å’Œ [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel) é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª Data2VecText æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Data2VecText [facebook/data2vec-text-base](https://huggingface.co/facebook/data2vec-text-base) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Data2VecTextConfig, Data2VecTextModel

>>> # Initializing a Data2VecText facebook/data2vec-text-base style configuration
>>> configuration = Data2VecTextConfig()

>>> # Initializing a model (with random weights) from the facebook/data2vec-text-base style configuration
>>> model = Data2VecTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Data2VecAudioConfig

### `class transformers.Data2VecAudioConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_audio.py#L31)

```py
( vocab_size = 32 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embedding_groups = 16 conv_pos_kernel_size = 19 num_conv_pos_embeddings = 5 mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 ctc_loss_reduction = 'sum' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 32) â€” Data2VecAudio æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel) æˆ– `TFData2VecAudioModel` æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†ä¼ é€’ç»™ [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel) çš„ *inputs_ids* å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) â€” å…¨è¿æ¥å±‚å†…æ¿€æ´»çš„ dropout æ¯”ç‡ã€‚

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚

+   `final_dropout` (`float`, *optional*, defaults to 0.1) â€” [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC) çš„æœ€ç»ˆæŠ•å½±å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `layerdrop` (`float`, *optional*, defaults to 0.1) â€” LayerDrop æ¦‚ç‡ã€‚æ›´å¤šç»†èŠ‚è¯·å‚é˜… [LayerDrop è®ºæ–‡](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `feat_proj_dropout` (`float`, *optional*, defaults to 0.0) â€” ç‰¹å¾ç¼–ç å™¨è¾“å‡ºçš„ dropout æ¦‚ç‡ã€‚

+   `feat_extract_activation` (`str,` optional`, defaults to` â€œgeluâ€`) -- ç‰¹å¾æå–å™¨ä¸­1Då·ç§¯å±‚çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ` â€œgeluâ€`ã€` â€œreluâ€`ã€` â€œseluâ€`å’Œ`â€œgelu_newâ€`ã€‚

+   `conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 512, 512, 512)`) â€” å®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°çš„æ•´æ•°å…ƒç»„ã€‚*conv_dim*çš„é•¿åº¦å®šä¹‰äº†1Då·ç§¯å±‚çš„æ•°é‡ã€‚

+   `conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2, 2, 2, 2, 2, 2)`) â€” å®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ­¥å¹…çš„æ•´æ•°å…ƒç»„ã€‚*conv_stride*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚

+   `conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3, 3, 3, 3, 3, 3)`) â€” å®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„å†…æ ¸å¤§å°çš„æ•´æ•°å…ƒç»„ã€‚*conv_kernel*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚

+   `conv_bias` (`bool`, *optional*, defaults to `False`) â€” 1Då·ç§¯å±‚æ˜¯å¦æœ‰åç½®ã€‚

+   `num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) â€” å·ç§¯ä½ç½®åµŒå…¥çš„æ•°é‡ã€‚å®šä¹‰äº†1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„å†…æ ¸å¤§å°ã€‚

+   `num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) â€” 1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„ç»„æ•°ã€‚

+   `mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” æ²¿æ—¶é—´è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_time_prob*len(time_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ* mask_time_prob *åº”è¯¥æ˜¯`prob_vector_start*mask_time_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½

+   `mask_time_length` (`int`, *optional*, defaults to 10) â€” æ²¿æ—¶é—´è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚

+   `mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” æ²¿æ—¶é—´è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_time_prob*len(time_axis)/mask_time_length < mask_time_min_masksâ€æ—¶ç›¸å…³

+   `mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” æ²¿ç‰¹å¾è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_feature_prob*len(feature_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ* mask_feature_prob *åº”è¯¥æ˜¯`prob_vector_start*mask_feature_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½æ©ç›–å‘é‡çš„å®é™…ç™¾åˆ†æ¯”ã€‚ä»…åœ¨`apply_spec_augmentä¸ºTrue`æ—¶ç›¸å…³ã€‚

+   `mask_feature_length` (`int`, *optional*, defaults to 10) â€” æ²¿ç‰¹å¾è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚

+   `mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” æ²¿ç‰¹å¾è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_feature_prob*len(feature_axis)/mask_feature_length < mask_feature_min_masksâ€æ—¶ç›¸å…³

+   `ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) â€” æŒ‡å®šåº”ç”¨äº`torch.nn.CTCLoss`è¾“å‡ºçš„å‡å°‘æ–¹å¼ã€‚ä»…åœ¨è®­ç»ƒ[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å°†`torch.nn.CTCLoss`çš„æ— é™æŸå¤±å’Œç›¸å…³æ¢¯åº¦ç½®é›¶ã€‚å½“è¾“å…¥å¤ªçŸ­æ— æ³•ä¸ç›®æ ‡å¯¹é½æ—¶ï¼Œä¸»è¦ä¼šå‡ºç°æ— é™æŸå¤±ã€‚ä»…åœ¨è®­ç»ƒ[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨å¸¦æœ‰å­¦ä¹ æƒé‡çš„å±‚è¾“å‡ºçš„åŠ æƒå¹³å‡ã€‚ä»…åœ¨ä½¿ç”¨[Data2VecAudioForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification)å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `classifier_proj_size` (`int`, *optional*, defaults to 256) â€” åˆ†ç±»å‰çš„æŠ•å½±ç»´åº¦ï¼Œç”¨äºæ ‡è®°å‡å€¼æ± åŒ–ã€‚

+   `tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 1500)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰*XVector*æ¨¡å‹ä¸­*TDNN*æ¨¡å—ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚*tdnn_dim*çš„é•¿åº¦å®šä¹‰äº†*TDNN*å±‚æ•°ã€‚

+   `tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3, 3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰*XVector*æ¨¡å‹ä¸­*TDNN*æ¨¡å—ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ ¸å¤§å°ã€‚*tdnn_kernel*çš„é•¿åº¦å¿…é¡»ä¸*tdnn_dim*çš„é•¿åº¦ç›¸åŒ¹é…ã€‚

+   `tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1, 2, 3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰*XVector*æ¨¡å‹ä¸­*TDNN*æ¨¡å—ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ‰©å¼ å› å­ã€‚*tdnn_dilation*çš„é•¿åº¦å¿…é¡»ä¸*tdnn_dim*çš„é•¿åº¦ç›¸åŒ¹é…ã€‚

+   `xvector_output_dim` (`int`, *optional*, defaults to 512) â€” *XVector*åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚

+   `add_adapter` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨Data2VecAudioç¼–ç å™¨é¡¶éƒ¨å †å å·ç§¯ç½‘ç»œã€‚å¯¹äºå¯åŠ¨Data2VecAudioç”¨äºSpeechEncoderDecoderæ¨¡å‹éå¸¸æœ‰ç”¨ã€‚

+   `adapter_kernel_size` (`int`, *optional*, defaults to 3) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„æ ¸å¤§å°ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚

+   `adapter_stride` (`int`, *optional*, defaults to 2) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„æ­¥å¹…ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚

+   `num_adapter_layers` (`int`, *optional*, defaults to 3) â€” é€‚é…å™¨ç½‘ç»œä¸­åº”ä½¿ç”¨çš„å·ç§¯å±‚æ•°é‡ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚

+   `output_hidden_size` (`int`, *optional*) â€” ç¼–ç å™¨è¾“å‡ºå±‚çš„ç»´åº¦ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™é»˜è®¤ä¸º*hidden-size*ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨[Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªData2VecAudioæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºData2VecAudio [facebook/data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Data2VecAudioConfig, Data2VecAudioModel

>>> # Initializing a Data2VecAudio facebook/data2vec-audio-base-960h style configuration
>>> configuration = Data2VecAudioConfig()

>>> # Initializing a model (with random weights) from the facebook/data2vec-audio-base-960h style configuration
>>> model = Data2VecAudioModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Data2VecVisionConfig

### `class transformers.Data2VecVisionConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_vision.py#L35)

```py
( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 use_mask_token = False use_absolute_position_embeddings = False use_relative_position_bias = False use_shared_relative_position_bias = False layer_scale_init_value = 0.1 drop_path_rate = 0.1 use_mean_pooling = True out_indices = [3, 5, 7, 11] pool_scales = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input = False semantic_loss_ignore_index = 255 **kwargs )
```

å‚æ•°

+   `hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `image_size` (`int`, *optional*, defaults to 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size` (`int`, *optional*, defaults to 16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“çš„æ•°é‡ã€‚

+   `use_mask_token` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ©è”½å›¾åƒå»ºæ¨¡ä¸­ä½¿ç”¨æ©è”½æ ‡è®°ã€‚

+   `use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨ç±»ä¼¼ BERT çš„ç»å¯¹ä½ç½®åµŒå…¥ã€‚

+   `use_relative_position_bias` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ T5 é£æ ¼çš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚

+   `use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨ Transformer çš„æ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ç›¸åŒçš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚

+   `layer_scale_init_value` (`float`, *optional*, defaults to 0.1) â€” è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨çš„æ¯”ä¾‹ã€‚åŸºç¡€ä¸º 0.1ï¼Œå¤§å‹ä¸º 1e-5ã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨å±‚æ¯”ä¾‹ã€‚

+   `drop_path_rate` (`float`, *optional*, defaults to 0.1) â€” æ¯ä¸ªæ ·æœ¬çš„éšæœºæ·±åº¦ç‡ï¼ˆå½“åº”ç”¨äºæ®‹å·®å±‚çš„ä¸»è·¯å¾„æ—¶ï¼‰ã€‚

+   `use_mean_pooling` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å¯¹è¡¥ä¸çš„æœ€ç»ˆéšè—çŠ¶æ€è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ CLS æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ååº”ç”¨åˆ†ç±»å¤´ã€‚

+   `out_indices` (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`) â€” ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç‰¹å¾å›¾çš„ç´¢å¼•ã€‚

+   `pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) â€” åº”ç”¨äºæœ€åç‰¹å¾å›¾çš„æ± åŒ–é‡‘å­—å¡”æ¨¡å—ä¸­ä½¿ç”¨çš„æ± åŒ–æ¯”ä¾‹ã€‚

+   `use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨è¾…åŠ©å¤´ã€‚

+   `auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±çš„æƒé‡ã€‚

+   `auxiliary_channels` (`int`, *optional*, defaults to 256) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„é€šé“æ•°ã€‚

+   `auxiliary_num_convs` (`int`, *optional*, defaults to 1) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„å·ç§¯å±‚æ•°é‡ã€‚

+   `auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨åˆ†ç±»å±‚ä¹‹å‰å°†è¾…åŠ©å¤´çš„è¾“å‡ºä¸è¾“å…¥è¿æ¥èµ·æ¥ã€‚

+   `semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚

è¿™æ˜¯å­˜å‚¨[Data2VecVisionModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªData2VecVisionæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºData2VecVision [facebook/data2vec-vision-base](https://huggingface.co/facebook/data2vec-vision-base)æ¶æ„çš„é…ç½®ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Data2VecVisionConfig, Data2VecVisionModel

>>> # Initializing a Data2VecVision data2vec_vision-base-patch16-224-in22k style configuration
>>> configuration = Data2VecVisionConfig()

>>> # Initializing a model (with random weights) from the data2vec_vision-base-patch16-224-in22k style configuration
>>> model = Data2VecVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

PytorchHide Pytorchå†…å®¹

## Data2VecAudioModel

### `class transformers.Data2VecAudioModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L812)

```py
( config: Data2VecAudioConfig )
```

å‚æ•°

+   `config`ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸Data2VecAudioæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚Data2VecAudioæ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael Auliåœ¨[æ•°æ®2vecï¼šè¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L887)

```py
( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†*.flac*æˆ–*.wav*éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°*List[float]*æˆ–*numpy.ndarray*ç±»å‹çš„æ•°ç»„ä¸­è·å¾—å€¼ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ*pip install soundfile*ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ*input_values*ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸ºç±»å‹ä¸º*torch.FloatTensor*çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1è¡¨ç¤º`æœªè¢«æ©ç `çš„ä»¤ç‰Œï¼Œ

    +   0è¡¨ç¤º`è¢«æ©ç `çš„ä»¤ç‰Œã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¦‚æœç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`ï¼Œåˆ™åº”ä¼ é€’`attention_mask`ï¼Œè¿™é€‚ç”¨äºæ‰€æœ‰é¢„è®­ç»ƒçš„Data2VecéŸ³é¢‘æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ä½¿ç”¨`attention_mask`ï¼Œé›¶å¡«å……çš„è¾“å…¥ä¸éå¡«å……çš„è¾“å…¥å°†å…·æœ‰ç•¥æœ‰ä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºåœ¨ä½ç½®ç¼–ç ä¸­æœ‰å¤šä¸ªå·ç§¯å±‚ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è§£é‡Šï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length, conv_dim[-1])`) â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚æå–çš„ç‰¹å¾å‘é‡åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoProcessor, Data2VecAudioModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("facebook/data2vec-audio-base-960h")
>>> model = Data2VecAudioModel.from_pretrained("facebook/data2vec-audio-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 292, 768]
```

## Data2VecAudioForAudioFrameClassification

### `class transformers.Data2VecAudioForAudioFrameClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1196)

```py
( config )
```

å‚æ•°

+   `config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Data2VecAudioæ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ç”¨äºè¯´è¯äººåˆ†ç¦»ç­‰ä»»åŠ¡çš„å¸§åˆ†ç±»å¤´ã€‚

Data2VecAudioæ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael Auliåœ¨[æ•°æ®2vec: è¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1247)

```py
( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°† *.flac* æˆ– *.wav* éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° *List[float]* ç±»å‹çš„æ•°ç»„æˆ– *numpy.ndarray* ä¸­è·å¾—è¿™äº›å€¼ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (*pip install soundfile*)ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ *input_values*ï¼Œåº”è¯¥ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º *torch.FloatTensor* ç±»å‹çš„å¼ é‡ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§ [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` èŒƒå›´å†…ã€‚

    +   å¯¹äºæœªè¢« `masked` çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢« `masked` çš„æ ‡è®°ä¸º 0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¦‚æœç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True`ï¼Œåˆ™åº”ä¼ é€’ `attention_mask`ï¼Œè¿™é€‚ç”¨äºæ‰€æœ‰é¢„è®­ç»ƒçš„ Data2Vec éŸ³é¢‘æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ä½¿ç”¨äº† `attention_mask`ï¼Œé›¶å¡«å……çš„è¾“å…¥ä¸éå¡«å……çš„è¾“å…¥ä¼šæœ‰ç¨å¾®ä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºåœ¨ä½ç½®ç¼–ç ä¸­æœ‰å¤šä¸ªå·ç§¯å±‚ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è§£é‡Šï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤± (å‡æ–¹æŸå¤±)ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤± (äº¤å‰ç†µ)ã€‚

è¿”å›

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ (å¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶) åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½® ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)) å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`) â€” åˆ†ç±»åˆ†æ•° (SoftMax ä¹‹å‰)ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Data2VecAudioForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoFeatureExtractor, Data2VecAudioForAudioFrameClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/data2vec-audio-base-960h")
>>> model = Data2VecAudioForAudioFrameClassification.from_pretrained("facebook/data2vec-audio-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], return_tensors="pt", sampling_rate=sampling_rate)
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> probabilities = torch.sigmoid(logits[0])
>>> # labels is a one-hot array of shape (num_frames, num_speakers)
>>> labels = (probabilities > 0.5).long()
```

## Data2VecAudioForCTC

### `class transformers.Data2VecAudioForCTC`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L948)

```py
( config )
```

å‚æ•°

+   `config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Data2VecAudio æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰ `è¯­è¨€å»ºæ¨¡` å¤´éƒ¨ï¼Œç”¨äº Connectionist Temporal Classification (CTC)ã€‚Data2VecAudio æ˜¯ç”± Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Gu å’Œ Michael Auli åœ¨ [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) ä¸­æå‡ºçš„ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L993)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°† *.flac* æˆ– *.wav* éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° *List[float]* æˆ– *numpy.ndarray* ç±»å‹çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (*pip install soundfile*)ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ *input_values*ï¼Œåº”ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º *torch.FloatTensor* ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´ï¼š

    +   1 ç”¨äº `æœªè¢«æ©ç ` çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äº `è¢«æ©ç ` çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¦‚æœç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`ï¼Œåˆ™åº”ä¼ é€’`attention_mask`ï¼Œè¿™å¯¹äºæ‰€æœ‰é¢„è®­ç»ƒçš„Data2Vec Audioæ¨¡å‹éƒ½æ˜¯å¦‚æ­¤ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ä½¿ç”¨`attention_mask`ï¼Œé›¶å¡«å……çš„è¾“å…¥ä¸éå¡«å……çš„è¾“å…¥å°†å…·æœ‰ç¨æœ‰ä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºåœ¨ä½ç½®ç¼–ç ä¸­æœ‰å¤šä¸ªå·ç§¯å±‚ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è§£é‡Šï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)ã€‚

+   `output_attentions` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_length)`ï¼Œ*optional*) â€” è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œ`target_length`å¿…é¡»å°äºæˆ–ç­‰äºè¾“å‡ºlogitsçš„åºåˆ—é•¿åº¦ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size - 1]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨`[0, ..., config.vocab_size - 1]`ä¸­çš„æ ‡ç­¾ã€‚

è¿”å›

[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*optional*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ª+æ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, Data2VecAudioForCTC
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("facebook/data2vec-audio-base-960h")
>>> model = Data2VecAudioForCTC.from_pretrained("facebook/data2vec-audio-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
>>> predicted_ids = torch.argmax(logits, dim=-1)

>>> # transcribe speech
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription[0]
'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'

>>> inputs["labels"] = processor(text=dataset[0]["text"], return_tensors="pt").input_ids

>>> # compute loss
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
66.95
```

## Data2VecAudioForSequenceClassification

### `class transformers.Data2VecAudioForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1074)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰-æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Data2VecAudioæ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´ï¼ˆä¸€ä¸ªçº¿æ€§å±‚åœ¨æ± åŒ–è¾“å‡ºä¹‹ä¸Šï¼‰ç”¨äºç±»ä¼¼SUPERBå…³é”®è¯æ£€æµ‹çš„ä»»åŠ¡ã€‚

Data2VecAudioæ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael Auliåœ¨[æ•°æ®2vec: è¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1126)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°
