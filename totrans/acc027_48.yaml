- en: Kwargs Handlers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/kwargs](https://huggingface.co/docs/accelerate/package_reference/kwargs)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The following objects can be passed to the main [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how some PyTorch objects related to distributed training or mixed
    precision are created.
  prefs: []
  type: TYPE_NORMAL
- en: AutocastKwargs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class accelerate.AutocastKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how `torch.autocast` behaves. Please refer to the documentation of
    this [context manager](https://pytorch.org/docs/stable/amp.html#torch.autocast)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: DistributedDataParallelKwargs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class accelerate.DistributedDataParallelKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how your model is wrapped in a `torch.nn.parallel.DistributedDataParallel`.
    Please refer to the documentation of this [wrapper](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: '`gradient_as_bucket_view` is only available in PyTorch 1.7.0 and later versions.'
  prefs: []
  type: TYPE_NORMAL
- en: '`static_graph` is only available in PyTorch 1.11.0 and later versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: FP8RecipeKwargs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.FP8RecipeKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`backend` (`str`, *optional*, defaults to “msamp”) — Which FP8 engine to use.
    Must be one of `"msamp"` (MS-AMP) or `"te"` (TransformerEngine).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`margin` (`int`, *optional*, default to 0) — The margin to use for the gradient
    scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interval` (`int`, *optional*, default to 1) — The interval to use for how
    often the scaling factor is recomputed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fp8_format` (`str`, *optional*, default to “E4M3”) — The format to use for
    the FP8 recipe. Must be one of `E4M3` or `HYBRID`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amax_history_len` (`int`, *optional*, default to 1024) — The length of the
    history to use for the scaling factor computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amax_compute_algo` (`str`, *optional*, default to “most_recent”) — The algorithm
    to use for the scaling factor computation. Must be one of `max` or `most_recent`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`override_linear_precision` (`tuple` of three `bool`, *optional*, default to
    `(False, False, False)`) — Whether or not to execute `fprop`, `dgrad`, and `wgrad`
    GEMMS in higher precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimization_level` (`str`), one of `O1`, `O2`. (default is `O2`) — What level
    of 8-bit collective communication should be used with MS-AMP. In general:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O1: Weight gradients and `all_reduce` communications are done in fp8, reducing
    GPU memory usage and communication bandwidth'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O2: First-order optimizer states are in 8-bit, and second order states are
    in FP16. Only available when using Adam or AdamW. This maintains accuracy and
    can potentially save the highest memory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '03: Specifically for DeepSpeed, implements capabilities so weights and master
    weights of models are stored in FP8\. If `fp8` is selected and deepspeed is enabled,
    will be used by default. (Not available currently).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the recipe for FP8 mixed precision training
    with `transformer-engine` or `ms-amp`.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on `transformer-engine` args, please refer to the API [documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html).
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the `ms-amp` args, please refer to the Optimization
    Level [documentation](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To use MS-AMP as an engine, pass `backend="msamp"` and the `optimization_level`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: GradScalerKwargs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class accelerate.GradScalerKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the behavior of mixed precision, specifically how the `torch.cuda.amp.GradScaler`
    used is created. Please refer to the documentation of this [scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: '`GradScaler` is only available in PyTorch 1.5.0 and later versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: InitProcessGroupKwargs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class accelerate.InitProcessGroupKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the distributed processes. Please refer to
    the documentation of this [method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
