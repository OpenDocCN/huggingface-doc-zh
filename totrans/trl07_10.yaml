- en: Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: 'Original text: [https://huggingface.co/docs/trl/models](https://huggingface.co/docs/trl/models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/trl/models](https://huggingface.co/docs/trl/models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: With the `AutoModelForCausalLMWithValueHead` class TRL supports all decoder
    model architectures in transformers such as GPT-2, OPT, and GPT-Neo. In addition,
    with `AutoModelForSeq2SeqLMWithValueHead` you can use encoder-decoder architectures
    such as T5\. TRL also requires reference models which are frozen copies of the
    model that is trained. With `create_reference_model` you can easily create a frozen
    copy and also share layers between the two models to save memory.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`AutoModelForCausalLMWithValueHead`类，TRL支持transformers中的所有解码器模型架构，如GPT-2、OPT和GPT-Neo。此外，使用`AutoModelForSeq2SeqLMWithValueHead`，您可以使用编码器-解码器架构，如T5。TRL还需要参考模型，这是训练的模型的冻结副本。使用`create_reference_model`，您可以轻松创建一个冻结副本，并在两个模型之间共享层以节省内存。
- en: PreTrainedModelWrapper
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTrainedModelWrapper
- en: '### `class trl.PreTrainedModelWrapper`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.PreTrainedModelWrapper`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L59)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L59)'
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A wrapper class around a (`transformers.PreTrainedModel`) to be compatible with
    the (`~transformers.PreTrained`) class in order to keep some attributes and methods
    of the (`~transformers.PreTrainedModel`) class.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个围绕(`transformers.PreTrainedModel`)的包装类，以便与(`~transformers.PreTrained`)类兼容，以保留(`~transformers.PreTrainedModel`)类的一些属性和方法。
- en: '#### `add_and_load_reward_modeling_adapter`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_and_load_reward_modeling_adapter`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L440)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L440)'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Add and load a reward modeling adapter. This method can only be used if the
    model is a `PeftModel` and if you have initialized the model with the `reward_modeling_adapter_id`
    argument, pointing to the id of the reward modeling adapter. The latest needs
    also to contain the score head in order to produce the reward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 添加和加载奖励建模适配器。只有当模型是`PeftModel`并且您已经使用`reward_modeling_adapter_id`参数初始化了模型，指向奖励建模适配器的id时，才能使用此方法。最新的还需要包含得分头以产生奖励。
- en: '#### `compute_reward_score`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_reward_score`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L571)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L571)'
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Computes the reward score for a given input. The method has first to enable
    the adapter and then compute the reward score. After that the model disables the
    reward modeling adapter and enables the default ppo adapter again.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算给定输入的奖励分数。该方法首先要启用适配器，然后计算奖励分数。之后，模型禁用奖励建模适配器，并再次启用默认的ppo适配器。
- en: '#### `from_pretrained`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L107)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L107)'
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name_or_path` (`str` or `transformers.PreTrainedModel`) —
    The path to the pretrained model or its name.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path` (`str` 或 `transformers.PreTrainedModel`) —
    预训练模型的路径或名称。'
- en: '`*model_args` (`list`, *optional*)) — Additional positional arguments passed
    along to the underlying model’s `from_pretrained` method.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*model_args` (`list`, *可选*)) — 传递给基础模型的`from_pretrained`方法的额外位置参数。'
- en: '*`*kwargs` (`dict`, *optional*) — Additional keyword arguments passed along
    to the underlying model’s `from_pretrained` method. We also pre-process the kwargs
    to extract the arguments that are specific to the `transformers.PreTrainedModel`
    class and the arguments that are specific to trl models. The kwargs also support
    `prepare_model_for_kbit_training` arguments from `peft` library.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs` (`dict`, *可选*) — 传递给基础模型的`from_pretrained`方法的额外关键字参数。我们还预处理kwargs以提取特定于`transformers.PreTrainedModel`类和特定于trl模型的参数。kwargs还支持来自`peft`库的`prepare_model_for_kbit_training`参数。'
- en: Instantiates a new model from a pretrained model from `transformers`. The pretrained
    model is loaded using the `from_pretrained` method of the `transformers.PreTrainedModel`
    class. The arguments that are specific to the `transformers.PreTrainedModel` class
    are passed along this method and filtered out from the `kwargs` argument.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从`transformers`的预训练模型实例化一个新模型。预训练模型是使用`transformers.PreTrainedModel`类的`from_pretrained`方法加载的。特定于`transformers.PreTrainedModel`类的参数通过此方法传递，并从`kwargs`参数中过滤出来。
- en: '#### `post_init`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_init`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L563)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L563)'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Post initialization method. This method is called after the model is instantiated
    and loaded from a checkpoint. It can be used to perform additional operations
    such as loading the state_dict.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 后初始化方法。此方法在模型实例化并从检查点加载后调用。它可用于执行其他操作，如加载state_dict。
- en: '#### `push_to_hub`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L512)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L512)'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`*args` (`list`, *optional*) — Positional arguments passed along to the underlying
    model’s `push_to_hub` method.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*args` (`list`, *可选*) — 传递给基础模型的`push_to_hub`方法的位置参数。'
- en: '*`*kwargs` (`dict`, *optional*) — Keyword arguments passed along to the underlying
    model’s `push_to_hub` method.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs` (`dict`, *可选*) — 传递给基础模型的`push_to_hub`方法的关键字参数。'
- en: Push the pretrained model to the hub. This method is a wrapper around `transformers.PreTrainedModel.push_to_hub`.
    Please refer to the documentation of `transformers.PreTrainedModel.push_to_hub`
    for more information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将预训练模型推送到hub。此方法是`transformers.PreTrainedModel.push_to_hub`的包装器。有关更多信息，请参阅`transformers.PreTrainedModel.push_to_hub`的文档。
- en: '#### `save_pretrained`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L528)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L528)'
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`*args` (`list`, *optional*) — Positional arguments passed along to the underlying
    model’s `save_pretrained` method.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*args` (`list`, *可选*) — 传递给基础模型的 `save_pretrained` 方法的位置参数。'
- en: '*`*kwargs` (`dict`, *optional*) — Keyword arguments passed along to the underlying
    model’s `save_pretrained` method.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs`（`dict`，*可选*）— 传递给底层模型的`save_pretrained`方法的关键字参数。'
- en: Save the pretrained model to a directory. This method is a wrapper around `transformers.PreTrainedModel.save_pretrained`.
    Please refer to the documentation of `transformers.PreTrainedModel.save_pretrained`
    for more information.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将预训练模型保存到目录中。此方法是`transformers.PreTrainedModel.save_pretrained`的包装器。请参考`transformers.PreTrainedModel.save_pretrained`的文档以获取更多信息。
- en: '#### `state_dict`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `state_dict`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L557)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L557)'
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Return the state_dict of the pretrained model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 返回预训练模型的state_dict。
- en: AutoModelForCausalLMWithValueHead
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoModelForCausalLMWithValueHead
- en: '### `class trl.AutoModelForCausalLMWithValueHead`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.AutoModelForCausalLMWithValueHead`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L61)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L61)'
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: An autoregressive model with a value head in addition to the language model
    head. This class inherits from `~trl.PreTrainedModelWrapper` and wraps a `transformers.PreTrainedModel`
    class. The wrapper class supports classic functions such as `from_pretrained`,
    `push_to_hub` and `generate`. To call a method of the wrapped model, simply manipulate
    the `pretrained_model` attribute of this class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自回归模型，除了语言模型头之外还有一个值头。此类继承自`~trl.PreTrainedModelWrapper`，并包装了一个`transformers.PreTrainedModel`类。包装器类支持经典函数，如`from_pretrained`，`push_to_hub`和`generate`。要调用包装模型的方法，只需操作此类的`pretrained_model`属性。
- en: 'Class attributes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类属性：
- en: '`transformers_parent_class` (`transformers.PreTrainedModel`) — The parent class
    of the wrapped model. This should be set to `transformers.AutoModelForCausalLM`
    for this class.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers_parent_class`（`transformers.PreTrainedModel`）— 包装模型的父类。对于此类，应设置为`transformers.AutoModelForCausalLM`。'
- en: '`lm_head_namings` (`tuple`) — A tuple of strings that are used to identify
    the language model head of the wrapped model. This is set to `("lm_head", "embed_out")`
    for this class but can be changed for other models in the future'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_head_namings`（`tuple`）— 用于识别包装模型的语言模型头的字符串元组。对于此类，设置为`("lm_head", "embed_out")`，但可以在将来更改为其他模型'
- en: '`supported_args` (`tuple`) — A tuple of strings that are used to identify the
    arguments that are supported by the `ValueHead` class. Currently, the supported
    args are:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supported_args`（`tuple`）— 用于识别`ValueHead`类支持的参数的字符串元组。目前支持的参数为：'
- en: '`summary_dropout_prob` (`float`, `optional`, defaults to `None`) — The dropout
    probability for the `ValueHead` class.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_dropout_prob`（`float`，`可选`，默认为`None`）— `ValueHead`类的丢弃概率。'
- en: '`v_head_initializer_range` (`float`, `optional`, defaults to `0.2`) — The initializer
    range for the `ValueHead` if a specific initialization strategy is selected.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`v_head_initializer_range`（`float`，`可选`，默认为`0.2`）— 如果选择了特定的初始化策略，则`ValueHead`的初始化器范围。'
- en: '`v_head_init_strategy` (`str`, `optional`, defaults to `None`) — The initialization
    strategy for the `ValueHead`. Currently, the supported strategies are:'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`v_head_init_strategy`（`str`，`可选`，默认为`None`）— `ValueHead`的初始化策略。目前支持的策略有：'
- en: '`None` — Initializes the weights of the `ValueHead` with a random distribution.
    This is the default strategy.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`None`— 使用随机分布初始化`ValueHead`的权重。这是默认策略。'
- en: '`“normal”` — Initializes the weights of the `ValueHead` with a normal distribution.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“normal”`— 使用正态分布初始化`ValueHead`的权重。'
- en: '#### `__init__`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__init__`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L96)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L96)'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model` (`transformers.PreTrainedModel`) — The model to wrap. It
    should be a causal language model such as GPT2. or any model mapped inside the
    `AutoModelForCausalLM` class.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model`（`transformers.PreTrainedModel`）— 要包装的模型。它应该是一个因果语言模型，如GPT2。或者任何映射到`AutoModelForCausalLM`类内部的模型。'
- en: '`kwargs` (`dict`, `optional`) — Additional keyword arguments, that are passed
    to the `ValueHead` class.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`dict`，`可选`）— 传递给`ValueHead`类的额外关键字参数。'
- en: Initializes the model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化模型。
- en: '#### `forward`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L140)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L140)'
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (*torch.LongTensor* of shape *(batch_size, sequence_length)*) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为*(batch_size, sequence_length)*的*torch.LongTensor*）— 词汇表中输入序列令牌的索引。'
- en: '`past_key_values` (*tuple(tuple(torch.FloatTensor))*, *optional*) — Contains
    pre-computed hidden-states (key and values in the attention blocks) as computed
    by the model (see *past_key_values* input) to speed up sequential decoding.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（*tuple(tuple(torch.FloatTensor))*，*可选*）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值，如*past_key_values*输入）以加速顺序解码。'
- en: '`attention_mask` (*torch.FloatTensor* of shape *(batch_size, sequence_length)*,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（*torch.FloatTensor*，形状为*(batch_size, sequence_length)*，*可选*）—
    用于避免在填充令牌索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未被掩码`的令牌为1，
- en: 0 for tokens that are `masked`.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被掩码`的令牌为0。
- en: '`kwargs` (*dict*, *optional*) — Additional keyword arguments, that are passed
    to the wrapped model.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（*dict*，*可选*）— 传递给包装模型的额外关键字参数。'
- en: Applies a forward pass to the wrapped model and returns the logits of the value
    head.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对包装模型执行前向传递，并返回值头的logits。
- en: '#### `generate`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L190)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L190)'
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`*args` (`list`, *optional*) — Positional arguments passed to the `generate`
    method of the wrapped model.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*args`（`list`，*可选*）— 传递给包装模型的`generate`方法的位置参数。'
- en: '*`*kwargs` (`dict`, *optional*) — Keyword arguments passed to the `generate`
    method of the wrapped model.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs` (`dict`, *optional*) — 传递给包装模型的`generate`方法的关键字参数。'
- en: A simple wrapper around the `generate` method of the wrapped model. Please refer
    to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)
    method of the wrapped model for more information about the supported arguments.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 包装模型的`generate`方法的简单包装器。有关支持的参数的更多信息，请参阅包装模型的[`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)方法。
- en: '#### `_init_weights`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `_init_weights`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L117)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L117)'
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '*`*kwargs` (`dict`, `optional`) — Additional keyword arguments, that are passed
    to the `ValueHead` class. These arguments can contain the `v_head_init_strategy`
    argument as well as the `v_head_initializer_range` argument.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs` (`dict`, `optional`) — 附加的关键字参数，传递给`ValueHead`类。这些参数可以包含`v_head_init_strategy`参数以及`v_head_initializer_range`参数。'
- en: 'Initializes the weights of the value head. The default initialization strategy
    is random. Users can pass a different initialization strategy by passing the `v_head_init_strategy`
    argument when calling `.from_pretrained`. Supported strategies are:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化值头的权重。默认的初始化策略是随机的。用户可以通过在调用`.from_pretrained`时传递`v_head_init_strategy`参数来传递不同的初始化策略。支持的策略有：
- en: '`normal`: initializes the weights with a normal distribution.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normal`: 使用正态分布初始化权重。'
- en: AutoModelForSeq2SeqLMWithValueHead
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoModelForSeq2SeqLMWithValueHead
- en: '### `class trl.AutoModelForSeq2SeqLMWithValueHead`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.AutoModelForSeq2SeqLMWithValueHead`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L264)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L264)'
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model` (`transformers.PreTrainedModel`) — The model to wrap. It
    should be a causal language model such as GPT2. or any model mapped inside the
    `AutoModelForSeq2SeqLM` class. kwargs — Additional keyword arguments passed along
    to the `ValueHead` class.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model` (`transformers.PreTrainedModel`) — 要包装的模型。它应该是一个因果语言模型，如GPT2。或者映射到`AutoModelForSeq2SeqLM`类内部的任何模型。kwargs
    — 传递给`ValueHead`类的附加关键字参数。'
- en: A seq2seq model with a value head in addition to the language model head. This
    class inherits from `~trl.PreTrainedModelWrapper` and wraps a `transformers.PreTrainedModel`
    class. The wrapper class supports classic functions such as `from_pretrained`
    and `push_to_hub` and also provides some additional functionalities such as `generate`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了语言模型头之外还带有值头的seq2seq模型。这个类继承自`~trl.PreTrainedModelWrapper`，并包装了一个`transformers.PreTrainedModel`类。包装器类支持经典功能，如`from_pretrained`和`push_to_hub`，还提供一些额外的功能，如`generate`。
- en: '#### `__init__`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__init__`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L287)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L287)'
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#### `forward`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L395)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L395)'
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#### `generate`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L425)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L425)'
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We call `generate` on the wrapped model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在包装的模型上调用`generate`。
- en: '#### `_init_weights`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `_init_weights`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L381)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L381)'
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We initialize the weights of the value head.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化值头的权重。
- en: create_reference_model
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: create_reference_model
- en: '#### `trl.create_reference_model`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `trl.create_reference_model`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L602)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L602)'
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`PreTrainedModelWrapper`) — The model to be copied.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`PreTrainedModelWrapper`) — 要复制的模型。'
- en: '`num_shared_layers` (`int`, *optional*) — The number of initial layers that
    are shared between both models and kept frozen.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_shared_layers` (`int`, *optional*) — 在两个模型之间共享并保持冻结的初始层的数量。'
- en: '`pattern` (`str`, *optional*) — The shared layers are selected with a string
    pattern (e.g. “transformer.h.{layer}” for GPT2) and if a custom pattern is necessary
    it can be passed here.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pattern` (`str`, *optional*) — 使用字符串模式选择共享层（例如，“transformer.h.{layer}”用于GPT2），如果需要自定义模式，则可以在这里传递。'
- en: Creates a static reference copy of a model. Note that model will be in `.eval()`
    mode.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型的静态参考副本。请注意，模型将处于`.eval()`模式。
- en: Returns `PreTrainedModelWrapper`
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 返回`PreTrainedModelWrapper`
