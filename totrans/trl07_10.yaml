- en: Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/trl/models](https://huggingface.co/docs/trl/models)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: With the `AutoModelForCausalLMWithValueHead` class TRL supports all decoder
    model architectures in transformers such as GPT-2, OPT, and GPT-Neo. In addition,
    with `AutoModelForSeq2SeqLMWithValueHead` you can use encoder-decoder architectures
    such as T5\. TRL also requires reference models which are frozen copies of the
    model that is trained. With `create_reference_model` you can easily create a frozen
    copy and also share layers between the two models to save memory.
  prefs: []
  type: TYPE_NORMAL
- en: PreTrainedModelWrapper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.PreTrainedModelWrapper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A wrapper class around a (`transformers.PreTrainedModel`) to be compatible with
    the (`~transformers.PreTrained`) class in order to keep some attributes and methods
    of the (`~transformers.PreTrainedModel`) class.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_and_load_reward_modeling_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L440)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Add and load a reward modeling adapter. This method can only be used if the
    model is a `PeftModel` and if you have initialized the model with the `reward_modeling_adapter_id`
    argument, pointing to the id of the reward modeling adapter. The latest needs
    also to contain the score head in order to produce the reward.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_reward_score`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L571)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Computes the reward score for a given input. The method has first to enable
    the adapter and then compute the reward score. After that the model disables the
    reward modeling adapter and enables the default ppo adapter again.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L107)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `transformers.PreTrainedModel`) —
    The path to the pretrained model or its name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*model_args` (`list`, *optional*)) — Additional positional arguments passed
    along to the underlying model’s `from_pretrained` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (`dict`, *optional*) — Additional keyword arguments passed along
    to the underlying model’s `from_pretrained` method. We also pre-process the kwargs
    to extract the arguments that are specific to the `transformers.PreTrainedModel`
    class and the arguments that are specific to trl models. The kwargs also support
    `prepare_model_for_kbit_training` arguments from `peft` library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiates a new model from a pretrained model from `transformers`. The pretrained
    model is loaded using the `from_pretrained` method of the `transformers.PreTrainedModel`
    class. The arguments that are specific to the `transformers.PreTrainedModel` class
    are passed along this method and filtered out from the `kwargs` argument.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L563)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Post initialization method. This method is called after the model is instantiated
    and loaded from a checkpoint. It can be used to perform additional operations
    such as loading the state_dict.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L512)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`*args` (`list`, *optional*) — Positional arguments passed along to the underlying
    model’s `push_to_hub` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (`dict`, *optional*) — Keyword arguments passed along to the underlying
    model’s `push_to_hub` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push the pretrained model to the hub. This method is a wrapper around `transformers.PreTrainedModel.push_to_hub`.
    Please refer to the documentation of `transformers.PreTrainedModel.push_to_hub`
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L528)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`*args` (`list`, *optional*) — Positional arguments passed along to the underlying
    model’s `save_pretrained` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (`dict`, *optional*) — Keyword arguments passed along to the underlying
    model’s `save_pretrained` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the pretrained model to a directory. This method is a wrapper around `transformers.PreTrainedModel.save_pretrained`.
    Please refer to the documentation of `transformers.PreTrainedModel.save_pretrained`
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `state_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L557)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Return the state_dict of the pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: AutoModelForCausalLMWithValueHead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.AutoModelForCausalLMWithValueHead`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L61)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: An autoregressive model with a value head in addition to the language model
    head. This class inherits from `~trl.PreTrainedModelWrapper` and wraps a `transformers.PreTrainedModel`
    class. The wrapper class supports classic functions such as `from_pretrained`,
    `push_to_hub` and `generate`. To call a method of the wrapped model, simply manipulate
    the `pretrained_model` attribute of this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Class attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers_parent_class` (`transformers.PreTrainedModel`) — The parent class
    of the wrapped model. This should be set to `transformers.AutoModelForCausalLM`
    for this class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lm_head_namings` (`tuple`) — A tuple of strings that are used to identify
    the language model head of the wrapped model. This is set to `("lm_head", "embed_out")`
    for this class but can be changed for other models in the future'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supported_args` (`tuple`) — A tuple of strings that are used to identify the
    arguments that are supported by the `ValueHead` class. Currently, the supported
    args are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_dropout_prob` (`float`, `optional`, defaults to `None`) — The dropout
    probability for the `ValueHead` class.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v_head_initializer_range` (`float`, `optional`, defaults to `0.2`) — The initializer
    range for the `ValueHead` if a specific initialization strategy is selected.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v_head_init_strategy` (`str`, `optional`, defaults to `None`) — The initialization
    strategy for the `ValueHead`. Currently, the supported strategies are:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`None` — Initializes the weights of the `ValueHead` with a random distribution.
    This is the default strategy.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“normal”` — Initializes the weights of the `ValueHead` with a normal distribution.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__init__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model` (`transformers.PreTrainedModel`) — The model to wrap. It
    should be a causal language model such as GPT2. or any model mapped inside the
    `AutoModelForCausalLM` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`, `optional`) — Additional keyword arguments, that are passed
    to the `ValueHead` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializes the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L140)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (*torch.LongTensor* of shape *(batch_size, sequence_length)*) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (*tuple(tuple(torch.FloatTensor))*, *optional*) — Contains
    pre-computed hidden-states (key and values in the attention blocks) as computed
    by the model (see *past_key_values* input) to speed up sequential decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (*torch.FloatTensor* of shape *(batch_size, sequence_length)*,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*dict*, *optional*) — Additional keyword arguments, that are passed
    to the wrapped model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies a forward pass to the wrapped model and returns the logits of the value
    head.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L190)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`*args` (`list`, *optional*) — Positional arguments passed to the `generate`
    method of the wrapped model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (`dict`, *optional*) — Keyword arguments passed to the `generate`
    method of the wrapped model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple wrapper around the `generate` method of the wrapped model. Please refer
    to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)
    method of the wrapped model for more information about the supported arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `_init_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L117)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*`*kwargs` (`dict`, `optional`) — Additional keyword arguments, that are passed
    to the `ValueHead` class. These arguments can contain the `v_head_init_strategy`
    argument as well as the `v_head_initializer_range` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initializes the weights of the value head. The default initialization strategy
    is random. Users can pass a different initialization strategy by passing the `v_head_init_strategy`
    argument when calling `.from_pretrained`. Supported strategies are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`normal`: initializes the weights with a normal distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoModelForSeq2SeqLMWithValueHead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.AutoModelForSeq2SeqLMWithValueHead`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L264)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model` (`transformers.PreTrainedModel`) — The model to wrap. It
    should be a causal language model such as GPT2. or any model mapped inside the
    `AutoModelForSeq2SeqLM` class. kwargs — Additional keyword arguments passed along
    to the `ValueHead` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seq2seq model with a value head in addition to the language model head. This
    class inherits from `~trl.PreTrainedModelWrapper` and wraps a `transformers.PreTrainedModel`
    class. The wrapper class supports classic functions such as `from_pretrained`
    and `push_to_hub` and also provides some additional functionalities such as `generate`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__init__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L287)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L395)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L425)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We call `generate` on the wrapped model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `_init_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_value_head.py#L381)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We initialize the weights of the value head.
  prefs: []
  type: TYPE_NORMAL
- en: create_reference_model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `trl.create_reference_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/models/modeling_base.py#L602)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`PreTrainedModelWrapper`) — The model to be copied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shared_layers` (`int`, *optional*) — The number of initial layers that
    are shared between both models and kept frozen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pattern` (`str`, *optional*) — The shared layers are selected with a string
    pattern (e.g. “transformer.h.{layer}” for GPT2) and if a custom pattern is necessary
    it can be passed here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates a static reference copy of a model. Note that model will be in `.eval()`
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Returns `PreTrainedModelWrapper`
  prefs: []
  type: TYPE_NORMAL
