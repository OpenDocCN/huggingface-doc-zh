- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/optimization/opt_overview](https://huggingface.co/docs/diffusers/optimization/opt_overview)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/128.854833d1.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: Generating high-quality outputs is computationally intensive, especially during
    each iterative step where you go from a noisy output to a less noisy output. One
    of ðŸ¤— Diffuserâ€™s goals is to make this technology widely accessible to everyone,
    which includes enabling fast inference on consumer and specialized hardware.
  prefs: []
  type: TYPE_NORMAL
- en: This section will cover tips and tricks - like half-precision weights and sliced
    attention - for optimizing inference speed and reducing memory-consumption. Youâ€™ll
    also learn how to speed up your PyTorch code with [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    or [ONNX Runtime](https://onnxruntime.ai/docs/), and enable memory-efficient attention
    with [xFormers](https://facebookresearch.github.io/xformers/). There are also
    guides for running inference on specific hardware like Apple Silicon, and Intel
    or Habana processors.
  prefs: []
  type: TYPE_NORMAL
