- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/opt_overview](https://huggingface.co/docs/diffusers/optimization/opt_overview)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Generating high-quality outputs is computationally intensive, especially during
    each iterative step where you go from a noisy output to a less noisy output. One
    of ðŸ¤— Diffuserâ€™s goals is to make this technology widely accessible to everyone,
    which includes enabling fast inference on consumer and specialized hardware.
  prefs: []
  type: TYPE_NORMAL
- en: This section will cover tips and tricks - like half-precision weights and sliced
    attention - for optimizing inference speed and reducing memory-consumption. Youâ€™ll
    also learn how to speed up your PyTorch code with [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    or [ONNX Runtime](https://onnxruntime.ai/docs/), and enable memory-efficient attention
    with [xFormers](https://facebookresearch.github.io/xformers/). There are also
    guides for running inference on specific hardware like Apple Silicon, and Intel
    or Habana processors.
  prefs: []
  type: TYPE_NORMAL
