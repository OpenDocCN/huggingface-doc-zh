- en: Examples of using peft with trl to finetune 8-bit models with Low Rank Adaption
    (LoRA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/trl/lora_tuning_peft](https://huggingface.co/docs/trl/lora_tuning_peft)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/trl/v0.7.10/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/start.d9a24ea1.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/singletons.9eef12cc.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/paths.1355483e.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/app.5bef33b8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/index.ded8f90d.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/0.abccdcd8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/14.1dab6372.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/CodeBlock.8580f3e8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/Heading.f027f30d.js">
  prefs: []
  type: TYPE_NORMAL
- en: The notebooks and scripts in this examples show how to use Low Rank Adaptation
    (LoRA) to fine-tune models in a memory efficient manner. Most of PEFT methods
    supported in peft library but note that some PEFT methods such as Prompt tuning
    are not supported. For more information on LoRA, see the [original paper](https://arxiv.org/abs/2106.09685).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an overview of the `peft`-enabled notebooks and scripts in the [trl
    repository](https://github.com/huggingface/trl/tree/main/examples):'
  prefs: []
  type: TYPE_NORMAL
- en: '| File | Task | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`stack_llama/rl_training.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/rl_training.py)
    | RLHF | Distributed fine-tuning of the 7b parameter LLaMA models with a learned
    reward model and `peft`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`stack_llama/reward_modeling.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py)
    | Reward Modeling | Distributed training of the 7b parameter LLaMA reward model
    with `peft`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`stack_llama/supervised_finetuning.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/supervised_finetuning.py)
    | SFT | Distributed instruction/supervised fine-tuning of the 7b parameter LLaMA
    model with `peft`. |'
  prefs: []
  type: TYPE_TB
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note: peft is in active development, so we install directly from their Github
    page. Peft also relies on the latest version of transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: if you don’t want to log with `wandb` remove `log_with="wandb"` in the
    scripts/notebooks. You can also replace it with your favourite experiment tracker
    that’s [supported by `accelerate`](https://huggingface.co/docs/accelerate/usage_guides/tracking).'
  prefs: []
  type: TYPE_NORMAL
- en: How to use it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply declare a `PeftConfig` object in your script and pass it through `.from_pretrained`
    to load the TRL+PEFT model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And if you want to load your model in 8bit precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '… or in 4bit precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Launch scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `trl` library is powered by `accelerate`. As such it is best to configure
    and launch trainings with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using trl + peft and Data Parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can scale up to as many GPUs as you want, as long as you are able to fit
    the training process in a single device. The only tweak you need to apply is to
    load the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And if you want to load your model in 8bit precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '… or in 4bit precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, make sure that the rewards are computed on correct device as well,
    for that you can use `ppo_trainer.model.current_device`.
  prefs: []
  type: TYPE_NORMAL
- en: Naive pipeline parallelism (NPP) for large models (>60B models)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `trl` library also supports naive pipeline parallelism (NPP) for large models
    (>60B models). This is a simple way to parallelize the model across multiple GPUs.
    This paradigm, termed as “Naive Pipeline Parallelism” (NPP) is a simple way to
    parallelize the model across multiple GPUs. We load the model and the adapters
    across multiple GPUs and the activations and gradients will be naively communicated
    across the GPUs. This supports `int8` models as well as other `dtype` models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cc560cbbb52fcacc592d467e360f3f4.png)'
  prefs: []
  type: TYPE_IMG
- en: How to use NPP?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simply load your model with a custom `device_map` argument on the `from_pretrained`
    to split your model across multiple devices. Check out this [nice tutorial](https://github.com/huggingface/blog/blob/main/accelerate-large-models.md)
    on how to properly create a `device_map` for your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also make sure to have the `lm_head` module on the first GPU device as it may
    throw an error if it is not on the first device. As this time of writing, you
    need to install the `main` branch of `accelerate`: `pip install git+https://github.com/huggingface/accelerate.git@main`
    and `peft`: `pip install git+https://github.com/huggingface/peft.git@main`.'
  prefs: []
  type: TYPE_NORMAL
- en: Launch scripts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although `trl` library is powered by `accelerate`, you should run your training
    script in a single process. Note that we do not support Data Parallelism together
    with NPP yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tuning Llama-2 model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can easily fine-tune Llama2 model using `SFTTrainer` and the official script!
    For example to fine-tune llama2-7b on the Guanaco dataset, run (tested on a single
    NVIDIA T4-16GB):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
