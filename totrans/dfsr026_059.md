# 稳定扩散 XL

> 原始文本：[`huggingface.co/docs/diffusers/training/sdxl`](https://huggingface.co/docs/diffusers/training/sdxl)

这个脚本是实验性的，很容易过拟合并遇到像灾难性遗忘这样的问题。尝试探索不同的超参数以获得数据集上的最佳结果。

[稳定扩散 XL（SDXL）](https://hf.co/papers/2307.01952)是稳定扩散模型的一个更大更强大的迭代版本，能够生成更高分辨率的图像。

SDXL 的 UNet 是 3 倍大，模型在架构中添加了第二个文本编码器。根据您可用的硬件，这可能非常计算密集，可能无法在像 Tesla T4 这样的消费者 GPU 上运行。为了将这个更大的模型适应内存并加快训练速度，请尝试启用`gradient_checkpointing`、`mixed_precision`和`gradient_accumulation_steps`。您还可以通过启用 xFormers 的内存高效注意力和使用[bitsandbytes'](https://github.com/TimDettmers/bitsandbytes) 8 位优化器来进一步减少内存使用。

本指南将探讨[train_text_to_image_sdxl.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py)训练脚本，以帮助您更熟悉它，以及如何为您自己的用例进行调整。

在运行脚本之前，请确保从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

然后转到包含训练脚本的示例文件夹，并为您正在使用的脚本安装所需的依赖项：

```py
cd examples/text_to_image
pip install -r requirements_sdxl.txt
```

🤗 Accelerate 是一个帮助您在多个 GPU/TPU 上或使用混合精度进行训练的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。

初始化一个🤗 Accelerate 环境：

```py
accelerate config
```

要设置一个默认的🤗 Accelerate 环境而不选择任何配置：

```py
accelerate config default
```

或者，如果您的环境不支持交互式 shell，如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看创建用于训练的数据集指南，了解如何创建适用于训练脚本的数据集。

## 脚本参数

以下各节突出了训练脚本的重要部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py)，并告诉我们您是否有任何问题或疑虑。

训练脚本提供了许多参数，以帮助您自定义训练运行。所有参数及其描述都可以在[`parse_args()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L129)函数中找到。该函数为每个参数提供了默认值，如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。

例如，为了使用 bf16 格式加快训练速度，将`--mixed_precision`参数添加到训练命令中：

```py
accelerate launch train_text_to_image_sdxl.py \
  --mixed_precision="bf16"
```

大多数参数与文本到图像训练指南中的参数相同，因此您将专注于本指南中与训练 SDXL 相关的参数。

+   `--pretrained_vae_model_name_or_path`：预训练 VAE 的路径；SDXL VAE 已知存在数值不稳定性问题，因此此参数允许您指定更好的[VAE](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)

+   `--proportion_empty_prompts`：要替换为空字符串的图像提示的比例

+   `--timestep_bias_strategy`：在时间步中应用偏差的位置（较早还是较晚），可以鼓励模型学习低频或高频细节

+   `--timestep_bias_multiplier`：应用于时间步的偏差权重

+   -- timestep_bias_begin：开始应用偏差的时间步

+   `--timestep_bias_end`：结束应用偏差的时间步

+   `--timestep_bias_portion`：应用偏差的时间步比例

### Min-SNR 加权

[Min-SNR](https://huggingface.co/papers/2303.09556)加权策略可以帮助训练，通过重新平衡损失来实现更快的收敛。训练脚本支持预测`epsilon`（噪声）或`v_prediction`，但 Min-SNR 与两种预测类型兼容。这种加权策略仅受 PyTorch 支持，在 Flax 训练脚本中不可用。

添加`--snr_gamma`参数，并将其设置为推荐值 5.0：

```py
accelerate launch train_text_to_image_sdxl.py \
  --snr_gamma=5.0
```

## 训练脚本

训练脚本也类似于文本到图像训练指南，但已经修改为支持 SDXL 训练。本指南将重点放在 SDXL 训练脚本中独特的代码上。

首先创建函数来[对提示进行标记化](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L478)以计算提示嵌入，并使用[VAE](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L519)计算图像嵌入。接下来，您将创建一个函数来[生成时间步权重](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L531)，具体取决于时间步数和要应用的时间步偏差策略。

在[`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L572)函数中，除了加载一个分词器外，脚本还加载了第二个分词器和文本编码器，因为 SDXL 架构使用了两个： 

```py
tokenizer_one = AutoTokenizer.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision, use_fast=False
)
tokenizer_two = AutoTokenizer.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="tokenizer_2", revision=args.revision, use_fast=False
)

text_encoder_cls_one = import_model_class_from_model_name_or_path(
    args.pretrained_model_name_or_path, args.revision
)
text_encoder_cls_two = import_model_class_from_model_name_or_path(
    args.pretrained_model_name_or_path, args.revision, subfolder="text_encoder_2"
)
```

首先计算并保存[提示和图像嵌入](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L857)，这通常对于较小的数据集不是问题，但对于较大的数据集可能会导致内存问题。如果是这种情况，您应该将预先计算的嵌入单独保存到磁盘，并在训练过程中将其加载到内存中（请参阅此[PR](https://github.com/huggingface/diffusers/pull/4505)以获取有关此主题的更多讨论）。

```py
text_encoders = [text_encoder_one, text_encoder_two]
tokenizers = [tokenizer_one, tokenizer_two]
compute_embeddings_fn = functools.partial(
    encode_prompt,
    text_encoders=text_encoders,
    tokenizers=tokenizers,
    proportion_empty_prompts=args.proportion_empty_prompts,
    caption_column=args.caption_column,
)

train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)
train_dataset = train_dataset.map(
    compute_vae_encodings_fn,
    batched=True,
    batch_size=args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps,
    new_fingerprint=new_fingerprint_for_vae,
)
```

计算嵌入后，文本编码器、VAE 和分词器将被删除以释放一些内存：

```py
del text_encoders, tokenizers, vae
gc.collect()
torch.cuda.empty_cache()
```

最后，[训练循环](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L943)会处理剩下的事情。如果选择应用时间步偏差策略，您将看到时间步权重被计算并添加为噪声：

```py
weights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(
        model_input.device
    )
    timesteps = torch.multinomial(weights, bsz, replacement=True).long()

noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)
```

如果您想了解训练循环的工作原理，请查看理解管道、模型和调度器教程，该教程解析了去噪过程的基本模式。

## 启动脚本

一旦您完成所有更改或对默认配置满意，您就可以启动训练脚本了！🚀

让我们在[Pokémon BLIP 标题](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)数据集上训练，生成您自己的 Pokémon。将环境变量`MODEL_NAME`和`DATASET_NAME`设置为模型和数据集（可以来自 Hub 或本地路径）。您还应该指定一个除 SDXL VAE 之外的 VAE（可以来自 Hub 或本地路径），并使用`VAE_NAME`避免数值不稳定。

使用 Weights＆Biases 监控训练进度，将`--report_to=wandb`参数添加到训练命令中。您还需要将`--validation_prompt`和`--validation_epochs`添加到训练命令中以跟踪结果。这对于调试模型和查看中间结果非常有用。

```py
export MODEL_NAME="stabilityai/stable-diffusion-xl-base-1.0"
export VAE_NAME="madebyollin/sdxl-vae-fp16-fix"
export DATASET_NAME="lambdalabs/pokemon-blip-captions"

accelerate launch train_text_to_image_sdxl.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --pretrained_vae_model_name_or_path=$VAE_NAME \
  --dataset_name=$DATASET_NAME \
  --enable_xformers_memory_efficient_attention \
  --resolution=512 \
  --center_crop \
  --random_flip \
  --proportion_empty_prompts=0.2 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --max_train_steps=10000 \
  --use_8bit_adam \
  --learning_rate=1e-06 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --mixed_precision="fp16" \
  --report_to="wandb" \
  --validation_prompt="a cute Sundar Pichai creature" \
  --validation_epochs 5 \
  --checkpointing_steps=5000 \
  --output_dir="sdxl-pokemon-model" \
  --push_to_hub
```

训练完成后，您可以使用新训练的 SDXL 模型进行推断！

PyTorchPyTorch XLA

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("path/to/your/model", torch_dtype=torch.float16).to("cuda")

prompt = "A pokemon with green eyes and red legs."
image = pipeline(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]
image.save("pokemon.png")
```

## 下一步

祝贺您训练了一个 SDXL 模型！要了解如何使用您的新模型，以下指南可能会有所帮助：

+   阅读 Stable Diffusion XL 指南，了解如何将其用于各种不同任务（文本到图像，图像到图像，修复），如何使用其细化器模型以及不同类型的微调。

+   查看 DreamBooth 和 LoRA 培训指南，了解如何仅使用几个示例图像训练个性化的 SDXL 模型。这两种训练技术甚至可以结合使用！
