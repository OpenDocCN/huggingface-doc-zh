["```py\npip install -U flash-attn --no-build-isolation\n```", "```py\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n\n>>> prompt = \"def hello_world():\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n>>> tokenizer.batch_decode(generated_ids)[0]\n'def hello_world():\\n    print(\"hello world\")\\n\\nif __name__ == \"__main__\":\\n    print(\"hello world\")\\n<|endoftext|>'\n```", "```py\n>>> from transformers import GPTBigCodeConfig, GPTBigCodeModel\n\n>>> # Initializing a GPTBigCode configuration\n>>> configuration = GPTBigCodeConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = GPTBigCodeModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, GPTBigCodeModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n>>> model = GPTBigCodeModel.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPTBigCodeForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n>>> model = GPTBigCodeForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```"]