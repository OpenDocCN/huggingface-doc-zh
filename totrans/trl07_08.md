# 日志记录

> 原始文本：[https://huggingface.co/docs/trl/logging](https://huggingface.co/docs/trl/logging)

由于强化学习算法在历史上很难调试，因此重要的是要仔细关注日志记录。默认情况下，TRL [PPOTrainer](/docs/trl/v0.7.10/en/trainer#trl.PPOTrainer)会将大量相关信息保存到`wandb`或`tensorboard`中。

在初始化时，将以下两个选项之一传递给[PPOConfig](/docs/trl/v0.7.10/en/trainer#trl.PPOConfig)：

```py
config = PPOConfig(
    model_name=args.model_name,
    log_with=`wandb`, # or `tensorboard`
)
```

如果要使用tensorboard进行日志记录，请将`project_kwargs={"logging_dir": PATH_TO_LOGS}`添加到PPOConfig中。

## PPO日志记录

以下是提供的数据中记录的指标的简要解释：

要监视的关键指标。我们希望最大化奖励，保持低KL散度，并最大化熵：

1.  `env/reward_mean`: 从环境中获得的平均奖励。别名`ppo/mean_scores'，用于专门监视奖励模型。

1.  `env/reward_std`: 环境获得奖励的标准差。别名``ppo/std_scores'，用于专门监视奖励模型。

1.  `env/reward_dist`: 从环境中获得的奖励的直方图分布。

1.  `objective/kl`: 旧政策和新政策之间的平均Kullback-Leibler（KL）散度。它衡量新政策偏离旧政策的程度。KL散度用于计算目标函数中的KL惩罚。

1.  `objective/kl_dist`: `objective/kl`的直方图分布。

1.  `objective/kl_coef`: 目标函数中Kullback-Leibler（KL）散度的系数。

1.  `ppo/mean_non_score_reward`: 由`objective/kl * objective/kl_coef`计算的**KL惩罚**，作为优化的总奖励，以防止新政策偏离太远旧政策。

1.  `objective/entropy`: 模型策略的熵，通过`-logprobs.sum(-1).mean()`计算。高熵意味着模型的动作更随机，这对于探索可能是有益的。

训练统计：

1.  `ppo/learning_rate`: PPO算法的学习率。

1.  `ppo/policy/entropy`: 模型策略的熵，通过`pd = torch.nn.functional.softmax(logits, dim=-1); entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)`计算。它衡量策略的随机性。

1.  `ppo/policy/clipfrac`: 在PPO目标中，概率比率（旧政策/新政策）超出剪切范围的比例。这可以用来监视优化过程。

1.  `ppo/policy/approxkl`: 旧政策和新政策之间的近似KL散度，由`0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)`测量，对应于[http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)中的`k2`估计器。

1.  `ppo/policy/policykl`: 类似于`ppo/policy/approxkl`，但由`masked_mean(old_logprobs - logprobs, mask)`测量，对应于[http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)中的`k1`估计器。

1.  `ppo/policy/ratio`: 新旧政策之间比率的直方图分布，用于计算PPO目标。

1.  `ppo/policy/advantages_mean`: GAE（广义优势估计）优势估计的平均值。优势函数衡量了一个动作相对于状态的平均动作有多好。

1.  `ppo/policy/advantages`: `ppo/policy/advantages_mean`的直方图分布。

1.  `ppo/returns/mean`: TD(λ)回报的平均值，通过`returns = advantage + values`计算，这是模型性能的另一个指标。有关更多详细信息，请参见[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)。

1.  `ppo/returns/var`: TD(λ)回报的方差，通过`returns = advantage + values`计算，这是模型性能的另一个指标。

1.  `ppo/val/mean`：值的平均值，用于监控值函数的性能。

1.  `ppo/val/var`：值的方差，用于监控值函数的性能。

1.  `ppo/val/var_explained`：值函数的解释方差，用于监控值函数的性能。

1.  `ppo/val/clipfrac`：值函数预测值被剪切的比例。

1.  `ppo/val/vpred`：值函数的预测值。

1.  `ppo/val/error`：`ppo/val/vpred`和returns之间的均方误差，用于监控值函数的性能。

1.  `ppo/loss/policy`：Proximal Policy Optimization (PPO)算法的策略损失。

1.  `ppo/loss/value`：PPO算法中值函数的损失。该值量化了函数估计未来预期奖励的能力。

1.  `ppo/loss/total`：PPO算法的总损失。它是策略损失和值函数损失的总和。

关于查询、响应和logprobs的统计数据：

1.  `tokens/queries_len_mean`：查询标记的平均长度。

1.  `tokens/queries_len_std`：查询标记长度的标准差。

1.  `tokens/queries_dist`：查询标记长度的直方图分布。

1.  `tokens/responses_len_mean`：响应标记的平均长度。

1.  `tokens/responses_len_std`：响应标记长度的标准差。

1.  `tokens/responses_dist`：响应标记长度的直方图分布。（Costa：命名不一致，应该是`tokens/responses_len_dist`）

1.  `objective/logprobs`：模型采取的动作的对数概率的直方图分布。

1.  `objective/ref_logprobs`：参考模型采取的动作的对数概率的直方图分布。

### 关键数值

在训练过程中，记录了许多数值，以下是最重要的数值：

1.  `env/reward_mean`,`env/reward_std`, `env/reward_dist`: 奖励分布的属性来自“环境”/奖励模型

1.  `ppo/mean_non_score_reward`：训练过程中的平均负的KL惩罚（显示参考模型和新策略在步骤中批次之间的差异）。

以下是一些有用的参数，用于监控稳定性（当这些参数发散或收敛为0时，请尝试调整变量）：

1.  `ppo/loss/value`：当情况不好时，它会激增/NaN。

1.  `ppo/policy/ratio`：`ratio`为1是一个基准值，意味着在新旧策略下采样标记的概率相同。如果比率太高，比如200，这意味着在新策略下采样标记的概率比旧策略高200倍。这表明新策略与旧策略差异太大，可能会导致过度优化和训练后期崩溃。

1.  `ppo/policy/clipfrac`和`ppo/policy/approxkl`：如果`ratio`太高，`ratio`将被剪切，导致`clipfrac`和`approxkl`也很高。

1.  `objective/kl`：应保持为正值，以便策略不会远离参考策略。

1.  `objective/kl_coef`：与`AdaptiveKLController`一起的目标系数。通常在数值不稳定之前增加。
