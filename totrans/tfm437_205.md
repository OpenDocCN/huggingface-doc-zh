# MPT

> 原始文本: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mpt)

## 概述

MPT 模型由 [MosaicML](https://www.mosaicml.com/) 团队提出，并发布了多种大小和微调变体。MPT 模型是一系列在 1T 令牌上预训练的开源和商业可用的LLM。

MPT 模型是类似于 GPT 的仅解码器变压器，具有几项改进：性能优化的层实现、提供更大训练稳定性的架构更改，以及通过用 ALiBi 替换位置嵌入来消除上下文长度限制。

+   MPT 基础: MPT 基础预训练模型用于下一个令牌预测

+   MPT 指导: MPT 基础模型在基于指令的任务上进行了微调

+   MPT 故事创作者: MPT 基础模型在 books3 语料库中的 65k 令牌摘录上进行了 2500 步的微调，这使得模型能够处理非常长的序列

原始代码可在 [`llm-foundry`](https://github.com/mosaicml/llm-foundry/tree/main) 存储库中找到。

了解更多 [在发布博客文章中](https://www.mosaicml.com/blog/mpt-7b)

## 使用提示

+   了解模型训练背后的一些技术 [在llm-foundry存储库的这一部分](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md#faqs)

+   如果要使用模型的高级版本（triton kernels、直接闪存注意力集成），仍然可以通过在调用 `from_pretrained` 时添加 `trust_remote_code=True` 来使用原始模型实现。

## 资源

+   [微调笔记本](https://colab.research.google.com/drive/1HCpQkLL7UXW8xJUJJ29X7QAeNJKO0frZ?usp=sharing) 关于如何在免费的 Google Colab 实例上微调 MPT-7B 以将模型转换为聊天机器人。

## MptConfig

### `class transformers.MptConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/configuration_mpt.py#L121)

```py
( d_model: int = 2048 n_heads: int = 16 n_layers: int = 24 expansion_ratio: int = 4 max_seq_len: int = 2048 vocab_size: int = 50368 resid_pdrop: float = 0.0 layer_norm_epsilon: float = 1e-05 emb_pdrop: float = 0.0 learned_pos_emb: bool = True attn_config: MptAttentionConfig = None init_device: str = 'cpu' logit_scale: Union = None no_bias: bool = True verbose: int = 0 embedding_fraction: float = 1.0 norm_type: str = 'low_precision_layernorm' use_cache: bool = False initializer_range = 0.02 **kwargs )
```

参数

+   `d_model` (`int`, *可选*, 默认为 2048) — 嵌入和隐藏状态的维度。

+   `n_heads` (`int`, *可选*, 默认为 16) — Transformer 编码器中每个注意力层的注意力头数。

+   `n_layers` (`int`, *可选*, 默认为 24) — Transformer 编码器中的隐藏层数量。

+   `expansion_ratio` (`int`, *可选*, 默认为 4) — MLP 中上/下缩放比率。

+   `max_seq_len` (`int`, *可选*, 默认为 2048) — 模型的最大序列长度。

+   `vocab_size` (`int`, *可选*, 默认为 50368) — Mpt 模型的词汇量。定义了在调用 [MptModel](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptModel) 时可以表示的不同令牌的最大数量。查看关于如何定义 `vocab_size` 的 [此讨论](https://huggingface.co/bigscience/mpt/discussions/120#633d28389addb8530b406c2a)。

+   `resid_pdrop` (`float`, *可选*, 默认为 0.0) — 在与残差结合之前应用于注意力输出的丢弃概率。

+   `layer_norm_epsilon` (`float`, *可选*, 默认为 1e-05) — 在层归一化层中使用的 epsilon。

+   `emb_pdrop` (`float`, *可选*, 默认为 0.0) — 嵌入层的丢弃概率。

+   `learned_pos_emb` (`bool`, *可选*, 默认为 `True`) — 是否使用学习的位置嵌入。

+   `attn_config` (`dict`, *可选*) — 用于配置模型注意力模块的字典。

+   `init_device` (`str`, *可选*, 默认为 `"cpu"`) — 用于参数初始化的设备。为了向后兼容性而定义

+   `logit_scale` (`float`, *可选*) — 如果不为 None，则通过此值缩放对数。

+   `no_bias` (`bool`, *可选*, 默认为 `True`) — 是否在所有线性层中使用偏置。

+   `verbose` (`int`, *可选*, 默认为 0) — 用于日志记录的详细级别。在先前版本的 MPT 模型中用于日志记录。此参数已弃用。

+   `embedding_fraction` (`float`, *optional*, defaults to 1.0) — 通过的嵌入层梯度的比例。

+   `norm_type` (`str`, *optional*, defaults to `"low_precision_layernorm"`) — 要使用的层归一化类型。所有 MPT 模型使用相同的层归一化实现。为了向后兼容性而定义。

+   `use_cache` (`bool`, *optional*, defaults to `False`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

这是用于存储 [MptModel](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptModel) 配置的配置类。它用于根据指定的参数实例化一个 Mpt 模型，定义模型架构。使用默认值实例化配置将产生类似于 Mpt-7b 架构 [mosaicml/mpt-7b](https://huggingface.co/mosaicml/mpt-7b) 的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import MptConfig, MptModel

>>> # Initializing a Mpt configuration
>>> configuration = MptConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = MptModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## MptModel

### `class transformers.MptModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L351)

```py
( config: MptConfig )
```

参数

+   `config` ([MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的 Mpt 模型变压器输出原始隐藏状态，没有特定的头部。

这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L387)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`) — `input_ids_length` = `sequence_length` 如果 `past_key_values` 是 `None`，否则 `past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了 `past_key_values`，则只应将未计算其过去的 `input_ids` 作为 `input_ids` 传递。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。查看 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 以获取详细信息。

    [什么是输入 ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含模型计算的预先计算的隐藏状态（注意力块中的键和值），如下面的`past_key_values`输出所示。可用于加速顺序解码。将过去给定给该模型的`input_ids`不应作为`input_ids`传递，因为它们已经被计算过。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充令牌索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示未被掩码的标记，

    +   0表示被掩码的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。

    如果使用`past_key_values`，可以选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回值

[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的输出的隐藏状态序列。

    如果使用`past_key_values`，则仅输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values`（*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及可选地如果`config.is_encoder_decoder=True`在交叉注意力块中）可用于加速顺序解码（参见`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当 `output_hidden_states=True` 被传递或者当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当 `output_attentions=True` 被传递或者当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当 `output_attentions=True` 和 `config.add_cross_attention=True` 被传递或者当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    解码器交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

[MptModel](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用 `Module` 实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, MptModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("mosaicml/mpt-7b")
>>> model = MptModel.from_pretrained("mosaicml/mpt-7b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## MptForCausalLM

### `class transformers.MptForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L505)

```py
( config: MptConfig )
```

参数

+   `config` ([MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法来加载模型权重。

带有语言建模头部的 MPT 模型变压器（线性层，其权重与输入嵌入绑定）。

该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入等）。

该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

`forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L566)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, input_ids_length)`） — 如果 `past_key_values` 是 `None`，则 `input_ids_length` = `sequence_length`，否则为 `past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了 `past_key_values`，只有那些未计算过去的 `input_ids` 应该被传递。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。查看 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 获取详细信息。

    [什么是输入 ID？](../glossary#input-ids)

+   `past_key_values` (`Tuple[Tuple[torch.Tensor]]` 长度为 `config.n_layers`) — 包含由模型计算的预计算隐藏状态（注意力块中的键和值），如下面的 `past_key_values` 输出所示。可用于加速顺序解码。将其过去给定给此模型的 `input_ids` 不应作为 `input_ids` 传递，因为它们已经计算过了。

    `past_key_values` 的每个元素都是一个元组（past_key, past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   对于未被掩盖的标记，为1，

    +   对于被掩盖的标记，为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果使用了 `past_key_values`，则可能只需输入最后的 `inputs_embeds`（参见 `past_key_values`）。

+   `use_cache` (`bool`, *optional*) — 如果设置为 `True`，则返回 `past_key_values` 键值状态，并可用于加速解码（参见 `past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 语言建模的标签。请注意，模型内部的标签 **已经被移位**，即您可以设置 `labels = input_ids` 索引选在 `[-100, 0, ..., config.vocab_size]` 所有设置为 `-100` 的标签都被忽略（掩盖），损失仅计算标签在 `[0, ..., config.vocab_size]` 中的标签。

返回

[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions) 或者 `torch.FloatTensor` 元组

一个 [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions) 或者一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或者当 `config.return_dict=False` 时）包含各种元素，取决于配置（[MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)）和输入。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True` 或者当 `config.output_hidden_states=True` 时返回) — 一个元组，包含 `torch.FloatTensor`（一个用于嵌入层的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）的形状为 `(batch_size, sequence_length, hidden_size)`。

    模型每一层的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回） — 长度为`config.n_layers`的`torch.FloatTensor`元组的元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态。仅在`config.is_decoder = True`时相关。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。

[MptForCausalLM](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptForCausalLM)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, MptForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("mosaicml/mpt-7b")
>>> model = MptForCausalLM.from_pretrained("mosaicml/mpt-7b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs, labels=inputs["input_ids"])
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## MptForSequenceClassification

### `class transformers.MptForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L656)

```py
( config: MptConfig )
```

参数

+   `config`（[MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)） — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

MPT模型变压器，顶部带有序列分类头（线性层）。

[MptForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptForSequenceClassification)使用最后一个标记来进行分类，就像其他因果模型（例如GPT-1）一样。

由于它对最后一个标记进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，它会找到每一行中不是填充标记的最后一个标记。如果未定义`pad_token_id`，它会简单地取每一批行中的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此它会执行相同操作（取每批行中的最后一个值）。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存，调整输入嵌入等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L681)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, input_ids_length)`) — `input_ids_length` = `sequence_length`（如果`past_key_values`为`None`）否则`past_key_values[0][0].shape[2]`（输入过去键值状态的`sequence_length`）。 词汇表中输入序列标记的索引。

    如果使用了`past_key_values`，则只能传递尚未计算其过去的`input_ids`作为`input_ids`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。 有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values` (`Tuple[Tuple[torch.Tensor]]`，长度为`config.n_layers`) — 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参见下面的`past_key_values`输出）。 可用于加速顺序解码。 将其过去传递给此模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免对填充标记索引执行注意力的掩码。 选择在`[0, 1]`范围内的掩码值：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。 如果要更好地控制如何将`input_ids`索引转换为相关向量，这很有用，而不是使用模型的内部嵌入查找矩阵。

    如果使用了`past_key_values`，则可以选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache` (`bool`，*可选*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。 有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。 有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。 索引应在`[0, ..., config.num_labels - 1]`范围内。 如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`或输入）包含根据配置（[MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)）和输入的不同元素。

+   `损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量）

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（用于嵌入的输出和每个层的输出）。

    每层模型输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    自注意力头中用于计算加权平均值的注意力softmax后的注意力权重。

[MptForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

单标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, MptForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("mosaicml/mpt-7b")
>>> model = MptForSequenceClassification.from_pretrained("mosaicml/mpt-7b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = MptForSequenceClassification.from_pretrained("mosaicml/mpt-7b", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
```

多标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, MptForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("mosaicml/mpt-7b")
>>> model = MptForSequenceClassification.from_pretrained("mosaicml/mpt-7b", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = MptForSequenceClassification.from_pretrained(
...     "mosaicml/mpt-7b", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

## MptForTokenClassification

### `class transformers.MptForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L780)

```py
( config: MptConfig )
```

参数

+   `config`（[MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部具有标记分类头的MPT模型（隐藏状态输出的线性层），例如用于命名实体识别（NER）任务。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L805)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **deprecated_arguments ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— 如果`past_key_values`为`None`，则`input_ids_length`=`sequence_length`，否则`input_ids_length`=`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past_key_values`，只应传递尚未计算其过去的`input_ids`作为`input_ids`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。将其过去传递给此模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）:

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：

    +   对于未被掩码的标记为1，

    +   对于被掩码的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

    如果使用了`past_key_values`，则可能只需输入最后的`inputs_embeds`（请参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 分类损失。

+   `logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）— 分类分数（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每一层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[MptForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, MptForTokenClassification
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("mosaicml/mpt-7b")
>>> model = MptForTokenClassification.from_pretrained("mosaicml/mpt-7b")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
... )

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_token_class_ids = logits.argmax(-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]

>>> labels = predicted_token_class_ids
>>> loss = model(**inputs, labels=labels).loss
```

## MptForQuestionAnswering

### `class transformers.MptForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L869)

```py
( config )
```

参数

+   `config` ([MptConfig](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法来加载模型权重。

MPT模型变压器，在顶部具有用于提取问答任务的跨度分类头，如SQuAD（在隐藏状态输出顶部的线性层，用于计算`跨度起始对数`和`跨度结束对数`）。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存，调整输入嵌入等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mpt/modeling_mpt.py#L885)

```py
( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, input_ids_length)`) — `input_ids_length` = `sequence_length`，如果`past_key_values`为`None`，否则为`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。输入序列标记在词汇表中的索引。

    如果使用`past_key_values`，则只有那些没有计算过去的`input_ids`应该作为`input_ids`传递。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参阅下面的`past_key_values`输出）。可用于加速顺序解码。已经给出其过去的`input_ids`不应作为`input_ids`传递给此模型，因为它们已经计算过了。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示`未被掩码`的标记，

    +   0表示`被掩码`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

    如果使用`past_key_values`，则可以选择仅输入最后的`inputs_embeds`（请参阅`past_key_values`）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则会返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `start_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记跨度开始位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内以计算损失。

+   `end_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记跨度结束位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内以计算损失。

[MptForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/mpt#transformers.MptForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
