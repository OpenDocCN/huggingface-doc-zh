# MobileNet V2

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilenet_v2)

## æ¦‚è¿°

MobileNet æ¨¡å‹æ˜¯ç”± Mark Sandlerã€Andrew Howardã€Menglong Zhuã€Andrey Zhmoginovã€Liang-Chieh Chen åœ¨[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)ä¸­æå‡ºçš„ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§æ–°çš„ç§»åŠ¨æ¶æ„ MobileNetV2ï¼Œå®ƒæé«˜äº†ç§»åŠ¨æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è·¨ä¸åŒæ¨¡å‹å¤§å°çš„å…‰è°±ã€‚æˆ‘ä»¬è¿˜æè¿°äº†å°†è¿™äº›ç§»åŠ¨æ¨¡å‹åº”ç”¨äºå¯¹è±¡æ£€æµ‹çš„é«˜æ•ˆæ–¹æ³•ï¼Œè¿™æ˜¯æˆ‘ä»¬ç§°ä¹‹ä¸º SSDLite çš„æ–°é¢–æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡æˆ‘ä»¬ç§°ä¹‹ä¸º Mobile DeepLabV3 çš„ DeepLabv3 çš„ç®€åŒ–å½¢å¼æ„å»ºç§»åŠ¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚*

*MobileNetV2 æ¶æ„åŸºäºå€’ç½®æ®‹å·®ç»“æ„ï¼Œå…¶ä¸­æ®‹å·®å—çš„è¾“å…¥å’Œè¾“å‡ºæ˜¯è–„ç“¶é¢ˆå±‚ï¼Œä¸ä¼ ç»Ÿçš„æ®‹å·®æ¨¡å‹ç›¸åï¼Œä¼ ç»Ÿæ¨¡å‹åœ¨è¾“å…¥ä¸­ä½¿ç”¨æ‰©å±•è¡¨ç¤ºï¼ŒMobileNetV2 ä½¿ç”¨è½»é‡çº§æ·±åº¦å·ç§¯æ¥è¿‡æ»¤ä¸­é—´æ‰©å±•å±‚ä¸­çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åœ¨çª„å±‚ä¸­å»é™¤éçº¿æ€§å¯¹äºä¿æŒè¡¨å¾èƒ½åŠ›æ˜¯é‡è¦çš„ã€‚æˆ‘ä»¬è¯æ˜è¿™å¯ä»¥æé«˜æ€§èƒ½ï¼Œå¹¶æä¾›å¯¼è‡´è¿™ç§è®¾è®¡çš„ç›´è§‰ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸å°†è¾“å…¥/è¾“å‡ºåŸŸä¸å˜æ¢çš„è¡¨ç°åŠ›åˆ†ç¦»ï¼Œä¸ºè¿›ä¸€æ­¥åˆ†ææä¾›äº†ä¾¿åˆ©çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨ Imagenet åˆ†ç±»ã€COCO ç›®æ ‡æ£€æµ‹ã€VOC å›¾åƒåˆ†å‰²ä¸Šè¡¡é‡æˆ‘ä»¬çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡†ç¡®æ€§å’Œæ“ä½œæ•°é‡ä¹‹é—´çš„æƒè¡¡ï¼Œæ“ä½œæ•°é‡ç”±ä¹˜åŠ æ“ä½œï¼ˆMAddï¼‰å’Œå‚æ•°æ•°é‡æ¥è¡¡é‡ã€‚*

è¯¥æ¨¡å‹ç”±[matthijs](https://huggingface.co/Matthijs)è´¡çŒ®ã€‚åŸå§‹ä»£ç å’Œæƒé‡å¯ä»¥åœ¨[ä¸»æ¨¡å‹è¿™é‡Œæ‰¾åˆ°](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)ï¼Œåœ¨[DeepLabV3+è¿™é‡Œæ‰¾åˆ°](https://github.com/tensorflow/models/tree/master/research/deeplab)ã€‚

## ä½¿ç”¨æç¤º

+   æ£€æŸ¥ç‚¹å‘½åä¸º**mobilenet_v2_*depth*_*size***ï¼Œä¾‹å¦‚**mobilenet_v2_1.0_224**ï¼Œå…¶ä¸­**1.0**æ˜¯æ·±åº¦ä¹˜æ•°ï¼ˆæœ‰æ—¶ä¹Ÿç§°ä¸ºâ€œalphaâ€æˆ–å®½åº¦ä¹˜æ•°ï¼‰ï¼Œ**224**æ˜¯æ¨¡å‹è®­ç»ƒçš„è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡ã€‚

+   å°½ç®¡æ£€æŸ¥ç‚¹æ˜¯åœ¨ç‰¹å®šå¤§å°çš„å›¾åƒä¸Šè®­ç»ƒçš„ï¼Œä½†æ¨¡å‹å¯ä»¥åœ¨ä»»ä½•å¤§å°çš„å›¾åƒä¸Šè¿è¡Œã€‚æ”¯æŒçš„æœ€å°å›¾åƒå¤§å°ä¸º 32x32ã€‚

+   å¯ä»¥ä½¿ç”¨ MobileNetV2ImageProcessor æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚

+   å¯ç”¨çš„å›¾åƒåˆ†ç±»æ£€æŸ¥ç‚¹æ˜¯åœ¨[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)ä¸Šé¢„è®­ç»ƒçš„ï¼ˆä¹Ÿç§°ä¸º ILSVRC 2012ï¼ŒåŒ…å« 130 ä¸‡å¼ å›¾åƒå’Œ 1000 ä¸ªç±»ï¼‰ã€‚ç„¶è€Œï¼Œæ¨¡å‹é¢„æµ‹ 1001 ä¸ªç±»åˆ«ï¼šæ¥è‡ª ImageNet çš„ 1000 ä¸ªç±»åˆ«åŠ ä¸Šé¢å¤–çš„â€œèƒŒæ™¯â€ç±»ï¼ˆç´¢å¼• 0ï¼‰ã€‚

+   åˆ†å‰²æ¨¡å‹ä½¿ç”¨[DeepLabV3+](https://arxiv.org/abs/1802.02611)å¤´éƒ¨ã€‚å¯ç”¨çš„è¯­ä¹‰åˆ†å‰²æ£€æŸ¥ç‚¹æ˜¯åœ¨[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)ä¸Šé¢„è®­ç»ƒçš„ã€‚

+   åŸå§‹çš„ TensorFlow æ£€æŸ¥ç‚¹ä½¿ç”¨ä¸åŒçš„å¡«å……è§„åˆ™ï¼Œéœ€è¦æ¨¡å‹åœ¨æ¨æ–­æ—¶ç¡®å®šå¡«å……é‡ï¼Œå› ä¸ºè¿™å–å†³äºè¾“å…¥å›¾åƒçš„å¤§å°ã€‚è¦ä½¿ç”¨åŸç”Ÿ PyTorch å¡«å……è¡Œä¸ºï¼Œè¯·åˆ›å»ºä¸€ä¸ª MobileNetV2Configï¼Œå…¶ä¸­`tf_padding = False`ã€‚

ä¸æ”¯æŒçš„åŠŸèƒ½ï¼š

+   MobileNetV2Model è¾“å‡ºæœ€åä¸€ä¸ªéšè—çŠ¶æ€çš„å…¨å±€æ± åŒ–ç‰ˆæœ¬ã€‚åœ¨åŸå§‹æ¨¡å‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å›ºå®š 7x7 çª—å£å’Œæ­¥å¹… 1 çš„å¹³å‡æ± åŒ–å±‚ä»£æ›¿å…¨å±€æ± åŒ–ã€‚å¯¹äºå¤§äºæ¨èå›¾åƒå°ºå¯¸çš„è¾“å…¥ï¼Œè¿™å°†äº§ç”Ÿä¸€ä¸ªå¤§äº 1x1 çš„æ± åŒ–è¾“å‡ºã€‚Hugging Face çš„å®ç°ä¸æ”¯æŒè¿™ä¸€ç‚¹ã€‚

+   åŸå§‹çš„ TensorFlow æ£€æŸ¥ç‚¹åŒ…æ‹¬é‡åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬ä¸æ”¯æŒè¿™äº›æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åŒ…æ‹¬é¢å¤–çš„â€œFakeQuantizationâ€æ“ä½œæ¥å–æ¶ˆé‡åŒ–æƒé‡ã€‚

+   é€šå¸¸ä¼šæå–æ‰©å±•å±‚çš„è¾“å‡ºï¼Œç´¢å¼•ä¸º 10 å’Œ 13ï¼Œä»¥åŠæœ€ç»ˆ 1x1 å·ç§¯å±‚çš„è¾“å‡ºï¼Œç”¨äºä¸‹æ¸¸ç›®çš„ã€‚ä½¿ç”¨`output_hidden_states=True`è¿”å›æ‰€æœ‰ä¸­é—´å±‚çš„è¾“å‡ºã€‚ç›®å‰æ— æ³•å°†å…¶é™åˆ¶ä¸ºç‰¹å®šå±‚ã€‚

+   DeepLabV3+åˆ†å‰²å¤´éƒ¨ä¸ä½¿ç”¨éª¨å¹²ç½‘ç»œçš„æœ€ç»ˆå·ç§¯å±‚ï¼Œä½†æ˜¯è¿™ä¸€å±‚ä»ç„¶ä¼šè¢«è®¡ç®—ã€‚ç›®å‰æ— æ³•å‘Šè¯‰ MobileNetV2Model åº”è¯¥è¿è¡Œåˆ°å“ªä¸€å±‚ã€‚

## èµ„æº

å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ MobileNetV2ã€‚

å›¾åƒåˆ†ç±»

+   MobileNetV2ForImageClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚

+   å¦è¯·å‚é˜…ï¼šå›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—

**è¯­ä¹‰åˆ†å‰²**

+   è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## MobileNetV2Config

### `class transformers.MobileNetV2Config`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/configuration_mobilenet_v2.py#L38)

```py
( num_channels = 3 image_size = 224 depth_multiplier = 1.0 depth_divisible_by = 8 min_depth = 8 expand_ratio = 6.0 output_stride = 32 first_layer_is_expansion = True finegrained_output = True hidden_act = 'relu6' tf_padding = True classifier_dropout_prob = 0.8 initializer_range = 0.02 layer_norm_eps = 0.001 semantic_loss_ignore_index = 255 **kwargs )
```

å‚æ•°

+   `num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“æ•°ã€‚

+   `image_size` (`int`, *optional*, defaults to 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `depth_multiplier` (`float`, *optional*, defaults to 1.0) â€” æ”¶ç¼©æˆ–æ‰©å±•æ¯ä¸€å±‚çš„é€šé“æ•°ã€‚é»˜è®¤ä¸º 1.0ï¼Œç½‘ç»œä» 32 ä¸ªé€šé“å¼€å§‹ã€‚æœ‰æ—¶ä¹Ÿç§°ä¸ºâ€œalphaâ€æˆ–â€œå®½åº¦ä¹˜æ•°â€ã€‚

+   `depth_divisible_by` (`int`, *optional*, defaults to 8) â€” æ¯ä¸€å±‚çš„é€šé“æ•°å°†å§‹ç»ˆæ˜¯è¿™ä¸ªæ•°å­—çš„å€æ•°ã€‚

+   `min_depth` (`int`, *optional*, defaults to 8) â€” æ‰€æœ‰å±‚è‡³å°‘å…·æœ‰è¿™ä¹ˆå¤šé€šé“ã€‚

+   `expand_ratio` (`float`, *optional*, defaults to 6.0) â€” æ¯ä¸ªå—ä¸­ç¬¬ä¸€å±‚çš„è¾“å‡ºé€šé“æ•°æ˜¯è¾“å…¥é€šé“æ•°ä¹˜ä»¥æ‰©å±•æ¯”ç‡ã€‚

+   `output_stride` (`int`, *optional*, defaults to 32) â€” è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾çš„ç©ºé—´åˆ†è¾¨ç‡ä¹‹é—´çš„æ¯”ç‡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†è¾“å…¥ç»´åº¦å‡å° 32 å€ã€‚å¦‚æœ`output_stride`ä¸º 8 æˆ– 16ï¼Œåˆ™æ¨¡å‹åœ¨æ·±åº¦å±‚ä¸Šä½¿ç”¨æ‰©å¼ å·ç§¯è€Œä¸æ˜¯å¸¸è§„å·ç§¯ï¼Œä»¥ç¡®ä¿ç‰¹å¾å›¾æ°¸è¿œä¸ä¼šæ¯”è¾“å…¥å›¾åƒå° 8 å€æˆ– 16 å€ã€‚

+   `first_layer_is_expansion` (`bool`, *optional*, defaults to `True`) â€” å¦‚æœç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªæ‰©å±•å—çš„æ‰©å±•å±‚ï¼Œåˆ™ä¸º Trueã€‚

+   `finegrained_output` (`bool`, *optional*, defaults to `True`) â€” å¦‚æœä¸ºçœŸï¼Œåˆ™æœ€ç»ˆå·ç§¯å±‚ä¸­çš„è¾“å‡ºé€šé“æ•°å°†ä¿æŒè¾ƒå¤§ï¼ˆ1280ï¼‰ï¼Œå³ä½¿`depth_multiplier`å°äº 1ã€‚

+   `hidden_act` (`str` æˆ– `function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"relu6"`) â€” Transformer ç¼–ç å™¨å’Œå·ç§¯å±‚ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚

+   `tf_padding` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨å·ç§¯å±‚ä¸Šä½¿ç”¨ TensorFlow å¡«å……è§„åˆ™ã€‚

+   `classifier_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.8) â€” é™„åŠ åˆ†ç±»å™¨çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.001) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `semantic_loss_ignore_index` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ MobileNetV2Model çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª MobileNetV2 æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº MobileNetV2 [google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import MobileNetV2Config, MobileNetV2Model

>>> # Initializing a "mobilenet_v2_1.0_224" style configuration
>>> configuration = MobileNetV2Config()

>>> # Initializing a model from the "mobilenet_v2_1.0_224" style configuration
>>> model = MobileNetV2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## MobileNetV2FeatureExtractor

### `class transformers.MobileNetV2FeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/feature_extraction_mobilenet_v2.py#L26)

```py
( *args **kwargs )
```

#### `preprocess`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L170)

```py
( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–æ‰¹å¤„ç†å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_rescale=False`ã€‚

+   `do_resize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å›¾åƒçš„æœ€çŸ­è¾¹è¢«è°ƒæ•´ä¸º size[â€œshortest_edgeâ€]ï¼Œæœ€é•¿è¾¹è¢«è°ƒæ•´ä»¥ä¿æŒè¾“å…¥çš„çºµæ¨ªæ¯”ã€‚

+   `resample` (`PILImageResampling` è¿‡æ»¤å™¨ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™ä½¿ç”¨çš„ `PILImageResampling` è¿‡æ»¤å™¨ï¼Œä¾‹å¦‚ `PILImageResampling.BILINEAR`ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_center_crop` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_center_crop`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚

+   `crop_size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.crop_size`) â€” ä¸­å¿ƒè£å‰ªçš„å°ºå¯¸ã€‚ä»…åœ¨ `do_center_crop` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_rescale` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_rescale`) â€” æ˜¯å¦é‡æ–°ç¼©æ”¾å›¾åƒå€¼åœ¨ [0 - 1] ä¹‹é—´ã€‚

+   `rescale_factor` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.rescale_factor`) â€” å¦‚æœ `do_rescale` è®¾ç½®ä¸º `True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚

+   `do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.image_mean`) â€” å¦‚æœ `do_normalize` è®¾ç½®ä¸º `True`ï¼Œåˆ™ä½¿ç”¨çš„å›¾åƒå‡å€¼ã€‚

+   `image_std` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.image_std`) â€” å¦‚æœ `do_normalize` è®¾ç½®ä¸º `True`ï¼Œåˆ™ä½¿ç”¨çš„å›¾åƒæ ‡å‡†å·®ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`ï¼šè¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹å¤„ç†ã€‚

+   `data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*, é»˜è®¤ä¸º `ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (é€šé“æ•°, é«˜åº¦, å®½åº¦)ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚

    +   æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (é€šé“æ•°, é«˜åº¦, å®½åº¦)ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚

    +   `"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)ã€‚

é¢„å¤„ç†å›¾åƒæˆ–å›¾åƒæ‰¹å¤„ç†ã€‚

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L313)

```py
( outputs target_sizes: List = None ) â†’ export const metadata = 'undefined';semantic_segmentation
```

å‚æ•°

+   `outputs` (MobileNetV2ForSemanticSegmentation) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes` (`List[Tuple]`ï¼Œé•¿åº¦ä¸º `batch_size`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå°ºå¯¸ (é«˜åº¦, å®½åº¦) å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

è¯­ä¹‰åˆ†å‰²

`List[torch.Tensor]`ï¼Œé•¿åº¦ä¸º `batch_size`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸º (é«˜åº¦, å®½åº¦) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº `target_sizes` æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”ä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚

å°† MobileNetV2ForSemanticSegmentation çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

## MobileNetV2ImageProcessor

### `class transformers.MobileNetV2ImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L49)

```py
( do_resize: bool = True size: Optional = None resample: Resampling = <Resampling.BILINEAR: 2> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ (é«˜åº¦, å®½åº¦) å°ºå¯¸è°ƒæ•´ä¸ºæŒ‡å®šçš„ `size`ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_resize` è¦†ç›–ã€‚

+   `size` (`Dict[str, int]` *å¯é€‰*, é»˜è®¤ä¸º `{"shortest_edge" -- 256}`): è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å›¾åƒçš„æœ€çŸ­è¾¹è¢«è°ƒæ•´ä¸º size[â€œshortest_edgeâ€]ï¼Œæœ€é•¿è¾¹è¢«è°ƒæ•´ä»¥ä¿æŒè¾“å…¥çš„é•¿å®½æ¯”ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `size` è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`ï¼Œ*å¯é€‰*, é»˜è®¤ä¸º `PILImageResampling.BILINEAR`) â€” è°ƒæ•´å›¾åƒå¤§å°æ—¶è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¦†ç›–ã€‚

+   `do_center_crop` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¦‚æœè¾“å…¥å°ºå¯¸å°äºä»»ä½•è¾¹ä¸Šçš„ `crop_size`ï¼Œåˆ™å›¾åƒå°†å¡«å……ä¸º 0ï¼Œç„¶åè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_center_crop` å‚æ•°è¦†ç›–ã€‚

+   `crop_size` (`Dict[str, int]`ï¼Œ*å¯é€‰*, é»˜è®¤ä¸º `{"height" -- 224, "width": 224}`): åº”ç”¨ä¸­å¿ƒè£å‰ªæ—¶çš„æœŸæœ›è¾“å‡ºå°ºå¯¸ã€‚ä»…åœ¨ `do_center_crop` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `crop_size` å‚æ•°è¦†ç›–ã€‚

+   `do_rescale` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹ `rescale_factor` é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¦†ç›–ã€‚

+   `rescale_factor` (`int` æˆ– `float`, *å¯é€‰*, é»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¦†ç›–ã€‚æ˜¯å¦å½’ä¸€åŒ–å›¾åƒã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_normalize` å‚æ•°è¦†ç›–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º `IMAGENET_STANDARD_MEAN`) â€” å¦‚æœå½’ä¸€åŒ–å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒä¸­é€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_mean` å‚æ•°è¦†ç›–ã€‚

+   `image_std` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º `IMAGENET_STANDARD_STD`) â€” å¦‚æœå½’ä¸€åŒ–å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒä¸­é€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ª MobileNetV2 å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L170)

```py
( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_rescale=False`ã€‚

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º `self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå¤§å°ã€‚å›¾åƒçš„æœ€çŸ­è¾¹è¢«è°ƒæ•´ä¸º size[â€œshortest_edgeâ€]ï¼Œæœ€é•¿è¾¹è¢«è°ƒæ•´ä»¥ä¿æŒè¾“å…¥çš„é•¿å®½æ¯”ã€‚

+   `resample` (`PILImageResampling` è¿‡æ»¤å™¨, *å¯é€‰*, é»˜è®¤ä¸º `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œè¦ä½¿ç”¨çš„ `PILImageResampling` è¿‡æ»¤å™¨ï¼Œä¾‹å¦‚ `PILImageResampling.BILINEAR`ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_center_crop` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.do_center_crop`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚

+   `crop_size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º `self.crop_size`) â€” ä¸­å¿ƒè£å‰ªçš„å¤§å°ã€‚ä»…åœ¨ `do_center_crop` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_rescale` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾åˆ° [0 - 1] ä¹‹é—´ã€‚

+   `rescale_factor` (`float`, *å¯é€‰*, é»˜è®¤ä¸º `self.rescale_factor`) â€” å¦‚æœ `do_rescale` è®¾ç½®ä¸º `True`ï¼Œåˆ™æŒ‰ç…§æ­¤å› å­é‡æ–°ç¼©æ”¾å›¾åƒã€‚

+   `do_normalize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º `self.image_mean`) â€” å¦‚æœ `do_normalize` è®¾ç½®ä¸º `True`ï¼Œåˆ™ä½¿ç”¨çš„å›¾åƒå‡å€¼ã€‚

+   `image_std` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º `self.image_std`) â€” å¦‚æœ `do_normalize` è®¾ç½®ä¸º `True`ï¼Œåˆ™ä½¿ç”¨çš„å›¾åƒæ ‡å‡†å·®ã€‚

+   `return_tensors` (`str` æˆ– `TensorType`, *å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€:

    +   æœªè®¾ç½®: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`: è¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹å¤„ç†ã€‚

+   `data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€:

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (num_channels, height, width)ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (height, width, num_channels)ã€‚

    +   æœªè®¾ç½®: ä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€:

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (num_channels, height, width)ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   `"none"`æˆ–`ChannelDimension.NONE`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

é¢„å¤„ç†ä¸€å¼ å›¾ç‰‡æˆ–ä¸€æ‰¹å›¾ç‰‡ã€‚

#### `post_process_semantic_segmentation`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py#L313)

```py
( outputs target_sizes: List = None ) â†’ export const metadata = 'undefined';semantic_segmentation
```

å‚æ•°

+   `outputs` (MobileNetV2ForSemanticSegmentation) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” ä¸æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

è¯­ä¹‰åˆ†å‰²

`List[torch.Tensor]`ï¼Œé•¿åº¦ä¸º`batch_size`ï¼Œæ¯ä¸ªé¡¹ç›®æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äºç›®æ ‡å¤§å°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†`target_sizes`ï¼‰ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« IDã€‚

å°† MobileNetV2ForSemanticSegmentation çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

## MobileNetV2Model

### `class transformers.MobileNetV2Model`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L498)

```py
( config: MobileNetV2Config add_pooling_layer: bool = True )
```

å‚æ•°

+   `config` (MobileNetV2Config) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ MobileNetV2 æ¨¡å‹è¾“å‡ºåŸå§‹çš„éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L566)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… MobileNetV2ImageProcessor.`call`()ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆMobileNetV2Configï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) â€” ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥å±‚çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

MobileNetV2Model çš„ forward æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, MobileNetV2Model
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/mobilenet_v2_1.0_224")
>>> model = MobileNetV2Model.from_pretrained("google/mobilenet_v2_1.0_224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 1280, 7, 7]
```

## MobileNetV2ForImageClassification

### `class transformers.MobileNetV2ForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L615)

```py
( config: MobileNetV2Config )
```

å‚æ•°

+   `config`ï¼ˆMobileNetV2Configï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

MobileNetV2 æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆæ± åŒ–ç‰¹å¾ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch çš„ä¸€ä¸ª[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L638)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… MobileNetV2ImageProcessor.`call`()ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.ImageClassifierOutputWithNoAttention æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.ImageClassifierOutputWithNoAttention æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆMobileNetV2Configï¼‰å’Œè¾“å…¥ã€‚

+   `æŸå¤±` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

MobileNetV2ForImageClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, MobileNetV2ForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/mobilenet_v2_1.0_224")
>>> model = MobileNetV2ForImageClassification.from_pretrained("google/mobilenet_v2_1.0_224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## MobileNetV2ForSemanticSegmentation

### `class transformers.MobileNetV2ForSemanticSegmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L775)

```py
( config: MobileNetV2Config )
```

å‚æ•°

+   `config` (MobileNetV2Config) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

MobileNetV2 æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­ä¹‰åˆ†å‰²å¤´ï¼Œä¾‹å¦‚ç”¨äº Pascal VOCã€‚

æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py#L792)

```py
( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… MobileNetV2ImageProcessor.`call`()ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.SemanticSegmenterOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.SemanticSegmenterOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆMobileNetV2Configï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height, logits_width)`çš„`torch.FloatTensor`ï¼‰ â€” æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚

    <tip warning="{true}">è¿”å›çš„ logits ä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°† logits è°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”å§‹ç»ˆæ£€æŸ¥æ‚¨çš„ logits å½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, patch_size, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

MobileNetV2ForSemanticSegmentation çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, MobileNetV2ForSemanticSegmentation
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("google/deeplabv3_mobilenet_v2_1.0_513")
>>> model = MobileNetV2ForSemanticSegmentation.from_pretrained("google/deeplabv3_mobilenet_v2_1.0_513")

>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # logits are of shape (batch_size, num_labels, height, width)
>>> logits = outputs.logits
```
