- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/lora](https://huggingface.co/docs/peft/developer_guides/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LoRA is low-rank decomposition method to reduce the number of trainable parameters
    which speeds up finetuning large models and uses less memory. In PEFT, using LoRA
    is as easy as setting up a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    and wrapping it with [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    to create a trainable [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: This guide explores in more detail other options and features for using LoRA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initialization of LoRA weights is controlled by the parameter `init_lora_weights`
    in [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig).
    By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and
    zeros for weight B resulting in an identity transform (same as the reference [implementation](https://github.com/microsoft/LoRA)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to pass `init_lora_weights="gaussian"`. As the name suggests,
    this initializes weight A with a Gaussian distribution and zeros for weight B
    (this is how [Diffusers](https://huggingface.co/docs/diffusers/index) initializes
    LoRA weights).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There is also an option to set `init_lora_weights=False` which is useful for
    debugging and testing. This should be the only time you use this option. When
    choosing this option, the LoRA weights are initialized such that they do *not*
    result in an identity transform.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: LoftQ
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When quantizing the base model for QLoRA training, consider using the [LoftQ
    initialization](https://arxiv.org/abs/2310.08659), which has been shown to improve
    performance when training quantized models. The idea is that the LoRA weights
    are initialized such that the quantization error is minimized. If you’re using
    LoftQ, *do not* quantize the base model. You should set up a `LoftQConfig` instead:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Learn more about how PEFT works with quantization in the [Quantization](quantization)
    guide.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Rank-stabilized LoRA
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to initialize [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    is with the [rank-stabilized LoRA (rsLoRA)](https://huggingface.co/papers/2312.03732)
    method. The LoRA architecture scales each adapter during every forward pass by
    a fixed scalar which is set at initialization and depends on the rank `r`. The
    scalar is given by `lora_alpha/r` in the original implementation, but rsLoRA uses
    `lora_alpha/math.sqrt(r)` which stabilizes the adapters and increases the performance
    potential from using a higher `r`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: QLoRA-style training
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default LoRA settings in PEFT add trainable weights to the query and value
    layers of each attention block. But [QLoRA](https://hf.co/papers/2305.14314),
    which adds trainable weights to all the linear layers of a transformer model,
    can provide performance equal to a fully finetuned model. To apply LoRA to all
    the linear layers, like in QLoRA, set `target_modules="all-linear"` (easier than
    specifying individual modules by name which can vary depending on the architecture).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Merge adapters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While LoRA is significantly smaller and faster to train, you may encounter latency
    issues during inference due to separately loading the base model and the LoRA
    adapter. To eliminate latency, use the [merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)
    function to merge the adapter weights with the base model. This allows you to
    use the newly merged model as a standalone model. The [merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)
    function doesn’t keep the adapter weights in memory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you need to keep a copy of the weights so you can unmerge the adapter later
    or delete and load different ones, you should use the [merge_adapter()](/docs/peft/v0.8.2/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_adapter)
    function instead. Now you have the option to use [unmerge_adapter()](/docs/peft/v0.8.2/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unmerge_adapter)
    to return the base model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要保留权重的副本，以便稍后取消合并适配器或删除并加载不同的适配器，应该使用merge_adapter()函数。现在您可以选择使用unmerge_adapter()来返回基础模型。
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [add_weighted_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.add_weighted_adapter)
    function is useful for merging multiple LoRAs into a new adapter based on a user
    provided weighting scheme in the `weights` parameter. Below is an end-to-end example.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: add_weighted_adapter()函数对于根据用户提供的权重方案将多个LoRA合并成一个新的适配器非常有用，权重方案在`weights`参数中指定。下面是一个端到端的示例。
- en: 'First load the base model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载基础模型：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we load the first adapter:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载第一个适配器：
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then load a different adapter and merge it with the first one:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载一个不同的适配器并将其与第一个合并：
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are several supported methods for `combination_type`. Refer to the [documentation](../package_reference/lora#peft.LoraModel.add_weighted_adapter)
    for more details. Note that “svd” as the `combination_type` is not supported when
    using `torch.float16` or `torch.bfloat16` as the datatype.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种支持的`combination_type`方法。有关更多详细信息，请参阅文档。请注意，在使用`torch.float16`或`torch.bfloat16`作为数据类型时，“svd”作为`combination_type`是不受支持的。
- en: 'Now, perform inference:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，执行推理：
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Load adapters
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载适配器
- en: Adapters can be loaded onto a pretrained model with [load_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.load_adapter),
    which is useful for trying out different adapters whose weights aren’t merged.
    Set the active adapter weights with the [set_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.set_adapter)
    function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器可以使用load_adapter()加载到预训练模型中，这对于尝试不合并权重的不同适配器非常有用。使用set_adapter()函数设置活动适配器的权重。
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To return the base model, you could use [unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.unload)
    to unload all of the LoRA modules or [delete_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.delete_adapter)
    to delete the adapter entirely.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要返回基础模型，您可以使用unload()来卸载所有的LoRA模块，或者使用delete_adapter()来完全删除适配器。
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
