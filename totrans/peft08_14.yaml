- en: LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/lora](https://huggingface.co/docs/peft/developer_guides/lora)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: LoRA is low-rank decomposition method to reduce the number of trainable parameters
    which speeds up finetuning large models and uses less memory. In PEFT, using LoRA
    is as easy as setting up a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    and wrapping it with [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    to create a trainable [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
  prefs: []
  type: TYPE_NORMAL
- en: This guide explores in more detail other options and features for using LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initialization of LoRA weights is controlled by the parameter `init_lora_weights`
    in [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig).
    By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and
    zeros for weight B resulting in an identity transform (same as the reference [implementation](https://github.com/microsoft/LoRA)).
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to pass `init_lora_weights="gaussian"`. As the name suggests,
    this initializes weight A with a Gaussian distribution and zeros for weight B
    (this is how [Diffusers](https://huggingface.co/docs/diffusers/index) initializes
    LoRA weights).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There is also an option to set `init_lora_weights=False` which is useful for
    debugging and testing. This should be the only time you use this option. When
    choosing this option, the LoRA weights are initialized such that they do *not*
    result in an identity transform.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: LoftQ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When quantizing the base model for QLoRA training, consider using the [LoftQ
    initialization](https://arxiv.org/abs/2310.08659), which has been shown to improve
    performance when training quantized models. The idea is that the LoRA weights
    are initialized such that the quantization error is minimized. If you’re using
    LoftQ, *do not* quantize the base model. You should set up a `LoftQConfig` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about how PEFT works with quantization in the [Quantization](quantization)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: Rank-stabilized LoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to initialize [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    is with the [rank-stabilized LoRA (rsLoRA)](https://huggingface.co/papers/2312.03732)
    method. The LoRA architecture scales each adapter during every forward pass by
    a fixed scalar which is set at initialization and depends on the rank `r`. The
    scalar is given by `lora_alpha/r` in the original implementation, but rsLoRA uses
    `lora_alpha/math.sqrt(r)` which stabilizes the adapters and increases the performance
    potential from using a higher `r`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: QLoRA-style training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default LoRA settings in PEFT add trainable weights to the query and value
    layers of each attention block. But [QLoRA](https://hf.co/papers/2305.14314),
    which adds trainable weights to all the linear layers of a transformer model,
    can provide performance equal to a fully finetuned model. To apply LoRA to all
    the linear layers, like in QLoRA, set `target_modules="all-linear"` (easier than
    specifying individual modules by name which can vary depending on the architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Merge adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While LoRA is significantly smaller and faster to train, you may encounter latency
    issues during inference due to separately loading the base model and the LoRA
    adapter. To eliminate latency, use the [merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)
    function to merge the adapter weights with the base model. This allows you to
    use the newly merged model as a standalone model. The [merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)
    function doesn’t keep the adapter weights in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you need to keep a copy of the weights so you can unmerge the adapter later
    or delete and load different ones, you should use the [merge_adapter()](/docs/peft/v0.8.2/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_adapter)
    function instead. Now you have the option to use [unmerge_adapter()](/docs/peft/v0.8.2/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unmerge_adapter)
    to return the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The [add_weighted_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.add_weighted_adapter)
    function is useful for merging multiple LoRAs into a new adapter based on a user
    provided weighting scheme in the `weights` parameter. Below is an end-to-end example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First load the base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the first adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then load a different adapter and merge it with the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are several supported methods for `combination_type`. Refer to the [documentation](../package_reference/lora#peft.LoraModel.add_weighted_adapter)
    for more details. Note that “svd” as the `combination_type` is not supported when
    using `torch.float16` or `torch.bfloat16` as the datatype.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, perform inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Load adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adapters can be loaded onto a pretrained model with [load_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.load_adapter),
    which is useful for trying out different adapters whose weights aren’t merged.
    Set the active adapter weights with the [set_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.set_adapter)
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To return the base model, you could use [unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.unload)
    to unload all of the LoRA modules or [delete_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.delete_adapter)
    to delete the adapter entirely.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
