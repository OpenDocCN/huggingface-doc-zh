# ç®¡é“å›è°ƒ

> [åŸæ–‡é“¾æ¥](https://huggingface.co/docs/diffusers/using-diffusers/callback)

ä½¿ç”¨`callback_on_step_end`å‚æ•°å¯ä»¥é€šè¿‡è‡ªå®šä¹‰å®šä¹‰çš„å‡½æ•°ä¿®æ”¹ç®¡é“çš„å»å™ªå¾ªç¯ã€‚è¿™å¯¹äº*åŠ¨æ€*è°ƒæ•´æŸäº›ç®¡é“å±æ€§æˆ–ä¿®æ”¹å¼ é‡å˜é‡éå¸¸æœ‰ç”¨ã€‚å›è°ƒçš„çµæ´»æ€§å¼€å¯äº†ä¸€äº›æœ‰è¶£çš„ç”¨ä¾‹ï¼Œä¾‹å¦‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥æ›´æ”¹æç¤ºåµŒå…¥ã€ä¸ºæç¤ºåµŒå…¥åˆ†é…ä¸åŒçš„æƒé‡ä»¥åŠç¼–è¾‘æŒ‡å¯¼æ¯”ä¾‹ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨`callback_on_step_end`å‚æ•°åœ¨æ¨ç†æ­¥éª¤çš„40%åç¦ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ï¼Œä»¥èŠ‚çœè®¡ç®—èµ„æºè€Œå¯¹æ€§èƒ½çš„å½±å“æœ€å°ã€‚

å›è°ƒå‡½æ•°åº”å…·æœ‰ä»¥ä¸‹å‚æ•°ï¼š

+   `pipe`ï¼ˆæˆ–ç®¡é“å®ä¾‹ï¼‰æä¾›å¯¹è¯¸å¦‚`num_timestep`å’Œ`guidance_scale`ç­‰æœ‰ç”¨å±æ€§çš„è®¿é—®ã€‚æ‚¨å¯ä»¥é€šè¿‡æ›´æ–°åº•å±‚å±æ€§æ¥ä¿®æ”¹è¿™äº›å±æ€§ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæ‚¨å°†é€šè¿‡è®¾ç½®`pipe._guidance_scale=0.0`æ¥ç¦ç”¨CFGã€‚

+   `step_index`å’Œ`timestep`å‘Šè¯‰æ‚¨åœ¨å»å™ªå¾ªç¯ä¸­çš„ä½ç½®ã€‚ä½¿ç”¨`step_index`åœ¨è¾¾åˆ°`num_timestep`çš„40%åå…³é—­CFGã€‚

+   `callback_kwargs`æ˜¯ä¸€ä¸ªåŒ…å«æ‚¨å¯ä»¥åœ¨å»å™ªå¾ªç¯ä¸­ä¿®æ”¹çš„å¼ é‡å˜é‡çš„å­—å…¸ã€‚å®ƒä»…åŒ…æ‹¬åœ¨ä¼ é€’ç»™ç®¡é“çš„`__call__`æ–¹æ³•çš„`callback_on_step_end_tensor_inputs`å‚æ•°ä¸­æŒ‡å®šçš„å˜é‡ã€‚ä¸åŒçš„ç®¡é“å¯èƒ½ä½¿ç”¨ä¸åŒçš„å˜é‡é›†ï¼Œå› æ­¤è¯·æ£€æŸ¥ç®¡é“çš„`_callback_tensor_inputs`å±æ€§ï¼Œä»¥è·å–æ‚¨å¯ä»¥ä¿®æ”¹çš„å˜é‡åˆ—è¡¨ã€‚ä¸€äº›å¸¸è§çš„å˜é‡åŒ…æ‹¬`latents`å’Œ`prompt_embeds`ã€‚å¯¹äºæ­¤å‡½æ•°ï¼Œåœ¨è®¾ç½®`guidance_scale=0.0`åï¼Œæ›´æ”¹`prompt_embeds`çš„æ‰¹å¤„ç†å¤§å°ï¼Œä»¥ä½¿å…¶æ­£å¸¸å·¥ä½œã€‚

æ‚¨çš„å›è°ƒå‡½æ•°åº”è¯¥ç±»ä¼¼äºè¿™æ ·ï¼š

```py
def callback_dynamic_cfg(pipe, step_index, timestep, callback_kwargs):
        # adjust the batch_size of prompt_embeds according to guidance_scale
        if step_index == int(pipe.num_timestep * 0.4):
                prompt_embeds = callback_kwargs["prompt_embeds"]
                prompt_embeds = prompt_embeds.chunk(2)[-1]

        # update guidance_scale and prompt_embeds
        pipe._guidance_scale = 0.0
        callback_kwargs["prompt_embeds"] = prompt_embeds
        return callback_kwargs
```

ç°åœ¨ï¼Œæ‚¨å¯ä»¥å°†å›è°ƒå‡½æ•°ä¼ é€’ç»™`callback_on_step_end`å‚æ•°ï¼Œå¹¶å°†`prompt_embeds`ä¼ é€’ç»™`callback_on_step_end_tensor_inputs`ã€‚

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"

generator = torch.Generator(device="cuda").manual_seed(1)
out = pipe(prompt, generator=generator, callback_on_step_end=callback_custom_cfg, callback_on_step_end_tensor_inputs=['prompt_embeds'])

out.images[0].save("out_custom_cfg.png")
```

å›è°ƒå‡½æ•°åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶æ‰§è¡Œï¼Œå¹¶ä¿®æ”¹ä¸‹ä¸€ä¸ªå»å™ªæ­¥éª¤çš„ç®¡é“å±æ€§å’Œå¼ é‡å˜é‡ã€‚

ä½¿ç”¨å›è°ƒï¼Œæ‚¨å¯ä»¥å®ç°åŠ¨æ€CFGç­‰åŠŸèƒ½ï¼Œè€Œæ— éœ€ä¿®æ”¹åº•å±‚ä»£ç ï¼

ğŸ¤— Diffusersç›®å‰ä»…æ”¯æŒ`callback_on_step_end`ï¼Œä½†å¦‚æœæ‚¨æœ‰ä¸€ä¸ªå¾ˆé…·çš„ç”¨ä¾‹å¹¶éœ€è¦åœ¨ä¸åŒçš„æ‰§è¡Œç‚¹ä¸Šå…·æœ‰å›è°ƒå‡½æ•°ï¼Œè¯·éšæ—¶æå‡º[åŠŸèƒ½è¯·æ±‚](https://github.com/huggingface/diffusers/issues/new/choose)ï¼

## ä¸­æ–­æ‰©æ•£è¿‡ç¨‹

åœ¨æ„å»ºä¸æ‰©æ•£å™¨ä¸€èµ·å·¥ä½œçš„UIæ—¶ï¼Œä¸­æ–­æ‰©æ•£è¿‡ç¨‹ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ç”¨æˆ·åœ¨å¯¹ä¸­é—´ç»“æœä¸æ»¡æ„æ—¶åœæ­¢ç”Ÿæˆè¿‡ç¨‹ã€‚æ‚¨å¯ä»¥é€šè¿‡å›è°ƒå°†æ­¤åŠŸèƒ½æ•´åˆåˆ°æ‚¨çš„ç®¡é“ä¸­ã€‚

ä¸­æ–­å›è°ƒæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®å¤å›¾åƒçš„[StableDiffusionPipeline](../api/pipelines/stable_diffusion/overview)å’Œ[StableDiffusionXLPipeline](../api/pipelines/stable_diffusion/stable_diffusion_xl)ã€‚

è¿™ä¸ªå›è°ƒå‡½æ•°åº”è¯¥æ¥å—ä»¥ä¸‹å‚æ•°ï¼š`pipe`ã€`i`ã€`t`å’Œ`callback_kwargs`ï¼ˆå¿…é¡»è¿”å›ï¼‰ã€‚å°†ç®¡é“çš„`_interrupt`å±æ€§è®¾ç½®ä¸º`True`ï¼Œä»¥åœ¨ä¸€å®šæ­¥æ•°ååœæ­¢æ‰©æ•£è¿‡ç¨‹ã€‚æ‚¨ä¹Ÿå¯ä»¥è‡ªç”±åœ°åœ¨å›è°ƒå‡½æ•°å†…éƒ¨å®ç°è‡ªå®šä¹‰çš„åœæ­¢é€»è¾‘ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œå°½ç®¡`num_inference_steps`è®¾ç½®ä¸º50ï¼Œä½†æ‰©æ•£è¿‡ç¨‹åœ¨10æ­¥ååœæ­¢ã€‚

```py
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
pipe.enable_model_cpu_offload()
num_inference_steps = 50

def interrupt_callback(pipe, i, t, callback_kwargs):
    stop_idx = 10
    if i == stop_idx:
        pipe._interrupt = True

    return callback_kwargs

pipe(
    "A photo of a cat",
    num_inference_steps=num_inference_steps,
    callback_on_step_end=interrupt_callback,
)
```
