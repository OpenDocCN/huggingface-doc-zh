- en: Data2Vec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Data2Vec
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by
    Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.
    Data2Vec proposes a unified framework for self-supervised learning across different
    data modalities - text, audio and images. Importantly, predicted targets for pre-training
    are contextualized latent representations of the inputs, rather than modality-specific,
    context-independent targets.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2Vec模型是由Alexei Baevski、Wei-Ning Hsu、Qiantong Xu、Arun Babu、Jiatao Gu和Michael
    Auli在[数据2vec: 语音、视觉和语言中的自监督学习的通用框架](https://arxiv.org/pdf/2202.03555)中提出的。Data2Vec提出了一个统一的框架，用于跨不同数据模态的自监督学习
    - 文本、音频和图像。重要的是，预训练的预测目标是输入的上下文化潜在表示，而不是特定于模态的、上下文无关的目标。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*While the general idea of self-supervised learning is identical across modalities,
    the actual algorithms and objectives differ widely because they were developed
    with a single modality in mind. To get us closer to general self-supervised learning,
    we present data2vec, a framework that uses the same learning method for either
    speech, NLP or computer vision. The core idea is to predict latent representations
    of the full input data based on a masked view of the input in a selfdistillation
    setup using a standard Transformer architecture. Instead of predicting modality-specific
    targets such as words, visual tokens or units of human speech which are local
    in nature, data2vec predicts contextualized latent representations that contain
    information from the entire input. Experiments on the major benchmarks of speech
    recognition, image classification, and natural language understanding demonstrate
    a new state of the art or competitive performance to predominant approaches. Models
    and code are available at [www.github.com/pytorch/fairseq/tree/master/examples/data2vec](http://www.github.com/pytorch/fairseq/tree/master/examples/data2vec).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管自监督学习的一般思想在各种模态之间是相同的，但实际的算法和目标差异很大，因为它们是针对单一模态开发的。为了让我们更接近于一般的自监督学习，我们提出了data2vec，这是一个框架，它使用相同的学习方法来处理语音、NLP或计算机视觉。核心思想是基于输入数据的遮蔽视图来预测完整输入数据的潜在表示，使用标准的Transformer架构进行自蒸馏设置。data2vec不是预测特定于模态的目标，比如单词、视觉标记或人类语音单元，而是预测包含来自整个输入的信息的上下文化潜在表示。对语音识别、图像分类和自然语言理解的主要基准进行的实验表明，与主流方法相比，取得了新的最先进或具有竞争力的性能。模型和代码可在[www.github.com/pytorch/fairseq/tree/master/examples/data2vec](http://www.github.com/pytorch/fairseq/tree/master/examples/data2vec)上找到。*'
- en: This model was contributed by [edugp](https://huggingface.co/edugp) and [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    [sayakpaul](https://github.com/sayakpaul) and [Rocketknight1](https://github.com/Rocketknight1)
    contributed Data2Vec for vision in TensorFlow.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[edugp](https://huggingface.co/edugp)和[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献的。[sayakpaul](https://github.com/sayakpaul)和[Rocketknight1](https://github.com/Rocketknight1)为TensorFlow中的视觉贡献了Data2Vec。
- en: The original code (for NLP and Speech) can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/data2vec).
    The original code for vision can be found [here](https://github.com/facebookresearch/data2vec_vision/tree/main/beit).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码（用于NLP和语音）可以在[这里](https://github.com/pytorch/fairseq/tree/main/examples/data2vec)找到。视觉的原始代码可以在[这里](https://github.com/facebookresearch/data2vec_vision/tree/main/beit)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using
    the same self-supervised learning method.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Data2VecAudio、Data2VecText和Data2VecVision都是使用相同的自监督学习方法进行训练的。
- en: For Data2VecAudio, preprocessing is identical to [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model),
    including feature extraction
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Data2VecAudio，预处理与[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)相同，包括特征提取。
- en: For Data2VecText, preprocessing is identical to [RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel),
    including tokenization.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Data2VecText，预处理与[RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)相同，包括标记化。
- en: For Data2VecVision, preprocessing is identical to [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel),
    including feature extraction.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Data2VecVision，预处理与[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)相同，包括特征提取。
- en: Resources
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with Data2Vec.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 官方Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用Data2Vec。
- en: Image Classification
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[Data2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Data2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification)由此[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: To fine-tune [TFData2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification)
    on a custom dataset, see [this notebook](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在自定义数据集上微调[TFData2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification)，请参阅[此笔记本](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb)。
- en: '**Data2VecText documentation resources**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2VecText文档资源**'
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标记分类任务指南](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掩码语言建模任务指南](../tasks/masked_language_modeling)'
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多项选择任务指南](../tasks/multiple_choice)'
- en: '**Data2VecAudio documentation resources**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2VecAudio文档资源**'
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[音频分类任务指南](../tasks/audio_classification)'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动语音识别任务指南](../tasks/asr)'
- en: '**Data2VecVision documentation resources**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2VecVision文档资源**'
- en: '[Image classification](../tasks/image_classification)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像分类](../tasks/image_classification)'
- en: '[Semantic segmentation](../tasks/semantic_segmentation)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义分割](../tasks/semantic_segmentation)'
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将对其进行审查！资源应该展示一些新内容，而不是重复现有资源。
- en: Data2VecTextConfig
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecTextConfig
- en: '### `class transformers.Data2VecTextConfig`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecTextConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_text.py#L31)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_text.py#L31)'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    DATA2VEC model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling `Data2VecModel`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *可选*, 默认为30522) — DATA2VEC模型的词汇量。定义了在调用`Data2VecModel`时可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *可选*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *可选*, 默认为3072) — Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`Callable`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *可选*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *可选*, 默认为0.1) — 注意力概率的丢失比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *可选*, 默认为512) — 此模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling `Data2VecModel`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *可选*, 默认为2) — 在调用`Data2VecModel`时传递的`token_type_ids`的词汇量。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *可选*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *可选*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`之一。有关`"relative_key"`的更多信息，请参阅[使用相对位置表示的自注意力（Shaw等人）](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参阅[使用更好的相对位置嵌入改进Transformer模型（Huang等人）](https://arxiv.org/abs/2009.13658)中的*方法4*。'
- en: '`is_decoder` (`bool`, *optional*, defaults to `False`) — Whether the model
    is used as a decoder or not. If `False`, the model is used as an encoder.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_decoder` (`bool`, *可选*, 默认为`False`) — 模型是否用作解码器。如果为`False`，则模型用作编码器。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在`config.is_decoder=True`时相关。'
- en: '`classifier_dropout` (`float`, *optional*) — The dropout ratio for the classification
    head.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout` (`float`, *optional*) — 分类头的 dropout 比率。'
- en: This is the configuration class to store the configuration of a [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel)
    and [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel).
    It is used to instantiate a Data2VecText model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Data2VecText [facebook/data2vec-text-base](https://huggingface.co/facebook/data2vec-text-base)
    architecture.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel)
    和 [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel)
    配置的配置类。根据指定的参数实例化一个 Data2VecText 模型，定义模型架构。使用默认值实例化配置将产生类似于 Data2VecText [facebook/data2vec-text-base](https://huggingface.co/facebook/data2vec-text-base)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data2VecAudioConfig
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioConfig
- en: '### `class transformers.Data2VecAudioConfig`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_audio.py#L31)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_audio.py#L31)'
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32) — Vocabulary size of the Data2VecAudio
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    or `TFData2VecAudioModel`. Vocabulary size of the model. Defines the different
    tokens that can be represented by the *inputs_ids* passed to the forward method
    of [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 32) — Data2VecAudio 模型的词汇表大小。定义了在调用
    [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    或 `TFData2VecAudioModel` 时可以表示的不同标记数量。模型的词汇表大小。定义了传递给 [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    的 *inputs_ids* 可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的
    dropout 概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 全连接层内激活的 dropout
    比率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout
    比率。'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the final projection layer of [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) — [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)
    的最终投影层的 dropout 概率。'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) — LayerDrop 概率。更多细节请参阅 [LayerDrop
    论文](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for output of the feature encoder.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的 dropout
    概率。'
- en: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` “gelu”`,` “relu”`,` “selu”`and`“gelu_new”` are
    supported.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`
    “gelu”`、` “relu”`、` “selu”`和`“gelu_new”`。'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the feature encoder. The
    length of *conv_dim* defines the number of 1D convolutional layers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — 定义特征编码器中每个1D卷积层的输入和输出通道数的整数元组。*conv_dim*的长度定义了1D卷积层的数量。'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the feature encoder. The length of *conv_stride* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — 定义特征编码器中每个1D卷积层的步幅的整数元组。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the feature encoder. The length of *conv_kernel* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) — 定义特征编码器中每个1D卷积层的内核大小的整数元组。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) — Whether the 1D convolutional
    layers have a bias.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否有偏置。'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的内核大小。'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates ”mask_time_prob*len(time_axis)/mask_time_length” independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — 沿时间轴的所有特征向量中将被掩盖的百分比（介于0和1之间）。掩码过程在轴上生成”mask_time_prob*len(time_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为要掩盖的向量跨度的起始的概率推理，*
    mask_time_prob *应该是`prob_vector_start*mask_time_length`。请注意，重叠可能会降低'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *optional*, defaults to 10) — 沿时间轴的向量跨度长度。'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), — 沿时间轴生成的长度为`mask_feature_length`的掩码的最小数量，每个时间步，与`mask_feature_prob`无关。仅在”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”时相关'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates ”mask_feature_prob*len(feature_axis)/mask_time_length”
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — 沿特征轴的所有特征向量中将被掩盖的百分比（介于0和1之间）。掩码过程在轴上生成”mask_feature_prob*len(feature_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为要掩盖的向量跨度的起始的概率推理，*
    mask_feature_prob *应该是`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低掩盖向量的实际百分比。仅在`apply_spec_augment为True`时相关。'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *optional*, defaults to 10) — 沿特征轴的向量跨度长度。'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), — The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), — 沿特征轴生成的长度为`mask_feature_length`的掩码的最小数量，每个时间步，与`mask_feature_prob`无关。仅在”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”时相关'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) — Specifies the
    reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training
    an instance of [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) — 指定应用于`torch.nn.CTCLoss`输出的减少方式。仅在训练[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)实例时相关。'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — 是否将`torch.nn.CTCLoss`的无限损失和相关梯度置零。当输入太短无法与目标对齐时，主要会出现无限损失。仅在训练[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)实例时相关。'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [Data2VecAudioForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — 是否使用带有学习权重的层输出的加权平均。仅在使用[Data2VecAudioForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification)实例时相关。'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) — Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *optional*, defaults to 256) — 分类前的投影维度，用于标记均值池化。'
- en: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) — A tuple of integers defining the number of output channels
    of each 1D convolutional layer in the *TDNN* module of the *XVector* model. The
    length of *tdnn_dim* defines the number of *TDNN* layers.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) — 一个整数元组，定义*XVector*模型中*TDNN*模块中每个1D卷积层的输出通道数。*tdnn_dim*的长度定义了*TDNN*层数。'
- en: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the *TDNN* module of the *XVector* model. The length of *tdnn_kernel*
    has to match the length of *tdnn_dim*.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) — 一个整数元组，定义*XVector*模型中*TDNN*模块中每个1D卷积层的核大小。*tdnn_kernel*的长度必须与*tdnn_dim*的长度相匹配。'
- en: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) — A tuple of integers defining the dilation factor of each 1D convolutional
    layer in *TDNN* module of the *XVector* model. The length of *tdnn_dilation* has
    to match the length of *tdnn_dim*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) — 一个整数元组，定义*XVector*模型中*TDNN*模块中每个1D卷积层的扩张因子。*tdnn_dilation*的长度必须与*tdnn_dim*的长度相匹配。'
- en: '`xvector_output_dim` (`int`, *optional*, defaults to 512) — Dimensionality
    of the *XVector* embedding vectors.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xvector_output_dim` (`int`, *optional*, defaults to 512) — *XVector*嵌入向量的维度。'
- en: '`add_adapter` (`bool`, *optional*, defaults to `False`) — Whether a convolutional
    network should be stacked on top of the Data2VecAudio Encoder. Can be very useful
    for warm-starting Data2VecAudio for SpeechEncoderDecoder models.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_adapter` (`bool`, *optional*, defaults to `False`) — 是否在Data2VecAudio编码器顶部堆叠卷积网络。对于启动Data2VecAudio用于SpeechEncoderDecoder模型非常有用。'
- en: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) — Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) — 适配器网络中卷积层的核大小。仅在`add_adapter`为True时相关。'
- en: '`adapter_stride` (`int`, *optional*, defaults to 2) — Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_stride` (`int`, *optional*, defaults to 2) — 适配器网络中卷积层的步幅。仅在`add_adapter`为True时相关。'
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 3) — Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_adapter_layers` (`int`, *optional*, defaults to 3) — 适配器网络中应使用的卷积层数量。仅在`add_adapter`为True时相关。'
- en: '`output_hidden_size` (`int`, *optional*) — Dimensionality of the encoder output
    layer. If not defined, this defaults to *hidden-size*. Only relevant if `add_adapter
    is True`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_size` (`int`, *optional*) — 编码器输出层的维度。如果未定义，则默认为*hidden-size*。仅在`add_adapter`为True时相关。'
- en: This is the configuration class to store the configuration of a [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel).
    It is used to instantiate an Data2VecAudio model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Data2VecAudio [facebook/data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h)
    architecture.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)配置的配置类。它用于根据指定的参数实例化一个Data2VecAudio模型，定义模型架构。使用默认值实例化配置将产生类似于Data2VecAudio
    [facebook/data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Data2VecVisionConfig
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecVisionConfig
- en: '### `class transformers.Data2VecVisionConfig`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecVisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_vision.py#L35)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_vision.py#L35)'
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢弃比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。'
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 16) — 每个补丁的大小（分辨率）。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。'
- en: '`use_mask_token` (`bool`, *optional*, defaults to `False`) — Whether to use
    a mask token for masked image modeling.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mask_token` (`bool`, *optional*, defaults to `False`) — 是否在掩蔽图像建模中使用掩蔽标记。'
- en: '`use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`)
    — Whether to use BERT-style absolute position embeddings.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`)
    — 是否使用类似 BERT 的绝对位置嵌入。'
- en: '`use_relative_position_bias` (`bool`, *optional*, defaults to `False`) — Whether
    to use T5-style relative position embeddings in the self-attention layers.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_relative_position_bias` (`bool`, *optional*, defaults to `False`) — 是否在自注意力层中使用
    T5 风格的相对位置嵌入。'
- en: '`use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`)
    — Whether to use the same relative position embeddings across all self-attention
    layers of the Transformer.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`)
    — 是否在 Transformer 的所有自注意力层中使用相同的相对位置嵌入。'
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.1) — Scale to
    use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable
    layer scale.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.1) — 自注意力层中使用的比例。基础为
    0.1，大型为 1e-5。设置为 0 以禁用层比例。'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) — Stochastic depth
    rate per sample (when applied in the main path of residual layers).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) — 每个样本的随机深度率（当应用于残差层的主路径时）。'
- en: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) — Whether to mean
    pool the final hidden states of the patches instead of using the final hidden
    state of the CLS token, before applying the classification head.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) — 是否对补丁的最终隐藏状态进行平均池化，而不是使用
    CLS 标记的最终隐藏状态后应用分类头。'
- en: '`out_indices` (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`) — Indices
    of the feature maps to use for semantic segmentation.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`) — 用于语义分割的特征图的索引。'
- en: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) — Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) — 应用于最后特征图的池化金字塔模块中使用的池化比例。'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) — Whether to
    use an auxiliary head during training.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) — 是否在训练期间使用辅助头。'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) — Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) — 辅助头的交叉熵损失的权重。'
- en: '`auxiliary_channels` (`int`, *optional*, defaults to 256) — Number of channels
    to use in the auxiliary head.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_channels` (`int`, *optional*, defaults to 256) — 辅助头中要使用的通道数。'
- en: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) — Number of convolutional
    layers to use in the auxiliary head.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) — 辅助头中要使用的卷积层数量。'
- en: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) — Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) — 是否在分类层之前将辅助头的输出与输入连接起来。'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) — The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) — 语义分割模型损失函数中被忽略的索引。'
- en: This is the configuration class to store the configuration of a [Data2VecVisionModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionModel).
    It is used to instantiate an Data2VecVision model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Data2VecVision [facebook/data2vec-vision-base](https://huggingface.co/facebook/data2vec-vision-base)
    architecture.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是存储[Data2VecVisionModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionModel)配置的配置类。它用于根据指定的参数实例化一个Data2VecVision模型，定义模型架构。使用默认值实例化配置将产生类似于Data2VecVision
    [facebook/data2vec-vision-base](https://huggingface.co/facebook/data2vec-vision-base)架构的配置。
- en: 'Example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: PytorchHide Pytorch content
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: Data2VecAudioModel
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioModel
- en: '### `class transformers.Data2VecAudioModel`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L812)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L812)'
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'The bare Data2VecAudio Model transformer outputting raw hidden-states without
    any specific head on top. Data2VecAudio was proposed in [data2vec: A General Framework
    for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555)
    by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael
    Auli.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Data2VecAudio模型变压器输出原始隐藏状态，没有特定的头部。Data2VecAudio是由Alexei Baevski、Wei-Ning Hsu、Qiantong
    Xu、Arun Babu、Jiatao Gu和Michael Auli在[数据2vec：语音、视觉和语言自监督学习的通用框架](https://arxiv.org/pdf/2202.03555)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L887)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L887)'
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type *List[float]* or a *numpy.ndarray*,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into *input_values*, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 输入原始语音波形的浮点值。可以通过将*.flac*或*.wav*音频文件加载到*List[float]*或*numpy.ndarray*类型的数组中获得值，例如通过soundfile库（*pip
    install soundfile*）。要将数组准备成*input_values*，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为类型为*torch.FloatTensor*的张量。有关详细信息，请参见[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    用于避免在填充令牌索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩码`的令牌，
- en: 0 for tokens that are `masked`.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的令牌。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should be passed if the corresponding processor has `config.return_attention_mask
    == True`, which is the case for all pre-trained Data2Vec Audio models. Be aware
    that that even with `attention_mask`, zero-padded inputs will have slightly different
    outputs compared to non-padded inputs because there are more than one convolutional
    layer in the positional encodings. For a more detailed explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果相应的处理器具有`config.return_attention_mask == True`，则应传递`attention_mask`，这适用于所有预训练的Data2Vec音频模型。请注意，即使使用`attention_mask`，零填充的输入与非填充的输入将具有略有不同的输出，因为在位置编码中有多个卷积层。有关更详细的解释，请参见[这里](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    and inputs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — 模型最后一个卷积层提取的特征向量序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每一层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    forward method, overrides the `__call__` special method.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Data2VecAudioForAudioFrameClassification
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioForAudioFrameClassification
- en: '### `class transformers.Data2VecAudioForAudioFrameClassification`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioForAudioFrameClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1196)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1196)'
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Data2VecAudio Model with a frame classification head on top for tasks like Speaker
    Diarization.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Data2VecAudio模型，顶部带有用于说话人分离等任务的帧分类头。
- en: 'Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by
    Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2VecAudio是由Alexei Baevski、Wei-Ning Hsu、Qiantong Xu、Arun Babu、Jiatao Gu和Michael
    Auli在[数据2vec: 语音、视觉和语言自监督学习的通用框架](https://arxiv.org/pdf/2202.03555)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1247)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1247)'
- en: '[PRE10]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type *List[float]* or a *numpy.ndarray*,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into *input_values*, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 输入原始语音波形的浮点值。可以通过将 *.flac* 或 *.wav* 音频文件加载到 *List[float]* 类型的数组或 *numpy.ndarray*
    中获得这些值，例如通过 soundfile 库 (*pip install soundfile*)。要将数组准备成 *input_values*，应该使用
    [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    进行填充和转换为 *torch.FloatTensor* 类型的张量。详细信息请参见 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在 `[0, 1]` 范围内。'
- en: 1 for tokens that are `not masked`,
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被 `masked` 的标记为 1，
- en: 0 for tokens that are `masked`.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被 `masked` 的标记为 0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should be passed if the corresponding processor has `config.return_attention_mask
    == True`, which is the case for all pre-trained Data2Vec Audio models. Be aware
    that that even with `attention_mask`, zero-padded inputs will have slightly different
    outputs compared to non-padded inputs because there are more than one convolutional
    layer in the positional encodings. For a more detailed explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果相应的处理器具有 `config.return_attention_mask == True`，则应传递 `attention_mask`，这适用于所有预训练的
    Data2Vec 音频模型。请注意，即使使用了 `attention_mask`，零填充的输入与非填充的输入会有稍微不同的输出，因为在位置编码中有多个卷积层。有关更详细的解释，请参见[这里](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失 (均方损失)，如果
    `config.num_labels > 1`，则计算分类损失 (交叉熵)。'
- en: Returns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    and inputs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    或一个 `torch.FloatTensor` 元组 (如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时) 包含各种元素，这取决于配置 ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回)
    — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — 分类分数 (SoftMax 之前)。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层，则为嵌入的输出 + 每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Data2VecAudioForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[Data2VecAudioForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Data2VecAudioForCTC
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioForCTC
- en: '### `class transformers.Data2VecAudioForCTC`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L948)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L948)'
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: 'Data2VecAudio Model with a `language modeling` head on top for Connectionist
    Temporal Classification (CTC). Data2VecAudio was proposed in [data2vec: A General
    Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555)
    by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael
    Auli.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2VecAudio 模型在顶部带有 `语言建模` 头部，用于 Connectionist Temporal Classification (CTC)。Data2VecAudio
    是由 Alexei Baevski、Wei-Ning Hsu、Qiantong Xu、Arun Babu、Jiatao Gu 和 Michael Auli
    在 [data2vec: A General Framework for Self-supervised Learning in Speech, Vision
    and Language](https://arxiv.org/pdf/2202.03555) 中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L993)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L993)'
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type *List[float]* or a *numpy.ndarray*,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into *input_values*, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 输入原始语音波形的浮点值。值可以通过将 *.flac* 或 *.wav* 音频文件加载到 *List[float]* 或 *numpy.ndarray*
    类型的数组中获得，例如通过 soundfile 库 (*pip install soundfile*)。要将数组准备成 *input_values*，应使用
    [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    进行填充和转换为 *torch.FloatTensor* 类型的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在 `[0, 1]` 之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于 `未被掩码` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于 `被掩码` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should be passed if the corresponding processor has `config.return_attention_mask
    == True`, which is the case for all pre-trained Data2Vec Audio models. Be aware
    that that even with `attention_mask`, zero-padded inputs will have slightly different
    outputs compared to non-padded inputs because there are more than one convolutional
    layer in the positional encodings. For a more detailed explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果相应的处理器具有`config.return_attention_mask == True`，则应传递`attention_mask`，这对于所有预训练的Data2Vec
    Audio模型都是如此。请注意，即使使用`attention_mask`，零填充的输入与非填充的输入将具有稍有不同的输出，因为在位置编码中有多个卷积层。有关更详细的解释，请参见[这里](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, target_length)`，*optional*) —
    连接主义时间分类的标签。请注意，`target_length`必须小于或等于输出logits的序列长度。索引在`[-100, 0, ..., config.vocab_size
    - 1]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0, ..., config.vocab_size - 1]`中的标签。'
- en: Returns
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    and inputs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)）和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每层输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Data2VecAudioForSequenceClassification
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioForSequenceClassification
- en: '### `class transformers.Data2VecAudioForSequenceClassification`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1074)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1074)'
- en: '[PRE15]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)）-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Data2VecAudio Model with a sequence classification head on top (a linear layer
    over the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Data2VecAudio模型在顶部具有一个序列分类头（一个线性层在池化输出之上）用于类似SUPERB关键词检测的任务。
- en: 'Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by
    Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2VecAudio是由Alexei Baevski、Wei-Ning Hsu、Qiantong Xu、Arun Babu、Jiatao Gu和Michael
    Auli在[数据2vec: 语音、视觉和语言自监督学习的通用框架](https://arxiv.org/pdf/2202.03555)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1126)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1126)'
- en: '[PRE16]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
