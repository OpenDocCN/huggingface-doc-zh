- en: Load
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Your data can be stored in various places; they can be on your local machine’s
    disk, in a Github repository, and in in-memory data structures like Python dictionaries
    and Pandas DataFrames. Wherever a dataset is stored, 🤗 Datasets can help you load
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will show you how to load a dataset from:'
  prefs: []
  type: TYPE_NORMAL
- en: The Hub without a dataset loading script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local loading script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A specific slice of a split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details specific to loading other dataset modalities, take a look at
    the [load audio dataset guide](./audio_load), the [load image dataset guide](./image_load),
    or the [load text dataset guide](./nlp_load).
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets are loaded from a dataset loading script that downloads and generates
    the dataset. However, you can also load a dataset from any dataset repository
    on the Hub without a loading script! Begin by [creating a dataset repository](share#create-the-repository)
    and upload your data files. Now you can use the [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function to load the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, try loading the files from this [demo repository](https://huggingface.co/datasets/lhoestq/demo1)
    by providing the repository namespace and dataset name. This dataset repository
    contains CSV files, and the code below loads the dataset from the CSV files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Some datasets may have more than one version based on Git tags, branches, or
    commits. Use the `revision` parameter to specify the dataset version you want
    to load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Refer to the [Upload a dataset to the Hub](./upload_dataset) tutorial for more
    details on how to create a dataset repository on the Hub, and how to upload your
    data files.
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset without a loading script by default loads all the data into the `train`
    split. Use the `data_files` parameter to map data files to splits like `train`,
    `validation` and `test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t specify which data files to use, [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    will return all the data files. This can take a long time if you load a large
    dataset like C4, which is approximately 13TB of data.
  prefs: []
  type: TYPE_NORMAL
- en: You can also load a specific subset of the files with the `data_files` or `data_dir`
    parameter. These parameters can accept a relative path which resolves to the base
    path corresponding to where the dataset is loaded from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `split` parameter can also map a data file to a specific split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Local loading script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have a 🤗 Datasets loading script locally on your computer. In this
    case, load the dataset by passing one of the following paths to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: The local path to the loading script file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The local path to the directory containing the loading script file (only if
    the script file has the same name as the directory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pass `trust_remote_code=True` to allow 🤗 Datasets to execute the loading script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Edit loading script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can also edit a loading script from the Hub to add your own modifications.
    Download the dataset repository locally so any data files referenced by a relative
    path in the loading script can be loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Make your edits to the loading script and then load it by passing its local
    path to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Local and remote files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets can be loaded from local files stored on your computer and from remote
    files. The datasets are most likely stored as a `csv`, `json`, `txt` or `parquet`
    file. The [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function can load each of these file types.
  prefs: []
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '🤗 Datasets can read a dataset made up of one or several CSV files (in this
    case, pass your CSV files as a list):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For more details, check out the [how to load tabular datasets from CSV files](tabular_load#csv-files)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'JSON files are loaded directly with [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'JSON files have diverse formats, but we think the most efficient format is
    to have multiple JSON objects; each line represents an individual row of data.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Another JSON format you may encounter is a nested field, in which case you’ll
    need to specify the `field` argument as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To load remote JSON files via HTTP, pass the URLs instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: While these are the most common JSON formats, you’ll see other datasets that
    are formatted differently. 🤗 Datasets recognizes these other formats and will
    fallback accordingly on the Python JSON loading methods to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parquet files are stored in a columnar format, unlike row-based files like a
    CSV. Large datasets may be stored in a Parquet file because it is more efficient
    and faster at returning your query.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a Parquet file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To load remote Parquet files via HTTP, pass the URLs instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Arrow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Arrow files are stored in an in-memory columnar format, unlike row-based formats
    like CSV and uncompressed formats like Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load an Arrow file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To load remote Arrow files via HTTP, pass the URLs instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Arrow is the file format used by 🤗 Datasets under the hood, therefore you can
    load a local Arrow file using [Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Unlike [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    [Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)
    memory maps the Arrow file without preparing the dataset in the cache, saving
    you disk space. The cache directory to store intermediate processing results will
    be the Arrow file directory in that case.
  prefs: []
  type: TYPE_NORMAL
- en: For now only the Arrow streaming format is supported. The Arrow IPC file format
    (also known as Feather V2) is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Read database contents with [from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)
    by specifying the URI to connect to your database. You can read both table names
    and queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: For more details, check out the [how to load tabular datasets from SQL databases](tabular_load#databases)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: WebDataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [WebDataset](https://github.com/webdataset/webdataset) format is based on
    TAR archives and is suitable for big image datasets. Because of their size, WebDatasets
    are generally loaded in streaming mode (using `streaming=True`).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can load a WebDataset like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To load remote WebDatasets via HTTP, pass the URLs instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a dataset is made of several files (that we call “shards”), it is possible
    to significantly speed up the dataset downloading and preparation step.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose how many processes you’d like to use to prepare a dataset in
    parallel using `num_proc`. In this case, each process is given a subset of shards
    to prepare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In-memory data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 🤗 Datasets will also allow you to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    directly from in-memory data structures like Python dictionaries and Pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Python dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Load Python dictionaries with [from_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_dict):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Python list of dictionaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Load a list of Python dictionaries with `from_list()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Python generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create a dataset from a Python generator with [from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_generator):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This approach supports loading data larger than available memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also define a sharded dataset by passing lists to `gen_kwargs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Pandas DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Load Pandas DataFrames with [from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: For more details, check out the [how to load tabular datasets from Pandas DataFrames](tabular_load#pandas-dataframes)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: Offline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if you don’t have an internet connection, it is still possible to load
    a dataset. As long as you’ve downloaded a dataset from the Hub repository before,
    it should be cached. This means you can reload the dataset from the cache and
    use it offline.
  prefs: []
  type: TYPE_NORMAL
- en: If you know you won’t have internet access, you can run 🤗 Datasets in full offline
    mode. This saves time because instead of waiting for the Dataset builder download
    to time out, 🤗 Datasets will look directly in the cache. Set the environment variable
    `HF_DATASETS_OFFLINE` to `1` to enable full offline mode.
  prefs: []
  type: TYPE_NORMAL
- en: Slice splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also choose only to load specific slices of a split. There are two
    options for slicing a split: using strings or the [ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    API. Strings are more compact and readable for simple cases, while [ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    is easier to use with variable slicing parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concatenate a `train` and `test` split by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
    Or select a percentage of a split with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
    Finally, you can even create cross-validated splits. The example below creates
    10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the
    training dataset makes up the remaining complementary 90% chunk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 19 records, from 500 (included) to 519 (excluded).
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")'
  prefs: []
  type: TYPE_NORMAL
- en: 20 records, from 519 (included) to 539 (excluded).
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 18 records, from 450 (included) to 468 (excluded).
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",
    from_=50, to=52, unit="%", rounding="pct1_dropremainder"))'
  prefs: []
  type: TYPE_NORMAL
- en: 18 records, from 468 (included) to 486 (excluded).
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52,
    to=54, unit="%", rounding="pct1_dropremainder"))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Or equivalently:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '>>> dataset = load_dataset("matinf", "summarization")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Downloading and preparing dataset matinf/summarization (download: Unknown size,
    generated: 246.89 MiB, post-processed: Unknown size, total: 246.89 MiB) to /root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...'
  prefs: []
  type: TYPE_NORMAL
- en: 'AssertionError: The dataset matinf with config summarization requires manual
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please follow the manual download instructions: To use MATINF you have to download
    it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9).
    You will receive a download link and a password once you complete the form. Please
    extract all files in one folder and load the dataset with: *datasets.load_dataset(''matinf'',
    data_dir=''path/to/folder/folder_name'')*.'
  prefs: []
  type: TYPE_NORMAL
- en: Manual data can be loaded with `datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> emotion_features = Features({''text'': Value(''string''), ''label'': ClassLabel(names=class_names)})'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '>>> dataset = load_dataset(''csv'', data_files=file_dict, delimiter='';'',
    column_names=[''text'', ''label''], features=emotion_features)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '>>> dataset[''train''].features'
  prefs: []
  type: TYPE_NORMAL
- en: '{''text'': Value(dtype=''string'', id=None),'
  prefs: []
  type: TYPE_NORMAL
- en: '''label'': ClassLabel(num_classes=6, names=[''sadness'', ''joy'', ''love'',
    ''anger'', ''fear'', ''surprise''], names_file=None, id=None)}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '>>> from datasets import load_metric'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> metric = load_metric(''PATH/TO/MY/METRIC/SCRIPT'')'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> # Example of typical usage'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> for batch in dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '...     inputs, references = batch'
  prefs: []
  type: TYPE_NORMAL
- en: '...     predictions = model(inputs)'
  prefs: []
  type: TYPE_NORMAL
- en: '...     metric.add_batch(predictions=predictions, references=references)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> score = metric.compute()'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '>>> from datasets import load_metric'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> metric = load_metric(''bleurt'', name=''bleurt-base-128'')'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> metric = load_metric(''bleurt'', name=''bleurt-base-512'')'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '>>> from datasets import load_metric'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> metric = load_metric(''glue'', ''mrpc'', num_process=num_process, process_id=rank)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '>>> from datasets import load_metric'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> metric = load_metric(''glue'', ''mrpc'', num_process=num_process, process_id=process_id,
    experiment_id="My_experiment_10")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
