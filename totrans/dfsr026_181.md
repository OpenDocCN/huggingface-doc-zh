# 价值引导规划

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling`](https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling)

🧪 这是一个用于强化学习的实验性管道！

该管道基于 Michael Janner、Yilun Du、Joshua B. Tenenbaum、Sergey Levine 的[Planning with Diffusion for Flexible Behavior Synthesis](https://huggingface.co/papers/2205.09991)论文。

该论文的摘要是：

*基于模型的强化学习方法通常仅用于估计近似动态模型的目的，将决策工作的其余部分转移到经典轨迹优化器。虽然在概念上简单，但这种组合存在许多经验上的缺陷，表明学习模型可能不适合标准轨迹优化。在本文中，我们考虑将尽可能多的轨迹优化管道折叠到建模问题中，使得从模型中采样和规划几乎相同。我们的技术方法的核心在于通过迭代去噪轨迹进行规划的扩散概率模型。我们展示了分类器引导采样和图像修复如何被重新解释为一致的规划策略，探索了基于扩散的规划方法的不寻常和有用的特性，并展示了我们的框架在强调长期决策和测试时间灵活性的控制设置中的有效性。*

您可以在[项目页面](https://diffusion-planning.github.io/)、[原始代码库](https://github.com/jannerm/diffuser)或在演示[笔记本](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1)中找到有关该模型的其他信息。

运行模型的脚本在[此处](https://github.com/huggingface/diffusers/tree/main/examples/reinforcement_learning)可用。

请务必查看调度程序指南以了解如何在调度程序速度和质量之间进行权衡，并查看跨管道重用组件部分，以了解如何有效地将相同组件加载到多个管道中。

## ValueGuidedRLPipeline

### `class diffusers.experimental.ValueGuidedRLPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/experimental/rl/value_guided_sampling.py#L25)

```py
( value_function: UNet1DModel unet: UNet1DModel scheduler: DDPMScheduler env )
```

参数

+   `value_function`（UNet1DModel）- 用于根据奖励微调轨迹的专门 UNet。

+   `unet`（UNet1DModel）- 用于去噪编码轨迹的 UNet 架构。

+   `scheduler`（SchedulerMixin）- 与`unet`结合使用以去噪编码轨迹的调度程序。此应用的默认值为 DDPMScheduler。

+   `env`（）- 遵循 OpenAI gym API 以行动的环境。目前只有 Hopper 有预训练模型。

从训练以预测状态序列的扩散模型中进行价值引导采样的管道。

该模型继承自 DiffusionPipeline。查看超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
