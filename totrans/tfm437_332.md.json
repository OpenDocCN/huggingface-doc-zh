["```py\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [\"an image of a cat\", \"an image of a dog\"]\n\ninputs = processor(text=candidate_labels, images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# this is the image-text similarity score\nlogits_per_image = outputs.logits_per_image\n\n# we can take the softmax to get the label probabilities\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)\n```", "```py\n>>> from transformers import AlignConfig, AlignModel\n\n>>> # Initializing a AlignConfig with kakaobrain/align-base style configuration\n>>> configuration = AlignConfig()\n\n>>> # Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration\n>>> model = AlignModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig\n>>> from transformers import AlignTextConfig, AlignVisionConfig\n\n>>> # Initializing ALIGN Text and Vision configurations\n>>> config_text = AlignTextConfig()\n>>> config_vision = AlignVisionConfig()\n\n>>> config = AlignConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n>>> from transformers import AlignTextConfig, AlignTextModel\n\n>>> # Initializing a AlignTextConfig with kakaobrain/align-base style configuration\n>>> configuration = AlignTextConfig()\n\n>>> # Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration\n>>> model = AlignTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AlignVisionConfig, AlignVisionModel\n\n>>> # Initializing a AlignVisionConfig with kakaobrain/align-base style configuration\n>>> configuration = AlignVisionConfig()\n\n>>> # Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration\n>>> model = AlignVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, AlignModel\n\n>>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AlignModel\n\n>>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n>>> processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, AlignTextModel\n\n>>> model = AlignTextModel.from_pretrained(\"kakaobrain/align-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AlignVisionModel\n\n>>> model = AlignVisionModel.from_pretrained(\"kakaobrain/align-base\")\n>>> processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]