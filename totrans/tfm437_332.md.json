["```py\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [\"an image of a cat\", \"an image of a dog\"]\n\ninputs = processor(text=candidate_labels, images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# this is the image-text similarity score\nlogits_per_image = outputs.logits_per_image\n\n# we can take the softmax to get the label probabilities\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)\n```", "```py\n( text_config = None vision_config = None projection_dim = 640 temperature_init_value = 1.0 initializer_range = 0.02 **kwargs )\n```", "```py\n>>> from transformers import AlignConfig, AlignModel\n\n>>> # Initializing a AlignConfig with kakaobrain/align-base style configuration\n>>> configuration = AlignConfig()\n\n>>> # Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration\n>>> model = AlignModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig\n>>> from transformers import AlignTextConfig, AlignVisionConfig\n\n>>> # Initializing ALIGN Text and Vision configurations\n>>> config_text = AlignTextConfig()\n>>> config_vision = AlignVisionConfig()\n\n>>> config = AlignConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: AlignTextConfig vision_config: AlignVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';AlignConfig\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True **kwargs )\n```", "```py\n>>> from transformers import AlignTextConfig, AlignTextModel\n\n>>> # Initializing a AlignTextConfig with kakaobrain/align-base style configuration\n>>> configuration = AlignTextConfig()\n\n>>> # Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration\n>>> model = AlignTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( num_channels: int = 3 image_size: int = 600 width_coefficient: float = 2.0 depth_coefficient: float = 3.1 depth_divisor: int = 8 kernel_sizes: List = [3, 3, 5, 3, 5, 5, 3] in_channels: List = [32, 16, 24, 40, 80, 112, 192] out_channels: List = [16, 24, 40, 80, 112, 192, 320] depthwise_padding: List = [] strides: List = [1, 2, 2, 2, 1, 2, 1] num_block_repeats: List = [1, 2, 2, 3, 3, 4, 1] expand_ratios: List = [1, 6, 6, 6, 6, 6, 6] squeeze_expansion_ratio: float = 0.25 hidden_act: str = 'swish' hidden_dim: int = 2560 pooling_type: str = 'mean' initializer_range: float = 0.02 batch_norm_eps: float = 0.001 batch_norm_momentum: float = 0.99 drop_connect_rate: float = 0.2 **kwargs )\n```", "```py\n>>> from transformers import AlignVisionConfig, AlignVisionModel\n\n>>> # Initializing a AlignVisionConfig with kakaobrain/align-base style configuration\n>>> configuration = AlignVisionConfig()\n\n>>> # Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration\n>>> model = AlignVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: AlignConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoTokenizer, AlignModel\n\n>>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AlignModel\n\n>>> model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n>>> processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config: AlignTextConfig add_pooling_layer: bool = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, AlignTextModel\n\n>>> model = AlignTextModel.from_pretrained(\"kakaobrain/align-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: AlignVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AlignVisionModel\n\n>>> model = AlignVisionModel.from_pretrained(\"kakaobrain/align-base\")\n>>> processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]