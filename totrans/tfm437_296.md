# ViTDet

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vitdet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vitdet)

## 概述

ViTDet模型是由李洋浩、毛汉子、Ross Girshick、何开明在[探索普通Vision Transformer骨干用于目标检测](https://arxiv.org/abs/2203.16527)中提出的。VitDet利用普通的Vision Transformer来进行目标检测任务。

论文摘要如下：

*我们探索了作为目标检测的骨干网络的普通、非分层的Vision Transformer（ViT）。这种设计使得原始的ViT架构可以在不需要重新设计分层骨干进行预训练的情况下进行微调以用于目标检测。通过最小的微调适应，我们的普通骨干检测器可以取得竞争性的结果。令人惊讶的是，我们观察到：（i）从单尺度特征图构建简单的特征金字塔就足够了（不需要常见的FPN设计），（ii）使用窗口注意力（无需移位）辅以极少的跨窗口传播块就足够了。通过将作为Masked Autoencoders（MAE）进行预训练的普通ViT骨干，我们的检测器ViTDet可以与以往基于分层骨干的领先方法竞争，仅使用ImageNet-1K预训练就能在COCO数据集上达到高达61.3的AP_box。我们希望我们的研究能引起对普通骨干检测器研究的关注。*

该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet)找到。

提示：

+   目前只有骨干网络可用。

## VitDetConfig

### `class transformers.VitDetConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitdet/configuration_vitdet.py#L30)

```py
( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 mlp_ratio = 4 hidden_act = 'gelu' dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-06 image_size = 224 pretrain_image_size = 224 patch_size = 16 num_channels = 3 qkv_bias = True drop_path_rate = 0.0 window_block_indices = [] residual_block_indices = [] use_absolute_position_embeddings = True use_relative_position_embeddings = False window_size = 0 out_features = None out_indices = None **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。

+   `mlp_ratio` (`int`, *optional*, defaults to 4) — mlp隐藏维度与嵌入维度的比率。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — 层归一化层使用的epsilon。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `pretrain_image_size` (`int`, *optional*, defaults to 224) — 预训练期间每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 16) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。

+   `drop_path_rate` (`float`, *optional*, defaults to 0.0) — 随机深度率。

+   `window_block_indices` (`List[int]`, *optional*, defaults to `[]`) — 应该使用窗口注意力而不是常规全局自注意力的块的索引列表。

+   `residual_block_indices` (`List[int]`, *optional*, defaults to `[]`) — 应在MLP后具有额外残差块的块的索引列表。

+   `use_absolute_position_embeddings` (`bool`, *optional*, 默认为`True`) — 是否向补丁嵌入中添加绝对位置嵌入。

+   `use_relative_position_embeddings` (`bool`, *optional*, 默认为`False`) — 是否向注意力图添加相对位置嵌入。

+   `window_size` (`int`，*可选*，默认为0) — 注意力窗口的大小。

+   `out_features` (`List[str]`，*可选*) — 如果用作骨干，要输出的特征列表。可以是任何一个`"stem"`、`"stage1"`、`"stage2"`等（取决于模型有多少阶段）。如果未设置并且设置了`out_indices`，将默认为相应的阶段。如果未设置并且`out_indices`未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序排列。

+   `out_indices` (`List[int]`，*可选*) — 如果用作骨干，要输出的特征的索引列表。可以是0、1、2等（取决于模型有多少阶段）。如果未设置并且设置了`out_features`，将默认为相应的阶段。如果未设置并且`out_features`未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序排列。

这是用于存储[VitDetModel](/docs/transformers/v4.37.2/en/model_doc/vitdet#transformers.VitDetModel)配置的配置类。它用于根据指定的参数实例化一个VitDet模型，定义模型架构。使用默认值实例化配置将产生类似于VitDet [google/vitdet-base-patch16-224](https://huggingface.co/google/vitdet-base-patch16-224)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import VitDetConfig, VitDetModel

>>> # Initializing a VitDet configuration
>>> configuration = VitDetConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = VitDetModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## VitDetModel

### `class transformers.VitDetModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitdet/modeling_vitdet.py#L698)

```py
( config: VitDetConfig )
```

参数

+   `config` ([VitDetConfig](/docs/transformers/v4.37.2/en/model_doc/vitdet#transformers.VitDetConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

VitDet Transformer模型裸输出原始隐藏状态，顶部没有特定的头。这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitdet/modeling_vitdet.py#L724)

```py
( pixel_values: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被`掩盖`。

    +   0表示头部被`掩盖`。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回值

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VitDetConfig](/docs/transformers/v4.37.2/en/model_doc/vitdet#transformers.VitDetConfig)）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层输出的隐藏状态序列。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每一层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[VitDetModel](/docs/transformers/v4.37.2/en/model_doc/vitdet#transformers.VitDetModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传播的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import VitDetConfig, VitDetModel
>>> import torch

>>> config = VitDetConfig()
>>> model = VitDetModel(config)

>>> pixel_values = torch.randn(1, 3, 224, 224)

>>> with torch.no_grad():
...     outputs = model(pixel_values)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 768, 14, 14]
```
