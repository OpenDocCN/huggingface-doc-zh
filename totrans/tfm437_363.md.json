["```py\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n\n>>> from transformers import Owlv2Processor, Owlv2ForObjectDetection\n\n>>> processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n>>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n>>> target_sizes = torch.Tensor([image.size[::-1]])\n>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\n>>> results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n>>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n>>> text = texts[i]\n>>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n>>> for box, score, label in zip(boxes, scores, labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a photo of a cat with confidence 0.614 at location [341.67, 17.54, 642.32, 278.51]\nDetected a photo of a cat with confidence 0.665 at location [6.75, 38.97, 326.62, 354.85]\n```", "```py\n( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 return_dict = True **kwargs )\n```", "```py\n( text_config: Dict vision_config: Dict **kwargs ) \u2192 export const metadata = 'undefined';Owlv2Config\n```", "```py\n( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 16 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 0 bos_token_id = 49406 eos_token_id = 49407 **kwargs )\n```", "```py\n>>> from transformers import Owlv2TextConfig, Owlv2TextModel\n\n>>> # Initializing a Owlv2TextModel with google/owlv2-base-patch16 style configuration\n>>> configuration = Owlv2TextConfig()\n\n>>> # Initializing a Owlv2TextConfig from the google/owlv2-base-patch16 style configuration\n>>> model = Owlv2TextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 768 patch_size = 16 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import Owlv2VisionConfig, Owlv2VisionModel\n\n>>> # Initializing a Owlv2VisionModel with google/owlv2-base-patch16 style configuration\n>>> configuration = Owlv2VisionConfig()\n\n>>> # Initializing a Owlv2VisionModel model from the google/owlv2-base-patch16 style configuration\n>>> model = Owlv2VisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_pad: bool = True do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )\n```", "```py\n( images: Union do_pad: bool = None do_resize: bool = None size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( outputs threshold: float = 0.1 target_sizes: Union = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold = 0.0 nms_threshold = 0.3 target_sizes = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( image_processor tokenizer **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: Owlv2Config )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_base_image_embeds: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.owlv2.modeling_owlv2.Owlv2Output or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Owlv2Model\n\n>>> model = Owlv2Model.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(text=[[\"a photo of a cat\", \"a photo of a dog\"]], images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoProcessor, Owlv2Model\n\n>>> model = Owlv2Model.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> inputs = processor(\n...     text=[[\"a photo of a cat\", \"a photo of a dog\"], [\"photo of a astranaut\"]], return_tensors=\"pt\"\n... )\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Owlv2Model\n\n>>> model = Owlv2Model.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config: Owlv2TextConfig )\n```", "```py\n( input_ids: Tensor attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Owlv2TextModel\n\n>>> model = Owlv2TextModel.from_pretrained(\"google/owlv2-base-patch16\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16\")\n>>> inputs = processor(\n...     text=[[\"a photo of a cat\", \"a photo of a dog\"], [\"photo of a astranaut\"]], return_tensors=\"pt\"\n... )\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: Owlv2VisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Owlv2VisionModel\n\n>>> model = Owlv2VisionModel.from_pretrained(\"google/owlv2-base-patch16\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n( config: Owlv2Config )\n```", "```py\n( input_ids: Tensor pixel_values: FloatTensor attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.owlv2.modeling_owlv2.Owlv2ObjectDetectionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import requests\n>>> from PIL import Image\n>>> import numpy as np\n>>> import torch\n>>> from transformers import AutoProcessor, Owlv2ForObjectDetection\n>>> from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n>>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Note: boxes need to be visualized on the padded, unnormalized image\n>>> # hence we'll set the target image sizes (height, width) based on that\n\n>>> def get_preprocessed_image(pixel_values):\n...     pixel_values = pixel_values.squeeze().numpy()\n...     unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n...     unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n...     unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n...     unnormalized_image = Image.fromarray(unnormalized_image)\n...     return unnormalized_image\n\n>>> unnormalized_image = get_preprocessed_image(inputs.pixel_values)\n\n>>> target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n>>> # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n>>> results = processor.post_process_object_detection(\n...     outputs=outputs, threshold=0.2, target_sizes=target_sizes\n... )\n\n>>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n>>> text = texts[i]\n>>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n\n>>> for box, score, label in zip(boxes, scores, labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a photo of a cat with confidence 0.614 at location [512.5, 35.08, 963.48, 557.02]\nDetected a photo of a cat with confidence 0.665 at location [10.13, 77.94, 489.93, 709.69]\n```", "```py\n( pixel_values: FloatTensor query_pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.owlv2.modeling_owlv2.Owlv2ImageGuidedObjectDetectionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n>>> import numpy as np\n>>> from transformers import AutoProcessor, Owlv2ForObjectDetection\n>>> from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n\n>>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> query_url = \"http://images.cocodataset.org/val2017/000000001675.jpg\"\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)\n>>> inputs = processor(images=image, query_images=query_image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model.image_guided_detection(**inputs)\n\n>>> # Note: boxes need to be visualized on the padded, unnormalized image\n>>> # hence we'll set the target image sizes (height, width) based on that\n\n>>> def get_preprocessed_image(pixel_values):\n...     pixel_values = pixel_values.squeeze().numpy()\n...     unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n...     unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n...     unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n...     unnormalized_image = Image.fromarray(unnormalized_image)\n...     return unnormalized_image\n\n>>> unnormalized_image = get_preprocessed_image(inputs.pixel_values)\n\n>>> target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n\n>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> results = processor.post_process_image_guided_detection(\n...     outputs=outputs, threshold=0.9, nms_threshold=0.3, target_sizes=target_sizes\n... )\n>>> i = 0  # Retrieve predictions for the first image\n>>> boxes, scores = results[i][\"boxes\"], results[i][\"scores\"]\n>>> for box, score in zip(boxes, scores):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected similar object with confidence {round(score.item(), 3)} at location {box}\")\nDetected similar object with confidence 0.938 at location [490.96, 109.89, 821.09, 536.11]\nDetected similar object with confidence 0.959 at location [8.67, 721.29, 928.68, 732.78]\nDetected similar object with confidence 0.902 at location [4.27, 720.02, 941.45, 761.59]\nDetected similar object with confidence 0.985 at location [265.46, -58.9, 1009.04, 365.66]\nDetected similar object with confidence 1.0 at location [9.79, 28.69, 937.31, 941.64]\nDetected similar object with confidence 0.998 at location [869.97, 58.28, 923.23, 978.1]\nDetected similar object with confidence 0.985 at location [309.23, 21.07, 371.61, 932.02]\nDetected similar object with confidence 0.947 at location [27.93, 859.45, 969.75, 915.44]\nDetected similar object with confidence 0.996 at location [785.82, 41.38, 880.26, 966.37]\nDetected similar object with confidence 0.998 at location [5.08, 721.17, 925.93, 998.41]\nDetected similar object with confidence 0.969 at location [6.7, 898.1, 921.75, 949.51]\nDetected similar object with confidence 0.966 at location [47.16, 927.29, 981.99, 942.14]\nDetected similar object with confidence 0.924 at location [46.4, 936.13, 953.02, 950.78]\n```"]