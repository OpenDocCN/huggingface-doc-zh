["```py\npip install -q pytorchvideo transformers evaluate\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\n>>> from huggingface_hub import hf_hub_download\n\n>>> hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n>>> filename = \"UCF101_subset.tar.gz\"\n>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")\n```", "```py\n>>> import tarfile\n\n>>> with tarfile.open(file_path) as t:\n...      t.extractall(\".\")\n```", "```py\nUCF101_subset/\n    train/\n        BandMarching/\n            video_1.mp4\n            video_2.mp4\n            ...\n        Archery\n            video_1.mp4\n            video_2.mp4\n            ...\n        ...\n    val/\n        BandMarching/\n            video_1.mp4\n            video_2.mp4\n            ...\n        Archery\n            video_1.mp4\n            video_2.mp4\n            ...\n        ...\n    test/\n        BandMarching/\n            video_1.mp4\n            video_2.mp4\n            ...\n        Archery\n            video_1.mp4\n            video_2.mp4\n            ...\n        ...\n```", "```py\n...\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n...\n```", "```py\n>>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n>>> label2id = {label: i for i, label in enumerate(class_labels)}\n>>> id2label = {i: label for label, i in label2id.items()}\n\n>>> print(f\"Unique classes: {list(label2id.keys())}.\")\n\n# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n```", "```py\n>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n\n>>> model_ckpt = \"MCG-NJU/videomae-base\"\n>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n>>> model = VideoMAEForVideoClassification.from_pretrained(\n...     model_ckpt,\n...     label2id=label2id,\n...     id2label=id2label,\n...     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n... )\n```", "```py\nSome weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\n- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```", "```py\n>>> import pytorchvideo.data\n\n>>> from pytorchvideo.transforms import (\n...     ApplyTransformToKey,\n...     Normalize,\n...     RandomShortSideScale,\n...     RemoveKey,\n...     ShortSideScale,\n...     UniformTemporalSubsample,\n... )\n\n>>> from torchvision.transforms import (\n...     Compose,\n...     Lambda,\n...     RandomCrop,\n...     RandomHorizontalFlip,\n...     Resize,\n... )\n```", "```py\n>>> mean = image_processor.image_mean\n>>> std = image_processor.image_std\n>>> if \"shortest_edge\" in image_processor.size:\n...     height = width = image_processor.size[\"shortest_edge\"]\n>>> else:\n...     height = image_processor.size[\"height\"]\n...     width = image_processor.size[\"width\"]\n>>> resize_to = (height, width)\n\n>>> num_frames_to_sample = model.config.num_frames\n>>> sample_rate = 4\n>>> fps = 30\n>>> clip_duration = num_frames_to_sample * sample_rate / fps\n```", "```py\n>>> train_transform = Compose(\n...     [\n...         ApplyTransformToKey(\n...             key=\"video\",\n...             transform=Compose(\n...                 [\n...                     UniformTemporalSubsample(num_frames_to_sample),\n...                     Lambda(lambda x: x / 255.0),\n...                     Normalize(mean, std),\n...                     RandomShortSideScale(min_size=256, max_size=320),\n...                     RandomCrop(resize_to),\n...                     RandomHorizontalFlip(p=0.5),\n...                 ]\n...             ),\n...         ),\n...     ]\n... )\n\n>>> train_dataset = pytorchvideo.data.Ucf101(\n...     data_path=os.path.join(dataset_root_path, \"train\"),\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n...     decode_audio=False,\n...     transform=train_transform,\n... )\n```", "```py\n>>> val_transform = Compose(\n...     [\n...         ApplyTransformToKey(\n...             key=\"video\",\n...             transform=Compose(\n...                 [\n...                     UniformTemporalSubsample(num_frames_to_sample),\n...                     Lambda(lambda x: x / 255.0),\n...                     Normalize(mean, std),\n...                     Resize(resize_to),\n...                 ]\n...             ),\n...         ),\n...     ]\n... )\n\n>>> val_dataset = pytorchvideo.data.Ucf101(\n...     data_path=os.path.join(dataset_root_path, \"val\"),\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n...     decode_audio=False,\n...     transform=val_transform,\n... )\n\n>>> test_dataset = pytorchvideo.data.Ucf101(\n...     data_path=os.path.join(dataset_root_path, \"test\"),\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n...     decode_audio=False,\n...     transform=val_transform,\n... )\n```", "```py\n>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\n# (300, 30, 75)\n```", "```py\n>>> import imageio\n>>> import numpy as np\n>>> from IPython.display import Image\n\n>>> def unnormalize_img(img):\n...     \"\"\"Un-normalizes the image pixels.\"\"\"\n...     img = (img * std) + mean\n...     img = (img * 255).astype(\"uint8\")\n...     return img.clip(0, 255)\n\n>>> def create_gif(video_tensor, filename=\"sample.gif\"):\n...     \"\"\"Prepares a GIF from a video tensor.\n...     \n...     The video tensor is expected to have the following shape:\n...     (num_frames, num_channels, height, width).\n...     \"\"\"\n...     frames = []\n...     for video_frame in video_tensor:\n...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n...         frames.append(frame_unnormalized)\n...     kargs = {\"duration\": 0.25}\n...     imageio.mimsave(filename, frames, \"GIF\", **kargs)\n...     return filename\n\n>>> def display_gif(video_tensor, gif_name=\"sample.gif\"):\n...     \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n...     video_tensor = video_tensor.permute(1, 0, 2, 3)\n...     gif_filename = create_gif(video_tensor, gif_name)\n...     return Image(filename=gif_filename)\n\n>>> sample_video = next(iter(train_dataset))\n>>> video_tensor = sample_video[\"video\"]\n>>> display_gif(video_tensor)\n```", "```py\n>>> from transformers import TrainingArguments, Trainer\n\n>>> model_name = model_ckpt.split(\"/\")[-1]\n>>> new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n>>> num_epochs = 4\n\n>>> args = TrainingArguments(\n...     new_model_name,\n...     remove_unused_columns=False,\n...     evaluation_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=batch_size,\n...     per_device_eval_batch_size=batch_size,\n...     warmup_ratio=0.1,\n...     logging_steps=10,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"accuracy\",\n...     push_to_hub=True,\n...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n... )\n```", "```py\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n```", "```py\n>>> def collate_fn(examples):\n...     # permute to (num_frames, num_channels, height, width)\n...     pixel_values = torch.stack(\n...         [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n...     )\n...     labels = torch.tensor([example[\"label\"] for example in examples])\n...     return {\"pixel_values\": pixel_values, \"labels\": labels}\n```", "```py\n>>> trainer = Trainer(\n...     model,\n...     args,\n...     train_dataset=train_dataset,\n...     eval_dataset=val_dataset,\n...     tokenizer=image_processor,\n...     compute_metrics=compute_metrics,\n...     data_collator=collate_fn,\n... )\n```", "```py\n>>> train_results = trainer.train()\n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> sample_test_video = next(iter(test_dataset))\n```", "```py\n>>> from transformers import pipeline\n\n>>> video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n>>> video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")\n[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},\n {'score': 0.017777055501937866, 'label': 'BabyCrawling'},\n {'score': 0.01663011871278286, 'label': 'BalanceBeam'},\n {'score': 0.009560945443809032, 'label': 'BandMarching'},\n {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]\n```", "```py\n>>> def run_inference(model, video):\n...     # (num_frames, num_channels, height, width)\n...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n...     inputs = {\n...         \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n...         \"labels\": torch.tensor(\n...             [sample_test_video[\"label\"]]\n...         ),  # this can be skipped if you don't have labels available.\n...     }\n\n...     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n...     inputs = {k: v.to(device) for k, v in inputs.items()}\n...     model = model.to(device)\n\n...     # forward pass\n...     with torch.no_grad():\n...         outputs = model(**inputs)\n...         logits = outputs.logits\n\n...     return logits\n```", "```py\n>>> logits = run_inference(trained_model, sample_test_video[\"video\"])\n```", "```py\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n# Predicted class: BasketballDunk\n```"]