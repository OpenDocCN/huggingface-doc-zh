["```py\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"Spiderman is surfing\"\nvideo_frames = pipe(prompt).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```", "```py\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.enable_model_cpu_offload()\n\n# memory optimization\npipe.enable_vae_slicing()\n\nprompt = \"Darth Vader surfing a wave\"\nvideo_frames = pipe(prompt, num_frames=64).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```", "```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Spiderman is surfing\"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```", "```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\nfrom PIL import Image\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\n# memory optimization\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\npipe.enable_vae_slicing()\n\nprompt = \"Darth Vader surfing a wave\"\nvideo_frames = pipe(prompt, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```", "```py\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_XL\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\n# memory optimization\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\npipe.enable_vae_slicing()\n\nvideo = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]\n\nvideo_frames = pipe(prompt, video=video, strength=0.6).frames\nvideo_path = export_to_video(video_frames)\nvideo_path\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet3DConditionModel scheduler: KarrasDiffusionSchedulers )\n```", "```py\n( prompt: Union = None height: Optional = None width: Optional = None num_frames: int = 16 num_inference_steps: int = 50 guidance_scale: float = 9.0 negative_prompt: Union = None eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'np' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) \u2192 export const metadata = 'undefined';TextToVideoSDPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import TextToVideoSDPipeline\n>>> from diffusers.utils import export_to_video\n\n>>> pipe = TextToVideoSDPipeline.from_pretrained(\n...     \"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\"\n... )\n>>> pipe.enable_model_cpu_offload()\n\n>>> prompt = \"Spiderman is surfing\"\n>>> video_frames = pipe(prompt).frames\n>>> video_path = export_to_video(video_frames)\n>>> video_path\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet3DConditionModel scheduler: KarrasDiffusionSchedulers )\n```", "```py\n( prompt: Union = None video: Union = None strength: float = 0.6 num_inference_steps: int = 50 guidance_scale: float = 15.0 negative_prompt: Union = None eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'np' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) \u2192 export const metadata = 'undefined';TextToVideoSDPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n>>> from diffusers.utils import export_to_video\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\n>>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n>>> pipe.to(\"cuda\")\n\n>>> prompt = \"spiderman running in the desert\"\n>>> video_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\n>>> # safe low-res video\n>>> video_path = export_to_video(video_frames, output_video_path=\"./video_576_spiderman.mp4\")\n\n>>> # let's offload the text-to-image model\n>>> pipe.to(\"cpu\")\n\n>>> # and load the image-to-image model\n>>> pipe = DiffusionPipeline.from_pretrained(\n...     \"cerspense/zeroscope_v2_XL\", torch_dtype=torch.float16, revision=\"refs/pr/15\"\n... )\n>>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n>>> pipe.enable_model_cpu_offload()\n\n>>> # The VAE consumes A LOT of memory, let's make sure we run it in sliced mode\n>>> pipe.vae.enable_slicing()\n\n>>> # now let's upscale it\n>>> video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]\n\n>>> # and denoise it\n>>> video_frames = pipe(prompt, video=video, strength=0.6).frames\n>>> video_path = export_to_video(video_frames, output_video_path=\"./video_1024_spiderman.mp4\")\n>>> video_path\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( frames: Union )\n```"]