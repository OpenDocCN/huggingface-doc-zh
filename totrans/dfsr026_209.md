# VQDiffusionScheduler

> 原始文本：[https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion](https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion)

`VQDiffusionScheduler` 将变压器模型的输出转换为前一个扩散时间步的未加噪图像样本。由 Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo 在 [用于文本到图像合成的矢量量化扩散模型](https://huggingface.co/papers/2111.14822) 中引入。

论文摘要如下：

*我们提出了用于文本到图像生成的矢量量化扩散（VQ-Diffusion）模型。该方法基于矢量量化变分自动编码器（VQ-VAE），其潜空间由最近开发的去噪扩散概率模型（DDPM）的条件变体建模。我们发现，这种潜空间方法非常适用于文本到图像生成任务，因为它不仅消除了现有方法的单向偏差，还允许我们结合掩码替换扩散策略，以避免错误的积累，这是现有方法的一个严重问题。我们的实验表明，与具有相似参数数量的传统自回归（AR）模型相比，VQ-Diffusion在文本到图像生成结果方面表现显著更好。与先前基于GAN的文本到图像方法相比，我们的VQ-Diffusion可以处理更复杂的场景，并显著提高合成图像的质量。最后，我们展示了我们的方法中的图像生成计算可以通过重新参数化实现高效。对于传统的AR方法，文本到图像生成时间随着输出图像分辨率的增加呈线性增长，因此即使对于普通大小的图像也是非常耗时的。VQ-Diffusion使我们能够在质量和速度之间取得更好的平衡。我们的实验表明，具有重新参数化的VQ-Diffusion模型比传统的AR方法快15倍，同时实现更好的图像质量。*

## VQDiffusionScheduler

### `class diffusers.VQDiffusionScheduler`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L106)

```py
( num_vec_classes: int num_train_timesteps: int = 100 alpha_cum_start: float = 0.99999 alpha_cum_end: float = 9e-06 gamma_cum_start: float = 9e-06 gamma_cum_end: float = 0.99999 )
```

参数

+   `num_vec_classes` (`int`) — 隐像素向量嵌入的类数。包括掩码隐像素的类。

+   `num_train_timesteps` (`int`，默认为 100）— 训练模型的扩散步数。

+   `alpha_cum_start` (`float`，默认为 0.99999）— 开始累积 alpha 值。

+   `alpha_cum_end` (`float`，默认为 0.00009）— 结束累积 alpha 值。

+   `gamma_cum_start` (`float`，默认为 0.00009）— 开始累积 gamma 值。

+   `gamma_cum_end` (`float`，默认为 0.99999）— 结束累积 gamma 值。

用于矢量量化扩散的调度器。

该模型继承自 [SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin) 和 [ConfigMixin](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin)。查看超类文档以了解库为所有调度器实现的通用方法，如加载和保存。

#### `log_Q_t_transitioning_to_known_class`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L356)

```py
( t: torch.int32 x_t: LongTensor log_onehot_x_t: FloatTensor cumulative: bool ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch size, num classes - 1, num latent pixels)
```

参数

+   `t` (`torch.Long`) — 确定使用哪个转移矩阵的时间步。

+   `x_t` (`torch.LongTensor` of shape `(batch size, num latent pixels)`) — 时间 `t` 时每个隐像素的类。

+   `log_onehot_x_t` (`torch.FloatTensor` of shape `(batch size, num classes, num latent pixels)`) — `x_t` 的对数独热向量。

+   `cumulative` (`bool`) — 如果 cumulative 为 `False`，则使用单步转移矩阵 `t-1`->`t`。如果 cumulative 为 `True`，则使用累积转移矩阵 `0`->`t`。

返回值

形状为`(batch size, num classes - 1, num latent pixels)`的`torch.FloatTensor`

返回的矩阵的每一列是完整概率转移矩阵的对数概率行。

当非累积时，返回`self.num_classes - 1`行，因为初始潜在像素不能被遮蔽。

其中：

+   `q_n`是第`n`个潜在像素正向过程的概率分布。

+   C_0是潜在像素嵌入的一个类别

+   C_k是被遮蔽的潜在像素的类别

非累积结果（省略对数）：

`q_0(x_t | x_{t-1} = C_0) ... q_n(x_t | x_{t-1} = C_0) . . . . . . . . . q_0(x_t | x_{t-1} = C_k) ... q_n(x_t | x_{t-1} = C_k)`

累积结果（省略对数）：

`q_0_cumulative(x_t | x_0 = C_0) ... q_n_cumulative(x_t | x_0 = C_0) . . . . . . . . . q_0_cumulative(x_t | x_0 = C_{k-1}) ... q_n_cumulative(x_t | x_0 = C_{k-1})`

计算`x_t`中每个潜在像素的（累积或非累积）转移矩阵的对数概率。

#### `q_posterior`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L245)

```py
( log_p_x_0 x_t t ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch size, num classes, num latent pixels)
```

参数

+   `log_p_x_0`（形状为`(batch size, num classes - 1, num latent pixels)`的`torch.FloatTensor`）— 初始潜在像素预测类别的对数概率。不包括被遮蔽类别的预测，因为初始未加噪声的图像不能被遮蔽。

+   `x_t`（形状为`(batch size, num latent pixels)`的`torch.LongTensor`）— 每个潜在像素在时间`t`的类别。

+   `t`（`torch.Long`）— 决定使用哪个转移矩阵的时间步。

返回

形状为`(batch size, num classes, num latent pixels)`的`torch.FloatTensor`

时间步`t-1`时图像预测类别的对数概率。

计算时间步`t-1`时图像预测类别的对数概率：

```py
p(x_{t-1} | x_t) = sum( q(x_t | x_{t-1}) * q(x_{t-1} | x_0) * p(x_0) / q(x_t | x_0) )
```

#### `set_timesteps`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L178)

```py
( num_inference_steps: int device: Union = None )
```

参数

+   `num_inference_steps` (`int`) — 在使用预训练模型生成样本时使用的扩散步数。

+   `device` (`str`或`torch.device`，*可选*) — 将时间步和扩散过程参数（alpha、beta、gamma）移动到的设备。

设置用于扩散链的离散时间步（在推断之前运行）。

#### `step`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L200)

```py
( model_output: FloatTensor timestep: torch.int64 sample: LongTensor generator: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';VQDiffusionSchedulerOutput or tuple
```

参数

+   `t` (`torch.long`) — 决定使用哪些转移矩阵的时间步。

+   `x_t` (`torch.LongTensor`，形状为`(batch size, num latent pixels)`) — 时间`t`时每个潜在像素的类。

+   `generator` (`torch.Generator`或`None`) — 用于在从中采样之前应用于`p(x_{t-1} | x_t)`的噪声的随机数生成器。

+   `return_dict` (`bool`，*可选*，默认为`True`) — 是否返回[VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)或`tuple`。

返回

[VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)或`tuple`

如果`return_dict`为`True`，则返回[VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)，否则返回一个元组，其中第一个元素是样本张量。

通过逆转移分布预测上一个时间步的样本。有关计算分布的更多详细信息，请参阅[q_posterior()](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.VQDiffusionScheduler.q_posterior)。

## VQDiffusionSchedulerOutput

### `class diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L27)

```py
( prev_sample: LongTensor )
```

参数

+   `prev_sample` (`torch.LongTensor`，形状为`(batch size, num latent pixels)`) — 计算上一个时间步的先前样本 x_{t-1}。`prev_sample`应该在去噪循环中作为下一个模型输入使用。

调度程序的步函数输出的输出类。
