["```py\nfrom tokenizers import Tokenizer\ntokenizer = Tokenizer.from_file(\"data/tokenizer-wiki.json\")\n```", "```py\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFD, StripAccents\nnormalizer = normalizers.Sequence([NFD(), StripAccents()])\n```", "```py\nnormalizer.normalize_str(\"H\u00e9ll\u00f2 h\u00f4w are \u00fc?\")\n# \"Hello how are u?\"\n```", "```py\ntokenizer.normalizer = normalizer\n```", "```py\nfrom tokenizers.pre_tokenizers import Whitespace\npre_tokenizer = Whitespace()\npre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")\n# [(\"Hello\", (0, 5)), (\"!\", (5, 6)), (\"How\", (7, 10)), (\"are\", (11, 14)), (\"you\", (15, 18)),\n#  (\"?\", (18, 19)), (\"I\", (20, 21)), (\"'\", (21, 22)), ('m', (22, 23)), (\"fine\", (24, 28)),\n#  (\",\", (28, 29)), (\"thank\", (30, 35)), (\"you\", (36, 39)), (\".\", (39, 40))]\n```", "```py\nfrom tokenizers import pre_tokenizers\nfrom tokenizers.pre_tokenizers import Digits\npre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\npre_tokenizer.pre_tokenize_str(\"Call 911!\")\n# [(\"Call\", (0, 4)), (\"9\", (5, 6)), (\"1\", (6, 7)), (\"1\", (7, 8)), (\"!\", (8, 9))]\n```", "```py\ntokenizer.pre_tokenizer = pre_tokenizer\n```", "```py\nfrom tokenizers.processors import TemplateProcessing\ntokenizer.post_processor = TemplateProcessing(\n    single=\"[CLS] $A [SEP]\",\n    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n)\n```", "```py\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nbert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n```", "```py\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFD, Lowercase, StripAccents\nbert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n```", "```py\nfrom tokenizers.pre_tokenizers import Whitespace\nbert_tokenizer.pre_tokenizer = Whitespace()\n```", "```py\nfrom tokenizers.processors import TemplateProcessing\nbert_tokenizer.post_processor = TemplateProcessing(\n    single=\"[CLS] $A [SEP]\",\n    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n    special_tokens=[\n        (\"[CLS]\", 1),\n        (\"[SEP]\", 2),\n    ],\n)\n```", "```py\nfrom tokenizers.trainers import WordPieceTrainer\ntrainer = WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\nfiles = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\nbert_tokenizer.train(files, trainer)\nbert_tokenizer.save(\"data/bert-wiki.json\")\n```", "```py\noutput = tokenizer.encode(\"Hello, y'all! How are you \ud83d\ude01 ?\")\nprint(output.ids)\n# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\ntokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])\n# \"Hello , y ' all ! How are you ?\"\n```", "```py\noutput = bert_tokenizer.encode(\"Welcome to the \ud83e\udd17 Tokenizers library.\")\nprint(output.tokens)\n# [\"[CLS]\", \"welcome\", \"to\", \"the\", \"[UNK]\", \"tok\", \"##eni\", \"##zer\", \"##s\", \"library\", \".\", \"[SEP]\"]\nbert_tokenizer.decode(output.ids)\n# \"welcome to the tok ##eni ##zer ##s library .\"\n```", "```py\nfrom tokenizers import decoders\nbert_tokenizer.decoder = decoders.WordPiece()\nbert_tokenizer.decode(output.ids)\n# \"welcome to the tokenizers library.\"\n```"]