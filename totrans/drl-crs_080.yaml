- en: The Problem of Variance in Reinforce
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Reinforce中的方差问题
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/variance-problem](https://huggingface.co/learn/deep-rl-course/unit6/variance-problem)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unit6/variance-problem](https://huggingface.co/learn/deep-rl-course/unit6/variance-problem)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In Reinforce, we want to **increase the probability of actions in a trajectory
    proportionally to how high the return is**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在Reinforce中，我们希望**根据回报的高低比例增加轨迹中动作的概率**。
- en: '![Reinforce](../Images/4bc5f3236faaea99c30f1a063052a9bf.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Reinforce](../Images/4bc5f3236faaea99c30f1a063052a9bf.png)'
- en: If the **return is high**, we will **push up** the probabilities of the (state,
    action) combinations.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**回报很高**，我们将**增加**（状态，动作）组合的概率。
- en: Otherwise, if the **return is low**, it will **push down** the probabilities
    of the (state, action) combinations.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，如果**回报很低**，它将**降低**（状态，动作）组合的概率。
- en: This return<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)
    is calculated using a *Monte-Carlo sampling*. We collect a trajectory and calculate
    the discounted return, **and use this score to increase or decrease the probability
    of every action taken in that trajectory**. If the return is good, all actions
    will be “reinforced” by increasing their likelihood of being taken. <math><semantics><mrow><mi>R</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mi
    mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation
    encoding="application/x-tex">R(\tau) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}
    + ...</annotation></semantics></math>R(τ)=Rt+1​+γRt+2​+γ2Rt+3​+...
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回报<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)是使用*蒙特卡罗采样*计算的。我们收集一个轨迹并计算折扣回报，**并使用这个分数来增加或减少在该轨迹中采取的每个动作的概率**。如果回报很好，所有动作都将通过增加它们被采取的可能性来“强化”。
- en: The advantage of this method is that **it’s unbiased. Since we’re not estimating
    the return**, we use only the true return we obtain.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是**它是无偏的。因为我们不是在估计回报**，我们只使用我们获得的真实回报。
- en: Given the stochasticity of the environment (random events during an episode)
    and stochasticity of the policy, **trajectories can lead to different returns,
    which can lead to high variance**. Consequently, the same starting state can lead
    to very different returns. Because of this, **the return starting at the same
    state can vary significantly across episodes**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到环境的随机性（在一个episode中的随机事件）和策略的随机性，**轨迹可能导致不同的回报，这可能导致高方差**。因此，相同的起始状态可能导致非常不同的回报。由于这个原因，**从相同状态开始的回报在不同episode中可能会有显著变化**。
- en: '![variance](../Images/de1a4c50c54146f39a2e910201757121.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![方差](../Images/de1a4c50c54146f39a2e910201757121.png)'
- en: The solution is to mitigate the variance by **using a large number of trajectories,
    hoping that the variance introduced in any one trajectory will be reduced in aggregate
    and provide a “true” estimation of the return.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是通过**使用大量轨迹来减少方差，希望任何一个轨迹引入的方差在总体中会减少，并提供“真实”的回报估计**。
- en: However, increasing the batch size significantly **reduces sample efficiency**.
    So we need to find additional mechanisms to reduce the variance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，显著增加批量大小**会降低样本效率**。因此，我们需要找到额外的机制来减少方差。
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'If you want to dive deeper into the question of variance and bias tradeoff
    in Deep Reinforcement Learning, you can check out these two articles:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想深入探讨深度强化学习中的方差和偏差权衡问题，可以查看这两篇文章：
- en: '[Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning](https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度]强化学习中的偏差/方差权衡的意义'
- en: '[Bias-variance Tradeoff in Reinforcement Learning](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在强化学习中的偏差-方差权衡
- en: '* * *'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
