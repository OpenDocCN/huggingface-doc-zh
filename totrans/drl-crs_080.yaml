- en: The Problem of Variance in Reinforce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit6/variance-problem](https://huggingface.co/learn/deep-rl-course/unit6/variance-problem)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/70.d4ed223e.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: In Reinforce, we want to **increase the probability of actions in a trajectory
    proportionally to how high the return is**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforce](../Images/4bc5f3236faaea99c30f1a063052a9bf.png)'
  prefs: []
  type: TYPE_IMG
- en: If the **return is high**, we will **push up** the probabilities of the (state,
    action) combinations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, if the **return is low**, it will **push down** the probabilities
    of the (state, action) combinations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This return<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)
    is calculated using a *Monte-Carlo sampling*. We collect a trajectory and calculate
    the discounted return, **and use this score to increase or decrease the probability
    of every action taken in that trajectory**. If the return is good, all actions
    will be “reinforced” by increasing their likelihood of being taken. <math><semantics><mrow><mi>R</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mi
    mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation
    encoding="application/x-tex">R(\tau) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}
    + ...</annotation></semantics></math>R(τ)=Rt+1​+γRt+2​+γ2Rt+3​+...
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this method is that **it’s unbiased. Since we’re not estimating
    the return**, we use only the true return we obtain.
  prefs: []
  type: TYPE_NORMAL
- en: Given the stochasticity of the environment (random events during an episode)
    and stochasticity of the policy, **trajectories can lead to different returns,
    which can lead to high variance**. Consequently, the same starting state can lead
    to very different returns. Because of this, **the return starting at the same
    state can vary significantly across episodes**.
  prefs: []
  type: TYPE_NORMAL
- en: '![variance](../Images/de1a4c50c54146f39a2e910201757121.png)'
  prefs: []
  type: TYPE_IMG
- en: The solution is to mitigate the variance by **using a large number of trajectories,
    hoping that the variance introduced in any one trajectory will be reduced in aggregate
    and provide a “true” estimation of the return.**
  prefs: []
  type: TYPE_NORMAL
- en: However, increasing the batch size significantly **reduces sample efficiency**.
    So we need to find additional mechanisms to reduce the variance.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to dive deeper into the question of variance and bias tradeoff
    in Deep Reinforcement Learning, you can check out these two articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning](https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bias-variance Tradeoff in Reinforcement Learning](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
