- en: Hyperparameter Search using Trainer API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Trainer API进行超参数搜索
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train](https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train](https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Transformers provides a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class optimized for training 🤗 Transformers models, making it easier to start
    training without manually writing your own training loop. The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    provides API for hyperparameter search. This doc shows how to enable it in example.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers提供了一个专为训练🤗 Transformers模型优化的[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，使得更容易开始训练而无需手动编写自己的训练循环。[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)提供了用于超参数搜索的API。本文档展示了如何在示例中启用它。
- en: Hyperparameter Search backend
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数搜索后端
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    supports four hyperparameter search backends currently: [optuna](https://optuna.org/),
    [sigopt](https://sigopt.com/), [raytune](https://docs.ray.io/en/latest/tune/index.html)
    and [wandb](https://wandb.ai/site/sweeps).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)目前支持四种超参数搜索后端：[optuna](https://optuna.org/)、[sigopt](https://sigopt.com/)、[raytune](https://docs.ray.io/en/latest/tune/index.html)和[wandb](https://wandb.ai/site/sweeps)。'
- en: you should install them before using them as the hyperparameter search backend
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用超参数搜索后端之前，您应该先安装它们
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to enable Hyperparameter search in example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在示例中启用超参数搜索
- en: Define the hyperparameter search space, different backends need different format.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 定义超参数搜索空间，不同的后端需要不同的格式。
- en: 'For sigopt, see sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter),
    it’s like following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于sigopt，请参阅sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter)，就像下面这样：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For optuna, see optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py),
    it’s like following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于optuna，请参阅optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)，就像下面这样：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Optuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search`
    and define your own compute_objective to return multiple objective values. The
    Pareto Front (`List[BestRun]`) will be returned in hyperparameter_search, you
    should refer to the test case `TrainerHyperParameterMultiObjectOptunaIntegrationTest`
    in [test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py).
    It’s like following
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna提供多目标HPO。您可以在`hyperparameter_search`中传递`direction`并定义自己的`compute_objective`来返回多个目标值。
    Pareto前沿（`List[BestRun]`）将在`hyperparameter_search`中返回，您应该参考[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py)中的测试用例`TrainerHyperParameterMultiObjectOptunaIntegrationTest`。就像下面这样
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For raytune, see raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html),
    it’s like following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于raytune，请参阅raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html)，就像下面这样：
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For wandb, see wandb [object_parameter](https://docs.wandb.ai/guides/sweeps/configuration),
    it’s like following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于wandb，请参阅wandb [object_parameter](https://docs.wandb.ai/guides/sweeps/configuration)，就像下面这样：
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define a `model_init` function and pass it to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    as an example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个`model_init`函数并将其传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，例如：
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    with your `model_init` function, training arguments, training and test datasets,
    and evaluation function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您的`model_init`函数、训练参数、训练和测试数据集以及评估函数创建一个[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)：
- en: '[PRE7]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Call hyperparameter search, get the best trial parameters, backend could be
    `"optuna"`/`"sigopt"`/`"wandb"`/`"ray"`. direction can be`"minimize"` or `"maximize"`,
    which indicates whether to optimize greater or lower objective.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 调用超参数搜索，获取最佳试验参数，后端可以是`"optuna"`/`"sigopt"`/`"wandb"`/`"ray"`。方向可以是`"minimize"`或`"maximize"`，表示是优化更大还是更小的目标。
- en: You could define your own compute_objective function, if not defined, the default
    compute_objective will be called, and the sum of eval metric like f1 is returned
    as objective value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以定义自己的`compute_objective`函数，如果未定义，将调用默认的`compute_objective`，并将类似f1的评估指标的总和作为目标值返回。
- en: '[PRE8]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Hyperparameter search For DDP finetune
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDP微调的超参数搜索
- en: Currently, Hyperparameter search for DDP is enabled for optuna and sigopt. Only
    the rank-zero process will generate the search trial and pass the argument to
    other ranks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，optuna和sigopt已启用DDP的超参数搜索。只有排名为零的进程才会生成搜索试验并将参数传递给其他排名。
