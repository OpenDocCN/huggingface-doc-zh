- en: Hyperparameter Search using Trainer API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Trainer APIè¿›è¡Œè¶…å‚æ•°æœç´¢
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train](https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train](https://huggingface.co/docs/transformers/v4.37.2/en/hpo_train)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Transformers provides a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class optimized for training ğŸ¤— Transformers models, making it easier to start
    training without manually writing your own training loop. The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    provides API for hyperparameter search. This doc shows how to enable it in example.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersæä¾›äº†ä¸€ä¸ªä¸“ä¸ºè®­ç»ƒğŸ¤— Transformersæ¨¡å‹ä¼˜åŒ–çš„[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç±»ï¼Œä½¿å¾—æ›´å®¹æ˜“å¼€å§‹è®­ç»ƒè€Œæ— éœ€æ‰‹åŠ¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯ã€‚[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æä¾›äº†ç”¨äºè¶…å‚æ•°æœç´¢çš„APIã€‚æœ¬æ–‡æ¡£å±•ç¤ºäº†å¦‚ä½•åœ¨ç¤ºä¾‹ä¸­å¯ç”¨å®ƒã€‚
- en: Hyperparameter Search backend
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°æœç´¢åç«¯
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    supports four hyperparameter search backends currently: [optuna](https://optuna.org/),
    [sigopt](https://sigopt.com/), [raytune](https://docs.ray.io/en/latest/tune/index.html)
    and [wandb](https://wandb.ai/site/sweeps).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç›®å‰æ”¯æŒå››ç§è¶…å‚æ•°æœç´¢åç«¯ï¼š[optuna](https://optuna.org/)ã€[sigopt](https://sigopt.com/)ã€[raytune](https://docs.ray.io/en/latest/tune/index.html)å’Œ[wandb](https://wandb.ai/site/sweeps)ã€‚'
- en: you should install them before using them as the hyperparameter search backend
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨è¶…å‚æ•°æœç´¢åç«¯ä¹‹å‰ï¼Œæ‚¨åº”è¯¥å…ˆå®‰è£…å®ƒä»¬
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to enable Hyperparameter search in example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ç¤ºä¾‹ä¸­å¯ç”¨è¶…å‚æ•°æœç´¢
- en: Define the hyperparameter search space, different backends need different format.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ï¼Œä¸åŒçš„åç«¯éœ€è¦ä¸åŒçš„æ ¼å¼ã€‚
- en: 'For sigopt, see sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter),
    itâ€™s like following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºsigoptï¼Œè¯·å‚é˜…sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter)ï¼Œå°±åƒä¸‹é¢è¿™æ ·ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For optuna, see optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py),
    itâ€™s like following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºoptunaï¼Œè¯·å‚é˜…optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)ï¼Œå°±åƒä¸‹é¢è¿™æ ·ï¼š
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Optuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search`
    and define your own compute_objective to return multiple objective values. The
    Pareto Front (`List[BestRun]`) will be returned in hyperparameter_search, you
    should refer to the test case `TrainerHyperParameterMultiObjectOptunaIntegrationTest`
    in [test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py).
    Itâ€™s like following
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Optunaæä¾›å¤šç›®æ ‡HPOã€‚æ‚¨å¯ä»¥åœ¨`hyperparameter_search`ä¸­ä¼ é€’`direction`å¹¶å®šä¹‰è‡ªå·±çš„`compute_objective`æ¥è¿”å›å¤šä¸ªç›®æ ‡å€¼ã€‚
    Paretoå‰æ²¿ï¼ˆ`List[BestRun]`ï¼‰å°†åœ¨`hyperparameter_search`ä¸­è¿”å›ï¼Œæ‚¨åº”è¯¥å‚è€ƒ[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py)ä¸­çš„æµ‹è¯•ç”¨ä¾‹`TrainerHyperParameterMultiObjectOptunaIntegrationTest`ã€‚å°±åƒä¸‹é¢è¿™æ ·
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For raytune, see raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html),
    itâ€™s like following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºraytuneï¼Œè¯·å‚é˜…raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html)ï¼Œå°±åƒä¸‹é¢è¿™æ ·ï¼š
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For wandb, see wandb [object_parameter](https://docs.wandb.ai/guides/sweeps/configuration),
    itâ€™s like following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºwandbï¼Œè¯·å‚é˜…wandb [object_parameter](https://docs.wandb.ai/guides/sweeps/configuration)ï¼Œå°±åƒä¸‹é¢è¿™æ ·ï¼š
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define a `model_init` function and pass it to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    as an example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ª`model_init`å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    with your `model_init` function, training arguments, training and test datasets,
    and evaluation function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨çš„`model_init`å‡½æ•°ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°åˆ›å»ºä¸€ä¸ª[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼š
- en: '[PRE7]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Call hyperparameter search, get the best trial parameters, backend could be
    `"optuna"`/`"sigopt"`/`"wandb"`/`"ray"`. direction can be`"minimize"` or `"maximize"`,
    which indicates whether to optimize greater or lower objective.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨è¶…å‚æ•°æœç´¢ï¼Œè·å–æœ€ä½³è¯•éªŒå‚æ•°ï¼Œåç«¯å¯ä»¥æ˜¯`"optuna"`/`"sigopt"`/`"wandb"`/`"ray"`ã€‚æ–¹å‘å¯ä»¥æ˜¯`"minimize"`æˆ–`"maximize"`ï¼Œè¡¨ç¤ºæ˜¯ä¼˜åŒ–æ›´å¤§è¿˜æ˜¯æ›´å°çš„ç›®æ ‡ã€‚
- en: You could define your own compute_objective function, if not defined, the default
    compute_objective will be called, and the sum of eval metric like f1 is returned
    as objective value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å®šä¹‰è‡ªå·±çš„`compute_objective`å‡½æ•°ï¼Œå¦‚æœæœªå®šä¹‰ï¼Œå°†è°ƒç”¨é»˜è®¤çš„`compute_objective`ï¼Œå¹¶å°†ç±»ä¼¼f1çš„è¯„ä¼°æŒ‡æ ‡çš„æ€»å’Œä½œä¸ºç›®æ ‡å€¼è¿”å›ã€‚
- en: '[PRE8]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Hyperparameter search For DDP finetune
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPå¾®è°ƒçš„è¶…å‚æ•°æœç´¢
- en: Currently, Hyperparameter search for DDP is enabled for optuna and sigopt. Only
    the rank-zero process will generate the search trial and pass the argument to
    other ranks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œoptunaå’Œsigoptå·²å¯ç”¨DDPçš„è¶…å‚æ•°æœç´¢ã€‚åªæœ‰æ’åä¸ºé›¶çš„è¿›ç¨‹æ‰ä¼šç”Ÿæˆæœç´¢è¯•éªŒå¹¶å°†å‚æ•°ä¼ é€’ç»™å…¶ä»–æ’åã€‚
