- en: Philosophy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/conceptual/philosophy](https://huggingface.co/docs/diffusers/conceptual/philosophy)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across
    multiple modalities. Its purpose is to serve as a **modular toolbox** for both
    inference and training.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We aim at building a library that stands the test of time and therefore take
    API design very seriously.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore,
    most of our design choices are based on [PyTorch’s Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy).
    Let’s go over the most important ones:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Usability over Performance
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Diffusers has many built-in performance-enhancing features (see [Memory
    and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are
    always loaded with the highest precision and lowest optimization. Therefore, by
    default diffusion pipelines are always instantiated on CPU with float32 precision
    if not otherwise defined by the user. This ensures usability across different
    platforms and accelerators and means that no complex installations are required
    to run the library.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusers aims to be a **light-weight** package and therefore has very few required
    dependencies, but many soft dependencies that can improve performance (such as
    `accelerate`, `safetensors`, `onnx`, etc…). We strive to keep the library as lightweight
    as possible so that it can be added without much concern as a dependency on other
    packages.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusers prefers simple, self-explainable code over condensed, magic code.
    This means that short-hand code syntaxes such as lambda functions, and advanced
    PyTorch operators are often not desired.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple over easy
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As PyTorch states, **explicit is better than implicit** and **simple is better
    than complex**. This design philosophy is reflected in multiple parts of the library:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We follow PyTorch’s API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to)
    to let the user handle device management.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raising concise error messages is preferred to silently correct erroneous input.
    Diffusers aims at teaching the user, rather than making the library as easy to
    use as possible.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex model vs. scheduler logic is exposed instead of magically handled inside.
    Schedulers/Samplers are separated from diffusion models with minimal dependencies
    on each other. This forces the user to write the unrolled denoising loop. However,
    the separation allows for easier debugging and gives the user more control over
    adapting the denoising process or switching out diffusion models or schedulers.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separately trained components of the diffusion pipeline, *e.g.* the text encoder,
    the unet, and the variational autoencoder, each have their own model class. This
    forces the user to handle the interaction between the different model components,
    and the serialization format separates the model components into different files.
    However, this allows for easier debugging and customization. DreamBooth or Textual
    Inversion training is very simple thanks to Diffusers’ ability to separate single
    components of the diffusion pipeline.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweakable, contributor-friendly over abstraction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For large parts of the library, Diffusers adopts an important design principle
    of the [Transformers library](https://github.com/huggingface/transformers), which
    is to prefer copy-pasted code over hasty abstractions. This design principle is
    very opinionated and stands in stark contrast to popular design principles such
    as [Don’t repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).
    In short, just like Transformers does for modeling files, Diffusers prefers to
    keep an extremely low level of abstraction and very self-contained code for pipelines
    and schedulers. Functions, long code blocks, and even classes can be copied across
    multiple files which at first can look like a bad, sloppy design choice that makes
    the library unmaintainable. **However**, this design has proven to be extremely
    successful for Transformers and makes a lot of sense for community-driven, open-source
    machine learning libraries because:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图书馆的大部分部分，Diffusers采用了[Transformers库](https://github.com/huggingface/transformers)的一个重要设计原则，即更喜欢复制粘贴的代码而不是仓促的抽象。这个设计原则非常有主见，与流行的设计原则如[不要重复自己（DRY）](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)形成鲜明对比。简而言之，就像Transformers为建模文件所做的那样，Diffusers更喜欢保持极低的抽象水平和非常自包含的管道和调度器代码。函数、长代码块，甚至类可以在多个文件之间复制，起初可能看起来像是一个糟糕、懒散的设计选择，使得图书馆难以维护。**然而**，这种设计已经被证明对Transformers非常成功，并且对于社区驱动的开源机器学习库来说是非常合理的，因为：
- en: Machine Learning is an extremely fast-moving field in which paradigms, model
    architectures, and algorithms are changing rapidly, which therefore makes it very
    difficult to define long-lasting code abstractions.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是一个快速发展的领域，范式、模型架构和算法都在迅速变化，因此很难定义持久的代码抽象。
- en: Machine Learning practitioners like to be able to quickly tweak existing code
    for ideation and research and therefore prefer self-contained code over one that
    contains many abstractions.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习从业者喜欢能够快速调整现有代码进行构思和研究，因此更喜欢自包含的代码而不是包含许多抽象的代码。
- en: Open-source libraries rely on community contributions and therefore must build
    a library that is easy to contribute to. The more abstract the code, the more
    dependencies, the harder to read, and the harder to contribute to. Contributors
    simply stop contributing to very abstract libraries out of fear of breaking vital
    functionality. If contributing to a library cannot break other fundamental code,
    not only is it more inviting for potential new contributors, but it is also easier
    to review and contribute to multiple parts in parallel.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源库依赖于社区贡献，因此必须构建一个易于贡献的库。代码越抽象，依赖越多，阅读越困难，贡献也越困难。贡献者简单地停止对非常抽象的库进行贡献，因为他们担心破坏重要功能。如果对库的贡献不会破坏其他基本代码，不仅对潜在的新贡献者更具吸引力，而且更容易审查和同时贡献多个部分。
- en: At Hugging Face, we call this design the **single-file policy** which means
    that almost all of the code of a certain class should be written in a single,
    self-contained file. To read more about the philosophy, you can have a look at
    [this blog post](https://huggingface.co/blog/transformers-design-philosophy).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face，我们称这种设计为**单文件政策**，这意味着某个类的几乎所有代码应该写在一个单独的自包含文件中。要了解更多关于这种哲学的信息，您可以查看[这篇博客文章](https://huggingface.co/blog/transformers-design-philosophy)。
- en: In Diffusers, we follow this philosophy for both pipelines and schedulers, but
    only partly for diffusion models. The reason we don’t follow this design fully
    for diffusion models is because almost all diffusion pipelines, such as [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm),
    [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines),
    [unCLIP (DALL·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
    and [Imagen](https://imagen.research.google/) all rely on the same diffusion model,
    the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Diffusers中，我们对管道和调度器都遵循这一哲学，但对扩散模型只部分遵循。我们不完全遵循这种设计的原因是因为几乎所有扩散管道，如[DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm)、[稳定扩散](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines)、[unCLIP（DALL·E
    2）](https://huggingface.co/docs/diffusers/api/pipelines/unclip)和[Imagen](https://imagen.research.google/)都依赖于相同的扩散模型，即[UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond)。
- en: Great, now you should have generally understood why 🧨 Diffusers is designed
    the way it is 🤗. We try to apply these design principles consistently across the
    library. Nevertheless, there are some minor exceptions to the philosophy or some
    unlucky design choices. If you have feedback regarding the design, we would ❤️
    to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在你应该大致了解为什么🧨 Diffusers被设计成这样了🤗。我们试图在整个库中一致应用这些设计原则。然而，也有一些对这一理念的小例外或一些不幸的设计选择。如果您对设计有反馈，我们会❤️直接在GitHub上听取意见（https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=）。
- en: Design Philosophy in Details
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 详细的设计哲学
- en: 'Now, let’s look a bit into the nitty-gritty details of the design philosophy.
    Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines),
    [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models),
    and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).
    Let’s walk through more in-detail design decisions for each class.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们稍微深入了解设计哲学的细节。Diffusers基本上由三个主要类组成：[管道](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines)、[模型](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)和[调度器](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)。让我们更详细地讨论每个类的设计决策。
- en: Pipelines
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道
- en: Pipelines are designed to be easy to use (therefore do not follow [*Simple over
    easy*](#simple-over-easy) 100%), are not feature complete, and should loosely
    be seen as examples of how to use [models](#models) and [schedulers](#schedulers)
    for inference.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 管道被设计为易于使用（因此不完全遵循[*简单胜过容易*](#简单胜过容易) 100%），不是功能完备的，并且应该宽松地被视为如何使用[模型](#模型)和[调度器](#调度器)进行推断的示例。
- en: 'The following design principles are followed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以下设计原则：
- en: Pipelines follow the single-file policy. All pipelines can be found in individual
    directories under src/diffusers/pipelines. One pipeline folder corresponds to
    one diffusion paper/project/release. Multiple pipeline files can be gathered in
    one pipeline folder, as it’s done for [`src/diffusers/pipelines/stable-diffusion`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion).
    If pipelines share similar functionality, one can make use of the [#Copied from
    mechanism](https://github.com/huggingface/diffusers/blob/125d783076e5bd9785beb05367a2d2566843a271/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L251).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道遵循单文件策略。所有管道都可以在src/diffusers/pipelines的各个目录下找到。一个管道文件夹对应一个扩散论文/项目/发布。多个管道文件可以收集在一个管道文件夹中，就像对于[`src/diffusers/pipelines/stable-diffusion`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion)所做的那样。如果管道共享类似功能，可以利用[#从机制复制](https://github.com/huggingface/diffusers/blob/125d783076e5bd9785beb05367a2d2566843a271/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L251)。
- en: Pipelines all inherit from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有管道都继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。
- en: Every pipeline consists of different model and scheduler components, that are
    documented in the [`model_index.json` file](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json),
    are accessible under the same name as attributes of the pipeline and can be shared
    between pipelines with [`DiffusionPipeline.components`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.components)
    function.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个管道由不同的模型和调度器组件组成，这些组件在[`model_index.json`文件](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json)中有文档记录，可以在管道的属性下以相同的名称访问，并且可以通过[`DiffusionPipeline.components`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.components)函数在管道之间共享。
- en: Every pipeline should be loadable via the [`DiffusionPipeline.from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained)
    function.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个管道应该可以通过[`DiffusionPipeline.from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained)函数加载。
- en: Pipelines should be used **only** for inference.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道应该**仅**用于推断。
- en: Pipelines should be very readable, self-explanatory, and easy to tweak.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道应该非常易读，自解释，并且易于调整。
- en: Pipelines should be designed to build on top of each other and be easy to integrate
    into higher-level APIs.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道应该被设计为相互构建，并且易于集成到更高级别的API中。
- en: Pipelines are **not** intended to be feature-complete user interfaces. For future
    complete user interfaces one should rather have a look at [InvokeAI](https://github.com/invoke-ai/InvokeAI),
    [Diffuzers](https://github.com/abhishekkrthakur/diffuzers), and [lama-cleaner](https://github.com/Sanster/lama-cleaner).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道**不**旨在成为功能完备的用户界面。对于未来完整的用户界面，应该更多地查看[InvokeAI](https://github.com/invoke-ai/InvokeAI)，[Diffuzers](https://github.com/abhishekkrthakur/diffuzers)和[lama-cleaner](https://github.com/Sanster/lama-cleaner)。
- en: Every pipeline should have one and only one way to run it via a `__call__` method.
    The naming of the `__call__` arguments should be shared across all pipelines.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个管道应该只有一种通过`__call__`方法运行的方式。`__call__`参数的命名应该在所有管道中共享。
- en: Pipelines should be named after the task they are intended to solve.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道应该以其所打算解决的任务命名。
- en: In almost all cases, novel diffusion pipelines shall be implemented in a new
    pipeline folder/file.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在几乎所有情况下，新颖的扩散管道应该在新的管道文件夹/文件中实现。
- en: Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: Models are designed as configurable toolboxes that are natural extensions of
    [PyTorch’s Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).
    They only partly follow the **single-file policy**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型被设计为可配置的工具箱，是[PyTorch的Module类](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)的自然扩展。它们只部分遵循**单文件策略**。
- en: 'The following design principles are followed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以下设计原则：
- en: Models correspond to **a type of model architecture**. *E.g.* the [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    class is used for all UNet variations that expect 2D image inputs and are conditioned
    on some context.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型对应于**一种模型架构**。*例如* [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)类用于所有期望2D图像输入并受一些上下文影响的UNet变体。
- en: All models can be found in [`src/diffusers/models`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)
    and every model architecture shall be defined in its file, e.g. [`unet_2d_condition.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py),
    [`transformer_2d.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformer_2d.py),
    etc…
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型都可以在[`src/diffusers/models`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)中找到，每个模型架构应该在其文件中定义，例如[`unet_2d_condition.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py)，[`transformer_2d.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformer_2d.py)，等等...
- en: 'Models **do not** follow the single-file policy and should make use of smaller
    model building blocks, such as [`attention.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py),
    [`resnet.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py),
    [`embeddings.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py),
    etc… **Note**: This is in stark contrast to Transformers’ modeling files and shows
    that models do not really follow the single-file policy.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型**不**遵循单文件政策，应该利用较小的模型构建模块，例如[`attention.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py)、[`resnet.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py)、[`embeddings.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py)等等...
    **注意**：这与Transformers的建模文件形成鲜明对比，表明模型实际上并没有遵循单文件政策。
- en: Models intend to expose complexity, just like PyTorch’s `Module` class, and
    give clear error messages.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型意图公开复杂性，就像PyTorch的“Module”类一样，并提供清晰的错误消息。
- en: Models all inherit from `ModelMixin` and `ConfigMixin`.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型都继承自`ModelMixin`和`ConfigMixin`。
- en: Models can be optimized for performance when it doesn’t demand major code changes,
    keeps backward compatibility, and gives significant memory or compute gain.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当不需要进行重大代码更改时，模型可以针对性能进行优化，保持向后兼容性，并提供显著的内存或计算增益。
- en: Models should by default have the highest precision and lowest performance setting.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型应默认具有最高精度和最低性能设置。
- en: To integrate new model checkpoints whose general architecture can be classified
    as an architecture that already exists in Diffusers, the existing model architecture
    shall be adapted to make it work with the new checkpoint. One should only create
    a new file if the model architecture is fundamentally different.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要集成新的模型检查点，其一般架构可以归类为Diffusers中已存在的架构，现有的模型架构应该被调整以使其与新检查点配合工作。只有当模型架构根本不同的情况下才应该创建新文件。
- en: Models should be designed to be easily extendable to future changes. This can
    be achieved by limiting public function arguments, configuration arguments, and
    “foreseeing” future changes, *e.g.* it is usually better to add `string` “…type”
    arguments that can easily be extended to new future types instead of boolean `is_..._type`
    arguments. Only the minimum amount of changes shall be made to existing architectures
    to make a new model checkpoint work.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型应设计为易于扩展以适应未来的变化。这可以通过限制公共函数参数、配置参数和“预见”未来变化来实现，例如最好添加`string`“…type”参数，可以轻松扩展到新的未来类型，而不是布尔`is_..._type`参数。只需对现有架构进行最少的更改，使新模型检查点能够正常工作。
- en: The model design is a difficult trade-off between keeping code readable and
    concise and supporting many model checkpoints. For most parts of the modeling
    code, classes shall be adapted for new model checkpoints, while there are some
    exceptions where it is preferred to add new classes to make sure the code is kept
    concise and readable long-term, such as [UNet blocks](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_blocks.py)
    and [Attention processors](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型设计是在保持代码可读性和简洁性以及支持许多模型检查点之间进行艰难权衡。在建模代码的大部分部分，类应该适应新的模型检查点，但也有一些例外情况，最好添加新的类以确保代码长期保持简洁和可读性，例如[UNet
    blocks](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_blocks.py)和[Attention
    processors](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)。
- en: Schedulers
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度器
- en: Schedulers are responsible to guide the denoising process for inference as well
    as to define a noise schedule for training. They are designed as individual classes
    with loadable configuration files and strongly follow the **single-file policy**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器负责引导推理的去噪过程，同时为训练定义噪声计划。它们被设计为具有可加载配置文件的单独类，并严格遵循“单文件政策”。
- en: 'The following design principles are followed:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以下设计原则：
- en: All schedulers are found in [`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有调度器都位于[`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)中。
- en: Schedulers are **not** allowed to import from large utils files and shall be
    kept very self-contained.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器**不**允许从大型utils文件中导入，并应保持非常自包含。
- en: One scheduler Python file corresponds to one scheduler algorithm (as might be
    defined in a paper).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个调度器Python文件对应一个调度算法（可能在论文中定义）。
- en: If schedulers share similar functionalities, we can make use of the `#Copied
    from` mechanism.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果调度器共享类似的功能，我们可以利用“#Copied from”机制。
- en: Schedulers all inherit from `SchedulerMixin` and `ConfigMixin`.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有调度器都继承自`SchedulerMixin`和`ConfigMixin`。
- en: Schedulers can be easily swapped out with the [`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config)
    method as explained in detail [here](../using-diffusers/schedulers.md).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器可以通过[`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config)方法轻松替换，详细说明见[此处](../using-diffusers/schedulers.md)。
- en: Every scheduler has to have a `set_num_inference_steps`, and a `step` function.
    `set_num_inference_steps(...)` has to be called before every denoising process,
    *i.e.* before `step(...)` is called.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个调度器都必须有一个`set_num_inference_steps`和一个`step`函数。在每次去噪过程之前必须调用`set_num_inference_steps(...)`，即在调用`step(...)`之前。
- en: Every scheduler exposes the timesteps to be “looped over” via a `timesteps`
    attribute, which is an array of timesteps the model will be called upon.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个调度器通过一个“timesteps”属性公开要“循环遍历”的时间步，这是模型将被调用的时间步数组。
- en: The `step(...)` function takes a predicted model output and the “current” sample
    (x_t) and returns the “previous”, slightly more denoised sample (x_t-1).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step(...)`函数接受一个预测的模型输出和“当前”样本（x_t），并返回“上一个”，稍微去噪的样本（x_t-1）。'
- en: Given the complexity of diffusion schedulers, the `step` function does not expose
    all the complexity and can be a bit of a “black box”.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴于扩散调度器的复杂性，“step”函数不会暴露所有复杂性，可能有点像一个“黑匣子”。
- en: In almost all cases, novel schedulers shall be implemented in a new scheduling
    file.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在几乎所有情况下，新的调度程序应该在一个新的调度文件中实现。
