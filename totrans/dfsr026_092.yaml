- en: Philosophy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/conceptual/philosophy](https://huggingface.co/docs/diffusers/conceptual/philosophy)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ§¨ Diffusers provides **state-of-the-art** pretrained diffusion models across
    multiple modalities. Its purpose is to serve as a **modular toolbox** for both
    inference and training.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We aim at building a library that stands the test of time and therefore take
    API design very seriously.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore,
    most of our design choices are based on [PyTorchâ€™s Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy).
    Letâ€™s go over the most important ones:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Usability over Performance
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Diffusers has many built-in performance-enhancing features (see [Memory
    and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are
    always loaded with the highest precision and lowest optimization. Therefore, by
    default diffusion pipelines are always instantiated on CPU with float32 precision
    if not otherwise defined by the user. This ensures usability across different
    platforms and accelerators and means that no complex installations are required
    to run the library.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusers aims to be a **light-weight** package and therefore has very few required
    dependencies, but many soft dependencies that can improve performance (such as
    `accelerate`, `safetensors`, `onnx`, etcâ€¦). We strive to keep the library as lightweight
    as possible so that it can be added without much concern as a dependency on other
    packages.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusers prefers simple, self-explainable code over condensed, magic code.
    This means that short-hand code syntaxes such as lambda functions, and advanced
    PyTorch operators are often not desired.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple over easy
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As PyTorch states, **explicit is better than implicit** and **simple is better
    than complex**. This design philosophy is reflected in multiple parts of the library:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We follow PyTorchâ€™s API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to)
    to let the user handle device management.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raising concise error messages is preferred to silently correct erroneous input.
    Diffusers aims at teaching the user, rather than making the library as easy to
    use as possible.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex model vs. scheduler logic is exposed instead of magically handled inside.
    Schedulers/Samplers are separated from diffusion models with minimal dependencies
    on each other. This forces the user to write the unrolled denoising loop. However,
    the separation allows for easier debugging and gives the user more control over
    adapting the denoising process or switching out diffusion models or schedulers.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separately trained components of the diffusion pipeline, *e.g.* the text encoder,
    the unet, and the variational autoencoder, each have their own model class. This
    forces the user to handle the interaction between the different model components,
    and the serialization format separates the model components into different files.
    However, this allows for easier debugging and customization. DreamBooth or Textual
    Inversion training is very simple thanks to Diffusersâ€™ ability to separate single
    components of the diffusion pipeline.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweakable, contributor-friendly over abstraction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For large parts of the library, Diffusers adopts an important design principle
    of the [Transformers library](https://github.com/huggingface/transformers), which
    is to prefer copy-pasted code over hasty abstractions. This design principle is
    very opinionated and stands in stark contrast to popular design principles such
    as [Donâ€™t repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).
    In short, just like Transformers does for modeling files, Diffusers prefers to
    keep an extremely low level of abstraction and very self-contained code for pipelines
    and schedulers. Functions, long code blocks, and even classes can be copied across
    multiple files which at first can look like a bad, sloppy design choice that makes
    the library unmaintainable. **However**, this design has proven to be extremely
    successful for Transformers and makes a lot of sense for community-driven, open-source
    machine learning libraries because:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾ä¹¦é¦†çš„å¤§éƒ¨åˆ†éƒ¨åˆ†ï¼ŒDiffusersé‡‡ç”¨äº†[Transformersåº“](https://github.com/huggingface/transformers)çš„ä¸€ä¸ªé‡è¦è®¾è®¡åŸåˆ™ï¼Œå³æ›´å–œæ¬¢å¤åˆ¶ç²˜è´´çš„ä»£ç è€Œä¸æ˜¯ä»“ä¿ƒçš„æŠ½è±¡ã€‚è¿™ä¸ªè®¾è®¡åŸåˆ™éå¸¸æœ‰ä¸»è§ï¼Œä¸æµè¡Œçš„è®¾è®¡åŸåˆ™å¦‚[ä¸è¦é‡å¤è‡ªå·±ï¼ˆDRYï¼‰](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå°±åƒTransformersä¸ºå»ºæ¨¡æ–‡ä»¶æ‰€åšçš„é‚£æ ·ï¼ŒDiffusersæ›´å–œæ¬¢ä¿æŒæä½çš„æŠ½è±¡æ°´å¹³å’Œéå¸¸è‡ªåŒ…å«çš„ç®¡é“å’Œè°ƒåº¦å™¨ä»£ç ã€‚å‡½æ•°ã€é•¿ä»£ç å—ï¼Œç”šè‡³ç±»å¯ä»¥åœ¨å¤šä¸ªæ–‡ä»¶ä¹‹é—´å¤åˆ¶ï¼Œèµ·åˆå¯èƒ½çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªç³Ÿç³•ã€æ‡’æ•£çš„è®¾è®¡é€‰æ‹©ï¼Œä½¿å¾—å›¾ä¹¦é¦†éš¾ä»¥ç»´æŠ¤ã€‚**ç„¶è€Œ**ï¼Œè¿™ç§è®¾è®¡å·²ç»è¢«è¯æ˜å¯¹Transformerséå¸¸æˆåŠŸï¼Œå¹¶ä¸”å¯¹äºç¤¾åŒºé©±åŠ¨çš„å¼€æºæœºå™¨å­¦ä¹ åº“æ¥è¯´æ˜¯éå¸¸åˆç†çš„ï¼Œå› ä¸ºï¼š
- en: Machine Learning is an extremely fast-moving field in which paradigms, model
    architectures, and algorithms are changing rapidly, which therefore makes it very
    difficult to define long-lasting code abstractions.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼ŒèŒƒå¼ã€æ¨¡å‹æ¶æ„å’Œç®—æ³•éƒ½åœ¨è¿…é€Ÿå˜åŒ–ï¼Œå› æ­¤å¾ˆéš¾å®šä¹‰æŒä¹…çš„ä»£ç æŠ½è±¡ã€‚
- en: Machine Learning practitioners like to be able to quickly tweak existing code
    for ideation and research and therefore prefer self-contained code over one that
    contains many abstractions.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ä»ä¸šè€…å–œæ¬¢èƒ½å¤Ÿå¿«é€Ÿè°ƒæ•´ç°æœ‰ä»£ç è¿›è¡Œæ„æ€å’Œç ”ç©¶ï¼Œå› æ­¤æ›´å–œæ¬¢è‡ªåŒ…å«çš„ä»£ç è€Œä¸æ˜¯åŒ…å«è®¸å¤šæŠ½è±¡çš„ä»£ç ã€‚
- en: Open-source libraries rely on community contributions and therefore must build
    a library that is easy to contribute to. The more abstract the code, the more
    dependencies, the harder to read, and the harder to contribute to. Contributors
    simply stop contributing to very abstract libraries out of fear of breaking vital
    functionality. If contributing to a library cannot break other fundamental code,
    not only is it more inviting for potential new contributors, but it is also easier
    to review and contribute to multiple parts in parallel.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€æºåº“ä¾èµ–äºç¤¾åŒºè´¡çŒ®ï¼Œå› æ­¤å¿…é¡»æ„å»ºä¸€ä¸ªæ˜“äºè´¡çŒ®çš„åº“ã€‚ä»£ç è¶ŠæŠ½è±¡ï¼Œä¾èµ–è¶Šå¤šï¼Œé˜…è¯»è¶Šå›°éš¾ï¼Œè´¡çŒ®ä¹Ÿè¶Šå›°éš¾ã€‚è´¡çŒ®è€…ç®€å•åœ°åœæ­¢å¯¹éå¸¸æŠ½è±¡çš„åº“è¿›è¡Œè´¡çŒ®ï¼Œå› ä¸ºä»–ä»¬æ‹…å¿ƒç ´åé‡è¦åŠŸèƒ½ã€‚å¦‚æœå¯¹åº“çš„è´¡çŒ®ä¸ä¼šç ´åå…¶ä»–åŸºæœ¬ä»£ç ï¼Œä¸ä»…å¯¹æ½œåœ¨çš„æ–°è´¡çŒ®è€…æ›´å…·å¸å¼•åŠ›ï¼Œè€Œä¸”æ›´å®¹æ˜“å®¡æŸ¥å’ŒåŒæ—¶è´¡çŒ®å¤šä¸ªéƒ¨åˆ†ã€‚
- en: At Hugging Face, we call this design the **single-file policy** which means
    that almost all of the code of a certain class should be written in a single,
    self-contained file. To read more about the philosophy, you can have a look at
    [this blog post](https://huggingface.co/blog/transformers-design-philosophy).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Hugging Faceï¼Œæˆ‘ä»¬ç§°è¿™ç§è®¾è®¡ä¸º**å•æ–‡ä»¶æ”¿ç­–**ï¼Œè¿™æ„å‘³ç€æŸä¸ªç±»çš„å‡ ä¹æ‰€æœ‰ä»£ç åº”è¯¥å†™åœ¨ä¸€ä¸ªå•ç‹¬çš„è‡ªåŒ…å«æ–‡ä»¶ä¸­ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿™ç§å“²å­¦çš„ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/transformers-design-philosophy)ã€‚
- en: In Diffusers, we follow this philosophy for both pipelines and schedulers, but
    only partly for diffusion models. The reason we donâ€™t follow this design fully
    for diffusion models is because almost all diffusion pipelines, such as [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm),
    [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines),
    [unCLIP (DALLÂ·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
    and [Imagen](https://imagen.research.google/) all rely on the same diffusion model,
    the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Diffusersä¸­ï¼Œæˆ‘ä»¬å¯¹ç®¡é“å’Œè°ƒåº¦å™¨éƒ½éµå¾ªè¿™ä¸€å“²å­¦ï¼Œä½†å¯¹æ‰©æ•£æ¨¡å‹åªéƒ¨åˆ†éµå¾ªã€‚æˆ‘ä»¬ä¸å®Œå…¨éµå¾ªè¿™ç§è®¾è®¡çš„åŸå› æ˜¯å› ä¸ºå‡ ä¹æ‰€æœ‰æ‰©æ•£ç®¡é“ï¼Œå¦‚[DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm)ã€[ç¨³å®šæ‰©æ•£](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines)ã€[unCLIPï¼ˆDALLÂ·E
    2ï¼‰](https://huggingface.co/docs/diffusers/api/pipelines/unclip)å’Œ[Imagen](https://imagen.research.google/)éƒ½ä¾èµ–äºç›¸åŒçš„æ‰©æ•£æ¨¡å‹ï¼Œå³[UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond)ã€‚
- en: Great, now you should have generally understood why ğŸ§¨ Diffusers is designed
    the way it is ğŸ¤—. We try to apply these design principles consistently across the
    library. Nevertheless, there are some minor exceptions to the philosophy or some
    unlucky design choices. If you have feedback regarding the design, we would â¤ï¸
    to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨ä½ åº”è¯¥å¤§è‡´äº†è§£ä¸ºä»€ä¹ˆğŸ§¨ Diffusersè¢«è®¾è®¡æˆè¿™æ ·äº†ğŸ¤—ã€‚æˆ‘ä»¬è¯•å›¾åœ¨æ•´ä¸ªåº“ä¸­ä¸€è‡´åº”ç”¨è¿™äº›è®¾è®¡åŸåˆ™ã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›å¯¹è¿™ä¸€ç†å¿µçš„å°ä¾‹å¤–æˆ–ä¸€äº›ä¸å¹¸çš„è®¾è®¡é€‰æ‹©ã€‚å¦‚æœæ‚¨å¯¹è®¾è®¡æœ‰åé¦ˆï¼Œæˆ‘ä»¬ä¼šâ¤ï¸ç›´æ¥åœ¨GitHubä¸Šå¬å–æ„è§ï¼ˆhttps://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=ï¼‰ã€‚
- en: Design Philosophy in Details
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯¦ç»†çš„è®¾è®¡å“²å­¦
- en: 'Now, letâ€™s look a bit into the nitty-gritty details of the design philosophy.
    Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines),
    [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models),
    and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).
    Letâ€™s walk through more in-detail design decisions for each class.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç¨å¾®æ·±å…¥äº†è§£è®¾è®¡å“²å­¦çš„ç»†èŠ‚ã€‚DiffusersåŸºæœ¬ä¸Šç”±ä¸‰ä¸ªä¸»è¦ç±»ç»„æˆï¼š[ç®¡é“](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines)ã€[æ¨¡å‹](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)å’Œ[è°ƒåº¦å™¨](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)ã€‚è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°è®¨è®ºæ¯ä¸ªç±»çš„è®¾è®¡å†³ç­–ã€‚
- en: Pipelines
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®¡é“
- en: Pipelines are designed to be easy to use (therefore do not follow [*Simple over
    easy*](#simple-over-easy) 100%), are not feature complete, and should loosely
    be seen as examples of how to use [models](#models) and [schedulers](#schedulers)
    for inference.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“è¢«è®¾è®¡ä¸ºæ˜“äºä½¿ç”¨ï¼ˆå› æ­¤ä¸å®Œå…¨éµå¾ª[*ç®€å•èƒœè¿‡å®¹æ˜“*](#ç®€å•èƒœè¿‡å®¹æ˜“) 100%ï¼‰ï¼Œä¸æ˜¯åŠŸèƒ½å®Œå¤‡çš„ï¼Œå¹¶ä¸”åº”è¯¥å®½æ¾åœ°è¢«è§†ä¸ºå¦‚ä½•ä½¿ç”¨[æ¨¡å‹](#æ¨¡å‹)å’Œ[è°ƒåº¦å™¨](#è°ƒåº¦å™¨)è¿›è¡Œæ¨æ–­çš„ç¤ºä¾‹ã€‚
- en: 'The following design principles are followed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: éµå¾ªä»¥ä¸‹è®¾è®¡åŸåˆ™ï¼š
- en: Pipelines follow the single-file policy. All pipelines can be found in individual
    directories under src/diffusers/pipelines. One pipeline folder corresponds to
    one diffusion paper/project/release. Multiple pipeline files can be gathered in
    one pipeline folder, as itâ€™s done for [`src/diffusers/pipelines/stable-diffusion`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion).
    If pipelines share similar functionality, one can make use of the [#Copied from
    mechanism](https://github.com/huggingface/diffusers/blob/125d783076e5bd9785beb05367a2d2566843a271/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L251).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡é“éµå¾ªå•æ–‡ä»¶ç­–ç•¥ã€‚æ‰€æœ‰ç®¡é“éƒ½å¯ä»¥åœ¨src/diffusers/pipelinesçš„å„ä¸ªç›®å½•ä¸‹æ‰¾åˆ°ã€‚ä¸€ä¸ªç®¡é“æ–‡ä»¶å¤¹å¯¹åº”ä¸€ä¸ªæ‰©æ•£è®ºæ–‡/é¡¹ç›®/å‘å¸ƒã€‚å¤šä¸ªç®¡é“æ–‡ä»¶å¯ä»¥æ”¶é›†åœ¨ä¸€ä¸ªç®¡é“æ–‡ä»¶å¤¹ä¸­ï¼Œå°±åƒå¯¹äº[`src/diffusers/pipelines/stable-diffusion`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion)æ‰€åšçš„é‚£æ ·ã€‚å¦‚æœç®¡é“å…±äº«ç±»ä¼¼åŠŸèƒ½ï¼Œå¯ä»¥åˆ©ç”¨[#ä»æœºåˆ¶å¤åˆ¶](https://github.com/huggingface/diffusers/blob/125d783076e5bd9785beb05367a2d2566843a271/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L251)ã€‚
- en: Pipelines all inherit from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ç®¡é“éƒ½ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚
- en: Every pipeline consists of different model and scheduler components, that are
    documented in the [`model_index.json` file](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json),
    are accessible under the same name as attributes of the pipeline and can be shared
    between pipelines with [`DiffusionPipeline.components`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.components)
    function.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç®¡é“ç”±ä¸åŒçš„æ¨¡å‹å’Œè°ƒåº¦å™¨ç»„ä»¶ç»„æˆï¼Œè¿™äº›ç»„ä»¶åœ¨[`model_index.json`æ–‡ä»¶](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json)ä¸­æœ‰æ–‡æ¡£è®°å½•ï¼Œå¯ä»¥åœ¨ç®¡é“çš„å±æ€§ä¸‹ä»¥ç›¸åŒçš„åç§°è®¿é—®ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡[`DiffusionPipeline.components`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.components)å‡½æ•°åœ¨ç®¡é“ä¹‹é—´å…±äº«ã€‚
- en: Every pipeline should be loadable via the [`DiffusionPipeline.from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained)
    function.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç®¡é“åº”è¯¥å¯ä»¥é€šè¿‡[`DiffusionPipeline.from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained)å‡½æ•°åŠ è½½ã€‚
- en: Pipelines should be used **only** for inference.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡é“åº”è¯¥**ä»…**ç”¨äºæ¨æ–­ã€‚
- en: Pipelines should be very readable, self-explanatory, and easy to tweak.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡é“åº”è¯¥éå¸¸æ˜“è¯»ï¼Œè‡ªè§£é‡Šï¼Œå¹¶ä¸”æ˜“äºè°ƒæ•´ã€‚
- en: Pipelines should be designed to build on top of each other and be easy to integrate
    into higher-level APIs.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡é“åº”è¯¥è¢«è®¾è®¡ä¸ºç›¸äº’æ„å»ºï¼Œå¹¶ä¸”æ˜“äºé›†æˆåˆ°æ›´é«˜çº§åˆ«çš„APIä¸­ã€‚
- en: Pipelines are **not** intended to be feature-complete user interfaces. For future
    complete user interfaces one should rather have a look at [InvokeAI](https://github.com/invoke-ai/InvokeAI),
    [Diffuzers](https://github.com/abhishekkrthakur/diffuzers), and [lama-cleaner](https://github.com/Sanster/lama-cleaner).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡é“**ä¸**æ—¨åœ¨æˆä¸ºåŠŸèƒ½å®Œå¤‡çš„ç”¨æˆ·ç•Œé¢ã€‚å¯¹äºæœªæ¥å®Œæ•´çš„ç”¨æˆ·ç•Œé¢ï¼Œåº”è¯¥æ›´å¤šåœ°æŸ¥çœ‹[InvokeAI](https://github.com/invoke-ai/InvokeAI)ï¼Œ[Diffuzers](https://github.com/abhishekkrthakur/diffuzers)å’Œ[lama-cleaner](https://github.com/Sanster/lama-cleaner)ã€‚
- en: Every pipeline should have one and only one way to run it via a `__call__` method.
    The naming of the `__call__` arguments should be shared across all pipelines.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç®¡é“åº”è¯¥åªæœ‰ä¸€ç§é€šè¿‡`__call__`æ–¹æ³•è¿è¡Œçš„æ–¹å¼ã€‚`__call__`å‚æ•°çš„å‘½ååº”è¯¥åœ¨æ‰€æœ‰ç®¡é“ä¸­å…±äº«ã€‚
- en: Pipelines should be named after the task they are intended to solve.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡é“åº”è¯¥ä»¥å…¶æ‰€æ‰“ç®—è§£å†³çš„ä»»åŠ¡å‘½åã€‚
- en: In almost all cases, novel diffusion pipelines shall be implemented in a new
    pipeline folder/file.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å‡ ä¹æ‰€æœ‰æƒ…å†µä¸‹ï¼Œæ–°é¢–çš„æ‰©æ•£ç®¡é“åº”è¯¥åœ¨æ–°çš„ç®¡é“æ–‡ä»¶å¤¹/æ–‡ä»¶ä¸­å®ç°ã€‚
- en: Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: Models are designed as configurable toolboxes that are natural extensions of
    [PyTorchâ€™s Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).
    They only partly follow the **single-file policy**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¢«è®¾è®¡ä¸ºå¯é…ç½®çš„å·¥å…·ç®±ï¼Œæ˜¯[PyTorchçš„Moduleç±»](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)çš„è‡ªç„¶æ‰©å±•ã€‚å®ƒä»¬åªéƒ¨åˆ†éµå¾ª**å•æ–‡ä»¶ç­–ç•¥**ã€‚
- en: 'The following design principles are followed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: éµå¾ªä»¥ä¸‹è®¾è®¡åŸåˆ™ï¼š
- en: Models correspond to **a type of model architecture**. *E.g.* the [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    class is used for all UNet variations that expect 2D image inputs and are conditioned
    on some context.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¯¹åº”äº**ä¸€ç§æ¨¡å‹æ¶æ„**ã€‚*ä¾‹å¦‚* [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ç±»ç”¨äºæ‰€æœ‰æœŸæœ›2Då›¾åƒè¾“å…¥å¹¶å—ä¸€äº›ä¸Šä¸‹æ–‡å½±å“çš„UNetå˜ä½“ã€‚
- en: All models can be found in [`src/diffusers/models`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)
    and every model architecture shall be defined in its file, e.g. [`unet_2d_condition.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py),
    [`transformer_2d.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformer_2d.py),
    etcâ€¦
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹éƒ½å¯ä»¥åœ¨[`src/diffusers/models`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)ä¸­æ‰¾åˆ°ï¼Œæ¯ä¸ªæ¨¡å‹æ¶æ„åº”è¯¥åœ¨å…¶æ–‡ä»¶ä¸­å®šä¹‰ï¼Œä¾‹å¦‚[`unet_2d_condition.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py)ï¼Œ[`transformer_2d.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformer_2d.py)ï¼Œç­‰ç­‰...
- en: 'Models **do not** follow the single-file policy and should make use of smaller
    model building blocks, such as [`attention.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py),
    [`resnet.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py),
    [`embeddings.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py),
    etcâ€¦ **Note**: This is in stark contrast to Transformersâ€™ modeling files and shows
    that models do not really follow the single-file policy.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹**ä¸**éµå¾ªå•æ–‡ä»¶æ”¿ç­–ï¼Œåº”è¯¥åˆ©ç”¨è¾ƒå°çš„æ¨¡å‹æ„å»ºæ¨¡å—ï¼Œä¾‹å¦‚[`attention.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py)ã€[`resnet.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py)ã€[`embeddings.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py)ç­‰ç­‰...
    **æ³¨æ„**ï¼šè¿™ä¸Transformersçš„å»ºæ¨¡æ–‡ä»¶å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œè¡¨æ˜æ¨¡å‹å®é™…ä¸Šå¹¶æ²¡æœ‰éµå¾ªå•æ–‡ä»¶æ”¿ç­–ã€‚
- en: Models intend to expose complexity, just like PyTorchâ€™s `Module` class, and
    give clear error messages.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ„å›¾å…¬å¼€å¤æ‚æ€§ï¼Œå°±åƒPyTorchçš„â€œModuleâ€ç±»ä¸€æ ·ï¼Œå¹¶æä¾›æ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯ã€‚
- en: Models all inherit from `ModelMixin` and `ConfigMixin`.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹éƒ½ç»§æ‰¿è‡ª`ModelMixin`å’Œ`ConfigMixin`ã€‚
- en: Models can be optimized for performance when it doesnâ€™t demand major code changes,
    keeps backward compatibility, and gives significant memory or compute gain.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“ä¸éœ€è¦è¿›è¡Œé‡å¤§ä»£ç æ›´æ”¹æ—¶ï¼Œæ¨¡å‹å¯ä»¥é’ˆå¯¹æ€§èƒ½è¿›è¡Œä¼˜åŒ–ï¼Œä¿æŒå‘åå…¼å®¹æ€§ï¼Œå¹¶æä¾›æ˜¾è‘—çš„å†…å­˜æˆ–è®¡ç®—å¢ç›Šã€‚
- en: Models should by default have the highest precision and lowest performance setting.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹åº”é»˜è®¤å…·æœ‰æœ€é«˜ç²¾åº¦å’Œæœ€ä½æ€§èƒ½è®¾ç½®ã€‚
- en: To integrate new model checkpoints whose general architecture can be classified
    as an architecture that already exists in Diffusers, the existing model architecture
    shall be adapted to make it work with the new checkpoint. One should only create
    a new file if the model architecture is fundamentally different.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦é›†æˆæ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå…¶ä¸€èˆ¬æ¶æ„å¯ä»¥å½’ç±»ä¸ºDiffusersä¸­å·²å­˜åœ¨çš„æ¶æ„ï¼Œç°æœ‰çš„æ¨¡å‹æ¶æ„åº”è¯¥è¢«è°ƒæ•´ä»¥ä½¿å…¶ä¸æ–°æ£€æŸ¥ç‚¹é…åˆå·¥ä½œã€‚åªæœ‰å½“æ¨¡å‹æ¶æ„æ ¹æœ¬ä¸åŒçš„æƒ…å†µä¸‹æ‰åº”è¯¥åˆ›å»ºæ–°æ–‡ä»¶ã€‚
- en: Models should be designed to be easily extendable to future changes. This can
    be achieved by limiting public function arguments, configuration arguments, and
    â€œforeseeingâ€ future changes, *e.g.* it is usually better to add `string` â€œâ€¦typeâ€
    arguments that can easily be extended to new future types instead of boolean `is_..._type`
    arguments. Only the minimum amount of changes shall be made to existing architectures
    to make a new model checkpoint work.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹åº”è®¾è®¡ä¸ºæ˜“äºæ‰©å±•ä»¥é€‚åº”æœªæ¥çš„å˜åŒ–ã€‚è¿™å¯ä»¥é€šè¿‡é™åˆ¶å…¬å…±å‡½æ•°å‚æ•°ã€é…ç½®å‚æ•°å’Œâ€œé¢„è§â€æœªæ¥å˜åŒ–æ¥å®ç°ï¼Œä¾‹å¦‚æœ€å¥½æ·»åŠ `string`â€œâ€¦typeâ€å‚æ•°ï¼Œå¯ä»¥è½»æ¾æ‰©å±•åˆ°æ–°çš„æœªæ¥ç±»å‹ï¼Œè€Œä¸æ˜¯å¸ƒå°”`is_..._type`å‚æ•°ã€‚åªéœ€å¯¹ç°æœ‰æ¶æ„è¿›è¡Œæœ€å°‘çš„æ›´æ”¹ï¼Œä½¿æ–°æ¨¡å‹æ£€æŸ¥ç‚¹èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚
- en: The model design is a difficult trade-off between keeping code readable and
    concise and supporting many model checkpoints. For most parts of the modeling
    code, classes shall be adapted for new model checkpoints, while there are some
    exceptions where it is preferred to add new classes to make sure the code is kept
    concise and readable long-term, such as [UNet blocks](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_blocks.py)
    and [Attention processors](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹è®¾è®¡æ˜¯åœ¨ä¿æŒä»£ç å¯è¯»æ€§å’Œç®€æ´æ€§ä»¥åŠæ”¯æŒè®¸å¤šæ¨¡å‹æ£€æŸ¥ç‚¹ä¹‹é—´è¿›è¡Œè‰°éš¾æƒè¡¡ã€‚åœ¨å»ºæ¨¡ä»£ç çš„å¤§éƒ¨åˆ†éƒ¨åˆ†ï¼Œç±»åº”è¯¥é€‚åº”æ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›ä¾‹å¤–æƒ…å†µï¼Œæœ€å¥½æ·»åŠ æ–°çš„ç±»ä»¥ç¡®ä¿ä»£ç é•¿æœŸä¿æŒç®€æ´å’Œå¯è¯»æ€§ï¼Œä¾‹å¦‚[UNet
    blocks](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_blocks.py)å’Œ[Attention
    processors](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)ã€‚
- en: Schedulers
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨
- en: Schedulers are responsible to guide the denoising process for inference as well
    as to define a noise schedule for training. They are designed as individual classes
    with loadable configuration files and strongly follow the **single-file policy**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨è´Ÿè´£å¼•å¯¼æ¨ç†çš„å»å™ªè¿‡ç¨‹ï¼ŒåŒæ—¶ä¸ºè®­ç»ƒå®šä¹‰å™ªå£°è®¡åˆ’ã€‚å®ƒä»¬è¢«è®¾è®¡ä¸ºå…·æœ‰å¯åŠ è½½é…ç½®æ–‡ä»¶çš„å•ç‹¬ç±»ï¼Œå¹¶ä¸¥æ ¼éµå¾ªâ€œå•æ–‡ä»¶æ”¿ç­–â€ã€‚
- en: 'The following design principles are followed:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: éµå¾ªä»¥ä¸‹è®¾è®¡åŸåˆ™ï¼š
- en: All schedulers are found in [`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è°ƒåº¦å™¨éƒ½ä½äº[`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)ä¸­ã€‚
- en: Schedulers are **not** allowed to import from large utils files and shall be
    kept very self-contained.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨**ä¸**å…è®¸ä»å¤§å‹utilsæ–‡ä»¶ä¸­å¯¼å…¥ï¼Œå¹¶åº”ä¿æŒéå¸¸è‡ªåŒ…å«ã€‚
- en: One scheduler Python file corresponds to one scheduler algorithm (as might be
    defined in a paper).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè°ƒåº¦å™¨Pythonæ–‡ä»¶å¯¹åº”ä¸€ä¸ªè°ƒåº¦ç®—æ³•ï¼ˆå¯èƒ½åœ¨è®ºæ–‡ä¸­å®šä¹‰ï¼‰ã€‚
- en: If schedulers share similar functionalities, we can make use of the `#Copied
    from` mechanism.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè°ƒåº¦å™¨å…±äº«ç±»ä¼¼çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨â€œ#Copied fromâ€æœºåˆ¶ã€‚
- en: Schedulers all inherit from `SchedulerMixin` and `ConfigMixin`.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è°ƒåº¦å™¨éƒ½ç»§æ‰¿è‡ª`SchedulerMixin`å’Œ`ConfigMixin`ã€‚
- en: Schedulers can be easily swapped out with the [`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config)
    method as explained in detail [here](../using-diffusers/schedulers.md).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨å¯ä»¥é€šè¿‡[`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config)æ–¹æ³•è½»æ¾æ›¿æ¢ï¼Œè¯¦ç»†è¯´æ˜è§[æ­¤å¤„](../using-diffusers/schedulers.md)ã€‚
- en: Every scheduler has to have a `set_num_inference_steps`, and a `step` function.
    `set_num_inference_steps(...)` has to be called before every denoising process,
    *i.e.* before `step(...)` is called.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè°ƒåº¦å™¨éƒ½å¿…é¡»æœ‰ä¸€ä¸ª`set_num_inference_steps`å’Œä¸€ä¸ª`step`å‡½æ•°ã€‚åœ¨æ¯æ¬¡å»å™ªè¿‡ç¨‹ä¹‹å‰å¿…é¡»è°ƒç”¨`set_num_inference_steps(...)`ï¼Œå³åœ¨è°ƒç”¨`step(...)`ä¹‹å‰ã€‚
- en: Every scheduler exposes the timesteps to be â€œlooped overâ€ via a `timesteps`
    attribute, which is an array of timesteps the model will be called upon.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè°ƒåº¦å™¨é€šè¿‡ä¸€ä¸ªâ€œtimestepsâ€å±æ€§å…¬å¼€è¦â€œå¾ªç¯éå†â€çš„æ—¶é—´æ­¥ï¼Œè¿™æ˜¯æ¨¡å‹å°†è¢«è°ƒç”¨çš„æ—¶é—´æ­¥æ•°ç»„ã€‚
- en: The `step(...)` function takes a predicted model output and the â€œcurrentâ€ sample
    (x_t) and returns the â€œpreviousâ€, slightly more denoised sample (x_t-1).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step(...)`å‡½æ•°æ¥å—ä¸€ä¸ªé¢„æµ‹çš„æ¨¡å‹è¾“å‡ºå’Œâ€œå½“å‰â€æ ·æœ¬ï¼ˆx_tï¼‰ï¼Œå¹¶è¿”å›â€œä¸Šä¸€ä¸ªâ€ï¼Œç¨å¾®å»å™ªçš„æ ·æœ¬ï¼ˆx_t-1ï¼‰ã€‚'
- en: Given the complexity of diffusion schedulers, the `step` function does not expose
    all the complexity and can be a bit of a â€œblack boxâ€.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‰´äºæ‰©æ•£è°ƒåº¦å™¨çš„å¤æ‚æ€§ï¼Œâ€œstepâ€å‡½æ•°ä¸ä¼šæš´éœ²æ‰€æœ‰å¤æ‚æ€§ï¼Œå¯èƒ½æœ‰ç‚¹åƒä¸€ä¸ªâ€œé»‘åŒ£å­â€ã€‚
- en: In almost all cases, novel schedulers shall be implemented in a new scheduling
    file.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å‡ ä¹æ‰€æœ‰æƒ…å†µä¸‹ï¼Œæ–°çš„è°ƒåº¦ç¨‹åºåº”è¯¥åœ¨ä¸€ä¸ªæ–°çš„è°ƒåº¦æ–‡ä»¶ä¸­å®ç°ã€‚
