["```py\npip install -U albumentations opencv-python\n```", "```py\n>>> from datasets import load_dataset\n\n>>> ds = load_dataset(\"cppe-5\")\n>>> example = ds['train'][0]\n>>> example\n{'height': 663,\n 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7FC3DC756250>,\n 'image_id': 15,\n 'objects': {'area': [3796, 1596, 152768, 81002],\n  'bbox': [[302.0, 109.0, 73.0, 52.0],\n   [810.0, 100.0, 57.0, 28.0],\n   [160.0, 31.0, 248.0, 616.0],\n   [741.0, 68.0, 202.0, 401.0]],\n  'category': [4, 4, 0, 0],\n  'id': [114, 115, 116, 117]},\n 'width': 943}\n```", "```py\n>>> import torch\n>>> from torchvision.ops import box_convert\n>>> from torchvision.utils import draw_bounding_boxes\n>>> from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n\n>>> categories = ds['train'].features['objects'].feature['category']\n\n>>> boxes_xywh = torch.tensor(example['objects']['bbox'])\n>>> boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')\n>>> labels = [categories.int2str(x) for x in example['objects']['category']]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         pil_to_tensor(example['image']),\n...         boxes_xyxy,\n...         colors=\"red\",\n...         labels=labels,\n...     )\n... )\n```", "```py\n>>> import albumentations\n>>> import numpy as np\n\n>>> transform = albumentations.Compose([\n...     albumentations.Resize(480, 480),\n...     albumentations.HorizontalFlip(p=1.0),\n...     albumentations.RandomBrightnessContrast(p=1.0),\n... ], bbox_params=albumentations.BboxParams(format='coco',  label_fields=['category']))\n\n>>> image = np.array(example['image'])\n>>> out = transform(\n...     image=image,\n...     bboxes=example['objects']['bbox'],\n...     category=example['objects']['category'],\n... )\n```", "```py\n>>> image = torch.tensor(out['image']).permute(2, 0, 1)\n>>> boxes_xywh = torch.stack([torch.tensor(x) for x in out['bboxes']])\n>>> boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')\n>>> labels = [categories.int2str(x) for x in out['category']]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         image,\n...         boxes_xyxy,\n...         colors='red',\n...         labels=labels\n...     )\n... )\n```", "```py\n>>> def transforms(examples):\n...     images, bboxes, categories = [], [], []\n...     for image, objects in zip(examples['image'], examples['objects']):\n...         image = np.array(image.convert(\"RGB\"))\n...         out = transform(\n...             image=image,\n...             bboxes=objects['bbox'],\n...             category=objects['category']\n...         )\n...         images.append(torch.tensor(out['image']).permute(2, 0, 1))\n...         bboxes.append(torch.tensor(out['bboxes']))\n...         categories.append(out['category'])\n...     return {'image': images, 'bbox': bboxes, 'category': categories}\n```", "```py\n>>> ds['train'].set_transform(transforms)\n```", "```py\n>>> example = ds['train'][10]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         example['image'],\n...         box_convert(example['bbox'], 'xywh', 'xyxy'),\n...         colors='red',\n...         labels=[categories.int2str(x) for x in example['category']]\n...     )\n... )\n```"]