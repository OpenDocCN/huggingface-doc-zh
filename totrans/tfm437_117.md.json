["```py\n>>> from transformers import GenerationConfig\n\n>>> # Download configuration from huggingface.co and cache.\n>>> generation_config = GenerationConfig.from_pretrained(\"gpt2\")\n\n>>> # E.g. config was saved using *save_pretrained('./test/saved_model/')*\n>>> generation_config.save_pretrained(\"./test/saved_model/\")\n>>> generation_config = GenerationConfig.from_pretrained(\"./test/saved_model/\")\n\n>>> # You can also specify configuration names to your generation configuration file\n>>> generation_config.save_pretrained(\"./test/saved_model/\", config_file_name=\"my_configuration.json\")\n>>> generation_config = GenerationConfig.from_pretrained(\"./test/saved_model/\", \"my_configuration.json\")\n\n>>> # If you'd like to try a minor variation to an existing configuration, you can also pass generation\n>>> # arguments to `.from_pretrained()`. Be mindful that typos and unused arguments will be ignored\n>>> generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n...     \"gpt2\", top_k=1, foo=False, do_sample=True, return_unused_kwargs=True\n... )\n>>> generation_config.top_k\n1\n\n>>> unused_kwargs\n{'foo': False}\n```", "```py\n>>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n>>> import numpy as np\n\n>>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer.pad_token_id = tokenizer.eos_token_id\n>>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n\n>>> # Example 1: Print the scores for each token generated with Greedy Search\n>>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n>>> transition_scores = model.compute_transition_scores(\n...     outputs.sequences, outputs.scores, normalize_logits=True\n... )\n>>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n>>> # encoder-decoder models, like BART or T5.\n>>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n>>> generated_tokens = outputs.sequences[:, input_length:]\n>>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n...     # | token | token string | logits | probability\n...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n|   262 |  the     | -1.414 | 24.33%\n|  1110 |  day     | -2.609 | 7.36%\n|   618 |  when    | -2.010 | 13.40%\n|   356 |  we      | -1.859 | 15.58%\n|   460 |  can     | -2.508 | 8.14%\n\n>>> # Example 2: Reconstruct the sequence scores from Beam Search\n>>> outputs = model.generate(\n...     **inputs,\n...     max_new_tokens=5,\n...     num_beams=4,\n...     num_return_sequences=4,\n...     return_dict_in_generate=True,\n...     output_scores=True,\n... )\n>>> transition_scores = model.compute_transition_scores(\n...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n... )\n>>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n>>> # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n>>> # use case, you might want to recompute it with `normalize_logits=True`.\n>>> # Tip 2: the output length does NOT include the input length\n>>> output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n>>> length_penalty = model.generation_config.length_penalty\n>>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n>>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\nTrue\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForCausalLM,\n...     LogitsProcessorList,\n...     MinLengthLogitsProcessor,\n...     StoppingCriteriaList,\n...     MaxLengthCriteria,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n>>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n>>> model.generation_config.pad_token_id = model.generation_config.eos_token_id\n\n>>> input_prompt = \"It might be possible to\"\n>>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n\n>>> # instantiate logits processors\n>>> logits_processor = LogitsProcessorList(\n...     [\n...         MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n...     ]\n... )\n>>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n\n>>> outputs = model.greedy_search(\n...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n... )\n\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n[\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForCausalLM,\n...     LogitsProcessorList,\n...     MinLengthLogitsProcessor,\n...     TopKLogitsWarper,\n...     TemperatureLogitsWarper,\n...     StoppingCriteriaList,\n...     MaxLengthCriteria,\n... )\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n>>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n>>> model.config.pad_token_id = model.config.eos_token_id\n>>> model.generation_config.pad_token_id = model.config.eos_token_id\n\n>>> input_prompt = \"Today is a beautiful day, and\"\n>>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n\n>>> # instantiate logits processors\n>>> logits_processor = LogitsProcessorList(\n...     [\n...         MinLengthLogitsProcessor(15, eos_token_id=model.generation_config.eos_token_id),\n...     ]\n... )\n>>> # instantiate logits processors\n>>> logits_warper = LogitsProcessorList(\n...     [\n...         TopKLogitsWarper(50),\n...         TemperatureLogitsWarper(0.7),\n...     ]\n... )\n\n>>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n\n>>> torch.manual_seed(0)\n>>> outputs = model.sample(\n...     input_ids,\n...     logits_processor=logits_processor,\n...     logits_warper=logits_warper,\n...     stopping_criteria=stopping_criteria,\n... )\n\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Today is a beautiful day, and we must do everything possible to make it a day of celebration.']\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForSeq2SeqLM,\n...     LogitsProcessorList,\n...     MinLengthLogitsProcessor,\n...     BeamSearchScorer,\n... )\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n>>> encoder_input_str = \"translate English to German: How old are you?\"\n>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n>>> # lets run beam search using 3 beams\n>>> num_beams = 3\n>>> # define decoder start token ids\n>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n>>> input_ids = input_ids * model.config.decoder_start_token_id\n\n>>> # add encoder_outputs to model keyword arguments\n>>> model_kwargs = {\n...     \"encoder_outputs\": model.get_encoder()(\n...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n...     )\n... }\n\n>>> # instantiate beam scorer\n>>> beam_scorer = BeamSearchScorer(\n...     batch_size=1,\n...     num_beams=num_beams,\n...     device=model.device,\n... )\n\n>>> # instantiate logits processors\n>>> logits_processor = LogitsProcessorList(\n...     [\n...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n...     ]\n... )\n\n>>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Wie alt bist du?']\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForSeq2SeqLM,\n...     LogitsProcessorList,\n...     MinLengthLogitsProcessor,\n...     TopKLogitsWarper,\n...     TemperatureLogitsWarper,\n...     BeamSearchScorer,\n... )\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n>>> encoder_input_str = \"translate English to German: How old are you?\"\n>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n>>> # lets run beam search using 3 beams\n>>> num_beams = 3\n>>> # define decoder start token ids\n>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n>>> input_ids = input_ids * model.config.decoder_start_token_id\n\n>>> # add encoder_outputs to model keyword arguments\n>>> model_kwargs = {\n...     \"encoder_outputs\": model.get_encoder()(\n...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n...     )\n... }\n\n>>> # instantiate beam scorer\n>>> beam_scorer = BeamSearchScorer(\n...     batch_size=1,\n...     max_length=model.config.max_length,\n...     num_beams=num_beams,\n...     device=model.device,\n... )\n\n>>> # instantiate logits processors\n>>> logits_processor = LogitsProcessorList(\n...     [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]\n... )\n>>> # instantiate logits processors\n>>> logits_warper = LogitsProcessorList(\n...     [\n...         TopKLogitsWarper(50),\n...         TemperatureLogitsWarper(0.7),\n...     ]\n... )\n\n>>> outputs = model.beam_sample(\n...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n... )\n\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Wie alt bist du?']\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForCausalLM,\n...     StoppingCriteriaList,\n...     MaxLengthCriteria,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n>>> # set pad_token_id to eos_token_id because OPT does not have a PAD token\n>>> model.config.pad_token_id = model.config.eos_token_id\n>>> input_prompt = \"DeepMind Company is\"\n>>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\")\n>>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=64)])\n>>> outputs = model.contrastive_search(\n...     **input_ids, penalty_alpha=0.6, top_k=4, stopping_criteria=stopping_criteria\n... )\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['DeepMind Company is a company that focuses on the development and commercialization of artificial intelligence (AI). DeepMind\u2019s mission is to help people understand and solve problems that are difficult to solve in the world today.\\n\\nIn this post, we talk about the benefits of deep learning in business and how it']\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForSeq2SeqLM,\n...     LogitsProcessorList,\n...     MinLengthLogitsProcessor,\n...     HammingDiversityLogitsProcessor,\n...     BeamSearchScorer,\n... )\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n>>> encoder_input_str = \"translate English to German: How old are you?\"\n>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n>>> # lets run diverse beam search using 6 beams\n>>> num_beams = 6\n>>> # define decoder start token ids\n>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n>>> input_ids = input_ids * model.config.decoder_start_token_id\n\n>>> # add encoder_outputs to model keyword arguments\n>>> model_kwargs = {\n...     \"encoder_outputs\": model.get_encoder()(\n...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n...     )\n... }\n\n>>> # instantiate beam scorer\n>>> beam_scorer = BeamSearchScorer(\n...     batch_size=1,\n...     max_length=model.config.max_length,\n...     num_beams=num_beams,\n...     device=model.device,\n...     num_beam_groups=3,\n... )\n\n>>> # instantiate logits processors\n>>> logits_processor = LogitsProcessorList(\n...     [\n...         HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n...     ]\n... )\n\n>>> outputs = model.group_beam_search(\n...     input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n... )\n\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Wie alt bist du?']\n```", "```py\n>>> from transformers import (\n...     AutoTokenizer,\n...     AutoModelForSeq2SeqLM,\n...     LogitsProcessorList,\n...     MinLengthLogitsProcessor,\n...     ConstrainedBeamSearchScorer,\n...     PhrasalConstraint,\n... )\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n>>> encoder_input_str = \"translate English to German: How old are you?\"\n>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n>>> # lets run beam search using 3 beams\n>>> num_beams = 3\n>>> # define decoder start token ids\n>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n>>> input_ids = input_ids * model.config.decoder_start_token_id\n\n>>> # add encoder_outputs to model keyword arguments\n>>> model_kwargs = {\n...     \"encoder_outputs\": model.get_encoder()(\n...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n...     )\n... }\n\n>>> constraint_str = \"Sie\"\n>>> constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n>>> constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n\n>>> # instantiate beam scorer\n>>> beam_scorer = ConstrainedBeamSearchScorer(\n...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints\n... )\n\n>>> # instantiate logits processors\n>>> logits_processor = LogitsProcessorList(\n...     [\n...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n...     ]\n... )\n\n>>> outputs = model.constrained_beam_search(\n...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n... )\n\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Wie alt sind Sie?']\n```", "```py\n>>> from transformers import GPT2Tokenizer, TFAutoModelForCausalLM\n>>> import numpy as np\n\n>>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n>>> model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer.pad_token_id = tokenizer.eos_token_id\n>>> inputs = tokenizer([\"Today is\"], return_tensors=\"tf\")\n\n>>> # Example 1: Print the scores for each token generated with Greedy Search\n>>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n>>> transition_scores = model.compute_transition_scores(\n...     outputs.sequences, outputs.scores, normalize_logits=True\n... )\n>>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n>>> # encoder-decoder models, like BART or T5.\n>>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n>>> generated_tokens = outputs.sequences[:, input_length:]\n>>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n...     # | token | token string | logits | probability\n...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n|   262 |  the     | -1.413 | 24.33%\n|  1110 |  day     | -2.609 | 7.36%\n|   618 |  when    | -2.009 | 13.41%\n|   356 |  we      | -1.859 | 15.58%\n|   460 |  can     | -2.508 | 8.14%\n\n>>> # Example 2: Reconstruct the sequence scores from Beam Search\n>>> outputs = model.generate(\n...     **inputs,\n...     max_new_tokens=5,\n...     num_beams=4,\n...     num_return_sequences=4,\n...     return_dict_in_generate=True,\n...     output_scores=True,\n... )\n>>> transition_scores = model.compute_transition_scores(\n...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n... )\n>>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n>>> # Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n>>> # use case, you might want to recompute it with `normalize_logits=True`.\n>>> output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n>>> length_penalty = model.generation_config.length_penalty\n>>> reconstructed_scores = np.sum(transition_scores, axis=1) / (output_length**length_penalty)\n>>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\nTrue\n```"]