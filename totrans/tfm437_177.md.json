["```py\n>>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer\n\n>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n\n>>> prompt = \"\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n\n>>> print(gen_text)\n\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001AI\u3068\u4eba\u304c\u5171\u5b58\u3057\u3001AI\u3092\u6b63\u3057\u304f\u7406\u89e3\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n```", "```py\n( vocab_size = 32000 hidden_size = 2560 num_hidden_layers = 32 num_attention_heads = 32 intermediate_multiple_size = 4 hidden_act = 'gelu' rotary_pct = 1.0 rotary_emb_base = 10000 max_position_embeddings = 2048 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True bos_token_id = 31996 eos_token_id = 31999 attention_dropout = 0.1 hidden_dropout = 0.0 **kwargs )\n```", "```py\n>>> from transformers import GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel\n\n>>> # Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration\n>>> configuration = GPTNeoXJapaneseConfig()\n\n>>> # Initializing a model (with random weights) from the gpt-neox-japanese-2.7b style configuration\n>>> model = GPTNeoXJapaneseModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file emoji_file unk_token = '<|endoftext|>' pad_token = '<|endoftext|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' do_clean_text = False **kwargs )\n```", "```py\n>>> from transformers import GPTNeoXJapaneseTokenizer\n\n>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n>>> # You can confirm both \u6176\u5fdc and \u6176\u61c9 are encoded to 17749\n>>> tokenizer(\"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\u5b9f\u306f\u6176\u5fdc(\u6176\u61c9)\u5927\u5b66\u51fa\u8eab\")[\"input_ids\"]\n[30014, 26883, 26638, 27228, 25, 26650, 31732, 31679, 27809, 26638, 17749, 31592, 17749, 31593, 321, 1281]\n\n>>> # Both \u6176\u5fdc and \u6176\u61c9 are decoded to \u6176\u5fdc\n>>> tokenizer.decode(tokenizer(\"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\u5b9f\u306f\u6176\u5fdc(\u6176\u61c9)\u5927\u5b66\u51fa\u8eab\")[\"input_ids\"])\n'\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\ud83d\udc2f\u3002\u5b9f\u306f\u6176\u5fdc(\u6176\u5fdc)\u5927\u5b66\u51fa\u8eab'\n```", "```py\n( tokens )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, GPTNeoXJapaneseModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n>>> model = GPTNeoXJapaneseModel.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n\n>>> inputs = tokenizer(\"\u65e5\u672c\u8a9e\u306eGPT-neox\u304cHugging Face\u3067\u4f7f\u3048\u307e\u3059\ud83d\ude00\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None head_mask: Optional = None past_key_values: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n>>> config = GPTNeoXJapaneseConfig.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n>>> config.is_decoder = True\n>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\", config=config)\n\n>>> inputs = tokenizer(\"\u65e5\u672c\u8a9e\u306eGPT-neox\u304cHugging Face\u3067\u4f7f\u3048\u307e\u3059\ud83d\ude00\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.logits\n```"]