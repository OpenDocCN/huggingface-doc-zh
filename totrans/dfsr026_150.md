# 潜在扩散

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion)

Robin Rombach、Andreas Blattmann、Dominik Lorenz、Patrick Esser、Björn Ommer在[使用潜在扩散模型进行高分辨率图像合成](https://huggingface.co/papers/2112.10752)中提出了潜在扩散。

该论文的摘要是：

*通过将图像形成过程分解为去噪自动编码器的顺序应用，扩散模型（DMs）在图像数据及其他领域取得了最先进的合成结果。此外，它们的制定允许通过引导机制控制图像生成过程而无需重新训练。然而，由于这些模型通常直接在像素空间中运行，优化强大的DMs通常消耗数百个GPU天，并且由于顺序评估而导致推理成本高昂。为了在有限的计算资源上训练DM并保持其质量和灵活性，我们将它们应用于强大的预训练自动编码器的潜在空间。与以往的工作相比，对这种表示进行扩散模型的训练首次实现了在复杂性减少和细节保留之间达到近乎最佳点，极大地提升了视觉保真度。通过在模型架构中引入交叉注意力层，我们将扩散模型转变为通用条件输入（如文本或边界框）的强大灵活生成器，高分辨率合成变得可能以卷积方式进行。我们的潜在扩散模型（LDMs）在图像修补方面取得了最新的技术水平，并在各种任务上表现出高竞争力，包括无条件图像生成、语义场景合成和超分辨率，同时与基于像素的DM相比显著降低了计算要求。*

原始代码库可在[CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion)找到。

请务必查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。

## LDMTextToImagePipeline

### `class diffusers.LDMTextToImagePipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py#L32)

```py
( vqvae: Union bert: PreTrainedModel tokenizer: PreTrainedTokenizer unet: Union scheduler: Union )
```

参数

+   `vqvae` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) — 用于将图像编码和解码为潜在表示的矢量量化（VQ）模型。

+   `bert` (`LDMBertModel`) — 基于`BERT`的文本编码器模型。

+   `tokenizer` ([BertTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)) — 用于对文本进行标记化的`BertTokenizer`。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪编码图像潜在表示的`UNet2DConditionModel`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 用于与`unet`结合使用以去噪编码图像潜在表示的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

使用潜在扩散进行文本到图像生成的管道。

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py#L67)

```py
( prompt: Union height: Optional = None width: Optional = None num_inference_steps: Optional = 50 guidance_scale: Optional = 1.0 eta: Optional = 0.0 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str`或`List[str]`) — 用于指导图像生成的提示或提示。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的高度（以像素为单位）。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的宽度（以像素为单位）。

+   `num_inference_steps` (`int`, *可选*, 默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为1.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。 当`guidance_scale > 1`时启用引导比例。

+   `generator` (`torch.Generator`, *可选*) — 一个[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中预先生成的嘈杂潜在空间，用作图像生成的输入。可以用来调整相同生成过程的不同提示。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在空间张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是一个普通的tuple。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> from diffusers import DiffusionPipeline

>>> # load model and scheduler
>>> ldm = DiffusionPipeline.from_pretrained("CompVis/ldm-text2im-large-256")

>>> # run pipeline in inference (sample random noise and denoise)
>>> prompt = "A painting of a squirrel eating a burger"
>>> images = ldm([prompt], num_inference_steps=50, eta=0.3, guidance_scale=6).images

>>> # save images
>>> for idx, image in enumerate(images):
...     image.save(f"squirrel-{idx}.png")
```

## LDMSuperResolutionPipeline

### `class diffusers.LDMSuperResolutionPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py#L33)

```py
( vqvae: VQModel unet: UNet2DModel scheduler: Union )
```

参数

+   `vqvae` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) — 用于将图像编码和解码为潜在表示的矢量量化（VQ）模型。

+   `unet` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)) — 用于去噪编码图像的`UNet2DModel`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 一个调度器，与`unet`结合使用，用于去噪编码图像的潜在空间。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)，[EulerDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler#diffusers.EulerDiscreteScheduler)，[EulerAncestralDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler_ancestral#diffusers.EulerAncestralDiscreteScheduler)，[DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)，或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

使用潜在扩散进行图像超分辨率的管道。

此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py#L67)

```py
( image: Union = None batch_size: Optional = 1 num_inference_steps: Optional = 100 eta: Optional = 0.0 generator: Union = None output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `image` (`torch.Tensor` 或 `PIL.Image.Image`) — 代表要用作过程起点的图像批次的 `Image` 或张量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 要生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是一个普通的元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

如果 `return_dict` 为 `True`，将返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则将返回一个 `tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import requests
>>> from PIL import Image
>>> from io import BytesIO
>>> from diffusers import LDMSuperResolutionPipeline
>>> import torch

>>> # load model and scheduler
>>> pipeline = LDMSuperResolutionPipeline.from_pretrained("CompVis/ldm-super-resolution-4x-openimages")
>>> pipeline = pipeline.to("cuda")

>>> # let's download an  image
>>> url = (
...     "https://user-images.githubusercontent.com/38061659/199705896-b48e17b8-b231-47cd-a270-4ffa5a93fa3e.png"
... )
>>> response = requests.get(url)
>>> low_res_img = Image.open(BytesIO(response.content)).convert("RGB")
>>> low_res_img = low_res_img.resize((128, 128))

>>> # run pipeline in inference (sample random noise and denoise)
>>> upscaled_image = pipeline(low_res_img, num_inference_steps=100, eta=1).images[0]
>>> # save image
>>> upscaled_image.save("ldm_generated_image.png")
```

## 图像管道输出

### `class diffusers.ImagePipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)

```py
( images: Union )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

图像管道的输出类。
