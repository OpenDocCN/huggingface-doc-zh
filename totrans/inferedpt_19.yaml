- en: Use a custom Container Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/guides/custom_container](https://huggingface.co/docs/inference-endpoints/guides/custom_container)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Inference Endpoints not only allows you to [customize your inference handler](/docs/inference-endpoints/guides/custom_handler),
    but it also allows you to provide a custom container image. Those can be public
    images like `tensorflow/serving:2.7.3` or private Images hosted on [Docker Hub](https://hub.docker.com/),
    [AWS ECR](https://aws.amazon.com/ecr/?nc1=h_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/),
    or [Google GCR](https://cloud.google.com/container-registry?hl=de).
  prefs: []
  type: TYPE_NORMAL
- en: '![custom container config](../Images/2edd4889d6faa4c5faa3ba08780c78a9.png)'
  prefs: []
  type: TYPE_IMG
- en: The [creation flow](/docs/inference-endpoints/guides/create_endpoint) of your
    Image artifacts from a custom image is the same as the base image. This means
    Inference Endpoints will create a unique image artifact derived from your provided
    image, including all Model Artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Model Artifacts (weights) are stored under `/repository`. For example,
    if you use`tensorflow/serving` as your custom image, then you have to set `model_base_path=“/repository”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
