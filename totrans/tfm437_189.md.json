["```py\ninput_ids = tokenizer.encode(\"This is a sentence from [MASK] training data\", return_tensors=\"pt\")\nmlm_labels = tokenizer.encode(\"This is a sentence from the training data\", return_tensors=\"pt\")\n\nloss = model(input_ids, labels=input_ids, masked_lm_labels=mlm_labels)[0]\n```", "```py\n( attention_window: Union = 512 sep_token_id: int = 2 pad_token_id: int = 1 bos_token_id: int = 0 eos_token_id: int = 2 vocab_size: int = 30522 hidden_size: int = 768 num_hidden_layers: int = 12 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: str = 'gelu' hidden_dropout_prob: float = 0.1 attention_probs_dropout_prob: float = 0.1 max_position_embeddings: int = 512 type_vocab_size: int = 2 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 onnx_export: bool = False **kwargs )\n```", "```py\n>>> from transformers import LongformerConfig, LongformerModel\n\n>>> # Initializing a Longformer configuration\n>>> configuration = LongformerConfig()\n\n>>> # Initializing a model from the configuration\n>>> model = LongformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False **kwargs )\n```", "```py\n>>> from transformers import LongformerTokenizer\n\n>>> tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False trim_offsets = True **kwargs )\n```", "```py\n>>> from transformers import LongformerTokenizerFast\n\n>>> tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( last_hidden_state: FloatTensor hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( last_hidden_state: FloatTensor pooler_output: FloatTensor = None hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None start_logits: FloatTensor = None end_logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( last_hidden_state: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( last_hidden_state: tf.Tensor = None pooler_output: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None start_logits: tf.Tensor = None end_logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None global_attention_mask: Optional = None head_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import LongformerModel, AutoTokenizer\n\n>>> model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n\n>>> SAMPLE_TEXT = \" \".join([\"Hello world! \"] * 1000)  # long input document\n>>> input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n\n>>> attention_mask = torch.ones(\n...     input_ids.shape, dtype=torch.long, device=input_ids.device\n... )  # initialize to local attention\n>>> global_attention_mask = torch.zeros(\n...     input_ids.shape, dtype=torch.long, device=input_ids.device\n... )  # initialize to global attention to be deactivated for all tokens\n>>> global_attention_mask[\n...     :,\n...     [\n...         1,\n...         4,\n...         21,\n...     ],\n... ] = 1  # Set global attention to random tokens for the sake of this example\n>>> # Usually, set global attention based on the task. For example,\n>>> # classification: the <s> token\n>>> # QA: question tokens\n>>> # LM: potentially on the beginning of sentences and paragraphs\n>>> outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n>>> sequence_output = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None global_attention_mask: Optional = None head_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongformerForMaskedLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> model = LongformerForMaskedLM.from_pretrained(\"allenai/longformer-base-4096\")\n```", "```py\n>>> TXT = (\n...     \"My friends are <mask> but they eat too many carbs.\"\n...     + \" That's why I decide not to eat with them.\" * 300\n... )\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n\n>>> tokenizer.decode(predictions).split()\n['healthy', 'skinny', 'thin', 'good', 'vegetarian']\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None global_attention_mask: Optional = None head_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LongformerForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\")\n>>> model = LongformerForSequenceClassification.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'ORIGINAL'\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LongformerForSequenceClassification.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n5.44\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LongformerForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\")\n>>> model = LongformerForSequenceClassification.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LongformerForSequenceClassification.from_pretrained(\n...     \"jpwahle/longformer-base-plagiarism-detection\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None token_type_ids: Optional = None attention_mask: Optional = None global_attention_mask: Optional = None head_mask: Optional = None labels: Optional = None position_ids: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongformerForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> model = LongformerForMultipleChoice.from_pretrained(\"allenai/longformer-base-4096\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None global_attention_mask: Optional = None head_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongformerForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"brad1141/Longformer-finetuned-norm\")\n>>> model = LongformerForTokenClassification.from_pretrained(\"brad1141/Longformer-finetuned-norm\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes\n['Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence']\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n0.63\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None global_attention_mask: Optional = None head_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongformerForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n>>> model = LongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n>>> encoding = tokenizer(question, text, return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n\n>>> # default is local attention everywhere\n>>> # the forward method will automatically set global attention on question tokens\n>>> attention_mask = encoding[\"attention_mask\"]\n\n>>> outputs = model(input_ids, attention_mask=attention_mask)\n>>> start_logits = outputs.start_logits\n>>> end_logits = outputs.end_logits\n>>> all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n\n>>> answer_tokens = all_tokens[torch.argmax(start_logits) : torch.argmax(end_logits) + 1]\n>>> answer = tokenizer.decode(\n...     tokenizer.convert_tokens_to_ids(answer_tokens)\n... )  # remove space prepending space token\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False )\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLongformerForMaskedLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> model = TFLongformerForMaskedLM.from_pretrained(\"allenai/longformer-base-4096\")\n\n>>> inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"tf\")\n>>> logits = model(**inputs).logits\n\n>>> # retrieve index of <mask>\n>>> mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n>>> selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n\n>>> predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n>>> tokenizer.decode(predicted_token_id)\n' Paris'\n```", "```py\n>>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n>>> # mask labels of non-<mask> tokens\n>>> labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n\n>>> outputs = model(**inputs, labels=labels)\n>>> round(float(outputs.loss), 2)\n0.44\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLongformerForQuestionAnswering\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n>>> model = TFLongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n\n>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens)\n' puppet'\n```", "```py\n>>> # target is \"nice puppet\"\n>>> target_start_index = tf.constant([14])\n>>> target_end_index = tf.constant([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = tf.math.reduce_mean(outputs.loss)\n>>> round(float(loss), 2)\n0.96\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLongformerForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> model = TFLongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFLongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: Optional[Union[np.array, tf.Tensor]] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLongformerForTokenClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> model = TFLongformerForTokenClassification.from_pretrained(\"allenai/longformer-base-4096\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"tf\"\n... )\n\n>>> logits = model(**inputs).logits\n>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n```", "```py\n>>> labels = predicted_token_class_ids\n>>> loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLongformerForMultipleChoice\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n>>> model = TFLongformerForMultipleChoice.from_pretrained(\"allenai/longformer-base-4096\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"tf\", padding=True)\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}\n>>> outputs = model(inputs)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> logits = outputs.logits\n```"]