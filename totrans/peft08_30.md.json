["```py\n( model: PreTrainedModel peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n( adapter_name: str peft_config: PeftConfig )\n```", "```py\n( output_dir: str )\n```", "```py\n( )\n```", "```py\n>>> with model.disable_adapter():\n...     model(inputs)\n```", "```py\n( *args: Any **kwargs: Any )\n```", "```py\n( model: torch.nn.Module model_id: Union[str, os.PathLike] adapter_name: str = 'default' is_trainable: bool = False config: Optional[PeftConfig] = None **kwargs: Any )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( batch_size: int task_ids: Optional[torch.Tensor] = None )\n```", "```py\n( adapter_name: str )\n```", "```py\n( model_id: str adapter_name: str is_trainable: bool = False **kwargs: Any )\n```", "```py\n( )\n```", "```py\n( save_directory: str safe_serialization: bool = True selected_adapters: Optional[list[str]] = None save_embedding_layers: Union[str, bool] = 'auto' is_main_process: bool = True **kwargs: Any )\n```", "```py\n( adapter_name: str )\n```", "```py\n>>> for name, param in model_peft.named_parameters():\n...     if ...:  # some check on name (ex. if 'lora' in name)\n...         param.requires_grad = False\n```", "```py\n( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForSequenceClassification, get_peft_config\n\n>>> config = {\n...     \"peft_type\": \"PREFIX_TUNING\",\n...     \"task_type\": \"SEQ_CLS\",\n...     \"inference_mode\": False,\n...     \"num_virtual_tokens\": 20,\n...     \"token_dim\": 768,\n...     \"num_transformer_submodules\": 1,\n...     \"num_attention_heads\": 12,\n...     \"num_layers\": 12,\n...     \"encoder_hidden_size\": 768,\n...     \"prefix_projection\": False,\n...     \"postprocess_past_key_value_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```", "```py\n( model: torch.nn.Module peft_config: PeftConfig = None adapter_name: str = 'default' )\n```", "```py\n>>> from transformers import AutoModelForSequenceClassification\n>>> from peft import PeftModelForTokenClassification, get_peft_config\n\n>>> config = {\n...     \"peft_type\": \"PREFIX_TUNING\",\n...     \"task_type\": \"TOKEN_CLS\",\n...     \"inference_mode\": False,\n...     \"num_virtual_tokens\": 20,\n...     \"token_dim\": 768,\n...     \"num_transformer_submodules\": 1,\n...     \"num_attention_heads\": 12,\n...     \"num_layers\": 12,\n...     \"encoder_hidden_size\": 768,\n...     \"prefix_projection\": False,\n...     \"postprocess_past_key_value_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForTokenClassification(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n```", "```py\n( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModelForCausalLM, get_peft_config\n\n>>> config = {\n...     \"peft_type\": \"PREFIX_TUNING\",\n...     \"task_type\": \"CAUSAL_LM\",\n...     \"inference_mode\": False,\n...     \"num_virtual_tokens\": 20,\n...     \"token_dim\": 1280,\n...     \"num_transformer_submodules\": 1,\n...     \"num_attention_heads\": 20,\n...     \"num_layers\": 36,\n...     \"encoder_hidden_size\": 1280,\n...     \"prefix_projection\": False,\n...     \"postprocess_past_key_value_function\": None,\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n>>> peft_model = PeftModelForCausalLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n```", "```py\n( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM\n>>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n>>> config = {\n...     \"peft_type\": \"LORA\",\n...     \"task_type\": \"SEQ_2_SEQ_LM\",\n...     \"inference_mode\": False,\n...     \"r\": 8,\n...     \"target_modules\": [\"q\", \"v\"],\n...     \"lora_alpha\": 32,\n...     \"lora_dropout\": 0.1,\n...     \"fan_in_fan_out\": False,\n...     \"enable_lora\": None,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n```", "```py\n( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n>>> from transformers import AutoModelForQuestionAnswering\n>>> from peft import PeftModelForQuestionAnswering, get_peft_config\n\n>>> config = {\n...     \"peft_type\": \"LORA\",\n...     \"task_type\": \"QUESTION_ANS\",\n...     \"inference_mode\": False,\n...     \"r\": 16,\n...     \"target_modules\": [\"query\", \"value\"],\n...     \"lora_alpha\": 32,\n...     \"lora_dropout\": 0.05,\n...     \"fan_in_fan_out\": False,\n...     \"bias\": \"none\",\n... }\n\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)\n>>> peft_model.print_trainable_parameters()\ntrainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013\n```", "```py\n( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n>>> from transformers import AutoModel\n>>> from peft import PeftModelForFeatureExtraction, get_peft_config\n\n>>> config = {\n...     \"peft_type\": \"LORA\",\n...     \"task_type\": \"FEATURE_EXTRACTION\",\n...     \"inference_mode\": False,\n...     \"r\": 16,\n...     \"target_modules\": [\"query\", \"value\"],\n...     \"lora_alpha\": 32,\n...     \"lora_dropout\": 0.05,\n...     \"fan_in_fan_out\": False,\n...     \"bias\": \"none\",\n... }\n>>> peft_config = get_peft_config(config)\n>>> model = AutoModel.from_pretrained(\"bert-base-cased\")\n>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)\n>>> peft_model.print_trainable_parameters()\n```", "```py\n( model: nn.Module peft_config: PeftConfig adapter_name: str = 'default' )\n```", "```py\n>>> from peft import get_peft_model\n\n>>> base_model = ...  # load the base model, e.g. from transformers\n>>> peft_model = PeftMixedModel.from_pretrained(base_model, path_to_adapter1, \"adapter1\").eval()\n>>> peft_model.load_adapter(path_to_adapter2, \"adapter2\")\n>>> peft_model.set_adapter([\"adapter1\", \"adapter2\"])  # activate both adapters\n>>> peft_model(data)  # forward pass using both adapters\n```", "```py\n( )\n```", "```py\n( *args: Any **kwargs: Any )\n```", "```py\n( model: nn.Module model_id: str | os.PathLike adapter_name: str = 'default' is_trainable: bool = False config: Optional[PeftConfig] = None **kwargs: Any )\n```", "```py\n( *args: Any **kwargs: Any )\n```", "```py\n( )\n```", "```py\n( *args: Any **kwargs: Any )\n```", "```py\n( )\n```", "```py\n( adapter_name: Union[str, list[str]] )\n```", "```py\n>>> for name, param in model_peft.named_parameters():\n...     if ...:  # some check on name (ex. if 'lora' in name)\n...         param.requires_grad = False\n```", "```py\n( *args: Any **kwargs: Any )\n```", "```py\n( model dtype )\n```", "```py\n( model: PreTrainedModel peft_config: PeftConfig adapter_name: str = 'default' mixed: bool = False )\n```", "```py\n( peft_config: PeftConfig model: torch.nn.Module adapter_name: str = 'default' )\n```", "```py\n( model state_dict = None adapter_name = 'default' unwrap_compiled = False save_embedding_layers = 'auto' )\n```", "```py\n( model use_gradient_checkpointing = True gradient_checkpointing_kwargs = None )\n```"]