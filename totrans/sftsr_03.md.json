["```py\npip install safetensors huggingface_hub torch\n```", "```py\n>>> import os\n>>> import datetime\n>>> from huggingface_hub import hf_hub_download\n>>> from safetensors.torch import load_file\n>>> import torch\n```", "```py\n>>> sf_filename = hf_hub_download(\"gpt2\", filename=\"model.safetensors\")\n>>> pt_filename = hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\")\n```", "```py\n>>> start_st = datetime.datetime.now()\n>>> weights = load_file(sf_filename, device=\"cpu\")\n>>> load_time_st = datetime.datetime.now() - start_st\n>>> print(f\"Loaded safetensors {load_time_st}\")\n\n>>> start_pt = datetime.datetime.now()\n>>> weights = torch.load(pt_filename, map_location=\"cpu\")\n>>> load_time_pt = datetime.datetime.now() - start_pt\n>>> print(f\"Loaded pytorch {load_time_pt}\")\n\n>>> print(f\"on CPU, safetensors is faster than pytorch by: {load_time_pt/load_time_st:.1f} X\")\nLoaded safetensors 0:00:00.004015\nLoaded pytorch 0:00:00.307460\non CPU, safetensors is faster than pytorch by: 76.6 X\n```", "```py\n>>> # This is required because this feature hasn't been fully verified yet, but \n>>> # it's been tested on many different environments\n>>> os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n\n>>> # CUDA startup out of the measurement\n>>> torch.zeros((2, 2)).cuda()\n\n>>> start_st = datetime.datetime.now()\n>>> weights = load_file(sf_filename, device=\"cuda:0\")\n>>> load_time_st = datetime.datetime.now() - start_st\n>>> print(f\"Loaded safetensors {load_time_st}\")\n\n>>> start_pt = datetime.datetime.now()\n>>> weights = torch.load(pt_filename, map_location=\"cuda:0\")\n>>> load_time_pt = datetime.datetime.now() - start_pt\n>>> print(f\"Loaded pytorch {load_time_pt}\")\n\n>>> print(f\"on GPU, safetensors is faster than pytorch by: {load_time_pt/load_time_st:.1f} X\")\nLoaded safetensors 0:00:00.165206\nLoaded pytorch 0:00:00.353889\non GPU, safetensors is faster than pytorch by: 2.1 X\n```"]