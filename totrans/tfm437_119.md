# 优化

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules)

`.optimization`模块提供：

+   一个带有固定权重衰减的优化器，可用于微调模型，以及

+   `_LRSchedule`继承的形式有几个调度对象：

+   一个梯度累积类，用于累积多个批次的梯度

## AdamW（PyTorch）

### `class transformers.AdamW`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L396)

```py
( params: Iterable lr: float = 0.001 betas: Tuple = (0.9, 0.999) eps: float = 1e-06 weight_decay: float = 0.0 correct_bias: bool = True no_deprecation_warning: bool = False )
```

参数

+   `params`（`Iterable[nn.parameter.Parameter]`）— 要优化的参数的可迭代对象或定义参数组的字典。

+   `lr`（`float`，*可选*，默认为0.001）— 要使用的学习率。

+   `betas`（`Tuple[float,float]`，*可选*，默认为`(0.9, 0.999)`）— Adam的betas参数（b1，b2）。

+   `eps`（`float`，*可选*，默认为1e-06）— Adam的数值稳定性epsilon。

+   `weight_decay`（`float`，*可选*，默认为0.0）— 要应用的解耦权重衰减。

+   `correct_bias`（`bool`，*可选*，默认为`True`）— 是否在Adam中校正偏差（例如，在Bert TF存储库中，它们使用`False`）。

+   `no_deprecation_warning`（`bool`，*可选*，默认为`False`）— 用于禁用弃用警告的标志（设置为`True`以禁用警告）。

实现了带有权重衰减修复的Adam算法，该算法在[解耦权重衰减正则化](https://arxiv.org/abs/1711.05101)中引入。

#### `step`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L447)

```py
( closure: Callable = None )
```

参数

+   `closure`（`Callable`，*可选*）— 重新评估模型并返回损失的闭包。

执行单个优化步骤。

## AdaFactor（PyTorch）

### `class transformers.Adafactor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L510)

```py
( params lr = None eps = (1e-30, 0.001) clip_threshold = 1.0 decay_rate = -0.8 beta1 = None weight_decay = 0.0 scale_parameter = True relative_step = True warmup_init = False )
```

参数

+   `params`（`Iterable[nn.parameter.Parameter]`）— 要优化的参数的可迭代对象或定义参数组的字典。

+   `lr`（`float`，*可选*）— 外部学习率。

+   `eps`（`Tuple[float, float]`，*可选*，默认为`(1e-30, 0.001)`）— 平方梯度和参数比例的正则化常数

+   `clip_threshold`（`float`，*可选*，默认为1.0）— 最终梯度更新的均方根阈值

+   `decay_rate`（`float`，*可选*，默认为-0.8）— 用于计算平方运行平均值的系数

+   `beta1`（`float`，*可选*）— 用于计算梯度的运行平均值的系数

+   `weight_decay`（`float`，*可选*，默认为0.0）— 权重衰减（L2惩罚）

+   `scale_parameter`（`bool`，*可选*，默认为`True`）— 如果为True，则学习率将按均方根缩放

+   `relative_step`（`bool`，*可选*，默认为`True`）— 如果为True，则计算时间相关的学习率，而不是外部学习率

+   `warmup_init`（`bool`，*可选*，默认为`False`）— 时间相关的学习率计算取决于是否使用了热身初始化

AdaFactor的PyTorch实现可用作Adam原始fairseq代码的替代品：[https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py](https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py)

论文：*Adafactor：自适应学习率与亚线性内存成本* [https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235) 请注意，此优化器根据`scale_parameter`、`relative_step`和`warmup_init`选项内部调整学习率。要使用手动（外部）学习率调度，您应将`scale_parameter=False`和`relative_step=False`。

此实现处理低精度（FP16，bfloat）值，但我们尚未进行彻底测试。

推荐的T5微调设置（[https://discuss.huggingface.co/t/t5-finetuning-tips/684/3](https://discuss.huggingface.co/t/t5-finetuning-tips/684/3)）：

+   不建议在没有LR热身或clip_threshold的情况下进行训练。

    +   使用计划的LR热身到固定的LR

    +   使用clip_threshold=1.0 ([https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235))

+   禁用相对更新

+   使用scale_parameter=False

+   不应该在Adafactor旁边使用额外的优化器操作，如梯度裁剪。

示例：

```py
Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)
```

其他人报告以下组合效果很好：

```py
Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
```

当使用`lr=None`与[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)时，您很可能需要使用`AdafactorSchedule`

调度器如下：

```py
from transformers.optimization import Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))
```

用法：

```py
# replace AdamW with Adafactor
optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False,
)
```

#### `步骤`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L656)

```py
( closure = None )
```

参数

+   `闭包`（可调用，可选）— 重新评估模型并返回损失的闭包。

执行单个优化步骤

## AdamWeightDecay（TensorFlow）

### `class transformers.AdamWeightDecay`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L172)

```py
( learning_rate: Union = 0.001 beta_1: float = 0.9 beta_2: float = 0.999 epsilon: float = 1e-07 amsgrad: bool = False weight_decay_rate: float = 0.0 include_in_weight_decay: Optional = None exclude_from_weight_decay: Optional = None name: str = 'AdamWeightDecay' **kwargs )
```

参数

+   `learning_rate`（`Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]`，*可选*，默认为0.001）— 要使用的学习率或计划。

+   `beta_1`（`float`，*可选*，默认为0.9）— Adam中的beta1参数，即第1动量估计的指数衰减率。

+   `beta_2`（`float`，*可选*，默认为0.999）— Adam中的beta2参数，即第2动量估计的指数衰减率。

+   `epsilon`（`float`，*可选*，默认为1e-07）— Adam中的epsilon参数，这是用于数值稳定性的小常数。

+   `amsgrad`（`bool`，*可选*，默认为`False`）— 是否应用AMSGrad变体的算法，参见[关于Adam及其更多的收敛性](https://arxiv.org/abs/1904.09237)。

+   `weight_decay_rate`（`float`，*可选*，默认为0.0）— 要应用的权重衰减。

+   `include_in_weight_decay`（`List[str]`，*可选*）— 要应用权重衰减的参数名称（或re模式）的列表。如果没有传递，则默认情况下将权重衰减应用于所有参数（除非它们在`exclude_from_weight_decay`中）。

+   `exclude_from_weight_decay`（`List[str]`，*可选*）— 要排除不应用权重衰减的参数名称（或re模式）的列表。如果传递了`include_in_weight_decay`，则其中的名称将取代此列表。

+   `name`（`str`，*可选*，默认为`"AdamWeightDecay"`）— 应用梯度时创建的操作的可选名称。

+   `kwargs`（`Dict[str, Any]`，*可选*）— 关键字参数。允许为{`clipnorm`，`clipvalue`，`lr`，`decay`}。`clipnorm`是按范数裁剪梯度；`clipvalue`是按值裁剪梯度，`decay`包含了向后兼容性，允许学习率的时间反转衰减。`lr`包含了向后兼容性，建议使用`learning_rate`代替。

Adam启用L2权重衰减和梯度的全局范数裁剪。只是将权重的平方添加到损失函数中*不是*使用Adam进行L2正则化/权重衰减的正确方式，因为这将以奇怪的方式与m和v参数交互，如[解耦权重衰减正则化](https://arxiv.org/abs/1711.05101)所示。

相反，我们希望以一种不会与m/v参数交互的方式衰减权重。这相当于使用纯（非动量）SGD将权重的平方添加到损失中。

#### `from_config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L229)

```py
( config )
```

使用其配置创建具有WarmUp自定义对象的优化器。

#### `transformers.create_optimizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L88)

```py
( init_lr: float num_train_steps: int num_warmup_steps: int min_lr_ratio: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 adam_clipnorm: Optional = None adam_global_clipnorm: Optional = None weight_decay_rate: float = 0.0 power: float = 1.0 include_in_weight_decay: Optional = None )
```

参数

+   `init_lr`（`float`）—热身阶段结束时的期望学习率。

+   `num_train_steps`（`int`）—训练步骤的总数。

+   `num_warmup_steps`（`int`）—热身步骤的数量。

+   `min_lr_ratio`（`float`，*可选*，默认为0）—线性衰减结束时的最终学习率将为`init_lr * min_lr_ratio`。

+   `adam_beta1`（`float`，*可选*，默认为0.9）—Adam中使用的beta1。

+   `adam_beta2`（`float`，*可选*，默认为0.999）—Adam中使用的beta2。

+   `adam_epsilon`（`float`，*可选*，默认为1e-8）—Adam中使用的epsilon。

+   `adam_clipnorm`（`float`，*可选*，默认为`None`）—如果不是`None`，则将每个权重张量的梯度范数剪裁为此值。

+   `adam_global_clipnorm`（`float`，*可选*，默认为`None`）—如果不是`None`，则将梯度范数剪裁为此值。使用此参数时，规范是在所有权重张量上计算的，就好像它们被连接成一个单一向量。

+   `weight_decay_rate`（`float`，*可选*，默认为0）—要使用的权重衰减。

+   `power`（`float`，*可选*，默认为1.0）—用于PolynomialDecay的幂。

+   `include_in_weight_decay`（`List[str]`，*可选*）—要应用权重衰减的参数名称（或re模式）的列表。如果未传递任何内容，则将权重衰减应用于除偏置和层归一化参数之外的所有参数。

创建一个使用热身阶段后跟线性衰减的学习率时间表的优化器。

## 时间表

### 学习率时间表（Pytorch）

### `class transformers.SchedulerType`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L394)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

一个枚举。

#### `transformers.get_scheduler`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L338)

```py
( name: Union optimizer: Optimizer num_warmup_steps: Optional = None num_training_steps: Optional = None scheduler_specific_kwargs: Optional = None )
```

参数

+   `name`（`str`或`SchedulerType`）—要使用的调度程序的名称。

+   `optimizer`（`torch.optim.Optimizer`）—训练期间将使用的优化器。

+   `num_warmup_steps`（`int`，*可选*）—要执行的热身步骤数。并非所有调度程序都需要（因此参数是可选的），如果未设置并且调度程序类型需要，则函数将引发错误。

+   `num_training_steps`（`int`，*可选*）—要执行的训练步骤数。并非所有调度程序都需要（因此参数是可选的），如果未设置并且调度程序类型需要，则函数将引发错误。

+   `scheduler_specific_kwargs`（`dict`，*可选*）—用于诸如带重启的余弦等调度程序的额外参数。不匹配的调度程序类型和调度程序参数将导致调度程序函数引发TypeError。

从其名称获取任何调度程序的统一API。

#### `transformers.get_constant_schedule`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L39)

```py
( optimizer: Optimizer last_epoch: int = -1 )
```

参数

+   `optimizer`（`~torch.optim.Optimizer`）—要为其调度学习率的优化器。

+   `last_epoch`（`int`，*可选*，默认为-1）—恢复训练时的最后一个时期的索引。

使用优化器中设置的学习率创建一个具有恒定学习率的时间表。

#### `transformers.get_constant_schedule_with_warmup`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L80)

```py
( optimizer: Optimizer num_warmup_steps: int last_epoch: int = -1 )
```

参数

+   `optimizer`（`~torch.optim.Optimizer`）—要为其调度学习率的优化器。

+   `num_warmup_steps`（`int`）—热身阶段的步数。

+   `last_epoch`（`int`，*可选*，默认为-1）—恢复训练时的最后一个时期的索引。

创建一个具有恒定学习率的时间表，在此期间学习率在0和优化器中设置的初始lr之间线性增加的热身期之前。

![](../Images/72024b1b3539399498dd9d47bf49625e.png) #### `transformers.get_cosine_schedule_with_warmup`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L143)

```py
( optimizer: Optimizer num_warmup_steps: int num_training_steps: int num_cycles: float = 0.5 last_epoch: int = -1 )
```

参数

+   `optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。

+   `num_warmup_steps` (`int`) — 热身阶段的步数。

+   `num_training_steps` (`int`) — 总训练步数。

+   `num_cycles` (`float`, *optional*, defaults to 0.5) — 余弦计划中波数的数量（默认值是从最大值到0按照半余弦减少）。

+   `last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。

创建一个学习率随余弦函数值下降的计划，从优化器中设置的初始lr到0，经过一个热身阶段，在此期间学习率线性增加从0到优化器中设置的初始lr。

![](../Images/8f52d964e94bc88eb74e955db94a8a49.png) #### `transformers.get_cosine_with_hard_restarts_schedule_with_warmup`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L188)

```py
( optimizer: Optimizer num_warmup_steps: int num_training_steps: int num_cycles: int = 1 last_epoch: int = -1 )
```

参数

+   `optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。

+   `num_warmup_steps` (`int`) — 热身阶段的步数。

+   `num_training_steps` (`int`) — 总训练步数。

+   `num_cycles` (`int`, *optional*, defaults to 1) — 要使用的硬重启次数。

+   `last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。

创建一个学习率随余弦函数值下降的计划，从优化器中设置的初始lr到0，经过几次硬重启，在此期间学习率线性增加从0到优化器中设置的初始lr。

![](../Images/0a747d841c70030341cbfbcf4078836b.png) #### `transformers.get_linear_schedule_with_warmup`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L107)

```py
( optimizer num_warmup_steps num_training_steps last_epoch = -1 )
```

参数

+   `optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。

+   `num_warmup_steps` (`int`) — 热身阶段的步数。

+   `num_training_steps` (`int`) — 总训练步数。

+   `last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。

创建一个学习率从优化器中设置的初始lr线性下降到0的计划，在此期间学习率从0线性增加到优化器中设置的初始lr。

![](../Images/9e1bcf23c9d854d0c98549eec5391c06.png) #### `transformers.get_polynomial_decay_schedule_with_warmup`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L242)

```py
( optimizer num_warmup_steps num_training_steps lr_end = 1e-07 power = 1.0 last_epoch = -1 )
```

参数

+   `optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。

+   `num_warmup_steps` (`int`) — 热身阶段的步数。

+   `num_training_steps` (`int`) — 总训练步数。

+   `lr_end` (`float`, *optional*, defaults to 1e-7) — 最终LR。

+   `power` (`float`, *optional*, defaults to 1.0) — 功率因子。

+   `last_epoch` (`int`, *optional*, defaults to -1) — 恢复训练时的最后一个周期的索引。

创建一个学习率从优化器中设置的初始lr按多项式衰减到由*lr_end*定义的最终lr的计划，在此期间学习率从0线性增加到优化器中设置的初始lr。

注意：*power* 默认为1.0，与fairseq实现相同，fairseq实现又基于原始BERT实现 [https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37](https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37)

#### `transformers.get_inverse_sqrt_schedule`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization.py#L296)

```py
( optimizer: Optimizer num_warmup_steps: int timescale: int = None last_epoch: int = -1 )
```

参数

+   `optimizer` (`~torch.optim.Optimizer`) — 要调整学习率的优化器。

+   `num_warmup_steps` (`int`) — 热身阶段的步数。

+   `timescale` (`int`, *可选*, 默认为 `num_warmup_steps`) — 时间尺度。

+   `last_epoch` (`int`, *可选*, 默认为 -1) — 恢复训练时的最后一个时代的索引。

创建一个具有反平方根学习率的调度，从优化器中设置的初始lr开始，在一个热身期间之后，该期间将使lr从0线性增加到优化器中设置的初始lr。

### Warmup（TensorFlow）

### `类 transformers.WarmUp`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L30)

```py
( initial_learning_rate: float decay_schedule_fn: Callable warmup_steps: int power: float = 1.0 name: str = None )
```

参数

+   `initial_learning_rate` (`float`) — 热身后调度的初始学习率（这将是热身结束时的学习率）。

+   `decay_schedule_fn` (`Callable`) — 在热身后应用于剩余训练的调度函数。

+   `warmup_steps` (`int`) — 训练中热身阶段的步数。

+   `power` (`float`, *可选*, 默认为 1.0) — 用于多项式热身的幂（默认为线性热身）。

+   `name` (`str`, *可选*) — 调度期间返回张量的可选名称前缀。

对给定学习率衰减调度应用热身调度。

## 梯度策略

### GradientAccumulator（TensorFlow）

### `类 transformers.GradientAccumulator`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L302)

```py
( )
```

梯度累积实用程序。与分布策略一起使用时，应在副本上下文中调用累加器。梯度将在每个副本上本地累积，无需同步。然后用户应调用`.gradients`，根据需要缩放梯度，并将结果传递给`apply_gradients`。

#### `重置`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/optimization_tf.py#L364)

```py
( )
```

重置当前副本上累积的梯度。
