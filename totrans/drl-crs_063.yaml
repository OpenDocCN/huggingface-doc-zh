- en: Glossary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/glossary](https://huggingface.co/learn/deep-rl-course/unit4/glossary)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This is a community-created glossary. Contributions are welcome!
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Q-Learning:** A value-based deep reinforcement learning algorithm that
    uses a deep neural network to approximate Q-values for actions in a given state.
    The goal of Deep Q-learning is to find the optimal policy that maximizes the expected
    cumulative reward by learning the action-values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value-based methods:** Reinforcement Learning methods that estimate a value
    function as an intermediate step towards finding an optimal policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy-based methods:** Reinforcement Learning methods that directly learn
    to approximate the optimal policy without learning a value function. In practice
    they output a probability distribution over actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The benefits of using policy-gradient methods over value-based methods include:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'simplicity of integration: no need to store action values;'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ability to learn a stochastic policy: the agent explores the state space without
    always taking the same trajectory, and avoids the problem of perceptual aliasing;'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: effectiveness in high-dimensional and continuous action spaces; and
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: improved convergence properties.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy Gradient:** A subset of policy-based methods where the objective is
    to maximize the performance of a parameterized policy using gradient ascent. The
    goal of a policy-gradient is to control the probability distribution of actions
    by tuning the policy such that good actions (that maximize the return) are sampled
    more frequently in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo Reinforce:** A policy-gradient algorithm that uses an estimated
    return from an entire episode to update the policy parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to improve the course, you can [open a Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)
  prefs: []
  type: TYPE_NORMAL
- en: 'This glossary was made possible thanks to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Diego Carpintero](https://github.com/dcarpintero)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
