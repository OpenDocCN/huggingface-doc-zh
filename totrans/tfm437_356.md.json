["```py\n\"USER: <image>\\n<prompt>ASSISTANT:\"\n```", "```py\n\"USER: <image>\\n<prompt1>ASSISTANT: <answer1>USER: <prompt2>ASSISTANT: <answer2>USER: <prompt3>ASSISTANT:\"\n```", "```py\n( vision_config = None text_config = None ignore_index = -100 image_token_index = 32000 projector_hidden_act = 'gelu' vision_feature_select_strategy = 'default' vision_feature_layer = -2 vocab_size = 32000 **kwargs )\n```", "```py\n>>> from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig\n\n>>> # Initializing a CLIP-vision config\n>>> vision_config = CLIPVisionConfig()\n\n>>> # Initializing a Llama config\n>>> text_config = LlamaConfig()\n\n>>> # Initializing a Llava llava-1.5-7b style configuration\n>>> configuration = LlavaConfig(vision_config, text_config)\n\n>>> # Initializing a model from the llava-1.5-7b style configuration\n>>> model = LlavaForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor = None tokenizer = None )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: LlavaConfig )\n```", "```py\n( input_ids: LongTensor = None pixel_values: FloatTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None vision_feature_layer: Optional = None vision_feature_select_strategy: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, LlavaForConditionalGeneration\n\n>>> model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n>>> processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n\n>>> prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n>>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(**inputs, max_length=30)\n>>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"\\nUSER: What's the content of the image?\\nASSISTANT: The image features a stop sign on a street corner\"\n```"]