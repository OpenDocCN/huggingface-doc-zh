["```py\n\"USER: <image>\\n<prompt>ASSISTANT:\"\n```", "```py\n\"USER: <image>\\n<prompt1>ASSISTANT: <answer1>USER: <prompt2>ASSISTANT: <answer2>USER: <prompt3>ASSISTANT:\"\n```", "```py\n>>> from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig\n\n>>> # Initializing a CLIP-vision config\n>>> vision_config = CLIPVisionConfig()\n\n>>> # Initializing a Llama config\n>>> text_config = LlamaConfig()\n\n>>> # Initializing a Llava llava-1.5-7b style configuration\n>>> configuration = LlavaConfig(vision_config, text_config)\n\n>>> # Initializing a model from the llava-1.5-7b style configuration\n>>> model = LlavaForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, LlavaForConditionalGeneration\n\n>>> model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n>>> processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n\n>>> prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n>>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(**inputs, max_length=30)\n>>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"\\nUSER: What's the content of the image?\\nASSISTANT: The image features a stop sign on a street corner\"\n```"]