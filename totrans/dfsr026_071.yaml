- en: Reinforcement learning training with DDPO
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DDPOè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/ddpo](https://huggingface.co/docs/diffusers/training/ddpo)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/training/ddpo](https://huggingface.co/docs/diffusers/training/ddpo)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: You can fine-tune Stable Diffusion on a reward function via reinforcement learning
    with the ğŸ¤— TRL library and ğŸ¤— Diffusers. This is done with the Denoising Diffusion
    Policy Optimization (DDPO) algorithm introduced by Black et al. in [Training Diffusion
    Models with Reinforcement Learning](https://arxiv.org/abs/2305.13301), which is
    implemented in ğŸ¤— TRL with the [DDPOTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.DDPOTrainer).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ğŸ¤— TRLåº“å’ŒğŸ¤— Diffusersåœ¨å¥–åŠ±å‡½æ•°ä¸Šå¾®è°ƒç¨³å®šæ‰©æ•£ã€‚è¿™æ˜¯é€šè¿‡Blackç­‰äººåœ¨ã€Šä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‹ä¸­ä»‹ç»çš„Denoising
    Diffusion Policy Optimizationï¼ˆDDPOï¼‰ç®—æ³•å®Œæˆçš„ï¼Œè¯¥ç®—æ³•åœ¨ğŸ¤— TRLä¸­å®ç°ï¼Œå¹¶ä¸”åœ¨[Training Diffusion Models
    with Reinforcement Learning](https://arxiv.org/abs/2305.13301)ä¸­æœ‰ä»‹ç»ã€‚
- en: For more information, check out the [DDPOTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.DDPOTrainer)
    API reference and the [Finetune Stable Diffusion Models with DDPO via TRL](https://huggingface.co/blog/trl-ddpo)
    blog post.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[DDPOTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.DDPOTrainer)çš„APIå‚è€ƒå’Œ[é€šè¿‡TRLä½¿ç”¨DDPOå¾®è°ƒç¨³å®šæ‰©æ•£æ¨¡å‹](https://huggingface.co/blog/trl-ddpo)åšå®¢æ–‡ç« ã€‚
