["```py\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n\n>>> ## Input Japanese Text\n>>> line = \"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u3002\"\n\n>>> inputs = tokenizer(line, return_tensors=\"pt\")\n\n>>> print(tokenizer.decode(inputs[\"input_ids\"][0]))\n[CLS] \u543e\u8f29 \u306f \u732b \u3067 \u3042\u308b \u3002 [SEP]\n\n>>> outputs = bertjapanese(**inputs)\n```", "```py\n>>> bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n\n>>> ## Input Japanese Text\n>>> line = \"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u3002\"\n\n>>> inputs = tokenizer(line, return_tensors=\"pt\")\n\n>>> print(tokenizer.decode(inputs[\"input_ids\"][0]))\n[CLS] \u543e \u8f29 \u306f \u732b \u3067 \u3042 \u308b \u3002 [SEP]\n\n>>> outputs = bertjapanese(**inputs)\n```", "```py\n( vocab_file spm_file = None do_lower_case = False do_word_tokenize = True do_subword_tokenize = True word_tokenizer_type = 'basic' subword_tokenizer_type = 'wordpiece' never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' mecab_kwargs = None sudachi_kwargs = None jumanpp_kwargs = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```"]