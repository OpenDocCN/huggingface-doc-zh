# Kandinsky 3

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/kandinsky3`](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky3)

Kandinsky 3 是由[Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov), [Zein Shaheen](https://github.com/zeinsh)创建。

来自其 Github 页面的描述：

*Kandinsky 3.0 是基于 Kandinsky2-x 模型系列构建的开源文本到图像扩散模型。与其前身相比，通过增加文本编码器和 Diffusion U-Net 模型的大小，提高了模型的文本理解和视觉质量。*

其架构包括 3 个主要组件：

1.  [FLAN-UL2](https://huggingface.co/google/flan-ul2)，这是基于 T5 架构的编码器解码器模型。

1.  新的 U-Net 架构采用 BigGAN-deep 块，将深度加倍，同时保持相同数量的参数。

1.  Sber-MoVQGAN 是一个已被证明在图像恢复方面具有优越结果的解码器。

原始代码库可以在[ai-forever/Kandinsky-3](https://github.com/ai-forever/Kandinsky-3)找到。

查看 Hub 上的[Kandinsky 社区](https://huggingface.co/kandinsky-community)组织，获取官方模型检查点，用于文本到图像、图像到图像和修复等任务。

确保查看调度器的指南，了解如何探索调度器速度和质量之间的权衡，以及查看跨流水线重用组件部分，了解如何有效地将相同组件加载到多个流水线中。

## Kandinsky3Pipeline

### `class diffusers.Kandinsky3Pipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py#L49)

```py
( tokenizer: T5Tokenizer text_encoder: T5EncoderModel unet: Kandinsky3UNet scheduler: DDPMScheduler movq: VQModel )
```

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py#L338)

```py
( prompt: Union = None num_inference_steps: int = 25 guidance_scale: float = 3.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 height: Optional = 1024 width: Optional = 1024 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None attention_mask: Optional = None negative_attention_mask: Optional = None output_type: Optional = 'pil' return_dict: bool = True latents = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则必须传递`prompt_embeds`。

+   `num_inference_steps` (`int`, *optional*, 默认为 25) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `timesteps` (`List[int]`, *optional*) — 用于降噪过程的自定义时间步。如果未定义，则使用等间距的`num_inference_steps`时间步。必须按降序排列。

+   `guidance_scale` (`float`, *optional*, 默认为 3.0) — 如[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`来启用指导比例。更高的指导比例鼓励生成与文本`prompt`紧密相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。当不使用指导时（即，如果`guidance_scale`小于`1`，则忽略）。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `height` (`int`, *optional*, 默认为 self.unet.config.sample_size) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认为 self.unet.config.sample_size) — 生成图像的像素宽度。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η): [`arxiv.org/abs/2010.02502`](https://arxiv.org/abs/2010.02502)。仅适用于 schedulers.DDIMScheduler，对其他情况将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html) 以使生成过程确定性。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负文本嵌入。

+   `attention_mask` (`torch.FloatTensor`, *可选*) — 预生成的注意力掩码。如果直接传递 `prompt_embeds`，必须提供。

+   `negative_attention_mask` (`torch.FloatTensor`, *可选*) — 预生成的负注意力掩码。如果直接传递 `negative_prompt_embeds`，必须提供。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择 [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` 或 `np.array`。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 `~pipelines.stable_diffusion.IFPipelineOutput` 而不是普通元组。

+   `callback` (`Callable`, *可选*) — 在推断期间每 `callback_steps` 步调用的函数。该函数将使用以下参数调用: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.

+   `callback_steps` (`int`, *可选*, 默认为 1) — `callback` 函数被调用的频率。如果未指定，将在每一步调用回调函数。

+   `clean_caption` (`bool`, *可选*, 默认为 `True`) — 在创建嵌入之前是否清理标题。需要安装 `beautifulsoup4` 和 `ftfy`。如果依赖项未安装，嵌入将从原始提示中创建。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给 `AttentionProcessor` 中定义的 `self.processor` 下的 kwargs 字典，详见 [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)。

返回

ImagePipelineOutput 或 `tuple`

调用管道进行生成时调用的函数。

示例:

```py
>>> from diffusers import AutoPipelineForText2Image
>>> import torch

>>> pipe = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-3", variant="fp16", torch_dtype=torch.float16)
>>> pipe.enable_model_cpu_offload()

>>> prompt = "A photograph of the inside of a subway train. There are raccoons sitting on the seats. One of them is reading a newspaper. The window shows the city in the background."

>>> generator = torch.Generator(device="cpu").manual_seed(0)
>>> image = pipe(prompt, num_inference_steps=25, generator=generator).images[0]
```

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py#L95)

```py
( prompt do_classifier_free_guidance = True num_images_per_prompt = 1 device = None negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None _cut_context = False attention_mask: Optional = None negative_attention_mask: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示 device — (`torch.device`, *可选*): 放置生成的嵌入的 torch 设备

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`, *可选*, 默认为 `True`) — 是否使用分类器自由指导。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。如果未定义，则必须传递 `negative_prompt_embeds`。在不使用指导时被忽略 (即，如果 `guidance_scale` 小于 `1`，则被忽略)。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。  

+   `attention_mask`（`torch.FloatTensor`，*可选*）— 预生成的注意力蒙版。如果直接传递`prompt_embeds`，必须提供。  

+   `negative_attention_mask`（`torch.FloatTensor`，*可选*）— 预生成的负注意力蒙版。如果直接传递`negative_prompt_embeds`，必须提供。  

将提示编码为文本编码器隐藏状态。  

## Kandinsky3Img2ImgPipeline  

`class diffusers.Kandinsky3Img2ImgPipeline`  

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py#L62)  

```py
( tokenizer: T5Tokenizer text_encoder: T5EncoderModel unet: Kandinsky3UNet scheduler: DDPMScheduler movq: VQModel )
```

#### `__call__`  

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py#L412)

```py
( prompt: Union = None image: Union = None strength: float = 0.3 num_inference_steps: int = 25 guidance_scale: float = 3.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None attention_mask: Optional = None negative_attention_mask: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数  

+   `prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成的提示或提示。如果未定义，则必须传递`prompt_embeds`。  

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，`List[PIL.Image.Image]`或`List[np.ndarray]`）— 用作过程起点的`Image`或表示图像批次的张量。  

+   `strength`（`float`，*可选*，默认为 0.8）— 表示转换参考`image`的程度。必须介于 0 和 1 之间。`image`用作起点，`strength`越高，添加的噪音越多。去噪步骤的数量取决于最初添加的噪音量。当`strength`为 1 时，添加的噪音最大，去噪过程将运行指定的`num_inference_steps`的全部迭代次数。值为 1 实际上忽略了`image`。  

+   `num_inference_steps`（`int`，*可选*，默认为 50）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。  

+   `guidance_scale`（`float`，*可选*，默认为 3.0）— 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。  

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用引导时被忽略（即，如果`guidance_scale`小于`1`，则会被忽略）。  

+   `num_images_per_prompt`（`int`，*可选*，默认为 1）— 每个提示生成的图像数量。  

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)列表，以使生成过程确定性。  

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。  

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。  

+   `attention_mask`（`torch.FloatTensor`，*可选*）— 预生成的注意力蒙版。如果直接传递`prompt_embeds`，必须提供。  

+   `negative_attention_mask`（`torch.FloatTensor`，*可选*）— 预生成的负注意力蒙版。如果直接传递`negative_prompt_embeds`，必须提供。  

+   `output_type`（`str`，*可选*，默认为`"pil"`）—生成图像的输出格式。在[PIL](https://pillow.readthedocs.io/en/stable/)之间选择：`PIL.Image.Image`或`np.array`。

+   `return_dict`（`bool`，*可选*，默认为`True`）—是否返回`~pipelines.stable_diffusion.IFPipelineOutput`而不是普通元组。

+   `callback_on_step_end`（`Callable`，*可选*）—在推断期间每个去噪步骤结束时调用的函数。该函数使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs`（`List`，*可选*）—`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包括在您的管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

ImagePipelineOutput 或`tuple`

调用管道进行生成时调用的函数。

示例：

```py
>>> from diffusers import AutoPipelineForImage2Image
>>> from diffusers.utils import load_image
>>> import torch

>>> pipe = AutoPipelineForImage2Image.from_pretrained("kandinsky-community/kandinsky-3", variant="fp16", torch_dtype=torch.float16)
>>> pipe.enable_model_cpu_offload()

>>> prompt = "A painting of the inside of a subway train with tiny raccoons."
>>> image = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky3/t2i.png")

>>> generator = torch.Generator(device="cpu").manual_seed(0)
>>> image = pipe(prompt, image=image, strength=0.75, num_inference_steps=25, generator=generator).images[0]
```

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py#L118)

```py
( prompt do_classifier_free_guidance = True num_images_per_prompt = 1 device = None negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None _cut_context = False attention_mask: Optional = None negative_attention_mask: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）—要编码的提示

将提示编码为文本编码器隐藏状态。

设备：（`torch.device`，*可选*）：将生成的嵌入放置在的 torch 设备上 num_images_per_prompt（`int`，*可选*，默认为 1）：应该为每个提示生成的图像数量 do_classifier_free_guidance（`bool`，*可选*，默认为`True`）：是否使用分类器自由指导否则。如果未定义，必须传递`negative_prompt_embeds`。如果未定义，必须传递`negative_prompt_embeds`。不使用指导时忽略（即，如果`guidance_scale`小于`1`，则忽略）。prompt_embeds（`torch.FloatTensor`，*可选*）：预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。negative_prompt_embeds（`torch.FloatTensor`，*可选*）：预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负提示嵌入。attention_mask（`torch.FloatTensor`，*可选*）：预生成的注意力掩码。如果直接传递`prompt_embeds`，必须提供。negative_attention_mask（`torch.FloatTensor`，*可选*）：预生成的负注意力掩码。如果直接传递`negative_prompt_embeds`，必须提供。
