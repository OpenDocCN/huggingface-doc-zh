# ControlNet

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/controlnet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)

ControlNetæ˜¯ç”±Lvmin Zhangã€Anyi Raoå’ŒManeesh Agrawalaåœ¨[å‘æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ·»åŠ æ¡ä»¶æ§åˆ¶](https://huggingface.co/papers/2302.05543)ä¸­ä»‹ç»çš„ã€‚

ä½¿ç”¨ControlNetæ¨¡å‹ï¼Œæ‚¨å¯ä»¥æä¾›é¢å¤–çš„æ§åˆ¶å›¾åƒæ¥è°ƒèŠ‚å’Œæ§åˆ¶ç¨³å®šæ‰©æ•£ç”Ÿæˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æä¾›æ·±åº¦å›¾ï¼ŒControlNetæ¨¡å‹å°†ç”Ÿæˆä¸€å¹…ä¿ç•™æ·±åº¦å›¾ä¸­ç©ºé—´ä¿¡æ¯çš„å›¾åƒã€‚è¿™æ˜¯ä¸€ç§æ›´çµæ´»å’Œå‡†ç¡®çš„æ§åˆ¶å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ–¹å¼ã€‚

è®ºæ–‡æ‘˜è¦ï¼š

*æˆ‘ä»¬æå‡ºäº†ControlNetï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºå‘å¤§å‹ã€é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ·»åŠ ç©ºé—´æ¡ä»¶æ§åˆ¶ã€‚ControlNeté”å®šäº†ç”Ÿäº§å°±ç»ªçš„å¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡å¤ä½¿ç”¨å®ƒä»¬çš„æ·±åº¦å’Œç¨³å¥çš„ç¼–ç å±‚ï¼Œè¿™äº›å±‚ç»è¿‡æ•°åäº¿å›¾åƒçš„é¢„è®­ç»ƒï¼Œä½œä¸ºå­¦ä¹ å¤šæ ·åŒ–æ¡ä»¶æ§åˆ¶çš„å¼ºå¤§æ”¯æ’‘ã€‚ç¥ç»æ¶æ„è¿æ¥äº†â€œé›¶å·ç§¯â€ï¼ˆä»é›¶åˆå§‹åŒ–çš„å·ç§¯å±‚ï¼‰ï¼Œé€æ¸å¢åŠ å‚æ•°ï¼Œç¡®ä¿æ²¡æœ‰æœ‰å®³å™ªéŸ³ä¼šå½±å“å¾®è°ƒã€‚æˆ‘ä»¬ä½¿ç”¨ç¨³å®šæ‰©æ•£æµ‹è¯•äº†å„ç§æ¡ä»¶æ§åˆ¶ï¼Œä¾‹å¦‚è¾¹ç¼˜ã€æ·±åº¦ã€åˆ†å‰²ã€äººä½“å§¿åŠ¿ç­‰ï¼Œä½¿ç”¨å•ä¸ªæˆ–å¤šä¸ªæ¡ä»¶ï¼Œæœ‰æˆ–æ²¡æœ‰æç¤ºã€‚æˆ‘ä»¬å±•ç¤ºäº†ControlNetçš„è®­ç»ƒå¯¹äºå°å‹ï¼ˆ<50kï¼‰å’Œå¤§å‹ï¼ˆ>1mï¼‰æ•°æ®é›†æ˜¯ç¨³å¥çš„ã€‚å¹¿æ³›çš„ç»“æœè¡¨æ˜ï¼ŒControlNetå¯èƒ½ä¿ƒè¿›æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œä»¥æ§åˆ¶å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚*

è¯¥æ¨¡å‹ç”±[takuma104](https://huggingface.co/takuma104)è´¡çŒ®ã€‚â¤ï¸

åŸå§‹ä»£ç åº“å¯ä»¥åœ¨[lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)æ‰¾åˆ°ï¼Œæ‚¨å¯ä»¥åœ¨[lllyasvielçš„](https://huggingface.co/lllyasviel) Hubä¸ªäººèµ„æ–™ä¸­æ‰¾åˆ°å®˜æ–¹ControlNetæ£€æŸ¥ç‚¹ã€‚

ç¡®ä¿æŸ¥çœ‹è°ƒåº¦å™¨[æŒ‡å—](../../using-diffusers/schedulers)ï¼Œä»¥äº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŸ¥çœ‹[è·¨ç®¡é“é‡ç”¨ç»„ä»¶](../../using-diffusers/loading#reuse-components-across-pipelines)éƒ¨åˆ†ï¼Œä»¥äº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚

## StableDiffusionControlNetPipeline

### `class diffusers.StableDiffusionControlNetPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L139)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

å‚æ•°

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç ï¼Œå¹¶ä»æ½œåœ¨è¡¨ç¤ºä¸­è§£ç å›¾åƒã€‚

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14))ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„`CLIPTokenizer`ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ä¸€ä¸ª`UNet2DConditionModel`ï¼Œç”¨äºå»å™ªç¼–ç å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚

+   `controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)æˆ–`List[ControlNetModel]`) â€” åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¸º`unet`æä¾›é¢å¤–çš„æ¡ä»¶ã€‚å¦‚æœå°†å¤šä¸ªControlNetè®¾ç½®ä¸ºåˆ—è¡¨ï¼Œåˆ™æ¯ä¸ªControlNetçš„è¾“å‡ºå°†ç›¸åŠ ï¼Œä»¥åˆ›å»ºä¸€ä¸ªç»„åˆçš„é¢å¤–æ¡ä»¶ã€‚

+   `scheduler`ï¼ˆ[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)ï¼‰â€” ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„è°ƒåº¦ç¨‹åºã€‚å¯ä»¥æ˜¯[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œ[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)æˆ–[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)ä¹‹ä¸€ã€‚

+   `safety_checker`ï¼ˆ`StableDiffusionSafetyChecker`ï¼‰â€” ç”¨äºä¼°è®¡ç”Ÿæˆå›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§çš„åˆ†ç±»æ¨¡å—ã€‚è¯·å‚è€ƒ[æ¨¡å‹å¡ç‰‡](https://huggingface.co/runwayml/stable-diffusion-v1-5)ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

+   `feature_extractor`ï¼ˆ[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼‰â€” ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„`CLIPImageProcessor`ï¼›ä½œä¸º`å®‰å…¨æ£€æŸ¥å™¨`çš„è¾“å…¥ã€‚

ä½¿ç”¨å¸¦æœ‰ControlNetå¼•å¯¼çš„ç¨³å®šæ‰©æ•£è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æµæ°´çº¿ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æµæ°´çº¿å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥æµæ°´çº¿è¿˜ç»§æ‰¿ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) ç”¨äºåŠ è½½LoRAæƒé‡

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) ç”¨äºä¿å­˜LoRAæƒé‡

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) ç”¨äºåŠ è½½`.ckpt`æ–‡ä»¶

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) ç”¨äºåŠ è½½IPé€‚é…å™¨

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L894)

```py
( prompt: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 1.0 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) â†’ export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œ`List[PIL.Image.Image]`ï¼Œ`List[np.ndarray]`ï¼Œâ€” `List[List[torch.FloatTensor]]`ï¼Œ`List[List[np.ndarray]]`æˆ–`List[List[PIL.Image.Image]]`ï¼‰ï¼šæä¾›ç»™`unet`ç”ŸæˆæŒ‡å¯¼çš„ControlNetè¾“å…¥æ¡ä»¶ã€‚å¦‚æœç±»å‹æŒ‡å®šä¸º`torch.FloatTensor`ï¼Œåˆ™æŒ‰åŸæ ·ä¼ é€’ç»™ControlNetã€‚`PIL.Image.Image`ä¹Ÿå¯ä»¥ä½œä¸ºå›¾åƒæ¥å—ã€‚è¾“å‡ºå›¾åƒçš„å°ºå¯¸é»˜è®¤ä¸º`image`çš„å°ºå¯¸ã€‚å¦‚æœä¼ é€’äº†é«˜åº¦å’Œ/æˆ–å®½åº¦ï¼Œåˆ™ç›¸åº”è°ƒæ•´`image`çš„å¤§å°ã€‚å¦‚æœåœ¨`init`ä¸­æŒ‡å®šäº†å¤šä¸ªControlNetsï¼Œåˆ™å¿…é¡»å°†å›¾åƒä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼Œä»¥ä¾¿åˆ—è¡¨çš„æ¯ä¸ªå…ƒç´ å¯ä»¥æ­£ç¡®æ‰¹å¤„ç†ä¸ºå•ä¸ªControlNetçš„è¾“å…¥ã€‚å½“`prompt`æ˜¯ä¸€ä¸ªåˆ—è¡¨æ—¶ï¼Œå¦‚æœä¸ºå•ä¸ªControlNetä¼ é€’äº†å›¾åƒåˆ—è¡¨ï¼Œåˆ™æ¯ä¸ªå›¾åƒå°†ä¸`prompt`åˆ—è¡¨ä¸­çš„æ¯ä¸ªæç¤ºé…å¯¹ã€‚å¯¹äºå¤šä¸ªControlNetsï¼Œå¯ä»¥ä¼ é€’å›¾åƒåˆ—è¡¨çš„åˆ—è¡¨ä»¥æ‰¹å¤„ç†æ¯ä¸ªæç¤ºå’Œæ¯ä¸ªControlNetã€‚

+   `height`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `num_inference_steps` (`int`, *optional*, defaults to 50) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `timesteps` (`List[int]`, *optional*) â€” ç”¨äºå…·æœ‰åœ¨å…¶ `set_timesteps` æ–¹æ³•ä¸­æ”¯æŒ `timesteps` å‚æ•°çš„è°ƒåº¦å™¨çš„é™å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥ã€‚å¦‚æœæœªå®šä¹‰ï¼Œå°†ä½¿ç”¨ä¼ é€’ `num_inference_steps` æ—¶çš„é»˜è®¤è¡Œä¸ºã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) â€” æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` or `List[str]`, *optional*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œæ‚¨éœ€è¦ä¼ é€’ `negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶ï¼ˆ`guidance_scale < 1`ï¼‰å°†è¢«å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *optional*, defaults to 0.0) â€” å¯¹åº”äº[DDIM](https://arxiv.org/abs/2010.02502)è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œåœ¨å…¶ä»–è°ƒåº¦å™¨ä¸­å°†è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `latents` (`torch.FloatTensor`, *optional*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆçš„å˜ˆæ‚æ½œåœ¨å› ç´ ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œåœ¨å› ç´ å¼ é‡ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾å¾®è°ƒæ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾å¾®è°ƒæ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆ `negative_prompt_embeds`ã€‚

+   `output_type` (`str`, *optional*, defaults to `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ– `np.array` ä¹‹é—´ã€‚

+   `return_dict` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `callback` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *optional*, defaults to 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒã€‚

+   `cross_attention_kwargs` (`dict`, *optional*) â€” å¦‚æœæŒ‡å®šï¼Œå°†ä¼ é€’ç»™ [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) ä¸­å®šä¹‰çš„ `AttentionProcessor` çš„ kwargs å­—å…¸ã€‚

+   `controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults to 1.0) â€” æ§åˆ¶ç½‘ç»œçš„è¾“å‡ºåœ¨æ·»åŠ åˆ°åŸå§‹ `unet` ä¸­çš„æ®‹å·®ä¹‹å‰ä¼šä¹˜ä»¥ `controlnet_conditioning_scale`ã€‚å¦‚æœåœ¨ `init` ä¸­æŒ‡å®šäº†å¤šä¸ªæ§åˆ¶ç½‘ç»œï¼Œå¯ä»¥å°†ç›¸åº”çš„æ¯”ä¾‹è®¾ç½®ä¸ºåˆ—è¡¨ã€‚

+   `guess_mode` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” å³ä½¿åˆ é™¤æ‰€æœ‰æç¤ºï¼ŒControlNetç¼–ç å™¨ä¹Ÿä¼šå°è¯•è¯†åˆ«è¾“å…¥å›¾åƒçš„å†…å®¹ã€‚å»ºè®®ä½¿ç”¨`guidance_scale`å€¼åœ¨3.0åˆ°5.0ä¹‹é—´ã€‚

+   `control_guidance_start` (`float` or `List[float]`, *optional*, é»˜è®¤ä¸º0.0) â€” ControlNetå¼€å§‹åº”ç”¨çš„æ€»æ­¥éª¤ç™¾åˆ†æ¯”ã€‚

+   `control_guidance_end` (`float` or `List[float]`, *optional*, é»˜è®¤ä¸º1.0) â€” ControlNetåœæ­¢åº”ç”¨çš„æ€»æ­¥éª¤ç™¾åˆ†æ¯”ã€‚

+   `clip_skip` (`int`, *optional*) â€” è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦è·³è¿‡çš„CLIPå±‚æ•°ã€‚å€¼ä¸º1æ„å‘³ç€å°†ä½¿ç”¨é¢„ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

+   `callback_on_step_end` (`Callable`, *optional*) â€” åœ¨æ¨ç†æœŸé—´æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…å«ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è¿”å›

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) æˆ– `tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«â€œä¸å®‰å…¨å†…å®¹â€ï¼ˆnsfwï¼‰çš„ç”Ÿæˆå›¾åƒçš„å¸ƒå°”å€¼åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> # !pip install opencv-python transformers accelerate
>>> from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> import cv2
>>> from PIL import Image

>>> # download an image
>>> image = load_image(
...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
... )
>>> image = np.array(image)

>>> # get canny image
>>> image = cv2.Canny(image, 100, 200)
>>> image = image[:, :, None]
>>> image = np.concatenate([image, image, image], axis=2)
>>> canny_image = Image.fromarray(image)

>>> # load control net and stable diffusion v1-5
>>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
>>> pipe = StableDiffusionControlNetPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
... )

>>> # speed up diffusion process with faster scheduler and memory optimization
>>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
>>> # remove following line if xformers is not installed
>>> pipe.enable_xformers_memory_efficient_attention()

>>> pipe.enable_model_cpu_offload()

>>> # generate image
>>> generator = torch.manual_seed(0)
>>> image = pipe(
...     "futuristic-looking woman", num_inference_steps=20, generator=generator, image=canny_image
... ).images[0]
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

å‚æ•°

+   `slice_size` (`str` or `int`, *optional*, é»˜è®¤ä¸º`"auto"`) â€” å½“ä¸º`"auto"`æ—¶ï¼Œå°†è¾“å…¥å‡åŠåˆ°æ³¨æ„åŠ›å¤´éƒ¨ï¼Œå› æ­¤æ³¨æ„åŠ›å°†åˆ†ä¸¤æ­¥è®¡ç®—ã€‚å¦‚æœä¸º`"max"`ï¼Œå°†é€šè¿‡ä¸€æ¬¡è¿è¡Œä¸€ä¸ªåˆ‡ç‰‡æ¥èŠ‚çœæœ€å¤§å†…å­˜é‡ã€‚å¦‚æœæä¾›äº†ä¸€ä¸ªæ•°å­—ï¼Œåˆ™ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚

å¯ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªåˆ‡ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚å¯¹äºå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å°†æŒ‰é¡ºåºåœ¨æ¯ä¸ªå¤´ä¸Šæ‰§è¡Œã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜ä»¥æ¢å–ä¸€ç‚¹é€Ÿåº¦é™ä½æ˜¯æœ‰ç”¨çš„ã€‚

âš ï¸ å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨PyTorch 2.0æˆ–xFormersä¸­çš„`scaled_dot_product_attention`ï¼ˆSDPAï¼‰ï¼Œè¯·ä¸è¦å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ã€‚è¿™äº›æ³¨æ„åŠ›è®¡ç®—å·²ç»éå¸¸å†…å­˜é«˜æ•ˆï¼Œå› æ­¤æ‚¨ä¸éœ€è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚å¦‚æœæ‚¨åœ¨SDPAæˆ–xFormersä¸­å¯ç”¨äº†æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡å‡é€Ÿï¼

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¦‚æœä¹‹å‰è°ƒç”¨äº†`enable_attention_slicing`ï¼Œåˆ™ä¼šä¸€æ¬¡æ€§è®¡ç®—æ³¨æ„åŠ›ã€‚

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L237)

```py
( )
```

å¯ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç‰‡æ®µï¼Œä»¥ä¾¿åœ¨å¤šä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°å¾ˆæœ‰ç”¨ã€‚

#### `disable_vae_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L245)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `enable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op` (`Callable`, *å¯é€‰*) â€” ç”¨ä½œ`op`å‚æ•°ä¼ é€’ç»™xFormersçš„[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)å‡½æ•°çš„é»˜è®¤`None`æ“ä½œç¬¦çš„è¦†ç›–ã€‚

ä»[xFormers](https://facebookresearch.github.io/xformers/)å¯ç”¨å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ‚¨åº”è¯¥è§‚å¯Ÿåˆ°æ›´ä½çš„GPUå†…å­˜ä½¿ç”¨é‡ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ½œåœ¨çš„åŠ é€Ÿã€‚è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ é€Ÿä¸è¢«ä¿è¯ã€‚

âš ï¸ å½“å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å’Œåˆ‡ç‰‡æ³¨æ„åŠ›éƒ½å¯ç”¨æ—¶ï¼Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä¼˜å…ˆã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

ç¦ç”¨[xFormers](https://facebookresearch.github.io/xformers/)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚

#### `load_textual_inversion`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path` (`str`æˆ–`os.PathLike`æˆ–`List[stræˆ–os.PathLike]`æˆ–`Dict`æˆ–`List[Dict]`) â€” å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å®ƒä»¬çš„åˆ—è¡¨ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹Hubä¸Šæ‰˜ç®¡çš„*æ¨¡å‹ID*ï¼ˆä¾‹å¦‚`sd-concepts-library/low-poly-hd-logos-icons`ï¼‰ã€‚

    +   åŒ…å«æ–‡æœ¬åè½¬æƒé‡çš„*ç›®å½•*è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversion_directory/`ï¼‰ã€‚

    +   åŒ…å«æ–‡æœ¬åè½¬æƒé‡çš„*æ–‡ä»¶*è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversions.pt`ï¼‰ã€‚

    +   ä¸€ä¸ª[torchçŠ¶æ€å­—å…¸](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚

+   `token` (`str`æˆ–`List[str]`, *å¯é€‰*) â€” è¦†ç›–ç”¨äºæ–‡æœ¬åè½¬æƒé‡çš„ä»¤ç‰Œã€‚å¦‚æœ`pretrained_model_name_or_path`æ˜¯åˆ—è¡¨ï¼Œåˆ™`token`ä¹Ÿå¿…é¡»æ˜¯ç›¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel), *å¯é€‰*) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨self.tokenizerã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer), *å¯é€‰*) â€” ç”¨äºæ ‡è®°æ–‡æœ¬çš„`CLIPTokenizer`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨self.tokenizerã€‚

+   `weight_name` (`str`, *å¯é€‰*) â€” è‡ªå®šä¹‰æƒé‡æ–‡ä»¶çš„åç§°ã€‚åœ¨ä»¥ä¸‹æƒ…å†µä¸‹åº”ä½¿ç”¨æ­¤é€‰é¡¹ï¼š

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶ä»¥ğŸ¤— Diffusersæ ¼å¼ä¿å­˜ï¼Œä½†æ˜¯ä»¥ç‰¹å®šæƒé‡åç§°ï¼ˆä¾‹å¦‚`text_inv.bin`ï¼‰ä¿å­˜ã€‚

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶ä»¥Automatic1111æ ¼å¼ä¿å­˜ã€‚

+   `cache_dir` (`Union[str, os.PathLike]`, *å¯é€‰*) â€” ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚

+   `force_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ¢å¤ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™ä¼šåˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚

+   `proxies` (`Dict[str, str]`, *optional*) â€” ä¸€ä¸ªæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚ä»£ç†æœåŠ¡å™¨åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚

+   `local_files_only` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œæ¨¡å‹å°†ä¸ä¼šä»Hubä¸‹è½½ã€‚

+   `token` (`str` or *bool*, *optional*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶HTTPä»¤ç‰Œæˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚

+   `revision` (`str`, *optional*, defaults to `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤IDæˆ–Gitå…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `subfolder` (`str`, *optional*, defaults to `""`) â€” æ¨¡å‹æ–‡ä»¶åœ¨Hubæˆ–æœ¬åœ°è¾ƒå¤§æ¨¡å‹å­˜å‚¨åº“ä¸­çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚

+   `mirror` (`str`, *optional*) â€” é•œåƒæºï¼Œç”¨äºè§£å†³åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶çš„å¯è®¿é—®æ€§é—®é¢˜ã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

å°†æ–‡æœ¬åæ¼”åµŒå…¥åŠ è½½åˆ°[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼ˆæ”¯æŒğŸ¤— Diffuserså’ŒAutomatic1111æ ¼å¼ï¼‰ã€‚

ç¤ºä¾‹ï¼š

è¦åŠ è½½ğŸ¤— Diffusersæ ¼å¼çš„æ–‡æœ¬åæ¼”åµŒå…¥å‘é‡ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

è¦åŠ è½½Automatic1111æ ¼å¼çš„æ–‡æœ¬åæ¼”åµŒå…¥å‘é‡ï¼Œè¯·ç¡®ä¿é¦–å…ˆä¸‹è½½å‘é‡ï¼ˆä¾‹å¦‚ä»[civitAI](https://civitai.com/models/3036?modelVersionId=9857)ï¼‰ï¼Œç„¶ååŠ è½½å‘é‡

æœ¬åœ°ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L838)

```py
( )
```

å¦‚æœå·²å¯ç”¨FreeUæœºåˆ¶ï¼Œåˆ™ç¦ç”¨è¯¥æœºåˆ¶ã€‚

#### `disable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L262)

```py
( )
```

ç¦ç”¨å¹³é“ºå¼VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `enable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L815)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1` (`float`) â€” é˜¶æ®µ1çš„ç¼©æ”¾å› å­ï¼Œç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `s2` (`float`) â€” é˜¶æ®µ2çš„ç¼©æ”¾å› å­ï¼Œç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `b1` (`float`) â€” é˜¶æ®µ1çš„ç¼©æ”¾å› å­ï¼Œç”¨äºæ”¾å¤§ä¸»å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚

+   `b2` (`float`) â€” é˜¶æ®µ2çš„ç¼©æ”¾å› å­ï¼Œç”¨äºæ”¾å¤§ä¸»å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚

å¯ç”¨FreeUæœºåˆ¶ï¼Œå¦‚[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºå®ƒä»¬è¢«åº”ç”¨çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ï¼Œäº†è§£å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚Stable Diffusion v1ã€v2å’ŒStable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `enable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L253)

```py
( )
```

å¯ç”¨å¹³é“ºå¼VAEè§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªç“¦ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚

#### `encode_prompt`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L303)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” è¦ç¼–ç çš„æç¤ºè®¾å¤‡ â€” (`torch.device`): torch è®¾å¤‡

+   `num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™è¢«å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿé¢æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆ`negative_prompt_embeds`ã€‚

+   `lora_scale` (`float`, *å¯é€‰*) â€” å¦‚æœåŠ è½½äº† LoRA å±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰ LoRA å±‚çš„ LoRA æ¯”ä¾‹ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä» CLIP ä¸­è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

#### `get_guidance_scale_embedding`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L843)

```py
( w embedding_dim = 512 dtype = torch.float32 ) â†’ export const metadata = 'undefined';torch.FloatTensor
```

å‚æ•°

+   `timesteps` (`torch.Tensor`) â€” åœ¨è¿™äº›æ—¶é—´æ­¥ç”ŸæˆåµŒå…¥å‘é‡

+   `embedding_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” è¦ç”Ÿæˆçš„åµŒå…¥çš„ç»´åº¦ dtype â€” ç”Ÿæˆçš„åµŒå…¥çš„æ•°æ®ç±»å‹

è¿”å›

`torch.FloatTensor`

å½¢çŠ¶ä¸º`(len(timesteps), embedding_dim)`çš„åµŒå…¥å‘é‡

å‚è§ [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

## StableDiffusionControlNetImg2ImgPipeline

### `class diffusers.StableDiffusionControlNetImg2ImgPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L132)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

å‚æ•°

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆclip-vit-large-patch14ï¼‰ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„`CLIPTokenizer`ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„`UNet2DConditionModel`ã€‚

+   `controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel) æˆ– `List[ControlNetModel]`) â€” åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¸º`unet`æä¾›é¢å¤–çš„æ¡ä»¶ã€‚å¦‚æœå°†å¤šä¸ª ControlNet è®¾ç½®ä¸ºåˆ—è¡¨ï¼Œåˆ™æ¯ä¸ª ControlNet çš„è¾“å‡ºå°†ç›¸åŠ ä»¥åˆ›å»ºä¸€ä¸ªç»„åˆçš„é¢å¤–æ¡ä»¶ã€‚

+   `scheduler`ï¼ˆ[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)ï¼‰- ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚å¯ä»¥æ˜¯[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ã€[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)æˆ–[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)ä¹‹ä¸€ã€‚

+   `safety_checker`ï¼ˆ`StableDiffusionSafetyChecker`ï¼‰- ä¸€ä¸ªåˆ†ç±»æ¨¡å—ï¼Œç”¨äºä¼°è®¡ç”Ÿæˆçš„å›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§ã€‚è¯·å‚è€ƒ[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

+   `feature_extractor`ï¼ˆ[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼‰- ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„`CLIPImageProcessor`ï¼›ä½œä¸ºè¾“å…¥ä¼ é€’ç»™`safety_checker`ã€‚

ä½¿ç”¨Stable Diffusionå’ŒControlNetæŒ‡å¯¼è¿›è¡Œå›¾åƒç”Ÿæˆçš„æµæ°´çº¿ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æµæ°´çº¿å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥æµæ°´çº¿è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) ç”¨äºåŠ è½½LoRAæƒé‡

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) ç”¨äºä¿å­˜LoRAæƒé‡

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) ç”¨äºåŠ è½½`.ckpt`æ–‡ä»¶

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) ç”¨äºåŠ è½½IPé€‚é…å™¨

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L905)

```py
( prompt: Union = None image: Union = None control_image: Union = None height: Optional = None width: Optional = None strength: float = 0.8 num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 0.8 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) â†’ export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œ`List[PIL.Image.Image]`ï¼Œ`List[np.ndarray]`ï¼Œ- `List[List[torch.FloatTensor]]`ï¼Œ`List[List[np.ndarray]]`æˆ–`List[List[PIL.Image.Image]]`ï¼‰ï¼šç”¨ä½œå›¾åƒç”Ÿæˆè¿‡ç¨‹èµ·ç‚¹çš„åˆå§‹å›¾åƒã€‚ä¹Ÿå¯ä»¥æ¥å—å›¾åƒæ½œå˜é‡ä½œä¸º`image`ï¼Œå¦‚æœç›´æ¥ä¼ é€’æ½œå˜é‡ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `control_image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œ`List[PIL.Image.Image]`ï¼Œ`List[np.ndarray]`ï¼Œ- `List[List[torch.FloatTensor]]`ï¼Œ`List[List[np.ndarray]]`æˆ–`List[List[PIL.Image.Image]]`ï¼‰ï¼šæä¾›ç»™`unet`ç”ŸæˆæŒ‡å¯¼çš„ControlNetè¾“å…¥æ¡ä»¶ã€‚å¦‚æœç±»å‹æŒ‡å®šä¸º`torch.FloatTensor`ï¼Œåˆ™æŒ‰åŸæ ·ä¼ é€’ç»™ControlNetã€‚`PIL.Image.Image`ä¹Ÿå¯ä»¥ä½œä¸ºå›¾åƒæ¥å—ã€‚è¾“å‡ºå›¾åƒçš„å°ºå¯¸é»˜è®¤ä¸º`image`çš„å°ºå¯¸ã€‚å¦‚æœä¼ é€’äº†é«˜åº¦å’Œ/æˆ–å®½åº¦ï¼Œåˆ™ç›¸åº”åœ°è°ƒæ•´`image`çš„å¤§å°ã€‚å¦‚æœåœ¨`init`ä¸­æŒ‡å®šäº†å¤šä¸ªControlNetsï¼Œåˆ™å¿…é¡»å°†å›¾åƒä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼Œä»¥ä¾¿åˆ—è¡¨çš„æ¯ä¸ªå…ƒç´ å¯ä»¥æ­£ç¡®æ‰¹å¤„ç†ä¸ºå•ä¸ªControlNetçš„è¾“å…¥ã€‚

+   `height`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰- ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `num_inference_steps` (`int`, *optional*, defaults to 50) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) â€” æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` or `List[str]`, *optional*) â€” æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’ `negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶ï¼ˆ`guidance_scale < 1`ï¼‰å°†è¢«å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *optional*, defaults to 0.0) â€” å¯¹åº”äº [DDIM](https://arxiv.org/abs/2010.02502) è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `latents` (`torch.FloatTensor`, *optional*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆå˜ˆæ‚æ½œåœ¨å˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œåœ¨å˜é‡å¼ é‡ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿé¢æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œ`negative_prompt_embeds` å°†ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆã€‚ip_adapter_image â€” (`PipelineImageInput`, *optional*): å¯é€‰çš„å›¾åƒè¾“å…¥ï¼Œç”¨äºä¸ IP é€‚é…å™¨ä¸€èµ·ä½¿ç”¨ã€‚

+   `output_type` (`str`, *optional*, defaults to `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ– `np.array` ä¹‹é—´çš„ä¸€ä¸ªã€‚

+   `return_dict` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¿”å› [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `cross_attention_kwargs` (`dict`, *optional*) â€” å¦‚æœæŒ‡å®šï¼Œåˆ™ä½œä¸ºå‚æ•°ä¼ é€’ç»™ `AttentionProcessor`ï¼Œå¦‚ [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) ä¸­å®šä¹‰çš„é‚£æ ·ã€‚

+   `controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults to 1.0) â€” ControlNet çš„è¾“å‡ºåœ¨æ·»åŠ åˆ°åŸå§‹ `unet` ä¸­çš„æ®‹å·®ä¹‹å‰ä¼šä¹˜ä»¥ `controlnet_conditioning_scale`ã€‚å¦‚æœåœ¨ `init` ä¸­æŒ‡å®šäº†å¤šä¸ª ControlNetsï¼Œåˆ™å¯ä»¥å°†ç›¸åº”çš„æ¯”ä¾‹è®¾ç½®ä¸ºåˆ—è¡¨ã€‚

+   `guess_mode` (`bool`, *optional*, defaults to `False`) â€” ControlNet ç¼–ç å™¨å°è¯•è¯†åˆ«è¾“å…¥å›¾åƒçš„å†…å®¹ï¼Œå³ä½¿æ‚¨åˆ é™¤æ‰€æœ‰æç¤ºã€‚å»ºè®®ä½¿ç”¨ä»‹äº 3.0 å’Œ 5.0 ä¹‹é—´çš„ `guidance_scale` å€¼ã€‚

+   `control_guidance_start` (`float` or `List[float]`, *optional*, defaults to 0.0) â€” ControlNet å¼€å§‹åº”ç”¨çš„æ€»æ­¥éª¤ç™¾åˆ†æ¯”ã€‚

+   `control_guidance_end` (`float` or `List[float]`, *optional*, defaults to 1.0) â€” ControlNet åœæ­¢åº”ç”¨çš„æ€»æ­¥éª¤ç™¾åˆ†æ¯”ã€‚

+   `clip_skip` (`int`, *optional*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨å‰ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

+   `callback_on_step_end` (`Callable`, *optional*) â€” æ¨ç†æœŸé—´åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs` å°†åŒ…æ‹¬ç”± `callback_on_step_end_tensor_inputs` æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” ç”¨äº `callback_on_step_end` å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º `callback_kwargs` å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„ `._callback_tensor_inputs` å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è¿”å›

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) æˆ– `tuple`

å¦‚æœ `return_dict` ä¸º `True`ï¼Œå°†è¿”å› [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)ï¼Œå¦åˆ™å°†è¿”å›ä¸€ä¸ª `tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«æŒ‡ç¤ºç›¸åº”ç”Ÿæˆå›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹çš„ `bool` åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> # !pip install opencv-python transformers accelerate
>>> from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> import cv2
>>> from PIL import Image

>>> # download an image
>>> image = load_image(
...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
... )
>>> np_image = np.array(image)

>>> # get canny image
>>> np_image = cv2.Canny(np_image, 100, 200)
>>> np_image = np_image[:, :, None]
>>> np_image = np.concatenate([np_image, np_image, np_image], axis=2)
>>> canny_image = Image.fromarray(np_image)

>>> # load control net and stable diffusion v1-5
>>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
>>> pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
... )

>>> # speed up diffusion process with faster scheduler and memory optimization
>>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
>>> pipe.enable_model_cpu_offload()

>>> # generate image
>>> generator = torch.manual_seed(0)
>>> image = pipe(
...     "futuristic-looking woman",
...     num_inference_steps=20,
...     generator=generator,
...     image=image,
...     control_image=canny_image,
... ).images[0]
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

å‚æ•°

+   `slice_size` (`str` or `int`, *optional*, é»˜è®¤ä¸º `"auto"`) â€” å½“ä¸º `"auto"` æ—¶ï¼Œå°†è¾“å…¥å‡åŠç»™æ³¨æ„åŠ›å¤´ï¼Œå› æ­¤æ³¨æ„åŠ›å°†åœ¨ä¸¤ä¸ªæ­¥éª¤ä¸­è®¡ç®—ã€‚å¦‚æœä¸º `"max"`ï¼Œå°†é€šè¿‡ä¸€æ¬¡åªè¿è¡Œä¸€ä¸ªåˆ‡ç‰‡æ¥ä¿å­˜æœ€å¤§å†…å­˜é‡ã€‚å¦‚æœæä¾›äº†ä¸€ä¸ªæ•°å­—ï¼Œåˆ™ä½¿ç”¨ `attention_head_dim // slice_size` ä¸ªåˆ‡ç‰‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`attention_head_dim` å¿…é¡»æ˜¯ `slice_size` çš„å€æ•°ã€‚

å¯ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªåˆ‡ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚å¯¹äºå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å°†æŒ‰é¡ºåºåœ¨æ¯ä¸ªå¤´ä¸Šæ‰§è¡Œã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜ä»¥æ¢å–è½»å¾®çš„é€Ÿåº¦é™ä½å¾ˆæœ‰ç”¨ã€‚

âš ï¸ å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨ PyTorch 2.0 æˆ– xFormers çš„ `scaled_dot_product_attention` (SDPA)ï¼Œè¯·ä¸è¦å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ã€‚è¿™äº›æ³¨æ„åŠ›è®¡ç®—å·²ç»éå¸¸é«˜æ•ˆï¼Œå› æ­¤æ‚¨ä¸éœ€è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚å¦‚æœæ‚¨åœ¨ä½¿ç”¨ SDPA æˆ– xFormers å¯ç”¨äº†æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡å‡é€Ÿï¼

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¦‚æœä¹‹å‰è°ƒç”¨äº† `enable_attention_slicing`ï¼Œåˆ™æ³¨æ„åŠ›å°†åœ¨ä¸€æ­¥ä¸­è®¡ç®—ã€‚

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L230)

```py
( )
```

å¯ç”¨åˆ‡ç‰‡ VAE è§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAE å°†æŠŠè¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªåˆ‡ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°å¾ˆæœ‰ç”¨ã€‚

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L238)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡ VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº† `enable_vae_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°åœ¨ä¸€æ­¥ä¸­è®¡ç®—è§£ç ã€‚

#### `enable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰- ç”¨ä½œ`op`å‚æ•°ä¼ é€’ç»™xFormersçš„[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)å‡½æ•°çš„é»˜è®¤`None`è¿ç®—ç¬¦çš„è¦†ç›–ã€‚

å¯ç”¨[xFormers](https://facebookresearch.github.io/xformers/)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ‚¨åº”è¯¥è§‚å¯Ÿåˆ°è¾ƒä½çš„GPUå†…å­˜ä½¿ç”¨é‡å’Œæ½œåœ¨çš„æ¨ç†åŠ é€Ÿã€‚ä¸èƒ½ä¿è¯è®­ç»ƒæœŸé—´çš„åŠ é€Ÿã€‚

âš ï¸ å½“å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å’Œåˆ‡ç‰‡æ³¨æ„åŠ›éƒ½å¯ç”¨æ—¶ï¼Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä¼˜å…ˆã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

ç¦ç”¨[xFormers](https://facebookresearch.github.io/xformers/)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚

#### `load_textual_inversion`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`æˆ–`List[stræˆ–os.PathLike]`æˆ–`Dict`æˆ–`List[Dict]`ï¼‰- å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å®ƒä»¬çš„åˆ—è¡¨ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨Hubä¸Šæ‰˜ç®¡çš„*æ¨¡å‹id*ï¼ˆä¾‹å¦‚`sd-concepts-library/low-poly-hd-logos-icons`ï¼‰ã€‚

    +   ä¸€ä¸ª*ç›®å½•*çš„è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversion_directory/ï¼‰åŒ…å«æ–‡æœ¬åè½¬æƒé‡ã€‚

    +   ä¸€ä¸ª*æ–‡ä»¶*çš„è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversions.pt`ï¼‰åŒ…å«æ–‡æœ¬åè½¬æƒé‡ã€‚

    +   ä¸€ä¸ª[torchçŠ¶æ€å­—å…¸](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚

+   `token`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰- è¦†ç›–ç”¨äºæ–‡æœ¬åè½¬æƒé‡çš„ä»¤ç‰Œã€‚å¦‚æœ`pretrained_model_name_or_path`æ˜¯åˆ—è¡¨ï¼Œåˆ™`token`ä¹Ÿå¿…é¡»æ˜¯ç›¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚

+   `text_encoder`ï¼ˆ[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)ï¼Œ*å¯é€‰*ï¼‰- å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨self.tokenizerã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„`CLIPTokenizer`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨self.tokenizerã€‚

+   `weight_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰- è‡ªå®šä¹‰æƒé‡æ–‡ä»¶çš„åç§°ã€‚åº”åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨ï¼š

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶é‡‡ç”¨ğŸ¤— Diffusersæ ¼å¼ï¼Œä½†æ˜¯ä¿å­˜åœ¨ç‰¹å®šæƒé‡åç§°ä¸‹ï¼Œä¾‹å¦‚`text_inv.bin`ã€‚

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶é‡‡ç”¨Automatic1111æ ¼å¼ã€‚

+   `cache_dir`ï¼ˆ`Union[str, os.PathLike]`ï¼Œ*å¯é€‰*ï¼‰- ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚

+   `force_download`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- æ˜¯å¦ç»§ç»­ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™åˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚

+   `proxies`ï¼ˆ`Dict[str, str]`ï¼Œ*å¯é€‰*ï¼‰- ä¸€ä¸ªæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚

+   `local_files_only`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œæ¨¡å‹å°†ä¸ä¼šä»Hubä¸‹è½½ã€‚

+   `token`ï¼ˆ`str`æˆ–*bool*ï¼Œ*å¯é€‰*ï¼‰- ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚

+   `revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤IDæˆ–Gitå…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `subfolder` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`""`) â€” åœ¨Hubæˆ–æœ¬åœ°è¾ƒå¤§æ¨¡å‹å­˜å‚¨åº“ä¸­æ¨¡å‹æ–‡ä»¶çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚

+   `mirror` (`str`, *å¯é€‰*) â€” å¦‚æœæ‚¨åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°è®¿é—®é—®é¢˜ï¼Œè¯·å°†æºé•œåƒä»¥è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

å°†æ–‡æœ¬åè½¬åµŒå…¥åŠ è½½åˆ°[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼ˆæ”¯æŒğŸ¤— Diffuserså’ŒAutomatic1111æ ¼å¼ï¼‰ã€‚

ç¤ºä¾‹ï¼š

è¦ä»¥ğŸ¤— Diffusersæ ¼å¼åŠ è½½æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

è¦ä»¥Automatic1111æ ¼å¼åŠ è½½æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼Œè¯·ç¡®ä¿é¦–å…ˆä¸‹è½½å‘é‡ï¼ˆä¾‹å¦‚ä»[civitAI](https://civitai.com/models/3036?modelVersionId=9857)ï¼‰ç„¶ååŠ è½½å‘é‡

æœ¬åœ°ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L878)

```py
( )
```

å¦‚æœå¯ç”¨ï¼Œå°†ç¦ç”¨FreeUæœºåˆ¶ã€‚

#### `disable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L255)

```py
( )
```

ç¦ç”¨å¹³é“ºçš„VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `enable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L855)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1` (`float`) â€” ç”¨äºé˜»å°¼è·³è¿‡ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ1çš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆæœâ€ã€‚

+   `s2` (`float`) â€” ç”¨äºé˜»å°¼è·³è¿‡ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ2çš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆæœâ€ã€‚

+   `b1` (`float`) â€” ç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ1çš„ç¼©æ”¾å› å­ã€‚

+   `b2` (`float`) â€” ç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ2çš„ç¼©æ”¾å› å­ã€‚

å¯ç”¨FreeUæœºåˆ¶ï¼Œå¦‚[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)ä¸­æ‰€è¿°ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºå®ƒä»¬è¢«åº”ç”¨çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ä»¥è·å–å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚Stable Diffusion v1ã€v2å’ŒStable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `enable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L246)

```py
( )
```

å¯ç”¨å¹³é“ºçš„VAEè§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†å‰²ä¸ºç“¦ç‰‡ä»¥åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚

#### `encode_prompt`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L296)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str`æˆ–`List[str]`, *å¯é€‰*) â€” è¦ç¼–ç çš„æç¤ºè®¾å¤‡ â€” (`torch.device`): torchè®¾å¤‡

+   `num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼

+   `negative_prompt` (`str`æˆ–`List[str]`, *å¯é€‰*) â€” ä¸æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œ*ä¾‹å¦‚*æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œ*ä¾‹å¦‚*æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆnegative_prompt_embedsã€‚

+   `lora_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåŠ è½½äº†LoRAå±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰LoRAå±‚çš„LoRAæ¯”ä¾‹ã€‚

+   `clip_skip`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä»CLIPè·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º1æ„å‘³ç€å°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

## StableDiffusionControlNetInpaintPipeline

### `class diffusers.StableDiffusionControlNetInpaintPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L243)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

å‚æ•°

+   `vae`ï¼ˆ[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)ï¼‰â€”å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç ä»¥åŠä»æ½œåœ¨è¡¨ç¤ºåˆ°å›¾åƒçš„è§£ç ã€‚

+   `text_encoder`ï¼ˆ[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)ï¼‰â€”å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼‰â€”ç”¨äºæ ‡è®°æ–‡æœ¬çš„`CLIPTokenizer`ã€‚

+   `unet`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰â€”ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œåœ¨çŠ¶æ€çš„`UNet2DConditionModel`ã€‚

+   `controlnet`ï¼ˆ[ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)æˆ–`List[ControlNetModel]`ï¼‰â€”åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¸º`unet`æä¾›é¢å¤–çš„è°ƒèŠ‚ã€‚å¦‚æœå°†å¤šä¸ªControlNetsè®¾ç½®ä¸ºåˆ—è¡¨ï¼Œåˆ™æ¯ä¸ªControlNetçš„è¾“å‡ºå°†ç›¸åŠ ä»¥åˆ›å»ºä¸€ä¸ªç»„åˆçš„é¢å¤–è°ƒèŠ‚ã€‚

+   `scheduler`ï¼ˆ[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)ï¼‰â€”ä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œåœ¨çŠ¶æ€çš„è°ƒåº¦ç¨‹åºã€‚å¯ä»¥æ˜¯[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ã€[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)æˆ–[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)ä¹‹ä¸€ã€‚

+   `safety_checker`ï¼ˆ`StableDiffusionSafetyChecker`ï¼‰â€”ä¼°è®¡ç”Ÿæˆçš„å›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„åˆ†ç±»æ¨¡å—ã€‚æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)ã€‚

+   `feature_extractor`ï¼ˆ[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼‰â€”ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„`CLIPImageProcessor`ï¼›ä½œä¸º`safety_checker`çš„è¾“å…¥ã€‚

ä½¿ç”¨Stable Diffusionå’ŒControlNetæŒ‡å¯¼è¿›è¡Œå›¾åƒä¿®å¤çš„ç®¡é“ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥ç®¡é“è¿˜ç»§æ‰¿ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) ç”¨äºåŠ è½½ LoRA æƒé‡

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) ç”¨äºä¿å­˜ LoRA æƒé‡

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) ç”¨äºåŠ è½½`.ckpt`æ–‡ä»¶

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) ç”¨äºåŠ è½½ IP é€‚é…å™¨

è¿™ä¸ªç®¡é“å¯ä»¥ä¸ä¸“é—¨ä¸ºä¿®è¡¥ï¼ˆ[runwayml/stable-diffusion-inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting)ï¼‰è¿›è¡Œäº†ç‰¹å®šå¾®è°ƒçš„æ£€æŸ¥ç‚¹ä¸€èµ·ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¸é»˜è®¤çš„æ–‡æœ¬åˆ°å›¾åƒç¨³å®šæ‰©æ•£æ£€æŸ¥ç‚¹ä¸€èµ·ä½¿ç”¨ï¼ˆ[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)ï¼‰ã€‚é»˜è®¤çš„æ–‡æœ¬åˆ°å›¾åƒç¨³å®šæ‰©æ•£æ£€æŸ¥ç‚¹å¯èƒ½æ›´é€‚åˆå·²åœ¨è¿™äº›æ£€æŸ¥ç‚¹ä¸Šè¿›è¡Œäº†å¾®è°ƒçš„ControlNetsï¼Œæ¯”å¦‚[lllyasviel/control_v11p_sd15_inpaint](https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint)ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1115)

```py
( prompt: Union = None image: Union = None mask_image: Union = None control_image: Union = None height: Optional = None width: Optional = None padding_mask_crop: Optional = None strength: float = 1.0 num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 0.5 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) â†’ export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œâ€” `List[PIL.Image.Image]`æˆ–`List[np.ndarray]`ï¼‰ï¼šè¡¨ç¤ºè¦ç”¨ä½œèµ·ç‚¹çš„å›¾åƒæ‰¹æ¬¡çš„`Image`ï¼ŒNumPyæ•°ç»„æˆ–å¼ é‡ã€‚å¯¹äºNumPyæ•°ç»„å’ŒPyTorchå¼ é‡ï¼ŒæœŸæœ›å€¼èŒƒå›´åœ¨`[0, 1]`ä¹‹é—´ã€‚å¦‚æœæ˜¯å¼ é‡æˆ–å¼ é‡åˆ—è¡¨ï¼Œåˆ™æœŸæœ›å½¢çŠ¶åº”ä¸º`(B, C, H, W)`æˆ–`(C, H, W)`ã€‚å¦‚æœæ˜¯NumPyæ•°ç»„æˆ–æ•°ç»„åˆ—è¡¨ï¼Œåˆ™æœŸæœ›å½¢çŠ¶åº”ä¸º`(B, H, W, C)`æˆ–`(H, W, C)`ã€‚å®ƒè¿˜å¯ä»¥æ¥å—å›¾åƒæ½œå˜é‡ä½œä¸º`image`ï¼Œä½†å¦‚æœç›´æ¥ä¼ é€’æ½œå˜é‡ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `mask_image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œâ€” `List[PIL.Image.Image]`æˆ–`List[np.ndarray]`ï¼‰ï¼šè¡¨ç¤ºè¦é®ç½©`image`çš„å›¾åƒæ‰¹æ¬¡çš„`Image`ï¼ŒNumPyæ•°ç»„æˆ–å¼ é‡ã€‚é®ç½©ä¸­çš„ç™½è‰²åƒç´ è¢«é‡æ–°ç»˜åˆ¶ï¼Œè€Œé»‘è‰²åƒç´ è¢«ä¿ç•™ã€‚å¦‚æœ`mask_image`æ˜¯PILå›¾åƒï¼Œåˆ™åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶è½¬æ¢ä¸ºå•é€šé“ï¼ˆäº®åº¦ï¼‰ã€‚å¦‚æœæ˜¯NumPyæ•°ç»„æˆ–PyTorchå¼ é‡ï¼Œåˆ™åº”åŒ…å«ä¸€ä¸ªé¢œè‰²é€šé“ï¼ˆLï¼‰è€Œä¸æ˜¯3ï¼Œå› æ­¤PyTorchå¼ é‡çš„é¢„æœŸå½¢çŠ¶ä¸º`(B, 1, H, W)`ï¼Œ`(B, H, W)`ï¼Œ`(1, H, W)`ï¼Œ`(H, W)`ã€‚å¯¹äºNumPyæ•°ç»„ï¼Œé¢„æœŸå½¢çŠ¶ä¸º`(B, H, W, 1)`ï¼Œ`(B, H, W)`ï¼Œ`(H, W, 1)`æˆ–`(H, W)`ã€‚

+   `control_image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`List[torch.FloatTensor]`ï¼Œ`List[PIL.Image.Image]`ï¼Œâ€” `List[List[torch.FloatTensor]]`æˆ–`List[List[PIL.Image.Image]]`ï¼‰ï¼šæä¾›æŒ‡å¯¼ç»™`unet`ä»¥ç”Ÿæˆçš„ControlNetè¾“å…¥æ¡ä»¶ã€‚å¦‚æœç±»å‹æŒ‡å®šä¸º`torch.FloatTensor`ï¼Œåˆ™æŒ‰åŸæ ·ä¼ é€’ç»™ControlNetã€‚`PIL.Image.Image`ä¹Ÿå¯ä»¥ä½œä¸ºå›¾åƒæ¥å—ã€‚è¾“å‡ºå›¾åƒçš„å°ºå¯¸é»˜è®¤ä¸º`image`çš„å°ºå¯¸ã€‚å¦‚æœä¼ é€’äº†é«˜åº¦å’Œ/æˆ–å®½åº¦ï¼Œ`image`å°†ç›¸åº”è°ƒæ•´å¤§å°ã€‚å¦‚æœåœ¨`init`ä¸­æŒ‡å®šäº†å¤šä¸ªControlNetsï¼Œåˆ™å¿…é¡»å°†å›¾åƒä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼Œä»¥ä¾¿åˆ—è¡¨çš„æ¯ä¸ªå…ƒç´ å¯ä»¥æ­£ç¡®æ‰¹å¤„ç†è¾“å…¥åˆ°å•ä¸ªControlNetã€‚

+   `height`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `padding_mask_crop` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” è¦åº”ç”¨äºå›¾åƒå’Œé®ç½©çš„è£å‰ªè¾¹è·å¤§å°ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¸å¯¹å›¾åƒå’Œ mask_image è¿›è¡Œè£å‰ªã€‚å¦‚æœ`padding_mask_crop`ä¸æ˜¯`None`ï¼Œå®ƒå°†é¦–å…ˆæ‰¾åˆ°ä¸€ä¸ªå…·æœ‰ä¸å›¾åƒç›¸åŒçºµæ¨ªæ¯”ä¸”åŒ…å«æ‰€æœ‰é®ç½©åŒºåŸŸçš„çŸ©å½¢åŒºåŸŸï¼Œç„¶åæ ¹æ®`padding_mask_crop`æ‰©å±•è¯¥åŒºåŸŸã€‚ç„¶åæ ¹æ®æ‰©å±•çš„åŒºåŸŸå¯¹å›¾åƒå’Œ mask_image è¿›è¡Œè£å‰ªï¼Œç„¶åå°†å…¶è°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°ä»¥è¿›è¡Œä¿®å¤ã€‚å½“é®ç½©åŒºåŸŸè¾ƒå°è€Œå›¾åƒè¾ƒå¤§ä¸”åŒ…å«ä¸ä¿®å¤æ— å…³çš„ä¿¡æ¯æ—¶ï¼Œè¿™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œæ¯”å¦‚èƒŒæ™¯ã€‚

+   `strength` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1.0) â€” æŒ‡ç¤ºè½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚`image`è¢«ç”¨ä½œèµ·ç‚¹ï¼Œ`strength`è¶Šé«˜ï¼Œæ·»åŠ çš„å™ªéŸ³å°±è¶Šå¤šã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“`strength`ä¸º 1 æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³æ˜¯æœ€å¤§çš„ï¼Œå»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`æ¬¡è¿­ä»£ã€‚å€¼ä¸º 1 å®è´¨ä¸Šå¿½ç•¥äº†`image`ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 50) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 7.5) â€” æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`ç´§å¯†ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale > 1`æ—¶å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” æŒ‡å¯¼ä¸åŒ…æ‹¬åœ¨å›¾åƒç”Ÿæˆä¸­çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆ`guidance_scale < 1`ï¼‰ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” å¯¹åº”äº[DDIM](https://arxiv.org/abs/2010.02502)è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­æŠ½æ ·çš„é¢„ç”Ÿæˆå˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡ŒæŠ½æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œ`negative_prompt_embeds`å°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚ip_adapter_image â€” (`PipelineImageInput`, *å¯é€‰*): å¯é€‰çš„å›¾åƒè¾“å…¥ä»¥ä¸ IP é€‚é…å™¨ä¸€èµ·ä½¿ç”¨ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©`PIL.Image`æˆ–`np.array`ä¹‹é—´ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `cross_attention_kwargs`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœæŒ‡å®šäº†kwargså­—å…¸ï¼Œåˆ™å°†å…¶ä¼ é€’ç»™[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)ä¸­å®šä¹‰çš„`AttentionProcessor`ã€‚

+   `controlnet_conditioning_scale`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰â€” åœ¨å°†è¾“å‡ºæ·»åŠ åˆ°åŸå§‹`unet`ä¸­çš„æ®‹å·®ä¹‹å‰ï¼ŒControlNetçš„è¾“å‡ºå°†ä¹˜ä»¥`controlnet_conditioning_scale`ã€‚å¦‚æœåœ¨`init`ä¸­æŒ‡å®šäº†å¤šä¸ªControlNetsï¼Œæ‚¨å¯ä»¥å°†ç›¸åº”çš„æ¯”ä¾‹è®¾ç½®ä¸ºåˆ—è¡¨ã€‚

+   `guess_mode`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” ControlNetç¼–ç å™¨å°è¯•è¯†åˆ«è¾“å…¥å›¾åƒçš„å†…å®¹ï¼Œå³ä½¿æ‚¨åˆ é™¤æ‰€æœ‰æç¤ºã€‚å»ºè®®ä½¿ç”¨ä»‹äº3.0å’Œ5.0ä¹‹é—´çš„`guidance_scale`å€¼ã€‚

+   `control_guidance_start`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” ControlNetå¼€å§‹åº”ç”¨çš„æ€»æ­¥éª¤çš„ç™¾åˆ†æ¯”ã€‚

+   `control_guidance_end`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1.0ï¼‰â€” ControlNetåœæ­¢åº”ç”¨çš„æ€»æ­¥éª¤çš„ç™¾åˆ†æ¯”ã€‚

+   `clip_skip`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä»CLIPä¸­è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º1æ„å‘³ç€å°†ä½¿ç”¨å‰ä¸€å±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

+   `callback_on_step_end`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨æ¨ç†æœŸé—´æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…æ‹¬ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs`ï¼ˆ`List`ï¼Œ*å¯é€‰*ï¼‰â€” `callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è¿”å›

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)æˆ–`tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«æŒ‡ç¤ºç›¸åº”ç”Ÿæˆå›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹çš„`bool`åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> # !pip install transformers accelerate
>>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> init_image = load_image(
...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png"
... )
>>> init_image = init_image.resize((512, 512))

>>> generator = torch.Generator(device="cpu").manual_seed(1)

>>> mask_image = load_image(
...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png"
... )
>>> mask_image = mask_image.resize((512, 512))

>>> def make_canny_condition(image):
...     image = np.array(image)
...     image = cv2.Canny(image, 100, 200)
...     image = image[:, :, None]
...     image = np.concatenate([image, image, image], axis=2)
...     image = Image.fromarray(image)
...     return image

>>> control_image = make_canny_condition(init_image)

>>> controlnet = ControlNetModel.from_pretrained(
...     "lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16
... )
>>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
... )

>>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
>>> pipe.enable_model_cpu_offload()

>>> # generate image
>>> image = pipe(
...     "a handsome man with ray-ban sunglasses",
...     num_inference_steps=20,
...     generator=generator,
...     eta=1.0,
...     image=init_image,
...     mask_image=mask_image,
...     control_image=control_image,
... ).images[0]
```

#### `enable_attention_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

å‚æ•°

+   `slice_size`ï¼ˆ`str`æˆ–`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"auto"`ï¼‰â€” å½“ä¸º`"auto"`æ—¶ï¼Œå°†è¾“å…¥å‡åŠåˆ°æ³¨æ„åŠ›å¤´éƒ¨ï¼Œå› æ­¤æ³¨æ„åŠ›å°†åœ¨ä¸¤ä¸ªæ­¥éª¤ä¸­è®¡ç®—ã€‚å¦‚æœä¸º`"max"`ï¼Œå°†é€šè¿‡ä¸€æ¬¡åªè¿è¡Œä¸€ä¸ªåˆ‡ç‰‡æ¥èŠ‚çœæœ€å¤§å†…å­˜é‡ã€‚å¦‚æœæä¾›äº†ä¸€ä¸ªæ•°å­—ï¼Œåˆ™ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚

å¯ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç‰‡æ®µï¼Œä»¥ä¾¿åœ¨å¤šä¸ªæ­¥éª¤ä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚å¯¹äºå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å°†æŒ‰é¡ºåºåœ¨æ¯ä¸ªå¤´ä¸Šæ‰§è¡Œã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜ä»¥æ¢å–ä¸€ç‚¹é€Ÿåº¦é™ä½æ˜¯æœ‰ç”¨çš„ã€‚

âš ï¸ å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨PyTorch 2.0æˆ–xFormersä¸­çš„`scaled_dot_product_attention`ï¼ˆSDPAï¼‰ï¼Œè¯·ä¸è¦å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ã€‚è¿™äº›æ³¨æ„åŠ›è®¡ç®—å·²ç»éå¸¸å†…å­˜é«˜æ•ˆï¼Œå› æ­¤æ‚¨ä¸éœ€è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚å¦‚æœæ‚¨åœ¨SDPAæˆ–xFormersä¸­å¯ç”¨äº†æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡å‡é€Ÿï¼

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¦‚æœä¹‹å‰è°ƒç”¨äº†`enable_attention_slicing`ï¼Œåˆ™æ³¨æ„åŠ›ä¼šåœ¨ä¸€æ­¥ä¸­è®¡ç®—ã€‚

å¯ç”¨VAEåˆ‡ç‰‡

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L355)

```py
( )
```

å¯ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç‰‡æ®µï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°å¾ˆæœ‰ç”¨ã€‚

ç¦ç”¨VAEåˆ‡ç‰‡

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L363)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

å¯ç”¨xFormersçš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰-ç”¨ä½œxFormersçš„[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)å‡½æ•°çš„`op`å‚æ•°çš„é»˜è®¤`None`è¿ç®—ç¬¦çš„è¦†ç›–ã€‚

ä»[xFormers](https://facebookresearch.github.io/xformers/)å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ‚¨åº”è¯¥è§‚å¯Ÿåˆ°æ›´ä½çš„GPUå†…å­˜ä½¿ç”¨é‡ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½ä¼šåŠ é€Ÿã€‚è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ é€Ÿä¸è¢«ä¿è¯ã€‚

âš ï¸å½“å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å’Œåˆ‡ç‰‡æ³¨æ„åŠ›éƒ½å¯ç”¨æ—¶ï¼Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä¼˜å…ˆã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

ç¦ç”¨xFormersçš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

ä»[xFormers](https://facebookresearch.github.io/xformers/)ä¸­ç¦ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚

åŠ è½½æ–‡æœ¬åè½¬

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`æˆ–`List[stræˆ–os.PathLike]`æˆ–`Dict`æˆ–`List[Dict]`ï¼‰-å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å®ƒä»¬çš„åˆ—è¡¨ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹Hubä¸Šæ‰˜ç®¡çš„*æ¨¡å‹ID*ï¼ˆä¾‹å¦‚`sd-concepts-library/low-poly-hd-logos-icons`ï¼‰ã€‚

    +   ä¸€ä¸ª*ç›®å½•*çš„è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversion_directory/`ï¼‰ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬åè½¬æƒé‡ã€‚

    +   ä¸€ä¸ª*æ–‡ä»¶*çš„è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversions.pt`ï¼‰ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬åè½¬æƒé‡ã€‚

    +   ä¸€ä¸ª[torchçŠ¶æ€å­—å…¸](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚

+   `token`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰-è¦†ç›–ç”¨äºæ–‡æœ¬åè½¬æƒé‡çš„ä»¤ç‰Œã€‚å¦‚æœ`pretrained_model_name_or_path`æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ™`token`ä¹Ÿå¿…é¡»æ˜¯ç›¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚

+   `text_encoder`ï¼ˆ[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)ï¼Œ*å¯é€‰*ï¼‰-å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†é‡‡ç”¨self.tokenizerã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼Œ*å¯é€‰*ï¼‰-ç”¨äºæ ‡è®°æ–‡æœ¬çš„`CLIPTokenizer`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†é‡‡ç”¨self.tokenizerã€‚

+   `weight_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰-è‡ªå®šä¹‰æƒé‡æ–‡ä»¶çš„åç§°ã€‚å½“ï¼š

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶æ˜¯ä»¥ğŸ¤— Diffusersæ ¼å¼ä¿å­˜çš„ï¼Œä½†æ˜¯ä»¥ç‰¹å®šçš„æƒé‡åç§°ä¿å­˜ï¼Œä¾‹å¦‚`text_inv.bin`ã€‚

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶æ˜¯ä»¥Automatic1111æ ¼å¼ä¿å­˜çš„ã€‚

+   `cache_dir`ï¼ˆ`Union[str, os.PathLike]`ï¼Œ*å¯é€‰*ï¼‰-ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚

+   `force_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ¢å¤ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™åˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚

+   `proxies` (`Dict[str, str]`, *å¯é€‰*) â€” ç”¨äºæ¯ä¸ªè¯·æ±‚çš„åè®®æˆ–ç«¯ç‚¹çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚

+   `local_files_only` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™æ¨¡å‹ä¸ä¼šä»Hubä¸‹è½½ã€‚

+   `token` (`str` æˆ– *bool*, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶HTTPä»¤ç‰Œçš„æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ï¼‰ã€‚

+   `revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤IDæˆ–Gitå…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `subfolder` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`""`) â€” Hubæˆ–æœ¬åœ°æ¨¡å‹å­˜å‚¨åº“ä¸­æ¨¡å‹æ–‡ä»¶çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚

+   `mirror` (`str`, *å¯é€‰*) â€” é•œåƒæºï¼Œç”¨äºè§£å†³åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶çš„å¯è®¿é—®æ€§é—®é¢˜ã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

å°†æ–‡æœ¬åæ¼”åµŒå…¥åŠ è½½åˆ°[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼ˆæ”¯æŒğŸ¤— Diffuserså’ŒAutomatic1111æ ¼å¼ï¼‰ã€‚

ç¤ºä¾‹ï¼š

è¦åŠ è½½ğŸ¤— Diffusersæ ¼å¼ä¸­çš„æ–‡æœ¬åæ¼”åµŒå…¥å‘é‡ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

è¦åŠ è½½Automatic1111æ ¼å¼ä¸­çš„æ–‡æœ¬åæ¼”åµŒå…¥å‘é‡ï¼Œè¯·ç¡®ä¿é¦–å…ˆä¸‹è½½å‘é‡ï¼ˆä¾‹å¦‚ä»[civitAI](https://civitai.com/models/3036?modelVersionId=9857)ï¼‰ç„¶ååŠ è½½å‘é‡

æœ¬åœ°ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1088)

```py
( )
```

å¦‚æœå·²å¯ç”¨ï¼Œåˆ™ç¦ç”¨FreeUæœºåˆ¶ã€‚

#### `disable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L380)

```py
( )
```

ç¦ç”¨å¹³é“ºçš„VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç çš„æ–¹å¼ã€‚

#### `enable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1065)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1` (`float`) â€” ç”¨äºè°ƒèŠ‚é˜¶æ®µ1çš„ç¼©æ”¾å› å­ï¼Œä»¥å‡å¼±è·³è·ƒç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `s2` (`float`) â€” ç”¨äºè°ƒèŠ‚é˜¶æ®µ2çš„è·³è·ƒç‰¹å¾è´¡çŒ®çš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `b1` (`float`) â€” ç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ1çš„ç¼©æ”¾å› å­ã€‚

+   `b2` (`float`) â€” ç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ2çš„ç¼©æ”¾å› å­ã€‚

å¯ç”¨FreeUæœºåˆ¶ï¼Œå¦‚[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)ä¸­æ‰€è¿°ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºå®ƒä»¬åº”ç”¨çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ä»¥è·å–å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚Stable Diffusion v1ã€v2å’ŒStable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `enable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L371)

```py
( )
```

å¯ç”¨å¹³é“º VAE è§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAE å°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç“¦ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚

#### `encode_prompt`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L421)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” è¦ç¼–ç çš„æç¤ºè®¾å¤‡ â€” (`torch.device`): torch è®¾å¤‡

+   `num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’ `negative_prompt_embeds`ã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³ï¼Œå¦‚æœ `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œ*ä¾‹å¦‚*æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œ*ä¾‹å¦‚*æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆè´Ÿæ–‡æœ¬åµŒå…¥ã€‚

+   `lora_scale` (`float`, *å¯é€‰*) â€” å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰ LoRA å±‚çš„ LoRA æ¯”ä¾‹ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä» CLIP è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

å‚æ•°

+   `images` (`List[PIL.Image.Image]` æˆ– `np.ndarray`) â€” é•¿åº¦ä¸º `batch_size` çš„å»å™ª PIL å›¾åƒåˆ—è¡¨æˆ–å½¢çŠ¶ä¸º `(batch_size, height, width, num_channels)` çš„ NumPy æ•°ç»„ã€‚

+   `nsfw_content_detected` (`List[bool]`) â€” åˆ—è¡¨æŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹ï¼Œæˆ–è€…å¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º `None`ã€‚

ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚

## FlaxStableDiffusionControlNetPipeline

### `class diffusers.FlaxStableDiffusionControlNetPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py#L111)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel controlnet: FlaxControlNetModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor dtype: dtype = <class 'jax.numpy.float32'> )
```

å‚æ•°

+   `vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL)) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç åˆ°å’Œä»æ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆclip-vit-large-patch14ï¼‰ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚

+   `unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel)) â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„ `FlaxUNet2DConditionModel`ã€‚

+   `controlnet` ([FlaxControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.FlaxControlNetModel) â€” åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¸º `unet` æä¾›é¢å¤–çš„æ¡ä»¶ã€‚

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) â€” ä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚å¯ä»¥æ˜¯`FlaxDDIMScheduler`ã€`FlaxLMSDiscreteScheduler`ã€`FlaxPNDMScheduler`æˆ–`FlaxDPMSolverMultistepScheduler`ä¹‹ä¸€ã€‚

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) â€” ç”¨äºä¼°è®¡ç”Ÿæˆå›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰æ”»å‡»æ€§æˆ–æœ‰å®³çš„åˆ†ç±»æ¨¡å—ã€‚è¯·å‚è€ƒ[æ¨¡å‹å¡ç‰‡](https://huggingface.co/runwayml/stable-diffusion-v1-5)ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) â€” ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„`CLIPImageProcessor`ï¼›ä½œä¸ºè¾“å…¥ä¼ é€’ç»™`safety_checker`ã€‚

ä½¿ç”¨ControlNet Guidanceè¿›è¡ŒåŸºäºFlaxçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py#L348)

```py
( prompt_ids: Array image: Array params: Union prng_seed: Array num_inference_steps: int = 50 guidance_scale: Union = 7.5 latents: Array = None neg_prompt_ids: Array = None controlnet_conditioning_scale: Union = 1.0 return_dict: bool = True jit: bool = False ) â†’ export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt_ids` (`jnp.ndarray`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `image` (`jnp.ndarray`) â€” ä»£è¡¨æ§åˆ¶ç½‘ç»œè¾“å…¥æ¡ä»¶çš„æ•°ç»„ï¼Œç”¨äºä¸º`unet`ç”Ÿæˆæä¾›æŒ‡å¯¼ã€‚

+   `params` (`Dict`æˆ–`FrozenDict`) â€” åŒ…å«æ¨¡å‹å‚æ•°/æƒé‡çš„å­—å…¸ã€‚

+   `prng_seed` (`jax.Array`) â€” åŒ…å«éšæœºæ•°ç”Ÿæˆå™¨å¯†é’¥çš„æ•°ç»„ã€‚

+   `num_inference_steps` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º50) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `guidance_scale` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º7.5) â€” æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`ç´§å¯†ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale > 1`æ—¶å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚

+   `latents` (`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆå™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ä»¥ç”¨æ¥ä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆä¸€ä¸ªæ½œå˜é‡æ•°ç»„ã€‚

+   `controlnet_conditioning_scale` (`float`æˆ–`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1.0) â€” ControlNetçš„è¾“å‡ºåœ¨æ·»åŠ åˆ°åŸå§‹`unet`çš„æ®‹å·®ä¹‹å‰ä¼šä¹˜ä»¥`controlnet_conditioning_scale`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `jit` (`bool`ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿è¡Œç”Ÿæˆå’Œå®‰å…¨è¯„åˆ†å‡½æ•°çš„`pmap`ç‰ˆæœ¬ã€‚

    æ­¤å‚æ•°å­˜åœ¨æ˜¯å› ä¸º`__call__`å°šä¸æ˜¯ç«¯åˆ°ç«¯çš„pmap-ableã€‚å®ƒå°†åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­è¢«ç§»é™¤ã€‚

è¿”å›å€¼

[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)æˆ–`tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å›[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹çš„å¸ƒå°”å€¼åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import jax
>>> import numpy as np
>>> import jax.numpy as jnp
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard
>>> from diffusers.utils import load_image, make_image_grid
>>> from PIL import Image
>>> from diffusers import FlaxStableDiffusionControlNetPipeline, FlaxControlNetModel

>>> def create_key(seed=0):
...     return jax.random.PRNGKey(seed)

>>> rng = create_key(0)

>>> # get canny image
>>> canny_image = load_image(
...     "https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/blog_post_cell_10_output_0.jpeg"
... )

>>> prompts = "best quality, extremely detailed"
>>> negative_prompts = "monochrome, lowres, bad anatomy, worst quality, low quality"

>>> # load control net and stable diffusion v1-5
>>> controlnet, controlnet_params = FlaxControlNetModel.from_pretrained(
...     "lllyasviel/sd-controlnet-canny", from_pt=True, dtype=jnp.float32
... )
>>> pipe, params = FlaxStableDiffusionControlNetPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, revision="flax", dtype=jnp.float32
... )
>>> params["controlnet"] = controlnet_params

>>> num_samples = jax.device_count()
>>> rng = jax.random.split(rng, jax.device_count())

>>> prompt_ids = pipe.prepare_text_inputs([prompts] * num_samples)
>>> negative_prompt_ids = pipe.prepare_text_inputs([negative_prompts] * num_samples)
>>> processed_image = pipe.prepare_image_inputs([canny_image] * num_samples)

>>> p_params = replicate(params)
>>> prompt_ids = shard(prompt_ids)
>>> negative_prompt_ids = shard(negative_prompt_ids)
>>> processed_image = shard(processed_image)

>>> output = pipe(
...     prompt_ids=prompt_ids,
...     image=processed_image,
...     params=p_params,
...     prng_seed=rng,
...     num_inference_steps=50,
...     neg_prompt_ids=negative_prompt_ids,
...     jit=True,
... ).images

>>> output_images = pipe.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))
>>> output_images = make_image_grid(output_images, num_samples // 4, 4)
>>> output_images.save("generated_image.png")
```

## äºšéº»ç¨³å®šæ‰©æ•£æ§åˆ¶ç½‘ç»œç®¡é“è¾“å‡º

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

å‚æ•°

+   `images` (`np.ndarray`) â€” å½¢çŠ¶ä¸º`(batch_size, height, width, num_channels)`çš„æ•°ç»„çš„å»å™ªå›¾åƒã€‚

+   `nsfw_content_detected` (`List[bool]`) â€” åˆ—è¡¨æŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€(nsfw)å†…å®¹ï¼Œæˆ–è€…å¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º`None`ã€‚

åŸºäºFlaxçš„ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚

#### `replace`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
