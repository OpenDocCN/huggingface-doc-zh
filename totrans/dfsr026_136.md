# ControlNet

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/controlnet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)

ControlNet是由Lvmin Zhang、Anyi Rao和Maneesh Agrawala在[向文本到图像扩散模型添加条件控制](https://huggingface.co/papers/2302.05543)中介绍的。

使用ControlNet模型，您可以提供额外的控制图像来调节和控制稳定扩散生成。例如，如果您提供深度图，ControlNet模型将生成一幅保留深度图中空间信息的图像。这是一种更灵活和准确的控制图像生成过程的方式。

论文摘要：

*我们提出了ControlNet，这是一种神经网络架构，用于向大型、预训练的文本到图像扩散模型添加空间条件控制。ControlNet锁定了生产就绪的大型扩散模型，并重复使用它们的深度和稳健的编码层，这些层经过数十亿图像的预训练，作为学习多样化条件控制的强大支撑。神经架构连接了“零卷积”（从零初始化的卷积层），逐渐增加参数，确保没有有害噪音会影响微调。我们使用稳定扩散测试了各种条件控制，例如边缘、深度、分割、人体姿势等，使用单个或多个条件，有或没有提示。我们展示了ControlNet的训练对于小型（<50k）和大型（>1m）数据集是稳健的。广泛的结果表明，ControlNet可能促进更广泛的应用，以控制图像扩散模型。*

该模型由[takuma104](https://huggingface.co/takuma104)贡献。❤️

原始代码库可以在[lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)找到，您可以在[lllyasviel的](https://huggingface.co/lllyasviel) Hub个人资料中找到官方ControlNet检查点。

确保查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。

## StableDiffusionControlNetPipeline

### `class diffusers.StableDiffusionControlNetPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L139)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

参数

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并从潜在表示中解码图像。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14))。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 一个`UNet2DConditionModel`，用于去噪编码图像的潜在表示。

+   `controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)或`List[ControlNetModel]`) — 在去噪过程中为`unet`提供额外的条件。如果将多个ControlNet设置为列表，则每个ControlNet的输出将相加，以创建一个组合的额外条件。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）— 用于与`unet`结合使用以去噪编码图像潜在特征的调度程序。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `safety_checker`（`StableDiffusionSafetyChecker`）— 用于估计生成图像是否可能被视为具有冒犯性或有害性的分类模块。请参考[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）— 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`安全检查器`的输入。

使用带有ControlNet引导的稳定扩散进行文本到图像生成的流水线。

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存LoRA权重

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) 用于加载`.ckpt`文件

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载IP适配器

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L894)

```py
( prompt: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 1.0 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 用于引导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，`List[PIL.Image.Image]`，`List[np.ndarray]`，— `List[List[torch.FloatTensor]]`，`List[List[np.ndarray]]`或`List[List[PIL.Image.Image]]`）：提供给`unet`生成指导的ControlNet输入条件。如果类型指定为`torch.FloatTensor`，则按原样传递给ControlNet。`PIL.Image.Image`也可以作为图像接受。输出图像的尺寸默认为`image`的尺寸。如果传递了高度和/或宽度，则相应调整`image`的大小。如果在`init`中指定了多个ControlNets，则必须将图像作为列表传递，以便列表的每个元素可以正确批处理为单个ControlNet的输入。当`prompt`是一个列表时，如果为单个ControlNet传递了图像列表，则每个图像将与`prompt`列表中的每个提示配对。对于多个ControlNets，可以传递图像列表的列表以批处理每个提示和每个ControlNet。

+   `height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成图像的像素高度。

+   `width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `timesteps` (`List[int]`, *optional*) — 用于具有在其 `set_timesteps` 方法中支持 `timesteps` 参数的调度器的降噪过程的自定义时间步。如果未定义，将使用传递 `num_inference_steps` 时的默认行为。必须按降序排列。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成中不包括的提示或提示。如果未定义，您需要传递 `negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中将被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成的嘈杂潜在因素，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜在因素张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，将从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，将从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)而不是普通元组。

+   `callback` (`Callable`, *optional*) — 在推理过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, defaults to 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor` 的 kwargs 字典。

+   `controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults to 1.0) — 控制网络的输出在添加到原始 `unet` 中的残差之前会乘以 `controlnet_conditioning_scale`。如果在 `init` 中指定了多个控制网络，可以将相应的比例设置为列表。

+   `guess_mode` (`bool`, *optional*, 默认为`False`) — 即使删除所有提示，ControlNet编码器也会尝试识别输入图像的内容。建议使用`guidance_scale`值在3.0到5.0之间。

+   `control_guidance_start` (`float` or `List[float]`, *optional*, 默认为0.0) — ControlNet开始应用的总步骤百分比。

+   `control_guidance_end` (`float` or `List[float]`, *optional*, 默认为1.0) — ControlNet停止应用的总步骤百分比。

+   `clip_skip` (`int`, *optional*) — 计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用预终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推理期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包含由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 或 `tuple`

如果`return_dict`为`True`，则返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个包含“不安全内容”（nsfw）的生成图像的布尔值列表。

用于生成的管道的调用函数。

示例：

```py
>>> # !pip install opencv-python transformers accelerate
>>> from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> import cv2
>>> from PIL import Image

>>> # download an image
>>> image = load_image(
...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
... )
>>> image = np.array(image)

>>> # get canny image
>>> image = cv2.Canny(image, 100, 200)
>>> image = image[:, :, None]
>>> image = np.concatenate([image, image, image], axis=2)
>>> canny_image = Image.fromarray(image)

>>> # load control net and stable diffusion v1-5
>>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
>>> pipe = StableDiffusionControlNetPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
... )

>>> # speed up diffusion process with faster scheduler and memory optimization
>>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
>>> # remove following line if xformers is not installed
>>> pipe.enable_xformers_memory_efficient_attention()

>>> pipe.enable_model_cpu_offload()

>>> # generate image
>>> generator = torch.manual_seed(0)
>>> image = pipe(
...     "futuristic-looking woman", num_inference_steps=20, generator=generator, image=canny_image
... ).images[0]
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size` (`str` or `int`, *optional*, 默认为`"auto"`) — 当为`"auto"`时，将输入减半到注意力头部，因此注意力将分两步计算。如果为`"max"`，将通过一次运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。启用此选项时，注意力模块将输入张量分割成多个切片，以便在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低是有用的。

⚠️ 如果您已经在使用PyTorch 2.0或xFormers中的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常内存高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则会一次性计算注意力。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L237)

```py
( )
```

启用切片VAE解码。当启用此选项时，VAE将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L245)

```py
( )
```

禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将回到一步计算解码。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *可选*) — 用作`op`参数传递给xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的默认`None`操作符的覆盖。

从[xFormers](https://facebookresearch.github.io/xformers/)启用内存高效的注意力。当启用此选项时，您应该观察到更低的GPU内存使用量，并在推理过程中潜在的加速。训练过程中的加速不被保证。

⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `load_textual_inversion`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str`或`os.PathLike`或`List[str或os.PathLike]`或`Dict`或`List[Dict]`) — 可以是以下之一或它们的列表：

    +   一个字符串，预训练模型Hub上托管的*模型ID*（例如`sd-concepts-library/low-poly-hd-logos-icons`）。

    +   包含文本反转权重的*目录*路径（例如`./my_text_inversion_directory/`）。

    +   包含文本反转权重的*文件*路径（例如`./my_text_inversions.pt`）。

    +   一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token` (`str`或`List[str]`, *可选*) — 覆盖用于文本反转权重的令牌。如果`pretrained_model_name_or_path`是列表，则`token`也必须是相同长度的列表。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel), *可选*) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用self.tokenizer。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer), *可选*) — 用于标记文本的`CLIPTokenizer`。如果未指定，函数将使用self.tokenizer。

+   `weight_name` (`str`, *可选*) — 自定义权重文件的名称。在以下情况下应使用此选项：

    +   保存的文本反转文件以🤗 Diffusers格式保存，但是以特定权重名称（例如`text_inv.bin`）保存。

    +   保存的文本反转文件以Automatic1111格式保存。

+   `cache_dir` (`Union[str, os.PathLike]`, *可选*) — 下载预训练模型配置的目录路径，如果未使用标准缓存。

+   `force_download` (`bool`, *可选*, 默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则会删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理服务器在每个请求上使用。

+   `local_files_only` (`bool`, *optional*, defaults to `False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，模型将不会从Hub下载。

+   `token` (`str` or *bool*, *optional*) — 用作远程文件HTTP令牌授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`中）。

+   `revision` (`str`, *optional*, defaults to `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `subfolder` (`str`, *optional*, defaults to `""`) — 模型文件在Hub或本地较大模型存储库中的子文件夹位置。

+   `mirror` (`str`, *optional*) — 镜像源，用于解决在中国下载模型时的可访问性问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反演嵌入加载到[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的文本编码器中（支持🤗 Diffusers和Automatic1111格式）。

示例：

要加载🤗 Diffusers格式的文本反演嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载Automatic1111格式的文本反演嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)），然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L838)

```py
( )
```

如果已启用FreeU机制，则禁用该机制。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L262)

```py
( )
```

禁用平铺式VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L815)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 阶段1的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 阶段2的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 阶段1的缩放因子，用于放大主干特征的贡献。

+   `b2` (`float`) — 阶段2的缩放因子，用于放大主干特征的贡献。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L253)

```py
( )
```

启用平铺式VAE解码。当启用此选项时，VAE将将输入张量分割成多个瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L303)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器引导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来引导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。当不使用引导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L843)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为 512) — 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为`(len(timesteps), embedding_dim)`的嵌入向量

参见 [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

## StableDiffusionControlNetImg2ImgPipeline

### `class diffusers.StableDiffusionControlNetImg2ImgPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L132)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

参数

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（clip-vit-large-patch14）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪编码图像潜在特征的`UNet2DConditionModel`。

+   `controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel) 或 `List[ControlNetModel]`) — 在去噪过程中为`unet`提供额外的条件。如果将多个 ControlNet 设置为列表，则每个 ControlNet 的输出将相加以创建一个组合的额外条件。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）- 用于与`unet`结合使用以去噪编码图像潜变量的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `safety_checker`（`StableDiffusionSafetyChecker`）- 一个分类模块，用于估计生成的图像是否可能被视为具有冒犯性或有害性。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）- 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为输入传递给`safety_checker`。

使用Stable Diffusion和ControlNet指导进行图像生成的流水线。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承了以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存LoRA权重

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) 用于加载`.ckpt`文件

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载IP适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L905)

```py
( prompt: Union = None image: Union = None control_image: Union = None height: Optional = None width: Optional = None strength: float = 0.8 num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 0.8 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）- 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，`List[PIL.Image.Image]`，`List[np.ndarray]`，- `List[List[torch.FloatTensor]]`，`List[List[np.ndarray]]`或`List[List[PIL.Image.Image]]`）：用作图像生成过程起点的初始图像。也可以接受图像潜变量作为`image`，如果直接传递潜变量，则不会再次编码。

+   `control_image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，`List[PIL.Image.Image]`，`List[np.ndarray]`，- `List[List[torch.FloatTensor]]`，`List[List[np.ndarray]]`或`List[List[PIL.Image.Image]]`）：提供给`unet`生成指导的ControlNet输入条件。如果类型指定为`torch.FloatTensor`，则按原样传递给ControlNet。`PIL.Image.Image`也可以作为图像接受。输出图像的尺寸默认为`image`的尺寸。如果传递了高度和/或宽度，则相应地调整`image`的大小。如果在`init`中指定了多个ControlNets，则必须将图像作为列表传递，以便列表的每个元素可以正确批处理为单个ControlNet的输入。

+   `height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）- 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成嘈杂潜在变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样生成潜在变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds` 将从 `negative_prompt` 输入参数生成。ip_adapter_image — (`PipelineImageInput`, *optional*): 可选的图像输入，用于与 IP 适配器一起使用。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，则作为参数传递给 `AttentionProcessor`，如 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的那样。

+   `controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults to 1.0) — ControlNet 的输出在添加到原始 `unet` 中的残差之前会乘以 `controlnet_conditioning_scale`。如果在 `init` 中指定了多个 ControlNets，则可以将相应的比例设置为列表。

+   `guess_mode` (`bool`, *optional*, defaults to `False`) — ControlNet 编码器尝试识别输入图像的内容，即使您删除所有提示。建议使用介于 3.0 和 5.0 之间的 `guidance_scale` 值。

+   `control_guidance_start` (`float` or `List[float]`, *optional*, defaults to 0.0) — ControlNet 开始应用的总步骤百分比。

+   `control_guidance_end` (`float` or `List[float]`, *optional*, defaults to 1.0) — ControlNet 停止应用的总步骤百分比。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的层数。值为 1 表示将使用前终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 推理期间在每个去噪步骤结束时调用的函数。该函数使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — 用于 `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 或 `tuple`

如果 `return_dict` 为 `True`，将返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则将返回一个 `tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个包含指示相应生成图像是否包含“不适宜工作”（nsfw）内容的 `bool` 列表。

用于生成的管道的调用函数。

示例：

```py
>>> # !pip install opencv-python transformers accelerate
>>> from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> import cv2
>>> from PIL import Image

>>> # download an image
>>> image = load_image(
...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
... )
>>> np_image = np.array(image)

>>> # get canny image
>>> np_image = cv2.Canny(np_image, 100, 200)
>>> np_image = np_image[:, :, None]
>>> np_image = np.concatenate([np_image, np_image, np_image], axis=2)
>>> canny_image = Image.fromarray(np_image)

>>> # load control net and stable diffusion v1-5
>>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
>>> pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
... )

>>> # speed up diffusion process with faster scheduler and memory optimization
>>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
>>> pipe.enable_model_cpu_offload()

>>> # generate image
>>> generator = torch.manual_seed(0)
>>> image = pipe(
...     "futuristic-looking woman",
...     num_inference_steps=20,
...     generator=generator,
...     image=image,
...     control_image=canny_image,
... ).images[0]
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size` (`str` or `int`, *optional*, 默认为 `"auto"`) — 当为 `"auto"` 时，将输入减半给注意力头，因此注意力将在两个步骤中计算。如果为 `"max"`，将通过一次只运行一个切片来保存最大内存量。如果提供了一个数字，则使用 `attention_head_dim // slice_size` 个切片。在这种情况下，`attention_head_dim` 必须是 `slice_size` 的倍数。

启用切片注意力计算。启用此选项时，注意力模块将输入张量分割成多个切片，以便在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取轻微的速度降低很有用。

⚠️ 如果您已经在使用 PyTorch 2.0 或 xFormers 的 `scaled_dot_product_attention` (SDPA)，请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在使用 SDPA 或 xFormers 启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了 `enable_attention_slicing`，则注意力将在一步中计算。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L230)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将把输入张量分割成多个切片，以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L238)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到在一步中计算解码。

#### `enable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op`（`Callable`，*可选*）- 用作`op`参数传递给xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的默认`None`运算符的覆盖。

启用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。启用此选项时，您应该观察到较低的GPU内存使用量和潜在的推理加速。不能保证训练期间的加速。

⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `load_textual_inversion`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`或`os.PathLike`或`List[str或os.PathLike]`或`Dict`或`List[Dict]`）- 可以是以下之一或它们的列表：

    +   一个字符串，预训练模型在Hub上托管的*模型id*（例如`sd-concepts-library/low-poly-hd-logos-icons`）。

    +   一个*目录*的路径（例如`./my_text_inversion_directory/）包含文本反转权重。

    +   一个*文件*的路径（例如`./my_text_inversions.pt`）包含文本反转权重。

    +   一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token`（`str`或`List[str]`，*可选*）- 覆盖用于文本反转权重的令牌。如果`pretrained_model_name_or_path`是列表，则`token`也必须是相同长度的列表。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)，*可选*）- 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用self.tokenizer。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，*可选*）- 用于对文本进行标记化的`CLIPTokenizer`。如果未指定，函数将使用self.tokenizer。

+   `weight_name`（`str`，*可选*）- 自定义权重文件的名称。应在以下情况下使用：

    +   保存的文本反转文件采用🤗 Diffusers格式，但是保存在特定权重名称下，例如`text_inv.bin`。

    +   保存的文本反转文件采用Automatic1111格式。

+   `cache_dir`（`Union[str, os.PathLike]`，*可选*）- 下载的预训练模型配置缓存在其中的目录路径，如果未使用标准缓存。

+   `force_download`（`bool`，*可选*，默认为`False`）- 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download`（`bool`，*可选*，默认为`False`）- 是否继续下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies`（`Dict[str, str]`，*可选*）- 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `local_files_only`（`bool`，*可选*，默认为`False`）- 是否仅加载本地模型权重和配置文件。如果设置为`True`，模型将不会从Hub下载。

+   `token`（`str`或*bool*，*可选*）- 用作远程文件的HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`中）。

+   `revision` (`str`, *可选*, 默认为`"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `subfolder` (`str`, *可选*, 默认为`""`) — 在Hub或本地较大模型存储库中模型文件的子文件夹位置。

+   `mirror` (`str`, *可选*) — 如果您在中国下载模型时遇到访问问题，请将源镜像以解决问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反转嵌入加载到[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的文本编码器中（支持🤗 Diffusers和Automatic1111格式）。

示例：

要以🤗 Diffusers格式加载文本反转嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要以Automatic1111格式加载文本反转嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)）然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L878)

```py
( )
```

如果启用，将禁用FreeU机制。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L255)

```py
( )
```

禁用平铺的VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L855)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 用于阻尼跳过特征贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `s2` (`float`) — 用于阻尼跳过特征贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `b1` (`float`) — 用于放大骨干特征贡献的阶段1的缩放因子。

+   `b2` (`float`) — 用于放大骨干特征贡献的阶段2的缩放因子。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L246)

```py
( )
```

启用平铺的VAE解码。启用此选项时，VAE将将输入张量分割为瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L296)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str`或`List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str`或`List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。

+   `lora_scale`（`float`，*可选*）—如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip`（`int`，*可选*）—在计算提示嵌入时要从CLIP跳过的层数。值为1意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionControlNetInpaintPipeline

### `class diffusers.StableDiffusionControlNetInpaintPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L243)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel controlnet: Union scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）—变分自动编码器（VAE）模型，用于对图像进行编码和解码以及从潜在表示到图像的解码。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)）—冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）—用于标记文本的`CLIPTokenizer`。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）—用于去噪编码图像潜在状态的`UNet2DConditionModel`。

+   `controlnet`（[ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)或`List[ControlNetModel]`）—在去噪过程中为`unet`提供额外的调节。如果将多个ControlNets设置为列表，则每个ControlNet的输出将相加以创建一个组合的额外调节。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）—与`unet`结合使用以去噪编码图像潜在状态的调度程序。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `safety_checker`（`StableDiffusionSafetyChecker`）—估计生成的图像是否可能被视为具有冒犯性或有害的分类模块。有关模型潜在危害的更多详细信息，请参阅[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）—用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

使用Stable Diffusion和ControlNet指导进行图像修复的管道。

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载 LoRA 权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存 LoRA 权重

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) 用于加载`.ckpt`文件

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载 IP 适配器

这个管道可以与专门为修补（[runwayml/stable-diffusion-inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting)）进行了特定微调的检查点一起使用，也可以与默认的文本到图像稳定扩散检查点一起使用（[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)）。默认的文本到图像稳定扩散检查点可能更适合已在这些检查点上进行了微调的ControlNets，比如[lllyasviel/control_v11p_sd15_inpaint](https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint)。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1115)

```py
( prompt: Union = None image: Union = None mask_image: Union = None control_image: Union = None height: Optional = None width: Optional = None padding_mask_crop: Optional = None strength: float = 1.0 num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None controlnet_conditioning_scale: Union = 0.5 guess_mode: bool = False control_guidance_start: Union = 0.0 control_guidance_end: Union = 1.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 用于引导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，— `List[PIL.Image.Image]`或`List[np.ndarray]`）：表示要用作起点的图像批次的`Image`，NumPy数组或张量。对于NumPy数组和PyTorch张量，期望值范围在`[0, 1]`之间。如果是张量或张量列表，则期望形状应为`(B, C, H, W)`或`(C, H, W)`。如果是NumPy数组或数组列表，则期望形状应为`(B, H, W, C)`或`(H, W, C)`。它还可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。

+   `mask_image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，— `List[PIL.Image.Image]`或`List[np.ndarray]`）：表示要遮罩`image`的图像批次的`Image`，NumPy数组或张量。遮罩中的白色像素被重新绘制，而黑色像素被保留。如果`mask_image`是PIL图像，则在使用之前将其转换为单通道（亮度）。如果是NumPy数组或PyTorch张量，则应包含一个颜色通道（L）而不是3，因此PyTorch张量的预期形状为`(B, 1, H, W)`，`(B, H, W)`，`(1, H, W)`，`(H, W)`。对于NumPy数组，预期形状为`(B, H, W, 1)`，`(B, H, W)`，`(H, W, 1)`或`(H, W)`。

+   `control_image`（`torch.FloatTensor`，`PIL.Image.Image`，`List[torch.FloatTensor]`，`List[PIL.Image.Image]`，— `List[List[torch.FloatTensor]]`或`List[List[PIL.Image.Image]]`）：提供指导给`unet`以生成的ControlNet输入条件。如果类型指定为`torch.FloatTensor`，则按原样传递给ControlNet。`PIL.Image.Image`也可以作为图像接受。输出图像的尺寸默认为`image`的尺寸。如果传递了高度和/或宽度，`image`将相应调整大小。如果在`init`中指定了多个ControlNets，则必须将图像作为列表传递，以便列表的每个元素可以正确批处理输入到单个ControlNet。

+   `height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成图像的像素高度。

+   `width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成图像的像素宽度。

+   `padding_mask_crop` (`int`, *可选*, 默认为 `None`) — 要应用于图像和遮罩的裁剪边距大小。如果为`None`，则不对图像和 mask_image 进行裁剪。如果`padding_mask_crop`不是`None`，它将首先找到一个具有与图像相同纵横比且包含所有遮罩区域的矩形区域，然后根据`padding_mask_crop`扩展该区域。然后根据扩展的区域对图像和 mask_image 进行裁剪，然后将其调整为原始图像大小以进行修复。当遮罩区域较小而图像较大且包含与修复无关的信息时，这是很有用的，比如背景。

+   `strength` (`float`, *可选*, 默认为 1.0) — 指示转换参考`image`的程度。必须在 0 和 1 之间。`image`被用作起点，`strength`越高，添加的噪音就越多。去噪步骤的数量取决于最初添加的噪音量。当`strength`为 1 时，添加的噪音是最大的，去噪过程将运行指定的`num_inference_steps`次迭代。值为 1 实质上忽略了`image`。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用指导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 指导不包括在图像生成中的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时被忽略（`guidance_scale < 1`）。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中抽样的预生成嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行抽样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。ip_adapter_image — (`PipelineImageInput`, *可选*): 可选的图像输入以与 IP 适配器一起使用。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)而不是普通元组。

+   `cross_attention_kwargs`（`dict`，*可选*）— 如果指定了kwargs字典，则将其传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`。

+   `controlnet_conditioning_scale`（`float`或`List[float]`，*可选*，默认为0.5）— 在将输出添加到原始`unet`中的残差之前，ControlNet的输出将乘以`controlnet_conditioning_scale`。如果在`init`中指定了多个ControlNets，您可以将相应的比例设置为列表。

+   `guess_mode`（`bool`，*可选*，默认为`False`）— ControlNet编码器尝试识别输入图像的内容，即使您删除所有提示。建议使用介于3.0和5.0之间的`guidance_scale`值。

+   `control_guidance_start`（`float`或`List[float]`，*可选*，默认为0.0）— ControlNet开始应用的总步骤的百分比。

+   `control_guidance_end`（`float`或`List[float]`，*可选*，默认为1.0）— ControlNet停止应用的总步骤的百分比。

+   `clip_skip`（`int`，*可选*）— 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用前一层的输出来计算提示嵌入。

+   `callback_on_step_end`（`Callable`，*可选*）— 在推理期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs`（`List`，*可选*）— `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个包含指示相应生成图像是否包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> # !pip install transformers accelerate
>>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler
>>> from diffusers.utils import load_image
>>> import numpy as np
>>> import torch

>>> init_image = load_image(
...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png"
... )
>>> init_image = init_image.resize((512, 512))

>>> generator = torch.Generator(device="cpu").manual_seed(1)

>>> mask_image = load_image(
...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png"
... )
>>> mask_image = mask_image.resize((512, 512))

>>> def make_canny_condition(image):
...     image = np.array(image)
...     image = cv2.Canny(image, 100, 200)
...     image = image[:, :, None]
...     image = np.concatenate([image, image, image], axis=2)
...     image = Image.fromarray(image)
...     return image

>>> control_image = make_canny_condition(init_image)

>>> controlnet = ControlNetModel.from_pretrained(
...     "lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16
... )
>>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
... )

>>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
>>> pipe.enable_model_cpu_offload()

>>> # generate image
>>> image = pipe(
...     "a handsome man with ray-ban sunglasses",
...     num_inference_steps=20,
...     generator=generator,
...     eta=1.0,
...     image=init_image,
...     mask_image=mask_image,
...     control_image=control_image,
... ).images[0]
```

#### `enable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size`（`str`或`int`，*可选*，默认为`"auto"`）— 当为`"auto"`时，将输入减半到注意力头部，因此注意力将在两个步骤中计算。如果为`"max"`，将通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。当启用此选项时，注意力模块将输入张量分割成片段，以便在多个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低是有用的。

⚠️ 如果您已经在使用PyTorch 2.0或xFormers中的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常内存高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力会在一步中计算。

启用VAE切片

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L355)

```py
( )
```

启用切片VAE解码。当启用此选项时，VAE将将输入张量分割成片段，以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

禁用VAE切片

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L363)

```py
( )
```

禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

启用xFormers的内存高效注意力

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op`（`Callable`，*可选*）-用作xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`运算符的覆盖。

从[xFormers](https://facebookresearch.github.io/xformers/)启用内存高效注意力。当启用此选项时，您应该观察到更低的GPU内存使用量，并且在推理过程中可能会加速。训练过程中的加速不被保证。

⚠️当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

禁用xFormers的内存高效注意力

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

从[xFormers](https://facebookresearch.github.io/xformers/)中禁用内存高效注意力。

加载文本反转

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`或`os.PathLike`或`List[str或os.PathLike]`或`Dict`或`List[Dict]`）-可以是以下之一或它们的列表：

    +   一个字符串，预训练模型Hub上托管的*模型ID*（例如`sd-concepts-library/low-poly-hd-logos-icons`）。

    +   一个*目录*的路径（例如`./my_text_inversion_directory/`），其中包含文本反转权重。

    +   一个*文件*的路径（例如`./my_text_inversions.pt`），其中包含文本反转权重。

    +   一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token`（`str`或`List[str]`，*可选*）-覆盖用于文本反转权重的令牌。如果`pretrained_model_name_or_path`是一个列表，则`token`也必须是相同长度的列表。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)，*可选*）-冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将采用self.tokenizer。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，*可选*）-用于标记文本的`CLIPTokenizer`。如果未指定，函数将采用self.tokenizer。

+   `weight_name`（`str`，*可选*）-自定义权重文件的名称。当：

    +   保存的文本反转文件是以🤗 Diffusers格式保存的，但是以特定的权重名称保存，例如`text_inv.bin`。

    +   保存的文本反转文件是以Automatic1111格式保存的。

+   `cache_dir`（`Union[str, os.PathLike]`，*可选*）-下载的预训练模型配置缓存在其中的目录路径，如果未使用标准缓存。

+   `force_download` (`bool`, *可选*, 默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *可选*) — 用于每个请求的协议或端点的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `local_files_only` (`bool`, *可选*, 默认为`False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则模型不会从Hub下载。

+   `token` (`str` 或 *bool*, *可选*) — 用作远程文件HTTP令牌的授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`, *可选*, 默认为`"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `subfolder` (`str`, *可选*, 默认为`""`) — Hub或本地模型存储库中模型文件的子文件夹位置。

+   `mirror` (`str`, *可选*) — 镜像源，用于解决在中国下载模型时的可访问性问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反演嵌入加载到[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的文本编码器中（支持🤗 Diffusers和Automatic1111格式）。

示例：

要加载🤗 Diffusers格式中的文本反演嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载Automatic1111格式中的文本反演嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)）然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1088)

```py
( )
```

如果已启用，则禁用FreeU机制。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L380)

```py
( )
```

禁用平铺的VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将回到一步计算解码的方式。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1065)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 用于调节阶段1的缩放因子，以减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 用于调节阶段2的跳跃特征贡献的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 用于放大骨干特征贡献的阶段1的缩放因子。

+   `b2` (`float`) — 用于放大骨干特征贡献的阶段2的缩放因子。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L371)

```py
( )
```

启用平铺 VAE 解码。启用此选项时，VAE 将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L421)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。如果不使用指导（即，如果 `guidance_scale` 小于 `1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负文本嵌入。

+   `lora_scale` (`float`, *可选*) — 将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不适宜工作”（nsfw）内容，或者如果无法执行安全检查，则为 `None`。

稳定扩散管道的输出类。

## FlaxStableDiffusionControlNetPipeline

### `class diffusers.FlaxStableDiffusionControlNetPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py#L111)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel controlnet: FlaxControlNetModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor dtype: dtype = <class 'jax.numpy.float32'> )
```

参数

+   `vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL)) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码到和从潜在表示。

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) — 冻结的文本编码器（clip-vit-large-patch14）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel)) — 用于去噪编码图像潜变量的 `FlaxUNet2DConditionModel`。

+   `controlnet` ([FlaxControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.FlaxControlNetModel) — 在去噪过程中为 `unet` 提供额外的条件。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与`unet`结合使用以去噪编码图像潜变量的调度器。可以是`FlaxDDIMScheduler`、`FlaxLMSDiscreteScheduler`、`FlaxPNDMScheduler`或`FlaxDPMSolverMultistepScheduler`之一。

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) — 用于估计生成图像是否可能被视为具有攻击性或有害的分类模块。请参考[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为输入传递给`safety_checker`。

使用ControlNet Guidance进行基于Flax的文本到图像生成的管道。

这个模型继承自[FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py#L348)

```py
( prompt_ids: Array image: Array params: Union prng_seed: Array num_inference_steps: int = 50 guidance_scale: Union = 7.5 latents: Array = None neg_prompt_ids: Array = None controlnet_conditioning_scale: Union = 1.0 return_dict: bool = True jit: bool = False ) → export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

参数

+   `prompt_ids` (`jnp.ndarray`) — 用于指导图像生成的提示或提示。

+   `image` (`jnp.ndarray`) — 代表控制网络输入条件的数组，用于为`unet`生成提供指导。

+   `params` (`Dict`或`FrozenDict`) — 包含模型参数/权重的字典。

+   `prng_seed` (`jax.Array`) — 包含随机数生成器密钥的数组。

+   `num_inference_steps` (`int`，*可选*，默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `guidance_scale` (`float`，*可选*，默认为7.5) — 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用指导比例。

+   `latents` (`jnp.ndarray`，*可选*）— 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示调整相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成一个潜变量数组。

+   `controlnet_conditioning_scale` (`float`或`jnp.ndarray`，*可选*，默认为1.0) — ControlNet的输出在添加到原始`unet`的残差之前会乘以`controlnet_conditioning_scale`。

+   `return_dict` (`bool`，*可选*，默认为`True`) — 是否返回[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)而不是普通元组。

+   `jit` (`bool`，默认为`False`) — 是否运行生成和安全评分函数的`pmap`版本。

    此参数存在是因为`__call__`尚不是端到端的pmap-able。它将在将来的版本中被移除。

返回值

[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个包含“不适宜工作”（nsfw）内容的布尔值列表。

用于生成的管道的调用函数。

示例：

```py
>>> import jax
>>> import numpy as np
>>> import jax.numpy as jnp
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard
>>> from diffusers.utils import load_image, make_image_grid
>>> from PIL import Image
>>> from diffusers import FlaxStableDiffusionControlNetPipeline, FlaxControlNetModel

>>> def create_key(seed=0):
...     return jax.random.PRNGKey(seed)

>>> rng = create_key(0)

>>> # get canny image
>>> canny_image = load_image(
...     "https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/blog_post_cell_10_output_0.jpeg"
... )

>>> prompts = "best quality, extremely detailed"
>>> negative_prompts = "monochrome, lowres, bad anatomy, worst quality, low quality"

>>> # load control net and stable diffusion v1-5
>>> controlnet, controlnet_params = FlaxControlNetModel.from_pretrained(
...     "lllyasviel/sd-controlnet-canny", from_pt=True, dtype=jnp.float32
... )
>>> pipe, params = FlaxStableDiffusionControlNetPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, revision="flax", dtype=jnp.float32
... )
>>> params["controlnet"] = controlnet_params

>>> num_samples = jax.device_count()
>>> rng = jax.random.split(rng, jax.device_count())

>>> prompt_ids = pipe.prepare_text_inputs([prompts] * num_samples)
>>> negative_prompt_ids = pipe.prepare_text_inputs([negative_prompts] * num_samples)
>>> processed_image = pipe.prepare_image_inputs([canny_image] * num_samples)

>>> p_params = replicate(params)
>>> prompt_ids = shard(prompt_ids)
>>> negative_prompt_ids = shard(negative_prompt_ids)
>>> processed_image = shard(processed_image)

>>> output = pipe(
...     prompt_ids=prompt_ids,
...     image=processed_image,
...     params=p_params,
...     prng_seed=rng,
...     num_inference_steps=50,
...     neg_prompt_ids=negative_prompt_ids,
...     jit=True,
... ).images

>>> output_images = pipe.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))
>>> output_images = make_image_grid(output_images, num_samples // 4, 4)
>>> output_images.save("generated_image.png")
```

## 亚麻稳定扩散控制网络管道输出

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

参数

+   `images` (`np.ndarray`) — 形状为`(batch_size, height, width, num_channels)`的数组的去噪图像。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不适宜工作”(nsfw)内容，或者如果无法执行安全检查，则为`None`。

基于Flax的稳定扩散管道的输出类。

#### `replace`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。
