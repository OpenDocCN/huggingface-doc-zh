# 监督微调训练器

> 原始文本：[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)

监督微调（或简称SFT）是RLHF中的一个关键步骤。在TRL中，我们提供了一个易于使用的API来创建您的SFT模型，并用几行代码在您的数据集上训练它们。

在[`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py)中查看一个完整灵活的示例。

## 快速开始

如果您在🤗 Hub上托管了一个数据集，您可以使用[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)从TRL轻松微调您的SFT模型。假设您的数据集是`imdb`，您要预测的文本位于数据集的`text`字段中，您想要微调`facebook/opt-350m`模型。以下代码片段会处理所有的数据预处理和训练工作：

```py
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset("imdb", split="train")

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
)
trainer.train()
```

确保为`max_seq_length`传递正确的值，因为默认值将设置为`min(tokenizer.model_max_length, 1024)`。

您还可以在训练师之外构建一个模型，并将其传递如下：

```py
from transformers import AutoModelForCausalLM
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset("imdb", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
)

trainer.train()
```

上述代码片段将使用[`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)类中的默认训练参数。如果您想修改它，请确保创建自己的`TrainingArguments`对象并将其传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)构造函数，就像在[`supervised_finetuning.py`脚本](https://github.com/huggingface/trl/blob/main/examples/stack_llama/scripts/supervised_finetuning.py)中的stack-llama示例中所做的那样。

## 高级用法

### 仅在完成上训练

您可以使用`DataCollatorForCompletionOnlyLM`仅在生成提示上训练您的模型。请注意，这仅在`packing=False`的情况下有效。为了为指令数据实例化该收集器，传递一个响应模板和分词器。以下是在CodeAlpaca数据集上仅对完成进行微调`opt-350m`的工作示例：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

dataset = load_dataset("lucasmccabe-lmi/CodeAlpaca-20k", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['instruction'])):
        text = f"### Question: {example['instruction'][i]}\n ### Answer: {example['output'][i]}"
        output_texts.append(text)
    return output_texts

response_template = " ### Answer:"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    formatting_func=formatting_prompts_func,
    data_collator=collator,
)

trainer.train()
```

为助手风格对话数据实例化该收集器时，请传递一个响应模板、一个指令模板和分词器。以下是在Open Assistant Guanaco数据集上仅对助手完成进行微调`opt-350m`的工作示例：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

instruction_template = "### Human:"
response_template = "### Assistant:"
collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field="text",
    data_collator=collator,
)

trainer.train()
```

确保有一个`pad_token_id`，它与`eos_token_id`不同，这可以防止模型在生成过程中无法正确预测EOS（句子结束）标记。

#### 直接使用token_ids作为response_template

一些分词器（如Llama 2（`meta-llama/Llama-2-XXb-hf`））根据是否有上下文而不同地对序列进行分词。例如：

```py
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

def print_tokens_with_ids(txt):
    tokens = tokenizer.tokenize(txt, add_special_tokens=False)
    token_ids = tokenizer.encode(txt, add_special_tokens=False)
    print(list(zip(tokens, token_ids)))

prompt = """### User: Hello\n\n### Assistant: Hi, how can I help you?"""
print_tokens_with_ids(prompt)  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]

response_template = "### Assistant:"
print_tokens_with_ids(response_template)  # [('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]
```

在这种情况下，由于`response_template`中缺乏上下文，相同的字符串（“### Assistant:”）被不同地分词：

+   文本（带上下文）：`[2277, 29937, 4007, 22137, 29901]`

+   `response_template`（无上下文）：`[835, 4007, 22137, 29901]`

当`DataCollatorForCompletionOnlyLM`在数据集示例文本中找不到`response_template`时，这将导致错误：

```py
RuntimeError: Could not find response key [835, 4007, 22137, 29901] in token IDs tensor([    1,   835,  ...])
```

为了解决这个问题，您可以使用与数据集中相同上下文的方式对`response_template`进行分词，根据需要截断它，并将`token_ids`直接传递给`DataCollatorForCompletionOnlyLM`类的`response_template`参数。例如：

```py
response_template_with_context = "\n### Assistant:"  # We added context here: "\n". This is enough for this tokenizer
response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`

data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)
```

### 为聊天格式添加特殊标记

为语言模型添加特殊标记对于训练聊天模型至关重要。这些标记被添加在对话中的不同角色之间，如用户、助手和系统，并帮助模型识别对话的结构和流程。这种设置对于使模型能够在聊天环境中生成连贯和上下文适当的响应至关重要。`trl`中的`setup_chat_format()`函数可以轻松为会话AI任务设置模型和分词器。这个函数：

+   向标记化器添加特殊标记，例如`<|im_start|>`和`<|imm_end|>`，以指示会话的开始和结束。

+   调整模型的嵌入层以容纳新的标记。

+   设置令牌化器的`chat_template`，用于将输入数据格式化为类似聊天的格式。默认值为OpenAI的`chatml`。

+   *可选*您可以传递`resize_to_multiple_of`来将嵌入层调整为`resize_to_multiple_of`参数的倍数，例如64。如果您希望将来支持更多格式，请在[trl](https://github.com/huggingface/trl)上打开GitHub问题

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

# Set up the chat format with default 'chatml' format
model, tokenizer = setup_chat_format(model, tokenizer)

```

有了我们的模型和分词器设置，我们现在可以在对话数据集上对模型进行微调。以下是一个格式化数据集进行微调的示例。

### 数据集格式支持

[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)支持流行的数据集格式。这使您可以直接将数据集传递给训练器，无需任何预处理。支持以下格式：

+   对话格式

```py
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "..."}]}
```

+   指令格式

```py
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
```

如果您的数据集使用上述格式之一，您可以直接将其传递给训练器而无需预处理。[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)将使用模型的分词器中定义的格式使用[apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)方法为您格式化数据集。

```py
from datasets import load_dataset
from trl import SFTTrainer

...

# load jsonl dataset
dataset = load_dataset("json", data_files="path/to/dataset.jsonl", split="train")
# load dataset from the HuggingFace Hub
dataset = load_dataset("philschmid/dolly-15k-oai-style", split="train")

...

trainer = SFTTrainer(
    "facebook/opt-350m",
    args=training_args,
    train_dataset=dataset,
    packing=True,
)
```

如果数据集不是这些格式之一，您可以预处理数据集以匹配格式，或者将格式化函数传递给SFTTrainer以代替。让我们看看。

### 格式化您的输入提示

对于指令微调，通常在数据集中有两列：一个用于提示，另一个用于响应。这使人们可以像[Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)那样格式化示例：

```py
Below is an instruction ...

### Instruction
{prompt}

### Response:
{completion}
```

假设您的数据集有两个字段，`question`和`answer`。因此您可以直接运行：

```py
...
def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['question'])):
        text = f"### Question: {example['question'][i]}\n ### Answer: {example['answer'][i]}"
        output_texts.append(text)
    return output_texts

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    formatting_func=formatting_prompts_func,
)

trainer.train()
```

为了正确格式化您的输入，请确保通过循环处理所有示例并返回处理过的文本列表。查看如何在alpaca数据集上使用SFTTrainer的完整示例[这里](https://github.com/huggingface/trl/pull/444#issue-1760952763)

### 打包数据集（ConstantLengthDataset）

[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)支持*示例打包*，其中多个短示例打包在同一输入序列中以增加训练效率。这是通过`ConstantLengthDataset`实用程序类完成的，该类从示例流中返回常长度的令牌块。要启用此数据集类的使用，只需将`packing=True`传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)构造函数。

```py
...

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    packing=True
)

trainer.train()
```

请注意，如果您使用打包数据集，并且在训练参数中传递了`max_steps`，则您可能会训练模型多个周期，具体取决于您如何配置打包数据集和训练协议。请确保您知道并理解您正在做什么。

#### 使用打包数据集自定义您的提示

如果您的数据集有几个字段要合并，例如数据集有`question`和`answer`字段，您想要合并它们，您可以将格式化函数传递给训练器来处理。例如：

```py
def formatting_func(example):
    text = f"### Question: {example['question']}\n ### Answer: {example['answer']}"
    return text

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    packing=True,
    formatting_func=formatting_func
)

trainer.train()
```

您还可以通过直接将参数传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)构造函数来更多地定制`ConstantLengthDataset`。请参考该类的签名以获取更多信息。

### 对预训练模型的控制

您可以直接将`from_pretrained()`方法的kwargs传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)。例如，如果要以不同的精度加载模型，类似于

```py
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.bfloat16)
```

```py
...

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    model_init_kwargs={
        "torch_dtype": torch.bfloat16,
    },
)

trainer.train()
```

请注意，`from_pretrained()`的所有关键字参数都受支持。

### 训练适配器

我们还支持与🤗 PEFT库的紧密集成，以便任何用户可以方便地训练适配器并在Hub上共享它们，而不是训练整个模型

```py
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig

dataset = load_dataset("imdb", split="train")

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

trainer = SFTTrainer(
    "EleutherAI/gpt-neo-125m",
    train_dataset=dataset,
    dataset_text_field="text",
    peft_config=peft_config
)

trainer.train()
```

您还可以继续训练您的`PeftModel`。为此，请首先在`SFTTrainer`之外加载`PeftModel`，并直接将其传递给训练器，而无需传递`peft_config`参数。

### 使用基础8位模型训练适配器

为此，您需要首先在Trainer之外加载您的8位模型，并将`PeftConfig`传递给训练器。例如：

```py
...

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = AutoModelForCausalLM.from_pretrained(
    "EleutherAI/gpt-neo-125m",
    load_in_8bit=True,
    device_map="auto",
)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field="text",
    peft_config=peft_config,
)

trainer.train()
```

## 使用Flash Attention和Flash Attention 2

您可以通过使用SFTTrainer中的Flash Attention 1和2来获益，只需进行最少的代码更改。首先，为了确保您拥有来自transformers的所有最新功能，请从源代码安装transformers

```py
pip install -U git+https://github.com/huggingface/transformers.git
```

请注意，Flash Attention现在仅在GPU上运行，并且在半精度制度下运行（当使用适配器时，基础模型以半精度加载）。还请注意，这两个功能与量化等其他工具完全兼容。

### 使用Flash-Attention 1

对于Flash Attention 1，您可以使用`BetterTransformer` API并强制分派API以使用Flash Attention内核。首先，安装最新的optimum包：

```py
pip install -U optimum
```

加载模型后，将`trainer.train()`调用包装在`with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):`上下文管理器中：

```py
...

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    trainer.train()
```

请注意，如果使用Flash Attention内核，您不能在任意数据集上训练您的模型，因为`torch.scaled_dot_product_attention`不支持使用填充令牌进行训练。因此，您只能在`packing=True`的情况下使用该功能。如果您的数据集包含填充令牌，请考虑切换到Flash Attention 2集成。

以下是在单个NVIDIA-T4 16GB上使用Flash Attention 1可以获得的一些加速和内存效率方面的数字。

| use_flash_attn_1 | model_name | max_seq_len | batch_size | time per training step |
| --- | --- | --- | --- | --- |
| x | facebook/opt-350m | 2048 | 8 | ~59.1s |
|  | facebook/opt-350m | 2048 | 8 | **OOM** |
| x | facebook/opt-350m | 2048 | 4 | ~30.3s |
|  | facebook/opt-350m | 2048 | 4 | ~148.9s |

### 使用Flash Attention-2

要使用Flash Attention 2，首先安装最新的`flash-attn`包：

```py
pip install -U flash-attn
```

在调用`from_pretrained`时添加`use_flash_attention_2=True`：

```py
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    use_flash_attention_2=True
)
```

如果不使用量化，请确保您的模型以半精度加载，并将您的模型分派到支持的GPU设备上。加载模型后，您可以按原样训练它，或者在您的模型经过量化的情况下，附加适配器并在其上训练适配器。

与Flash Attention 1相反，集成使得可以在包含填充令牌的任意数据集上训练您的模型成为可能。

### 使用NEFTune增强模型性能

NEFTune是一种提高聊天模型性能的技术，由Jain等人在论文["NEFTune: Noisy Embeddings Improve Instruction Finetuning"](https://arxiv.org/abs/2310.05914)中介绍。它包括在训练过程中向嵌入向量添加噪声。根据论文的摘要：

> 使用Alpaca对LLaMA-2-7B进行标准微调，在AlpacaEval上达到29.79%，使用嘈杂的嵌入提高到64.69%。NEFTune还改善了现代指令数据集上的强基线。使用Evol-Instruct训练的模型看到了10%的改进，使用ShareGPT看到了8%的改进，使用OpenPlatypus看到了8%的改进。即使是进一步通过RLHF精细调整的强大模型，如LLaMA-2-Chat，也受益于NEFTune的额外训练。

![](../Images/52cfc6d8e877350238b9e92fa688af9a.png)

要在`SFTTrainer`中使用它，只需在创建`SFTTrainer`实例时传递`neftune_noise_alpha`。请注意，为了避免任何意外行为，在训练后禁用NEFTune以恢复嵌入层的原始行为。

```py
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset("imdb", split="train")

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
    neftune_noise_alpha=5,
)
trainer.train()
```

我们通过在[OpenAssistant数据集](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)上训练`mistralai/Mistral-7B-v0.1`来测试NEFTune，并验证使用NEFTune导致MT Bench性能提升约25%。

![](../Images/2041067e08b1835ad3fb89fb4c7255c5.png)

但请注意，性能增益的数量是*数据集相关*的，特别是在合成数据集（如[UltraChat](https://huggingface.co/datasets/stingning/ultrachat)）上应用 NEFTune 通常会产生较小的增益。

### 使用 unsloth 加速微调 2 倍

您可以使用完全兼容`SFTTrainer`的[`unsloth`](https://github.com/unslothai/unsloth)库进一步加速 QLoRA / LoRA（速度提高 2 倍，内存减少 60%）。目前，`unsloth`仅支持 Llama（Yi、TinyLlama、Qwen、Deepseek 等）和 Mistral 架构。以下是 1x A100 的一些基准测试：

| 1 A100 40GB | 数据集 | 🤗 | 🤗 + 闪存注意力 2 | 🦥 Unsloth | 🦥 VRAM 已保存 |
| --- | --- | --- | --- | --- | --- |
| Code Llama 34b | 瘦鲸鱼 | 1x | 1.01x | **1.94x** | -22.7% |
| Llama-2 7b | 瘦鲸鱼 | 1x | 0.96x | **1.87x** | -39.3% |
| Mistral 7b | 瘦鲸鱼 | 1x | 1.17x | **1.88x** | -65.9% |
| Tiny Llama 1.1b | 羊驼 | 1x | 1.55x | **2.74x** | -57.8% |

首先根据[官方文档](https://github.com/unslothai/unsloth)安装`unsloth`。安装完成后，您可以非常简单地将 unsloth 整合到您的工作流程中；只需加载`FastLanguageModel`，而不是加载`AutoModelForCausalLM`，如下所示：

```py
import torch
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import FastLanguageModel

max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b",
    max_seq_length = max_seq_length,
    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Dropout = 0 is currently optimized
    bias = "none",    # Bias = "none" is currently optimized
    use_gradient_checkpointing = True,
    random_state = 3407,
)

args = TrainingArguments(output_dir = "./output")

trainer = SFTTrainer(
    model = model,
    args = args,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
)
trainer.train()
```

保存的模型与 Hugging Face 的 transformers 库完全兼容。在他们的[官方存储库](https://github.com/unslothai/unsloth)中了解更多关于 unsloth 的信息。

## 最佳实践

在使用该训练器训练模型时，请注意以下最佳实践：

+   [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) 默认情况下总是将序列填充到[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)的`max_seq_length`参数。如果没有传递值，训练器将从分词器中检索该值。一些分词器不提供默认值，因此有一个检查以检索 2048 和该值之间的最小值。在训练之前，请务必检查。

+   对于在 8 位中训练适配器，您可能需要调整 PEFT 的`prepare_model_for_kbit_training`方法的参数，因此我们建议用户使用`prepare_in_int8_kwargs`字段，或在[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)之外创建`PeftModel`并传递它。

+   为了更节省内存地使用适配器进行训练，您可以在 8 位中加载基础模型，只需在创建[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)时添加`load_in_8bit`参数，或在训练器之外创建一个 8 位的基础模型并传递它。

+   如果您在训练器之外创建模型，请确保不要向训练器传递相对于`from_pretrained()`方法的任何额外关键字参数。

## GPTQ 转换

在完成训练后，您可能会遇到一些 GPTQ 量化的问题。将`gradient_accumulation_steps`降低到`4`将解决大部分在量化过程中到 GPTQ 格式的问题。

## SFTTrainer

### `class trl.SFTTrainer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)

```py
( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None peft_config: Optional = None dataset_text_field: Optional = None packing: Optional = False formatting_func: Optional = None max_seq_length: Optional = None infinite: Optional = None num_of_sequences: Optional = 1024 chars_per_token: Optional = 3.6 dataset_num_proc: Optional = None dataset_batch_size: int = 1000 neftune_noise_alpha: Optional = None model_init_kwargs: Optional = None dataset_kwargs: Optional = None )
```

参数

+   `model`（Union[`transformers.PreTrainedModel`, `nn.Module`, `str`）— 要训练的模型，可以是`PreTrainedModel`，`torch.nn.Module`或包含要从缓存加载或下载的模型名称的字符串。如果将`PeftConfig`对象传递给`peft_config`参数，则模型也可以转换为`PeftModel`。

+   `args`（Optional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments)）— 用于调整训练的参数。请参考`transformers.TrainingArguments`的官方文档以获取更多信息。

+   `data_collator`（可选`transformers.DataCollator`）— 用于训练的数据整理器。

+   `train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)) — 用于训练的数据集。我们建议用户使用`trl.trainer.ConstantLengthDataset`来创建他们的数据集。

+   `eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]) — 用于评估的数据集。我们建议用户使用`trl.trainer.ConstantLengthDataset`来创建他们的数据集。

+   `tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)) — 用于训练的分词器。如果未指定，将使用与模型关联的分词器。

+   `model_init` (`Callable[[], transformers.PreTrainedModel]`) — 用于训练的模型初始化器。如果指定为None，则将使用默认的模型初始化器。

+   `compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None) — 用于在评估过程中计算指标的函数。它应该返回一个将指标名称映射到指标值的字典。如果未指定，评估过程中只会计算损失。

+   `callbacks` (`List[transformers.TrainerCallback]`) — 用于训练的回调函数。

+   `optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`) — 用于训练的优化器和调度器。

+   `preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`) — 用于在计算指标之前预处理logits的函数。

+   `peft_config` (`Optional[PeftConfig]`) — 用于初始化PeftModel的PeftConfig对象。

+   `dataset_text_field` (`Optional[str]`) — 数据集的文本字段的名称，如果用户传递了这个参数，训练器将自动基于`dataset_text_field`参数创建一个`ConstantLengthDataset`。

+   `formatting_func` (`Optional[Callable]`) — 用于创建`ConstantLengthDataset`的格式化函数。

+   `max_seq_length` (`Optional[int]`) — 用于`ConstantLengthDataset`和自动创建数据集的最大序列长度。默认为`512`。

+   `infinite` (`Optional[bool]`) — 是否使用无限数据集。默认为`False`。

+   `num_of_sequences` (`Optional[int]`) — 用于`ConstantLengthDataset`的序列数。默认为`1024`。

+   `chars_per_token` (`Optional[float]`) — 用于`ConstantLengthDataset`的每个标记的字符数。默认为`3.6`。您可以在stack-llama示例中查看如何计算这个值：[https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53)。

+   `packing` (`Optional[bool]`) — 仅在传递了`dataset_text_field`的情况下使用。这个参数由`ConstantLengthDataset`用于打包数据集的序列。

+   `dataset_num_proc` (`Optional[int]`) — 用于标记化数据的工作进程数。仅在`packing=False`时使用。默认为None。

+   `dataset_batch_size` (`int`) — 每批要标记化的示例数。如果batch_size <= 0或batch_size == None，则将整个数据集标记为单个批次。默认为1000。

+   `neftune_noise_alpha` (`Optional[float]`) — 如果不是 `None`，则会激活 NEFTune 噪声嵌入。这已被证明极大地提高了指令微调模型的性能。查看原始论文：[https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914) 和原始代码：[https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune) model_init_kwargs — (`Optional[Dict]`, *optional*): 传递给实例化模型时的可选参数字典 dataset_kwargs — (`Optional[Dict]`, *optional*): 创建打包或非打包数据集时传递的可选参数字典

监督微调训练器（SFT Trainer）的类定义。这个类是 `transformers.Trainer` 类的包装器，并继承了它的所有属性和方法。训练器负责在用户传递 `PeftConfig` 对象时正确初始化 PeftModel。

## ConstantLengthDataset

### `class trl.trainer.ConstantLengthDataset`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/utils.py#L355)

```py
( tokenizer dataset dataset_text_field = None formatting_func = None infinite = False seq_length = 1024 num_of_sequences = 1024 chars_per_token = 3.6 eos_token_id = 0 shuffle = True append_concat_token = True add_special_tokens = True )
```

参数

+   `tokenizer` (`transformers.PreTrainedTokenizer`) — 用于处理数据的处理器。

+   `dataset` (`dataset.Dataset`) — 包含文本文件的数据集。

+   `dataset_text_field` (`str`, **optional**) — 数据集中包含文本的字段名称。仅在 `formatting_func` 为 `None` 时使用。

+   `formatting_func` (`Callable`, **optional**) — 在分词之前格式化文本的函数。通常建议遵循特定模式，如 `"### 问题：{question} ### 答案：{answer}"`

+   `infinite` (`bool`, *optional*, defaults to `False`) — 如果为 True，则在数据集到达末尾后重置迭代器，否则停止。

+   `seq_length` (`int`, *optional*, defaults to `1024`) — 要返回的令牌序列的长度。

+   `num_of_sequences` (`int`, *optional*, defaults to `1024`) — 在缓冲区中保留的令牌序列数。

+   `chars_per_token` (`int`, *optional*, defaults to `3.6`) — 用于估计文本缓冲区中令牌数量的每个令牌的字符数。

+   `eos_token_id` (`int`, *optional*, defaults to `0`) — 如果传递的分词器没有 EOS 标记，则为序列结束标记的 ID。

+   `shuffle` (‘bool’, *optional*, defaults to True) — 在返回示例之前对示例进行洗牌

+   `append_concat_token` (‘bool’, *optional*, defaults to True) — 如果为 True，则在每个被打包的样本末尾附加 `eos_token_id`。

+   `add_special_tokens` (‘bool’, *optional*, defaults to True) — 如果为 True，则分词器会为每个被打包的样本添加特殊标记。

可迭代数据集，从文本文件流中返回恒定长度的令牌块。数据集还会在分词之前使用用户提供的特定格式对文本进行格式化。
