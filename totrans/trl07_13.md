# ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)

ç›‘ç£å¾®è°ƒï¼ˆæˆ–ç®€ç§°SFTï¼‰æ˜¯RLHFä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚åœ¨TRLä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„APIæ¥åˆ›å»ºæ‚¨çš„SFTæ¨¡å‹ï¼Œå¹¶ç”¨å‡ è¡Œä»£ç åœ¨æ‚¨çš„æ•°æ®é›†ä¸Šè®­ç»ƒå®ƒä»¬ã€‚

åœ¨[`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py)ä¸­æŸ¥çœ‹ä¸€ä¸ªå®Œæ•´çµæ´»çš„ç¤ºä¾‹ã€‚

## å¿«é€Ÿå¼€å§‹

å¦‚æœæ‚¨åœ¨ğŸ¤— Hubä¸Šæ‰˜ç®¡äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)ä»TRLè½»æ¾å¾®è°ƒæ‚¨çš„SFTæ¨¡å‹ã€‚å‡è®¾æ‚¨çš„æ•°æ®é›†æ˜¯`imdb`ï¼Œæ‚¨è¦é¢„æµ‹çš„æ–‡æœ¬ä½äºæ•°æ®é›†çš„`text`å­—æ®µä¸­ï¼Œæ‚¨æƒ³è¦å¾®è°ƒ`facebook/opt-350m`æ¨¡å‹ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µä¼šå¤„ç†æ‰€æœ‰çš„æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒå·¥ä½œï¼š

```py
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset("imdb", split="train")

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
)
trainer.train()
```

ç¡®ä¿ä¸º`max_seq_length`ä¼ é€’æ­£ç¡®çš„å€¼ï¼Œå› ä¸ºé»˜è®¤å€¼å°†è®¾ç½®ä¸º`min(tokenizer.model_max_length, 1024)`ã€‚

æ‚¨è¿˜å¯ä»¥åœ¨è®­ç»ƒå¸ˆä¹‹å¤–æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’å¦‚ä¸‹ï¼š

```py
from transformers import AutoModelForCausalLM
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset("imdb", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
)

trainer.train()
```

ä¸Šè¿°ä»£ç ç‰‡æ®µå°†ä½¿ç”¨[`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ç±»ä¸­çš„é»˜è®¤è®­ç»ƒå‚æ•°ã€‚å¦‚æœæ‚¨æƒ³ä¿®æ”¹å®ƒï¼Œè¯·ç¡®ä¿åˆ›å»ºè‡ªå·±çš„`TrainingArguments`å¯¹è±¡å¹¶å°†å…¶ä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ„é€ å‡½æ•°ï¼Œå°±åƒåœ¨[`supervised_finetuning.py`è„šæœ¬](https://github.com/huggingface/trl/blob/main/examples/stack_llama/scripts/supervised_finetuning.py)ä¸­çš„stack-llamaç¤ºä¾‹ä¸­æ‰€åšçš„é‚£æ ·ã€‚

## é«˜çº§ç”¨æ³•

### ä»…åœ¨å®Œæˆä¸Šè®­ç»ƒ

æ‚¨å¯ä»¥ä½¿ç”¨`DataCollatorForCompletionOnlyLM`ä»…åœ¨ç”Ÿæˆæç¤ºä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™ä»…åœ¨`packing=False`çš„æƒ…å†µä¸‹æœ‰æ•ˆã€‚ä¸ºäº†ä¸ºæŒ‡ä»¤æ•°æ®å®ä¾‹åŒ–è¯¥æ”¶é›†å™¨ï¼Œä¼ é€’ä¸€ä¸ªå“åº”æ¨¡æ¿å’Œåˆ†è¯å™¨ã€‚ä»¥ä¸‹æ˜¯åœ¨CodeAlpacaæ•°æ®é›†ä¸Šä»…å¯¹å®Œæˆè¿›è¡Œå¾®è°ƒ`opt-350m`çš„å·¥ä½œç¤ºä¾‹ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

dataset = load_dataset("lucasmccabe-lmi/CodeAlpaca-20k", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['instruction'])):
        text = f"### Question: {example['instruction'][i]}\n ### Answer: {example['output'][i]}"
        output_texts.append(text)
    return output_texts

response_template = " ### Answer:"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    formatting_func=formatting_prompts_func,
    data_collator=collator,
)

trainer.train()
```

ä¸ºåŠ©æ‰‹é£æ ¼å¯¹è¯æ•°æ®å®ä¾‹åŒ–è¯¥æ”¶é›†å™¨æ—¶ï¼Œè¯·ä¼ é€’ä¸€ä¸ªå“åº”æ¨¡æ¿ã€ä¸€ä¸ªæŒ‡ä»¤æ¨¡æ¿å’Œåˆ†è¯å™¨ã€‚ä»¥ä¸‹æ˜¯åœ¨Open Assistant Guanacoæ•°æ®é›†ä¸Šä»…å¯¹åŠ©æ‰‹å®Œæˆè¿›è¡Œå¾®è°ƒ`opt-350m`çš„å·¥ä½œç¤ºä¾‹ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

instruction_template = "### Human:"
response_template = "### Assistant:"
collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field="text",
    data_collator=collator,
)

trainer.train()
```

ç¡®ä¿æœ‰ä¸€ä¸ª`pad_token_id`ï¼Œå®ƒä¸`eos_token_id`ä¸åŒï¼Œè¿™å¯ä»¥é˜²æ­¢æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ— æ³•æ­£ç¡®é¢„æµ‹EOSï¼ˆå¥å­ç»“æŸï¼‰æ ‡è®°ã€‚

#### ç›´æ¥ä½¿ç”¨token_idsä½œä¸ºresponse_template

ä¸€äº›åˆ†è¯å™¨ï¼ˆå¦‚Llama 2ï¼ˆ`meta-llama/Llama-2-XXb-hf`ï¼‰ï¼‰æ ¹æ®æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡è€Œä¸åŒåœ°å¯¹åºåˆ—è¿›è¡Œåˆ†è¯ã€‚ä¾‹å¦‚ï¼š

```py
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

def print_tokens_with_ids(txt):
    tokens = tokenizer.tokenize(txt, add_special_tokens=False)
    token_ids = tokenizer.encode(txt, add_special_tokens=False)
    print(list(zip(tokens, token_ids)))

prompt = """### User: Hello\n\n### Assistant: Hi, how can I help you?"""
print_tokens_with_ids(prompt)  # [..., ('â–Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('â–Ass', 4007), ('istant', 22137), (':', 29901), ...]

response_template = "### Assistant:"
print_tokens_with_ids(response_template)  # [('â–###', 835), ('â–Ass', 4007), ('istant', 22137), (':', 29901)]
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äº`response_template`ä¸­ç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œç›¸åŒçš„å­—ç¬¦ä¸²ï¼ˆâ€œ### Assistant:â€ï¼‰è¢«ä¸åŒåœ°åˆ†è¯ï¼š

+   æ–‡æœ¬ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰ï¼š`[2277, 29937, 4007, 22137, 29901]`

+   `response_template`ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ï¼š`[835, 4007, 22137, 29901]`

å½“`DataCollatorForCompletionOnlyLM`åœ¨æ•°æ®é›†ç¤ºä¾‹æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°`response_template`æ—¶ï¼Œè¿™å°†å¯¼è‡´é”™è¯¯ï¼š

```py
RuntimeError: Could not find response key [835, 4007, 22137, 29901] in token IDs tensor([    1,   835,  ...])
```

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸æ•°æ®é›†ä¸­ç›¸åŒä¸Šä¸‹æ–‡çš„æ–¹å¼å¯¹`response_template`è¿›è¡Œåˆ†è¯ï¼Œæ ¹æ®éœ€è¦æˆªæ–­å®ƒï¼Œå¹¶å°†`token_ids`ç›´æ¥ä¼ é€’ç»™`DataCollatorForCompletionOnlyLM`ç±»çš„`response_template`å‚æ•°ã€‚ä¾‹å¦‚ï¼š

```py
response_template_with_context = "\n### Assistant:"  # We added context here: "\n". This is enough for this tokenizer
response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`

data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)
```

### ä¸ºèŠå¤©æ ¼å¼æ·»åŠ ç‰¹æ®Šæ ‡è®°

ä¸ºè¯­è¨€æ¨¡å‹æ·»åŠ ç‰¹æ®Šæ ‡è®°å¯¹äºè®­ç»ƒèŠå¤©æ¨¡å‹è‡³å…³é‡è¦ã€‚è¿™äº›æ ‡è®°è¢«æ·»åŠ åœ¨å¯¹è¯ä¸­çš„ä¸åŒè§’è‰²ä¹‹é—´ï¼Œå¦‚ç”¨æˆ·ã€åŠ©æ‰‹å’Œç³»ç»Ÿï¼Œå¹¶å¸®åŠ©æ¨¡å‹è¯†åˆ«å¯¹è¯çš„ç»“æ„å’Œæµç¨‹ã€‚è¿™ç§è®¾ç½®å¯¹äºä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨èŠå¤©ç¯å¢ƒä¸­ç”Ÿæˆè¿è´¯å’Œä¸Šä¸‹æ–‡é€‚å½“çš„å“åº”è‡³å…³é‡è¦ã€‚`trl`ä¸­çš„`setup_chat_format()`å‡½æ•°å¯ä»¥è½»æ¾ä¸ºä¼šè¯AIä»»åŠ¡è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™ä¸ªå‡½æ•°ï¼š

+   å‘æ ‡è®°åŒ–å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚`<|im_start|>`å’Œ`<|imm_end|>`ï¼Œä»¥æŒ‡ç¤ºä¼šè¯çš„å¼€å§‹å’Œç»“æŸã€‚

+   è°ƒæ•´æ¨¡å‹çš„åµŒå…¥å±‚ä»¥å®¹çº³æ–°çš„æ ‡è®°ã€‚

+   è®¾ç½®ä»¤ç‰ŒåŒ–å™¨çš„`chat_template`ï¼Œç”¨äºå°†è¾“å…¥æ•°æ®æ ¼å¼åŒ–ä¸ºç±»ä¼¼èŠå¤©çš„æ ¼å¼ã€‚é»˜è®¤å€¼ä¸ºOpenAIçš„`chatml`ã€‚

+   *å¯é€‰*æ‚¨å¯ä»¥ä¼ é€’`resize_to_multiple_of`æ¥å°†åµŒå…¥å±‚è°ƒæ•´ä¸º`resize_to_multiple_of`å‚æ•°çš„å€æ•°ï¼Œä¾‹å¦‚64ã€‚å¦‚æœæ‚¨å¸Œæœ›å°†æ¥æ”¯æŒæ›´å¤šæ ¼å¼ï¼Œè¯·åœ¨[trl](https://github.com/huggingface/trl)ä¸Šæ‰“å¼€GitHubé—®é¢˜

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

# Set up the chat format with default 'chatml' format
model, tokenizer = setup_chat_format(model, tokenizer)

```

æœ‰äº†æˆ‘ä»¬çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¾ç½®ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨å¯¹è¯æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ ¼å¼åŒ–æ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„ç¤ºä¾‹ã€‚

### æ•°æ®é›†æ ¼å¼æ”¯æŒ

[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ”¯æŒæµè¡Œçš„æ•°æ®é›†æ ¼å¼ã€‚è¿™ä½¿æ‚¨å¯ä»¥ç›´æ¥å°†æ•°æ®é›†ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œæ— éœ€ä»»ä½•é¢„å¤„ç†ã€‚æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š

+   å¯¹è¯æ ¼å¼

```py
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "..."}]}
```

+   æŒ‡ä»¤æ ¼å¼

```py
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
```

å¦‚æœæ‚¨çš„æ•°æ®é›†ä½¿ç”¨ä¸Šè¿°æ ¼å¼ä¹‹ä¸€ï¼Œæ‚¨å¯ä»¥ç›´æ¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨è€Œæ— éœ€é¢„å¤„ç†ã€‚[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)å°†ä½¿ç”¨æ¨¡å‹çš„åˆ†è¯å™¨ä¸­å®šä¹‰çš„æ ¼å¼ä½¿ç”¨[apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)æ–¹æ³•ä¸ºæ‚¨æ ¼å¼åŒ–æ•°æ®é›†ã€‚

```py
from datasets import load_dataset
from trl import SFTTrainer

...

# load jsonl dataset
dataset = load_dataset("json", data_files="path/to/dataset.jsonl", split="train")
# load dataset from the HuggingFace Hub
dataset = load_dataset("philschmid/dolly-15k-oai-style", split="train")

...

trainer = SFTTrainer(
    "facebook/opt-350m",
    args=training_args,
    train_dataset=dataset,
    packing=True,
)
```

å¦‚æœæ•°æ®é›†ä¸æ˜¯è¿™äº›æ ¼å¼ä¹‹ä¸€ï¼Œæ‚¨å¯ä»¥é¢„å¤„ç†æ•°æ®é›†ä»¥åŒ¹é…æ ¼å¼ï¼Œæˆ–è€…å°†æ ¼å¼åŒ–å‡½æ•°ä¼ é€’ç»™SFTTrainerä»¥ä»£æ›¿ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚

### æ ¼å¼åŒ–æ‚¨çš„è¾“å…¥æç¤º

å¯¹äºæŒ‡ä»¤å¾®è°ƒï¼Œé€šå¸¸åœ¨æ•°æ®é›†ä¸­æœ‰ä¸¤åˆ—ï¼šä¸€ä¸ªç”¨äºæç¤ºï¼Œå¦ä¸€ä¸ªç”¨äºå“åº”ã€‚è¿™ä½¿äººä»¬å¯ä»¥åƒ[Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)é‚£æ ·æ ¼å¼åŒ–ç¤ºä¾‹ï¼š

```py
Below is an instruction ...

### Instruction
{prompt}

### Response:
{completion}
```

å‡è®¾æ‚¨çš„æ•°æ®é›†æœ‰ä¸¤ä¸ªå­—æ®µï¼Œ`question`å’Œ`answer`ã€‚å› æ­¤æ‚¨å¯ä»¥ç›´æ¥è¿è¡Œï¼š

```py
...
def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['question'])):
        text = f"### Question: {example['question'][i]}\n ### Answer: {example['answer'][i]}"
        output_texts.append(text)
    return output_texts

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    formatting_func=formatting_prompts_func,
)

trainer.train()
```

ä¸ºäº†æ­£ç¡®æ ¼å¼åŒ–æ‚¨çš„è¾“å…¥ï¼Œè¯·ç¡®ä¿é€šè¿‡å¾ªç¯å¤„ç†æ‰€æœ‰ç¤ºä¾‹å¹¶è¿”å›å¤„ç†è¿‡çš„æ–‡æœ¬åˆ—è¡¨ã€‚æŸ¥çœ‹å¦‚ä½•åœ¨alpacaæ•°æ®é›†ä¸Šä½¿ç”¨SFTTrainerçš„å®Œæ•´ç¤ºä¾‹[è¿™é‡Œ](https://github.com/huggingface/trl/pull/444#issue-1760952763)

### æ‰“åŒ…æ•°æ®é›†ï¼ˆConstantLengthDatasetï¼‰

[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ”¯æŒ*ç¤ºä¾‹æ‰“åŒ…*ï¼Œå…¶ä¸­å¤šä¸ªçŸ­ç¤ºä¾‹æ‰“åŒ…åœ¨åŒä¸€è¾“å…¥åºåˆ—ä¸­ä»¥å¢åŠ è®­ç»ƒæ•ˆç‡ã€‚è¿™æ˜¯é€šè¿‡`ConstantLengthDataset`å®ç”¨ç¨‹åºç±»å®Œæˆçš„ï¼Œè¯¥ç±»ä»ç¤ºä¾‹æµä¸­è¿”å›å¸¸é•¿åº¦çš„ä»¤ç‰Œå—ã€‚è¦å¯ç”¨æ­¤æ•°æ®é›†ç±»çš„ä½¿ç”¨ï¼Œåªéœ€å°†`packing=True`ä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ„é€ å‡½æ•°ã€‚

```py
...

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    packing=True
)

trainer.train()
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨æ‰“åŒ…æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå‚æ•°ä¸­ä¼ é€’äº†`max_steps`ï¼Œåˆ™æ‚¨å¯èƒ½ä¼šè®­ç»ƒæ¨¡å‹å¤šä¸ªå‘¨æœŸï¼Œå…·ä½“å–å†³äºæ‚¨å¦‚ä½•é…ç½®æ‰“åŒ…æ•°æ®é›†å’Œè®­ç»ƒåè®®ã€‚è¯·ç¡®ä¿æ‚¨çŸ¥é“å¹¶ç†è§£æ‚¨æ­£åœ¨åšä»€ä¹ˆã€‚

#### ä½¿ç”¨æ‰“åŒ…æ•°æ®é›†è‡ªå®šä¹‰æ‚¨çš„æç¤º

å¦‚æœæ‚¨çš„æ•°æ®é›†æœ‰å‡ ä¸ªå­—æ®µè¦åˆå¹¶ï¼Œä¾‹å¦‚æ•°æ®é›†æœ‰`question`å’Œ`answer`å­—æ®µï¼Œæ‚¨æƒ³è¦åˆå¹¶å®ƒä»¬ï¼Œæ‚¨å¯ä»¥å°†æ ¼å¼åŒ–å‡½æ•°ä¼ é€’ç»™è®­ç»ƒå™¨æ¥å¤„ç†ã€‚ä¾‹å¦‚ï¼š

```py
def formatting_func(example):
    text = f"### Question: {example['question']}\n ### Answer: {example['answer']}"
    return text

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    packing=True,
    formatting_func=formatting_func
)

trainer.train()
```

æ‚¨è¿˜å¯ä»¥é€šè¿‡ç›´æ¥å°†å‚æ•°ä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ„é€ å‡½æ•°æ¥æ›´å¤šåœ°å®šåˆ¶`ConstantLengthDataset`ã€‚è¯·å‚è€ƒè¯¥ç±»çš„ç­¾åä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

### å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æ§åˆ¶

æ‚¨å¯ä»¥ç›´æ¥å°†`from_pretrained()`æ–¹æ³•çš„kwargsä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦ä»¥ä¸åŒçš„ç²¾åº¦åŠ è½½æ¨¡å‹ï¼Œç±»ä¼¼äº

```py
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.bfloat16)
```

```py
...

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    model_init_kwargs={
        "torch_dtype": torch.bfloat16,
    },
)

trainer.train()
```

è¯·æ³¨æ„ï¼Œ`from_pretrained()`çš„æ‰€æœ‰å…³é”®å­—å‚æ•°éƒ½å—æ”¯æŒã€‚

### è®­ç»ƒé€‚é…å™¨

æˆ‘ä»¬è¿˜æ”¯æŒä¸ğŸ¤— PEFTåº“çš„ç´§å¯†é›†æˆï¼Œä»¥ä¾¿ä»»ä½•ç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°è®­ç»ƒé€‚é…å™¨å¹¶åœ¨Hubä¸Šå…±äº«å®ƒä»¬ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ•´ä¸ªæ¨¡å‹

```py
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig

dataset = load_dataset("imdb", split="train")

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

trainer = SFTTrainer(
    "EleutherAI/gpt-neo-125m",
    train_dataset=dataset,
    dataset_text_field="text",
    peft_config=peft_config
)

trainer.train()
```

æ‚¨è¿˜å¯ä»¥ç»§ç»­è®­ç»ƒæ‚¨çš„`PeftModel`ã€‚ä¸ºæ­¤ï¼Œè¯·é¦–å…ˆåœ¨`SFTTrainer`ä¹‹å¤–åŠ è½½`PeftModel`ï¼Œå¹¶ç›´æ¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œè€Œæ— éœ€ä¼ é€’`peft_config`å‚æ•°ã€‚

### ä½¿ç”¨åŸºç¡€8ä½æ¨¡å‹è®­ç»ƒé€‚é…å™¨

ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦é¦–å…ˆåœ¨Trainerä¹‹å¤–åŠ è½½æ‚¨çš„8ä½æ¨¡å‹ï¼Œå¹¶å°†`PeftConfig`ä¼ é€’ç»™è®­ç»ƒå™¨ã€‚ä¾‹å¦‚ï¼š

```py
...

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = AutoModelForCausalLM.from_pretrained(
    "EleutherAI/gpt-neo-125m",
    load_in_8bit=True,
    device_map="auto",
)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field="text",
    peft_config=peft_config,
)

trainer.train()
```

## ä½¿ç”¨Flash Attentionå’ŒFlash Attention 2

æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨SFTTrainerä¸­çš„Flash Attention 1å’Œ2æ¥è·ç›Šï¼Œåªéœ€è¿›è¡Œæœ€å°‘çš„ä»£ç æ›´æ”¹ã€‚é¦–å…ˆï¼Œä¸ºäº†ç¡®ä¿æ‚¨æ‹¥æœ‰æ¥è‡ªtransformersçš„æ‰€æœ‰æœ€æ–°åŠŸèƒ½ï¼Œè¯·ä»æºä»£ç å®‰è£…transformers

```py
pip install -U git+https://github.com/huggingface/transformers.git
```

è¯·æ³¨æ„ï¼ŒFlash Attentionç°åœ¨ä»…åœ¨GPUä¸Šè¿è¡Œï¼Œå¹¶ä¸”åœ¨åŠç²¾åº¦åˆ¶åº¦ä¸‹è¿è¡Œï¼ˆå½“ä½¿ç”¨é€‚é…å™¨æ—¶ï¼ŒåŸºç¡€æ¨¡å‹ä»¥åŠç²¾åº¦åŠ è½½ï¼‰ã€‚è¿˜è¯·æ³¨æ„ï¼Œè¿™ä¸¤ä¸ªåŠŸèƒ½ä¸é‡åŒ–ç­‰å…¶ä»–å·¥å…·å®Œå…¨å…¼å®¹ã€‚

### ä½¿ç”¨Flash-Attention 1

å¯¹äºFlash Attention 1ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`BetterTransformer` APIå¹¶å¼ºåˆ¶åˆ†æ´¾APIä»¥ä½¿ç”¨Flash Attentionå†…æ ¸ã€‚é¦–å…ˆï¼Œå®‰è£…æœ€æ–°çš„optimumåŒ…ï¼š

```py
pip install -U optimum
```

åŠ è½½æ¨¡å‹åï¼Œå°†`trainer.train()`è°ƒç”¨åŒ…è£…åœ¨`with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼š

```py
...

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    trainer.train()
```

è¯·æ³¨æ„ï¼Œå¦‚æœä½¿ç”¨Flash Attentionå†…æ ¸ï¼Œæ‚¨ä¸èƒ½åœ¨ä»»æ„æ•°æ®é›†ä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹ï¼Œå› ä¸º`torch.scaled_dot_product_attention`ä¸æ”¯æŒä½¿ç”¨å¡«å……ä»¤ç‰Œè¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œæ‚¨åªèƒ½åœ¨`packing=True`çš„æƒ…å†µä¸‹ä½¿ç”¨è¯¥åŠŸèƒ½ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†åŒ…å«å¡«å……ä»¤ç‰Œï¼Œè¯·è€ƒè™‘åˆ‡æ¢åˆ°Flash Attention 2é›†æˆã€‚

ä»¥ä¸‹æ˜¯åœ¨å•ä¸ªNVIDIA-T4 16GBä¸Šä½¿ç”¨Flash Attention 1å¯ä»¥è·å¾—çš„ä¸€äº›åŠ é€Ÿå’Œå†…å­˜æ•ˆç‡æ–¹é¢çš„æ•°å­—ã€‚

| use_flash_attn_1 | model_name | max_seq_len | batch_size | time per training step |
| --- | --- | --- | --- | --- |
| x | facebook/opt-350m | 2048 | 8 | ~59.1s |
|  | facebook/opt-350m | 2048 | 8 | **OOM** |
| x | facebook/opt-350m | 2048 | 4 | ~30.3s |
|  | facebook/opt-350m | 2048 | 4 | ~148.9s |

### ä½¿ç”¨Flash Attention-2

è¦ä½¿ç”¨Flash Attention 2ï¼Œé¦–å…ˆå®‰è£…æœ€æ–°çš„`flash-attn`åŒ…ï¼š

```py
pip install -U flash-attn
```

åœ¨è°ƒç”¨`from_pretrained`æ—¶æ·»åŠ `use_flash_attention_2=True`ï¼š

```py
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    use_flash_attention_2=True
)
```

å¦‚æœä¸ä½¿ç”¨é‡åŒ–ï¼Œè¯·ç¡®ä¿æ‚¨çš„æ¨¡å‹ä»¥åŠç²¾åº¦åŠ è½½ï¼Œå¹¶å°†æ‚¨çš„æ¨¡å‹åˆ†æ´¾åˆ°æ”¯æŒçš„GPUè®¾å¤‡ä¸Šã€‚åŠ è½½æ¨¡å‹åï¼Œæ‚¨å¯ä»¥æŒ‰åŸæ ·è®­ç»ƒå®ƒï¼Œæˆ–è€…åœ¨æ‚¨çš„æ¨¡å‹ç»è¿‡é‡åŒ–çš„æƒ…å†µä¸‹ï¼Œé™„åŠ é€‚é…å™¨å¹¶åœ¨å…¶ä¸Šè®­ç»ƒé€‚é…å™¨ã€‚

ä¸Flash Attention 1ç›¸åï¼Œé›†æˆä½¿å¾—å¯ä»¥åœ¨åŒ…å«å¡«å……ä»¤ç‰Œçš„ä»»æ„æ•°æ®é›†ä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚

### ä½¿ç”¨NEFTuneå¢å¼ºæ¨¡å‹æ€§èƒ½

NEFTuneæ˜¯ä¸€ç§æé«˜èŠå¤©æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œç”±Jainç­‰äººåœ¨è®ºæ–‡["NEFTune: Noisy Embeddings Improve Instruction Finetuning"](https://arxiv.org/abs/2310.05914)ä¸­ä»‹ç»ã€‚å®ƒåŒ…æ‹¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘åµŒå…¥å‘é‡æ·»åŠ å™ªå£°ã€‚æ ¹æ®è®ºæ–‡çš„æ‘˜è¦ï¼š

> ä½¿ç”¨Alpacaå¯¹LLaMA-2-7Bè¿›è¡Œæ ‡å‡†å¾®è°ƒï¼Œåœ¨AlpacaEvalä¸Šè¾¾åˆ°29.79%ï¼Œä½¿ç”¨å˜ˆæ‚çš„åµŒå…¥æé«˜åˆ°64.69%ã€‚NEFTuneè¿˜æ”¹å–„äº†ç°ä»£æŒ‡ä»¤æ•°æ®é›†ä¸Šçš„å¼ºåŸºçº¿ã€‚ä½¿ç”¨Evol-Instructè®­ç»ƒçš„æ¨¡å‹çœ‹åˆ°äº†10%çš„æ”¹è¿›ï¼Œä½¿ç”¨ShareGPTçœ‹åˆ°äº†8%çš„æ”¹è¿›ï¼Œä½¿ç”¨OpenPlatypusçœ‹åˆ°äº†8%çš„æ”¹è¿›ã€‚å³ä½¿æ˜¯è¿›ä¸€æ­¥é€šè¿‡RLHFç²¾ç»†è°ƒæ•´çš„å¼ºå¤§æ¨¡å‹ï¼Œå¦‚LLaMA-2-Chatï¼Œä¹Ÿå—ç›ŠäºNEFTuneçš„é¢å¤–è®­ç»ƒã€‚

![](../Images/52cfc6d8e877350238b9e92fa688af9a.png)

è¦åœ¨`SFTTrainer`ä¸­ä½¿ç”¨å®ƒï¼Œåªéœ€åœ¨åˆ›å»º`SFTTrainer`å®ä¾‹æ—¶ä¼ é€’`neftune_noise_alpha`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†é¿å…ä»»ä½•æ„å¤–è¡Œä¸ºï¼Œåœ¨è®­ç»ƒåç¦ç”¨NEFTuneä»¥æ¢å¤åµŒå…¥å±‚çš„åŸå§‹è¡Œä¸ºã€‚

```py
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset("imdb", split="train")

trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
    neftune_noise_alpha=5,
)
trainer.train()
```

æˆ‘ä»¬é€šè¿‡åœ¨[OpenAssistantæ•°æ®é›†](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)ä¸Šè®­ç»ƒ`mistralai/Mistral-7B-v0.1`æ¥æµ‹è¯•NEFTuneï¼Œå¹¶éªŒè¯ä½¿ç”¨NEFTuneå¯¼è‡´MT Benchæ€§èƒ½æå‡çº¦25%ã€‚

![](../Images/2041067e08b1835ad3fb89fb4c7255c5.png)

ä½†è¯·æ³¨æ„ï¼Œæ€§èƒ½å¢ç›Šçš„æ•°é‡æ˜¯*æ•°æ®é›†ç›¸å…³*çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆæˆæ•°æ®é›†ï¼ˆå¦‚[UltraChat](https://huggingface.co/datasets/stingning/ultrachat)ï¼‰ä¸Šåº”ç”¨ NEFTune é€šå¸¸ä¼šäº§ç”Ÿè¾ƒå°çš„å¢ç›Šã€‚

### ä½¿ç”¨ unsloth åŠ é€Ÿå¾®è°ƒ 2 å€

æ‚¨å¯ä»¥ä½¿ç”¨å®Œå…¨å…¼å®¹`SFTTrainer`çš„[`unsloth`](https://github.com/unslothai/unsloth)åº“è¿›ä¸€æ­¥åŠ é€Ÿ QLoRA / LoRAï¼ˆé€Ÿåº¦æé«˜ 2 å€ï¼Œå†…å­˜å‡å°‘ 60%ï¼‰ã€‚ç›®å‰ï¼Œ`unsloth`ä»…æ”¯æŒ Llamaï¼ˆYiã€TinyLlamaã€Qwenã€Deepseek ç­‰ï¼‰å’Œ Mistral æ¶æ„ã€‚ä»¥ä¸‹æ˜¯ 1x A100 çš„ä¸€äº›åŸºå‡†æµ‹è¯•ï¼š

| 1 A100 40GB | æ•°æ®é›† | ğŸ¤— | ğŸ¤— + é—ªå­˜æ³¨æ„åŠ› 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM å·²ä¿å­˜ |
| --- | --- | --- | --- | --- | --- |
| Code Llama 34b | ç˜¦é²¸é±¼ | 1x | 1.01x | **1.94x** | -22.7% |
| Llama-2 7b | ç˜¦é²¸é±¼ | 1x | 0.96x | **1.87x** | -39.3% |
| Mistral 7b | ç˜¦é²¸é±¼ | 1x | 1.17x | **1.88x** | -65.9% |
| Tiny Llama 1.1b | ç¾Šé©¼ | 1x | 1.55x | **2.74x** | -57.8% |

é¦–å…ˆæ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://github.com/unslothai/unsloth)å®‰è£…`unsloth`ã€‚å®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥éå¸¸ç®€å•åœ°å°† unsloth æ•´åˆåˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼›åªéœ€åŠ è½½`FastLanguageModel`ï¼Œè€Œä¸æ˜¯åŠ è½½`AutoModelForCausalLM`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
import torch
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import FastLanguageModel

max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b",
    max_seq_length = max_seq_length,
    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Dropout = 0 is currently optimized
    bias = "none",    # Bias = "none" is currently optimized
    use_gradient_checkpointing = True,
    random_state = 3407,
)

args = TrainingArguments(output_dir = "./output")

trainer = SFTTrainer(
    model = model,
    args = args,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
)
trainer.train()
```

ä¿å­˜çš„æ¨¡å‹ä¸ Hugging Face çš„ transformers åº“å®Œå…¨å…¼å®¹ã€‚åœ¨ä»–ä»¬çš„[å®˜æ–¹å­˜å‚¨åº“](https://github.com/unslothai/unsloth)ä¸­äº†è§£æ›´å¤šå…³äº unsloth çš„ä¿¡æ¯ã€‚

## æœ€ä½³å®è·µ

åœ¨ä½¿ç”¨è¯¥è®­ç»ƒå™¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹æœ€ä½³å®è·µï¼š

+   [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) é»˜è®¤æƒ…å†µä¸‹æ€»æ˜¯å°†åºåˆ—å¡«å……åˆ°[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)çš„`max_seq_length`å‚æ•°ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’å€¼ï¼Œè®­ç»ƒå™¨å°†ä»åˆ†è¯å™¨ä¸­æ£€ç´¢è¯¥å€¼ã€‚ä¸€äº›åˆ†è¯å™¨ä¸æä¾›é»˜è®¤å€¼ï¼Œå› æ­¤æœ‰ä¸€ä¸ªæ£€æŸ¥ä»¥æ£€ç´¢ 2048 å’Œè¯¥å€¼ä¹‹é—´çš„æœ€å°å€¼ã€‚åœ¨è®­ç»ƒä¹‹å‰ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥ã€‚

+   å¯¹äºåœ¨ 8 ä½ä¸­è®­ç»ƒé€‚é…å™¨ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´ PEFT çš„`prepare_model_for_kbit_training`æ–¹æ³•çš„å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`prepare_in_int8_kwargs`å­—æ®µï¼Œæˆ–åœ¨[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)ä¹‹å¤–åˆ›å»º`PeftModel`å¹¶ä¼ é€’å®ƒã€‚

+   ä¸ºäº†æ›´èŠ‚çœå†…å­˜åœ°ä½¿ç”¨é€‚é…å™¨è¿›è¡Œè®­ç»ƒï¼Œæ‚¨å¯ä»¥åœ¨ 8 ä½ä¸­åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œåªéœ€åœ¨åˆ›å»º[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ—¶æ·»åŠ `load_in_8bit`å‚æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå™¨ä¹‹å¤–åˆ›å»ºä¸€ä¸ª 8 ä½çš„åŸºç¡€æ¨¡å‹å¹¶ä¼ é€’å®ƒã€‚

+   å¦‚æœæ‚¨åœ¨è®­ç»ƒå™¨ä¹‹å¤–åˆ›å»ºæ¨¡å‹ï¼Œè¯·ç¡®ä¿ä¸è¦å‘è®­ç»ƒå™¨ä¼ é€’ç›¸å¯¹äº`from_pretrained()`æ–¹æ³•çš„ä»»ä½•é¢å¤–å…³é”®å­—å‚æ•°ã€‚

## GPTQ è½¬æ¢

åœ¨å®Œæˆè®­ç»ƒåï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸€äº› GPTQ é‡åŒ–çš„é—®é¢˜ã€‚å°†`gradient_accumulation_steps`é™ä½åˆ°`4`å°†è§£å†³å¤§éƒ¨åˆ†åœ¨é‡åŒ–è¿‡ç¨‹ä¸­åˆ° GPTQ æ ¼å¼çš„é—®é¢˜ã€‚

## SFTTrainer

### `class trl.SFTTrainer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)

```py
( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None peft_config: Optional = None dataset_text_field: Optional = None packing: Optional = False formatting_func: Optional = None max_seq_length: Optional = None infinite: Optional = None num_of_sequences: Optional = 1024 chars_per_token: Optional = 3.6 dataset_num_proc: Optional = None dataset_batch_size: int = 1000 neftune_noise_alpha: Optional = None model_init_kwargs: Optional = None dataset_kwargs: Optional = None )
```

å‚æ•°

+   `model`ï¼ˆUnion[`transformers.PreTrainedModel`, `nn.Module`, `str`ï¼‰â€” è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥æ˜¯`PreTrainedModel`ï¼Œ`torch.nn.Module`æˆ–åŒ…å«è¦ä»ç¼“å­˜åŠ è½½æˆ–ä¸‹è½½çš„æ¨¡å‹åç§°çš„å­—ç¬¦ä¸²ã€‚å¦‚æœå°†`PeftConfig`å¯¹è±¡ä¼ é€’ç»™`peft_config`å‚æ•°ï¼Œåˆ™æ¨¡å‹ä¹Ÿå¯ä»¥è½¬æ¢ä¸º`PeftModel`ã€‚

+   `args`ï¼ˆOptional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments)ï¼‰â€” ç”¨äºè°ƒæ•´è®­ç»ƒçš„å‚æ•°ã€‚è¯·å‚è€ƒ`transformers.TrainingArguments`çš„å®˜æ–¹æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

+   `data_collator`ï¼ˆå¯é€‰`transformers.DataCollator`ï¼‰â€” ç”¨äºè®­ç»ƒçš„æ•°æ®æ•´ç†å™¨ã€‚

+   `train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)) â€” ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`trl.trainer.ConstantLengthDataset`æ¥åˆ›å»ºä»–ä»¬çš„æ•°æ®é›†ã€‚

+   `eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]) â€” ç”¨äºè¯„ä¼°çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`trl.trainer.ConstantLengthDataset`æ¥åˆ›å»ºä»–ä»¬çš„æ•°æ®é›†ã€‚

+   `tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)) â€” ç”¨äºè®­ç»ƒçš„åˆ†è¯å™¨ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨ä¸æ¨¡å‹å…³è”çš„åˆ†è¯å™¨ã€‚

+   `model_init` (`Callable[[], transformers.PreTrainedModel]`) â€” ç”¨äºè®­ç»ƒçš„æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚å¦‚æœæŒ‡å®šä¸ºNoneï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤çš„æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚

+   `compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None) â€” ç”¨äºåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­è®¡ç®—æŒ‡æ ‡çš„å‡½æ•°ã€‚å®ƒåº”è¯¥è¿”å›ä¸€ä¸ªå°†æŒ‡æ ‡åç§°æ˜ å°„åˆ°æŒ‡æ ‡å€¼çš„å­—å…¸ã€‚å¦‚æœæœªæŒ‡å®šï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­åªä¼šè®¡ç®—æŸå¤±ã€‚

+   `callbacks` (`List[transformers.TrainerCallback]`) â€” ç”¨äºè®­ç»ƒçš„å›è°ƒå‡½æ•°ã€‚

+   `optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`) â€” ç”¨äºè®­ç»ƒçš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚

+   `preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`) â€” ç”¨äºåœ¨è®¡ç®—æŒ‡æ ‡ä¹‹å‰é¢„å¤„ç†logitsçš„å‡½æ•°ã€‚

+   `peft_config` (`Optional[PeftConfig]`) â€” ç”¨äºåˆå§‹åŒ–PeftModelçš„PeftConfigå¯¹è±¡ã€‚

+   `dataset_text_field` (`Optional[str]`) â€” æ•°æ®é›†çš„æ–‡æœ¬å­—æ®µçš„åç§°ï¼Œå¦‚æœç”¨æˆ·ä¼ é€’äº†è¿™ä¸ªå‚æ•°ï¼Œè®­ç»ƒå™¨å°†è‡ªåŠ¨åŸºäº`dataset_text_field`å‚æ•°åˆ›å»ºä¸€ä¸ª`ConstantLengthDataset`ã€‚

+   `formatting_func` (`Optional[Callable]`) â€” ç”¨äºåˆ›å»º`ConstantLengthDataset`çš„æ ¼å¼åŒ–å‡½æ•°ã€‚

+   `max_seq_length` (`Optional[int]`) â€” ç”¨äº`ConstantLengthDataset`å’Œè‡ªåŠ¨åˆ›å»ºæ•°æ®é›†çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é»˜è®¤ä¸º`512`ã€‚

+   `infinite` (`Optional[bool]`) â€” æ˜¯å¦ä½¿ç”¨æ— é™æ•°æ®é›†ã€‚é»˜è®¤ä¸º`False`ã€‚

+   `num_of_sequences` (`Optional[int]`) â€” ç”¨äº`ConstantLengthDataset`çš„åºåˆ—æ•°ã€‚é»˜è®¤ä¸º`1024`ã€‚

+   `chars_per_token` (`Optional[float]`) â€” ç”¨äº`ConstantLengthDataset`çš„æ¯ä¸ªæ ‡è®°çš„å­—ç¬¦æ•°ã€‚é»˜è®¤ä¸º`3.6`ã€‚æ‚¨å¯ä»¥åœ¨stack-llamaç¤ºä¾‹ä¸­æŸ¥çœ‹å¦‚ä½•è®¡ç®—è¿™ä¸ªå€¼ï¼š[https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53)ã€‚

+   `packing` (`Optional[bool]`) â€” ä»…åœ¨ä¼ é€’äº†`dataset_text_field`çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚è¿™ä¸ªå‚æ•°ç”±`ConstantLengthDataset`ç”¨äºæ‰“åŒ…æ•°æ®é›†çš„åºåˆ—ã€‚

+   `dataset_num_proc` (`Optional[int]`) â€” ç”¨äºæ ‡è®°åŒ–æ•°æ®çš„å·¥ä½œè¿›ç¨‹æ•°ã€‚ä»…åœ¨`packing=False`æ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºNoneã€‚

+   `dataset_batch_size` (`int`) â€” æ¯æ‰¹è¦æ ‡è®°åŒ–çš„ç¤ºä¾‹æ•°ã€‚å¦‚æœbatch_size <= 0æˆ–batch_size == Noneï¼Œåˆ™å°†æ•´ä¸ªæ•°æ®é›†æ ‡è®°ä¸ºå•ä¸ªæ‰¹æ¬¡ã€‚é»˜è®¤ä¸º1000ã€‚

+   `neftune_noise_alpha` (`Optional[float]`) â€” å¦‚æœä¸æ˜¯ `None`ï¼Œåˆ™ä¼šæ¿€æ´» NEFTune å™ªå£°åµŒå…¥ã€‚è¿™å·²è¢«è¯æ˜æå¤§åœ°æé«˜äº†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚æŸ¥çœ‹åŸå§‹è®ºæ–‡ï¼š[https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914) å’ŒåŸå§‹ä»£ç ï¼š[https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune) model_init_kwargs â€” (`Optional[Dict]`, *optional*): ä¼ é€’ç»™å®ä¾‹åŒ–æ¨¡å‹æ—¶çš„å¯é€‰å‚æ•°å­—å…¸ dataset_kwargs â€” (`Optional[Dict]`, *optional*): åˆ›å»ºæ‰“åŒ…æˆ–éæ‰“åŒ…æ•°æ®é›†æ—¶ä¼ é€’çš„å¯é€‰å‚æ•°å­—å…¸

ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨ï¼ˆSFT Trainerï¼‰çš„ç±»å®šä¹‰ã€‚è¿™ä¸ªç±»æ˜¯ `transformers.Trainer` ç±»çš„åŒ…è£…å™¨ï¼Œå¹¶ç»§æ‰¿äº†å®ƒçš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•ã€‚è®­ç»ƒå™¨è´Ÿè´£åœ¨ç”¨æˆ·ä¼ é€’ `PeftConfig` å¯¹è±¡æ—¶æ­£ç¡®åˆå§‹åŒ– PeftModelã€‚

## ConstantLengthDataset

### `class trl.trainer.ConstantLengthDataset`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/utils.py#L355)

```py
( tokenizer dataset dataset_text_field = None formatting_func = None infinite = False seq_length = 1024 num_of_sequences = 1024 chars_per_token = 3.6 eos_token_id = 0 shuffle = True append_concat_token = True add_special_tokens = True )
```

å‚æ•°

+   `tokenizer` (`transformers.PreTrainedTokenizer`) â€” ç”¨äºå¤„ç†æ•°æ®çš„å¤„ç†å™¨ã€‚

+   `dataset` (`dataset.Dataset`) â€” åŒ…å«æ–‡æœ¬æ–‡ä»¶çš„æ•°æ®é›†ã€‚

+   `dataset_text_field` (`str`, **optional**) â€” æ•°æ®é›†ä¸­åŒ…å«æ–‡æœ¬çš„å­—æ®µåç§°ã€‚ä»…åœ¨ `formatting_func` ä¸º `None` æ—¶ä½¿ç”¨ã€‚

+   `formatting_func` (`Callable`, **optional**) â€” åœ¨åˆ†è¯ä¹‹å‰æ ¼å¼åŒ–æ–‡æœ¬çš„å‡½æ•°ã€‚é€šå¸¸å»ºè®®éµå¾ªç‰¹å®šæ¨¡å¼ï¼Œå¦‚ `"### é—®é¢˜ï¼š{question} ### ç­”æ¡ˆï¼š{answer}"`

+   `infinite` (`bool`, *optional*, defaults to `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ•°æ®é›†åˆ°è¾¾æœ«å°¾åé‡ç½®è¿­ä»£å™¨ï¼Œå¦åˆ™åœæ­¢ã€‚

+   `seq_length` (`int`, *optional*, defaults to `1024`) â€” è¦è¿”å›çš„ä»¤ç‰Œåºåˆ—çš„é•¿åº¦ã€‚

+   `num_of_sequences` (`int`, *optional*, defaults to `1024`) â€” åœ¨ç¼“å†²åŒºä¸­ä¿ç•™çš„ä»¤ç‰Œåºåˆ—æ•°ã€‚

+   `chars_per_token` (`int`, *optional*, defaults to `3.6`) â€” ç”¨äºä¼°è®¡æ–‡æœ¬ç¼“å†²åŒºä¸­ä»¤ç‰Œæ•°é‡çš„æ¯ä¸ªä»¤ç‰Œçš„å­—ç¬¦æ•°ã€‚

+   `eos_token_id` (`int`, *optional*, defaults to `0`) â€” å¦‚æœä¼ é€’çš„åˆ†è¯å™¨æ²¡æœ‰ EOS æ ‡è®°ï¼Œåˆ™ä¸ºåºåˆ—ç»“æŸæ ‡è®°çš„ IDã€‚

+   `shuffle` (â€˜boolâ€™, *optional*, defaults to True) â€” åœ¨è¿”å›ç¤ºä¾‹ä¹‹å‰å¯¹ç¤ºä¾‹è¿›è¡Œæ´—ç‰Œ

+   `append_concat_token` (â€˜boolâ€™, *optional*, defaults to True) â€” å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ¯ä¸ªè¢«æ‰“åŒ…çš„æ ·æœ¬æœ«å°¾é™„åŠ  `eos_token_id`ã€‚

+   `add_special_tokens` (â€˜boolâ€™, *optional*, defaults to True) â€” å¦‚æœä¸º Trueï¼Œåˆ™åˆ†è¯å™¨ä¼šä¸ºæ¯ä¸ªè¢«æ‰“åŒ…çš„æ ·æœ¬æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚

å¯è¿­ä»£æ•°æ®é›†ï¼Œä»æ–‡æœ¬æ–‡ä»¶æµä¸­è¿”å›æ’å®šé•¿åº¦çš„ä»¤ç‰Œå—ã€‚æ•°æ®é›†è¿˜ä¼šåœ¨åˆ†è¯ä¹‹å‰ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç‰¹å®šæ ¼å¼å¯¹æ–‡æœ¬è¿›è¡Œæ ¼å¼åŒ–ã€‚
