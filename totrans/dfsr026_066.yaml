- en: Textual Inversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/diffusers/training/text_inversion](https://huggingface.co/docs/diffusers/training/text_inversion)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/149.ef2b4fdd.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/HfOption.fc88c804.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/stores.aa51e67d.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique
    for personalizing image generation models with just a few example images of what
    you want it to learn. This technique works by learning and updating the text embeddings
    (the new embeddings are tied to a special word you must use in the prompt) to
    match the example images you provide.'
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre training on a GPU with limited vRAM, you should try enabling the `gradient_checkpointing`
    and `mixed_precision` parameters in the training command. You can also reduce
    your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers).
    JAX/Flax training is also supported for efficient training on TPUs and GPUs, but
    it doesn‚Äôt support gradient checkpointing or xFormers. With the same configuration
    and setup as PyTorch, the Flax training script should be at least ~70% faster!
  prefs: []
  type: TYPE_NORMAL
- en: This guide will explore the [textual_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py)
    script to help you become more familiar with it, and how you can adapt it for
    your own use-case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the script, make sure you install the library from source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the example folder with the training script and install the required
    dependencies for the script you‚Äôre using:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorchFlax
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ü§ó Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. It‚Äôll automatically configure your training setup based on your
    hardware and environment. Take a look at the ü§ó Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an ü§ó Accelerate environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To setup a default ü§ó Accelerate environment without choosing any configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if your environment doesn‚Äôt support an interactive shell, like a notebook,
    you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesn‚Äôt cover every aspect of the script
    in detail. If you‚Äôre interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py)
    and let us know if you have any questions or concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Script parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training script has many parameters to help you tailor the training run
    to your needs. All of the parameters and their descriptions are listed in the
    [`parse_args()`](https://github.com/huggingface/diffusers/blob/839c2a5ece0af4e75530cb520d77bc7ed8acf474/examples/textual_inversion/textual_inversion.py#L176)
    function. Where applicable, Diffusers provides default values for each parameter
    such as the training batch size and learning rate, but feel free to change these
    values in the training command if you‚Äôd like.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to increase the number of gradient accumulation steps above the
    default value of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Some other basic and important parameters to specify include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--pretrained_model_name_or_path`: the name of the model on the Hub or a local
    path to the pretrained model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--train_data_dir`: path to a folder containing the training dataset (example
    images)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--output_dir`: where to save the trained model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--push_to_hub`: whether to push the trained model to the Hub'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--checkpointing_steps`: frequency of saving a checkpoint as the model trains;
    this is useful if for some reason training is interrupted, you can continue training
    from that checkpoint by adding `--resume_from_checkpoint` to your training command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_vectors`: the number of vectors to learn the embeddings with; increasing
    this parameter helps the model learn better but it comes with increased training
    costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--placeholder_token`: the special word to tie the learned embeddings to (you
    must use the word in your prompt for inference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--initializer_token`: a single-word that roughly describes the object or style
    you‚Äôre trying to train on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--learnable_property`: whether you‚Äôre training the model to learn a new ‚Äústyle‚Äù
    (for example, Van Gogh‚Äôs painting style) or ‚Äúobject‚Äù (for example, your dog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike some of the other training scripts, textual_inversion.py has a custom
    dataset class, [`TextualInversionDataset`](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L487)
    for creating a dataset. You can customize the image size, placeholder token, interpolation
    method, whether to crop the image, and more. If you need to change how the dataset
    is created, you can modify `TextualInversionDataset`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you‚Äôll find the dataset preprocessing code and training loop in the [`main()`](https://github.com/huggingface/diffusers/blob/839c2a5ece0af4e75530cb520d77bc7ed8acf474/examples/textual_inversion/textual_inversion.py#L573)
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script starts by loading the [tokenizer](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L616),
    [scheduler and model](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L622):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The special [placeholder token](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L632)
    is added next to the tokenizer, and the embedding is readjusted to account for
    the new token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the script [creates a dataset](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L716)
    from the `TextualInversionDataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the [training loop](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L784)
    handles everything else from predicting the noisy residual to updating the embedding
    weights of the special placeholder token.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about how the training loop works, check out the [Understanding
    pipelines, models and schedulers](../using-diffusers/write_own_pipeline) tutorial
    which breaks down the basic pattern of the denoising process.
  prefs: []
  type: TYPE_NORMAL
- en: Launch the script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you‚Äôve made all your changes or you‚Äôre okay with the default configuration,
    you‚Äôre ready to launch the training script! üöÄ
  prefs: []
  type: TYPE_NORMAL
- en: For this guide, you‚Äôll download some images of a [cat toy](https://huggingface.co/datasets/diffusers/cat_toy_example)
    and store them in a directory. But remember, you can create and use your own dataset
    if you want (see the [Create a dataset for training](create_dataset) guide).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the environment variable `MODEL_NAME` to a model id on the Hub or a path
    to a local model, and `DATA_DIR` to the path where you just downloaded the cat
    images to. The script creates and saves the following files to your repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '`learned_embeds.bin`: the learned embedding vectors corresponding to your example
    images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_identifier.txt`: the special placeholder token'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type_of_concept.txt`: the type of concept you‚Äôre training on (either ‚Äúobject‚Äù
    or ‚Äústyle‚Äù)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A full training run takes ~1 hour on a single V100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing before you launch the script. If you‚Äôre interested in following
    along with the training process, you can periodically save generated images as
    training progresses. Add the following parameters to the training command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: PyTorchFlax
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After training is complete, you can use your newly trained model for inference
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorchFlax
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Congratulations on training your own Textual Inversion model! üéâ To learn more
    about how to use your new model, the following guides may be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to [load Textual Inversion embeddings](../using-diffusers/loading_adapters)
    and also use them as negative embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to use [Textual Inversion](textual_inversion_inference) for inference
    with Stable Diffusion 1/2 and Stable Diffusion XL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
