- en: Instantiating a big model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实例化一个大模型
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/big_models](https://huggingface.co/docs/transformers/v4.37.2/en/big_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/transformers/v4.37.2/en/big_models](https://huggingface.co/docs/transformers/v4.37.2/en/big_models)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When you want to use a very big pretrained model, one challenge is to minimize
    the use of the RAM. The usual workflow from PyTorch is:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要使用非常大的预训练模型时，一个挑战是尽量减少RAM的使用。来自PyTorch的通常工作流程是：
- en: Create your model with random weights.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用随机权重创建您的模型。
- en: Load your pretrained weights.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载您的预训练权重。
- en: Put those pretrained weights in your random model.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些预训练权重放入您的随机模型中。
- en: Step 1 and 2 both require a full version of the model in memory, which is not
    a problem in most cases, but if your model starts weighing several GigaBytes,
    those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed`
    to launch a distributed training, each process will load the pretrained model
    and store these two copies in RAM.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1和2都需要内存中的完整模型版本，在大多数情况下这不是问题，但是如果您的模型开始占用数千兆字节，这两个副本可能会使您的RAM不足。更糟糕的是，如果您使用`torch.distributed`启动分布式训练，每个进程都会加载预训练模型并将这两个副本存储在RAM中。
- en: Note that the randomly created model is initialized with “empty” tensors, which
    take the space in memory without filling it (thus the random values are whatever
    was in this chunk of memory at a given time). The random initialization following
    the appropriate distribution for the kind of model/parameters instantiated (like
    a normal distribution for instance) is only performed after step 3 on the non-initialized
    weights, to be as fast as possible!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随机创建的模型是用“空”张量初始化的，这些张量占用内存空间而不填充它（因此随机值是在给定时间内内存块中的内容）。适合模型/参数实例化的适当分布（例如正态分布）的随机初始化仅在第3步对未初始化的权重执行，以尽可能快地完成！
- en: In this guide, we explore the solutions Transformers offer to deal with this
    issue. Note that this is an area of active development, so the APIs explained
    here may change slightly in the future.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们探讨了Transformers提供的解决此问题的解决方案。请注意，这是一个正在积极发展的领域，因此这里解释的API可能在未来略有变化。
- en: Sharded checkpoints
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片检查点
- en: Since version 4.18.0, model checkpoints that end up taking more than 10GB of
    space are automatically sharded in smaller pieces. In terms of having one single
    checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with
    several partial checkpoints (each of which being of size < 10GB) and an index
    that maps parameter names to the files they are stored in.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本4.18.0以来，占用超过10GB空间的模型检查点会自动分片成较小的部分。在执行`model.save_pretrained(save_dir)`时，您将得到几个部分检查点（每个大小均小于10GB）和一个将参数名称映射到存储文件的索引。
- en: 'You can control the maximum size before sharding with the `max_shard_size`
    parameter, so for the sake of an example, we’ll use a normal-size models with
    a small shard size: let’s take a traditional BERT model.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`max_shard_size`参数控制分片之前的最大大小，因此为了举例，我们将使用一个具有小分片大小的正常大小模型：让我们使用传统的BERT模型。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you save it using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    you will get a new folder with two files: the config of the model and its weights:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存它，您将获得一个新文件夹，其中包含模型的配置和权重：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let’s use a maximum shard size of 200MB:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用最大分片大小为200MB：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On top of the configuration of the model, we see three different weights files,
    and an `index.json` file which is our index. A checkpoint like this can be fully
    reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型的配置之外，我们看到三个不同的权重文件，以及一个`index.json`文件，这是我们的索引。可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法完全重新加载这样的检查点：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The main advantage of doing this for big models is that during step 2 of the
    workflow shown above, each shard of the checkpoint is loaded after the previous
    one, capping the memory usage in RAM to the model size plus the size of the biggest
    shard.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做大模型的主要优势在于，在上述工作流程的第2步中，检查点的每个分片在前一个分片之后加载，将RAM中的内存使用限制在模型大小加上最大分片大小的大小。
- en: 'Behind the scenes, the index file is used to determine which keys are in the
    checkpoint, and where the corresponding weights are stored. We can load that index
    like any json and get a dictionary:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，索引文件用于确定检查点中的键以及相应权重存储的位置。我们可以像加载任何json一样加载该索引并获得一个字典：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The metadata just consists of the total size of the model for now. We plan
    to add other information in the future:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据目前只包含模型的总大小。我们计划在未来添加其他信息：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The weights map is the main part of this index, which maps each parameter name
    (as usually found in a PyTorch model `state_dict`) to the file it’s stored in:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 权重映射是此索引的主要部分，它将每个参数名称（通常在PyTorch模型`state_dict`中找到）映射到其存储的文件：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you want to directly load such a sharded checkpoint inside a model without
    using [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    (like you would do `model.load_state_dict()` for a full checkpoint) you should
    use [load_sharded_checkpoint()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在不使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)（就像您为完整检查点执行`model.load_state_dict()`一样）的情况下直接加载这样的分片检查点，您应该使用[load_sharded_checkpoint()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint)：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Low memory loading
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低内存加载
- en: Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned
    above, but in order to use that model in a low memory setting, we recommend leveraging
    our tools based on the Accelerate library.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分片检查点减少了上述工作流程第2步中的内存使用，但为了在低内存环境中使用该模型，我们建议利用基于Accelerate库的工具。
- en: 'Please read the following guide for more information: [Large model loading
    using Accelerate](./main_classes/model#large-model-loading)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请阅读以下指南以获取更多信息：[使用 Accelerate 进行大型模型加载](./main_classes/model#large-model-loading)
