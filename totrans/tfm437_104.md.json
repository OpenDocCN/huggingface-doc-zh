["```py\npip install transformers datasets accelerate nvidia-ml-py3\n```", "```py\n>>> import numpy as np\n>>> from datasets import Dataset\n\n>>> seq_len, dataset_size = 512, 512\n>>> dummy_data = {\n...     \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n...     \"labels\": np.random.randint(0, 1, (dataset_size)),\n... }\n>>> ds = Dataset.from_dict(dummy_data)\n>>> ds.set_format(\"pt\")\n```", "```py\n>>> from pynvml import *\n\n>>> def print_gpu_utilization():\n...     nvmlInit()\n...     handle = nvmlDeviceGetHandleByIndex(0)\n...     info = nvmlDeviceGetMemoryInfo(handle)\n...     print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n>>> def print_summary(result):\n...     print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n...     print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n...     print_gpu_utilization()\n```", "```py\n>>> print_gpu_utilization()\nGPU memory occupied: 0 MB.\n```", "```py\n>>> import torch\n\n>>> torch.ones((1, 1)).to(\"cuda\")\n>>> print_gpu_utilization()\nGPU memory occupied: 1343 MB.\n```", "```py\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(\"cuda\")\n>>> print_gpu_utilization()\nGPU memory occupied: 2631 MB.\n```", "```py\nnvidia-smi\n```", "```py\nTue Jan 11 08:58:05 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |\n+-----------------------------------------------------------------------------+\n```", "```py\ndefault_args = {\n    \"output_dir\": \"tmp\",\n    \"evaluation_strategy\": \"steps\",\n    \"num_train_epochs\": 1,\n    \"log_level\": \"error\",\n    \"report_to\": \"none\",\n}\n```", "```py\n>>> from transformers import TrainingArguments, Trainer, logging\n\n>>> logging.set_verbosity_error()\n\n>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n>>> result = trainer.train()\n>>> print_summary(result)\n```", "```py\nTime: 57.82\nSamples/second: 8.86\nGPU memory occupied: 14949 MB.\n```"]