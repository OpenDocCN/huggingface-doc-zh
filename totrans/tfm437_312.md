# SeamlessM4T

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t)

## 概述

SeamlessM4T 模型是由 Meta AI 的 Seamless Communication 团队在[SeamlessM4T — 大规模多语言和多模态机器翻译](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)中提出的。

这是模型的**版本 1**发布。有关更新的**版本 2**发布，请参阅[Seamless M4T v2 文档](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t_v2)。

SeamlessM4T 是一系列旨在提供高质量翻译的模型，使来自不同语言社区的人们能够通过语音和文本轻松交流。

SeamlessM4T 可以在不依赖于单独模型的情况下执行多个任务：

+   语音到语音翻译（S2ST）

+   语音到文本翻译（S2TT）

+   文本到语音翻译（T2ST）

+   文本到文本翻译（T2TT）

+   自动语音识别（ASR）

SeamlessM4TModel 可以执行所有上述任务，但每个任务也有自己的专用子模型。

论文摘要如下：

创建巴别鱼需要什么条件？这是一个可以帮助个人在任意两种语言之间翻译语音的工具。尽管最近基于文本的模型取得了突破性进展，将机器翻译覆盖范围推广到 200 多种语言，但统一的语音到语音翻译模型尚未取得类似的进展。更具体地说，传统的语音到语音翻译系统依赖于级联系统逐步执行翻译，使高性能的统一系统难以实现。为了解决这些差距，我们引入了 SeamlessM4T，这是一个支持语音到语音翻译、语音到文本翻译、文本到语音翻译、文本到文本翻译以及最多 100 种语言的自动语音识别的单一模型。为了构建这个模型，我们使用了 100 万小时的开放语音音频数据，学习了自监督语音表示与 w2v-BERT 2.0。随后，我们创建了一个自动对齐的语音翻译的多模态语料库。经过筛选并与人工标记和伪标记数据结合，我们开发了第一个能够在语音和文本之间进行英语翻译的多语言系统。在 FLEURS 上，SeamlessM4T 为多种目标语言的翻译设定了一个新标准，直接语音到文本翻译的 BLEU 值比之前的 SOTA 提高了 20%。与强级联模型相比，SeamlessM4T 在语音到文本翻译中将英语翻译质量提高了 1.3 个 BLEU 点，在语音到语音翻译中将 ASR-BLEU 值提高了 2.6 个点。在鲁棒性测试中，我们的系统在语音到文本任务中对抗背景噪音和说话者变化的表现优于当前的 SOTA 模型。关键是，我们评估了 SeamlessM4T 的性别偏见和添加了毒性以评估翻译安全性。最后，本工作中的所有贡献都是开源的，可在[`github.com/facebookresearch/seamless_communication`](https://github.com/facebookresearch/seamless_communication)上获取。

## 用法

首先，加载处理器和模型的检查点：

```py
>>> from transformers import AutoProcessor, SeamlessM4TModel

>>> processor = AutoProcessor.from_pretrained("facebook/hf-seamless-m4t-medium")
>>> model = SeamlessM4TModel.from_pretrained("facebook/hf-seamless-m4t-medium")
```

您可以无缝地在文本或音频上使用此模型，生成翻译后的文本或音频。

以下是如何使用处理器处理文本和音频：

```py
>>> # let's load an audio sample from an Arabic speech corpus
>>> from datasets import load_dataset
>>> dataset = load_dataset("arabic_speech_corpus", split="test", streaming=True)
>>> audio_sample = next(iter(dataset))["audio"]

>>> # now, process it
>>> audio_inputs = processor(audios=audio_sample["array"], return_tensors="pt")

>>> # now, process some English test as well
>>> text_inputs = processor(text = "Hello, my dog is cute", src_lang="eng", return_tensors="pt")
```

### 语音

SeamlessM4TModel 可以*无缝地*生成文本或语音，几乎不需要任何更改。让我们以俄语语音翻译为目标：

```py
>>> audio_array_from_text = model.generate(**text_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()
>>> audio_array_from_audio = model.generate(**audio_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()
```

基本上使用相同的代码，我已经将英文文本和阿拉伯语语音翻译成俄语语音样本。

### 文本

同样，您可以使用相同模型从音频文件或文本生成翻译文本。您只需将 `generate_speech=False` 传递给 SeamlessM4TModel.generate()。这次，让我们翻译成法语。

```py
>>> # from audio
>>> output_tokens = model.generate(**audio_inputs, tgt_lang="fra", generate_speech=False)
>>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)

>>> # from text
>>> output_tokens = model.generate(**text_inputs, tgt_lang="fra", generate_speech=False)
>>> translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)
```

### 提示

#### 1\. 使用专用模型

SeamlessM4TModel 是 transformers 的顶级模型，用于生成语音和文本，但您也可以使用专用模型执行任务而无需额外组件，从而减少内存占用。例如，您可以使用专用于 S2ST 任务的模型替换音频到音频生成片段，其余代码完全相同：

```py
>>> from transformers import SeamlessM4TForSpeechToSpeech
>>> model = SeamlessM4TForSpeechToSpeech.from_pretrained("facebook/hf-seamless-m4t-medium")
```

或者，您可以使用专用于 T2TT 任务的模型替换文本到文本生成片段，只需删除 `generate_speech=False`。

```py
>>> from transformers import SeamlessM4TForTextToText
>>> model = SeamlessM4TForTextToText.from_pretrained("facebook/hf-seamless-m4t-medium")
```

随时尝试 SeamlessM4TForSpeechToText 和 SeamlessM4TForTextToSpeech。

#### 2\. 更改说话者身份

您可以使用 `spkr_id` 参数更改用于语音合成的说话者。对于某些语言，某些 `spkr_id` 的效果比其他的好！

#### 3\. 更改生成策略

您可以为语音和文本生成使用不同的 生成策略，例如 `.generate(input_ids=input_ids, text_num_beams=4, speech_do_sample=True)`，这将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式采样。

#### 4\. 同时生成语音和文本

使用 SeamlessM4TModel 的 `return_intermediate_token_ids=True` 来返回语音和文本！

## 模型架构

SeamlessM4T 具有一个多功能架构，可以平滑处理文本和语音的顺序生成。此设置包括两个序列到序列 (seq2seq) 模型。第一个模型将输入模态转换为翻译文本，而第二个模型从翻译文本生成称为“单元标记”的语音标记。

每种模态都有自己专用的编码器，具有独特的架构。此外，对于语音输出，一个受 [HiFi-GAN](https://arxiv.org/abs/2010.05646) 架构启发的声码器被放置在第二个 seq2seq 模型的顶部。

以下是生成过程的工作原理：

+   输入文本或语音通过其特定编码器进行处理。

+   解码器在所需语言中创建文本标记。

+   如果需要语音生成，第二个遵循标准编码器-解码器结构的 seq2seq 模型会生成单元标记。

+   然后，这些单元标记通过最终的声码器传递，产生实际的语音。

此模型由 [ylacombe](https://huggingface.co/ylacombe) 贡献。原始代码可以在 [这里](https://github.com/facebookresearch/seamless_communication) 找到。

## SeamlessM4TModel

`transformers.SeamlessM4TModel` 类

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3927)

```py
( config current_modality = 'text' )
```

参数

+   `config` (~SeamlessM4TConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained() 方法以加载模型权重。

+   `current_modality` (`str`, *可选*，默认为 `"text"`) — 默认模态。用于初始化模型。

原始的 SeamlessM4T 模型变压器，可用于所有可用任务（S2ST、S2TT、T2TT、T2ST）。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `generate`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L4141)

```py
( input_ids: Optional = None input_features: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None spkr_id: Optional = 0 generate_speech: Optional = True **kwargs ) → export const metadata = 'undefined';Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*) — 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `return_intermediate_token_ids` (`bool`, *optional*) — 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，则设置为`True`。请注意，如果`generate_speech=True`，则此参数将被忽略。

+   `tgt_lang` (`str`, *optional*) — 用作翻译目标语言的语言。

+   `spkr_id` (`int`, *optional*, defaults to 0) — 用于语音合成的说话者 id。必须小于`config.vocoder_num_spkrs`。

+   `generate_speech` (`bool`, *optional*, defaults to `True`) — 如果为`False`，将仅返回文本标记，不会生成语音。

+   `kwargs` (*optional*) — 将传递给 GenerationMixin.generate()的剩余关键字参数字典。关键字参数有两种类型：

    +   如果没有前缀，它们将作为`generate`方法的`**kwargs`输入到每个子模型中，除了`decoder_input_ids`只会通过文本组件传递。

    +   带有*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。

    这意味着您可以例如为一个生成指定一种生成策略，但对另一个不指定。

返回

`Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`

+   如果`generate_speech`且`return_intermediate_token_ids`，则返回`SeamlessM4TGenerationOutput`。

+   如果`generate_speech`且不是`return_intermediate_token_ids`，则返回形状为`(batch_size, sequence_length)`的波形和`waveform_lengths`的元组，其中给出每个样本的长度。

+   如果`generate_speech=False`，它将返回`ModelOutput`。

生成翻译的标记 id 和/或翻译的音频波形。

此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的通用参数，或将传递给其中一个模型的前缀参数。

例如，调用`.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4TForTextToSpeech

### `class transformers.SeamlessM4TForTextToSpeech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3214)

```py
( config: SeamlessM4TConfig )
```

参数

+   `config` (~SeamlessM4TConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法加载模型权重。

用于 T2ST 的文本到语音 SeamlessM4T 模型变换器。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3366)

```py
( input_ids: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None spkr_id: Optional = 0 **kwargs ) → export const metadata = 'undefined';Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。查看 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()获取详细信息。

    什么是输入 ID？

+   `return_intermediate_token_ids` (`bool`，*可选*) — 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，请设置为`True`。

+   `tgt_lang` (`str`，*可选*) — 用作翻译目标语言的语言。

+   `spkr_id` (`int`，*可选*，默认为 0) — 用于语音合成的说话者的 id。必须小于`config.vocoder_num_spkrs`。

+   `kwargs` (*可选*) — 将传递给 GenerationMixin.generate()的剩余关键字参数字典。关键字参数有两种类型：

    +   没有前缀，它们将作为`**kwargs`输入到每个子模型的`generate`方法中，除了`decoder_input_ids`，它只会通过文本组件传递。

    +   带有*text_*或*speech_*前缀，它们将成为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。

    这意味着您可以为一个生成策略指定一个生成策略，但不能为另一个生成策略指定生成策略。

返回

`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`

+   如果`return_intermediate_token_ids`，返回`SeamlessM4TGenerationOutput`。

+   如果不是`return_intermediate_token_ids`，返回一个由形状为`(batch_size, sequence_length)`的波形和`waveform_lengths`组成的元组，其中给出每个样本的长度。

生成翻译后的音频波形。

此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的一般参数，或将传递给其中一个模型的带前缀的参数。

例如，调用`.generate(input_ids, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4TForSpeechToSpeech

### `class transformers.SeamlessM4TForSpeechToSpeech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3566)

```py
( config )
```

参数

+   `config` (~SeamlessM4TConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

可用于 S2ST 的语音到语音 SeamlessM4T 模型变换器。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3721)

```py
( input_features: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None spkr_id: Optional = 0 **kwargs ) → export const metadata = 'undefined';Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]
```

参数

+   `input_features` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_banks)`) — 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `return_intermediate_token_ids` (`bool`, *可选*) — 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，请设置为`True`。

+   `tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言。

+   `spkr_id` (`int`, *可选*，默认为 0) — 用于语音合成的说话者的 id。必须小于`config.vocoder_num_spkrs`。

+   `kwargs` (*可选*) — 将传递给 GenerationMixin.generate()的剩余关键字参数字典。关键字参数有两种类型：

    +   如果没有前缀，它们将作为`**kwargs`输入到每个子模型的`generate`方法中，除了`decoder_input_ids`只会通过文本组件传递。

    +   使用*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。

    这意味着您可以为一个生成指定生成策略，但对另一个生成不指定。 

返回

`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`

+   如果`return_intermediate_token_ids`，则返回`SeamlessM4TGenerationOutput`。

+   如果不是`return_intermediate_token_ids`，则返回一个由形状为`(batch_size, sequence_length)`的波形和给出每个样本长度的`waveform_lengths`组成的元组。

生成翻译音频波形。

此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的一般参数，或将传递给其中一个模型的前缀参数。

例如，调用`.generate(input_features, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4TForTextToText

### `class transformers.SeamlessM4TForTextToText`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2637)

```py
( config: SeamlessM4TConfig )
```

参数

+   `config`（~SeamlessM4TConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

文本到文本 SeamlessM4T 模型变压器可用于 T2TT。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2689)

```py
( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于未被`masked`的标记，为 1。

    +   对于被`masked`的标记，为 0。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 词汇表中解码器输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是解码器输入 ID？

    Bart 使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    用于翻译和总结训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将根据论文将`input_ids`向右移动以进行去噪预训练来创建此张量。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

    如果要更改填充行为，您应该阅读`modeling_bart._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包含(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的元组，每个元组包含 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的输入），而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更好地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（请参见`past_key_values`）。如果您希望更好地控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`将取`inputs_embeds`的值。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（请参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通的元组。

SeamlessM4TForTextToText 的前向方法重写了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

`generate`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2781)

```py
( input_ids = None tgt_lang = None generation_config = None logits_processor = None stopping_criteria = None prefix_allowed_tokens_fn = None synced_gpus = False **kwargs ) → export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

参数

+   `input_ids`（根据模态性质的不同形状的`torch.Tensor`，*可选*） — 词汇表中输入序列令牌的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言。

+   `generation_config` (`~generation.GenerationConfig`, *可选*) — 用作生成调用的基本参数化的生成配置。与`generation_config`属性匹配的`**kwargs`传递给 generate 将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中获取，如果存在；2）从模型配置中获取。请注意，未指定的参数将继承 GenerationConfig 的默认值，应检查其文档以参数化生成。

+   `logits_processor` (`LogitsProcessorList`, *可选*) — 自定义 logits 处理器，用于补充从参数和生成配置构建的默认 logits 处理器。如果传递了已经使用参数或生成配置创建的 logit 处理器，则会抛出错误。此功能适用于高级用户。

+   `stopping_criteria` (`StoppingCriteriaList`, *可选*) — 自定义停止标准，用于补充从参数和生成配置构建的默认停止标准。如果传递了已经使用参数或生成配置创建的停止标准，则会抛出错误。此功能适用于高级用户。

+   `prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *可选*) — 如果提供，此函数将在每个步骤中将 beam 搜索限制为仅允许的令牌。如果未提供，则不应用约束。此函数接受 2 个参数：批次 ID`batch_id`和`input_ids`。它必须返回一个列表，其中包含下一代步骤的允许令牌，条件是批次 ID`batch_id`和先前生成的令牌`inputs_ids`。此参数对于基于前缀的受约束生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。

+   `synced_gpus` (`bool`, *可选*，默认为`False`) — 是否继续运行 while 循环直到 max_length（需要 ZeRO 阶段 3）

+   `kwargs` (`Dict[str, Any]`, *可选*) — `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定 kwargs。

返回

ModelOutput 或`torch.LongTensor`

一个 ModelOutput（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。可能的 ModelOutput 类型为：

+   GenerateEncoderDecoderOutput,

+   GenerateBeamEncoderDecoderOutput

生成标记 ID 序列。

大多数生成控制参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过传递相应的参数给 generate()来覆盖任何`generation_config`，例如`.generate(inputs, num_beams=4, do_sample=True)`。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4TForSpeechToText

### `class transformers.SeamlessM4TForSpeechToText`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2924)

```py
( config: SeamlessM4TConfig )
```

参数

+   `config`（~SeamlessM4TConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

语音到文本 SeamlessM4T 模型变压器，可用于 S2TT。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `前进`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2971)

```py
( input_features: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )
```

参数

+   `input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）— 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中。

    +   1 代表未被“掩盖”的标记，

    +   0 代表被“掩盖”的标记。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中解码器输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    解码器输入 ID 是什么？

    Bart 使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    对于翻译和摘要训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将通过将`input_ids`向右移动来创建此张量，以进行去噪预训练，遵循论文。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

    如果您想要更改填充行为，您应该阅读`modeling_bart._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后`decoder_input_ids`（这些没有将其过去键值状态提供给此模型的）而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, target_sequence_length, hidden_size)`，*optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则只需输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `labels` (`torch.LongTensor`的形状为`(batch_size, sequence_length)`，*optional*) — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`内的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

SeamlessM4TForSpeechToText 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例而不是此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3070)

```py
( input_features = None tgt_lang = None generation_config = None logits_processor = None stopping_criteria = None prefix_allowed_tokens_fn = None synced_gpus = False **kwargs ) → export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

参数

+   `input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）— 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `tgt_lang`（`str`，*可选*）— 用作翻译目标语言的语言。

+   `generation_config`（`~generation.GenerationConfig`，*可选*）— 用作生成调用的基本参数化的生成配置。传递给生成匹配`generation_config`属性的`**kwargs`将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）来自`generation_config.json`模型文件，如果存在；2）来自模型配置。请注意，未指定的参数将继承 GenerationConfig 的默认值，应检查其文档以参数化生成。

+   `logits_processor`（`LogitsProcessorList`，*可选*）— 自定义 logits 处理器，补充从参数和生成配置构建的默认 logits 处理器。如果传递的 logit 处理器已经使用参数或生成配置创建，则会引发错误。此功能适用于高级用户。

+   `stopping_criteria`（`StoppingCriteriaList`，*可选*）— 自定义停止标准，补充从参数和生成配置构建的默认停止标准。如果传递的停止标准已经使用参数或生成配置创建，则会引发错误。此功能适用于高级用户。

+   `prefix_allowed_tokens_fn`（`Callable[[int, torch.Tensor], List[int]]`，*可选*）— 如果提供，此函数将限制每一步的波束搜索仅允许特定的标记。如果未提供，则不应用任何约束。此函数接受 2 个参数：批次 ID `batch_id` 和 `input_ids`。它必须返回一个列表，其中包含下一代步的允许标记，条件是批次 ID `batch_id` 和先前生成的标记 `inputs_ids`。此参数对于基于前缀的受限生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。

+   `synced_gpus`（`bool`，*可选*，默认为`False`）— 是否继续运行 while 循环直到 max_length（对于 ZeRO 阶段 3 是必需的）

+   `kwargs`（`Dict[str, Any]`，*可选*）— `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定 kwargs。

返回

ModelOutput 或`torch.LongTensor`

一个 ModelOutput（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。可能的 ModelOutput 类型是：

+   GenerateEncoderDecoderOutput,

+   GenerateBeamEncoderDecoderOutput

生成标记 id 序列。

大多数控制生成的参数设置在`generation_config`中，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应参数传递给 generate()来覆盖任何`generation_config`，例如`.generate(inputs, num_beams=4, do_sample=True)`。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4TConfig

### `class transformers.SeamlessM4TConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/configuration_seamless_m4t.py#L29)

```py
( vocab_size = 256102 t2u_vocab_size = 10082 hidden_size = 1024 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True max_position_embeddings = 1024 is_encoder_decoder = True encoder_layerdrop = 0.05 decoder_layerdrop = 0.05 activation_function = 'relu' dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 scale_embedding = True encoder_layers = 24 encoder_ffn_dim = 8192 encoder_attention_heads = 16 decoder_layers = 24 decoder_ffn_dim = 8192 decoder_attention_heads = 16 decoder_start_token_id = 3 max_new_tokens = 256 pad_token_id = 0 bos_token_id = 2 eos_token_id = 3 speech_encoder_layers = 24 speech_encoder_attention_heads = 16 speech_encoder_intermediate_size = 4096 speech_encoder_hidden_act = 'swish' speech_encoder_dropout = 0.0 add_adapter = True speech_encoder_layerdrop = 0.1 feature_projection_input_dim = 160 num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 adaptor_kernel_size = 8 adaptor_stride = 8 adaptor_dropout = 0.1 num_adapter_layers = 1 position_embeddings_type = 'relative' rotary_embedding_base = 10000 max_source_positions = 4096 conv_depthwise_kernel_size = 31 t2u_bos_token_id = 0 t2u_pad_token_id = 1 t2u_eos_token_id = 2 t2u_decoder_start_token_id = 2 t2u_max_new_tokens = 1024 t2u_encoder_layers = 6 t2u_encoder_ffn_dim = 8192 t2u_encoder_attention_heads = 16 t2u_decoder_layers = 6 t2u_decoder_ffn_dim = 8192 t2u_decoder_attention_heads = 16 t2u_max_position_embeddings = 2048 sampling_rate = 16000 upsample_initial_channel = 512 upsample_rates = [5, 4, 4, 2, 2] upsample_kernel_sizes = [11, 8, 8, 4, 4] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] leaky_relu_slope = 0.1 unit_hifi_gan_vocab_size = 10000 unit_embed_dim = 1280 lang_embed_dim = 256 spkr_embed_dim = 256 vocoder_num_langs = 36 vocoder_num_spkrs = 200 variance_predictor_kernel_size = 3 var_pred_dropout = 0.5 vocoder_offset = 4 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 256102) — SeamlessM4T 模型的词汇表大小。定义了在调用~SeamlessM4TModel、~SeamlessM4TForTextToSpeech 或~SeamlessM4TForTextToText 时可以表示的不同标记数量。

+   `t2u_vocab_size` (`int`, *optional*, 默认为 10082) — SeamlessM4T 模型的单元词汇表大小。定义了在调用~SeamlessM4TModel、~SeamlessM4TForSpeechToSpeech 或~SeamlessM4TForTextToSpeech 时可以表示的不同单元标记数量。

在子模型之间共享的参数

+   `hidden_size` (`int`, *optional*, 默认为 1024) — 架构中“中间”层的维度。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-05) — 层归一化层使用的 epsilon。

+   `use_cache` (`bool`, *optional*, 默认为 `True`) — 模型是否应返回最后一个键/值注意力（并非所有模型都使用）。

+   `max_position_embeddings` (`int`, *optional*, 默认为 1024) — 此模型文本编码器和解码器可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512 或 1024 或 2048）。

+   `is_encoder_decoder` (`bool`, *optional*, 默认为 `True`) — 模型是否用作编码器/解码器。

+   `encoder_layerdrop` (`float`, *optional*, 默认为 0.05) — 编码器的 LayerDrop 概率。更多细节请参阅[LayerDrop 论文](https://arxiv.org/abs/1909.11556)。

+   `decoder_layerdrop` (`float`, *optional*, 默认为 0.05) — 解码器的 LayerDrop 概率。更多细节请参阅[LayerDrop 论文](https://arxiv.org/abs/1909.11556)。

+   `activation_function` (`str` 或 `function`, *optional*, 默认为 `"relu"`) — 解码器和前馈层中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"`, `"swish"` 和 `"gelu_new"`。

+   `dropout` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器、解码器和池化器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, 默认为 0.1) — 所有注意力层的 dropout 概率。

+   `activation_dropout` (`float`, *optional*, 默认为 0.0) — 模型中所有激活层的 dropout 概率。

+   `scale_embedding` (`bool`, *optional*, 默认为 `True`) — 通过将其除以 sqrt(d_model)来缩放嵌入。

文本编码器和文本解码器特定参数

+   `encoder_layers` (`int`, *optional*, defaults to 24) — Transformer 文本编码器中的隐藏层数量。

+   `encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer 文本编码器中“中间”（即前馈）层的维度。

+   `encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 文本编码器中每个注意力层的注意力头数。

+   `decoder_layers` (`int`, *optional*, defaults to 24) — Transformer 文本解码器中的隐藏层数量。

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer 文本解码器中“中间”（即前馈）层的维度。

+   `decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 文本解码器中每个注意力层的注意力头数。

+   `decoder_start_token_id` (`int`, *optional*, defaults to 3) — 如果编码器-解码器模型以与*bos*不同的标记开始解码，则该标记的 id。仅在文本解码器中应用。

+   `max_new_tokens` (`int`, *optional*, defaults to 256) — 生成的文本标记的最大数量，忽略提示中的标记数量。

+   `pad_token_id` (`int`, *optional*, defaults to 0) — *填充*文本标记的 id。仅适用于文本解码器模型。

+   `bos_token_id` (`int`, *optional*, defaults to 2) — *流的开头*文本标记的 id。仅适用于文本解码器模型。

+   `eos_token_id` (`int`, *optional*, defaults to 3) — *流的结尾*文本标记的 id。仅适用于文本解码器模型。

语音编码器特定参数

+   `speech_encoder_layers` (`int`, *optional*, defaults to 24) — Transformer 语音编码器中的隐藏层数量。

+   `speech_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 语音编码器中每个注意力层的注意力头数。

+   `speech_encoder_intermediate_size` (`int`, *optional*, defaults to 4096) — Transformer 语音编码器中“中间”（即前馈）层的维度。

+   `speech_encoder_hidden_act` (`str` or `function`, *optional*, defaults to `"swish"`) — 语音编码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`、`"swish"`和`"gelu_new"`。

+   `speech_encoder_dropout` (`float`, *optional*, defaults to 0.0) — 语音编码器中所有层的 dropout 概率。

+   `add_adapter` (`bool`, *optional*, defaults to `True`) — 在语音编码器顶部添加一个适配器层。

+   `speech_encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 语音编码器的 LayerDrop 概率。有关更多详细信息，请参阅[LayerDrop 论文](https://arxiv.org/abs/1909.11556)。

+   `feature_projection_input_dim` (`int`, *optional*, defaults to 160) — 语音编码器输入特征投影的输入维度，即在使用 SeamlessM4TFeatureExtractor 处理输入音频后的维度。

+   `num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义语音编码器的 1D 卷积位置嵌入层的内核大小。

+   `num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 语音编码器的 1D 卷积位置嵌入层的组数。

+   `adaptor_kernel_size` (`int`, *optional*, defaults to 8) — 适配器网络中卷积层的内核大小。仅在`add_adapter`为 True 时相关。

+   `adaptor_stride` (`int`, *optional*, defaults to 8) — 适配器网络中卷积层的步幅。仅在`add_adapter`为 True 时相关。

+   `adaptor_dropout` (`float`, *optional*, defaults to 0.1) — 语音适配器中所有层的 dropout 概率。

+   `num_adapter_layers` (`int`, *optional*, defaults to 1) — 适配器网络中应使用的卷积层数。仅在`add_adapter`为 True 时相关。

+   `position_embeddings_type` (`str`, *optional*, defaults to `"relative"`) — 可以指定为`relative`或`rotary`，分别用于相对或旋转位置嵌入。如果保持为`None`，则不应用相对位置嵌入。仅应用于语音编码器。

+   `rotary_embedding_base` (`int`, *optional*, defaults to 10000) — 如果使用`"rotary"`位置嵌入，则定义嵌入基数的大小。仅应用于语音编码器。

+   `max_source_positions` (`int`, *optional*, defaults to 4096) — 如果使用`"relative"`位置嵌入，则定义最大源输入位置。仅应用于语音编码器。

+   `conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Conformer 块中卷积深度 1D 层的内核大小。仅应用于语音编码器。

文本到单元组件（t2u）模型特定参数

+   `t2u_bos_token_id` (`int`, *optional*, defaults to 0) — *beginning-of-stream*单元标记的 id。仅应用于文本到单元 seq2seq 模型。

+   `t2u_pad_token_id` (`int`, *optional*, defaults to 1) — *padding*单元标记的 id。仅应用于文本到单元 seq2seq 模型。

+   `t2u_eos_token_id` (`int`, *optional*, defaults to 2) — *end-of-stream*单元标记的 id。仅应用于文本到单元 seq2seq 模型。

+   `t2u_decoder_start_token_id` (`int`, *optional*, defaults to 2) — 如果编码器-解码器模型以与*bos*不同的标记开始解码，则为该标记的 id。仅应用于文本到单元 seq2seq 模型。

+   `t2u_max_new_tokens` (`int`, *optional*, defaults to 1024) — 生成的单元标记的最大数量，忽略提示中的标记数量。仅应用于文本到单元 seq2seq 模型。

+   `t2u_encoder_layers` (`int`, *optional*, defaults to 6) — Transformer 文本到单元编码器中的隐藏层数量。

+   `t2u_encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer 文本到单元编码器中“中间”（即前馈）层的维度。

+   `t2u_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 文本到单元编码器中每个注意力层的注意力头数。

+   `t2u_decoder_layers` (`int`, *optional*, defaults to 6) — Transformer 文本到单元解码器中的隐藏层数量。

+   `t2u_decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer 文本到单元解码器中“中间”（即前馈）层的维度。

+   `t2u_decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 文本到单元解码器中每个注意力层的注意力头数。

+   `t2u_max_position_embeddings` (`int`, *optional*, defaults to 2048) — 此模型文本到单元组件可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如 512、1024 或 2048）。

    > Hifi-Gan 声码器特定参数

+   `sampling_rate` (`int`, *optional*, defaults to 16000) — 生成输出音频的采样率，以赫兹（Hz）表示。

+   `upsample_initial_channel` (`int`, *optional*, defaults to 512) — 输入通道数到 hifi-gan 上采样网络的数量。仅适用于声码器。

+   `upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5, 4, 4, 2, 2]`) — 一个整数元组，定义声码器上采样网络中每个 1D 卷积层的步幅。*upsample_rates*的长度定义了卷积层的数量，并且必须与*upsample_kernel_sizes*的长度匹配。仅适用于声码器。

+   `upsample_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[11, 8, 8, 4, 4]`) — 一个整数元组，定义声码器上采样网络中每个 1D 卷积层的内核大小。*upsample_kernel_sizes*的长度定义了卷积层的数量，并且必须与*upsample_rates*的长度匹配。仅适用于声码器。

+   `resblock_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[3, 7, 11]`) — 一个整数元组，定义多接受域融合（MRF）模块中声码器 1D 卷积层的内核大小。仅适用于声码器。

+   `resblock_dilation_sizes` (`Tuple[Tuple[int]]` 或 `List[List[int]]`, *optional*, 默认为`[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — 一个嵌套的整数元组，定义多接受域融合（MRF）模块中声码器扩张的 1D 卷积层的扩张率。仅适用于声码器。

+   `leaky_relu_slope` (`float`, *optional*, 默认为 0.1) — 在声码器中使用的 leaky ReLU 激活的负斜率角度。仅适用于声码器。

+   `unit_hifi_gan_vocab_size` (`int`, *optional*, 默认为 10000) — SeamlessM4T 声码器的词汇大小。定义了在调用~SeamlessM4TModel、~SeamlessM4TForSpeechToSpeech 或~SeamlessM4TForTextToSpeech 时可以表示的不同单元标记数量。

+   `unit_embed_dim` (`int`, *optional*, 默认为 1280) — 提供给 hifi-gan 声码器的输入 id 的投影维度。仅适用于声码器。

+   `lang_embed_dim` (`int`, *optional*, 默认为 256) — 提供给 hifi-gan 声码器的目标语言的投影维度。仅适用于声码器。

+   `spkr_embed_dim` (`int`, *optional*, 默认为 256) — 提供给 hifi-gan 声码器的说话人 id 的投影维度。仅适用于声码器。

+   `vocoder_num_langs` (`int`, *optional*, 默认为 36) — 声码器支持的语言数量。可能与`t2u_num_langs`不同。

+   `vocoder_num_spkrs` (`int`, *optional*, 默认为 200) — 声码器支持的说话人数量。

+   `variance_predictor_kernel_size` (`int`, *optional*, 默认为 3) — 持续预测器的内核大小。仅适用于声码器。

+   `var_pred_dropout` (`float`, *optional*, 默认为 0.5) — 持续预测器的 dropout 概率。仅适用于声码器。

+   `vocoder_offset` (`int`, *optional*, 默认为 4) — 将单元标记 id 偏移此数字以考虑符号标记。仅适用于声码器。

这是用于存储~SeamlessM4TModel 配置的配置类。根据指定的参数实例化一个 SeamlessM4T 模型，定义模型架构。使用默认值实例化配置将产生类似于 SeamlessM4T[“facebook/hf-seamless-m4t-medium”](https://huggingface.co/%22facebook/hf-seamless-m4t-medium%22)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

```py
>>> from transformers import SeamlessM4TModel, SeamlessM4TConfig

>>> # Initializing a SeamlessM4T "facebook/hf-seamless-m4t-medium" style configuration
>>> configuration = SeamlessM4TConfig()

>>> # Initializing a model from the "facebook/hf-seamless-m4t-medium" style configuration
>>> model = SeamlessM4TModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## SeamlessM4TTokenizer

### `class transformers.SeamlessM4TTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L54)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' tokenizer_file = None src_lang = 'eng' tgt_lang = 'fra' sp_model_kwargs: Optional = None additional_special_tokens = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开头标记。可用作序列分类器标记。

    使用特殊标记构建序列时，这不是用于序列开头的标记。所使用的标记是`cls_token`。

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。

    使用特殊标记构建序列时，这不是用于序列结尾的标记。所使用的标记是`sep_token`。

+   `sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如，用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于序列分类时使用的分类器标记（对整个序列进行分类，而不是对每个标记进行分类）。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时。

+   `tokenizer_file` (`str`, *optional*) — 要使用的分词器文件的路径，而不是词汇文件。

+   `src_lang` (`str`, *optional*, defaults to `"eng"`) — 用作翻译源语言的语言。

+   `tgt_lang` (`str`, *optional*, defaults to `"fra"`) — 用作翻译目标语言的语言。

+   `sp_model_kwargs` (`Dict[str, Any]`, *optional*) — 传递给模型初始化的额外关键字参数。

+   `additional_special_tokens`（元组或`str`或`tokenizers.AddedToken`的列表，*optional*） — 附加特殊标记的元组或列表。可用于指定将由分词器支持的语言列表。

构建一个 SeamlessM4T 分词器。

改编自 RobertaTokenizer 和 XLNetTokenizer。基于[SentencePiece](https://github.com/google/sentencepiece)。

源语言文档的标记方法为`<language code> <tokens> <eos>`，目标语言文档的标记方法为`<eos> <language code> <tokens> <eos>`。

示例：

```py
>>> from transformers import SeamlessM4TTokenizer

>>> tokenizer = SeamlessM4TTokenizer.from_pretrained(
...     "facebook/hf-seamless-m4t-medium", src_lang="eng", tgt_lang="fra"
... )
>>> example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
>>> expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L217)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None padding: Union = True pad_to_multiple_of: Optional = 2 src_lang: Optional = None tgt_lang: Optional = None **kwargs )
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次。每个序列可以是一个字符串或一个字符串列表（预先标记化的字符串）。如果序列以字符串列表（预先标记化）的形式提供，则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `padding`（`bool`，`str`或 PaddingStrategy，*可选*，默认为`True`） — 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数`max_length`）或填充到模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：无填充（即可以输出长度不同的序列批次）。

+   `pad_to_multiple_of`（`int`，*可选*） — 如果设置，将填充序列到提供的值的倍数。

    这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力为`>= 7.5`（Volta）。

+   `src_lang`（`str`，*可选*） — 表示源语言的字符串。如果未指定，则将使用上次指定的`src_lang`（在初始化时或调用此分词器时）。

+   `tgt_lang`（`str`，*可选*） — 表示目标语言的字符串。如果未指定，则将使用上次指定的`tgt_lang`（在初始化时或调用此分词器时）。

+   `kwargs`（*可选*） — 传递给 PreTrainedTokenizer.`call`()的剩余关键字参数字典。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L347)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

带有适当特殊标记的 input IDs 列表。

通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。NLLB 序列具有以下格式，其中`X`表示序列：

+   `input_ids`（用于编码器）`X [eos, src_lang_code]`

+   `decoder_input_ids`:（用于解码器）`X [eos, tgt_lang_code]`

BOS 从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L316)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`） — 标记列表是否已经为模型格式化了特殊标记。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。在使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L375)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。nllb 不使用标记类型 ID，因此返回一个零列表。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L498)

```py
( save_directory: str filename_prefix: Optional = None )
```

## SeamlessM4TTokenizerFast

### `class transformers.SeamlessM4TTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L54)

```py
( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' src_lang = 'eng' tgt_lang = 'fra' additional_special_tokens = None **kwargs )
```

参数

+   `vocab_file` (`str`, *可选*) — 词汇表文件的路径。

+   `tokenizer_file` (`str`, *可选*) — 要使用的分词器文件的路径，而不是词汇表文件。

+   `bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开头标记。可以用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是 `cls_token`。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是 `sep_token`。

+   `sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 在进行序列分类（整个序列而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `src_lang` (`str`, *可选*, 默认为 `"eng"`) — 用作翻译源语言的语言。

+   `tgt_lang` (`str`, *可选*, 默认为 `"fra"`) — 用作翻译目标语言的语言。

+   `additional_special_tokens`（`str` 或 `tokenizers.AddedToken` 的元组或列表，*可选*） — 附加特殊标记的元组或列表。

构建一个“快速” SeamlessM4T 分词器（由 HuggingFace 的 *tokenizers* 库支持）。基于[BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)。

此分词器继承自 PreTrainedTokenizerFast，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

对于源语言文档，标记方法是 `<语言代码> <标记> <eos>`，对于目标语言文档，标记方法是 `<eos> <语言代码> <标记> <eos>`。

示例：

```py
>>> from transformers import SeamlessM4TTokenizerFast

>>> tokenizer = SeamlessM4TTokenizerFast.from_pretrained(
...     "facebook/hf-seamless-m4t-medium", src_lang="eng", tgt_lang="fra"
... )
>>> example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
>>> expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L390)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None padding: Union = True pad_to_multiple_of: Optional = 2 src_lang: Optional = None tgt_lang: Optional = None **kwargs )
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置 `is_split_into_words=True`（以消除与序列批次的歧义）。

+   `text_pair`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_target`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair_target`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `padding`（`bool`，`str`或 PaddingStrategy，*可选*，默认为`True`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：

    +   `True`或`'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型可接受的最大输入长度。

    +   `False`或`'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度序列的批次）。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。

    这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力为`>= 7.5`（Volta）。

+   `src_lang`（`str`，*可选*）— 表示源语言的字符串。如果未指定，则将使用上次指定的`src_lang`（在初始化期间或在调用此分词器时）。

+   `tgt_lang`（`str`，*可选*）— 表示目标语言的字符串。如果未指定，则将使用上次指定的`tgt_lang`（在初始化期间或在调用此分词器时）。

+   `kwargs`（*可选*）— 将传递给 PreTrainedTokenizerFast.`call`()的剩余关键字参数字典。

## SeamlessM4TFeatureExtractor

### `class transformers.SeamlessM4TFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L38)

```py
( feature_size = 80 sampling_rate = 16000 num_mel_bins = 80 padding_value = 0.0 stride = 2 **kwargs )
```

参数

+   `feature_size`（`int`，*可选*，默认为 80）— 提取特征的特征维度。

+   `sampling_rate`（`int`，*可选*，默认为 16000）— 应数字化音频文件的采样率，以赫兹（Hz）表示。

+   `num_mel_bins`（`int`，*可选*，默认为 80）— Mel 频率箱数。

+   `padding_value`（`float`，*可选*，默认为 0.0）— 用于填充向量的值。

+   `stride`（`int`，*可选*，默认为 2）— 用于将音频从形状（batch_size，num_frames，num_mel_bins）重塑为（batch_size，num_frames//stride，num_mel_bins*stride）的步幅。

构建一个 SeamlessM4T 特征提取器。

此特征提取器继承自 SequenceFeatureExtractor，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

该类从原始语音中提取 mel 滤波器组特征。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L144)

```py
( raw_speech: Union padding: Union = True pad_to_multiple_of: Optional = 2 max_length: Optional = None truncation: bool = False return_tensors: Union = None sampling_rate: Optional = None return_attention_mask: Optional = None do_normalize_per_mel_bins: Optional = True **kwargs )
```

参数

+   `raw_speech` (`np.ndarray`, `torch.Tensor`, `List[float]`, `List[np.ndarray]`, `List[torch.Tensor]`, —

+   `List[List[float]],` `List[List[List[float]]]`) — 要填充的序列或序列批次。每个序列可以是一个 numpy 数组、一个 torch 张量、一个浮点值列表、一个 numpy 数组列表、一个 torch 张量列表、一个浮点值列表的列表或一个浮点值列表的列表的列表。如果 `raw_speech` 是一维的 `np.ndarray`、`torch.Tensor` 或 `List[float]`，则将 `raw_speech` 视为单声道、单样本声音。在所有其他情况下，无论是来自 `np.ndarray`、`torch.Tensor` 还是 `List[...]` 的第一个维度，都对应于批次中的样本数，通道数（即单声道或立体声特征）从其他维度中派生（1D -> 单声道波形批次；2D -> 立体声波形批次）。

+   `padding` (`bool`, `str` 或 PaddingStrategy, *optional*, 默认为 `True`) — 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。

    +   `'max_length'`: 使用参数 `max_length` 指定的最大长度进行填充，或者如果未提供该参数，则填充到模型可接受的最大输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：无填充（即，可以输出长度不同的序列批次）。

+   `pad_to_multiple_of` (`int`, *optional*, 默认为 2) — 如果设置，将序列填充到提供的值的倍数。

    这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力 `>= 7.5`（Volta），或者对于受益于序列长度为 128 的倍数的 TPU。

+   `max_length` (`int`, *optional*) — 返回列表的最大长度和可选填充长度（见上文）。

+   `truncation` (`bool`) — 激活截断，将输入序列截断为长于 *max_length* 的部分至 *max_length*。

+   `return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定 feature_extractor 的默认设置返回注意力掩码。

    什么是注意力掩码？

    对于 SeamlessM4T 模型，在批量推理时应始终传递 `attention_mask`，以避免细微错误。

+   `return_tensors` (`str` 或 TensorType, *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `sampling_rate` (`int`, *optional*) — `raw_speech` 输入的采样率。强烈建议在前向调用时传递 `sampling_rate`，以防止潜在错误。

+   `do_normalize_per_mel_bins` (`bool`, *optional*, 默认为 `True`) — 是否对每个 mel 通道的输入进行零均值单位方差归一化。

+   `kwargs` (*optional*) — 将传递给分词器或特征提取器的剩余关键字参数字典。

对一个或多个序列进行特征化和准备模型的主要方法。

## SeamlessM4TProcessor

### `class transformers.SeamlessM4TProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L22)

```py
( feature_extractor tokenizer )
```

参数

+   `feature_extractor` (SeamlessM4TFeatureExtractor) — 音频处理器是必需的输入。

+   `tokenizer` (SeamlessM4TTokenizerFast) — 必需的输入是 tokenizer。

构建一个 SeamlessM4T 处理器，将 SeamlessM4T 特征提取器和 SeamlessM4T tokenizer 封装成一个单一处理器。

SeamlessM4TProcessor 提供了 SeamlessM4TFeatureExtractor 和 SeamlessM4TTokenizerFast 的所有功能。查看**call**()和`decode()`获取更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L44)

```py
( text = None audios = None src_lang = None tgt_lang = None **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。

+   `audios` (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`) — 要准备的音频或音频批次。每个音频可以是 NumPy 数组或 PyTorch 张量。对于 NumPy 数组/PyTorch 张量，每个音频的形状应为(C, T)，其中 C 是通道数，T 是音频的采样长度。

+   `src_lang` (`str`, *可选*) — 输入文本/音频的语言代码。如果未指定，将使用最后指定的`src_lang`。

+   `tgt_lang` (`str`, *可选*) — 目标语言的代码。如果未指定，将使用最后指定的`tgt_lang`。

+   `kwargs` (*可选*) — 将传递给特征提取器和/或 tokenizer 的剩余关键字参数字典。

返回

BatchEncoding

一个带有以下字段的 BatchEncoding:

+   `input_ids` — 要输入模型的标记 id 列表。当`text`不是`None`时返回。

+   `attention_mask` — 指定哪些标记应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中，且`text`不是`None`时）。

+   `input_features` — 要输入模型的音频输入特征。当`audios`不是`None`时返回。

准备模型的主要方法是准备一个或多个序列和音频。如果`text`不是`None`，则将`text`和`kwargs`参数转发给 SeamlessM4TTokenizerFast 的**call**()来对文本进行编码。要准备音频，如果`audios`不是`None`，则将`audios`和`kwrags`参数转发给 SeamlessM4TFeatureExtractor 的**call**()。更多信息请参考上述两种方法的文档。

## SeamlessM4TCodeHifiGan

### `class transformers.SeamlessM4TCodeHifiGan`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2477)

```py
( config )
```

参数

+   `config` (SeamlessM4TConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

根据此[存储库](https://github.com/facebookresearch/speech-resynthesis)中描述的 HiFi-GAN 声码器代码。此模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2557)

```py
( input_ids: LongTensor spkr_id: Tensor lang_id: Tensor )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTextToUnitForConditionalGeneration 获取索引。什么是输入 ID？

+   `spkr_id` (`int`, *可选*) — 用于语音合成的说话者 ID。必须小于`config.vocoder_num_spkrs`。

+   `tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言 ID。

## SeamlessM4THifiGan

### `class transformers.SeamlessM4THifiGan`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2405)

```py
( config: SeamlessM4TConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2440)

```py
( input_embeds: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `spectrogram` (`torch.FloatTensor`) — 包含对数梅尔频谱图的张量。可以是批处理的，形状为`(batch_size, sequence_length, model_in_dim)`，也可以是未经批处理的，形状为`(sequence_length, model_in_dim)`。请注意，`model_in_dim`是`config.unit_embed_dim`、`config.lang_embed_dim`和`config.spkr_embed_dim`的总和。

返回

`torch.FloatTensor`

包含语音波形的张量。如果输入的频谱图是批处理的，则形状为`(batch_size, num_frames,)`。如果未经批处理，则形状为`(num_frames,)`。

将对数梅尔频谱图转换为语音波形。传递一批对数梅尔频谱图将返回一批语音波形。传递单个、未经批处理的对数梅尔频谱图将返回单个、未经批处理的语音波形。

## SeamlessM4TTextToUnitModel

### `class transformers.SeamlessM4TTextToUnitModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2039)

```py
( config: SeamlessM4TConfig embed_tokens_decoder: Optional = None )
```

参数

+   `config` (~SeamlessM4TConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

+   `embed_tokens_decoder` (`nn.Embedding`, *可选*) — 解码器的输入嵌入。

Transformer 裸文本到单元编码器-解码器。编码器是一个没有嵌入的`SeamlessM4TEncoder`，解码器是一个`SeamlessM4TDecoder`。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

## SeamlessM4TTextToUnitForConditionalGeneration

### `class transformers.SeamlessM4TTextToUnitForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2128)

```py
( config: SeamlessM4TConfig embed_tokens_decoder: Optional = None )
```

参数

+   `config`（~SeamlessM4TConfig）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

+   `embed_tokens_decoder`（`nn.Embedding`，*可选*）- 解码器的输入嵌入。

带有语言模型头的文本到单元编码器-解码器。基本编码器-解码器模型是`SeamlessM4TTextToUnit`。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2181)

```py
( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。查看 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()获取详细信息。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 避免对填充标记索引执行注意力的掩码。掩码值选在`[0, 1]`范围内：

    +   1 表示未被`masked`的标记，

    +   0 表示被`masked`的标记。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 词汇表中解码器输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。查看 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()获取详细信息。

    什么是解码器输入 ID？

    Bart 使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

    对于翻译和摘要训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将通过将`input_ids`向右移动来创建此张量，以进行去噪预训练，遵循论文。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

    如果要更改填充行为，您应该阅读`modeling_bart._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包含（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 元组由长度为`config.n_layers`的`tuple(torch.FloatTensor)`组成，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，以及 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（请参阅`past_key_values`）。如果您想要更多控制权来将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

SeamlessM4TTextToUnitForConditionalGeneration 的前向方法重写了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
