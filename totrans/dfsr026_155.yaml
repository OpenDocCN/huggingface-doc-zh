- en: PixArt-α
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/pixart](https://huggingface.co/docs/diffusers/api/pipelines/pixart)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a666bb8097e14b9e3eb092b69530169.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image
    Synthesis](https://huggingface.co/papers/2310.00426) is Junsong Chen, Jincheng
    Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping
    Luo, Huchuan Lu, and Zhenguo Li.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The most advanced text-to-image (T2I) models require significant training
    costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation
    for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-α,
    a Transformer-based T2I diffusion model whose image generation quality is competitive
    with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney),
    reaching near-commercial application standards. Additionally, it supports high-resolution
    image synthesis up to 1024px resolution with low training cost, as shown in Figure
    1 and 2\. To achieve this goal, three core designs are proposed: (1) Training
    strategy decomposition: We devise three distinct training steps that separately
    optimize pixel dependency, text-image alignment, and image aesthetic quality;
    (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion
    Transformer (DiT) to inject text conditions and streamline the computation-intensive
    class-condition branch; (3) High-informative data: We emphasize the significance
    of concept density in text-image pairs and leverage a large Vision-Language model
    to auto-label dense pseudo-captions to assist text-image alignment learning. As
    a result, PIXART-α’s training speed markedly surpasses existing large-scale T2I
    models, e.g., PIXART-α only takes 10.8% of Stable Diffusion v1.5’s training time
    (675 vs. 6,250 A100 GPU days), saving nearly $300,000 ($26,000 vs. $320,000) and
    reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL,
    our training cost is merely 1%. Extensive experiments demonstrate that PIXART-α
    excels in image quality, artistry, and semantic control. We hope PIXART-α will
    provide new insights to the AIGC community and startups to accelerate building
    their own high-quality yet low-cost generative models from scratch.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the original codebase at [PixArt-alpha/PixArt-alpha](https://github.com/PixArt-alpha/PixArt-alpha)
    and all the available checkpoints at [PixArt-alpha](https://huggingface.co/PixArt-alpha).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some notes about this pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a Transformer backbone (instead of a UNet) for denoising. As such it
    has a similar architecture as [DiT](./dit).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was trained using text conditions computed from T5\. This aspect makes the
    pipeline better at following complex text prompts with intricate details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is good at producing high-resolution images at different aspect ratios. To
    get the best results, the authors recommend some size brackets which can be found
    [here](https://github.com/PixArt-alpha/PixArt-alpha/blob/08fbbd281ec96866109bdd2cdb75f2f58fb17610/diffusion/data/datasets/utils.py).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It rivals the quality of state-of-the-art text-to-image generation systems (as
    of this writing) such as Stable Diffusion XL, Imagen, and DALL-E 2, while being
    more efficient than them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Inference with under 8GB GPU VRAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Run the [PixArtAlphaPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pixart#diffusers.PixArtAlphaPipeline)
    with under 8GB GPU VRAM by loading the text encoder in 8-bit precision. Let’s
    walk through a full-fledged example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then load the text encoder in 8-bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the `pipe` to encode a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since text embeddings have been computed, remove the `text_encoder` and `pipe`
    from the memory, and free up som GPU VRAM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then compute the latents with the prompt embeddings as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that while initializing `pipe`, you’re setting `text_encoder` to `None`
    so that it’s not loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the latents are computed, pass it off to the VAE to decode into a real
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By deleting components you aren’t using and flushing the GPU VRAM, you should
    be able to run [PixArtAlphaPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pixart#diffusers.PixArtAlphaPipeline)
    with under 8GB GPU VRAM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9bdd252908e7f92b9ab48ae4b8febe6.png)'
  prefs: []
  type: TYPE_IMG
- en: If you want a report of your memory-usage, run this [script](https://gist.github.com/sayakpaul/3ae0f847001d342af27018a96f467e4e).
  prefs: []
  type: TYPE_NORMAL
- en: Text embeddings computed in 8-bit can impact the quality of the generated images
    because of the information loss in the representation space caused by the reduced
    precision. It’s recommended to compare the outputs with and without 8-bit.
  prefs: []
  type: TYPE_NORMAL
- en: While loading the `text_encoder`, you set `load_in_8bit` to `True`. You could
    also specify `load_in_4bit` to bring your memory requirements down even further
    to under 7GB.
  prefs: []
  type: TYPE_NORMAL
- en: PixArtAlphaPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.PixArtAlphaPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L182)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` (`T5EncoderModel`) — Frozen text-encoder. PixArt-Alpha uses
    [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel),
    specifically the [t5-v1_1-xxl](https://huggingface.co/PixArt-alpha/PixArt-alpha/tree/main/t5-v1_1-xxl)
    variant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`T5Tokenizer`) — Tokenizer of class [T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer` ([Transformer2DModel](/docs/diffusers/v0.26.3/en/api/models/transformer2d#diffusers.Transformer2DModel))
    — A text conditioned `Transformer2DModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `transformer` to denoise the encoded
    image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-image generation using PixArt-Alpha.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L666)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timesteps` (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.5) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to self.unet.config.sample_size) — The
    height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to self.unet.config.sample_size) — The
    width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) in the DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    will be ignored for others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_attention_mask` (`torch.FloatTensor`, *optional*) — Pre-generated attention
    mask for text embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. For PixArt-Alpha this negative prompt should be "".
    If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_attention_mask` (`torch.FloatTensor`, *optional*) — Pre-generated
    attention mask for negative text embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~pipelines.stable_diffusion.IFPipelineOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) — A function that will be called every
    `callback_steps` steps during inference. The function will be called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_caption` (`bool`, *optional*, defaults to `True`) — Whether or not to
    clean the caption before creating embeddings. Requires `beautifulsoup4` and `ftfy`
    to be installed. If the dependencies are not installed, the embeddings will be
    created from the raw prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_resolution_binning` (`bool` defaults to `True`) — If set to `True`, the
    requested height and width are first mapped to the closest resolutions using `ASPECT_RATIO_1024_BIN`.
    After the produced latents are decoded into images, they are resized back to the
    requested resolution. Useful for generating non-square images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#### `classify_height_width_bin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L634)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Returns binned height and width.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L251)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt not to guide
    the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`). For PixArt-Alpha, this should be "".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`, *optional*, defaults to `True`) — whether
    to use classifier free guidance or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — number of images
    that should be generated per prompt device — (`torch.device`, *optional*): torch
    device to place the resulting embeddings on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. For PixArt-Alpha, it’s should be the embeddings of the
    "" string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_caption` (bool, defaults to `False`) — If `True`, the function will
    preprocess and clean the provided caption before encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
