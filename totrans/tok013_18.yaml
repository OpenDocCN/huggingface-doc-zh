- en: Trainers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/api/trainers](https://huggingface.co/docs/tokenizers/api/trainers)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    PythonRustNode
  prefs: []
  type: TYPE_NORMAL
- en: BpeTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.trainers.BpeTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*) — The size of the final vocabulary, including
    all tokens and alphabet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_frequency` (`int`, *optional*) — The minimum frequency a pair should have
    in order to be merged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show_progress` (`bool`, *optional*) — Whether to show progress bars while
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens` (`List[Union[str, AddedToken]]`, *optional*) — A list of special
    tokens the model should know of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit_alphabet` (`int`, *optional*) — The maximum different characters to
    keep in the alphabet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_alphabet` (`List[str]`, *optional*) — A list of characters to include
    in the initial alphabet, even if not seen in the training dataset. If the strings
    contain more than one character, only the first one is kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`continuing_subword_prefix` (`str`, *optional*) — A prefix to be used for every
    subword that is not a beginning-of-word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_of_word_suffix` (`str`, *optional*) — A suffix to be used for every subword
    that is a end-of-word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_token_length` (`int`, *optional*) — Prevents creating tokens longer than
    the specified size. This can help with reducing polluting your vocabulary with
    highly repetitive tokens like *======* for wikipedia'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trainer capable of training a BPE model
  prefs: []
  type: TYPE_NORMAL
- en: UnigramTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.trainers.UnigramTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`) — The size of the final vocabulary, including all tokens
    and alphabet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show_progress` (`bool`) — Whether to show progress bars while training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens` (`List[Union[str, AddedToken]]`) — A list of special tokens
    the model should know of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_alphabet` (`List[str]`) — A list of characters to include in the initial
    alphabet, even if not seen in the training dataset. If the strings contain more
    than one character, only the first one is kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shrinking_factor` (`float`) — The shrinking factor used at each step of the
    training to prune the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`) — The token used for out-of-vocabulary tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_piece_length` (`int`) — The maximum length of a given token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_sub_iterations` (`int`) — The number of iterations of the EM algorithm to
    perform before pruning the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trainer capable of training a Unigram model
  prefs: []
  type: TYPE_NORMAL
- en: WordLevelTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.trainers.WordLevelTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*) — The size of the final vocabulary, including
    all tokens and alphabet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_frequency` (`int`, *optional*) — The minimum frequency a pair should have
    in order to be merged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show_progress` (`bool`, *optional*) — Whether to show progress bars while
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens` (`List[Union[str, AddedToken]]`) — A list of special tokens
    the model should know of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trainer capable of training a WorldLevel model
  prefs: []
  type: TYPE_NORMAL
- en: WordPieceTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.trainers.WordPieceTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*) — The size of the final vocabulary, including
    all tokens and alphabet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_frequency` (`int`, *optional*) — The minimum frequency a pair should have
    in order to be merged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show_progress` (`bool`, *optional*) — Whether to show progress bars while
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens` (`List[Union[str, AddedToken]]`, *optional*) — A list of special
    tokens the model should know of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit_alphabet` (`int`, *optional*) — The maximum different characters to
    keep in the alphabet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_alphabet` (`List[str]`, *optional*) — A list of characters to include
    in the initial alphabet, even if not seen in the training dataset. If the strings
    contain more than one character, only the first one is kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`continuing_subword_prefix` (`str`, *optional*) — A prefix to be used for every
    subword that is not a beginning-of-word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_of_word_suffix` (`str`, *optional*) — A suffix to be used for every subword
    that is a end-of-word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trainer capable of training a WordPiece model
  prefs: []
  type: TYPE_NORMAL
