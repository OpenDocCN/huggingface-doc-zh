["```py\npip install --upgrade torch diffusers\n```", "```py\n  import torch\n  from diffusers import DiffusionPipeline\n+ from diffusers.models.attention_processor import AttnProcessor2_0\n\n  pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n+ pipe.unet.set_attn_processor(AttnProcessor2_0())\n\n  prompt = \"a photo of an astronaut riding a horse on mars\"\n  image = pipe(prompt).images[0]\n```", "```py\n  import torch\n  from diffusers import DiffusionPipeline\n\n  pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n+ pipe.unet.set_default_attn_processor()\n\n  prompt = \"a photo of an astronaut riding a horse on mars\"\n  image = pipe(prompt).images[0]\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\nimages = pipe(prompt, num_inference_steps=steps, num_images_per_prompt=batch_size).images[0]\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npath = \"runwayml/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\n\npipe = DiffusionPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    images = pipe(prompt=prompt).images\n```", "```py\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom diffusers.utils import load_image\nimport torch\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\ninit_image = load_image(url)\ninit_image = init_image.resize((512, 512))\n\npath = \"runwayml/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image).images[0]\n```", "```py\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom diffusers.utils import load_image\nimport torch\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).resize((512, 512))\nmask_image = load_image(mask_url).resize((512, 512))\n\npath = \"runwayml/stable-diffusion-inpainting\"\n\nrun_compile = True  # Set True / False\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```", "```py\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.utils import load_image\nimport torch\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\ninit_image = load_image(url)\ninit_image = init_image.resize((512, 512))\n\npath = \"runwayml/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    path, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\npipe.controlnet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe.controlnet = torch.compile(pipe.controlnet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image).images[0]\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\nrun_compile = True  # Set True / False\n\npipe_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\", variant=\"fp16\", text_encoder=None, torch_dtype=torch.float16, use_safetensors=True)\npipe_1.to(\"cuda\")\npipe_2 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-II-M-v1.0\", variant=\"fp16\", text_encoder=None, torch_dtype=torch.float16, use_safetensors=True)\npipe_2.to(\"cuda\")\npipe_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", torch_dtype=torch.float16, use_safetensors=True)\npipe_3.to(\"cuda\")\n\npipe_1.unet.to(memory_format=torch.channels_last)\npipe_2.unet.to(memory_format=torch.channels_last)\npipe_3.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    pipe_1.unet = torch.compile(pipe_1.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe_2.unet = torch.compile(pipe_2.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe_3.unet = torch.compile(pipe_3.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"the blue hulk\"\n\nprompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\nneg_prompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\n\nfor _ in range(3):\n    image_1 = pipe_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type=\"pt\").images\n    image_2 = pipe_2(image=image_1, prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type=\"pt\").images\n    image_3 = pipe_3(prompt=prompt, image=image_1, noise_level=100).images\n```"]