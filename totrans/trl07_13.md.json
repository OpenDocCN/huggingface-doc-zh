["```py\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n)\ntrainer.train()\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n)\n\ntrainer.train()\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n\ndataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\nresponse_template = \" ### Answer:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    formatting_func=formatting_prompts_func,\n    data_collator=collator,\n)\n\ntrainer.train()\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\ninstruction_template = \"### Human:\"\nresponse_template = \"### Assistant:\"\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    data_collator=collator,\n)\n\ntrainer.train()\n```", "```py\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\ndef print_tokens_with_ids(txt):\n    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n    print(list(zip(tokens, token_ids)))\n\nprompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\nprint_tokens_with_ids(prompt)  # [..., ('\u2581Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('\u2581Ass', 4007), ('istant', 22137), (':', 29901), ...]\n\nresponse_template = \"### Assistant:\"\nprint_tokens_with_ids(response_template)  # [('\u2581###', 835), ('\u2581Ass', 4007), ('istant', 22137), (':', 29901)]\n```", "```py\nRuntimeError: Could not find response key [835, 4007, 22137, 29901] in token IDs tensor([    1,   835,  ...])\n```", "```py\nresponse_template_with_context = \"\\n### Assistant:\"  # We added context here: \"\\n\". This is enough for this tokenizer\nresponse_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n\ndata_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\n# Set up the chat format with default 'chatml' format\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n```", "```py\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n```", "```py\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n```", "```py\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\n...\n\n# load jsonl dataset\ndataset = load_dataset(\"json\", data_files=\"path/to/dataset.jsonl\", split=\"train\")\n# load dataset from the HuggingFace Hub\ndataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n\n...\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    args=training_args,\n    train_dataset=dataset,\n    packing=True,\n)\n```", "```py\nBelow is an instruction ...\n\n### Instruction\n{prompt}\n\n### Response:\n{completion}\n```", "```py\n...\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['question'])):\n        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    formatting_func=formatting_prompts_func,\n)\n\ntrainer.train()\n```", "```py\n...\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    packing=True\n)\n\ntrainer.train()\n```", "```py\ndef formatting_func(example):\n    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n    return text\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    packing=True,\n    formatting_func=formatting_func\n)\n\ntrainer.train()\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.bfloat16)\n```", "```py\n...\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    model_init_kwargs={\n        \"torch_dtype\": torch.bfloat16,\n    },\n)\n\ntrainer.train()\n```", "```py\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ntrainer = SFTTrainer(\n    \"EleutherAI/gpt-neo-125m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    peft_config=peft_config\n)\n\ntrainer.train()\n```", "```py\n...\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"EleutherAI/gpt-neo-125m\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    peft_config=peft_config,\n)\n\ntrainer.train()\n```", "```py\npip install -U git+https://github.com/huggingface/transformers.git\n```", "```py\npip install -U optimum\n```", "```py\n...\n\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    trainer.train()\n```", "```py\npip install -U flash-attn\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=True,\n    use_flash_attention_2=True\n)\n```", "```py\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    neftune_noise_alpha=5,\n)\ntrainer.train()\n```", "```py\nimport torch\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom unsloth import FastLanguageModel\n\nmax_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number\n\n# Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b\",\n    max_seq_length = max_seq_length,\n    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Dropout = 0 is currently optimized\n    bias = \"none\",    # Bias = \"none\" is currently optimized\n    use_gradient_checkpointing = True,\n    random_state = 3407,\n)\n\nargs = TrainingArguments(output_dir = \"./output\")\n\ntrainer = SFTTrainer(\n    model = model,\n    args = args,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n)\ntrainer.train()\n```", "```py\n( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None peft_config: Optional = None dataset_text_field: Optional = None packing: Optional = False formatting_func: Optional = None max_seq_length: Optional = None infinite: Optional = None num_of_sequences: Optional = 1024 chars_per_token: Optional = 3.6 dataset_num_proc: Optional = None dataset_batch_size: int = 1000 neftune_noise_alpha: Optional = None model_init_kwargs: Optional = None dataset_kwargs: Optional = None )\n```", "```py\n( tokenizer dataset dataset_text_field = None formatting_func = None infinite = False seq_length = 1024 num_of_sequences = 1024 chars_per_token = 3.6 eos_token_id = 0 shuffle = True append_concat_token = True add_special_tokens = True )\n```"]