["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/text_to_image\npip install -r requirements_sdxl.txt\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_text_to_image_sdxl.py \\\n  --mixed_precision=\"bf16\"\n```", "```py\naccelerate launch train_text_to_image_sdxl.py \\\n  --snr_gamma=5.0\n```", "```py\ntokenizer_one = AutoTokenizer.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision, use_fast=False\n)\ntokenizer_two = AutoTokenizer.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"tokenizer_2\", revision=args.revision, use_fast=False\n)\n\ntext_encoder_cls_one = import_model_class_from_model_name_or_path(\n    args.pretrained_model_name_or_path, args.revision\n)\ntext_encoder_cls_two = import_model_class_from_model_name_or_path(\n    args.pretrained_model_name_or_path, args.revision, subfolder=\"text_encoder_2\"\n)\n```", "```py\ntext_encoders = [text_encoder_one, text_encoder_two]\ntokenizers = [tokenizer_one, tokenizer_two]\ncompute_embeddings_fn = functools.partial(\n    encode_prompt,\n    text_encoders=text_encoders,\n    tokenizers=tokenizers,\n    proportion_empty_prompts=args.proportion_empty_prompts,\n    caption_column=args.caption_column,\n)\n\ntrain_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\ntrain_dataset = train_dataset.map(\n    compute_vae_encodings_fn,\n    batched=True,\n    batch_size=args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps,\n    new_fingerprint=new_fingerprint_for_vae,\n)\n```", "```py\ndel text_encoders, tokenizers, vae\ngc.collect()\ntorch.cuda.empty_cache()\n```", "```py\nweights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(\n        model_input.device\n    )\n    timesteps = torch.multinomial(weights, bsz, replacement=True).long()\n\nnoisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)\n```", "```py\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch train_text_to_image_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --enable_xformers_memory_efficient_attention \\\n  --resolution=512 \\\n  --center_crop \\\n  --random_flip \\\n  --proportion_empty_prompts=0.2 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=10000 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-06 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --report_to=\"wandb\" \\\n  --validation_prompt=\"a cute Sundar Pichai creature\" \\\n  --validation_epochs 5 \\\n  --checkpointing_steps=5000 \\\n  --output_dir=\"sdxl-pokemon-model\" \\\n  --push_to_hub\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"path/to/your/model\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A pokemon with green eyes and red legs.\"\nimage = pipeline(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"pokemon.png\")\n```"]