["```py\nimport os\nfrom transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n\nos.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n\ncheckpoint = 'transfo-xl-wt103'\nrevision = '40a186da79458c9f9de846edfaea79c412137f97'\n\ntokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\nmodel = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n```", "```py\n>>> from transformers import TransfoXLConfig, TransfoXLModel\n\n>>> # Initializing a Transformer XL configuration\n>>> configuration = TransfoXLConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = TransfoXLModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, TransfoXLModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, TransfoXLLMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLLMHeadModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, TransfoXLForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, TransfoXLForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\n...     \"transfo-xl-wt103\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, TFTransfoXLModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TFTransfoXLModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFTransfoXLLMHeadModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TFTransfoXLLMHeadModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, TFTransfoXLForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TFTransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFTransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```"]