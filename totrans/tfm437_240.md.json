["```py\nimport os\nfrom transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n\nos.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n\ncheckpoint = 'transfo-xl-wt103'\nrevision = '40a186da79458c9f9de846edfaea79c412137f97'\n\ntokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\nmodel = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n```", "```py\n( vocab_size = 267735 cutoffs = [20000, 40000, 200000] d_model = 1024 d_embed = 1024 n_head = 16 d_head = 64 d_inner = 4096 div_val = 4 pre_lnorm = False n_layer = 18 mem_len = 1600 clamp_len = 1000 same_length = True proj_share_all_but_first = True attn_type = 0 sample_softmax = -1 adaptive = True dropout = 0.1 dropatt = 0.0 untie_r = True init = 'normal' init_range = 0.01 proj_init_std = 0.01 init_std = 0.02 layer_norm_epsilon = 1e-05 eos_token_id = 0 **kwargs )\n```", "```py\n>>> from transformers import TransfoXLConfig, TransfoXLModel\n\n>>> # Initializing a Transformer XL configuration\n>>> configuration = TransfoXLConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = TransfoXLModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( special = None min_freq = 0 max_size = None lower_case = False delimiter = None vocab_file = None pretrained_vocab_file: str = None never_split = None unk_token = '<unk>' eos_token = '<eos>' additional_special_tokens = ['<formula>'] language = 'en' **kwargs )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( last_hidden_state: FloatTensor mems: List = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( losses: Optional = None prediction_scores: FloatTensor = None mems: List = None hidden_states: Optional = None attentions: Optional = None loss: Optional = None )\n```", "```py\n( last_hidden_state: tf.Tensor = None mems: List[tf.Tensor] = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( prediction_scores: tf.Tensor = None mems: List[tf.Tensor] = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None mems: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TransfoXLModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None mems: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, TransfoXLLMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLLMHeadModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None mems: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, TransfoXLForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, TransfoXLForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TransfoXLForSequenceClassification.from_pretrained(\n...     \"transfo-xl-wt103\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None mems: List[tf.Tensor] | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFTransfoXLModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TFTransfoXLModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: TFModelInputType | None = None mems: List[tf.Tensor] | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFTransfoXLLMHeadModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TFTransfoXLLMHeadModel.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None mems: List[tf.Tensor] | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFTransfoXLForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"transfo-xl-wt103\")\n>>> model = TFTransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFTransfoXLForSequenceClassification.from_pretrained(\"transfo-xl-wt103\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( n_token d_embed d_proj cutoffs div_val = 1 sample_softmax = False )\n```", "```py\n( n_token d_embed d_proj cutoffs div_val = 1 init_std = 0.02 sample_softmax = False **kwargs )\n```"]