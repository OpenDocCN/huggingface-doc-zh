- en: Stable Diffusion XL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£XL
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/using-diffusers/sdxl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŽŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/using-diffusers/sdxl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Stable Diffusion XL](https://huggingface.co/papers/2307.01952) (SDXL) is a
    powerful text-to-image generation model that iterates on the previous Stable Diffusion
    models in three key ways:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç¨³å®šæ‰©æ•£XL](https://huggingface.co/papers/2307.01952)ï¼ˆSDXLï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œé€šè¿‡ä¸‰ç§å…³é”®æ–¹å¼æ”¹è¿›äº†å…ˆå‰çš„ç¨³å®šæ‰©æ•£æ¨¡åž‹ï¼š'
- en: the UNet is 3x larger and SDXL combines a second text encoder (OpenCLIP ViT-bigG/14)
    with the original text encoder to significantly increase the number of parameters
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UNetæ˜¯3å€å¤§ï¼ŒSDXLå°†ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆOpenCLIP ViT-bigG/14ï¼‰ä¸ŽåŽŸå§‹æ–‡æœ¬ç¼–ç å™¨ç»“åˆèµ·æ¥ï¼Œæ˜¾è‘—å¢žåŠ å‚æ•°æ•°é‡
- en: introduces size and crop-conditioning to preserve training data from being discarded
    and gain more control over how a generated image should be cropped
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼•å…¥å°ºå¯¸å’Œè£å‰ªæ¡ä»¶ï¼Œä»¥ä¿ç•™è®­ç»ƒæ•°æ®ä¸è¢«ä¸¢å¼ƒï¼Œå¹¶æ›´å¥½åœ°æŽ§åˆ¶ç”Ÿæˆå›¾åƒçš„è£å‰ªæ–¹å¼
- en: introduces a two-stage model process; the *base* model (can also be run as a
    standalone model) generates an image as an input to the *refiner* model which
    adds additional high-quality details
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¨¡åž‹è¿‡ç¨‹ï¼›*åŸºç¡€*æ¨¡åž‹ï¼ˆä¹Ÿå¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡åž‹è¿è¡Œï¼‰ç”Ÿæˆä¸€ä¸ªå›¾åƒä½œä¸º*ç²¾ç‚¼å™¨*æ¨¡åž‹çš„è¾“å…¥ï¼ŒåŽè€…æ·»åŠ é¢å¤–çš„é«˜è´¨é‡ç»†èŠ‚
- en: This guide will show you how to use SDXL for text-to-image, image-to-image,
    and inpainting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨SDXLè¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®è¡¥ã€‚
- en: 'Before you begin, make sure you have the following libraries installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We recommend installing the [invisible-watermark](https://pypi.org/project/invisible-watermark/)
    library to help identify images that are generated. If the invisible-watermark
    library is installed, it is used by default. To disable the watermarker:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®å®‰è£…[invisible-watermark](https://pypi.org/project/invisible-watermark/)åº“æ¥å¸®åŠ©è¯†åˆ«ç”Ÿæˆçš„å›¾åƒã€‚å¦‚æžœå®‰è£…äº†invisible-watermarkåº“ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šä½¿ç”¨å®ƒã€‚è¦ç¦ç”¨æ°´å°ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load model checkpoints
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡åž‹æ£€æŸ¥ç‚¹
- en: 'Model weights may be stored in separate subfolders on the Hub or locally, in
    which case, you should use the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)
    method:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡åž‹æƒé‡å¯èƒ½å­˜å‚¨åœ¨Hubæˆ–æœ¬åœ°çš„å•ç‹¬å­æ–‡ä»¶å¤¹ä¸­ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨[from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)æ–¹æ³•ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can also use the [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`)
    from the Hub or locally:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)æ–¹æ³•æ¥åŠ è½½å­˜å‚¨åœ¨å•ä¸ªæ–‡ä»¶æ ¼å¼ï¼ˆ`.ckpt`æˆ–`.safetensors`ï¼‰ä¸­çš„æ¨¡åž‹æ£€æŸ¥ç‚¹ï¼Œä»ŽHubæˆ–æœ¬åœ°ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Text-to-image
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒ
- en: For text-to-image, pass a text prompt. By default, SDXL generates a 1024x1024
    image for the best results. You can try setting the `height` and `width` parameters
    to 768x768 or 512x512, but anything below 512x512 is not likely to work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºŽæ–‡æœ¬åˆ°å›¾åƒï¼Œä¼ é€’ä¸€ä¸ªæ–‡æœ¬æç¤ºã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDXLç”Ÿæˆ1024x1024å›¾åƒä»¥èŽ·å¾—æœ€ä½³ç»“æžœã€‚æ‚¨å¯ä»¥å°è¯•å°†`height`å’Œ`width`å‚æ•°è®¾ç½®ä¸º768x768æˆ–512x512ï¼Œä½†ä½ŽäºŽ512x512çš„å°ºå¯¸ä¸å¤ªå¯èƒ½æœ‰æ•ˆã€‚
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![generated image of an astronaut in a jungle](../Images/47eb848e4e09fd8ca2c920b1f98d089d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ç”Ÿæˆçš„å›¾åƒï¼Œä¸€ä¸ªå®‡èˆªå‘˜åœ¨ä¸›æž—ä¸­](../Images/47eb848e4e09fd8ca2c920b1f98d089d.png)'
- en: Image-to-image
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒ
- en: 'For image-to-image, SDXL works especially well with image sizes between 768x768
    and 1024x1024\. Pass an initial image, and a text prompt to condition the image
    with:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºŽå›¾åƒåˆ°å›¾åƒï¼ŒSDXLåœ¨768x768å’Œ1024x1024ä¹‹é—´çš„å›¾åƒå°ºå¯¸ä¸Šè¡¨çŽ°ç‰¹åˆ«å¥½ã€‚ä¼ é€’ä¸€ä¸ªåˆå§‹å›¾åƒå’Œä¸€ä¸ªæ–‡æœ¬æç¤ºæ¥è°ƒæ•´å›¾åƒï¼š
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![generated image of a dog catching a frisbee in a jungle](../Images/55a0d39eaf8bd9f5f77b3d034b991391.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![ç”Ÿæˆçš„å›¾åƒï¼Œä¸€åªç‹—åœ¨ä¸›æž—ä¸­æŽ¥ä½é£žç›˜](../Images/55a0d39eaf8bd9f5f77b3d034b991391.png)'
- en: Inpainting
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿®è¡¥
- en: For inpainting, youâ€™ll need the original image and a mask of what you want to
    replace in the original image. Create a prompt to describe what you want to replace
    the masked area with.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºŽä¿®è¡¥ï¼Œæ‚¨å°†éœ€è¦åŽŸå§‹å›¾åƒå’Œè¦æ›¿æ¢åŽŸå§‹å›¾åƒä¸­å†…å®¹çš„è’™ç‰ˆã€‚åˆ›å»ºä¸€ä¸ªæç¤ºæ¥æè¿°æ‚¨æƒ³ç”¨è’™ç‰ˆåŒºåŸŸæ›¿æ¢çš„å†…å®¹ã€‚
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![generated image of a deep sea diver in a jungle](../Images/1f8afc98a8a746ad39b81d94dcd646d0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![ç”Ÿæˆçš„å›¾åƒï¼Œä¸€ä¸ªæ·±æµ·æ½œæ°´å‘˜åœ¨ä¸›æž—ä¸­](../Images/1f8afc98a8a746ad39b81d94dcd646d0.png)'
- en: Refine image quality
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æé«˜å›¾åƒè´¨é‡
- en: 'SDXL includes a [refiner model](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)
    specialized in denoising low-noise stage images to generate higher-quality images
    from the base model. There are two ways to use the refiner:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SDXLåŒ…æ‹¬ä¸€ä¸ª[ç²¾ç‚¼å™¨æ¨¡åž‹](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)ï¼Œä¸“é—¨ç”¨äºŽåŽ»å™ªä½Žå™ªå£°é˜¶æ®µå›¾åƒï¼Œä»¥ä»ŽåŸºç¡€æ¨¡åž‹ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚æœ‰ä¸¤ç§ä½¿ç”¨ç²¾ç‚¼å™¨çš„æ–¹æ³•ï¼š
- en: use the base and refiner models together to produce a refined image
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸºç¡€å’Œç²¾ç‚¼å™¨æ¨¡åž‹ä¸€èµ·ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼å›¾åƒ
- en: use the base model to produce an image, and subsequently use the refiner model
    to add more details to the image (this is how SDXL was originally trained)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸºç¡€æ¨¡åž‹ç”Ÿæˆä¸€ä¸ªå›¾åƒï¼Œç„¶åŽä½¿ç”¨ç²¾ç‚¼å™¨æ¨¡åž‹ä¸ºå›¾åƒæ·»åŠ æ›´å¤šç»†èŠ‚ï¼ˆè¿™æ˜¯SDXLæœ€åˆè®­ç»ƒçš„æ–¹å¼ï¼‰
- en: Base + refiner model
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºç¡€+ç²¾ç‚¼å™¨æ¨¡åž‹
- en: When you use the base and refiner model together to generate an image, this
    is known as an [*ensemble of expert denoisers*](https://research.nvidia.com/labs/dir/eDiff-I/).
    The ensemble of expert denoisers approach requires fewer overall denoising steps
    versus passing the base modelâ€™s output to the refiner model, so it should be significantly
    faster to run. However, you wonâ€™t be able to inspect the base modelâ€™s output because
    it still contains a large amount of noise.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨å°†åŸºç¡€æ¨¡åž‹å’Œç²¾ç‚¼å™¨æ¨¡åž‹ä¸€èµ·ä½¿ç”¨ç”Ÿæˆå›¾åƒæ—¶ï¼Œè¿™è¢«ç§°ä¸º[*ä¸“å®¶åŽ»å™ªå™¨é›†åˆ*](https://research.nvidia.com/labs/dir/eDiff-I/)ã€‚ä¸“å®¶åŽ»å™ªå™¨é›†åˆæ–¹æ³•éœ€è¦è¾ƒå°‘çš„æ•´ä½“åŽ»å™ªæ­¥éª¤ï¼Œä¸Žå°†åŸºç¡€æ¨¡åž‹çš„è¾“å‡ºä¼ é€’ç»™ç²¾ç‚¼å™¨æ¨¡åž‹ç›¸æ¯”ï¼Œè¿è¡Œé€Ÿåº¦åº”è¯¥æ›´å¿«ã€‚ä½†æ˜¯ï¼Œæ‚¨å°†æ— æ³•æ£€æŸ¥åŸºç¡€æ¨¡åž‹çš„è¾“å‡ºï¼Œå› ä¸ºå®ƒä»ç„¶åŒ…å«å¤§é‡å™ªéŸ³ã€‚
- en: 'As an ensemble of expert denoisers, the base model serves as the expert during
    the high-noise diffusion stage and the refiner model serves as the expert during
    the low-noise diffusion stage. Load the base and refiner model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸“å®¶åŽ»å™ªå™¨çš„é›†åˆï¼ŒåŸºç¡€æ¨¡åž‹åœ¨é«˜å™ªå£°æ‰©æ•£é˜¶æ®µå……å½“ä¸“å®¶ï¼Œç²¾ç‚¼å™¨æ¨¡åž‹åœ¨ä½Žå™ªå£°æ‰©æ•£é˜¶æ®µå……å½“ä¸“å®¶ã€‚åŠ è½½åŸºç¡€å’Œç²¾ç‚¼å™¨æ¨¡åž‹ï¼š
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To use this approach, you need to define the number of timesteps for each model
    to run through their respective stages. For the base model, this is controlled
    by the [`denoising_end`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.denoising_end)
    parameter and for the refiner model, it is controlled by the [`denoising_start`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline.__call__.denoising_start)
    parameter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨éœ€è¦ä¸ºæ¯ä¸ªæ¨¡åž‹å®šä¹‰é€šè¿‡å„è‡ªé˜¶æ®µè¿è¡Œçš„æ—¶é—´æ­¥æ•°ã€‚å¯¹äºŽåŸºç¡€æ¨¡åž‹ï¼Œè¿™ç”±[`denoising_end`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.denoising_end)å‚æ•°æŽ§åˆ¶ï¼Œå¯¹äºŽç²¾ç‚¼æ¨¡åž‹ï¼Œç”±[`denoising_start`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline.__call__.denoising_start)å‚æ•°æŽ§åˆ¶ã€‚
- en: The `denoising_end` and `denoising_start` parameters should be a float between
    0 and 1\. These parameters are represented as a proportion of discrete timesteps
    as defined by the scheduler. If youâ€™re also using the `strength` parameter, itâ€™ll
    be ignored because the number of denoising steps is determined by the discrete
    timesteps the model is trained on and the declared fractional cutoff.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`denoising_end`å’Œ`denoising_start`å‚æ•°åº”è¯¥æ˜¯0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ã€‚è¿™äº›å‚æ•°è¡¨ç¤ºä¸ºç¦»æ•£æ—¶é—´æ­¥æ•°çš„æ¯”ä¾‹ï¼Œç”±è°ƒåº¦å™¨å®šä¹‰ã€‚å¦‚æžœæ‚¨è¿˜ä½¿ç”¨`strength`å‚æ•°ï¼Œå®ƒå°†è¢«å¿½ç•¥ï¼Œå› ä¸ºåŽ»å™ªæ­¥æ•°ç”±æ¨¡åž‹è®­ç»ƒçš„ç¦»æ•£æ—¶é—´æ­¥æ•°å’Œå£°æ˜Žçš„åˆ†æ•°æˆªæ­¢ç¡®å®šã€‚'
- en: Letâ€™s set `denoising_end=0.8` so the base model performs the first 80% of denoising
    the **high-noise** timesteps and set `denoising_start=0.8` so the refiner model
    performs the last 20% of denoising the **low-noise** timesteps. The base model
    output should be in **latent** space instead of a PIL image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¾ç½®`denoising_end=0.8`ï¼Œè¿™æ ·åŸºç¡€æ¨¡åž‹å°†æ‰§è¡Œå‰80%çš„åŽ»å™ª**é«˜å™ªå£°**æ—¶é—´æ­¥ï¼Œå¹¶è®¾ç½®`denoising_start=0.8`ï¼Œè¿™æ ·ç²¾ç‚¼æ¨¡åž‹å°†æ‰§è¡Œæœ€åŽ20%çš„åŽ»å™ª**ä½Žå™ªå£°**æ—¶é—´æ­¥ã€‚åŸºç¡€æ¨¡åž‹çš„è¾“å‡ºåº”è¯¥æ˜¯**æ½œåœ¨**ç©ºé—´è€Œä¸æ˜¯PILå›¾åƒã€‚
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![generated image of a lion on a rock at night](../Images/ef6f87a67e4302925ad70cad452060f0.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å¤œæ™šå²©çŸ³ä¸Šç‹®å­çš„ç”Ÿæˆå›¾åƒ](../Images/ef6f87a67e4302925ad70cad452060f0.png)'
- en: default base model
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤åŸºç¡€æ¨¡åž‹
- en: '![generated image of a lion on a rock at night in higher quality](../Images/33eab9fdb93d52d1ea5ac9c0abb385b9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![å¤œæ™šå²©çŸ³ä¸Šç‹®å­çš„é«˜è´¨é‡ç”Ÿæˆå›¾åƒ](../Images/33eab9fdb93d52d1ea5ac9c0abb385b9.png)'
- en: ensemble of expert denoisers
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“å®¶åŽ»å™ªçš„é›†åˆ
- en: 'The refiner model can also be used for inpainting in the [StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç²¾ç‚¼æ¨¡åž‹ä¹Ÿå¯ä»¥ç”¨äºŽ[StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline)ä¸­çš„ä¿®è¡¥ï¼š
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This ensemble of expert denoisers method works well for all available schedulers!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸“å®¶åŽ»å™ªæ–¹æ³•çš„é›†åˆå¯¹æ‰€æœ‰å¯ç”¨çš„è°ƒåº¦å™¨éƒ½æœ‰æ•ˆï¼
- en: Base to refiner model
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºç¡€åˆ°ç²¾ç‚¼æ¨¡åž‹
- en: SDXL gets a boost in image quality by using the refiner model to add additional
    high-quality details to the fully-denoised image from the base model, in an image-to-image
    setting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨å›¾åƒåˆ°å›¾åƒè®¾ç½®ä¸­ä½¿ç”¨ç²¾ç‚¼æ¨¡åž‹å‘åŸºç¡€æ¨¡åž‹çš„å®Œå…¨åŽ»å™ªå›¾åƒæ·»åŠ é¢å¤–çš„é«˜è´¨é‡ç»†èŠ‚ï¼ŒSDXLåœ¨å›¾åƒè´¨é‡ä¸Šå¾—åˆ°æå‡ã€‚
- en: 'Load the base and refiner models:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½åŸºç¡€å’Œç²¾ç‚¼æ¨¡åž‹ï¼š
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Generate an image from the base model, and set the model output to **latent**
    space:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ŽåŸºç¡€æ¨¡åž‹ç”Ÿæˆå›¾åƒï¼Œå¹¶å°†æ¨¡åž‹è¾“å‡ºè®¾ç½®ä¸º**æ½œåœ¨**ç©ºé—´ï¼š
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Pass the generated image to the refiner model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç”Ÿæˆçš„å›¾åƒä¼ é€’ç»™ç²¾ç‚¼æ¨¡åž‹ï¼š
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![generated image of an astronaut riding a green horse on Mars](../Images/80df2aefcda4915968249be118157434.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![ç”Ÿæˆçš„ç«æ˜Ÿä¸Šéª‘ç»¿é©¬çš„å®‡èˆªå‘˜å›¾åƒ](../Images/80df2aefcda4915968249be118157434.png)'
- en: base model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡åž‹
- en: '![higher quality generated image of an astronaut riding a green horse on Mars](../Images/e69b90229b2f9360e7773d6580ae6cf1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![ç«æ˜Ÿä¸Šéª‘ç»¿é©¬çš„å®‡èˆªå‘˜çš„é«˜è´¨é‡ç”Ÿæˆå›¾åƒ](../Images/e69b90229b2f9360e7773d6580ae6cf1.png)'
- en: base model + refiner model
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡åž‹ + ç²¾ç‚¼æ¨¡åž‹
- en: For inpainting, load the base and the refiner model in the [StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline),
    remove the `denoising_end` and `denoising_start` parameters, and choose a smaller
    number of inference steps for the refiner.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºŽä¿®è¡¥ï¼ŒåŠ è½½åŸºç¡€å’Œç²¾ç‚¼æ¨¡åž‹åœ¨[StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline)ä¸­ï¼Œåˆ é™¤`denoising_end`å’Œ`denoising_start`å‚æ•°ï¼Œå¹¶ä¸ºç²¾ç‚¼å™¨é€‰æ‹©è¾ƒå°‘çš„æŽ¨ç†æ­¥éª¤ã€‚
- en: Micro-conditioning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¾®è°ƒèŠ‚
- en: SDXL training involves several additional conditioning techniques, which are
    referred to as *micro-conditioning*. These include original image size, target
    image size, and cropping parameters. The micro-conditionings can be used at inference
    time to create high-quality, centered images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: SDXLè®­ç»ƒæ¶‰åŠå‡ ç§é¢å¤–çš„è°ƒèŠ‚æŠ€æœ¯ï¼Œè¢«ç§°ä¸º*å¾®è°ƒèŠ‚*ã€‚è¿™äº›åŒ…æ‹¬åŽŸå§‹å›¾åƒå°ºå¯¸ã€ç›®æ ‡å›¾åƒå°ºå¯¸å’Œè£å‰ªå‚æ•°ã€‚è¿™äº›å¾®è°ƒèŠ‚å¯ä»¥åœ¨æŽ¨ç†æ—¶ç”¨äºŽåˆ›å»ºé«˜è´¨é‡ã€å±…ä¸­çš„å›¾åƒã€‚
- en: You can use both micro-conditioning and negative micro-conditioning parameters
    thanks to classifier-free guidance. They are available in the [StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline),
    [StableDiffusionXLImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline),
    [StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline),
    and [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºŽæ— éœ€åˆ†ç±»å™¨çš„æŒ‡å¯¼ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¾®è°ƒèŠ‚å’Œè´Ÿå¾®è°ƒèŠ‚å‚æ•°ã€‚å®ƒä»¬åœ¨[StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)ã€[StableDiffusionXLImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline)ã€[StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline)å’Œ[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ä¸­éƒ½å¯ç”¨ã€‚
- en: Size conditioning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°ºå¯¸è°ƒèŠ‚
- en: 'There are two types of size conditioning:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§ç±»åž‹çš„å°ºå¯¸è°ƒèŠ‚ï¼š
- en: '[`original_size`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.original_size)
    conditioning comes from upscaled images in the training batch (because it would
    be wasteful to discard the smaller images which make up almost 40% of the total
    training data). This way, SDXL learns that upscaling artifacts are not supposed
    to be present in high-resolution images. During inference, you can use `original_size`
    to indicate the original image resolution. Using the default value of `(1024,
    1024)` produces higher-quality images that resemble the 1024x1024 images in the
    dataset. If you choose to use a lower resolution, such as `(256, 256)`, the model
    still generates 1024x1024 images, but theyâ€™ll look like the low resolution images
    (simpler patterns, blurring) in the dataset.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`target_size`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.target_size)
    conditioning comes from finetuning SDXL to support different image aspect ratios.
    During inference, if you use the default value of `(1024, 1024)`, youâ€™ll get an
    image that resembles the composition of square images in the dataset. We recommend
    using the same value for `target_size` and `original_size`, but feel free to experiment
    with other options!'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ðŸ¤— Diffusers also lets you specify negative conditions about an imageâ€™s size
    to steer generation away from certain image resolutions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/eb8daaf025f019bf5475af302cf825d4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Images negatively conditioned on image resolutions of (128, 128), (256, 256),
    and (512, 512).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Crop conditioning
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Images generated by previous Stable Diffusion models may sometimes appear to
    be cropped. This is because images are actually cropped during training so that
    all the images in a batch have the same size. By conditioning on crop coordinates,
    SDXL *learns* that no cropping - coordinates `(0, 0)` - usually correlates with
    centered subjects and complete faces (this is the default value in ðŸ¤— Diffusers).
    You can experiment with different coordinates if you want to generate off-centered
    compositions!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![generated image of an astronaut in a jungle, slightly cropped](../Images/f09d2bde8c0bb94924d692c1760dfca3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'You can also specify negative cropping coordinates to steer generation away
    from certain cropping parameters:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Use a different prompt for each text-encoder
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SDXL uses two text-encoders, so it is possible to pass a different prompt to
    each text-encoder, which can [improve quality](https://github.com/huggingface/diffusers/issues/4004#issuecomment-1627764201).
    Pass your original prompt to `prompt` and the second prompt to `prompt_2` (use
    `negative_prompt` and `negative_prompt_2` if youâ€™re using negative prompts):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![generated image of an astronaut in a jungle in the style of a van gogh painting](../Images/3bd090b79f879a79477c88a713c15375.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: The dual text-encoders also support textual inversion embeddings that need to
    be loaded separately as explained in the [SDXL textual inversion](textual_inversion_inference#stable-diffusion-xl)
    section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Optimizations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SDXL is a large model, and you may need to optimize memory to get it to run
    on your hardware. Here are some tips to save memory and speed up inference.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Offload the model to the CPU with [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    for out-of-memory errors:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use `torch.compile` for ~20% speed-up (you need `torch>=2.0`):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Enable [xFormers](../optimization/xformers) to run SDXL if `torch<2.0`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Other resources
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If youâ€™re interested in experimenting with a minimal version of the [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    used in SDXL, take a look at the [minSDXL](https://github.com/cloneofsimo/minSDXL)
    implementation which is written in PyTorch and directly compatible with ðŸ¤— Diffusers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æžœæ‚¨å¯¹åœ¨SDXLä¸­ä½¿ç”¨çš„[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)çš„æœ€å°ç‰ˆæœ¬è¿›è¡Œå®žéªŒæ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹åœ¨PyTorchä¸­ç¼–å†™çš„å¹¶ä¸ŽðŸ¤—
    Diffusersç›´æŽ¥å…¼å®¹çš„[minSDXL](https://github.com/cloneofsimo/minSDXL)å®žçŽ°ã€‚
