- en: Stable Diffusion XL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定扩散XL
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/using-diffusers/sdxl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/using-diffusers/sdxl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Stable Diffusion XL](https://huggingface.co/papers/2307.01952) (SDXL) is a
    powerful text-to-image generation model that iterates on the previous Stable Diffusion
    models in three key ways:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[稳定扩散XL](https://huggingface.co/papers/2307.01952)（SDXL）是一个强大的文本到图像生成模型，通过三种关键方式改进了先前的稳定扩散模型：'
- en: the UNet is 3x larger and SDXL combines a second text encoder (OpenCLIP ViT-bigG/14)
    with the original text encoder to significantly increase the number of parameters
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UNet是3倍大，SDXL将第二个文本编码器（OpenCLIP ViT-bigG/14）与原始文本编码器结合起来，显著增加参数数量
- en: introduces size and crop-conditioning to preserve training data from being discarded
    and gain more control over how a generated image should be cropped
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入尺寸和裁剪条件，以保留训练数据不被丢弃，并更好地控制生成图像的裁剪方式
- en: introduces a two-stage model process; the *base* model (can also be run as a
    standalone model) generates an image as an input to the *refiner* model which
    adds additional high-quality details
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入了一个两阶段模型过程；*基础*模型（也可以作为独立模型运行）生成一个图像作为*精炼器*模型的输入，后者添加额外的高质量细节
- en: This guide will show you how to use SDXL for text-to-image, image-to-image,
    and inpainting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何使用SDXL进行文本到图像、图像到图像和修补。
- en: 'Before you begin, make sure you have the following libraries installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保已安装以下库：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We recommend installing the [invisible-watermark](https://pypi.org/project/invisible-watermark/)
    library to help identify images that are generated. If the invisible-watermark
    library is installed, it is used by default. To disable the watermarker:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议安装[invisible-watermark](https://pypi.org/project/invisible-watermark/)库来帮助识别生成的图像。如果安装了invisible-watermark库，默认情况下会使用它。要禁用水印：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load model checkpoints
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型检查点
- en: 'Model weights may be stored in separate subfolders on the Hub or locally, in
    which case, you should use the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)
    method:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型权重可能存储在Hub或本地的单独子文件夹中，这种情况下，您应该使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)方法：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can also use the [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`)
    from the Hub or locally:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)方法来加载存储在单个文件格式（`.ckpt`或`.safetensors`）中的模型检查点，从Hub或本地：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Text-to-image
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像
- en: For text-to-image, pass a text prompt. By default, SDXL generates a 1024x1024
    image for the best results. You can try setting the `height` and `width` parameters
    to 768x768 or 512x512, but anything below 512x512 is not likely to work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本到图像，传递一个文本提示。默认情况下，SDXL生成1024x1024图像以获得最佳结果。您可以尝试将`height`和`width`参数设置为768x768或512x512，但低于512x512的尺寸不太可能有效。
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![generated image of an astronaut in a jungle](../Images/47eb848e4e09fd8ca2c920b1f98d089d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![生成的图像，一个宇航员在丛林中](../Images/47eb848e4e09fd8ca2c920b1f98d089d.png)'
- en: Image-to-image
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像到图像
- en: 'For image-to-image, SDXL works especially well with image sizes between 768x768
    and 1024x1024\. Pass an initial image, and a text prompt to condition the image
    with:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像到图像，SDXL在768x768和1024x1024之间的图像尺寸上表现特别好。传递一个初始图像和一个文本提示来调整图像：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![generated image of a dog catching a frisbee in a jungle](../Images/55a0d39eaf8bd9f5f77b3d034b991391.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![生成的图像，一只狗在丛林中接住飞盘](../Images/55a0d39eaf8bd9f5f77b3d034b991391.png)'
- en: Inpainting
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修补
- en: For inpainting, you’ll need the original image and a mask of what you want to
    replace in the original image. Create a prompt to describe what you want to replace
    the masked area with.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于修补，您将需要原始图像和要替换原始图像中内容的蒙版。创建一个提示来描述您想用蒙版区域替换的内容。
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![generated image of a deep sea diver in a jungle](../Images/1f8afc98a8a746ad39b81d94dcd646d0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![生成的图像，一个深海潜水员在丛林中](../Images/1f8afc98a8a746ad39b81d94dcd646d0.png)'
- en: Refine image quality
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高图像质量
- en: 'SDXL includes a [refiner model](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)
    specialized in denoising low-noise stage images to generate higher-quality images
    from the base model. There are two ways to use the refiner:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL包括一个[精炼器模型](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)，专门用于去噪低噪声阶段图像，以从基础模型生成更高质量的图像。有两种使用精炼器的方法：
- en: use the base and refiner models together to produce a refined image
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基础和精炼器模型一起生成一个精炼图像
- en: use the base model to produce an image, and subsequently use the refiner model
    to add more details to the image (this is how SDXL was originally trained)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基础模型生成一个图像，然后使用精炼器模型为图像添加更多细节（这是SDXL最初训练的方式）
- en: Base + refiner model
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础+精炼器模型
- en: When you use the base and refiner model together to generate an image, this
    is known as an [*ensemble of expert denoisers*](https://research.nvidia.com/labs/dir/eDiff-I/).
    The ensemble of expert denoisers approach requires fewer overall denoising steps
    versus passing the base model’s output to the refiner model, so it should be significantly
    faster to run. However, you won’t be able to inspect the base model’s output because
    it still contains a large amount of noise.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将基础模型和精炼器模型一起使用生成图像时，这被称为[*专家去噪器集合*](https://research.nvidia.com/labs/dir/eDiff-I/)。专家去噪器集合方法需要较少的整体去噪步骤，与将基础模型的输出传递给精炼器模型相比，运行速度应该更快。但是，您将无法检查基础模型的输出，因为它仍然包含大量噪音。
- en: 'As an ensemble of expert denoisers, the base model serves as the expert during
    the high-noise diffusion stage and the refiner model serves as the expert during
    the low-noise diffusion stage. Load the base and refiner model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为专家去噪器的集合，基础模型在高噪声扩散阶段充当专家，精炼器模型在低噪声扩散阶段充当专家。加载基础和精炼器模型：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To use this approach, you need to define the number of timesteps for each model
    to run through their respective stages. For the base model, this is controlled
    by the [`denoising_end`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.denoising_end)
    parameter and for the refiner model, it is controlled by the [`denoising_start`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline.__call__.denoising_start)
    parameter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种方法，您需要为每个模型定义通过各自阶段运行的时间步数。对于基础模型，这由[`denoising_end`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.denoising_end)参数控制，对于精炼模型，由[`denoising_start`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline.__call__.denoising_start)参数控制。
- en: The `denoising_end` and `denoising_start` parameters should be a float between
    0 and 1\. These parameters are represented as a proportion of discrete timesteps
    as defined by the scheduler. If you’re also using the `strength` parameter, it’ll
    be ignored because the number of denoising steps is determined by the discrete
    timesteps the model is trained on and the declared fractional cutoff.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`denoising_end`和`denoising_start`参数应该是0到1之间的浮点数。这些参数表示为离散时间步数的比例，由调度器定义。如果您还使用`strength`参数，它将被忽略，因为去噪步数由模型训练的离散时间步数和声明的分数截止确定。'
- en: Let’s set `denoising_end=0.8` so the base model performs the first 80% of denoising
    the **high-noise** timesteps and set `denoising_start=0.8` so the refiner model
    performs the last 20% of denoising the **low-noise** timesteps. The base model
    output should be in **latent** space instead of a PIL image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置`denoising_end=0.8`，这样基础模型将执行前80%的去噪**高噪声**时间步，并设置`denoising_start=0.8`，这样精炼模型将执行最后20%的去噪**低噪声**时间步。基础模型的输出应该是**潜在**空间而不是PIL图像。
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![generated image of a lion on a rock at night](../Images/ef6f87a67e4302925ad70cad452060f0.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![夜晚岩石上狮子的生成图像](../Images/ef6f87a67e4302925ad70cad452060f0.png)'
- en: default base model
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认基础模型
- en: '![generated image of a lion on a rock at night in higher quality](../Images/33eab9fdb93d52d1ea5ac9c0abb385b9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![夜晚岩石上狮子的高质量生成图像](../Images/33eab9fdb93d52d1ea5ac9c0abb385b9.png)'
- en: ensemble of expert denoisers
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 专家去噪的集合
- en: 'The refiner model can also be used for inpainting in the [StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼模型也可以用于[StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline)中的修补：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This ensemble of expert denoisers method works well for all available schedulers!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个专家去噪方法的集合对所有可用的调度器都有效！
- en: Base to refiner model
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础到精炼模型
- en: SDXL gets a boost in image quality by using the refiner model to add additional
    high-quality details to the fully-denoised image from the base model, in an image-to-image
    setting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在图像到图像设置中使用精炼模型向基础模型的完全去噪图像添加额外的高质量细节，SDXL在图像质量上得到提升。
- en: 'Load the base and refiner models:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 加载基础和精炼模型：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Generate an image from the base model, and set the model output to **latent**
    space:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础模型生成图像，并将模型输出设置为**潜在**空间：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Pass the generated image to the refiner model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的图像传递给精炼模型：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![generated image of an astronaut riding a green horse on Mars](../Images/80df2aefcda4915968249be118157434.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![生成的火星上骑绿马的宇航员图像](../Images/80df2aefcda4915968249be118157434.png)'
- en: base model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型
- en: '![higher quality generated image of an astronaut riding a green horse on Mars](../Images/e69b90229b2f9360e7773d6580ae6cf1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![火星上骑绿马的宇航员的高质量生成图像](../Images/e69b90229b2f9360e7773d6580ae6cf1.png)'
- en: base model + refiner model
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型 + 精炼模型
- en: For inpainting, load the base and the refiner model in the [StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline),
    remove the `denoising_end` and `denoising_start` parameters, and choose a smaller
    number of inference steps for the refiner.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于修补，加载基础和精炼模型在[StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline)中，删除`denoising_end`和`denoising_start`参数，并为精炼器选择较少的推理步骤。
- en: Micro-conditioning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调节
- en: SDXL training involves several additional conditioning techniques, which are
    referred to as *micro-conditioning*. These include original image size, target
    image size, and cropping parameters. The micro-conditionings can be used at inference
    time to create high-quality, centered images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL训练涉及几种额外的调节技术，被称为*微调节*。这些包括原始图像尺寸、目标图像尺寸和裁剪参数。这些微调节可以在推理时用于创建高质量、居中的图像。
- en: You can use both micro-conditioning and negative micro-conditioning parameters
    thanks to classifier-free guidance. They are available in the [StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline),
    [StableDiffusionXLImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline),
    [StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline),
    and [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无需分类器的指导，您可以使用微调节和负微调节参数。它们在[StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)、[StableDiffusionXLImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLImg2ImgPipeline)、[StableDiffusionXLInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLInpaintPipeline)和[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)中都可用。
- en: Size conditioning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尺寸调节
- en: 'There are two types of size conditioning:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的尺寸调节：
- en: '[`original_size`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.original_size)
    conditioning comes from upscaled images in the training batch (because it would
    be wasteful to discard the smaller images which make up almost 40% of the total
    training data). This way, SDXL learns that upscaling artifacts are not supposed
    to be present in high-resolution images. During inference, you can use `original_size`
    to indicate the original image resolution. Using the default value of `(1024,
    1024)` produces higher-quality images that resemble the 1024x1024 images in the
    dataset. If you choose to use a lower resolution, such as `(256, 256)`, the model
    still generates 1024x1024 images, but they’ll look like the low resolution images
    (simpler patterns, blurring) in the dataset.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`target_size`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.__call__.target_size)
    conditioning comes from finetuning SDXL to support different image aspect ratios.
    During inference, if you use the default value of `(1024, 1024)`, you’ll get an
    image that resembles the composition of square images in the dataset. We recommend
    using the same value for `target_size` and `original_size`, but feel free to experiment
    with other options!'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '🤗 Diffusers also lets you specify negative conditions about an image’s size
    to steer generation away from certain image resolutions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/eb8daaf025f019bf5475af302cf825d4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Images negatively conditioned on image resolutions of (128, 128), (256, 256),
    and (512, 512).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Crop conditioning
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Images generated by previous Stable Diffusion models may sometimes appear to
    be cropped. This is because images are actually cropped during training so that
    all the images in a batch have the same size. By conditioning on crop coordinates,
    SDXL *learns* that no cropping - coordinates `(0, 0)` - usually correlates with
    centered subjects and complete faces (this is the default value in 🤗 Diffusers).
    You can experiment with different coordinates if you want to generate off-centered
    compositions!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![generated image of an astronaut in a jungle, slightly cropped](../Images/f09d2bde8c0bb94924d692c1760dfca3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'You can also specify negative cropping coordinates to steer generation away
    from certain cropping parameters:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Use a different prompt for each text-encoder
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SDXL uses two text-encoders, so it is possible to pass a different prompt to
    each text-encoder, which can [improve quality](https://github.com/huggingface/diffusers/issues/4004#issuecomment-1627764201).
    Pass your original prompt to `prompt` and the second prompt to `prompt_2` (use
    `negative_prompt` and `negative_prompt_2` if you’re using negative prompts):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![generated image of an astronaut in a jungle in the style of a van gogh painting](../Images/3bd090b79f879a79477c88a713c15375.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: The dual text-encoders also support textual inversion embeddings that need to
    be loaded separately as explained in the [SDXL textual inversion](textual_inversion_inference#stable-diffusion-xl)
    section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Optimizations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SDXL is a large model, and you may need to optimize memory to get it to run
    on your hardware. Here are some tips to save memory and speed up inference.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Offload the model to the CPU with [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    for out-of-memory errors:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use `torch.compile` for ~20% speed-up (you need `torch>=2.0`):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Enable [xFormers](../optimization/xformers) to run SDXL if `torch<2.0`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Other resources
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re interested in experimenting with a minimal version of the [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    used in SDXL, take a look at the [minSDXL](https://github.com/cloneofsimo/minSDXL)
    implementation which is written in PyTorch and directly compatible with 🤗 Diffusers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对在SDXL中使用的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)的最小版本进行实验感兴趣，请查看在PyTorch中编写的并与🤗
    Diffusers直接兼容的[minSDXL](https://github.com/cloneofsimo/minSDXL)实现。
