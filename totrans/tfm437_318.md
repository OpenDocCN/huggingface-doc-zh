# SpeechT5

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5)

## 概述

SpeechT5 模型是由 Junyi Ao、Rui Wang、Long Zhou、Chengyi Wang、Shuo Ren、Yu Wu、Shujie Liu、Tom Ko、Qing Li、Yu Zhang、Zhihua Wei、Yao Qian、Jinyu Li、Furu Wei 在 [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) 中提出的。

论文摘要如下：

受 T5（文本到文本转换变压器）在预训练自然语言处理模型中的成功启发，我们提出了一个统一的 SpeechT5 框架，探索编码器-解码器的预训练，用于自监督语音/文本表示学习。SpeechT5 框架由一个共享的编码器-解码器网络和六个模态特定的（语音/文本）预/后网络组成。通过预网络对输入的语音/文本进行预处理后，共享的编码器-解码器网络对序列到序列的转换进行建模，然后后网络根据解码器的输出在语音/文本模态中生成输出。利用大规模未标记的语音和文本数据，我们预训练 SpeechT5 来学习一个统一的模态表示，希望提高对语音和文本的建模能力。为了将文本和语音信息对齐到这个统一的语义空间中，我们提出了一种跨模态向量量化方法，随机混合语音/文本状态和潜在单元作为编码器和解码器之间的接口。广泛的评估显示了所提出的 SpeechT5 框架在各种口语处理任务上的优越性，包括自动语音识别、语音合成、语音翻译、语音转换、语音增强和说话人识别。

该模型由 [Matthijs](https://huggingface.co/Matthijs) 贡献。原始代码可以在 [这里](https://github.com/microsoft/SpeechT5) 找到。

## SpeechT5Config

### `class transformers.SpeechT5Config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/configuration_speecht5.py#L37)

```py
( vocab_size = 81 hidden_size = 768 encoder_layers = 12 encoder_attention_heads = 12 encoder_ffn_dim = 3072 encoder_layerdrop = 0.1 decoder_layers = 6 decoder_ffn_dim = 3072 decoder_attention_heads = 12 decoder_layerdrop = 0.1 hidden_act = 'gelu' positional_dropout = 0.1 hidden_dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 scale_embedding = False feat_extract_norm = 'group' feat_proj_dropout = 0.0 feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 decoder_start_token_id = 2 num_mel_bins = 80 speech_decoder_prenet_layers = 2 speech_decoder_prenet_units = 256 speech_decoder_prenet_dropout = 0.5 speaker_embedding_dim = 512 speech_decoder_postnet_layers = 5 speech_decoder_postnet_units = 256 speech_decoder_postnet_kernel = 5 speech_decoder_postnet_dropout = 0.5 reduction_factor = 2 max_speech_positions = 4000 max_text_positions = 450 encoder_max_relative_position = 160 use_guided_attention_loss = True guided_attention_loss_num_heads = 2 guided_attention_loss_sigma = 0.4 guided_attention_loss_scale = 10.0 use_cache = True is_encoder_decoder = True **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为 81）— SpeechT5 模型的词汇量。定义了可以由传递给 SpeechT5Model 的 `inputs_ids` 表示的不同标记数量。

+   `hidden_size`（`int`，*可选*，默认为 768）— 编码器层和池化层的维度。

+   `encoder_layers`（`int`，*可选*，默认为 12）— Transformer 编码器中的隐藏层数。

+   `encoder_attention_heads`（`int`，*可选*，默认为 12）— Transformer 编码器中每个注意力层的注意力头数。

+   `encoder_ffn_dim`（`int`，*可选*，默认为 3072）— Transformer 编码器中“中间”（即前馈）层的维度。

+   `encoder_layerdrop`（`float`，*可选*，默认为 0.1）— 编码器的 LayerDrop 概率。有关更多详细信息，请参阅 [LayerDrop 论文](https://arxiv.org/abs/1909.11556)。

+   `decoder_layers`（`int`，*可选*，默认为 6）— Transformer 解码器中的隐藏层数。

+   `decoder_attention_heads`（`int`，*可选*，默认为 12）— Transformer 解码器中每个注意力层的注意力头数。

+   `decoder_ffn_dim`（`int`，*可选*，默认为 3072）— Transformer 解码器中“中间”（通常称为前馈）层的维度。

+   `decoder_layerdrop`（`float`，*可选*，默认为 0.1）— 解码器的 LayerDrop 概率。有关更多详细信息，请参阅 [LayerDrop 论文](https://arxiv.org/abs/1909.11556)。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `positional_dropout` (`float`, *optional*, defaults to 0.1) — 文本位置编码层的 dropout 概率。

+   `hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout 比率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) — 全连接层内部激活的 dropout 比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的 epsilon。

+   `scale_embedding` (`bool`, *optional*, defaults to `False`) — 通过 sqrt(d_model)缩放嵌入。

+   `feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 应用于语音编码器预网络中 1D 卷积层的规范化方式。可以选择`"group"`表示仅对第一个 1D 卷积层进行组归一化，或者选择`"layer"`表示对所有 1D 卷积层进行层归一化。

+   `feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 语音编码器预网络输出的 dropout 概率。

+   `feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- 特征提取器中 1D 卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`“gelu”`、`“relu”`、`“selu”`和`“gelu_new”`。

+   `conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 512, 512, 512)`) — 一个整数元组，定义语音编码器预网络中每个 1D 卷积层的输入和输出通道数。*conv_dim*的长度定义了 1D 卷积层的数量。

+   `conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2, 2, 2, 2, 2, 2)`) — 一个整数元组，定义语音编码器预网络中每个 1D 卷积层的步幅。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。

+   `conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3, 3, 3, 3, 3, 3)`) — 一个整数元组，定义语音编码器预网络中每个 1D 卷积层的内核大小。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。

+   `conv_bias` (`bool`, *optional*, defaults to `False`) — 1D 卷积层是否具有偏置。

+   `num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了 1D 卷积位置嵌入层的内核大小。

+   `num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D 卷积位置嵌入层的组数。

+   `apply_spec_augment` (`bool`, *optional*, defaults to `True`) — 是否对语音编码器预网络的输出应用*SpecAugment*数据增强。有关详细信息，请参阅[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)。

+   `mask_time_prob` (`float`, *optional*, 默认为 0.05) — 沿时间轴的所有特征向量中将被掩盖的百分比（介于 0 和 1 之间）。掩盖过程在该轴上生成”mask_time_prob*len(time_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为掩盖的向量跨度的起始的概率推理，*mask_time_prob*应该是`prob_vector_start*mask_time_length`。请注意，重叠可能会降低实际掩盖向量的百分比。只有在`apply_spec_augment`为 True 时才相关。

+   `mask_time_length` (`int`, *optional*, 默认为 10) — 沿时间轴的向量跨度长度。

+   `mask_time_min_masks` (`int`, *optional*, 默认为 2) — 沿时间轴生成的长度为`mask_feature_length`的掩码的最小数量，每个时间步，与`mask_feature_prob`无关。只有在”mask_time_prob*len(time_axis)/mask_time_length < mask_time_min_masks”时才相关。

+   `mask_feature_prob` (`float`, *optional*, 默认为 0.0) — 沿特征轴的所有特征向量中将被掩盖的百分比（介于 0 和 1 之间）。掩盖过程在该轴上生成”mask_feature_prob*len(feature_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为掩盖的向量跨度的起始的概率推理，*mask_feature_prob*应该是`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低实际掩盖向量的百分比。只有在`apply_spec_augment`为 True 时才相关。

+   `mask_feature_length` (`int`, *optional*, 默认为 10) — 沿特征轴的向量跨度长度。

+   `mask_feature_min_masks` (`int`, *optional*, 默认为 0) — 沿特征轴生成的长度为`mask_feature_length`的掩码的最小数量，每个时间步，与`mask_feature_prob`无关。只有在”mask_feature_prob*len(feature_axis)/mask_feature_length < mask_feature_min_masks”时才相关。

+   `num_mel_bins` (`int`, *optional*, 默认为 80) — 每个输入特征中使用的梅尔特征数量。由语音解码器预网络使用。应与 SpeechT5Processor 类中使用的值对应。

+   `speech_decoder_prenet_layers` (`int`, *optional*, 默认为 2) — 语音解码器预网络中的层数。

+   `speech_decoder_prenet_units` (`int`, *optional*, 默认为 256) — 语音解码器预网络中层的维度。

+   `speech_decoder_prenet_dropout` (`float`, *optional*, 默认为 0.5) — 语音解码器预网络层的丢失概率。

+   `speaker_embedding_dim` (`int`, *optional*, 默认为 512) — *XVector*嵌入向量的维度。

+   `speech_decoder_postnet_layers` (`int`, *optional*, 默认为 5) — 语音解码器后置网络中的层数。

+   `speech_decoder_postnet_units` (`int`, *optional*, 默认为 256) — 语音解码器后置网络中层的维度。

+   `speech_decoder_postnet_kernel` (`int`, *optional*, 默认为 5) — 语音解码器后置网络中的卷积滤波器通道数。

+   `speech_decoder_postnet_dropout` (`float`, *optional*, 默认为 0.5) — 语音解码器后置网络层的丢失概率。

+   `reduction_factor` (`int`, *optional*, 默认为 2) — 用于语音解码器输入的频谱长度缩减因子。

+   `max_speech_positions` (`int`, *optional*, 默认为 4000) — 该模型可能用于的语音特征的最大序列长度。

+   `max_text_positions` (`int`, *optional*, 默认为 450) — 该模型可能用于的文本特征的最大序列长度。

+   `encoder_max_relative_position` (`int`, *optional*, 默认为 160) — 编码器中相对位置嵌入的最大距离。

+   `use_guided_attention_loss` (`bool`, *optional*, 默认为`True`) — 在训练 TTS 模型时是否应用引导注意力损失。

+   `guided_attention_loss_num_heads` (`int`, *optional*, defaults to 2) — 引导注意力损失将应用的注意力头数。使用-1 将此损失应用于所有注意力头。

+   `guided_attention_loss_sigma` (`float`, *optional*, defaults to 0.4) — 引导注意力损失的标准差。

+   `guided_attention_loss_scale` (`float`, *optional*, defaults to 10.0) — 引导注意力损失的缩放系数（也称为 lambda）。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

这是一个配置类，用于存储 SpeechT5Model 的配置。它用于根据指定的参数实例化一个 SpeechT5 模型，定义模型架构。使用默认值实例化配置将产生类似于 SpeechT5 [microsoft/speecht5_asr](https://huggingface.co/microsoft/speecht5_asr)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import SpeechT5Model, SpeechT5Config

>>> # Initializing a "microsoft/speecht5_asr" style configuration
>>> configuration = SpeechT5Config()

>>> # Initializing a model (with random weights) from the "microsoft/speecht5_asr" style configuration
>>> model = SpeechT5Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## SpeechT5HifiGanConfig

### `class transformers.SpeechT5HifiGanConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/configuration_speecht5.py#L350)

```py
( model_in_dim = 80 sampling_rate = 16000 upsample_initial_channel = 512 upsample_rates = [4, 4, 4, 4] upsample_kernel_sizes = [8, 8, 8, 8] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] initializer_range = 0.01 leaky_relu_slope = 0.1 normalize_before = True **kwargs )
```

参数

+   `model_in_dim` (`int`, *optional*, defaults to 80) — 输入对数梅尔频谱中的频率箱数。

+   `sampling_rate` (`int`, *optional*, defaults to 16000) — 生成输出音频的采样率，以赫兹（Hz）表示。

+   `upsample_initial_channel` (`int`, *optional*, defaults to 512) — 输入通道数到上采样网络。

+   `upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[4, 4, 4, 4]`) — 一个整数元组，定义了上采样网络中每个 1D 卷积层的步幅。*upsample_rates*的长度定义了卷积层的数量，并且必须与*upsample_kernel_sizes*的长度匹配。

+   `upsample_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[8, 8, 8, 8]`) — 一个整数元组，定义了上采样网络中每个 1D 卷积层的内核大小。*upsample_kernel_sizes*的长度定义了卷积层的数量，并且必须与*upsample_rates*的长度匹配。

+   `resblock_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[3, 7, 11]`) — 一个整数元组，定义了多接受域融合（MRF）模块中 1D 卷积层的内核大小。

+   `resblock_dilation_sizes` (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*, defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — 一个嵌套的整数元组，定义了多接受域融合（MRF）模块中扩张的 1D 卷积层的扩张率。

+   `initializer_range` (`float`, *optional*, defaults to 0.01) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `leaky_relu_slope` (`float`, *optional*, defaults to 0.1) — 漏斗 ReLU 激活使用的负斜率的角度。

+   `normalize_before` (`bool`, *optional*, defaults to `True`) — 是否在使用声码器的学习均值和方差进行声码之前对频谱进行归一化。

这是用于存储`SpeechT5HifiGanModel`配置的配置类。它用于根据指定的参数实例化一个 SpeechT5 HiFi-GAN 声码器模型，定义模型架构。使用默认值实例化配置将产生类似于 SpeechT5 [microsoft/speecht5_hifigan](https://huggingface.co/microsoft/speecht5_hifigan)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import SpeechT5HifiGan, SpeechT5HifiGanConfig

>>> # Initializing a "microsoft/speecht5_hifigan" style configuration
>>> configuration = SpeechT5HifiGanConfig()

>>> # Initializing a model (with random weights) from the "microsoft/speecht5_hifigan" style configuration
>>> model = SpeechT5HifiGan(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## SpeechT5Tokenizer

### `class transformers.SpeechT5Tokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/tokenization_speecht5.py#L48)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' normalize = False sp_model_kwargs: Optional = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含实例化分词器所需词汇的[SentencePiece](https://github.com/google/sentencepiece)文件（通常具有*.spm*扩展名）。

+   `bos_token` (`str`, *可选*, 默认为`"<s>"`) — 序列开始标记。

+   `eos_token` (`str`, *可选*, 默认为`"</s>"`) — 序列结束标记。

+   `unk_token` (`str`, *可选*, 默认为`"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为`"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时。

+   `normalize` (`bool`, *可选*, 默认为`False`) — 是否将文本中的数字量转换为其拼写的英文对应项。

+   `sp_model_kwargs` (`dict`, *可选*) — 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece 的 Python 包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`: 启用子词正则化。

    +   `nbest_size`: 一元采样的参数。对于 BPE-Dropout 无效。

        +   `nbest_size = {0,1}`: 不执行采样。

        +   `nbest_size > 1`: 从 nbest_size 结果中进行采样。

        +   `nbest_size < 0`: 假设 nbest_size 为无限，并使用前向过滤和后向采样算法从所有假设（格）中进行采样。

    +   `alpha`: 一元采样的平滑参数，以及 BPE-dropout 的合并操作的丢弃概率。

+   `sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和 ID）的*SentencePiece*处理器。

构建一个 SpeechT5 分词器。基于[SentencePiece](https://github.com/google/sentencepiece)。

此分词器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或批量序列。每个序列可以是一个字符串或一个字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或批量序列。每个序列可以是一个字符串或一个字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。

+   `add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding` (`bool`, `str` 或 PaddingStrategy, *可选*, 默认为 `False`) — 激活和控制填充。接受以下值:

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False` 或 `'do_not_pad'`（默认）: 无填充（即，可以输出具有不同长度序列的批次）。

+   `truncation` (`bool`, `str` 或 TruncationStrategy, *可选*, 默认为 `False`) — 激活和控制截断。接受以下值:

    +   `True` 或 `'longest_first'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则将逐标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则只会截断第一个序列。

    +   `'only_second'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）: 无截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。

+   `max_length` (`int`, *可选*) — 控制截断/填充参数使用的最大长度。

    如果未设置或设置为`None`，则如果截断/填充参数中的一个需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将停用截断/填充到最大长度。

+   `stride` (`int`, *可选*, 默认为 0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`，*可选*，默认为`False`) — 输入是否已经预分词（例如，已经分成单词）。如果设置为`True`，分词器会假定输入已经分成单词（例如，通过在空格上分割），然后进行分词。这对于 NER 或标记分类很有用。

+   `pad_to_multiple_of` (`int`，*可选*) — 如果设置，将填充序列到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors` (`str`或 TensorType，*可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.constant`对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor`对象。

    +   `'np'`：返回 Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`，*可选*） — 是否返回 token 类型 ID。如果保持默认值，将根据特定分词器的默认值返回 token 类型 ID，由`return_outputs`属性定义。

    什么是 token 类型 ID？

+   `return_attention_mask` (`bool`，*可选*) — 是否返回注意力掩码。如果保持默认值，将根据特定分词器的默认值返回注意力掩码，由`return_outputs`属性定义。

    什么是注意力掩码？

+   `return_overflowing_tokens` (`bool`，*可选*，默认为`False`) — 是否返回溢出的标记序列。如果提供一对输入 ID 序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的标记。

+   `return_special_tokens_mask` (`bool`，*可选*，默认为`False`) — 是否返回特殊标记掩码信息。

+   `return_offsets_mapping` (`bool`，*可选*，默认为`False`) — 是否返回每个标记的`(char_start, char_end)`。

    这仅适用于继承自 PreTrainedTokenizerFast 的快速分词器，如果使用 Python 的分词器，此方法将引发`NotImplementedError`。

+   `return_length` (`bool`，*可选*，默认为`False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`，*可选*，默认为`True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

BatchEncoding

一个带有以下字段的 BatchEncoding： 

+   `input_ids` — 要提供给模型的标记 ID 列表。

    什么是输入 ID？

+   `token_type_ids` — 要提供给模型的 token 类型 ID 列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    什么是 token 类型 ID？

+   `attention_mask` — 指定哪些标记应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    什么是注意力掩码？

+   `overflowing_tokens` — 溢出标记序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的标记数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由 0 和 1 组成的列表，其中 1 指定添加的特殊标记，0 指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

将一个或多个序列或一个或多个序列对标记化并准备好传递给模型的主要方法。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/tokenization_speecht5.py#L220)

```py
( save_directory: str filename_prefix: Optional = None )
```

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';str
```

参数

+   `token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`) — 标记化输入 id 的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens` (`bool`, *optional*, 默认为`False`) — 是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces` (`bool`, *optional*) — 是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（额外的关键字参数，*optional*） — 将传递给底层模型特定的解码方法。

返回

`str`

解码后的句子。

使用标记器和词汇表将 id 序列转换为字符串，具有删除特殊标记和清理标记化空格的选项。

类似于执行`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';List[str]
```

参数

+   `sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`) — 标记化输入 id 的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens` (`bool`, *optional*, 默认为`False`) — 是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces` (`bool`, *optional*) — 是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（额外的关键字参数，*optional*） — 将传递给底层模型特定的解码方法。

返回

`List[str]`

解码后的句子列表。

通过调用解码将标记 id 的列表转换为字符串列表。

## SpeechT5FeatureExtractor

### `class transformers.SpeechT5FeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/feature_extraction_speecht5.py#L31)

```py
( feature_size: int = 1 sampling_rate: int = 16000 padding_value: float = 0.0 do_normalize: bool = False num_mel_bins: int = 80 hop_length: int = 16 win_length: int = 64 win_function: str = 'hann_window' frame_signal_scale: float = 1.0 fmin: float = 80 fmax: float = 7600 mel_floor: float = 1e-10 reduction_factor: int = 2 return_attention_mask: bool = True **kwargs )
```

参数

+   `feature_size` (`int`, *optional*, 默认为 1) — 提取特征的特征维度。

+   `sampling_rate` (`int`, *optional*, 默认为 16000) — 音频文件应数字化的采样率，以赫兹（Hz）表示。

+   `padding_value` (`float`, *optional*, 默认为 0.0) — 用于填充值的值。

+   `do_normalize` (`bool`, *optional*, 默认为`False`) — 是否对输入进行零均值单位方差归一化。归一化可以帮助显著提高某些模型的性能。

+   `num_mel_bins` (`int`, *optional*, 默认为 80) — 提取的频谱图特征中的 mel 频率箱数。

+   `hop_length` (`int`, *optional*, 默认为 16) — 窗口之间的毫秒数。在许多论文中也称为“shift”。

+   `win_length` (`int`, *optional*, 默认为 64) — 每个窗口的毫秒数。

+   `win_function` (`str`, *optional*, 默认为`"hann_window"`) — 用于窗口化的窗口函数的名称，必须可以通过`torch.{win_function}`访问。

+   `frame_signal_scale` (`float`, *optional*, 默认为 1.0) — 在应用 DFT 之前创建帧时乘以的常数。此参数已弃用。

+   `fmin` (`float`, *optional*, 默认为 80) — Hz 中的最小 mel 频率。

+   `fmax` (`float`, *optional*, 默认为 7600) — Hz 中的最大 mel 频率。

+   `mel_floor` (`float`, *optional*, 默认为 1e-10) — mel 频率银行的最小值。

+   `reduction_factor` (`int`, *optional*, 默认为 2) — 频谱长度缩减因子。此参数已弃用。

+   `return_attention_mask` (`bool`, *可选*, 默认为`True`) — 是否`call`()应该返回`attention_mask`。

构建一个 SpeechT5 特征提取器。

这个类可以通过（可选）将原始语音信号归一化为零均值单位方差，供 SpeechT5 语音编码器 prenet 使用。

这个类还可以从原始语音中提取 log-mel 滤波器组特征，供 SpeechT5 语音解码器 prenet 使用。

这个特征提取器继承自 SequenceFeatureExtractor，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/feature_extraction_speecht5.py#L180)

```py
( audio: Union = None audio_target: Union = None padding: Union = False max_length: Optional = None truncation: bool = False pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None sampling_rate: Optional = None **kwargs )
```

参数

+   `audio` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`, *可选*) — 要处理的序列或批次。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。这将输出波形特征。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。

+   `audio_target` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`, *可选*) — 要处理为目标的序列或批次。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。这将输出 log-mel 频谱特征。

+   `padding` (`bool`, `str` 或 PaddingStrategy, *可选*, 默认为`False`) — 在以下选项中选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供一个序列，则不填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则使用模型可接受的最大输入长度进行填充。

    +   `False` 或 `'do_not_pad'` (默认): 不填充（即，可以输出具有不同长度序列的批次）。

+   `max_length` (`int`, *可选*) — 返回列表的最大长度和可选填充长度（见上文）。

+   `truncation` (`bool`) — 激活截断，将输入序列截断为*max_length*长。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。

    这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力为`>= 7.5`（Volta），或者对于受益于序列长度为 128 的倍数的 TPUs。

+   `return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定的 feature_extractor 的默认设置返回注意力掩码。

    什么是注意力掩码？

+   `return_tensors` (`str` 或 TensorType, *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回 Numpy `np.ndarray`对象。

+   `sampling_rate` (`int`, *可选*) — `audio`或`audio_target`输入的采样率。强烈建议在前向调用时传递`sampling_rate`以防止潜在错误。

对一个或多个序列进行特征化和为模型准备的主要方法。

传入一个值给`audio`以提取波形特征。传入一个值给`audio_target`以提取 log-mel 频谱特征。

## SpeechT5Processor

### `class transformers.SpeechT5Processor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L20)

```py
( feature_extractor tokenizer )
```

参数

+   `feature_extractor`（`SpeechT5FeatureExtractor`）- SpeechT5FeatureExtractor 的实例。特征提取器是必需的输入。

+   `tokenizer`（`SpeechT5Tokenizer`）- SpeechT5Tokenizer 的实例。分词器是必需的输入。

构建一个 SpeechT5 处理器，将特征提取器和分词器包装成一个单一的处理器。

SpeechT5Processor 提供了 SpeechT5FeatureExtractor 和 SpeechT5Tokenizer 的所有功能。有关更多信息，请参阅**call**()和 decode()的文档字符串。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L40)

```py
( *args **kwargs )
```

处理音频和文本输入，以及音频和文本目标。

您可以使用参数`audio`处理音频，或者使用参数`audio_target`处理音频目标。这将参数转发到 SpeechT5FeatureExtractor 的**call**()。

您可以使用参数`text`处理文本，或者使用参数`text_target`处理文本标签。这将参数转发到 SpeechT5Tokenizer 的**call**()。

有效的输入组合是：

+   仅`text`

+   仅`audio`

+   仅`text_target`

+   仅`audio_target`

+   `text`和`audio_target`

+   `audio`和`audio_target`

+   `text`和`text_target`

+   `audio`和`text_target`

有关更多信息，请参阅上述两种方法的文档字符串。

#### `pad`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L111)

```py
( *args **kwargs )
```

将音频和文本输入以及它们的目标整合到填充的批次中。

音频输入由 SpeechT5FeatureExtractor 的 pad()进行填充。文本输入由 SpeechT5Tokenizer 的 pad()进行填充。

有效的输入组合是：

+   仅`input_ids`

+   仅`input_values`

+   仅`labels`，可以是 log-mel 频谱图或文本标记

+   `input_ids`和 log-mel 频谱`labels`

+   `input_values`和文本`labels`

有关更多信息，请参阅上述两种方法的文档字符串。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)

```py
( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`或`os.PathLike`）- 这可以是：

    +   一个字符串，预训练特征提取器的*模型 ID*，托管在 huggingface.co 上的模型存储库中。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者命名空间在用户或组织名称下，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用 save_pretrained()方法保存的特征提取器文件，例如，`./my_model_directory/`。

    +   保存特征提取器 JSON *文件*的路径或 url，例如`./my_model_directory/preprocessor_config.json`。**kwargs — 传递给 from_pretrained()和`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`的其他关键字参数。

实例化与预训练模型相关联的处理器。

此类方法只是调用特征提取器 from_pretrained()、图像处理器 ImageProcessingMixin 和标记器`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`方法。有关更多信息，请参考上述方法的文档字符串。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)

```py
( save_directory push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str`或`os.PathLike`) — 特征提取器 JSON 文件和标记器文件将保存在的目录（如果目录不存在，则将创建该目录）。

+   `push_to_hub` (`bool`, *可选*, 默认为`False`) — 在保存模型后是否将其推送到 Hugging Face 模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`名称）。

+   `kwargs`（`Dict[str, Any]`，*可选*） — 传递给 push_to_hub()方法的其他关键字参数。

将此处理器的属性（特征提取器、标记器等）保存在指定目录中，以便可以使用 from_pretrained()方法重新加载。

此类方法只是调用 save_pretrained()和 save_pretrained()。有关更多信息，请参考上述方法的文档字符串。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L171)

```py
( *args **kwargs )
```

该方法将其所有参数转发到 SpeechT5Tokenizer 的 batch_decode()。有关更多信息，请参考此方法的文档字符串。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L178)

```py
( *args **kwargs )
```

该方法将其所有参数转发到 SpeechT5Tokenizer 的 decode()。有关更多信息，请参考此方法的文档字符串。

## SpeechT5Model

### `class transformers.SpeechT5Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2090)

```py
( config: SpeechT5Config encoder: Optional = None decoder: Optional = None )
```

参数

+   `config`（SpeechT5Config） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

+   `encoder`（`SpeechT5EncoderWithSpeechPrenet`或`SpeechT5EncoderWithTextPrenet`或`None`）— 应用适当的语音或文本编码器预网络的 Transformer 编码器模块。如果为`None`，将使用`SpeechT5EncoderWithoutPrenet`，并假定`input_values`为隐藏状态。

+   `decoder`（`SpeechT5DecoderWithSpeechPrenet`或`SpeechT5DecoderWithTextPrenet`或`None`）— 应用适当的语音或文本解码器预网络的 Transformer 解码器模块。如果为`None`，将使用`SpeechT5DecoderWithoutPrenet`，并假定`decoder_input_values`为隐藏状态。

裸的 SpeechT5 编码器-解码器模型输出原始隐藏状态，没有任何特定的预处理或后处理网络。该模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头部等）。

该模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2136)

```py
( input_values: Optional = None attention_mask: Optional = None decoder_input_values: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None speaker_embeddings: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)
```

参数

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于避免在填充令牌索引上执行卷积和注意力的蒙版。蒙版值选定在`[0, 1]`之间：

    +   1 表示令牌未被遮蔽，

    +   0 表示令牌被遮蔽。

    什么是注意力蒙版？

    只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，应避免在进行批量推断时传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应简单地填充为 0 并在不传递`attention_mask`的情况下传递。请注意，这些模型的结果也会因`input_values`是否填充而略有不同。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_values`中填充令牌的张量。默认情况下还将使用因果蒙版。

    如果要更改填充行为，应阅读`SpeechT5Decoder._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.FloatTensor`，*可选*）— 用于在编码器中使注意力模块的选定头部失效的蒙版。蒙版值选定在`[0, 1]`之间：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.FloatTensor`，*可选*）— 用于在解码器中使注意力模块的选定头部失效的蒙版。蒙版值选定在`[0, 1]`之间：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）— 用于使交叉注意力模块的选定头部失效的蒙版。蒙版值选定在`[0, 1]`之间：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或者`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择只输入最后一个`decoder_input_values`（那些没有将它们的过去键值状态提供给该模型的）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_values`。decoder_inputs_embeds（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）：可选地，可以直接传递嵌入表示，而不是传递`decoder_input_values`。如果使用`past_key_values`，可选择只输入最后一个`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制如何将`decoder_input_values`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

+   `input_values`（形状为`(batch_size, sequence_length)`的`torch.Tensor`）- 根据使用的编码器，`input_values`可以是：输入原始语音波形的浮点值，或者词汇表中输入序列标记的索引，或者隐藏状态。

+   `decoder_input_values`（形状为`(batch_size, target_sequence_length)`的`torch.Tensor`，*可选*）- 根据使用的解码器，`decoder_input_values`可以是：从原始语音波形中提取的对数梅尔滤波器特征的浮点值，或者词汇表中解码器输入序列标记的索引，或者隐藏状态。

+   `speaker_embeddings`（形状为`(batch_size, config.speaker_embedding_dim)`的`torch.FloatTensor`，*可选*）- 包含说话者嵌入的张量。

返回

transformers.modeling_outputs.Seq2SeqModelOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.Seq2SeqModelOutput 或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者`config.return_dict=False`时）包括不同的元素，取决于配置（SpeechT5Config）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型解码器最后一层的隐藏状态序列。

    如果使用`past_key_values`，则仅输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出和每一层的输出各一个）。

    解码器在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`, *可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出和每一层的输出各一个）。

    编码器在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

SpeechT5Model 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。

## SpeechT5ForSpeechToText

### `class transformers.SpeechT5ForSpeechToText`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2238)

```py
( config: SpeechT5Config )
```

参数

+   `config` (SpeechT5Config) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained() 方法以加载模型权重。

SpeechT5 模型具有语音编码器和文本解码器。该模型继承自 PreTrainedModel。检查超类文档以获取库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般使用和行为相关的所有事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2284)

```py
( input_values: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行卷积和注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被 `masked` 的标记，

    +   0 表示被 `masked` 的标记。

    什么是注意力掩码？

    只有当相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。对于所有处理器具有 `config.return_attention_mask == False` 的模型，应避免传递 `attention_mask` 以避免在进行批量推理时性能下降。对于这样的模型，`input_values` 应该简单地用 0 填充并在不传递 `attention_mask` 的情况下传递。请注意，这些模型根据 `input_values` 是否填充也会产生略有不同的结果。

+   `decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个忽略 `decoder_input_values` 中填充标记的张量。默认情况下还将使用因果掩码。

    如果要更改填充行为，应阅读 `SpeechT5Decoder._prepare_decoder_attention_mask` 并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `head_mask` (`torch.FloatTensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*) — 编码器中注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 中选择。

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `decoder_head_mask` (`torch.FloatTensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 解码器中注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 交叉注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括 (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` 的形状为 `(batch_size, sequence_length, hidden_size)`，是编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_values`（即不将其过去的键值状态提供给此模型的那些）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_values`。`decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）：可选择直接传递嵌入表示，而不是传递`decoder_input_values`。如果使用`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（请参阅`past_key_values`）。如果要更好地控制如何将`decoder_input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

+   `input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 输入原始语音波形的浮点值。可以通过将*.flac*或*.wav*音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过声音文件库（*pip install soundfile*）。要准备好将数组转换为`input_values`，应使用 SpeechT5Processor 进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅 SpeechT5Processor.`call`()。

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 词汇表中解码器输入序列标记的索引。

    可以使用 SpeechT5Tokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是解码器输入 ID？

    SpeechT5 使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，可以选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 用于计算语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`范围内，或者为-100（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

    可以使用 SpeechT5Tokenizer 获取标签索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

返回

transformers.modeling_outputs.Seq2SeqLMOutput 或 `tuple(torch.FloatTensor)`

transformers.modeling_outputs.Seq2SeqLMOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（SpeechT5Config）和输入的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出和每一层的输出）。

    每一层解码器的隐藏状态以及初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。

    解码器的注意力权重，在注意力 SoftMax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。

    解码器交叉注意力层的注意力权重，在注意力 SoftMax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出和每一层的输出）。

    编码器每一层的隐藏状态以及初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

SpeechT5ForSpeechToText 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToText
>>> from datasets import load_dataset

>>> dataset = load_dataset(
...     "hf-internal-testing/librispeech_asr_demo", "clean", split="validation"
... )  # doctest: +IGNORE_RESULT
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_asr")
>>> model = SpeechT5ForSpeechToText.from_pretrained("microsoft/speecht5_asr")

>>> # audio file is decoded on the fly
>>> inputs = processor(audio=dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> predicted_ids = model.generate(**inputs, max_length=100)

>>> # transcribe speech
>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)
>>> transcription[0]
'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'
```

```py
>>> inputs["labels"] = processor(text_target=dataset[0]["text"], return_tensors="pt").input_ids

>>> # compute loss
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
19.68
```

## SpeechT5ForTextToSpeech

### `class transformers.SpeechT5ForTextToSpeech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2602)

```py
( config: SpeechT5Config )
```

参数

+   `config`（SpeechT5Config）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

具有文本编码器和语音解码器的 SpeechT5 模型。该模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2635)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_values: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None speaker_embeddings: Optional = None labels: Optional = None stop_labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSpectrogramOutput or tuple(torch.FloatTensor)
```

参数

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于避免在填充标记索引上执行卷积和注意力的遮罩。遮罩值选定在`[0, 1]`之间：

    +   1 表示未被遮罩的标记，

    +   0 表示被遮罩的标记。

    什么是注意力遮罩？

    只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，在进行批量推理时，应避免传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应简单地用 0 填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充，也会产生略有不同的结果。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_values`中填充标记的张量。因果遮罩也将默认使用。

    如果要更改填充行为，应阅读`SpeechT5Decoder._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.FloatTensor`，*可选*）— 用于在编码器中使注意力模块中的选定头部失效的遮罩。遮罩值选定在`[0, 1]`之间：

    +   1 表示头部未被遮罩，

    +   0 表示头部被遮罩。

+   `decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.FloatTensor`，*可选*）- 用于使解码器中注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于使解码器中交叉注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`的元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_values`（这些值没有传递给该模型的过去键值状态）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_values`。decoder_inputs_embeds（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）：可选择直接传递嵌入表示，而不是传递`decoder_input_values`。如果使用`past_key_values`，可以选择只输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果要更好地控制如何将`decoder_input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用 SpeechT5Tokenizer 获取索引。有关详细信息，请参见 encode()和`call`()。

    什么是输入 ID？

+   `decoder_input_values`（形状为`(batch_size, sequence_length, config.num_mel_bins)`的`torch.FloatTensor`）- 输入梅尔频谱的浮点值。

    SpeechT5 使用全零频谱作为`decoder_input_values`生成的起始令牌。如果使用`past_key_values`，可以选择只输入最后的`decoder_input_values`（参见`past_key_values`）。

+   `speaker_embeddings`（形状为`(batch_size, config.speaker_embedding_dim)`的`torch.FloatTensor`，*可选*）- 包含说话者嵌入的张量。

+   `labels`（形状为`(batch_size, sequence_length, config.num_mel_bins)`的`torch.FloatTensor`，*可选*）- 目标梅尔频谱的浮点值。时间步设置为`-100.0`将被忽略（掩码）用于损失计算。可以使用 SpeechT5Processor 获取频谱图。有关详细信息，请参阅 SpeechT5Processor.`call`()。

返回值

transformers.modeling_outputs.Seq2SeqSpectrogramOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.Seq2SeqSpectrogramOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（SpeechT5Config）和输入的各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 频谱生成损失。

+   `spectrogram`（形状为`(batch_size, sequence_length, num_bins)`的`torch.FloatTensor`）- 预测的频谱图。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    解码器每一层的隐藏状态以及初始嵌入输出。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    编码器在每一层的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组，每层一个。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

SpeechT5ForTextToSpeech 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, set_seed
>>> import torch

>>> processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
>>> model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
>>> vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

>>> inputs = processor(text="Hello, my dog is cute", return_tensors="pt")
>>> speaker_embeddings = torch.zeros((1, 512))  # or load xvectors from a file

>>> set_seed(555)  # make deterministic

>>> # generate speech
>>> speech = model.generate(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)
>>> speech.shape
torch.Size([15872])
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2755)

```py
( input_ids: LongTensor attention_mask: Optional = None speaker_embeddings: Optional = None threshold: float = 0.5 minlenratio: float = 0.0 maxlenratio: float = 20.0 vocoder: Optional = None output_cross_attentions: bool = False return_output_lengths: bool = False **kwargs ) → export const metadata = 'undefined';tuple(torch.FloatTensor) comprising various elements depending on the inputs
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。

    可以使用 SpeechT5Tokenizer 获取索引。有关详细信息，请参阅 encode()和`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 来自分词器的注意力蒙版，用于批量推理，指示模型在哪里忽略输入的填充标记。

+   `speaker_embeddings`（形状为`(batch_size, config.speaker_embedding_dim)`的`torch.FloatTensor`，*可选*） — 包含说话者嵌入的张量。

+   `threshold`（*可选*，默认为 0.5 的`float`） — 当预测的停止标记概率超过此值时，生成的序列结束。

+   `minlenratio`（*可选*，默认为 0.0 的`float`） — 用于计算输出序列的最小所需长度。

+   `maxlenratio`（*可选*，默认为 20.0 的`float`） — 用于计算输出序列的最大允许长度。

+   `vocoder`（*可选*，`nn.Module`） — 将梅尔频谱图转换为语音波形的声码器。如果为`None`，则输出为梅尔频谱图。

+   `output_cross_attentions`（*可选*，默认为`False`的`bool`） — 是否返回解码器的交叉注意力层的注意力张量。

+   `return_output_lengths`（*可选*，默认为`False`的`bool`） — 是否返回具体的频谱/波形长度。

返回

`torch.FloatTensor`元组，包含根据输入不同的元素

+   当`return_output_lengths`为 False 时

    +   `spectrogram`（*可选*，当未提供`vocoder`时返回）`torch.FloatTensor`，形状为`(output_sequence_length, config.num_mel_bins)` — 预测的对数梅尔频谱图。

    +   `waveform`（*可选*，当提供`vocoder`时返回）`torch.FloatTensor`，形状为`(num_frames,)` — 预测的语音波形。

    +   `cross_attentions`（*可选*，当`output_cross_attentions`为`True`时返回）`torch.FloatTensor`，形状为`(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)` — 解码器的交叉注意力层的输出。

+   当`return_output_lengths`为 True 时

    +   `spectrograms`（*可选*，当未提供`vocoder`时返回）`torch.FloatTensor`，形状为`(batch_size, output_sequence_length, config.num_mel_bins)` — 预测的对数梅尔频谱图，填充到最大长度。

    +   `spectrogram_lengths`（*可选*，在没有提供`vocoder`时返回）`List[Int]` — 每个频谱图的具体长度的列表。

    +   `waveforms`（*可选*，在提供`vocoder`时返回）`torch.FloatTensor`，形状为`(batch_size, num_frames)` — 预测的语音波形，填充到最大长度。

    +   `waveform_lengths`（*可选*，在提供`vocoder`时返回）`List[Int]` — 每个波形的具体长度的列表。

    +   `cross_attentions`（*可选*，当`output_cross_attentions`为`True`时返回）`torch.FloatTensor`，形状为`(batch_size, config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)` — 解码器的交叉注意力层的输出。

将一系列输入标记转换为一系列 mel 频谱图，随后使用声码器将其转换为语音波形。

## SpeechT5ForSpeechToSpeech

### `class transformers.SpeechT5ForSpeechToSpeech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2944)

```py
( config: SpeechT5Config )
```

参数

+   `config`（SpeechT5Config）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

具有语音编码器和语音解码器的 SpeechT5 模型。该模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2974)

```py
( input_values: Optional = None attention_mask: Optional = None decoder_input_values: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None speaker_embeddings: Optional = None labels: Optional = None stop_labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSpectrogramOutput or tuple(torch.FloatTensor)
```

参数

+   `attention_mask`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选定在`[0, 1]`之间：

    +   1 表示未被`masked`的标记，

    +   0 表示被`masked`的标记。

    什么是注意力掩码？

    只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，应避免传递`attention_mask`以避免在进行批量推断时性能下降。对于这些模型，`input_values`应简单地用 0 填充并在不传递`attention_mask`的情况下传递。请注意，这些模型还会根据`input_values`是否填充而产生略有不同的结果。

+   `decoder_attention_mask`（`torch.LongTensor`，形状为`(batch_size, target_sequence_length)`，*可选*）— 默认行为：生成一个张量，忽略`decoder_input_values`中的填充标记。因果掩码也将默认使用。

    如果要更改填充行为，您应阅读`SpeechT5Decoder._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `head_mask`（`torch.FloatTensor`，形状为`(encoder_layers, encoder_attention_heads)`，*可选*）— 用于将编码器中注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `decoder_head_mask` (`torch.FloatTensor`，形状为`(decoder_layers, decoder_attention_heads)`，*可选*）— 用于使解码器中选择的注意力模块的头部失效的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `cross_attn_head_mask` (`torch.Tensor`，形状为`(decoder_layers, decoder_attention_heads)`，*可选*）— 用于使交叉注意力模块中选择的头部失效的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包含(`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`) `last_hidden_state`形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_values`（那些没有将其过去键值状态提供给此模型的值），而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_values`。`decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*可选*）：可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_values`。如果使用`past_key_values`，可以选择仅输入最后一个`decoder_inputs_embeds`（请参阅`past_key_values`）。如果您想要更多控制权来将`decoder_input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `use_cache` (`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions` (`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

+   `input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`）— 输入原始语音波形的浮点值。可以通过将*.flac*或*.wav*音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过 soundfile 库（*pip install soundfile*）。要将数组准备成`input_values`，应使用 SpeechT5Processor 进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅 SpeechT5Processor.`call`()。

+   `decoder_input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.num_mel_bins)`）— 输入梅尔频谱图的浮点值。

    SpeechT5 使用全零频谱作为`decoder_input_values`生成的起始标记。如果使用`past_key_values`，可以选择只输入最后的`decoder_input_values`（参见`past_key_values`）。

+   `speaker_embeddings` (`torch.FloatTensor`，形状为`(batch_size, config.speaker_embedding_dim)`，*可选*) — 包含说话者嵌入的张量。

+   `labels` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.num_mel_bins)`，*可选*) — 目标梅尔频谱的浮点值。可以使用 SpeechT5Processor 获取频谱图。详细信息请参见 SpeechT5Processor.`call`()。

返回

transformers.modeling_outputs.Seq2SeqSpectrogramOutput 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.Seq2SeqSpectrogramOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（SpeechT5Config）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回) — 频谱图生成损失。

+   `spectrogram` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_bins)`) — 预测的频谱图。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，在传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（查看`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，在传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个 + 每一层的输出一个）。

    解码器每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，在传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`，*可选*，在传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    编码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

SpeechT5ForSpeechToSpeech 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan, set_seed
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset(
...     "hf-internal-testing/librispeech_asr_demo", "clean", split="validation"
... )  # doctest: +IGNORE_RESULT
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_vc")
>>> model = SpeechT5ForSpeechToSpeech.from_pretrained("microsoft/speecht5_vc")
>>> vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

>>> # audio file is decoded on the fly
>>> inputs = processor(audio=dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

>>> speaker_embeddings = torch.zeros((1, 512))  # or load xvectors from a file

>>> set_seed(555)  # make deterministic

>>> # generate speech
>>> speech = model.generate_speech(inputs["input_values"], speaker_embeddings, vocoder=vocoder)
>>> speech.shape
torch.Size([77824])
```

#### `generate_speech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L3088)

```py
( input_values: FloatTensor speaker_embeddings: Optional = None attention_mask: Optional = None threshold: float = 0.5 minlenratio: float = 0.0 maxlenratio: float = 20.0 vocoder: Optional = None output_cross_attentions: bool = False return_output_lengths: bool = False ) → export const metadata = 'undefined';tuple(torch.FloatTensor) comprising various elements depending on the inputs
```

参数

+   `input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。

    可以通过将*.flac*或*.wav*音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中来获取值，例如通过声音文件库(*pip install soundfile*)。要将数组准备成`input_values`，应该使用 SpeechT5Processor 进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅 SpeechT5Processor.`call`()。

+   `speaker_embeddings` (`torch.FloatTensor`，形状为`(batch_size, config.speaker_embedding_dim)`，*optional*) — 包含说话者嵌入的张量。

+   `attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记，值为 1。

    +   对于被`masked`的标记，值为 0。

    什么是注意力掩码？

+   `threshold` (`float`，*optional*，默认为 0.5) — 当预测的停止标记概率超过此值时，生成的序列结束。

+   `minlenratio` (`float`，*optional*，默认为 0.0) — 用于计算输出序列的最小所需长度。

+   `maxlenratio` (`float`，*optional*，默认为 20.0) — 用于计算输出序列的最大允许长度。

+   `vocoder` (`nn.Module`，*optional*，默认为`None`) — 将梅尔频谱图转换为语音波形的声码器。如果为`None`，则输出为梅尔频谱图。

+   `output_cross_attentions` (`bool`，*optional*，默认为`False`) — 是否返回解码器交叉注意力层的注意力张量。

+   `return_output_lengths` (`bool`，*optional*，默认为`False`) — 是否返回具体的频谱/波形长度。

返回

包含各种元素的`tuple(torch.FloatTensor)`，取决于输入

+   当`return_output_lengths`为 False 时

    +   `spectrogram` (*optional*，当未提供`vocoder`时返回) 形状为`(output_sequence_length, config.num_mel_bins)`的`torch.FloatTensor` — 预测的对数梅尔频谱图。

    +   `waveform` (*optional*，当提供`vocoder`时返回) 形状为`(num_frames,)`的`torch.FloatTensor` — 预测的语音波形。

    +   `cross_attentions` (*可选*, 当`output_cross_attentions`为`True`时返回) `torch.FloatTensor`，形状为`(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)` — 解码器交叉注意力层的输出。

+   当`return_output_lengths`为 True 时

    +   `spectrograms` (*可选*, 当未提供`vocoder`时返回) `torch.FloatTensor`，形状为`(batch_size, output_sequence_length, config.num_mel_bins)` — 预测的对数梅尔频谱图，已填充到最大长度。

    +   `spectrogram_lengths` (*可选*, 当未提供`vocoder`时返回) `List[Int]` — 每个频谱图的具体长度列表。

    +   `waveforms` (*可选*, 当提供`vocoder`时返回) `torch.FloatTensor`，形状为`(batch_size, num_frames)` — 预测的语音波形，已填充到最大长度。

    +   `waveform_lengths` (*可选*, 当提供`vocoder`时返回) `List[Int]` — 每个波形的具体长度列表。

    +   `cross_attentions` (*可选*, 当`output_cross_attentions`为`True`时返回) `torch.FloatTensor`，形状为`(batch_size, config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)` — 解码器交叉注意力层的输出。

将原始语音波形转换为一系列梅尔频谱图，随后使用语音合成器将其转换回语音波形。

## SpeechT5HifiGan

### `class transformers.SpeechT5HifiGan`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L3253)

```py
( config: SpeechT5HifiGanConfig )
```

参数

+   `config` (SpeechT5HifiGanConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

HiFi-GAN 语音合成器。该模型继承自 PreTrainedModel。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L3322)

```py
( spectrogram: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `spectrogram` (`torch.FloatTensor`) — 包含对数梅尔频谱图的张量。可以是批处理的，形状为`(batch_size, sequence_length, config.model_in_dim)`，也可以是未经批处理的，形状为`(sequence_length, config.model_in_dim)`。

返回

`torch.FloatTensor`

包含语音波形的张量。如果输入的频谱图是批处理的，则形状为`(batch_size, num_frames,)`。如果未经批处理，则形状为`(num_frames,)`。

将对数梅尔频谱图转换为语音波形。传递一批对数梅尔频谱图将返回一批语音波形。传递单个、未经批处理的对数梅尔频谱图将返回单个、未经批处理的语音波形。
