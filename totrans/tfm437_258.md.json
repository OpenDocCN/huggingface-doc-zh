["```py\n( num_channels = 3 patch_sizes = [7, 3, 3] patch_stride = [4, 2, 2] patch_padding = [2, 1, 1] embed_dim = [64, 192, 384] num_heads = [1, 3, 6] depth = [1, 2, 10] mlp_ratio = [4.0, 4.0, 4.0] attention_drop_rate = [0.0, 0.0, 0.0] drop_rate = [0.0, 0.0, 0.0] drop_path_rate = [0.0, 0.0, 0.1] qkv_bias = [True, True, True] cls_token = [False, False, True] qkv_projection_method = ['dw_bn', 'dw_bn', 'dw_bn'] kernel_qkv = [3, 3, 3] padding_kv = [1, 1, 1] stride_kv = [2, 2, 2] padding_q = [1, 1, 1] stride_q = [1, 1, 1] initializer_range = 0.02 layer_norm_eps = 1e-12 **kwargs )\n```", "```py\n>>> from transformers import CvtConfig, CvtModel\n\n>>> # Initializing a Cvt msft/cvt style configuration\n>>> configuration = CvtConfig()\n\n>>> # Initializing a model (with random weights) from the msft/cvt style configuration\n>>> model = CvtModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, CvtModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n>>> model = CvtModel.from_pretrained(\"microsoft/cvt-13\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 384, 14, 14]\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, CvtForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n>>> model = CvtForImageClassification.from_pretrained(\"microsoft/cvt-13\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntabby, tabby cat\n```", "```py\n( config: CvtConfig *inputs **kwargs )\n```", "```py\n( pixel_values: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFCvtModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n>>> model = TFCvtModel.from_pretrained(\"microsoft/cvt-13\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: CvtConfig *inputs **kwargs )\n```", "```py\n( pixel_values: tf.Tensor | None = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TFCvtForImageClassification\n>>> import tensorflow as tf\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n>>> model = TFCvtForImageClassification.from_pretrained(\"microsoft/cvt-13\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = tf.math.argmax(logits, axis=-1)[0]\n>>> print(\"Predicted class:\", model.config.id2label[int(predicted_class_idx)])\n```"]