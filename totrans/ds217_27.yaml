- en: Beam Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/datasets/beam](https://huggingface.co/docs/datasets/beam)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/datasets/v2.17.0/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/start.146395b0.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/scheduler.bdbef820.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/singletons.98dc5b8b.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.8a885b74.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/paths.a483fec8.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/app.e612c4fb.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.c0aea24a.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/0.5e8dbda6.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/13.dc09c092.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Tip.31005f7d.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/CodeBlock.6ccca92e.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Heading.2eb892cb.js">
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets are too large to be processed on a single machine. Instead, you
    can process them with [Apache Beam](https://beam.apache.org/), a library for parallel
    data processing. The processing pipeline is executed on a distributed processing
    backend such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/),
    or [Google Cloud Dataflow](https://cloud.google.com/dataflow).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already created Beam pipelines for some of the larger datasets like
    [wikipedia](https://huggingface.co/datasets/wikipedia), and [wiki40b](https://huggingface.co/datasets/wiki40b).
    You can load these normally with [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    But if you want to run your own Beam pipeline with Dataflow, here is how:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the dataset and configuration you want to process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Input your Google Cloud Platform information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify your Python requirements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When you run your pipeline, you can adjust the parameters to change the runner
    (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers.
  prefs: []
  type: TYPE_NORMAL
