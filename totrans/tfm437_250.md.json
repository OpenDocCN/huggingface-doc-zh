["```py\n( vocab_size = 32000 d_model = 1024 n_layer = 24 n_head = 16 d_inner = 4096 ff_activation = 'gelu' untie_r = True attn_type = 'bi' initializer_range = 0.02 layer_norm_eps = 1e-12 dropout = 0.1 mem_len = 512 reuse_len = None use_mems_eval = True use_mems_train = False bi_data = False clamp_len = -1 same_length = False summary_type = 'last' summary_use_proj = True summary_activation = 'tanh' summary_last_dropout = 0.1 start_n_top = 5 end_n_top = 5 pad_token_id = 5 bos_token_id = 1 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import XLNetConfig, XLNetModel\n\n>>> # Initializing a XLNet configuration\n>>> configuration = XLNetConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = XLNetModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file do_lower_case = False remove_space = True keep_accents = False bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' sep_token = '<sep>' pad_token = '<pad>' cls_token = '<cls>' mask_token = '<mask>' additional_special_tokens = ['<eop>', '<eod>'] sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = False remove_space = True keep_accents = False bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' sep_token = '<sep>' pad_token = '<pad>' cls_token = '<cls>' mask_token = '<mask>' additional_special_tokens = ['<eop>', '<eod>'] **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( last_hidden_state: FloatTensor mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None start_logits: FloatTensor = None end_logits: FloatTensor = None mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None start_top_log_probs: Optional = None start_top_index: Optional = None end_top_log_probs: Optional = None end_top_index: Optional = None cls_logits: Optional = None mems: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( last_hidden_state: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None start_logits: tf.Tensor = None end_logits: tf.Tensor = None mems: List[tf.Tensor] | None = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids: Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLNetModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids: Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLNetLMHeadModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-large-cased\")\n>>> model = XLNetLMHeadModel.from_pretrained(\"xlnet-large-cased\")\n\n>>> # We show how to setup inputs to predict a next token using a bi-directional context.\n>>> input_ids = torch.tensor(\n...     tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=False)\n... ).unsqueeze(\n...     0\n... )  # We will predict the masked token\n>>> perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n>>> perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n>>> target_mapping = torch.zeros(\n...     (1, 1, input_ids.shape[1]), dtype=torch.float\n... )  # Shape [1, 1, seq_length] => let's predict one token\n>>> target_mapping[\n...     0, 0, -1\n... ] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n\n>>> outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n>>> next_token_logits = outputs[\n...     0\n... ]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n\n>>> # The same way can the XLNetLMHeadModel be used to be trained by standard auto-regressive language modeling.\n>>> input_ids = torch.tensor(\n...     tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=False)\n... ).unsqueeze(\n...     0\n... )  # We will predict the masked token\n>>> labels = torch.tensor(tokenizer.encode(\"cute\", add_special_tokens=False)).unsqueeze(0)\n>>> assert labels.shape[0] == 1, \"only one word will be predicted\"\n>>> perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n>>> perm_mask[\n...     :, :, -1\n... ] = 1.0  # Previous tokens don't see last token as is done in standard auto-regressive lm training\n>>> target_mapping = torch.zeros(\n...     (1, 1, input_ids.shape[1]), dtype=torch.float\n... )  # Shape [1, 1, seq_length] => let's predict one token\n>>> target_mapping[\n...     0, 0, -1\n... ] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n\n>>> outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping, labels=labels)\n>>> loss = outputs.loss\n>>> next_token_logits = (\n...     outputs.logits\n... )  # Logits have shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids: Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, XLNetForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, XLNetForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = XLNetForSequenceClassification.from_pretrained(\n...     \"xlnet-base-cased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None token_type_ids: Optional = None input_mask: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLNetForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetForMultipleChoice.from_pretrained(\"xlnet-base-cased\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids: Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLNetForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetForTokenClassification.from_pretrained(\"xlnet-base-cased\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids: Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLNetForQuestionAnsweringSimple\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetForQuestionAnsweringSimple.from_pretrained(\"xlnet-base-cased\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None mems: Optional = None perm_mask: Optional = None target_mapping: Optional = None token_type_ids: Optional = None input_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None is_impossible: Optional = None cls_index: Optional = None p_mask: Optional = None use_mems: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XLNetForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = XLNetForQuestionAnswering.from_pretrained(\"xlnet-base-cased\")\n\n>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(\n...     0\n... )  # Batch size 1\n>>> start_positions = torch.tensor([1])\n>>> end_positions = torch.tensor([3])\n>>> outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n\n>>> loss = outputs.loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFXLNetModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = TFXLNetModel.from_pretrained(\"xlnet-base-cased\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> import numpy as np\n>>> from transformers import AutoTokenizer, TFXLNetLMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-large-cased\")\n>>> model = TFXLNetLMHeadModel.from_pretrained(\"xlnet-large-cased\")\n\n>>> # We show how to setup inputs to predict a next token using a bi-directional context.\n>>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=True))[\n...     None, :\n... ]  # We will predict the masked token\n\n>>> perm_mask = np.zeros((1, input_ids.shape[1], input_ids.shape[1]))\n>>> perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n\n>>> target_mapping = np.zeros(\n...     (1, 1, input_ids.shape[1])\n... )  # Shape [1, 1, seq_length] => let's predict one token\n>>> target_mapping[\n...     0, 0, -1\n... ] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n\n>>> outputs = model(\n...     input_ids,\n...     perm_mask=tf.constant(perm_mask, dtype=tf.float32),\n...     target_mapping=tf.constant(target_mapping, dtype=tf.float32),\n... )\n\n>>> next_token_logits = outputs[\n...     0\n... ]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFXLNetForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = TFXLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFXLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray | tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFXLNetForMultipleChoice\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = TFXLNetForMultipleChoice.from_pretrained(\"xlnet-base-cased\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"tf\", padding=True)\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}\n>>> outputs = model(inputs)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> logits = outputs.logits\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFXLNetForTokenClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = TFXLNetForTokenClassification.from_pretrained(\"xlnet-base-cased\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"tf\"\n... )\n\n>>> logits = model(**inputs).logits\n>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n```", "```py\n>>> labels = predicted_token_class_ids\n>>> loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None mems: np.ndarray | tf.Tensor | None = None perm_mask: np.ndarray | tf.Tensor | None = None target_mapping: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None input_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None use_mems: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFXLNetForQuestionAnsweringSimple\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> model = TFXLNetForQuestionAnsweringSimple.from_pretrained(\"xlnet-base-cased\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n\n>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n```", "```py\n>>> # target is \"nice puppet\"\n>>> target_start_index = tf.constant([14])\n>>> target_end_index = tf.constant([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = tf.math.reduce_mean(outputs.loss)\n```"]