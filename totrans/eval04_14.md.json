["```py\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport evaluate\n\n# We pull example code from Keras.io's guide on classifying with MNIST\n# Located here: https://keras.io/examples/vision/mnist_convnet/\n\n# Model / data parameters\ninput_shape = (28, 28, 1)\n\n# Load the data and split it between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n\n# Only select tshirts/tops and trousers, classes 0 and 1\ndef get_tshirts_tops_and_trouser(x_vals, y_vals):\n    mask = np.where((y_vals == 0) | (y_vals == 1))\n    return x_vals[mask], y_vals[mask]\n\nx_train, y_train = get_tshirts_tops_and_trouser(x_train, y_train)\nx_test, y_test = get_tshirts_tops_and_trouser(x_test, y_test)\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\n```", "```py\nclass MetricsCallback(keras.callbacks.Callback):\n\n    def __init__(self, metric_name, x_data, y_data) -> None:\n        super(MetricsCallback, self).__init__()\n\n        self.x_data = x_data\n        self.y_data = y_data\n        self.metric_name = metric_name\n        self.metric = evaluate.load(metric_name)\n\n    def on_epoch_end(self, epoch, logs=dict()):\n        m = self.model \n        # Ensure we get labels of \"1\" or \"0\"\n        training_preds = np.round(m.predict(self.x_data))\n        training_labels = self.y_data\n\n        # Compute score and save\n        score = self.metric.compute(predictions = training_preds, references = training_labels)\n\n        logs.update(score)\n```", "```py\nbatch_size = 128\nepochs = 2\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n\nmodel_history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, \ncallbacks = [MetricsCallback(x_data = x_train, y_data = y_train, metric_name = \"accuracy\")])\n```", "```py\nacc = evaluate.load(\"accuracy\")\n# Round the predictions to turn them into \"0\" or \"1\" labels\ntest_preds = np.round(model.predict(x_test))\ntest_labels = y_test\n```", "```py\nprint(\"Test accuracy is : \", acc.compute(predictions = test_preds, references = test_labels))\n# Test accuracy is : 0.9855\n```"]