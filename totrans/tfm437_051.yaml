- en: Multilingual models for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于推理的多语言模型
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/multilingual](https://huggingface.co/docs/transformers/v4.37.2/en/multilingual)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/multilingual](https://huggingface.co/docs/transformers/v4.37.2/en/multilingual)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are several multilingual models in 🤗 Transformers, and their inference
    usage differs from monolingual models. Not *all* multilingual model usage is different
    though. Some models, like [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased),
    can be used just like a monolingual model. This guide will show you how to use
    multilingual models whose usage differs for inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers中有几个多语言模型，它们的推理用法与单语模型不同。不过，并非*所有*多语言模型的用法都不同。一些模型，如[bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased)，可以像单语模型一样使用。本指南将向您展示如何使用推理中用法不同的多语言模型。
- en: XLM
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLM
- en: 'XLM has ten different checkpoints, only one of which is monolingual. The nine
    remaining model checkpoints can be split into two categories: the checkpoints
    that use language embeddings and those that don’t.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: XLM有十个不同的检查点，其中只有一个是单语的。剩下的九个模型检查点可以分为两类：使用语言嵌入和不使用语言嵌入的检查点。
- en: XLM with language embeddings
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有语言嵌入的XLM
- en: 'The following XLM models use language embeddings to specify the language used
    at inference:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下XLM模型使用语言嵌入来指定推理中使用的语言：
- en: '`xlm-mlm-ende-1024` (Masked language modeling, English-German)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-ende-1024`（掩码语言建模，英语-德语）'
- en: '`xlm-mlm-enfr-1024` (Masked language modeling, English-French)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-enfr-1024`（掩码语言建模，英语-法语）'
- en: '`xlm-mlm-enro-1024` (Masked language modeling, English-Romanian)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-enro-1024`（掩码语言建模，英语-罗马尼亚语）'
- en: '`xlm-mlm-xnli15-1024` (Masked language modeling, XNLI languages)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-xnli15-1024`（掩码语言建模，XNLI语言）'
- en: '`xlm-mlm-tlm-xnli15-1024` (Masked language modeling + translation, XNLI languages)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-tlm-xnli15-1024`（掩码语言建模+翻译，XNLI语言）'
- en: '`xlm-clm-enfr-1024` (Causal language modeling, English-French)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-clm-enfr-1024`（因果语言建模，英语-法语）'
- en: '`xlm-clm-ende-1024` (Causal language modeling, English-German)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-clm-ende-1024`（因果语言建模，英语-德语）'
- en: Language embeddings are represented as a tensor of the same shape as the `input_ids`
    passed to the model. The values in these tensors depend on the language used and
    are identified by the tokenizer’s `lang2id` and `id2lang` attributes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言嵌入表示为与传递给模型的`input_ids`相同形状的张量。这些张量中的值取决于使用的语言，并由标记器的`lang2id`和`id2lang`属性识别。
- en: 'In this example, load the `xlm-clm-enfr-1024` checkpoint (Causal language modeling,
    English-French):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，加载`xlm-clm-enfr-1024`检查点（因果语言建模，英语-法语）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `lang2id` attribute of the tokenizer displays this model’s languages and
    their ids:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器的`lang2id`属性显示了该模型的语言及其ID：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, create an example input:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个示例输入：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Set the language id as `"en"` and use it to define the language embedding. The
    language embedding is a tensor filled with `0` since that is the language id for
    English. This tensor should be the same size as `input_ids`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将语言ID设置为`"en"`，并用它来定义语言嵌入。语言嵌入是一个填充了`0`的张量，因为这是英语的语言ID。这个张量应该与`input_ids`的大小相同。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now you can pass the `input_ids` and language embedding to the model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将`input_ids`和语言嵌入传递给模型：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The [run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py)
    script can generate text with language embeddings using the `xlm-clm` checkpoints.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py)脚本可以使用`xlm-clm`检查点生成带有语言嵌入的文本。'
- en: XLM without language embeddings
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 没有语言嵌入的XLM
- en: 'The following XLM models do not require language embeddings during inference:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下XLM模型在推理过程中不需要语言嵌入：
- en: '`xlm-mlm-17-1280` (Masked language modeling, 17 languages)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-17-1280`（掩码语言建模，17种语言）'
- en: '`xlm-mlm-100-1280` (Masked language modeling, 100 languages)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-100-1280`（掩码语言建模，100种语言）'
- en: These models are used for generic sentence representations, unlike the previous
    XLM checkpoints.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型用于通用句子表示，不同于之前的XLM检查点。
- en: BERT
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT
- en: 'The following BERT models can be used for multilingual tasks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下BERT模型可用于多语言任务：
- en: '`bert-base-multilingual-uncased` (Masked language modeling + Next sentence
    prediction, 102 languages)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-base-multilingual-uncased`（掩码语言建模+下一句预测，102种语言）'
- en: '`bert-base-multilingual-cased` (Masked language modeling + Next sentence prediction,
    104 languages)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-base-multilingual-cased`（掩码语言建模+下一句预测，104种语言）'
- en: These models do not require language embeddings during inference. They should
    identify the language from the context and infer accordingly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在推理过程中不需要语言嵌入。它们应该根据上下文识别语言并相应地推断。
- en: XLM-RoBERTa
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLM-RoBERTa
- en: 'The following XLM-RoBERTa models can be used for multilingual tasks:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下XLM-RoBERTa模型可用于多语言任务：
- en: '`xlm-roberta-base` (Masked language modeling, 100 languages)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-roberta-base`（掩码语言建模，100种语言）'
- en: '`xlm-roberta-large` (Masked language modeling, 100 languages)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-roberta-large`（掩码语言建模，100种语言）'
- en: XLM-RoBERTa was trained on 2.5TB of newly created and cleaned CommonCrawl data
    in 100 languages. It provides strong gains over previously released multilingual
    models like mBERT or XLM on downstream tasks like classification, sequence labeling,
    and question answering.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-RoBERTa在100种语言中新创建和清理的2.5TB CommonCrawl数据上进行了训练。在分类、序列标记和问题回答等下游任务上，它比以前发布的多语言模型如mBERT或XLM提供了强大的性能提升。
- en: M2M100
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: M2M100
- en: 'The following M2M100 models can be used for multilingual translation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下M2M100模型可用于多语言翻译：
- en: '`facebook/m2m100_418M` (Translation)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/m2m100_418M`（翻译）'
- en: '`facebook/m2m100_1.2B` (Translation)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/m2m100_1.2B`（翻译）'
- en: 'In this example, load the `facebook/m2m100_418M` checkpoint to translate from
    Chinese to English. You can set the source language in the tokenizer:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，加载`facebook/m2m100_418M`检查点以将中文翻译成英文。您可以在标记器中设置源语言：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Tokenize the text:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本进行标记化：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'M2M100 forces the target language id as the first generated token to translate
    to the target language. Set the `forced_bos_token_id` to `en` in the `generate`
    method to translate to English:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: M2M100强制将目标语言ID作为第一个生成的标记以翻译为目标语言。在`generate`方法中将`forced_bos_token_id`设置为`en`以翻译为英语：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: MBart
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MBart
- en: 'The following MBart models can be used for multilingual translation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下MBart模型可用于多语言翻译：
- en: '`facebook/mbart-large-50-one-to-many-mmt` (One-to-many multilingual machine
    translation, 50 languages)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50-one-to-many-mmt`（一对多多语言机器翻译，50种语言）'
- en: '`facebook/mbart-large-50-many-to-many-mmt` (Many-to-many multilingual machine
    translation, 50 languages)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50-many-to-many-mmt`（多对多多语言机器翻译，50种语言）'
- en: '`facebook/mbart-large-50-many-to-one-mmt` (Many-to-one multilingual machine
    translation, 50 languages)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50-many-to-one-mmt`（多对一多语言机器翻译，50种语言）'
- en: '`facebook/mbart-large-50` (Multilingual translation, 50 languages)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50`（多语言翻译，50种语言）'
- en: '`facebook/mbart-large-cc25`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-cc25`'
- en: 'In this example, load the `facebook/mbart-large-50-many-to-many-mmt` checkpoint
    to translate Finnish to English. You can set the source language in the tokenizer:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，加载`facebook/mbart-large-50-many-to-many-mmt`检查点以将芬兰语翻译为英语。您可以在标记器中设置源语言：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Tokenize the text:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本进行标记化：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'MBart forces the target language id as the first generated token to translate
    to the target language. Set the `forced_bos_token_id` to `en` in the `generate`
    method to translate to English:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MBart强制将目标语言ID作为第一个生成的标记以翻译为目标语言。在`generate`方法中将`forced_bos_token_id`设置为`en`以翻译为英语：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you are using the `facebook/mbart-large-50-many-to-one-mmt` checkpoint, you
    don’t need to force the target language id as the first generated token otherwise
    the usage is the same.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用`facebook/mbart-large-50-many-to-one-mmt`检查点，则不需要强制目标语言ID作为第一个生成的标记，否则用法相同。
