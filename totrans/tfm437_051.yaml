- en: Multilingual models for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºæ¨ç†çš„å¤šè¯­è¨€æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/multilingual](https://huggingface.co/docs/transformers/v4.37.2/en/multilingual)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/multilingual](https://huggingface.co/docs/transformers/v4.37.2/en/multilingual)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are several multilingual models in ğŸ¤— Transformers, and their inference
    usage differs from monolingual models. Not *all* multilingual model usage is different
    though. Some models, like [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased),
    can be used just like a monolingual model. This guide will show you how to use
    multilingual models whose usage differs for inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersä¸­æœ‰å‡ ä¸ªå¤šè¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬çš„æ¨ç†ç”¨æ³•ä¸å•è¯­æ¨¡å‹ä¸åŒã€‚ä¸è¿‡ï¼Œå¹¶é*æ‰€æœ‰*å¤šè¯­è¨€æ¨¡å‹çš„ç”¨æ³•éƒ½ä¸åŒã€‚ä¸€äº›æ¨¡å‹ï¼Œå¦‚[bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased)ï¼Œå¯ä»¥åƒå•è¯­æ¨¡å‹ä¸€æ ·ä½¿ç”¨ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ¨ç†ä¸­ç”¨æ³•ä¸åŒçš„å¤šè¯­è¨€æ¨¡å‹ã€‚
- en: XLM
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLM
- en: 'XLM has ten different checkpoints, only one of which is monolingual. The nine
    remaining model checkpoints can be split into two categories: the checkpoints
    that use language embeddings and those that donâ€™t.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: XLMæœ‰åä¸ªä¸åŒçš„æ£€æŸ¥ç‚¹ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªæ˜¯å•è¯­çš„ã€‚å‰©ä¸‹çš„ä¹ä¸ªæ¨¡å‹æ£€æŸ¥ç‚¹å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šä½¿ç”¨è¯­è¨€åµŒå…¥å’Œä¸ä½¿ç”¨è¯­è¨€åµŒå…¥çš„æ£€æŸ¥ç‚¹ã€‚
- en: XLM with language embeddings
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¸¦æœ‰è¯­è¨€åµŒå…¥çš„XLM
- en: 'The following XLM models use language embeddings to specify the language used
    at inference:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹XLMæ¨¡å‹ä½¿ç”¨è¯­è¨€åµŒå…¥æ¥æŒ‡å®šæ¨ç†ä¸­ä½¿ç”¨çš„è¯­è¨€ï¼š
- en: '`xlm-mlm-ende-1024` (Masked language modeling, English-German)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-ende-1024`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-å¾·è¯­ï¼‰'
- en: '`xlm-mlm-enfr-1024` (Masked language modeling, English-French)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-enfr-1024`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-æ³•è¯­ï¼‰'
- en: '`xlm-mlm-enro-1024` (Masked language modeling, English-Romanian)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-enro-1024`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-ç½—é©¬å°¼äºšè¯­ï¼‰'
- en: '`xlm-mlm-xnli15-1024` (Masked language modeling, XNLI languages)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-xnli15-1024`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼ŒXNLIè¯­è¨€ï¼‰'
- en: '`xlm-mlm-tlm-xnli15-1024` (Masked language modeling + translation, XNLI languages)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-tlm-xnli15-1024`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡+ç¿»è¯‘ï¼ŒXNLIè¯­è¨€ï¼‰'
- en: '`xlm-clm-enfr-1024` (Causal language modeling, English-French)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-clm-enfr-1024`ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-æ³•è¯­ï¼‰'
- en: '`xlm-clm-ende-1024` (Causal language modeling, English-German)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-clm-ende-1024`ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-å¾·è¯­ï¼‰'
- en: Language embeddings are represented as a tensor of the same shape as the `input_ids`
    passed to the model. The values in these tensors depend on the language used and
    are identified by the tokenizerâ€™s `lang2id` and `id2lang` attributes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€åµŒå…¥è¡¨ç¤ºä¸ºä¸ä¼ é€’ç»™æ¨¡å‹çš„`input_ids`ç›¸åŒå½¢çŠ¶çš„å¼ é‡ã€‚è¿™äº›å¼ é‡ä¸­çš„å€¼å–å†³äºä½¿ç”¨çš„è¯­è¨€ï¼Œå¹¶ç”±æ ‡è®°å™¨çš„`lang2id`å’Œ`id2lang`å±æ€§è¯†åˆ«ã€‚
- en: 'In this example, load the `xlm-clm-enfr-1024` checkpoint (Causal language modeling,
    English-French):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒåŠ è½½`xlm-clm-enfr-1024`æ£€æŸ¥ç‚¹ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-æ³•è¯­ï¼‰ï¼š
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `lang2id` attribute of the tokenizer displays this modelâ€™s languages and
    their ids:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°å™¨çš„`lang2id`å±æ€§æ˜¾ç¤ºäº†è¯¥æ¨¡å‹çš„è¯­è¨€åŠå…¶IDï¼š
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, create an example input:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¾“å…¥ï¼š
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Set the language id as `"en"` and use it to define the language embedding. The
    language embedding is a tensor filled with `0` since that is the language id for
    English. This tensor should be the same size as `input_ids`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯­è¨€IDè®¾ç½®ä¸º`"en"`ï¼Œå¹¶ç”¨å®ƒæ¥å®šä¹‰è¯­è¨€åµŒå…¥ã€‚è¯­è¨€åµŒå…¥æ˜¯ä¸€ä¸ªå¡«å……äº†`0`çš„å¼ é‡ï¼Œå› ä¸ºè¿™æ˜¯è‹±è¯­çš„è¯­è¨€IDã€‚è¿™ä¸ªå¼ é‡åº”è¯¥ä¸`input_ids`çš„å¤§å°ç›¸åŒã€‚
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now you can pass the `input_ids` and language embedding to the model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†`input_ids`å’Œè¯­è¨€åµŒå…¥ä¼ é€’ç»™æ¨¡å‹ï¼š
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The [run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py)
    script can generate text with language embeddings using the `xlm-clm` checkpoints.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py)è„šæœ¬å¯ä»¥ä½¿ç”¨`xlm-clm`æ£€æŸ¥ç‚¹ç”Ÿæˆå¸¦æœ‰è¯­è¨€åµŒå…¥çš„æ–‡æœ¬ã€‚'
- en: XLM without language embeddings
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²¡æœ‰è¯­è¨€åµŒå…¥çš„XLM
- en: 'The following XLM models do not require language embeddings during inference:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹XLMæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦è¯­è¨€åµŒå…¥ï¼š
- en: '`xlm-mlm-17-1280` (Masked language modeling, 17 languages)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-17-1280`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œ17ç§è¯­è¨€ï¼‰'
- en: '`xlm-mlm-100-1280` (Masked language modeling, 100 languages)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-mlm-100-1280`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œ100ç§è¯­è¨€ï¼‰'
- en: These models are used for generic sentence representations, unlike the previous
    XLM checkpoints.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹ç”¨äºé€šç”¨å¥å­è¡¨ç¤ºï¼Œä¸åŒäºä¹‹å‰çš„XLMæ£€æŸ¥ç‚¹ã€‚
- en: BERT
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT
- en: 'The following BERT models can be used for multilingual tasks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹BERTæ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼š
- en: '`bert-base-multilingual-uncased` (Masked language modeling + Next sentence
    prediction, 102 languages)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-base-multilingual-uncased`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡+ä¸‹ä¸€å¥é¢„æµ‹ï¼Œ102ç§è¯­è¨€ï¼‰'
- en: '`bert-base-multilingual-cased` (Masked language modeling + Next sentence prediction,
    104 languages)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-base-multilingual-cased`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡+ä¸‹ä¸€å¥é¢„æµ‹ï¼Œ104ç§è¯­è¨€ï¼‰'
- en: These models do not require language embeddings during inference. They should
    identify the language from the context and infer accordingly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦è¯­è¨€åµŒå…¥ã€‚å®ƒä»¬åº”è¯¥æ ¹æ®ä¸Šä¸‹æ–‡è¯†åˆ«è¯­è¨€å¹¶ç›¸åº”åœ°æ¨æ–­ã€‚
- en: XLM-RoBERTa
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLM-RoBERTa
- en: 'The following XLM-RoBERTa models can be used for multilingual tasks:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹XLM-RoBERTaæ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼š
- en: '`xlm-roberta-base` (Masked language modeling, 100 languages)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-roberta-base`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œ100ç§è¯­è¨€ï¼‰'
- en: '`xlm-roberta-large` (Masked language modeling, 100 languages)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlm-roberta-large`ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œ100ç§è¯­è¨€ï¼‰'
- en: XLM-RoBERTa was trained on 2.5TB of newly created and cleaned CommonCrawl data
    in 100 languages. It provides strong gains over previously released multilingual
    models like mBERT or XLM on downstream tasks like classification, sequence labeling,
    and question answering.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-RoBERTaåœ¨100ç§è¯­è¨€ä¸­æ–°åˆ›å»ºå’Œæ¸…ç†çš„2.5TB CommonCrawlæ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚åœ¨åˆ†ç±»ã€åºåˆ—æ ‡è®°å’Œé—®é¢˜å›ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œå®ƒæ¯”ä»¥å‰å‘å¸ƒçš„å¤šè¯­è¨€æ¨¡å‹å¦‚mBERTæˆ–XLMæä¾›äº†å¼ºå¤§çš„æ€§èƒ½æå‡ã€‚
- en: M2M100
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: M2M100
- en: 'The following M2M100 models can be used for multilingual translation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹M2M100æ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ç¿»è¯‘ï¼š
- en: '`facebook/m2m100_418M` (Translation)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/m2m100_418M`ï¼ˆç¿»è¯‘ï¼‰'
- en: '`facebook/m2m100_1.2B` (Translation)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/m2m100_1.2B`ï¼ˆç¿»è¯‘ï¼‰'
- en: 'In this example, load the `facebook/m2m100_418M` checkpoint to translate from
    Chinese to English. You can set the source language in the tokenizer:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒåŠ è½½`facebook/m2m100_418M`æ£€æŸ¥ç‚¹ä»¥å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ã€‚æ‚¨å¯ä»¥åœ¨æ ‡è®°å™¨ä¸­è®¾ç½®æºè¯­è¨€ï¼š
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Tokenize the text:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼š
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'M2M100 forces the target language id as the first generated token to translate
    to the target language. Set the `forced_bos_token_id` to `en` in the `generate`
    method to translate to English:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: M2M100å¼ºåˆ¶å°†ç›®æ ‡è¯­è¨€IDä½œä¸ºç¬¬ä¸€ä¸ªç”Ÿæˆçš„æ ‡è®°ä»¥ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€ã€‚åœ¨`generate`æ–¹æ³•ä¸­å°†`forced_bos_token_id`è®¾ç½®ä¸º`en`ä»¥ç¿»è¯‘ä¸ºè‹±è¯­ï¼š
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: MBart
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MBart
- en: 'The following MBart models can be used for multilingual translation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹MBartæ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ç¿»è¯‘ï¼š
- en: '`facebook/mbart-large-50-one-to-many-mmt` (One-to-many multilingual machine
    translation, 50 languages)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50-one-to-many-mmt`ï¼ˆä¸€å¯¹å¤šå¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œ50ç§è¯­è¨€ï¼‰'
- en: '`facebook/mbart-large-50-many-to-many-mmt` (Many-to-many multilingual machine
    translation, 50 languages)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50-many-to-many-mmt`ï¼ˆå¤šå¯¹å¤šå¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œ50ç§è¯­è¨€ï¼‰'
- en: '`facebook/mbart-large-50-many-to-one-mmt` (Many-to-one multilingual machine
    translation, 50 languages)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50-many-to-one-mmt`ï¼ˆå¤šå¯¹ä¸€å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œ50ç§è¯­è¨€ï¼‰'
- en: '`facebook/mbart-large-50` (Multilingual translation, 50 languages)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-50`ï¼ˆå¤šè¯­è¨€ç¿»è¯‘ï¼Œ50ç§è¯­è¨€ï¼‰'
- en: '`facebook/mbart-large-cc25`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`facebook/mbart-large-cc25`'
- en: 'In this example, load the `facebook/mbart-large-50-many-to-many-mmt` checkpoint
    to translate Finnish to English. You can set the source language in the tokenizer:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼ŒåŠ è½½`facebook/mbart-large-50-many-to-many-mmt`æ£€æŸ¥ç‚¹ä»¥å°†èŠ¬å…°è¯­ç¿»è¯‘ä¸ºè‹±è¯­ã€‚æ‚¨å¯ä»¥åœ¨æ ‡è®°å™¨ä¸­è®¾ç½®æºè¯­è¨€ï¼š
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Tokenize the text:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼š
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'MBart forces the target language id as the first generated token to translate
    to the target language. Set the `forced_bos_token_id` to `en` in the `generate`
    method to translate to English:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MBartå¼ºåˆ¶å°†ç›®æ ‡è¯­è¨€IDä½œä¸ºç¬¬ä¸€ä¸ªç”Ÿæˆçš„æ ‡è®°ä»¥ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€ã€‚åœ¨`generate`æ–¹æ³•ä¸­å°†`forced_bos_token_id`è®¾ç½®ä¸º`en`ä»¥ç¿»è¯‘ä¸ºè‹±è¯­ï¼š
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you are using the `facebook/mbart-large-50-many-to-one-mmt` checkpoint, you
    donâ€™t need to force the target language id as the first generated token otherwise
    the usage is the same.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨`facebook/mbart-large-50-many-to-one-mmt`æ£€æŸ¥ç‚¹ï¼Œåˆ™ä¸éœ€è¦å¼ºåˆ¶ç›®æ ‡è¯­è¨€IDä½œä¸ºç¬¬ä¸€ä¸ªç”Ÿæˆçš„æ ‡è®°ï¼Œå¦åˆ™ç”¨æ³•ç›¸åŒã€‚
