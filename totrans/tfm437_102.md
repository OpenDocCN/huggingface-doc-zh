# 固定长度模型的困惑度

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/perplexity](https://huggingface.co/docs/transformers/v4.37.2/en/perplexity)

困惑度（PPL）是评估语言模型最常见的指标之一。在深入讨论之前，我们应该注意，该指标特别适用于传统语言模型（有时称为自回归或因果语言模型），对于像BERT这样的掩码语言模型，该指标并不明确定义（请参阅[模型摘要](model_summary)）。

困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化的序列<math><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X = (x_0, x_1, \dots, x_t)</annotation></semantics></math>X=(x0​,x1​,…,xt​)，那么<math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math>X的困惑度为，<math display="block"><semantics><mrow><mtext>PPL</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">{</mo><mrow><mo>−</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><munderover><mo>∑</mo><mi>i</mi><mi>t</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo><</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{PPL}(X) = \exp \left\{ {-\frac{1}{t}\sum_i^t \log p_\theta (x_i|x_{<i}) } \right\}</annotation></semantics></math>PPL(X)=exp{−t1​i∑t​logpθ​(xi​∣x<i​)}

其中<math><semantics><mrow><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo><</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p_\theta (x_i|x_{<i})</annotation></semantics></math>是第i个标记在我们模型的先前标记<math><semantics><mrow><msub><mi>x</mi><mrow><mo><</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{<i}</annotation></semantics></math>的对数似然。直观地，它可以被视为模型在语料库中一组指定标记中均匀预测的评估。重要的是，这意味着标记化过程对模型的困惑度有直接影响，比较不同模型时应始终考虑这一点。

这也等同于数据和模型预测之间交叉熵的指数。关于困惑度及其与每字符比特（BPC）和数据压缩的关系的更多直觉，请查看[The Gradient](https://thegradient.pub/understanding-evaluation-metrics-for-language-models)上的这篇[精彩博客文章](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)。

## 使用固定长度模型计算PPL

如果我们不受模型上下文大小的限制，我们将通过自回归地分解序列并在每一步上都对整个先前子序列进行条件评估模型的困惑度，如下所示。

![具有无限上下文长度的序列的完全分解](../Images/c3c9c9f2fa6dbedcb4e30aa153057b06.png)

然而，在使用近似模型时，通常会对模型可以处理的标记数量有限制。例如，[GPT-2](model_doc/gpt2)的最大版本具有固定长度的1024个标记，因此当t大于1024时，我们无法直接计算pθ​(xt​∣x<t​)。

相反，序列通常被分成与模型最大输入大小相等的子序列。如果模型的最大输入大小为k，那么我们通过仅在之前的k-1个标记上进行条件化来近似标记xt​的可能性，而不是整个上下文。在评估序列的模型困惑度时，一种诱人但次优的方法是将序列分成不相交的块，并独立地将每个段的分解对数似然相加。

不利用完全可用上下文的次优PPL

这种方法计算快速，因为每个段的困惑度可以在一个前向传递中计算，但作为完全因子化困惑度的一个很差的近似，并且通常会产生更高（更差）的PPL，因为模型在大多数预测步骤中将具有更少的上下文。

相反，应该使用滑动窗口策略评估固定长度模型的PPL。这涉及反复滑动上下文窗口，以便模型在进行每个预测时具有更多上下文。

利用所有可用上下文的滑动窗口PPL

这是对序列概率的真实分解的更接近近似，并且通常会产生更有利的分数。缺点是它需要为语料库中的每个标记进行单独的前向传递。一个很好的实际折衷方案是使用跨度滑动窗口，通过更大的跨度移动上下文，而不是每次滑动一个标记。这样可以使计算速度更快，同时仍然使模型在每一步中具有更大的上下文来进行预测。

## 示例：在🤗 Transformers中使用GPT-2计算困惑度

让我们用GPT-2演示这个过程。

```py
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = "cuda"
model_id = "gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

我们将加载WikiText-2数据集，并使用几种不同的滑动窗口策略评估困惑度。由于这个数据集很小，我们只需对整个数据集进行一次前向传递，因此可以将整个数据集加载和编码到内存中。

```py
from datasets import load_dataset

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encodings = tokenizer("\n\n".join(test["text"]), return_tensors="pt")
```

使用🤗 Transformers，我们可以简单地将`input_ids`作为`labels`传递给我们的模型，每个标记的平均负对数似然将作为损失返回。然而，使用我们的滑动窗口方法，在每次迭代中传递给模型的标记存在重叠。我们不希望将我们只将其视为上下文的标记的对数似然包括在我们的损失中，因此我们可以将这些目标设置为`-100`，以便忽略它们。以下是我们如何使用步幅为`512`的示例。这意味着在计算任何一个标记的条件概率时，模型将至少有512个标记的上下文（前提是有512个先前的标记可用于条件）。

```py
import torch
from tqdm import tqdm

max_length = model.config.n_positions
stride = 512
seq_len = encodings.input_ids.size(1)

nlls = []
prev_end_loc = 0
for begin_loc in tqdm(range(0, seq_len, stride)):
    end_loc = min(begin_loc + max_length, seq_len)
    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -100

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        # loss is calculated using CrossEntropyLoss which averages over valid labels
        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
        # to the left by 1.
        neg_log_likelihood = outputs.loss

    nlls.append(neg_log_likelihood)

    prev_end_loc = end_loc
    if end_loc == seq_len:
        break

ppl = torch.exp(torch.stack(nlls).mean())
```

将步长设置为最大输入长度时运行此操作等同于我们上面讨论的次优、非滑动窗口策略。步长越小，模型在进行每次预测时获得的上下文就越多，通常报告的困惑度也会更好。

当我们使用 `stride = 1024`，即没有重叠时，得到的困惑度为 `19.44`，与 GPT-2 论文中报告的 `19.93` 差不多。通过使用 `stride = 512`，从而采用我们的滑动窗口策略，这个值降至 `16.45`。这不仅是一个更有利的分数，而且计算方式更接近于序列可能性的真实自回归分解。
