# Beam数据集

> 原文链接：[https://huggingface.co/docs/datasets/beam](https://huggingface.co/docs/datasets/beam)

一些数据集太大，无法在单台机器上处理。相反，您可以使用[Apache Beam](https://beam.apache.org/)进行处理，这是一个用于并行数据处理的库。处理管道在分布式处理后端上执行，如[Apache Flink](https://flink.apache.org/)、[Apache Spark](https://spark.apache.org/)或[Google Cloud Dataflow](https://cloud.google.com/dataflow)。

我们已经为一些较大的数据集（如[wikipedia](https://huggingface.co/datasets/wikipedia)和[wiki40b](https://huggingface.co/datasets/wiki40b)）创建了Beam管道。您可以使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)正常加载这些数据集。但是，如果您想要在Dataflow中运行自己的Beam管道，可以按照以下步骤操作：

1.  指定您要处理的数据集和配置：

```py
DATASET_NAME=your_dataset_name  # ex: wikipedia
CONFIG_NAME=your_config_name    # ex: 20220301.en
```

1.  输入您的Google Cloud Platform信息：

```py
PROJECT=your_project
BUCKET=your_bucket
REGION=your_region
```

1.  指定您的Python要求：

```py
echo "datasets" > /tmp/beam_requirements.txt
echo "apache_beam" >> /tmp/beam_requirements.txt
```

1.  运行管道：

```py
datasets-cli run_beam datasets/$DATASET_NAME \
--name $CONFIG_NAME \
--save_info \
--cache_dir gs://$BUCKET/cache/datasets \
--beam_pipeline_options=\
"runner=DataflowRunner,project=$PROJECT,job_name=$DATASET_NAME-gen,"\
"staging_location=gs://$BUCKET/binaries,temp_location=gs://$BUCKET/temp,"\
"region=$REGION,requirements_file=/tmp/beam_requirements.txt"
```

运行管道时，您可以调整参数以更改运行程序（Flink或Spark）、输出位置（S3存储桶或HDFS）和工作人员数量。
