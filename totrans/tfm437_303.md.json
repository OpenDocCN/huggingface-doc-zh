["```py\n( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 patch_size = 16 qkv_bias = True frequency_stride = 10 time_stride = 10 max_length = 1024 num_mel_bins = 128 **kwargs )\n```", "```py\n>>> from transformers import ASTConfig, ASTModel\n\n>>> # Initializing a AST MIT/ast-finetuned-audioset-10-10-0.4593 style configuration\n>>> configuration = ASTConfig()\n\n>>> # Initializing a model (with random weights) from the MIT/ast-finetuned-audioset-10-10-0.4593 style configuration\n>>> model = ASTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( feature_size = 1 sampling_rate = 16000 num_mel_bins = 128 max_length = 1024 padding_value = 0.0 do_normalize = True mean = -4.2677393 std = 4.5689974 return_attention_mask = False **kwargs )\n```", "```py\n( raw_speech: Union sampling_rate: Optional = None return_tensors: Union = None **kwargs )\n```", "```py\n( config: ASTConfig )\n```", "```py\n( input_values: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, ASTModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n>>> model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 1214, 768]\n```", "```py\n( config: ASTConfig )\n```", "```py\n( input_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, ASTForAudioClassification\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n>>> model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.argmax(logits, dim=-1).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'Speech'\n\n>>> # compute loss - target_label is e.g. \"down\"\n>>> target_label = model.config.id2label[0]\n>>> inputs[\"labels\"] = torch.tensor([model.config.label2id[target_label]])\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n0.17\n```"]