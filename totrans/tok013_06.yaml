- en: Components
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»„ä»¶
- en: 'Original text: [https://huggingface.co/docs/tokenizers/components](https://huggingface.co/docs/tokenizers/components)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/tokenizers/components](https://huggingface.co/docs/tokenizers/components)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: When building a Tokenizer, you can attach various types of components to this
    Tokenizer in order to customize its behavior. This page lists most provided components.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºåˆ†è¯å™¨æ—¶ï¼Œæ‚¨å¯ä»¥é™„åŠ å„ç§ç±»å‹çš„ç»„ä»¶åˆ°è¿™ä¸ªåˆ†è¯å™¨ï¼Œä»¥å®šåˆ¶å…¶è¡Œä¸ºã€‚æœ¬é¡µåˆ—å‡ºäº†å¤§å¤šæ•°æä¾›çš„ç»„ä»¶ã€‚
- en: Normalizers
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§„èŒƒåŒ–å™¨
- en: A `Normalizer` is in charge of pre-processing the input string in order to normalize
    it as relevant for a given use case. Some common examples of normalization are
    the Unicode normalization algorithms (NFD, NFKD, NFC & NFKC), lowercasing etcâ€¦
    The specificity of `tokenizers` is that we keep track of the alignment while normalizing.
    This is essential to allow mapping from the generated tokens back to the input
    text.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalizer` è´Ÿè´£é¢„å¤„ç†è¾“å…¥å­—ç¬¦ä¸²ï¼Œä»¥ä½¿å…¶åœ¨ç»™å®šç”¨ä¾‹ä¸­è§„èŒƒåŒ–ã€‚ä¸€äº›å¸¸è§çš„è§„èŒƒåŒ–ç¤ºä¾‹æ˜¯ Unicode è§„èŒƒåŒ–ç®—æ³•ï¼ˆNFDã€NFKDã€NFC
    å’Œ NFKCï¼‰ã€å°å†™åŒ–ç­‰ç­‰... `tokenizers` çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºæˆ‘ä»¬åœ¨è§„èŒƒåŒ–æ—¶ä¿æŒå¯¹é½ã€‚è¿™å¯¹äºå…è®¸ä»ç”Ÿæˆçš„æ ‡è®°æ˜ å°„å›è¾“å…¥æ–‡æœ¬æ˜¯è‡³å…³é‡è¦çš„ã€‚'
- en: The `Normalizer` is optional.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalizer` æ˜¯å¯é€‰çš„ã€‚'
- en: PythonRustNode
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '| Name | Description | Example |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | æè¿° | ç¤ºä¾‹ |'
- en: '| :-- | :-- | :-- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- |'
- en: '| NFD | NFD unicode normalization |  |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| NFD | NFD Unicode è§„èŒƒåŒ– |  |'
- en: '| NFKD | NFKD unicode normalization |  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| NFKD | NFKD Unicode è§„èŒƒåŒ– |  |'
- en: '| NFC | NFC unicode normalization |  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| NFC | NFC Unicode è§„èŒƒåŒ– |  |'
- en: '| NFKC | NFKC unicode normalization |  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| NFKC | NFKC Unicode è§„èŒƒåŒ– |  |'
- en: '| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO á½ˆÎ”Î¥Î£Î£Î•ÎÎ£`
    Output: `hello`á½€Î´Ï…ÏƒÏƒÎµÏÏ‚` |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| Lowercase | å°†æ‰€æœ‰å¤§å†™å­—æ¯æ›¿æ¢ä¸ºå°å†™å­—æ¯ | è¾“å…¥ï¼š`HELLO á½ˆÎ”Î¥Î£Î£Î•ÎÎ£` è¾“å‡ºï¼š`hello`á½€Î´Ï…ÏƒÏƒÎµÏÏ‚` |'
- en: '| Strip | Removes all whitespace characters on the specified sides (left, right
    or both) of the input | Input: `"`hi`"` Output: `"hi"` |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Strip | ç§»é™¤è¾“å…¥çš„æŒ‡å®šä¾§ï¼ˆå·¦ä¾§ã€å³ä¾§æˆ–ä¸¤ä¾§ï¼‰çš„æ‰€æœ‰ç©ºç™½å­—ç¬¦ | è¾“å…¥ï¼š`"`hi`"` è¾“å‡ºï¼š`"hi"` |'
- en: '| StripAccents | Removes all accent symbols in unicode (to be used with NFD
    for consistency) | Input: `Ã©` Ouput: `e` |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| StripAccents | ç§»é™¤ Unicode ä¸­çš„æ‰€æœ‰é‡éŸ³ç¬¦å·ï¼ˆä¸ NFD ä¸€èµ·ä½¿ç”¨ä»¥ä¿æŒä¸€è‡´æ€§ï¼‰ | è¾“å…¥ï¼š`Ã©` è¾“å‡ºï¼š`e` |'
- en: '| Replace | Replaces a custom string or regexp and changes it with given content
    | `Replace("a", "e")` will behave like this: Input: `"banana"`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '| Replace | æ›¿æ¢è‡ªå®šä¹‰å­—ç¬¦ä¸²æˆ–æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¹¶ç”¨ç»™å®šå†…å®¹æ›´æ”¹å®ƒ | `Replace("a", "e")` çš„è¡Œä¸ºå¦‚ä¸‹ï¼šè¾“å…¥ï¼š"banana"'
- en: 'Ouput: `"benene"` |'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š"benene" |
- en: '| BertNormalizer | Provides an implementation of the Normalizer used in the
    original BERT. Options that can be set are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '| BertNormalizer | æä¾›äº†åŸå§‹ BERT ä¸­ä½¿ç”¨çš„è§„èŒƒåŒ–å™¨çš„å®ç°ã€‚å¯ä»¥è®¾ç½®çš„é€‰é¡¹æœ‰ï¼š'
- en: clean_text
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: clean_text
- en: handle_chinese_chars
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: handle_chinese_chars
- en: strip_accents
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: strip_accents
- en: lowercase
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lowercase
- en: '|  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Sequence | Composes multiple normalizers that will run in the provided order
    | `Sequence([NFKC(), Lowercase()])` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Sequence | ç»„åˆå¤šä¸ªè§„èŒƒåŒ–å™¨ï¼ŒæŒ‰æä¾›çš„é¡ºåºè¿è¡Œ | `Sequence([NFKC(), Lowercase()])` |'
- en: Pre-tokenizers
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„åˆ†è¯å™¨
- en: The `PreTokenizer` takes care of splitting the input according to a set of rules.
    This pre-processing lets you ensure that the underlying `Model` does not build
    tokens across multiple â€œsplitsâ€. For example if you donâ€™t want to have whitespaces
    inside a token, then you can have a `PreTokenizer` that splits on these whitespaces.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreTokenizer` è´Ÿè´£æ ¹æ®ä¸€ç»„è§„åˆ™æ‹†åˆ†è¾“å…¥ã€‚è¿™ç§é¢„å¤„ç†å¯ä»¥ç¡®ä¿åº•å±‚çš„ `Model` ä¸ä¼šåœ¨å¤šä¸ªâ€œåˆ†å‰²â€ä¹‹é—´æ„å»ºæ ‡è®°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä¸å¸Œæœ›åœ¨æ ‡è®°å†…éƒ¨æœ‰ç©ºæ ¼ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåœ¨è¿™äº›ç©ºæ ¼ä¸Šæ‹†åˆ†çš„
    `PreTokenizer`ã€‚'
- en: You can easily combine multiple `PreTokenizer` together using a `Sequence` (see
    below). The `PreTokenizer` is also allowed to modify the string, just like a `Normalizer`
    does. This is necessary to allow some complicated algorithms that require to split
    before normalizing (e.g. the ByteLevel)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ `Sequence` è½»æ¾åœ°å°†å¤šä¸ª `PreTokenizer` ç»„åˆåœ¨ä¸€èµ·ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚`PreTokenizer` ä¹Ÿå…è®¸ä¿®æ”¹å­—ç¬¦ä¸²ï¼Œå°±åƒ
    `Normalizer` ä¸€æ ·ã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œä»¥å…è®¸ä¸€äº›å¤æ‚çš„ç®—æ³•åœ¨è§„èŒƒåŒ–ä¹‹å‰è¿›è¡Œæ‹†åˆ†ï¼ˆä¾‹å¦‚ ByteLevelï¼‰
- en: PythonRustNode
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '| Name | Description | Example |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | æè¿° | ç¤ºä¾‹ |'
- en: '| :-- | :-- | :-- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- |'
- en: '| ByteLevel | Splits on whitespaces while remapping all the bytes to a set
    of visible characters. This technique as been introduced by OpenAI with GPT-2
    and has some more or less nice properties:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '| ByteLevel | åœ¨ç©ºæ ¼ä¸Šæ‹†åˆ†ï¼ŒåŒæ—¶å°†æ‰€æœ‰å­—èŠ‚é‡æ–°æ˜ å°„ä¸ºä¸€ç»„å¯è§å­—ç¬¦ã€‚è¿™ç§æŠ€æœ¯ç”± OpenAI ä¸ GPT-2 ä¸€èµ·å¼•å…¥ï¼Œå…·æœ‰ä¸€äº›æ›´æˆ–å¤šæˆ–å°‘å¥½çš„ç‰¹æ€§ï¼š'
- en: Since it maps on bytes, a tokenizer using this only requires **256** characters
    as initial alphabet (the number of values a byte can have), as opposed to the
    130,000+ Unicode characters.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºå®ƒæ˜ å°„åœ¨å­—èŠ‚ä¸Šï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•çš„åˆ†è¯å™¨åªéœ€è¦ **256** ä¸ªå­—ç¬¦ä½œä¸ºåˆå§‹å­—æ¯è¡¨ï¼ˆä¸€ä¸ªå­—èŠ‚å¯ä»¥æœ‰çš„å€¼çš„æ•°é‡ï¼‰ï¼Œè€Œä¸æ˜¯ 130,000 å¤šä¸ª Unicode
    å­—ç¬¦ã€‚
- en: A consequence of the previous point is that it is absolutely unnecessary to
    have an unknown token using this since we can represent anything with 256 tokens
    (Youhou!! ğŸ‰ğŸ‰)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰é¢ä¸€ç‚¹çš„ç»“æœæ˜¯ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•å®Œå…¨ä¸éœ€è¦ä½¿ç”¨æœªçŸ¥æ ‡è®°ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç”¨ 256 ä¸ªæ ‡è®°è¡¨ç¤ºä»»ä½•å†…å®¹ï¼ˆYouhou!! ğŸ‰ğŸ‰ï¼‰
- en: For non ascii characters, it gets completely unreadable, but it works nonetheless!
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºé ASCII å­—ç¬¦ï¼Œå®ƒå˜å¾—å®Œå…¨ä¸å¯è¯»ï¼Œä½†å®ƒä»ç„¶æœ‰æ•ˆï¼
- en: '| Input: `"Hello my friend, how are you?"` Ouput: `"Hello", "Ä my", Ä friend",
    ",", "Ä how", "Ä are", "Ä you", "?"` |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| è¾“å…¥ï¼š"Hello my friend, how are you?" è¾“å‡ºï¼š"Hello", "Ä my", Ä friend", ",", "Ä how",
    "Ä are", "Ä you", "?" |'
- en: '| Whitespace | Splits on word boundaries (using the following regular expression:
    `\w+&#124;[^\w\s]+` | Input: `"Hello there!"` Output: `"Hello", "there", "!"`
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Whitespace | åœ¨å•è¯è¾¹ç•Œä¸Šæ‹†åˆ†ï¼ˆä½¿ç”¨ä»¥ä¸‹æ­£åˆ™è¡¨è¾¾å¼ï¼š`\w+&#124;[^\w\s]+` | è¾“å…¥ï¼š"Hello there!" è¾“å‡ºï¼š"Hello",
    "there", "!" |'
- en: '| WhitespaceSplit | Splits on any whitespace character | Input: `"Hello there!"`
    Output: `"Hello", "there!"` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| WhitespaceSplit | ä»¥ä»»ä½•ç©ºç™½å­—ç¬¦åˆ†å‰² | è¾“å…¥ï¼š"Hello there!" è¾“å‡ºï¼š"Hello", "there!" |'
- en: '| Punctuation | Will isolate all punctuation characters | Input: `"Hello?"`
    Ouput: `"Hello", "?"` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Punctuation | å°†å­¤ç«‹æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å­—ç¬¦ | è¾“å…¥ï¼š"Hello?" è¾“å‡ºï¼š"Hello", "?" |'
- en: '| Metaspace | Splits on whitespaces and replaces them with a special char â€œâ–â€
    (U+2581) | Input: `"Hello there"` Ouput: `"Hello", "â–there"` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Metaspace | åœ¨ç©ºæ ¼ä¸Šæ‹†åˆ†å¹¶ç”¨ç‰¹æ®Šå­—ç¬¦â€œâ–â€ï¼ˆU+2581ï¼‰æ›¿æ¢å®ƒä»¬ | è¾“å…¥ï¼š"Hello there" è¾“å‡ºï¼š"Hello", "â–there"
    |'
- en: '| CharDelimiterSplit | Splits on a given character | Example with `x`: Input:
    `"Helloxthere"`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '| CharDelimiterSplit | åœ¨ç»™å®šå­—ç¬¦ä¸Šæ‹†åˆ† | ä»¥ `x` ä¸ºä¾‹ï¼šè¾“å…¥ï¼š"Helloxthere"'
- en: 'Ouput: `"Hello", "there"` |'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š"Hello", "there" |
- en: '| Digits | Splits the numbers from any other characters. | Input: `"Hello123there"`
    Output: `"Hello", "123", "there"` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| æ•°å­— | å°†æ•°å­—ä¸å…¶ä»–å­—ç¬¦åˆ†å¼€ã€‚ | è¾“å…¥ï¼š`"Hello123there"` è¾“å‡ºï¼š`"Hello"ï¼Œ"123"ï¼Œ"there"` |'
- en: '| Split | Versatile pre-tokenizer that splits on provided pattern and according
    to provided behavior. The pattern can be inverted if necessary.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '| åˆ†å‰² | å¤šåŠŸèƒ½çš„é¢„åˆ†è¯å™¨ï¼Œæ ¹æ®æä¾›çš„æ¨¡å¼å’Œè¡Œä¸ºè¿›è¡Œåˆ†å‰’ã€‚å¦‚æœéœ€è¦ï¼Œå¯ä»¥åè½¬æ¨¡å¼ã€‚'
- en: pattern should be either a custom string or regexp.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å¼åº”è¯¥æ˜¯è‡ªå®šä¹‰å­—ç¬¦ä¸²æˆ–æ­£åˆ™è¡¨è¾¾å¼ã€‚
- en: 'behavior should be one of:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡Œä¸ºåº”è¯¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š
- en: removed
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·²ç§»é™¤
- en: isolated
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¤ç«‹
- en: merged_with_previous
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ä¸Šä¸€ä¸ªåˆå¹¶
- en: merged_with_next
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ä¸‹ä¸€ä¸ªåˆå¹¶
- en: contiguous
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿ç»­çš„
- en: invert should be a boolean flag.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: invertåº”è¯¥æ˜¯ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ã€‚
- en: '| Example with pattern = `, behavior = `"isolated"`, invert = `False`: Input:
    `"Hello, how are you?"`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '| ä¾‹å¦‚ï¼Œä½¿ç”¨æ¨¡å¼=`,`ï¼Œè¡Œä¸º=`"isolated"`ï¼Œinvert=`False`ï¼šè¾“å…¥ï¼š`"ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"`'
- en: 'Output: `"Hello,", " ", "how", " ", "are", " ", "you?"`` |'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š`"ä½ å¥½,"ï¼Œ" "ï¼Œ"ä½ å¥½"ï¼Œ" "ï¼Œ"ä½ å¥½"ï¼Œ" "ï¼Œ"ä½ å¥½ï¼Ÿ"` |
- en: '| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the
    given order | `Sequence([Punctuation(), WhitespaceSplit()])` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| åºåˆ— | å…è®¸æ‚¨ç»„åˆå¤šä¸ªå°†æŒ‰ç»™å®šé¡ºåºè¿è¡Œçš„`PreTokenizer` | `Sequence([Punctuation(), WhitespaceSplit()])`
    |'
- en: Models
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: Models are the core algorithms used to actually tokenize, and therefore, they
    are the only mandatory component of a Tokenizer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯å®é™…æ ‡è®°åŒ–æ‰€ä½¿ç”¨çš„æ ¸å¿ƒç®—æ³•ï¼Œå› æ­¤å®ƒä»¬æ˜¯Tokenizerçš„å”¯ä¸€å¼ºåˆ¶ç»„ä»¶ã€‚
- en: '| Name | Description |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | æè¿° |'
- en: '| :-- | :-- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- |'
- en: '| WordLevel | This is the â€œclassicâ€ tokenization algorithm. It letâ€™s you simply
    map words to IDs without anything fancy. This has the advantage of being really
    simple to use and understand, but it requires extremely large vocabularies for
    a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice
    will be made by this model directly, it simply maps input tokens to IDs. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| WordLevel | è¿™æ˜¯â€œç»å…¸â€æ ‡è®°åŒ–ç®—æ³•ã€‚å®ƒè®©æ‚¨ç®€å•åœ°å°†å•è¯æ˜ å°„åˆ°IDï¼Œè€Œæ— éœ€ä»»ä½•èŠ±å“¨çš„ä¸œè¥¿ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯éå¸¸ç®€å•æ˜“ç”¨ï¼Œä½†éœ€è¦æå¤§çš„è¯æ±‡é‡æ‰èƒ½è·å¾—è‰¯å¥½çš„è¦†ç›–ç‡ã€‚ä½¿ç”¨æ­¤`Model`éœ€è¦ä½¿ç”¨`PreTokenizer`ã€‚æ­¤æ¨¡å‹ä¸ä¼šç›´æ¥åšå‡ºé€‰æ‹©ï¼Œåªæ˜¯å°†è¾“å…¥æ ‡è®°æ˜ å°„åˆ°IDã€‚
    |'
- en: '| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding
    works by starting with characters, while merging those that are the most frequently
    seen together, thus creating new tokens. It then works iteratively to build new
    tokens out of the most frequent pairs it sees in a corpus. BPE is able to build
    words it has never seen by using multiple subword tokens, and thus requires smaller
    vocabularies, with less chances of having â€œunkâ€ (unknown) tokens. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| BPE | æœ€æµè¡Œçš„å­è¯æ ‡è®°åŒ–ç®—æ³•ä¹‹ä¸€ã€‚å­—èŠ‚å¯¹ç¼–ç é€šè¿‡ä»å­—ç¬¦å¼€å§‹ï¼Œåˆå¹¶æœ€å¸¸ä¸€èµ·å‡ºç°çš„å­—ç¬¦ï¼Œä»è€Œåˆ›å»ºæ–°æ ‡è®°ã€‚ç„¶åï¼Œå®ƒè¿­ä»£åœ°æ„å»ºæ–°æ ‡è®°ï¼Œä½¿ç”¨è¯­æ–™åº“ä¸­çœ‹åˆ°çš„æœ€å¸¸è§çš„å¯¹ã€‚BPEèƒ½å¤Ÿé€šè¿‡ä½¿ç”¨å¤šä¸ªå­è¯æ ‡è®°æ„å»ºä»æœªè§è¿‡çš„å•è¯ï¼Œå› æ­¤éœ€è¦æ›´å°çš„è¯æ±‡é‡ï¼Œå‡å°‘å‡ºç°â€œunkâ€ï¼ˆæœªçŸ¥ï¼‰æ ‡è®°çš„æœºä¼šã€‚
    |'
- en: '| WordPiece | This is a subword tokenization algorithm quite similar to BPE,
    used mainly by Google in models like BERT. It uses a greedy algorithm, that tries
    to build long words first, splitting in multiple tokens when entire words donâ€™t
    exist in the vocabulary. This is different from BPE that starts from characters,
    building bigger tokens as possible. It uses the famous `##` prefix to identify
    tokens that are part of a word (ie not starting a word). |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| WordPiece | è¿™æ˜¯ä¸€ç§å­è¯æ ‡è®°åŒ–ç®—æ³•ï¼Œä¸BPEéå¸¸ç›¸ä¼¼ï¼Œä¸»è¦ç”±è°·æ­Œåœ¨BERTç­‰æ¨¡å‹ä¸­ä½¿ç”¨ã€‚å®ƒä½¿ç”¨ä¸€ç§è´ªå©ªç®—æ³•ï¼Œè¯•å›¾é¦–å…ˆæ„å»ºé•¿å•è¯ï¼Œåœ¨æ•´ä¸ªè¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨æ•´ä¸ªå•è¯æ—¶æ‹†åˆ†ä¸ºå¤šä¸ªæ ‡è®°ã€‚è¿™ä¸BPEä¸åŒï¼ŒBPEä»å­—ç¬¦å¼€å§‹æ„å»ºæ›´å¤§çš„æ ‡è®°ã€‚å®ƒä½¿ç”¨è‘—åçš„`##`å‰ç¼€æ¥è¯†åˆ«ä½œä¸ºå•è¯ä¸€éƒ¨åˆ†çš„æ ‡è®°ï¼ˆå³ä¸æ˜¯å•è¯çš„å¼€å¤´ï¼‰ã€‚'
- en: '| Unigram | Unigram is also a subword tokenization algorithm, and works by
    trying to identify the best set of subword tokens to maximize the probability
    for a given sentence. This is different from BPE in the way that this is not deterministic
    based on a set of rules applied sequentially. Instead Unigram will be able to
    compute multiple ways of tokenizing, while choosing the most probable one. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Unigram | Unigramä¹Ÿæ˜¯ä¸€ç§å­è¯æ ‡è®°åŒ–ç®—æ³•ï¼Œé€šè¿‡å°è¯•è¯†åˆ«æœ€ä½³å­è¯æ ‡è®°é›†æ¥æœ€å¤§åŒ–ç»™å®šå¥å­çš„æ¦‚ç‡ã€‚è¿™ä¸BPEä¸åŒï¼Œå› ä¸ºå®ƒä¸æ˜¯åŸºäºä¸€ç»„æŒ‰é¡ºåºåº”ç”¨çš„è§„åˆ™è€Œç¡®å®šçš„ã€‚ç›¸åï¼ŒUnigramå°†èƒ½å¤Ÿè®¡ç®—å¤šç§æ ‡è®°åŒ–æ–¹å¼ï¼ŒåŒæ—¶é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ä¸€ç§ã€‚
    |'
- en: Post-Processors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå¤„ç†å™¨
- en: After the whole pipeline, we sometimes want to insert some special tokens before
    feed a tokenized string into a model like â€[CLS] My horse is amazing [SEP]â€. The
    `PostProcessor` is the component doing just that.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªæµæ°´çº¿ä¹‹åï¼Œæœ‰æ—¶æˆ‘ä»¬å¸Œæœ›åœ¨å°†æ ‡è®°åŒ–çš„å­—ç¬¦ä¸²é¦ˆé€åˆ°æ¨¡å‹ä¹‹å‰æ’å…¥ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚â€œ[CLS]æˆ‘çš„é©¬å¾ˆæ£’[SEP]â€ã€‚`PostProcessor`å°±æ˜¯æ‰§è¡Œè¿™é¡¹ä»»åŠ¡çš„ç»„ä»¶ã€‚
- en: '| Name | Description | Example |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | æè¿° | ç¤ºä¾‹ |'
- en: '| :-- | :-- | :-- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- |'
- en: '| TemplateProcessing | Letâ€™s you easily template the post processing, adding
    special tokens, and specifying the `type_id` for each sequence/special token.
    The template is given two strings representing the single sequence and the pair
    of sequences, as well as a set of special tokens to use. | Example, when specifying
    a template with these values:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '| æ¨¡æ¿å¤„ç† | è®©æ‚¨è½»æ¾åœ°å¯¹åå¤„ç†è¿›è¡Œæ¨¡æ¿åŒ–ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œå¹¶ä¸ºæ¯ä¸ªåºåˆ—/ç‰¹æ®Šæ ‡è®°æŒ‡å®š`type_id`ã€‚æ¨¡æ¿æä¾›äº†ä¸¤ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºå•ä¸ªåºåˆ—å’Œä¸€å¯¹åºåˆ—ï¼Œä»¥åŠä¸€ç»„è¦ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ã€‚
    | ä¾‹å¦‚ï¼Œå½“æŒ‡å®šå…·æœ‰è¿™äº›å€¼çš„æ¨¡æ¿æ—¶ï¼š'
- en: 'single: `"[CLS] $A [SEP]"`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•ä¸ªï¼š`"[CLS] $A [SEP]"`
- en: 'pair: `"[CLS] $A [SEP] $B [SEP]"`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ï¼š`"[CLS] $A [SEP] $B [SEP]"`
- en: 'special tokens:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼š
- en: '`"[CLS]"`'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"[CLS]"`'
- en: '`"[SEP]"`'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"[SEP]"`'
- en: 'Input: `("I like this", "but not this")`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥ï¼š`("æˆ‘å–œæ¬¢è¿™ä¸ª"ï¼Œ"ä½†ä¸å–œæ¬¢è¿™ä¸ª")`
- en: 'Output: `"[CLS] I like this [SEP] but not this [SEP]"` |'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š`"[CLS] æˆ‘å–œæ¬¢è¿™ä¸ª [SEP] ä½†ä¸å–œæ¬¢è¿™ä¸ª [SEP]"` |
- en: Decoders
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£ç å™¨
- en: The Decoder knows how to go from the IDs used by the Tokenizer, back to a readable
    piece of text. Some `Normalizer` and `PreTokenizer` use special characters or
    identifiers that need to be reverted for example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨çŸ¥é“å¦‚ä½•ä»Tokenizerä½¿ç”¨çš„IDè½¬æ¢å›å¯è¯»çš„æ–‡æœ¬ã€‚ä¸€äº›`Normalizer`å’Œ`PreTokenizer`ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦æˆ–æ ‡è¯†ç¬¦ï¼Œéœ€è¦è¿›è¡Œåè½¬ã€‚
- en: '| Name | Description |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | æè¿° |'
- en: '| :-- | :-- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- |'
- en: '| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes
    at the byte-level, using a set of visible Unicode characters to represent each
    byte, so we need a Decoder to revert this process and get something readable again.
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ByteLevel | åè½¬ByteLevelé¢„åˆ†è¯å™¨ã€‚è¯¥é¢„åˆ†è¯å™¨åœ¨å­—èŠ‚çº§åˆ«è¿›è¡Œç¼–ç ï¼Œä½¿ç”¨ä¸€ç»„å¯è§çš„Unicodeå­—ç¬¦æ¥è¡¨ç¤ºæ¯ä¸ªå­—èŠ‚ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè§£ç å™¨æ¥æ¢å¤æ­¤è¿‡ç¨‹å¹¶å†æ¬¡è·å¾—å¯è¯»å†…å®¹ã€‚
    |'
- en: '| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a
    special identifer `â–` to identify whitespaces, and so this Decoder helps with
    decoding these. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Metaspace | åè½¬ Metaspace é¢„å¤„ç†å™¨ã€‚è¯¥é¢„å¤„ç†å™¨ä½¿ç”¨ç‰¹æ®Šæ ‡è¯†ç¬¦ `â–` æ¥è¯†åˆ«ç©ºæ ¼ï¼Œå› æ­¤è¯¥è§£ç å™¨æœ‰åŠ©äºè§£ç è¿™äº›ã€‚ |'
- en: '| WordPiece | Reverts the WordPiece Model. This model uses a special identifier
    `##` for continuing subwords, and so this Decoder helps with decoding these. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| WordPiece | åè½¬ WordPiece æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ç‰¹æ®Šæ ‡è¯†ç¬¦ `##` æ¥è¡¨ç¤ºç»§ç»­çš„å­è¯ï¼Œå› æ­¤è¯¥è§£ç å™¨æœ‰åŠ©äºè§£ç è¿™äº›ã€‚ |'
