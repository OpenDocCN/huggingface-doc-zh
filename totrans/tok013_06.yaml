- en: Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/components](https://huggingface.co/docs/tokenizers/components)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: When building a Tokenizer, you can attach various types of components to this
    Tokenizer in order to customize its behavior. This page lists most provided components.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `Normalizer` is in charge of pre-processing the input string in order to normalize
    it as relevant for a given use case. Some common examples of normalization are
    the Unicode normalization algorithms (NFD, NFKD, NFC & NFKC), lowercasing etc‚Ä¶
    The specificity of `tokenizers` is that we keep track of the alignment while normalizing.
    This is essential to allow mapping from the generated tokens back to the input
    text.
  prefs: []
  type: TYPE_NORMAL
- en: The `Normalizer` is optional.
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Description | Example |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| NFD | NFD unicode normalization |  |'
  prefs: []
  type: TYPE_TB
- en: '| NFKD | NFKD unicode normalization |  |'
  prefs: []
  type: TYPE_TB
- en: '| NFC | NFC unicode normalization |  |'
  prefs: []
  type: TYPE_TB
- en: '| NFKC | NFKC unicode normalization |  |'
  prefs: []
  type: TYPE_TB
- en: '| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO ·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£`
    Output: `hello`·ΩÄŒ¥œÖœÉœÉŒµœçœÇ` |'
  prefs: []
  type: TYPE_TB
- en: '| Strip | Removes all whitespace characters on the specified sides (left, right
    or both) of the input | Input: `"`hi`"` Output: `"hi"` |'
  prefs: []
  type: TYPE_TB
- en: '| StripAccents | Removes all accent symbols in unicode (to be used with NFD
    for consistency) | Input: `√©` Ouput: `e` |'
  prefs: []
  type: TYPE_TB
- en: '| Replace | Replaces a custom string or regexp and changes it with given content
    | `Replace("a", "e")` will behave like this: Input: `"banana"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ouput: `"benene"` |'
  prefs: []
  type: TYPE_NORMAL
- en: '| BertNormalizer | Provides an implementation of the Normalizer used in the
    original BERT. Options that can be set are:'
  prefs: []
  type: TYPE_NORMAL
- en: clean_text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: handle_chinese_chars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strip_accents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lowercase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | Composes multiple normalizers that will run in the provided order
    | `Sequence([NFKC(), Lowercase()])` |'
  prefs: []
  type: TYPE_TB
- en: Pre-tokenizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `PreTokenizer` takes care of splitting the input according to a set of rules.
    This pre-processing lets you ensure that the underlying `Model` does not build
    tokens across multiple ‚Äúsplits‚Äù. For example if you don‚Äôt want to have whitespaces
    inside a token, then you can have a `PreTokenizer` that splits on these whitespaces.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily combine multiple `PreTokenizer` together using a `Sequence` (see
    below). The `PreTokenizer` is also allowed to modify the string, just like a `Normalizer`
    does. This is necessary to allow some complicated algorithms that require to split
    before normalizing (e.g. the ByteLevel)
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Description | Example |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| ByteLevel | Splits on whitespaces while remapping all the bytes to a set
    of visible characters. This technique as been introduced by OpenAI with GPT-2
    and has some more or less nice properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Since it maps on bytes, a tokenizer using this only requires **256** characters
    as initial alphabet (the number of values a byte can have), as opposed to the
    130,000+ Unicode characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A consequence of the previous point is that it is absolutely unnecessary to
    have an unknown token using this since we can represent anything with 256 tokens
    (Youhou!! üéâüéâ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For non ascii characters, it gets completely unreadable, but it works nonetheless!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Input: `"Hello my friend, how are you?"` Ouput: `"Hello", "ƒ†my", ƒ†friend",
    ",", "ƒ†how", "ƒ†are", "ƒ†you", "?"` |'
  prefs: []
  type: TYPE_TB
- en: '| Whitespace | Splits on word boundaries (using the following regular expression:
    `\w+&#124;[^\w\s]+` | Input: `"Hello there!"` Output: `"Hello", "there", "!"`
    |'
  prefs: []
  type: TYPE_TB
- en: '| WhitespaceSplit | Splits on any whitespace character | Input: `"Hello there!"`
    Output: `"Hello", "there!"` |'
  prefs: []
  type: TYPE_TB
- en: '| Punctuation | Will isolate all punctuation characters | Input: `"Hello?"`
    Ouput: `"Hello", "?"` |'
  prefs: []
  type: TYPE_TB
- en: '| Metaspace | Splits on whitespaces and replaces them with a special char ‚Äú‚ñÅ‚Äù
    (U+2581) | Input: `"Hello there"` Ouput: `"Hello", "‚ñÅthere"` |'
  prefs: []
  type: TYPE_TB
- en: '| CharDelimiterSplit | Splits on a given character | Example with `x`: Input:
    `"Helloxthere"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ouput: `"Hello", "there"` |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Digits | Splits the numbers from any other characters. | Input: `"Hello123there"`
    Output: `"Hello", "123", "there"` |'
  prefs: []
  type: TYPE_TB
- en: '| Split | Versatile pre-tokenizer that splits on provided pattern and according
    to provided behavior. The pattern can be inverted if necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: pattern should be either a custom string or regexp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'behavior should be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: removed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: isolated
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: merged_with_previous
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: merged_with_next
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: contiguous
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: invert should be a boolean flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Example with pattern = `, behavior = `"isolated"`, invert = `False`: Input:
    `"Hello, how are you?"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: `"Hello,", " ", "how", " ", "are", " ", "you?"`` |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the
    given order | `Sequence([Punctuation(), WhitespaceSplit()])` |'
  prefs: []
  type: TYPE_TB
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models are the core algorithms used to actually tokenize, and therefore, they
    are the only mandatory component of a Tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| WordLevel | This is the ‚Äúclassic‚Äù tokenization algorithm. It let‚Äôs you simply
    map words to IDs without anything fancy. This has the advantage of being really
    simple to use and understand, but it requires extremely large vocabularies for
    a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice
    will be made by this model directly, it simply maps input tokens to IDs. |'
  prefs: []
  type: TYPE_TB
- en: '| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding
    works by starting with characters, while merging those that are the most frequently
    seen together, thus creating new tokens. It then works iteratively to build new
    tokens out of the most frequent pairs it sees in a corpus. BPE is able to build
    words it has never seen by using multiple subword tokens, and thus requires smaller
    vocabularies, with less chances of having ‚Äúunk‚Äù (unknown) tokens. |'
  prefs: []
  type: TYPE_TB
- en: '| WordPiece | This is a subword tokenization algorithm quite similar to BPE,
    used mainly by Google in models like BERT. It uses a greedy algorithm, that tries
    to build long words first, splitting in multiple tokens when entire words don‚Äôt
    exist in the vocabulary. This is different from BPE that starts from characters,
    building bigger tokens as possible. It uses the famous `##` prefix to identify
    tokens that are part of a word (ie not starting a word). |'
  prefs: []
  type: TYPE_TB
- en: '| Unigram | Unigram is also a subword tokenization algorithm, and works by
    trying to identify the best set of subword tokens to maximize the probability
    for a given sentence. This is different from BPE in the way that this is not deterministic
    based on a set of rules applied sequentially. Instead Unigram will be able to
    compute multiple ways of tokenizing, while choosing the most probable one. |'
  prefs: []
  type: TYPE_TB
- en: Post-Processors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the whole pipeline, we sometimes want to insert some special tokens before
    feed a tokenized string into a model like ‚Äù[CLS] My horse is amazing [SEP]‚Äù. The
    `PostProcessor` is the component doing just that.
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Description | Example |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| TemplateProcessing | Let‚Äôs you easily template the post processing, adding
    special tokens, and specifying the `type_id` for each sequence/special token.
    The template is given two strings representing the single sequence and the pair
    of sequences, as well as a set of special tokens to use. | Example, when specifying
    a template with these values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single: `"[CLS] $A [SEP]"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair: `"[CLS] $A [SEP] $B [SEP]"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'special tokens:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"[CLS]"`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"[SEP]"`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input: `("I like this", "but not this")`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: `"[CLS] I like this [SEP] but not this [SEP]"` |'
  prefs: []
  type: TYPE_NORMAL
- en: Decoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Decoder knows how to go from the IDs used by the Tokenizer, back to a readable
    piece of text. Some `Normalizer` and `PreTokenizer` use special characters or
    identifiers that need to be reverted for example.
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- |'
  prefs: []
  type: TYPE_TB
- en: '| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes
    at the byte-level, using a set of visible Unicode characters to represent each
    byte, so we need a Decoder to revert this process and get something readable again.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a
    special identifer `‚ñÅ` to identify whitespaces, and so this Decoder helps with
    decoding these. |'
  prefs: []
  type: TYPE_TB
- en: '| WordPiece | Reverts the WordPiece Model. This model uses a special identifier
    `##` for continuing subwords, and so this Decoder helps with decoding these. |'
  prefs: []
  type: TYPE_TB
