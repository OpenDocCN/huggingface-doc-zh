- en: Components
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组件
- en: 'Original text: [https://huggingface.co/docs/tokenizers/components](https://huggingface.co/docs/tokenizers/components)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/tokenizers/components](https://huggingface.co/docs/tokenizers/components)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: When building a Tokenizer, you can attach various types of components to this
    Tokenizer in order to customize its behavior. This page lists most provided components.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分词器时，您可以附加各种类型的组件到这个分词器，以定制其行为。本页列出了大多数提供的组件。
- en: Normalizers
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规范化器
- en: A `Normalizer` is in charge of pre-processing the input string in order to normalize
    it as relevant for a given use case. Some common examples of normalization are
    the Unicode normalization algorithms (NFD, NFKD, NFC & NFKC), lowercasing etc…
    The specificity of `tokenizers` is that we keep track of the alignment while normalizing.
    This is essential to allow mapping from the generated tokens back to the input
    text.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalizer` 负责预处理输入字符串，以使其在给定用例中规范化。一些常见的规范化示例是 Unicode 规范化算法（NFD、NFKD、NFC
    和 NFKC）、小写化等等... `tokenizers` 的特殊之处在于我们在规范化时保持对齐。这对于允许从生成的标记映射回输入文本是至关重要的。'
- en: The `Normalizer` is optional.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalizer` 是可选的。'
- en: PythonRustNode
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '| Name | Description | Example |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 示例 |'
- en: '| :-- | :-- | :-- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- |'
- en: '| NFD | NFD unicode normalization |  |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| NFD | NFD Unicode 规范化 |  |'
- en: '| NFKD | NFKD unicode normalization |  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| NFKD | NFKD Unicode 规范化 |  |'
- en: '| NFC | NFC unicode normalization |  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| NFC | NFC Unicode 规范化 |  |'
- en: '| NFKC | NFKC unicode normalization |  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| NFKC | NFKC Unicode 规范化 |  |'
- en: '| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO ὈΔΥΣΣΕΎΣ`
    Output: `hello`ὀδυσσεύς` |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| Lowercase | 将所有大写字母替换为小写字母 | 输入：`HELLO ὈΔΥΣΣΕΎΣ` 输出：`hello`ὀδυσσεύς` |'
- en: '| Strip | Removes all whitespace characters on the specified sides (left, right
    or both) of the input | Input: `"`hi`"` Output: `"hi"` |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Strip | 移除输入的指定侧（左侧、右侧或两侧）的所有空白字符 | 输入：`"`hi`"` 输出：`"hi"` |'
- en: '| StripAccents | Removes all accent symbols in unicode (to be used with NFD
    for consistency) | Input: `é` Ouput: `e` |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| StripAccents | 移除 Unicode 中的所有重音符号（与 NFD 一起使用以保持一致性） | 输入：`é` 输出：`e` |'
- en: '| Replace | Replaces a custom string or regexp and changes it with given content
    | `Replace("a", "e")` will behave like this: Input: `"banana"`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '| Replace | 替换自定义字符串或正则表达式，并用给定内容更改它 | `Replace("a", "e")` 的行为如下：输入："banana"'
- en: 'Ouput: `"benene"` |'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输出："benene" |
- en: '| BertNormalizer | Provides an implementation of the Normalizer used in the
    original BERT. Options that can be set are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '| BertNormalizer | 提供了原始 BERT 中使用的规范化器的实现。可以设置的选项有：'
- en: clean_text
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: clean_text
- en: handle_chinese_chars
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: handle_chinese_chars
- en: strip_accents
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: strip_accents
- en: lowercase
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lowercase
- en: '|  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Sequence | Composes multiple normalizers that will run in the provided order
    | `Sequence([NFKC(), Lowercase()])` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Sequence | 组合多个规范化器，按提供的顺序运行 | `Sequence([NFKC(), Lowercase()])` |'
- en: Pre-tokenizers
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预分词器
- en: The `PreTokenizer` takes care of splitting the input according to a set of rules.
    This pre-processing lets you ensure that the underlying `Model` does not build
    tokens across multiple “splits”. For example if you don’t want to have whitespaces
    inside a token, then you can have a `PreTokenizer` that splits on these whitespaces.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreTokenizer` 负责根据一组规则拆分输入。这种预处理可以确保底层的 `Model` 不会在多个“分割”之间构建标记。例如，如果您不希望在标记内部有空格，则可以使用一个在这些空格上拆分的
    `PreTokenizer`。'
- en: You can easily combine multiple `PreTokenizer` together using a `Sequence` (see
    below). The `PreTokenizer` is also allowed to modify the string, just like a `Normalizer`
    does. This is necessary to allow some complicated algorithms that require to split
    before normalizing (e.g. the ByteLevel)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `Sequence` 轻松地将多个 `PreTokenizer` 组合在一起（见下文）。`PreTokenizer` 也允许修改字符串，就像
    `Normalizer` 一样。这是必要的，以允许一些复杂的算法在规范化之前进行拆分（例如 ByteLevel）
- en: PythonRustNode
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '| Name | Description | Example |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 示例 |'
- en: '| :-- | :-- | :-- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- |'
- en: '| ByteLevel | Splits on whitespaces while remapping all the bytes to a set
    of visible characters. This technique as been introduced by OpenAI with GPT-2
    and has some more or less nice properties:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '| ByteLevel | 在空格上拆分，同时将所有字节重新映射为一组可见字符。这种技术由 OpenAI 与 GPT-2 一起引入，具有一些更或多或少好的特性：'
- en: Since it maps on bytes, a tokenizer using this only requires **256** characters
    as initial alphabet (the number of values a byte can have), as opposed to the
    130,000+ Unicode characters.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它映射在字节上，使用这种方法的分词器只需要 **256** 个字符作为初始字母表（一个字节可以有的值的数量），而不是 130,000 多个 Unicode
    字符。
- en: A consequence of the previous point is that it is absolutely unnecessary to
    have an unknown token using this since we can represent anything with 256 tokens
    (Youhou!! 🎉🎉)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前面一点的结果是，使用这种方法完全不需要使用未知标记，因为我们可以用 256 个标记表示任何内容（Youhou!! 🎉🎉）
- en: For non ascii characters, it gets completely unreadable, but it works nonetheless!
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非 ASCII 字符，它变得完全不可读，但它仍然有效！
- en: '| Input: `"Hello my friend, how are you?"` Ouput: `"Hello", "Ġmy", Ġfriend",
    ",", "Ġhow", "Ġare", "Ġyou", "?"` |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 输入："Hello my friend, how are you?" 输出："Hello", "Ġmy", Ġfriend", ",", "Ġhow",
    "Ġare", "Ġyou", "?" |'
- en: '| Whitespace | Splits on word boundaries (using the following regular expression:
    `\w+&#124;[^\w\s]+` | Input: `"Hello there!"` Output: `"Hello", "there", "!"`
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Whitespace | 在单词边界上拆分（使用以下正则表达式：`\w+&#124;[^\w\s]+` | 输入："Hello there!" 输出："Hello",
    "there", "!" |'
- en: '| WhitespaceSplit | Splits on any whitespace character | Input: `"Hello there!"`
    Output: `"Hello", "there!"` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| WhitespaceSplit | 以任何空白字符分割 | 输入："Hello there!" 输出："Hello", "there!" |'
- en: '| Punctuation | Will isolate all punctuation characters | Input: `"Hello?"`
    Ouput: `"Hello", "?"` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Punctuation | 将孤立所有标点符号字符 | 输入："Hello?" 输出："Hello", "?" |'
- en: '| Metaspace | Splits on whitespaces and replaces them with a special char “▁”
    (U+2581) | Input: `"Hello there"` Ouput: `"Hello", "▁there"` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Metaspace | 在空格上拆分并用特殊字符“▁”（U+2581）替换它们 | 输入："Hello there" 输出："Hello", "▁there"
    |'
- en: '| CharDelimiterSplit | Splits on a given character | Example with `x`: Input:
    `"Helloxthere"`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '| CharDelimiterSplit | 在给定字符上拆分 | 以 `x` 为例：输入："Helloxthere"'
- en: 'Ouput: `"Hello", "there"` |'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输出："Hello", "there" |
- en: '| Digits | Splits the numbers from any other characters. | Input: `"Hello123there"`
    Output: `"Hello", "123", "there"` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 数字 | 将数字与其他字符分开。 | 输入：`"Hello123there"` 输出：`"Hello"，"123"，"there"` |'
- en: '| Split | Versatile pre-tokenizer that splits on provided pattern and according
    to provided behavior. The pattern can be inverted if necessary.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '| 分割 | 多功能的预分词器，根据提供的模式和行为进行分剒。如果需要，可以反转模式。'
- en: pattern should be either a custom string or regexp.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式应该是自定义字符串或正则表达式。
- en: 'behavior should be one of:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为应该是以下之一：
- en: removed
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已移除
- en: isolated
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孤立
- en: merged_with_previous
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与上一个合并
- en: merged_with_next
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与下一个合并
- en: contiguous
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续的
- en: invert should be a boolean flag.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: invert应该是一个布尔标志。
- en: '| Example with pattern = `, behavior = `"isolated"`, invert = `False`: Input:
    `"Hello, how are you?"`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '| 例如，使用模式=`,`，行为=`"isolated"`，invert=`False`：输入：`"你好，你好吗？"`'
- en: 'Output: `"Hello,", " ", "how", " ", "are", " ", "you?"`` |'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：`"你好,"，" "，"你好"，" "，"你好"，" "，"你好？"` |
- en: '| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the
    given order | `Sequence([Punctuation(), WhitespaceSplit()])` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | 允许您组合多个将按给定顺序运行的`PreTokenizer` | `Sequence([Punctuation(), WhitespaceSplit()])`
    |'
- en: Models
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: Models are the core algorithms used to actually tokenize, and therefore, they
    are the only mandatory component of a Tokenizer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是实际标记化所使用的核心算法，因此它们是Tokenizer的唯一强制组件。
- en: '| Name | Description |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 |'
- en: '| :-- | :-- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- |'
- en: '| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply
    map words to IDs without anything fancy. This has the advantage of being really
    simple to use and understand, but it requires extremely large vocabularies for
    a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice
    will be made by this model directly, it simply maps input tokens to IDs. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| WordLevel | 这是“经典”标记化算法。它让您简单地将单词映射到ID，而无需任何花哨的东西。这样做的好处是非常简单易用，但需要极大的词汇量才能获得良好的覆盖率。使用此`Model`需要使用`PreTokenizer`。此模型不会直接做出选择，只是将输入标记映射到ID。
    |'
- en: '| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding
    works by starting with characters, while merging those that are the most frequently
    seen together, thus creating new tokens. It then works iteratively to build new
    tokens out of the most frequent pairs it sees in a corpus. BPE is able to build
    words it has never seen by using multiple subword tokens, and thus requires smaller
    vocabularies, with less chances of having “unk” (unknown) tokens. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| BPE | 最流行的子词标记化算法之一。字节对编码通过从字符开始，合并最常一起出现的字符，从而创建新标记。然后，它迭代地构建新标记，使用语料库中看到的最常见的对。BPE能够通过使用多个子词标记构建从未见过的单词，因此需要更小的词汇量，减少出现“unk”（未知）标记的机会。
    |'
- en: '| WordPiece | This is a subword tokenization algorithm quite similar to BPE,
    used mainly by Google in models like BERT. It uses a greedy algorithm, that tries
    to build long words first, splitting in multiple tokens when entire words don’t
    exist in the vocabulary. This is different from BPE that starts from characters,
    building bigger tokens as possible. It uses the famous `##` prefix to identify
    tokens that are part of a word (ie not starting a word). |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| WordPiece | 这是一种子词标记化算法，与BPE非常相似，主要由谷歌在BERT等模型中使用。它使用一种贪婪算法，试图首先构建长单词，在整个词汇表中不存在整个单词时拆分为多个标记。这与BPE不同，BPE从字符开始构建更大的标记。它使用著名的`##`前缀来识别作为单词一部分的标记（即不是单词的开头）。'
- en: '| Unigram | Unigram is also a subword tokenization algorithm, and works by
    trying to identify the best set of subword tokens to maximize the probability
    for a given sentence. This is different from BPE in the way that this is not deterministic
    based on a set of rules applied sequentially. Instead Unigram will be able to
    compute multiple ways of tokenizing, while choosing the most probable one. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Unigram | Unigram也是一种子词标记化算法，通过尝试识别最佳子词标记集来最大化给定句子的概率。这与BPE不同，因为它不是基于一组按顺序应用的规则而确定的。相反，Unigram将能够计算多种标记化方式，同时选择最有可能的一种。
    |'
- en: Post-Processors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后处理器
- en: After the whole pipeline, we sometimes want to insert some special tokens before
    feed a tokenized string into a model like ”[CLS] My horse is amazing [SEP]”. The
    `PostProcessor` is the component doing just that.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个流水线之后，有时我们希望在将标记化的字符串馈送到模型之前插入一些特殊标记，例如“[CLS]我的马很棒[SEP]”。`PostProcessor`就是执行这项任务的组件。
- en: '| Name | Description | Example |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 示例 |'
- en: '| :-- | :-- | :-- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- |'
- en: '| TemplateProcessing | Let’s you easily template the post processing, adding
    special tokens, and specifying the `type_id` for each sequence/special token.
    The template is given two strings representing the single sequence and the pair
    of sequences, as well as a set of special tokens to use. | Example, when specifying
    a template with these values:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '| 模板处理 | 让您轻松地对后处理进行模板化，添加特殊标记，并为每个序列/特殊标记指定`type_id`。模板提供了两个字符串，表示单个序列和一对序列，以及一组要使用的特殊标记。
    | 例如，当指定具有这些值的模板时：'
- en: 'single: `"[CLS] $A [SEP]"`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个：`"[CLS] $A [SEP]"`
- en: 'pair: `"[CLS] $A [SEP] $B [SEP]"`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对：`"[CLS] $A [SEP] $B [SEP]"`
- en: 'special tokens:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊标记：
- en: '`"[CLS]"`'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"[CLS]"`'
- en: '`"[SEP]"`'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"[SEP]"`'
- en: 'Input: `("I like this", "but not this")`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：`("我喜欢这个"，"但不喜欢这个")`
- en: 'Output: `"[CLS] I like this [SEP] but not this [SEP]"` |'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：`"[CLS] 我喜欢这个 [SEP] 但不喜欢这个 [SEP]"` |
- en: Decoders
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: The Decoder knows how to go from the IDs used by the Tokenizer, back to a readable
    piece of text. Some `Normalizer` and `PreTokenizer` use special characters or
    identifiers that need to be reverted for example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器知道如何从Tokenizer使用的ID转换回可读的文本。一些`Normalizer`和`PreTokenizer`使用特殊字符或标识符，需要进行反转。
- en: '| Name | Description |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 |'
- en: '| :-- | :-- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- |'
- en: '| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes
    at the byte-level, using a set of visible Unicode characters to represent each
    byte, so we need a Decoder to revert this process and get something readable again.
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ByteLevel | 反转ByteLevel预分词器。该预分词器在字节级别进行编码，使用一组可见的Unicode字符来表示每个字节，因此我们需要一个解码器来恢复此过程并再次获得可读内容。
    |'
- en: '| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a
    special identifer `▁` to identify whitespaces, and so this Decoder helps with
    decoding these. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Metaspace | 反转 Metaspace 预处理器。该预处理器使用特殊标识符 `▁` 来识别空格，因此该解码器有助于解码这些。 |'
- en: '| WordPiece | Reverts the WordPiece Model. This model uses a special identifier
    `##` for continuing subwords, and so this Decoder helps with decoding these. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| WordPiece | 反转 WordPiece 模型。该模型使用特殊标识符 `##` 来表示继续的子词，因此该解码器有助于解码这些。 |'
