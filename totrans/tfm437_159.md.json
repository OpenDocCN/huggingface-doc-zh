["```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' projection_dim: int = 0 **kwargs )\n```", "```py\n>>> from transformers import DPRConfig, DPRContextEncoder\n\n>>> # Initializing a DPR facebook/dpr-ctx_encoder-single-nq-base style configuration\n>>> configuration = DPRConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/dpr-ctx_encoder-single-nq-base style configuration\n>>> model = DPRContextEncoder(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs ) \u2192 export const metadata = 'undefined';Dict[str, List[List[int]]]\n```", "```py\n[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs ) \u2192 export const metadata = 'undefined';Dict[str, List[List[int]]]\n```", "```py\n( pooler_output: FloatTensor hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( pooler_output: FloatTensor hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( start_logits: FloatTensor end_logits: FloatTensor = None relevance_logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( config: DPRConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n\n>>> tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n>>> model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n>>> input_ids = tokenizer(\"Hello, is my dog cute ?\", return_tensors=\"pt\")[\"input_ids\"]\n>>> embeddings = model(input_ids).pooler_output\n```", "```py\n( config: DPRConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n\n>>> tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n>>> model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n>>> input_ids = tokenizer(\"Hello, is my dog cute ?\", return_tensors=\"pt\")[\"input_ids\"]\n>>> embeddings = model(input_ids).pooler_output\n```", "```py\n( config: DPRConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.dpr.modeling_dpr.DPRReaderOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import DPRReader, DPRReaderTokenizer\n\n>>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n>>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n>>> encoded_inputs = tokenizer(\n...     questions=[\"What is love ?\"],\n...     titles=[\"Haddaway\"],\n...     texts=[\"'What Is Love' is a song recorded by the artist Haddaway\"],\n...     return_tensors=\"pt\",\n... )\n>>> outputs = model(**encoded_inputs)\n>>> start_logits = outputs.start_logits\n>>> end_logits = outputs.end_logits\n>>> relevance_logits = outputs.relevance_logits\n```", "```py\n( config: DPRConfig *args **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer\n\n>>> tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n>>> model = TFDPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", from_pt=True)\n>>> input_ids = tokenizer(\"Hello, is my dog cute ?\", return_tensors=\"tf\")[\"input_ids\"]\n>>> embeddings = model(input_ids).pooler_output\n```", "```py\n( config: DPRConfig *args **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer\n\n>>> tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n>>> model = TFDPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\", from_pt=True)\n>>> input_ids = tokenizer(\"Hello, is my dog cute ?\", return_tensors=\"tf\")[\"input_ids\"]\n>>> embeddings = model(input_ids).pooler_output\n```", "```py\n( config: DPRConfig *args **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import TFDPRReader, DPRReaderTokenizer\n\n>>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n>>> model = TFDPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\", from_pt=True)\n>>> encoded_inputs = tokenizer(\n...     questions=[\"What is love ?\"],\n...     titles=[\"Haddaway\"],\n...     texts=[\"'What Is Love' is a song recorded by the artist Haddaway\"],\n...     return_tensors=\"tf\",\n... )\n>>> outputs = model(encoded_inputs)\n>>> start_logits = outputs.start_logits\n>>> end_logits = outputs.end_logits\n>>> relevance_logits = outputs.relevance_logits\n```"]