- en: Image tasks with IDEFICS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/343.b7cf27ed.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/DocNotebookDropdown.3e6b3817.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: While individual tasks can be tackled by fine-tuning specialized models, an
    alternative approach that has recently emerged and gained popularity is to use
    large models for a diverse set of tasks without fine-tuning. For instance, large
    language models can handle such NLP tasks as summarization, translation, classification,
    and more. This approach is no longer limited to a single modality, such as text,
    and in this guide, we will illustrate how you can solve image-text tasks with
    a large multimodal model called IDEFICS.
  prefs: []
  type: TYPE_NORMAL
- en: '[IDEFICS](../model_doc/idefics) is an open-access vision and language model
    based on [Flamingo](https://huggingface.co/papers/2204.14198), a state-of-the-art
    visual language model initially developed by DeepMind. The model accepts arbitrary
    sequences of image and text inputs and generates coherent text as output. It can
    answer questions about images, describe visual content, create stories grounded
    in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)
    and [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both
    of which are available on the ðŸ¤— Hub. For each variant, you can also find fine-tuned
    instructed versions of the model adapted for conversational use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: This model is exceptionally versatile and can be used for a wide range of image
    and multimodal tasks. However, being a large model means it requires significant
    computational resources and infrastructure. It is up to you to decide whether
    this approach suits your use case better than fine-tuning specialized models for
    each individual task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide, youâ€™ll learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#loading-the-quantized-version-of-the-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use IDEFICS for:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image captioning](#image-captioning)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prompted image captioning](#prompted-image-captioning)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Few-shot prompting](#few-shot-prompting)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visual question answering](#visual-question-answering)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image classificaiton](#image-classification)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image-guided text generation](#image-guided-text-generation)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run inference in batch mode](#running-inference-in-batch-mode)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run IDEFICS instruct for conversational use](#idefics-instruct-for-conversational-use)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you begin, make sure you have all the necessary libraries installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To run the following examples with a non-quantized version of the model checkpoint
    you will need at least 20GB of GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Letâ€™s start by loading the modelâ€™s 9 billion parameters checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Just like for other Transformers models, you need to load a processor and the
    model itself from the checkpoint. The IDEFICS processor wraps a [LlamaTokenizer](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizer)
    and IDEFICS image processor into a single processor to take care of preparing
    text and image inputs for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Setting `device_map` to `"auto"` will automatically determine how to load and
    store the model weights in the most optimized manner given existing devices.
  prefs: []
  type: TYPE_NORMAL
- en: Quantized model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If high-memory GPU availability is an issue, you can load the quantized version
    of the model. To load the model and the processor in 4bit precision, pass a `BitsAndBytesConfig`
    to the `from_pretrained` method and the model will be compressed on the fly while
    loading.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have the model loaded in one of the suggested ways, letâ€™s move
    on to exploring tasks that you can use IDEFICS for.
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image captioning is the task of predicting a caption for a given image. A common
    application is to aid visually impaired people navigate through different situations,
    for instance, explore image content online.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the task, get an image to be captioned, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of a puppy in a flower bed](../Images/1ae9f6a999a65c594f8ffed6366c14b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hendo Wang](https://unsplash.com/@hendoo).
  prefs: []
  type: TYPE_NORMAL
- en: IDEFICS accepts text and image prompts. However, to caption an image, you do
    not have to provide a text prompt to the model, only the preprocessed input image.
    Without a text prompt, the model will start generating text from the BOS (beginning-of-sequence)
    token thus creating a caption.
  prefs: []
  type: TYPE_NORMAL
- en: As image input to the model, you can use either an image object (`PIL.Image`)
    or a url from which the image can be retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a good idea to include the `bad_words_ids` in the call to `generate`
    to avoid errors arising when increasing the `max_new_tokens`: the model will want
    to generate a new `<image>` or `<fake_token_around_image>` token when there is
    no image being generated by the model. You can set it on-the-fly as in this guide,
    or store in the `GenerationConfig` as described in the [Text generation strategies](../generation_strategies)
    guide.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompted image captioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can extend image captioning by providing a text prompt, which the model
    will continue given the image. Letâ€™s take another image to illustrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of the Eiffel Tower at night](../Images/21827a2f63908e6e4dca537122fd4df7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Denys Nevozhai](https://unsplash.com/@dnevozhai).
  prefs: []
  type: TYPE_NORMAL
- en: Textual and image prompts can be passed to the modelâ€™s processor as a single
    list to create appropriate inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Few-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While IDEFICS demonstrates great zero-shot results, your task may require a
    certain format of the caption, or come with other restrictions or requirements
    that increase taskâ€™s complexity. Few-shot prompting can be used to enable in-context
    learning. By providing examples in the prompt, you can steer the model to generate
    results that mimic the format of given examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s use the previous image of the Eiffel Tower as an example for the model
    and build a prompt that demonstrates to the model that in addition to learning
    what the object in an image is, we would also like to get some interesting information
    about it. Then, letâ€™s see, if we can get the same response format for an image
    of the Statue of Liberty:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of the Statue of Liberty](../Images/759caa1ce68c4cc710b290421bd9520d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Juan Mayobre](https://unsplash.com/@jmayobres).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that just from a single example (i.e., 1-shot) the model has learned
    how to perform the task. For more complex tasks, feel free to experiment with
    a larger number of examples (e.g., 3-shot, 5-shot, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Visual question answering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visual Question Answering (VQA) is the task of answering open-ended questions
    based on an image. Similar to image captioning it can be used in accessibility
    applications, but also in education (reasoning about visual materials), customer
    service (questions about products based on images), and image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s get a new image for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of a couple having a picnic](../Images/d3e04f9a26ae586b92097eb4396b1db0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jarritos Mexican Soda](https://unsplash.com/@jarritos).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can steer the model from image captioning to visual question answering
    by prompting it with appropriate instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IDEFICS is capable of classifying images into different categories without being
    explicitly trained on data containing labeled examples from those specific categories.
    Given a list of categories and using its image and text understanding capabilities,
    the model can infer which category the image likely belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, we have this image of a vegetable stand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of a vegetable stand](../Images/0f1d778c8084e2f63cd81d21fdb55d1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Peter Wendt](https://unsplash.com/@peterwendt).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can instruct the model to classify the image into one of the categories
    that we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the example above we instruct the model to classify the image into a single
    category, however, you can also prompt the model to do rank classification.
  prefs: []
  type: TYPE_NORMAL
- en: Image-guided text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more creative applications, you can use image-guided text generation to
    generate text based on an image. This can be useful to create descriptions of
    products, ads, descriptions of a scene, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s prompt IDEFICS to write a story based on a simple image of a red door:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of a red door with a pumpkin on the steps](../Images/6e5de95de1888a89f56e9744b5af7215.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Craig Tidball](https://unsplash.com/@devonshiremedia).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Looks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky
    Halloween story about a ghost.
  prefs: []
  type: TYPE_NORMAL
- en: For longer outputs like this, you will greatly benefit from tweaking the text
    generation strategy. This can help you significantly improve the quality of the
    generated output. Check out [Text generation strategies](../generation_strategies)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Running inference in batch mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of the earlier sections illustrated IDEFICS for a single example. In a
    very similar fashion, you can run inference for a batch of examples by passing
    a list of prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: IDEFICS instruct for conversational use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For conversational use cases, you can find fine-tuned instructed versions of
    the model on the ðŸ¤— Hub: `HuggingFaceM4/idefics-80b-instruct` and `HuggingFaceM4/idefics-9b-instruct`.'
  prefs: []
  type: TYPE_NORMAL
- en: These checkpoints are the result of fine-tuning the respective base models on
    a mixture of supervised and instruction fine-tuning datasets, which boosts the
    downstream performance while making the models more usable in conversational settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use and prompting for the conversational use is very similar to using the
    base models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
