- en: Image tasks with IDEFICS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨IDEFICSè¿›è¡Œå›¾åƒä»»åŠ¡
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: While individual tasks can be tackled by fine-tuning specialized models, an
    alternative approach that has recently emerged and gained popularity is to use
    large models for a diverse set of tasks without fine-tuning. For instance, large
    language models can handle such NLP tasks as summarization, translation, classification,
    and more. This approach is no longer limited to a single modality, such as text,
    and in this guide, we will illustrate how you can solve image-text tasks with
    a large multimodal model called IDEFICS.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å¯ä»¥é€šè¿‡å¾®è°ƒä¸“é—¨çš„æ¨¡å‹æ¥è§£å†³å•ä¸ªä»»åŠ¡ï¼Œä½†æœ€è¿‘å‡ºç°å¹¶å—åˆ°æ¬¢è¿çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å¤§å‹æ¨¡å‹å¤„ç†å„ç§ä»»åŠ¡è€Œæ— éœ€å¾®è°ƒã€‚ä¾‹å¦‚ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å¤„ç†è¯¸å¦‚æ‘˜è¦ã€ç¿»è¯‘ã€åˆ†ç±»ç­‰NLPä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•ä¸å†å±€é™äºå•ä¸€æ¨¡æ€ï¼Œæ¯”å¦‚æ–‡æœ¬ï¼Œåœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•ä½¿ç”¨åä¸ºIDEFICSçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è§£å†³å›¾åƒæ–‡æœ¬ä»»åŠ¡ã€‚
- en: '[IDEFICS](../model_doc/idefics) is an open-access vision and language model
    based on [Flamingo](https://huggingface.co/papers/2204.14198), a state-of-the-art
    visual language model initially developed by DeepMind. The model accepts arbitrary
    sequences of image and text inputs and generates coherent text as output. It can
    answer questions about images, describe visual content, create stories grounded
    in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)
    and [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both
    of which are available on the ğŸ¤— Hub. For each variant, you can also find fine-tuned
    instructed versions of the model adapted for conversational use cases.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[IDEFICS](../model_doc/idefics)æ˜¯ä¸€ä¸ªåŸºäº[Flamingo](https://huggingface.co/papers/2204.14198)çš„å¼€æ”¾å¼è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ŒFlamingoæ˜¯ç”±DeepMindæœ€åˆå¼€å‘çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ¥å—ä»»æ„åºåˆ—çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚å®ƒå¯ä»¥å›ç­”å…³äºå›¾åƒçš„é—®é¢˜ï¼Œæè¿°è§†è§‰å†…å®¹ï¼Œåˆ›å»ºåŸºäºå¤šä¸ªå›¾åƒçš„æ•…äº‹ç­‰ã€‚IDEFICSæœ‰ä¸¤ä¸ªå˜ä½“
    - [80äº¿å‚æ•°](https://huggingface.co/HuggingFaceM4/idefics-80b)å’Œ[90äº¿å‚æ•°](https://huggingface.co/HuggingFaceM4/idefics-9b)ï¼Œè¿™ä¸¤ä¸ªå˜ä½“éƒ½å¯ä»¥åœ¨ğŸ¤—
    Hubä¸Šæ‰¾åˆ°ã€‚å¯¹äºæ¯ä¸ªå˜ä½“ï¼Œæ‚¨è¿˜å¯ä»¥æ‰¾åˆ°ä¸ºå¯¹è¯ä½¿ç”¨æ¡ˆä¾‹è°ƒæ•´çš„æ¨¡å‹çš„å¾®è°ƒæŒ‡å¯¼ç‰ˆæœ¬ã€‚'
- en: This model is exceptionally versatile and can be used for a wide range of image
    and multimodal tasks. However, being a large model means it requires significant
    computational resources and infrastructure. It is up to you to decide whether
    this approach suits your use case better than fine-tuning specialized models for
    each individual task.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹éå¸¸çµæ´»ï¼Œå¯ä»¥ç”¨äºå„ç§å›¾åƒå’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä½œä¸ºä¸€ä¸ªå¤§å‹æ¨¡å‹æ„å‘³ç€å®ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’ŒåŸºç¡€è®¾æ–½ã€‚æ‚¨éœ€è¦å†³å®šè¿™ç§æ–¹æ³•æ˜¯å¦æ¯”ä¸ºæ¯ä¸ªå•ç‹¬ä»»åŠ¡å¾®è°ƒä¸“é—¨çš„æ¨¡å‹æ›´é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚
- en: 'In this guide, youâ€™ll learn how to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š
- en: '[Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#loading-the-quantized-version-of-the-model)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åŠ è½½IDEFICS](#åŠ è½½æ¨¡å‹)å’Œ[åŠ è½½æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬](#åŠ è½½æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬)'
- en: 'Use IDEFICS for:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨IDEFICSè¿›è¡Œï¼š
- en: '[Image captioning](#image-captioning)'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒåŠ æ ‡é¢˜](#å›¾åƒåŠ æ ‡é¢˜)'
- en: '[Prompted image captioning](#prompted-image-captioning)'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æç¤ºçš„å›¾åƒåŠ æ ‡é¢˜](#æç¤ºçš„å›¾åƒåŠ æ ‡é¢˜)'
- en: '[Few-shot prompting](#few-shot-prompting)'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å°‘æ ·æœ¬æç¤º](#å°‘æ ·æœ¬æç¤º)'
- en: '[Visual question answering](#visual-question-answering)'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è§†è§‰é—®ç­”](#è§†è§‰é—®ç­”)'
- en: '[Image classificaiton](#image-classification)'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒåˆ†ç±»](#å›¾åƒåˆ†ç±»)'
- en: '[Image-guided text generation](#image-guided-text-generation)'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒå¼•å¯¼æ–‡æœ¬ç”Ÿæˆ](#å›¾åƒå¼•å¯¼æ–‡æœ¬ç”Ÿæˆ)'
- en: '[Run inference in batch mode](#running-inference-in-batch-mode)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ‰¹å¤„ç†æ¨¡å¼ä¸‹è¿è¡Œæ¨ç†](#åœ¨æ‰¹å¤„ç†æ¨¡å¼ä¸‹è¿è¡Œæ¨ç†)'
- en: '[Run IDEFICS instruct for conversational use](#idefics-instruct-for-conversational-use)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¿è¡ŒIDEFICSæŒ‡å¯¼è¿›è¡Œå¯¹è¯ä½¿ç”¨](#ç”¨äºå¯¹è¯ä½¿ç”¨çš„IDEFICSæŒ‡å¯¼)'
- en: Before you begin, make sure you have all the necessary libraries installed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To run the following examples with a non-quantized version of the model checkpoint
    you will need at least 20GB of GPU memory.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡Œä»¥ä¸‹ç¤ºä¾‹ï¼Œæ‚¨å°†éœ€è¦è‡³å°‘20GBçš„GPUå†…å­˜æ¥ä½¿ç”¨æ¨¡å‹æ£€æŸ¥ç‚¹çš„éé‡åŒ–ç‰ˆæœ¬ã€‚
- en: Loading the model
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹
- en: 'Letâ€™s start by loading the modelâ€™s 9 billion parameters checkpoint:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»åŠ è½½æ¨¡å‹çš„90äº¿å‚æ•°æ£€æŸ¥ç‚¹å¼€å§‹ï¼š
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Just like for other Transformers models, you need to load a processor and the
    model itself from the checkpoint. The IDEFICS processor wraps a [LlamaTokenizer](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizer)
    and IDEFICS image processor into a single processor to take care of preparing
    text and image inputs for the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå…¶ä»–Transformeræ¨¡å‹ä¸€æ ·ï¼Œæ‚¨éœ€è¦ä»æ£€æŸ¥ç‚¹åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹æœ¬èº«ã€‚IDEFICSå¤„ç†å™¨å°†[LlamaTokenizer](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizer)å’ŒIDEFICSå›¾åƒå¤„ç†å™¨åŒ…è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ï¼Œä»¥è´Ÿè´£ä¸ºæ¨¡å‹å‡†å¤‡æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ã€‚
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Setting `device_map` to `"auto"` will automatically determine how to load and
    store the model weights in the most optimized manner given existing devices.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`device_map`è®¾ç½®ä¸º`"auto"`å°†è‡ªåŠ¨ç¡®å®šå¦‚ä½•ä»¥æœ€ä¼˜åŒ–çš„æ–¹å¼åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼Œè€ƒè™‘åˆ°ç°æœ‰è®¾å¤‡ã€‚
- en: Quantized model
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡åŒ–æ¨¡å‹
- en: If high-memory GPU availability is an issue, you can load the quantized version
    of the model. To load the model and the processor in 4bit precision, pass a `BitsAndBytesConfig`
    to the `from_pretrained` method and the model will be compressed on the fly while
    loading.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœé«˜å†…å­˜GPUå¯ç”¨æ€§æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥åŠ è½½æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬ã€‚è¦åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨çš„4ä½ç²¾åº¦ç‰ˆæœ¬ï¼Œè¯·å°†`BitsAndBytesConfig`ä¼ é€’ç»™`from_pretrained`æ–¹æ³•ï¼Œæ¨¡å‹å°†åœ¨åŠ è½½æ—¶å³æ—¶å‹ç¼©ã€‚
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that you have the model loaded in one of the suggested ways, letâ€™s move
    on to exploring tasks that you can use IDEFICS for.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»ä»¥å»ºè®®çš„æ–¹å¼ä¹‹ä¸€åŠ è½½äº†æ¨¡å‹ï¼Œè®©æˆ‘ä»¬ç»§ç»­æ¢ç´¢æ‚¨å¯ä»¥ä½¿ç”¨IDEFICSçš„ä»»åŠ¡ã€‚
- en: Image captioning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒåŠ æ ‡é¢˜
- en: Image captioning is the task of predicting a caption for a given image. A common
    application is to aid visually impaired people navigate through different situations,
    for instance, explore image content online.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåŠ æ ‡é¢˜æ˜¯é¢„æµ‹ç»™å®šå›¾åƒçš„æ ‡é¢˜çš„ä»»åŠ¡ã€‚ä¸€ä¸ªå¸¸è§çš„åº”ç”¨æ˜¯å¸®åŠ©è§†éšœäººå£«åœ¨ä¸åŒæƒ…å†µä¸‹å¯¼èˆªï¼Œä¾‹å¦‚ï¼Œåœ¨çº¿æ¢ç´¢å›¾åƒå†…å®¹ã€‚
- en: 'To illustrate the task, get an image to be captioned, e.g.:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜ä»»åŠ¡ï¼Œè·å–ä¸€ä¸ªéœ€è¦åŠ æ ‡é¢˜çš„å›¾åƒï¼Œä¾‹å¦‚ï¼š
- en: '![Image of a puppy in a flower bed](../Images/1ae9f6a999a65c594f8ffed6366c14b3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![èŠ±å›­é‡Œçš„å°ç‹—çš„å›¾ç‰‡](../Images/1ae9f6a999a65c594f8ffed6366c14b3.png)'
- en: Photo by [Hendo Wang](https://unsplash.com/@hendoo).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Hendo Wang](https://unsplash.com/@hendoo)æ‹æ‘„ã€‚
- en: IDEFICS accepts text and image prompts. However, to caption an image, you do
    not have to provide a text prompt to the model, only the preprocessed input image.
    Without a text prompt, the model will start generating text from the BOS (beginning-of-sequence)
    token thus creating a caption.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: IDEFICSæ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºã€‚ä½†æ˜¯ï¼Œè¦ä¸ºå›¾åƒæ·»åŠ å­—å¹•ï¼Œæ‚¨ä¸å¿…å‘æ¨¡å‹æä¾›æ–‡æœ¬æç¤ºï¼Œåªéœ€æä¾›é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒã€‚æ²¡æœ‰æ–‡æœ¬æç¤ºï¼Œæ¨¡å‹å°†ä»BOSï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°å¼€å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œä»è€Œåˆ›å»ºå­—å¹•ã€‚
- en: As image input to the model, you can use either an image object (`PIL.Image`)
    or a url from which the image can be retrieved.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ¨¡å‹çš„å›¾åƒè¾“å…¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å›¾åƒå¯¹è±¡ï¼ˆ`PIL.Image`ï¼‰æˆ–å¯ä»¥ä»ä¸­æ£€ç´¢å›¾åƒçš„urlã€‚
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It is a good idea to include the `bad_words_ids` in the call to `generate`
    to avoid errors arising when increasing the `max_new_tokens`: the model will want
    to generate a new `<image>` or `<fake_token_around_image>` token when there is
    no image being generated by the model. You can set it on-the-fly as in this guide,
    or store in the `GenerationConfig` as described in the [Text generation strategies](../generation_strategies)
    guide.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨`generate`æ—¶ï¼Œæœ€å¥½åŒ…å«`bad_words_ids`ï¼Œä»¥é¿å…åœ¨å¢åŠ `max_new_tokens`æ—¶å‡ºç°é”™è¯¯ï¼šå½“æ¨¡å‹è¦ç”Ÿæˆä¸€ä¸ªæ–°çš„`<image>`æˆ–`<fake_token_around_image>`æ ‡è®°æ—¶ï¼Œè€Œæ¨¡å‹æ²¡æœ‰ç”Ÿæˆå›¾åƒæ—¶ï¼Œä¼šå‡ºç°é”™è¯¯ã€‚æ‚¨å¯ä»¥åƒæœ¬æŒ‡å—ä¸­é‚£æ ·å³æ—¶è®¾ç½®å®ƒï¼Œæˆ–è€…åƒ[æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies)æŒ‡å—ä¸­æè¿°çš„é‚£æ ·å­˜å‚¨åœ¨`GenerationConfig`ä¸­ã€‚
- en: Prompted image captioning
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºçš„å›¾åƒå­—å¹•
- en: 'You can extend image captioning by providing a text prompt, which the model
    will continue given the image. Letâ€™s take another image to illustrate:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡æä¾›æ–‡æœ¬æç¤ºæ¥æ‰©å±•å›¾åƒå­—å¹•ï¼Œæ¨¡å‹å°†ç»§ç»­ç»™å‡ºå›¾åƒã€‚è®©æˆ‘ä»¬æ‹¿å¦ä¸€å¼ å›¾ç‰‡æ¥è¯´æ˜ï¼š
- en: '![Image of the Eiffel Tower at night](../Images/21827a2f63908e6e4dca537122fd4df7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å¤œæ™šçš„åŸƒè²å°”é“å¡”çš„å›¾ç‰‡](../Images/21827a2f63908e6e4dca537122fd4df7.png)'
- en: Photo by [Denys Nevozhai](https://unsplash.com/@dnevozhai).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Denys Nevozhai](https://unsplash.com/@dnevozhai)æ‹æ‘„ã€‚
- en: Textual and image prompts can be passed to the modelâ€™s processor as a single
    list to create appropriate inputs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬å’Œå›¾åƒæç¤ºå¯ä»¥ä½œä¸ºå•ä¸ªåˆ—è¡¨ä¼ é€’ç»™æ¨¡å‹çš„å¤„ç†å™¨ï¼Œä»¥åˆ›å»ºé€‚å½“çš„è¾“å…¥ã€‚
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Few-shot prompting
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°‘é‡æç¤º
- en: While IDEFICS demonstrates great zero-shot results, your task may require a
    certain format of the caption, or come with other restrictions or requirements
    that increase taskâ€™s complexity. Few-shot prompting can be used to enable in-context
    learning. By providing examples in the prompt, you can steer the model to generate
    results that mimic the format of given examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶IDEFICSå±•ç¤ºäº†å‡ºè‰²çš„é›¶-shotç»“æœï¼Œä½†æ‚¨çš„ä»»åŠ¡å¯èƒ½éœ€è¦ä¸€å®šæ ¼å¼çš„å­—å¹•ï¼Œæˆ–è€…ä¼´éšå…¶ä»–é™åˆ¶æˆ–è¦æ±‚ï¼Œå¢åŠ ä»»åŠ¡çš„å¤æ‚æ€§ã€‚å°‘é‡æç¤ºå¯ç”¨äºå¯ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡åœ¨æç¤ºä¸­æä¾›ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆç±»ä¼¼äºç»™å®šç¤ºä¾‹æ ¼å¼çš„ç»“æœã€‚
- en: 'Letâ€™s use the previous image of the Eiffel Tower as an example for the model
    and build a prompt that demonstrates to the model that in addition to learning
    what the object in an image is, we would also like to get some interesting information
    about it. Then, letâ€™s see, if we can get the same response format for an image
    of the Statue of Liberty:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥åŸƒè²å°”é“å¡”çš„ä¸Šä¸€å¼ å›¾ç‰‡ä½œä¸ºæ¨¡å‹çš„ç¤ºä¾‹ï¼Œå¹¶æ„å»ºä¸€ä¸ªæç¤ºï¼Œå‘æ¨¡å‹å±•ç¤ºé™¤äº†å­¦ä¹ å›¾åƒä¸­çš„å¯¹è±¡æ˜¯ä»€ä¹ˆä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›è·å¾—ä¸€äº›æœ‰è¶£çš„ä¿¡æ¯ã€‚ç„¶åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ï¼Œå¦‚æœæˆ‘ä»¬å¯ä»¥ä¸ºè‡ªç”±å¥³ç¥åƒçš„å›¾ç‰‡è·å¾—ç›¸åŒçš„å“åº”æ ¼å¼ï¼š
- en: '![Image of the Statue of Liberty](../Images/759caa1ce68c4cc710b290421bd9520d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![è‡ªç”±å¥³ç¥åƒçš„å›¾ç‰‡](../Images/759caa1ce68c4cc710b290421bd9520d.png)'
- en: Photo by [Juan Mayobre](https://unsplash.com/@jmayobres).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Juan Mayobre](https://unsplash.com/@jmayobres)æ‹æ‘„ã€‚
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that just from a single example (i.e., 1-shot) the model has learned
    how to perform the task. For more complex tasks, feel free to experiment with
    a larger number of examples (e.g., 3-shot, 5-shot, etc.).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä»…ä»å•ä¸ªç¤ºä¾‹ï¼ˆå³1-shotï¼‰ä¸­ï¼Œæ¨¡å‹å·²ç»å­¦ä¼šäº†å¦‚ä½•æ‰§è¡Œä»»åŠ¡ã€‚å¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œè¯·éšæ—¶å°è¯•ä½¿ç”¨æ›´å¤šçš„ç¤ºä¾‹ï¼ˆä¾‹å¦‚3-shotï¼Œ5-shotç­‰ï¼‰ã€‚
- en: Visual question answering
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§†è§‰é—®é¢˜å›ç­”
- en: Visual Question Answering (VQA) is the task of answering open-ended questions
    based on an image. Similar to image captioning it can be used in accessibility
    applications, but also in education (reasoning about visual materials), customer
    service (questions about products based on images), and image retrieval.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰é—®é¢˜å›ç­”ï¼ˆVQAï¼‰æ˜¯æ ¹æ®å›¾åƒå›ç­”å¼€æ”¾å¼é—®é¢˜çš„ä»»åŠ¡ã€‚ä¸å›¾åƒå­—å¹•ç±»ä¼¼ï¼Œå®ƒå¯ä»¥ç”¨äºè¾…åŠ©åŠŸèƒ½åº”ç”¨ç¨‹åºï¼Œè¿˜å¯ä»¥ç”¨äºæ•™è‚²ï¼ˆå…³äºè§†è§‰ææ–™çš„æ¨ç†ï¼‰ã€å®¢æˆ·æœåŠ¡ï¼ˆåŸºäºå›¾åƒçš„äº§å“é—®é¢˜ï¼‰å’Œå›¾åƒæ£€ç´¢ã€‚
- en: 'Letâ€™s get a new image for this task:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸ºè¿™ä¸ªä»»åŠ¡è·å–ä¸€å¼ æ–°çš„å›¾ç‰‡ï¼š
- en: '![Image of a couple having a picnic](../Images/d3e04f9a26ae586b92097eb4396b1db0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸€å¯¹æ­£åœ¨é‡é¤çš„å¤«å¦‡çš„å›¾ç‰‡](../Images/d3e04f9a26ae586b92097eb4396b1db0.png)'
- en: Photo by [Jarritos Mexican Soda](https://unsplash.com/@jarritos).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Jarritos Mexican Soda](https://unsplash.com/@jarritos)æ‹æ‘„ã€‚
- en: 'You can steer the model from image captioning to visual question answering
    by prompting it with appropriate instructions:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡é€‚å½“çš„æŒ‡ç¤ºå°†æ¨¡å‹ä»å›¾åƒå­—å¹•è½¬å‘è§†è§‰é—®é¢˜å›ç­”ï¼š
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Image classification
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: IDEFICS is capable of classifying images into different categories without being
    explicitly trained on data containing labeled examples from those specific categories.
    Given a list of categories and using its image and text understanding capabilities,
    the model can infer which category the image likely belongs to.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: IDEFICSèƒ½å¤Ÿå°†å›¾åƒåˆ†ç±»ä¸ºä¸åŒçš„ç±»åˆ«ï¼Œè€Œæ— éœ€æ˜ç¡®åœ¨åŒ…å«æ¥è‡ªè¿™äº›ç‰¹å®šç±»åˆ«çš„æ ‡è®°ç¤ºä¾‹çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç»™å®šä¸€ç»„ç±»åˆ«å¹¶åˆ©ç”¨å…¶å›¾åƒå’Œæ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œæ¨¡å‹å¯ä»¥æ¨æ–­å›¾åƒå¯èƒ½å±äºå“ªä¸ªç±»åˆ«ã€‚
- en: 'Say, we have this image of a vegetable stand:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªè”¬èœæ‘Šçš„å›¾ç‰‡ï¼š
- en: '![Image of a vegetable stand](../Images/0f1d778c8084e2f63cd81d21fdb55d1b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![è”¬èœæ‘Šçš„å›¾ç‰‡](../Images/0f1d778c8084e2f63cd81d21fdb55d1b.png)'
- en: Photo by [Peter Wendt](https://unsplash.com/@peterwendt).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Peter Wendt](https://unsplash.com/@peterwendt)æ‹æ‘„ã€‚
- en: 'We can instruct the model to classify the image into one of the categories
    that we have:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æŒ‡ç¤ºæ¨¡å‹å°†å›¾åƒåˆ†ç±»ä¸ºæˆ‘ä»¬æ‹¥æœ‰çš„ç±»åˆ«ä¹‹ä¸€ï¼š
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the example above we instruct the model to classify the image into a single
    category, however, you can also prompt the model to do rank classification.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æŒ‡ç¤ºæ¨¡å‹å°†å›¾åƒåˆ†ç±»ä¸ºå•ä¸ªç±»åˆ«ï¼Œä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥æç¤ºæ¨¡å‹è¿›è¡Œæ’ååˆ†ç±»ã€‚
- en: Image-guided text generation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒå¼•å¯¼çš„æ–‡æœ¬ç”Ÿæˆ
- en: For more creative applications, you can use image-guided text generation to
    generate text based on an image. This can be useful to create descriptions of
    products, ads, descriptions of a scene, etc.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´æœ‰åˆ›æ„çš„åº”ç”¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºäºå›¾åƒçš„æ–‡æœ¬ç”Ÿæˆæ¥æ ¹æ®å›¾åƒç”Ÿæˆæ–‡æœ¬ã€‚è¿™å¯ä»¥ç”¨äºåˆ›å»ºäº§å“æè¿°ã€å¹¿å‘Šã€åœºæ™¯æè¿°ç­‰ã€‚
- en: 'Letâ€™s prompt IDEFICS to write a story based on a simple image of a red door:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æç¤ºIDEFICSæ ¹æ®ä¸€æ‰‡çº¢é—¨çš„ç®€å•å›¾åƒæ’°å†™ä¸€ä¸ªæ•…äº‹ï¼š
- en: '![Image of a red door with a pumpkin on the steps](../Images/6e5de95de1888a89f56e9744b5af7215.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸€æ‰‡çº¢é—¨ä¸Šæœ‰ä¸€ä¸ªå—ç“œçš„å›¾ç‰‡](../Images/6e5de95de1888a89f56e9744b5af7215.png)'
- en: Photo by [Craig Tidball](https://unsplash.com/@devonshiremedia).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Craig Tidball](https://unsplash.com/@devonshiremedia)æä¾›ã€‚
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Looks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky
    Halloween story about a ghost.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥IDEFICSæ³¨æ„åˆ°äº†é—¨å»Šä¸Šçš„å—ç“œï¼Œå¹¶é€‰æ‹©äº†ä¸€ä¸ªå…³äºé¬¼é­‚çš„ææ€–ä¸‡åœ£èŠ‚æ•…äº‹ã€‚
- en: For longer outputs like this, you will greatly benefit from tweaking the text
    generation strategy. This can help you significantly improve the quality of the
    generated output. Check out [Text generation strategies](../generation_strategies)
    to learn more.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåƒè¿™æ ·çš„è¾ƒé•¿è¾“å‡ºï¼Œæ‚¨å°†å—ç›Šäºè°ƒæ•´æ–‡æœ¬ç”Ÿæˆç­–ç•¥ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨æ˜¾ç€æé«˜ç”Ÿæˆè¾“å‡ºçš„è´¨é‡ã€‚æŸ¥çœ‹[æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: Running inference in batch mode
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹é‡æ¨¡å¼ä¸‹è¿è¡Œæ¨ç†
- en: 'All of the earlier sections illustrated IDEFICS for a single example. In a
    very similar fashion, you can run inference for a batch of examples by passing
    a list of prompts:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„æ‰€æœ‰éƒ¨åˆ†éƒ½å±•ç¤ºäº†IDEFICSçš„ä¸€ä¸ªç¤ºä¾‹ã€‚ä»¥éå¸¸ç›¸ä¼¼çš„æ–¹å¼ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¼ é€’æç¤ºåˆ—è¡¨æ¥ä¸ºä¸€æ‰¹ç¤ºä¾‹è¿è¡Œæ¨ç†ï¼š
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: IDEFICS instruct for conversational use
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨äºä¼šè¯ä½¿ç”¨çš„IDEFICSæŒ‡å¯¼
- en: 'For conversational use cases, you can find fine-tuned instructed versions of
    the model on the ğŸ¤— Hub: `HuggingFaceM4/idefics-80b-instruct` and `HuggingFaceM4/idefics-9b-instruct`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¼šè¯ä½¿ç”¨æƒ…å†µï¼Œæ‚¨å¯ä»¥åœ¨ğŸ¤— Hubä¸Šæ‰¾åˆ°æ¨¡å‹çš„ç»è¿‡å¾®è°ƒçš„æŒ‡å¯¼ç‰ˆæœ¬ï¼š`HuggingFaceM4/idefics-80b-instruct`å’Œ`HuggingFaceM4/idefics-9b-instruct`ã€‚
- en: These checkpoints are the result of fine-tuning the respective base models on
    a mixture of supervised and instruction fine-tuning datasets, which boosts the
    downstream performance while making the models more usable in conversational settings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ£€æŸ¥ç‚¹æ˜¯åœ¨æ··åˆç›‘ç£å’ŒæŒ‡å¯¼å¾®è°ƒæ•°æ®é›†ä¸Šå¯¹å„è‡ªåŸºæœ¬æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ç»“æœï¼Œè¿™å¯ä»¥æé«˜ä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒæ—¶ä½¿æ¨¡å‹åœ¨ä¼šè¯è®¾ç½®ä¸­æ›´æ˜“äºä½¿ç”¨ã€‚
- en: 'The use and prompting for the conversational use is very similar to using the
    base models:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼šè¯ä½¿ç”¨å’Œæç¤ºä¸ä½¿ç”¨åŸºæœ¬æ¨¡å‹éå¸¸ç›¸ä¼¼ï¼š
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
