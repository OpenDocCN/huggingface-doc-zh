# BridgeTower

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bridgetower](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bridgetower)

## 概述

BridgeTower模型是由Xiao Xu、Chenfei Wu、Shachar Rosenman、Vasudev Lal、Wanxiang Che、Nan Duan在[《BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning》](https://arxiv.org/abs/2206.08657)中提出的。该模型的目标是在每个交叉模态编码器的每一层之间建立桥梁，以实现全面和详细的交互，从而在各种下游任务中取得显著的性能，几乎没有额外的性能和计算成本。

本文已被[AAAI’23](https://aaai.org/Conferences/AAAI-23/)会议接受。

论文摘要如下：

*近年来，具有双塔架构的视觉语言（VL）模型在视觉语言表示学习中占据主导地位。当前的VL模型要么使用轻量级的单模编码器并学习同时提取、对齐和融合两种模态，要么将深度预训练的单模编码器的最后一层单模表示馈送到顶部交叉模态编码器中。这两种方法都可能限制视觉语言表示学习并限制模型性能。在本文中，我们提出了BRIDGETOWER，它引入了多个桥接层，建立了单模编码器的顶层与交叉模态编码器的每一层之间的连接。这使得在交叉模态编码器中能够有效地进行自底向上的跨模态对齐和融合，从而实现不同语义级别的预训练单模编码器的视觉和文本表示之间的交叉模态对齐和融合。仅使用4M张图像进行预训练，BRIDGETOWER在各种下游视觉语言任务上实现了最先进的性能。特别是在VQAv2测试集上，BRIDGETOWER实现了78.73%的准确率，比之前的最先进模型METER高出1.09%，使用相同的预训练数据几乎没有额外的参数和计算成本。值得注意的是，当进一步扩展模型时，BRIDGETOWER实现了81.15%的准确率，超过了在数量级更大的数据集上进行预训练的模型。*

![drawing](../Images/2cf6cac1e694d76fda85d82f66c9a1aa.png) BridgeTower架构。摘自[原始论文。](https://arxiv.org/abs/2206.08657)

该模型由[Anahita Bhiwandiwalla](https://huggingface.co/anahita-b)、[Tiep Le](https://huggingface.co/Tile)和[Shaoyen Tseng](https://huggingface.co/shaoyent)贡献。原始代码可以在[这里](https://github.com/microsoft/BridgeTower)找到。

## 使用提示和示例

BridgeTower包括一个视觉编码器、一个文本编码器和一个带有多个轻量级桥接层的交叉模态编码器。该方法的目标是在每个交叉模态编码器的每一层之间建立桥梁，以实现全面和详细的交互。原则上，可以在提出的架构中应用任何视觉、文本或交叉模态编码器。

[BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)将[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)和[BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor)封装成一个单一实例，用于同时对文本进行编码和准备图像。

以下示例展示了如何使用[BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)和[BridgeTowerForContrastiveLearning](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForContrastiveLearning)来运行对比学习。

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
>>> import requests
>>> from PIL import Image

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")
>>> model = BridgeTowerForContrastiveLearning.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")

>>> # forward pass
>>> scores = dict()
>>> for text in texts:
...     # prepare inputs
...     encoding = processor(image, text, return_tensors="pt")
...     outputs = model(**encoding)
...     scores[text] = outputs
```

以下示例显示如何使用[BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)和[BridgeTowerForImageAndTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForImageAndTextRetrieval)运行图像文本检索。

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
>>> import requests
>>> from PIL import Image

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
>>> model = BridgeTowerForImageAndTextRetrieval.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

>>> # forward pass
>>> scores = dict()
>>> for text in texts:
...     # prepare inputs
...     encoding = processor(image, text, return_tensors="pt")
...     outputs = model(**encoding)
...     scores[text] = outputs.logits[0, 1].item()
```

以下示例显示如何使用[BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)和[BridgeTowerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForMaskedLM)运行掩码语言建模。

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000360943.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
>>> text = "a <mask> looking out of the window"

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
>>> model = BridgeTowerForMaskedLM.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

>>> # prepare inputs
>>> encoding = processor(image, text, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**encoding)

>>> results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())

>>> print(results)
.a cat looking out of the window.
```

提示：

+   此BridgeTower的实现使用[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)生成文本嵌入，并使用OpenAI的CLIP/ViT模型计算视觉嵌入。

+   发布了预训练的[bridgeTower-base](https://huggingface.co/BridgeTower/bridgetower-base)和[bridgetower masked language modeling and image text matching](https://huggingface.co/BridgeTower/bridgetower-base-itm-mlm)的检查点。

+   请参考[Table 5](https://arxiv.org/pdf/2206.08657.pdf)了解BridgeTower在图像检索和其他下游任务上的性能。

+   此模型的PyTorch版本仅在torch 1.10及更高版本中可用。

## BridgeTowerConfig

### `class transformers.BridgeTowerConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L243)

```py
( share_cross_modal_transformer_layers = True hidden_act = 'gelu' hidden_size = 768 initializer_factor = 1 layer_norm_eps = 1e-05 share_link_tower_layers = False link_tower_type = 'add' num_attention_heads = 12 num_hidden_layers = 6 tie_word_embeddings = False init_layernorm_from_vision_encoder = False text_config = None vision_config = None **kwargs )
```

参数

+   `share_cross_modal_transformer_layers` (`bool`, *optional*, defaults to `True`) — 是否共享跨模态transformer层。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `initializer_factor` (`float`, *optional*, defaults to 1) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。

+   `share_link_tower_layers` (`bool`, *optional*, defaults to `False`) — 是否共享桥/链接塔层。

+   `link_tower_type` (`str`, *optional*, defaults to `"add"`) — 桥/链接层的类型。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。

+   `num_hidden_layers` (`int`, *optional*, defaults to 6) — Transformer编码器中的隐藏层数量。

+   `tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — 是否绑定输入和输出嵌入。

+   `init_layernorm_from_vision_encoder` (`bool`, *optional*, defaults to `False`) — 是否从视觉编码器初始化LayerNorm。

+   `text_config` (`dict`, *optional*) — 用于初始化[BridgeTowerTextConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerTextConfig)的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化[BridgeTowerVisionConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerVisionConfig)的配置选项字典。

这是用于存储[BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel)配置的配置类。根据指定的参数实例化一个BridgeTower模型，定义模型架构。使用默认值实例化配置将产生与bridgetower-base [BridgeTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)架构类似的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 并可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例:

```py
>>> from transformers import BridgeTowerModel, BridgeTowerConfig

>>> # Initializing a BridgeTower BridgeTower/bridgetower-base style configuration
>>> configuration = BridgeTowerConfig()

>>> # Initializing a model from the BridgeTower/bridgetower-base style configuration
>>> model = BridgeTowerModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `from_text_vision_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L344)

```py
( text_config: BridgeTowerTextConfig vision_config: BridgeTowerVisionConfig **kwargs )
```

从 BridgeTower 文本模型配置中实例化一个 [BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)（或派生类）。返回: [BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig): 配置对象的实例

## BridgeTowerTextConfig

### `class transformers.BridgeTowerTextConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L121)

```py
( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 initializer_factor = 1 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size = 1 layer_norm_eps = 1e-05 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为50265) — 模型文本部分的词汇表大小。定义了在调用 [BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel) 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*, 默认为12) — Transformer 编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *可选*, 默认为3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` 或 `Callable`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *可选*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `attention_probs_dropout_prob` (`float`, *可选*, 默认为0.1) — 注意力概率的丢弃比率。

+   `max_position_embeddings` (`int`, *可选*, 默认为514) — 此模型可能使用的最大序列长度。通常将其设置为一个较大的值以防万一（例如512、1024或2048）。

+   `type_vocab_size` (`int`, *可选*, 默认为2) — `token_type_ids` 的词汇表大小。

+   `initializer_factor` (`float`, *可选*, 默认为1) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

+   `layer_norm_eps` (`float`, *可选*, 默认为1e-05) — 层归一化层使用的 epsilon。

+   `position_embedding_type` (`str`, *可选*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`、`"relative_key"`、`"relative_key_query"`中的一个。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658) 中的 *Method 4*。

+   `is_decoder` (`bool`, *可选*, 默认为`False`) — 模型是否用作解码器。如果为`False`，则模型用作编码器。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在`config.is_decoder=True`时相关。

这是用于存储[BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel)的文本配置的配置类。这里的默认值是从RoBERTa复制的。使用默认值实例化配置将产生与bridgetower-base [BridegTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import BridgeTowerTextConfig

>>> # Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the text model
>>> configuration = BridgeTowerTextConfig()

>>> # Accessing the configuration
>>> configuration
```

## BridgeTowerVisionConfig

### `class transformers.BridgeTowerVisionConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L34)

```py
( hidden_size = 768 num_hidden_layers = 12 num_channels = 3 patch_size = 16 image_size = 288 initializer_factor = 1 layer_norm_eps = 1e-05 stop_gradient = False share_layernorm = True remove_last_layer = False **kwargs )
```

参数

+   `hidden_size` (`int`，*可选*，默认为768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`，*可选*，默认为12) — 视觉编码器模型中的隐藏层数量。

+   `patch_size` (`int`，*可选*，默认为16) — 每个补丁的大小（分辨率）。

+   `image_size` (`int`，*可选*，默认为288) — 每个图像的大小（分辨率）。

+   `initializer_factor` (`float`，*可选*，默认为1) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

+   `layer_norm_eps` (`float`，*可选*，默认为1e-05) — 层归一化层使用的epsilon。

+   `stop_gradient` (`bool`，*可选*，默认为`False`) — 是否停止训练的梯度。

+   `share_layernorm` (`bool`，*可选*，默认为`True`) — 是否共享LayerNorm层。

+   `remove_last_layer` (`bool`，*可选*，默认为`False`) — 是否从视觉编码器中移除最后一层。

这是用于存储[BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel)的视觉配置的配置类。使用默认值实例化配置将产生与bridgetower-base [BridgeTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import BridgeTowerVisionConfig

>>> # Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the vision model
>>> configuration = BridgeTowerVisionConfig()

>>> # Accessing the configuration
>>> configuration
```

## BridgeTowerImageProcessor

### `class transformers.BridgeTowerImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/image_processing_bridgetower.py#L123)

```py
( do_resize: bool = True size: Dict = 288 size_divisor: int = 32 resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_center_crop: bool = True do_pad: bool = True **kwargs )
```

参数

+   `do_resize` (`bool`，*可选*，默认为`True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。

+   `size` (`Dict[str, int]` *可选*，默认为288) — 将输入的较短边调整为`size["shortest_edge"]`。较长边将受限于`int((1333 / 800) * size["shortest_edge"])`，同时保持纵横比。仅在`do_resize`设置为`True`时有效。可以被`preprocess`方法中的`size`参数覆盖。

+   `size_divisor` (`int`，*可选*，默认为32) — 确保高度和宽度都可以被划分的大小。仅在`do_resize`设置为`True`时有效。可以被`preprocess`方法中的`size_divisor`参数覆盖。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则使用的重采样滤波器。仅在 `do_resize` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *可选*, 默认为 `True`) — 是否按照指定的比例 `rescale_factor` 对图像进行重新缩放。可以被 `preprocess` 方法中的 `do_rescale` 参数覆盖。

+   `rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新缩放图像，则使用的缩放因子。仅在 `do_rescale` 设置为 `True` 时有效。

+   `do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行归一化。可以被 `preprocess` 方法中的 `do_normalize` 参数覆盖。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_MEAN`) — 如果归一化图像，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_STD`) — 如果归一化图像，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

+   `do_center_crop` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行中心裁剪。可以被 `preprocess` 方法中的 `do_center_crop` 参数覆盖。

+   `do_pad` (`bool`, *可选*, 默认为 `True`) — 是否将图像填充到批次中图像的 `(max_height, max_width)`。可以被 `preprocess` 方法中的 `do_pad` 参数覆盖。

构建一个 BridgeTower 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/image_processing_bridgetower.py#L367)

```py
( images: Union do_resize: Optional = None size: Optional = None size_divisor: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None do_center_crop: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望传入单个图像或图像批次，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 控制 `resize` 后图像的大小。图像的最短边被调整为 `size["shortest_edge"]`，同时保持纵横比。如果调整后图像的最长边 > `int(size["shortest_edge"] * (1333 / 800))`，则再次调整图像大小，使最长边等于 `int(size["shortest_edge"] * (1333 / 800))`。

+   `size_divisor` (`int`, *可选*, 默认为 `self.size_divisor`) — 将图像调整为此值的倍数。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `self.resample`) — 如果调整图像大小，则使用的重采样滤波器。仅在 `do_resize` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放到 [0 - 1] 之间。

+   `rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则用于重新缩放图像的缩放因子。

+   `do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 如果 `do_normalize` 设置为 `True`，则用于归一化图像的图像均值。

+   `image_std`（`float`或`List[float]`，*可选*，默认为`self.image_std`）— 如果`do_normalize`设置为`True`，用于归一化图像的图像标准差。

+   `do_pad`（`bool`，*可选*，默认为`self.do_pad`）— 是否将图像填充到批处理中的(max_height, max_width)。如果为`True`，还会创建并返回像素掩码。

+   `do_center_crop`（`bool`，*可选*，默认为`self.do_center_crop`）— 是否对图像进行中心裁剪。如果输入尺寸小于任何边缘的`crop_size`，则图像将填充为0，然后进行中心裁剪。

+   `return_tensors`（`str`或`TensorType`，*可选*）— 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个`np.ndarray`列表。

    +   `TensorType.TENSORFLOW`或`'tf'`：返回一个`tf.Tensor`类型的批处理。

    +   `TensorType.PYTORCH`或`'pt'`：返回一个`torch.Tensor`类型的批处理。

    +   `TensorType.NUMPY`或`'np'`：返回一个`np.ndarray`类型的批处理。

    +   `TensorType.JAX`或`'jax'`：返回一个`jax.numpy.ndarray`类型的批处理。

+   `data_format`（`ChannelDimension`或`str`，*可选*，默认为`ChannelDimension.FIRST`）— 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以(num_channels, height, width)格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以(height, width, num_channels)格式。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format`（`ChannelDimension`或`str`，*可选*）— 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以(num_channels, height, width)格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以(height, width, num_channels)格式。

    +   `"none"`或`ChannelDimension.NONE`：图像以(height, width)格式。

预处理一张图片或一批图片。

## BridgeTowerProcessor

### `class transformers.BridgeTowerProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/processing_bridgetower.py#L26)

```py
( image_processor tokenizer )
```

参数

+   `image_processor`（`BridgeTowerImageProcessor`）— 一个[BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor)的实例。图像处理器是必需的输入。

+   `tokenizer`（`RobertaTokenizerFast`）— 一个[‘RobertaTokenizerFast`]的实例。分词器是必需的输入。

构建一个BridgeTower处理器，将一个Roberta分词器和一个BridgeTower图像处理器包装成一个处理器。

[BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)提供了[BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor)和[RobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizerFast)的所有功能。查看[**call**()](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor.__call__)和`decode()`的文档字符串以获取更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/processing_bridgetower.py#L49)

```py
( images text: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

此方法使用[BridgeTowerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)方法准备模型的图像，并使用[RobertaTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)准备模型的文本。

有关更多信息，请参考上述两种方法的文档字符串。

## BridgeTowerModel

### `class transformers.BridgeTowerModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1197)

```py
( config )
```

参数

+   `config` ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的 BridgeTower 模型，输出 BridgeTowerModelOutput 对象，没有特定的头部在顶部。这个模型是 PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1265)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None image_token_type_idx: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `({0})`) — 词汇表中输入序列标记的索引。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

+   `attention_mask` (`torch.FloatTensor`，形状为 `({0})`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被 `掩码` 的标记，

    +   0 表示被 `掩码` 的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为 `({0})`，*可选*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于一个 *句子 A* 标记，

    +   1 对应于一个 *句子 B* 标记。[什么是标记类型 ID？](../glossary#token-type-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。可以使用 [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor) 获取像素值。有关详细信息，请参阅 [BridgeTowerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `pixel_mask` (`torch.LongTensor`，形状为 `(batch_size, height, width)`，*可选*) — 避免在填充像素值上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示真实的像素（即未被 `掩码`），

    +   0 表示填充的像素（即被 `掩码`）。`什么是注意力掩码？ <../glossary.html#attention-mask>`__

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被 `掩码`，

    +   0 表示头部被 `掩码`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为 `({0}, hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是模型的内部嵌入查找矩阵。

+   `image_embeds` (`torch.FloatTensor`，形状为 `(batch_size, num_patches, hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示，而不是传递 `pixel_values`。如果您想要更多控制如何将 `pixel_values` 转换为补丁嵌入，这将非常有用。

+   `image_token_type_idx` (`int`，*可选*) —

    +   图像的标记类型 ID。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `output_hidden_states` (`bool`, *optional*) — 如果设置为`True`，则返回隐藏状态作为一个列表，分别包含文本、图像和跨模态组件的隐藏状态。即`(hidden_states_text, hidden_states_image, hidden_states_cross_modal)`，其中每个元素都是对应模态的隐藏状态列表。`hidden_states_txt/img`是对应单模态隐藏状态的张量列表，`hidden_states_cross_modal`是一个包含每个桥接层的`cross_modal_text_hidden_states`和`cross_modal_image_hidden_states`的元组列表。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 目前不支持标签。

返回

`transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)）和输入的不同元素。

+   `text_features` (`torch.FloatTensor`，形状为`(batch_size, text_sequence_length, hidden_size)`) — 模型最后一层文本输出的隐藏状态序列。

+   `image_features` (`torch.FloatTensor`，形状为`(batch_size, image_sequence_length, hidden_size)`) — 模型最后一层图像输出的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size x 2)`) — 文本和图像序列的第一个标记（分类标记）的最后一层隐藏状态的连接，分别经过用于辅助预训练任务的层进一步处理。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerModel
>>> from PIL import Image
>>> import requests

>>> # prepare image and text
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text = "hello world"
>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base")
>>> model = BridgeTowerModel.from_pretrained("BridgeTower/bridgetower-base")

>>> inputs = processor(image, text, return_tensors="pt")
>>> outputs = model(**inputs)
>>> outputs.keys()
odict_keys(['text_features', 'image_features', 'pooler_output'])
```

## BridgeTowerForContrastiveLearning

### `class transformers.BridgeTowerForContrastiveLearning`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1761)

```py
( config )
```

参数

+   `config` ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部具有图像文本对比头部的BridgeTower模型，计算图像文本对比损失。

这个模型是PyTorch的`torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1781)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = True return_dict: Optional = None return_loss: Optional = None ) → export const metadata = 'undefined';transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`({0})`) — 词汇表中输入序列标记的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。[什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为`({0})`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示`未被遮罩`的标记，

    +   0 表示`被遮罩`的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为`({0})`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：

    +   0 对应于*句子A*标记，

    +   1 对应于*句子B*标记。[什么是标记类型ID？](../glossary#token-type-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。可以使用[BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor)获取像素值。查看[BridgeTowerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)获取详细信息。

+   `pixel_mask` (`torch.LongTensor`，形状为`(batch_size, height, width)`，*optional*) — 用于避免在填充像素值上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示真实像素（即`未被遮罩`），

    +   0 表示填充像素（即`被遮罩`）。`什么是注意力掩码？<../glossary.html#attention-mask>`__

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部`未被遮罩`，

    +   0 表示头部`被遮罩`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`({0}, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为与模型内部嵌入查找矩阵相关联的向量，则这很有用。

+   `image_embeds` (`torch.FloatTensor`，形状为`(batch_size, num_patches, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`pixel_values`。如果您想要更多控制如何将`pixel_values`转换为补丁嵌入，则这很有用。

+   `image_token_type_idx` (`int`, *optional*) —

    +   图像的标记类型ID。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

返回

`transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput` 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False` 或 `config.return_dict=False`）包含根据配置（[BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)）和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当 `return_loss` 为 `True` 时返回） — 图像-文本对比损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 前每个词汇标记的分数）。

+   `text_embeds` (`torch.FloatTensor)`，*可选*，当使用 `with_projection=True` 初始化模型时返回） — 通过将投影层应用于 pooler_output 获得的文本嵌入。

+   `image_embeds` (`torch.FloatTensor)`，*可选*，当使用 `with_projection=True` 初始化模型时返回） — 通过将投影层应用于 pooler_output 获得的图像嵌入。

+   `cross_embeds` (`torch.FloatTensor)`，*可选*，当使用 `with_projection=True` 初始化模型时返回） — 通过将投影层应用于 pooler_output 获得的文本-图像跨模态嵌入。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组。模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组。

[BridgeTowerForContrastiveLearning](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForContrastiveLearning) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在之后调用 `Module` 实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
>>> import requests
>>> from PIL import Image
>>> import torch

>>> image_urls = [
...     "https://farm4.staticflickr.com/3395/3428278415_81c3e27f15_z.jpg",
...     "http://images.cocodataset.org/val2017/000000039769.jpg",
... ]
>>> texts = ["two dogs in a car", "two cats sleeping on a couch"]
>>> images = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")
>>> model = BridgeTowerForContrastiveLearning.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")

>>> inputs = processor(images, texts, padding=True, return_tensors="pt")
>>> loss = model(**inputs, return_loss=True).loss

>>> inputs = processor(images, texts[::-1], padding=True, return_tensors="pt")
>>> loss_swapped = model(**inputs, return_loss=True).loss

>>> print("Loss", round(loss.item(), 4))
Loss 0.0019

>>> print("Loss with swapped images", round(loss_swapped.item(), 4))
Loss with swapped images 2.126
```

## BridgeTowerForMaskedLM

### `class transformers.BridgeTowerForMaskedLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1541)

```py
( config )
```

参数

+   `config` ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

BridgeTower 模型在预训练期间在顶部具有语言建模头。

这个模型是 PyTorch 的 `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1565)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被遮蔽的标记，

    +   0 表示被遮蔽的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 的标记，

    +   1 对应于 *句子 B* 的标记。[什么是标记类型 ID？](../glossary#token-type-ids)

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。可以使用 [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor) 获取像素值。有关详细信息，请参阅 [BridgeTowerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*) — 用于避免在填充像素值上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示真实的像素（即未被遮蔽），

    +   0 表示填充的像素（即 `masked`）。`什么是注意力掩码？<../glossary.html#attention-mask>`__

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是模型的内部嵌入查找矩阵，这将非常有用。

+   `image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `pixel_values`。如果您想要更多控制如何将 `pixel_values` 转换为补丁嵌入，这将非常有用。

+   `image_token_type_idx` (`int`, *optional*) —

    +   图像的标记类型 ID。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通的元组。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`内（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

返回

[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括各种元素，取决于配置（[BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 掩码语言建模（MLM）损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BridgeTowerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000360943.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
>>> text = "a <mask> looking out of the window"

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
>>> model = BridgeTowerForMaskedLM.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

>>> # prepare inputs
>>> encoding = processor(image, text, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**encoding)

>>> results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())

>>> print(results)
.a cat looking out of the window.
```

## BridgeTowerForImageAndTextRetrieval

### `class transformers.BridgeTowerForImageAndTextRetrieval`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1649)

```py
( config )
```

参数

+   `config`（[BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

BridgeTower模型变压器，顶部带有分类器头（在[CLS]标记的最终隐藏状态之上的线性层），用于图像到文本匹配。

这个模型是一个PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1667)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `({0})`) — 词汇表中输入序列标记的索引。 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。 有关详细信息，请参见 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。 [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。 选择的掩码值在 `[0, 1]` 之间：

    +   值为 1 的标记是 `not masked`，

    +   值为 0 的标记是 `masked`。 [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。 索引在 `[0, 1]` 中选择：

    +   值为 0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。 [什么是标记类型 ID？](../glossary#token-type-ids)

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。 可以使用 [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor) 获取像素值。 有关详细信息，请参见 [BridgeTowerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*) — 用于避免在填充像素值上执行注意力的掩码。 选择的掩码值在 `[0, 1]` 之间：

    +   值为 1 的像素是真实的（即 `not masked`），

    +   填充像素的值为 0（即 `masked`）。 `什么是注意力掩码？ <../glossary.html#attention-mask>`__

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。 选择的掩码值在 `[0, 1]` 之间：

    +   值为 1 表示头部未被 `masked`，

    +   值为 0 表示头部被 `masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。 如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是模型的内部嵌入查找矩阵，这将非常有用。

+   `image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `pixel_values`。 如果您想要更多控制如何将 `pixel_values` 转换为补丁嵌入，这将非常有用。

+   `image_token_type_idx` (`int`, *optional*) —

    +   图像的标记类型 ID。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。 有关更多详细信息，请参见返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。 有关更多详细信息，请参见返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*) — 用于计算图像文本匹配损失的标签。 0 表示配对不匹配，1 表示匹配。 标签为 0 的配对将被跳过计算。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（[BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） - 分类（如果config.num_labels==1则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`） - 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[BridgeTowerForImageAndTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForImageAndTextRetrieval)的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
>>> import requests
>>> from PIL import Image

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
>>> model = BridgeTowerForImageAndTextRetrieval.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

>>> # forward pass
>>> scores = dict()
>>> for text in texts:
...     # prepare inputs
...     encoding = processor(image, text, return_tensors="pt")
...     outputs = model(**encoding)
...     scores[text] = outputs.logits[0, 1].item()
```
