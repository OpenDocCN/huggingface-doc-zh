# 修补

> 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint)

稳定扩散模型也可以应用于修补，通过提供掩模和文本提示，使用稳定扩散编辑图像的特定部分。

## 提示

建议使用已经专门为修补而微调的检查点与此管道一起使用，例如[runwayml/stable-diffusion-inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting)。默认的文本到图像稳定扩散检查点，例如[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)也兼容，但它们可能性能较差。

确保查看稳定扩散[Tips](overview#tips)部分，了解如何探索调度器速度和质量之间的权衡，以及如何高效地重用管道组件！

如果您有兴趣使用任务的官方检查点之一，请探索[CompVis](https://huggingface.co/CompVis)、[Runway](https://huggingface.co/runwayml)和[Stability AI](https://huggingface.co/stabilityai) Hub 组织！

## StableDiffusionInpaintPipeline

### `class diffusers.StableDiffusionInpaintPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L222)

```py
( vae: Union text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

参数

+   `vae`（[`AutoencoderKL`、`AsymmetricAutoencoderKL`]）- 变分自动编码器（VAE）模型，用于对图像进行编码和解码到和从潜在表示。

+   `text_encoder`（`CLIPTextModel`）- 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）- 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）- 用于去噪编码图像潜在特征的`UNet2DConditionModel`。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）- 与`unet`结合使用以去噪编码图像潜在特征的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `safety_checker`（`StableDiffusionSafetyChecker`）- 估计生成的图像是否可能被视为具有冒犯性或有害性的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）- 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

用于使用稳定扩散进行文本引导图像修补的管道。

这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载 LoRA 权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存 LoRA 权重

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载 IP 适配器

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) 用于加载`.ckpt`文件

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L1010)

```py
( prompt: Union = None image: Union = None mask_image: Union = None masked_image_latents: FloatTensor = None height: Optional = None width: Optional = None padding_mask_crop: Optional = None strength: float = 1.0 num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: int = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`) — 表示要修复的图像批次的图像、numpy数组或张量（要用`mask_image`遮罩的图像部分，并根据`prompt`重新绘制）。对于numpy数组和pytorch张量，期望值范围在`[0, 1]`之间。如果是张量或张量列表，则期望形状应为`(B, C, H, W)`或`(C, H, W)`。如果是numpy数组或数组列表，则期望形状应为`(B, H, W, C)`或`(H, W, C)`。它还可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。

+   `mask_image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 表示要遮罩`image`的图像批次的图像、numpy数组或张量。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。如果`mask_image`是PIL图像，则在使用之前将其转换为单通道（亮度）。如果是numpy数组或pytorch张量，则应包含一个颜色通道（L）而不是3，因此pytorch张量的预期形状将是`(B, 1, H, W)`、`(B, H, W)`、`(1, H, W)`、`(H, W)`。对于numpy数组，预期形状将是`(B, H, W, 1)`、`(B, H, W)`、`(H, W, 1)`或`(H, W)`。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `padding_mask_crop` (`int`, *可选*, 默认为`None`) — 要应用于图像和遮罩的裁剪边距大小。如果为`None`，则不对图像和mask_image应用裁剪。如果`padding_mask_crop`不是`None`，它将首先找到一个具有与图像相同纵横比且包含所有遮罩区域的矩形区域，然后根据`padding_mask_crop`扩展该区域。然后将根据扩展区域对图像和mask_image进行裁剪，然后将其调整为修复的原始图像大小。当遮罩区域较小而图像较大且包含与修复无关的信息（如背景）时，这很有用。

+   `strength` (`float`, *可选*, 默认为1.0) — 表示转换参考`image`的程度。必须在0和1之间。`image`用作起点，`strength`越高，添加的噪音越多。降噪步骤的数量取决于最初添加的噪音量。当`strength`为1时，添加的噪音最大，降噪过程将运行指定的`num_inference_steps`的完整迭代次数。值为1基本上忽略`image`。

+   `num_inference_steps` (`int`, *可选*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。此参数由`strength`调节。

+   `timesteps` (`List[int]`, *optional*) — 用于具有支持 `set_timesteps` 方法中的 `timesteps` 参数的调度器的去噪过程的自定义时间步。如果未定义，则将使用传递 `num_inference_steps` 时的默认行为。必须按降序排列。

+   `guidance_scale` (`float`, *optional*, 默认为 7.5) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中将被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。ip_adapter_image — (`PipelineImageInput`, *optional*): 可选的图像输入，用于与 IP 适配器一起使用。

+   `output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定了的 kwargs 字典将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor`。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 推理过程中在每个去噪步骤结束时调用的函数。该函数使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 或 `tuple`

如果 `return_dict` 为 `True`，则返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则返回一个 `tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”（nsfw）内容的 `bool` 列表。

生成管道的调用函数。

示例：

```py
>>> import PIL
>>> import requests
>>> import torch
>>> from io import BytesIO

>>> from diffusers import StableDiffusionInpaintPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
>>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

>>> init_image = download_image(img_url).resize((512, 512))
>>> mask_image = download_image(mask_url).resize((512, 512))

>>> pipe = StableDiffusionInpaintPipeline.from_pretrained(
...     "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
>>> image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]
```

#### `enable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size` (`str` 或 `int`，*可选*，默认为 `"auto"`) — 当为 `"auto"` 时，将输入划分为注意力头的一半，因此注意力将分两步计算。如果为 `"max"`，将通过一次只运行一个切片来节省最大内存量。如果提供一个数字，则使用 `attention_head_dim // slice_size` 作为切片数量。在这种情况下，`attention_head_dim` 必须是 `slice_size` 的倍数。

启用切片注意力计算。启用此选项时，注意力模块将输入张量分割为多个切片，以便在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用PyTorch 2.0或xFormers的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了 `enable_attention_slicing`，则注意力将在一步中计算。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`，*可选*） — 用作 `xFormers` 的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的 `op` 参数的默认 `None` 操作符的覆盖。

启用来自[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。启用此选项时，您应该观察到较低的GPU内存使用量，并在推断期间可能加速。训练期间的加速不被保证。

⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用来自[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `load_textual_inversion`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `os.PathLike` 或 `List[str or os.PathLike]` 或 `Dict` 或 `List[Dict]`) — 可以是以下之一或它们的列表：

    +   一个字符串，预训练模型Hub上托管的*模型ID*（例如 `sd-concepts-library/low-poly-hd-logos-icons`）。

    +   包含文本反演权重的*目录*路径（例如 `./my_text_inversion_directory/`）。

    +   包含文本反演权重的*文件*路径（例如 `./my_text_inversions.pt`）。

    +   一个 [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token` (`str` 或 `List[str]`, *可选*) — 覆盖用于文本反转权重的令牌。如果 `pretrained_model_name_or_path` 是一个列表，则 `token` 也必须是相同长度的列表。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel), *可选*) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用 self.tokenizer。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer), *可选*) — 用于对文本进行标记化的 `CLIPTokenizer`。如果未指定，函数将使用 self.tokenizer。

+   `weight_name` (`str`, *可选*) — 自定义权重文件的名称。在以下情况下应使用此选项：

    +   保存的文本反转文件采用 🤗 Diffusers 格式，但保存在特定权重名称下，例如 `text_inv.bin`。

    +   保存的文本反转文件采用 Automatic1111 格式。

+   `cache_dir` (`Union[str, os.PathLike]`, *可选*) — 如果未使用标准缓存，则缓存已下载的预训练模型配置的目录路径。

+   `force_download` (`bool`, *可选*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为 `False`) — 是否继续下载模型权重和配置文件。如果设置为 `False`，则会删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *可选*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理将在每个请求中使用。

+   `local_files_only` (`bool`, *可选*, 默认为 `False`) — 是否仅加载本地模型权重和配置文件。如果设置为 `True`，则不会从 Hub 下载模型。

+   `token` (`str` 或 *bool*, *可选*) — 用作远程文件 HTTP 令牌的令牌。如果为 `True`，则使用从 `diffusers-cli login` 生成的令牌（存储在 `~/.huggingface` 中）。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `subfolder` (`str`, *可选*, 默认为 `""`) — 在 Hub 或本地较大模型存储库中的模型文件的子文件夹位置。

+   `mirror` (`str`, *可选*) — 如果您在中国下载模型时遇到可访问性问题，请将源镜像以解决问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反转嵌入加载到 [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline) 的文本编码器中（支持 🤗 Diffusers 和 Automatic1111 格式）。

示例：

要加载 🤗 Diffusers 格式的文本反转嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载 Automatic1111 格式的文本反转嵌入向量，请确保首先下载向量（例如从 [civitAI](https://civitai.com/models/3036?modelVersionId=9857)）然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `load_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict` (`str` 或 `os.PathLike` 或 `dict`) — 请参阅 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `kwargs` (`dict`, *可选*) — 请参阅 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是要加载的适配器总数。

将在`pretrained_model_name_or_path_or_dict`中指定的LoRA权重加载到`self.unet`和`self.text_encoder`中。

所有kwargs都将转发到`self.lora_state_dict`。

查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)以获取有关如何加载状态字典的更多详细信息。

查看[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)以获取有关如何将状态字典加载到`self.unet`中的更多详细信息。

查看[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)以获取有关如何加载状态字典到`self.text_encoder`中的更多详细信息。

#### `save_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

参数

+   `save_directory` (`str`或`os.PathLike`) — 保存LoRA参数的目录。如果不存在，将会创建。

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`) — 与`unet`对应的LoRA层的状态字典。

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`) — 与`text_encoder`对应的LoRA层的状态字典。必须显式传递文本编码器LoRA状态字典，因为它来自🤗 Transformers。

+   `is_main_process` (`bool`, *可选*, 默认为`True`) — 调用此函数的进程是否为主进程。在分布式训练中很有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`以避免竞争条件。

+   `save_function` (`Callable`) — 用于保存状态字典的函数。在分布式训练期间很有用，当您需要用另一种方法替换`torch.save`时。可以使用环境变量`DIFFUSERS_SAVE_MODE`进行配置。

+   `safe_serialization` (`bool`, *可选*, 默认为`True`) — 是否使用`safetensors`保存模型，或者使用传统的PyTorch方式与`pickle`保存。

保存对应于UNet和文本编码器的LoRA参数。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L889)

```py
( )
```

如果启用，则禁用FreeU机制。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L866)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 用于调节阶段1的缩放因子，以减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 用于阶段2的缩放因子，以减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 用于阶段1的缩放因子，以放大骨干特征的贡献。

+   `b2` (`float`) — 用于阶段2的缩放因子，以放大骨干特征的贡献。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[official repository](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L397)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示 device — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不要引导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。如果不使用指导（即，如果 `guidance_scale` 小于 `1`，则忽略）时忽略。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。

+   `lora_scale` (`float`, *可选*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从CLIP跳过的层数。值为1表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `fuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L894)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 对UNet应用融合。

+   `vae` (`bool`, 默认为 `True`) — 对VAE应用融合。

启用融合的QKV投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此API为🧪实验性质。

#### `get_guidance_scale_embedding`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L955)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为512) — 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为 `(len(timesteps), embedding_dim)` 的嵌入向量

参见 [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L926)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 对UNet应用融合。

+   `vae` (`bool`, 默认为 `True`) — 对VAE应用融合。

如果启用，请禁用QKV投影融合。

此API为🧪实验性质。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪PIL图像列表或形状为 `(batch_size, height, width, num_channels)` 的NumPy数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw），如果无法执行安全检查，则为 `None`。

稳定扩散管道的输出类。

## FlaxStableDiffusionInpaintPipeline

### `class diffusers.FlaxStableDiffusionInpaintPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py#L102)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor dtype: dtype = <class 'jax.numpy.float32'> )
```

参数

+   `vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL)) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel)) — 用于去噪编码图像潜变量的 `FlaxUNet2DConditionModel`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与 `unet` 结合使用的调度器，用于去噪编码图像潜变量。可以是 `FlaxDDIMScheduler`、`FlaxLMSDiscreteScheduler`、`FlaxPNDMScheduler` 或 `FlaxDPMSolverMultistepScheduler` 中的一个。

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) — 用于估计生成图像是否可能被视为具有冒犯性或有害的分类模块。请参考[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为 `safety_checker` 的输入。

基于 Flax 的管道，用于使用 Stable Diffusion 进行文本引导的图像修复。

🧪 这是一个实验性功能！

此模型继承自 [FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py#L394)

```py
( prompt_ids: Array mask: Array masked_image: Array params: Union prng_seed: Array num_inference_steps: int = 50 height: Optional = None width: Optional = None guidance_scale: Union = 7.5 latents: Array = None neg_prompt_ids: Array = None return_dict: bool = True jit: bool = False ) → export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或提示。

+   `height` (`int`, *可选*，默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*，默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*，默认为 50) — 除噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。此参数受 `strength` 调节。

+   `guidance_scale` (`float`, *可选*，默认为 7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `latents` (`jnp.ndarray`, *可选*) — 从高斯分布中预先生成的嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样来生成潜变量数组。

+   `jit` (`bool`，默认为 `False`) — 是否运行生成和安全评分函数的 `pmap` 版本。

    此参数存在是因为 `__call__` 尚不是端到端的 pmap 可用。它将在未来的版本中被移除。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)而不是一个普通的元组。

返回

[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)或 `tuple`

如果 `return_dict` 为 `True`，则返回[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)，否则返回一个 `tuple`，其中第一个元素是生成的图像的列表，第二个元素是一个列表，指示相应生成的图像是否包含“不安全内容”（nsfw）。

调用管道进行生成时调用的函数。

示例：

```py
>>> import jax
>>> import numpy as np
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard
>>> import PIL
>>> import requests
>>> from io import BytesIO
>>> from diffusers import FlaxStableDiffusionInpaintPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
>>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

>>> init_image = download_image(img_url).resize((512, 512))
>>> mask_image = download_image(mask_url).resize((512, 512))

>>> pipeline, params = FlaxStableDiffusionInpaintPipeline.from_pretrained(
...     "xvjiarui/stable-diffusion-2-inpainting"
... )

>>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
>>> prng_seed = jax.random.PRNGKey(0)
>>> num_inference_steps = 50

>>> num_samples = jax.device_count()
>>> prompt = num_samples * [prompt]
>>> init_image = num_samples * [init_image]
>>> mask_image = num_samples * [mask_image]
>>> prompt_ids, processed_masked_images, processed_masks = pipeline.prepare_inputs(
...     prompt, init_image, mask_image
... )
# shard inputs and rng

>>> params = replicate(params)
>>> prng_seed = jax.random.split(prng_seed, jax.device_count())
>>> prompt_ids = shard(prompt_ids)
>>> processed_masked_images = shard(processed_masked_images)
>>> processed_masks = shard(processed_masks)

>>> images = pipeline(
...     prompt_ids, processed_masks, processed_masked_images, params, prng_seed, num_inference_steps, jit=True
... ).images
>>> images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
```

## FlaxStableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

参数

+   `images` (`np.ndarray`) — 形状为 `(batch_size, height, width, num_channels)` 的去噪图像数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw）或如果无法执行安全检查，则为 `None`。

基于Flax的稳定扩散管道的输出类。

#### `replace`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。
