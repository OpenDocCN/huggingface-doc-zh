# ä¿®è¡¥

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint)

ç¨³å®šæ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥åº”ç”¨äºä¿®è¡¥ï¼Œé€šè¿‡æä¾›æ©æ¨¡å’Œæ–‡æœ¬æç¤ºï¼Œä½¿ç”¨ç¨³å®šæ‰©æ•£ç¼–è¾‘å›¾åƒçš„ç‰¹å®šéƒ¨åˆ†ã€‚

## æç¤º

å»ºè®®ä½¿ç”¨å·²ç»ä¸“é—¨ä¸ºä¿®è¡¥è€Œå¾®è°ƒçš„æ£€æŸ¥ç‚¹ä¸æ­¤ç®¡é“ä¸€èµ·ä½¿ç”¨ï¼Œä¾‹å¦‚[runwayml/stable-diffusion-inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting)ã€‚é»˜è®¤çš„æ–‡æœ¬åˆ°å›¾åƒç¨³å®šæ‰©æ•£æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)ä¹Ÿå…¼å®¹ï¼Œä½†å®ƒä»¬å¯èƒ½æ€§èƒ½è¾ƒå·®ã€‚

ç¡®ä¿æŸ¥çœ‹ç¨³å®šæ‰©æ•£[Tips](overview#tips)éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆåœ°é‡ç”¨ç®¡é“ç»„ä»¶ï¼

å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨ä»»åŠ¡çš„å®˜æ–¹æ£€æŸ¥ç‚¹ä¹‹ä¸€ï¼Œè¯·æ¢ç´¢[CompVis](https://huggingface.co/CompVis)ã€[Runway](https://huggingface.co/runwayml)å’Œ[Stability AI](https://huggingface.co/stabilityai) Hub ç»„ç»‡ï¼

## StableDiffusionInpaintPipeline

### `class diffusers.StableDiffusionInpaintPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L222)

```py
( vae: Union text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

å‚æ•°

+   `vae`ï¼ˆ[`AutoencoderKL`ã€`AsymmetricAutoencoderKL`]ï¼‰- å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç åˆ°å’Œä»æ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder`ï¼ˆ`CLIPTextModel`ï¼‰- å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼‰- ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„`CLIPTokenizer`ã€‚

+   `unet`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰- ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„`UNet2DConditionModel`ã€‚

+   `scheduler`ï¼ˆ[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)ï¼‰- ä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„è°ƒåº¦å™¨ã€‚å¯ä»¥æ˜¯[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ã€[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)æˆ–[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)ä¹‹ä¸€ã€‚

+   `safety_checker`ï¼ˆ`StableDiffusionSafetyChecker`ï¼‰- ä¼°è®¡ç”Ÿæˆçš„å›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§çš„åˆ†ç±»æ¨¡å—ã€‚è¯·å‚è€ƒ[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

+   `feature_extractor`ï¼ˆ[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼‰- ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„`CLIPImageProcessor`ï¼›ä½œä¸º`safety_checker`çš„è¾“å…¥ã€‚

ç”¨äºä½¿ç”¨ç¨³å®šæ‰©æ•£è¿›è¡Œæ–‡æœ¬å¼•å¯¼å›¾åƒä¿®è¡¥çš„ç®¡é“ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥ç®¡é“è¿˜ç»§æ‰¿ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) ç”¨äºåŠ è½½ LoRA æƒé‡

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) ç”¨äºä¿å­˜ LoRA æƒé‡

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) ç”¨äºåŠ è½½ IP é€‚é…å™¨

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) ç”¨äºåŠ è½½`.ckpt`æ–‡ä»¶

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L1010)

```py
( prompt: Union = None image: Union = None mask_image: Union = None masked_image_latents: FloatTensor = None height: Optional = None width: Optional = None padding_mask_crop: Optional = None strength: float = 1.0 num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: int = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) â†’ export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`) â€” è¡¨ç¤ºè¦ä¿®å¤çš„å›¾åƒæ‰¹æ¬¡çš„å›¾åƒã€numpyæ•°ç»„æˆ–å¼ é‡ï¼ˆè¦ç”¨`mask_image`é®ç½©çš„å›¾åƒéƒ¨åˆ†ï¼Œå¹¶æ ¹æ®`prompt`é‡æ–°ç»˜åˆ¶ï¼‰ã€‚å¯¹äºnumpyæ•°ç»„å’Œpytorchå¼ é‡ï¼ŒæœŸæœ›å€¼èŒƒå›´åœ¨`[0, 1]`ä¹‹é—´ã€‚å¦‚æœæ˜¯å¼ é‡æˆ–å¼ é‡åˆ—è¡¨ï¼Œåˆ™æœŸæœ›å½¢çŠ¶åº”ä¸º`(B, C, H, W)`æˆ–`(C, H, W)`ã€‚å¦‚æœæ˜¯numpyæ•°ç»„æˆ–æ•°ç»„åˆ—è¡¨ï¼Œåˆ™æœŸæœ›å½¢çŠ¶åº”ä¸º`(B, H, W, C)`æˆ–`(H, W, C)`ã€‚å®ƒè¿˜å¯ä»¥æ¥å—å›¾åƒæ½œå˜é‡ä½œä¸º`image`ï¼Œä½†å¦‚æœç›´æ¥ä¼ é€’æ½œå˜é‡ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `mask_image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, æˆ– `List[np.ndarray]`) â€” è¡¨ç¤ºè¦é®ç½©`image`çš„å›¾åƒæ‰¹æ¬¡çš„å›¾åƒã€numpyæ•°ç»„æˆ–å¼ é‡ã€‚é®ç½©ä¸­çš„ç™½è‰²åƒç´ å°†è¢«é‡æ–°ç»˜åˆ¶ï¼Œè€Œé»‘è‰²åƒç´ å°†è¢«ä¿ç•™ã€‚å¦‚æœ`mask_image`æ˜¯PILå›¾åƒï¼Œåˆ™åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶è½¬æ¢ä¸ºå•é€šé“ï¼ˆäº®åº¦ï¼‰ã€‚å¦‚æœæ˜¯numpyæ•°ç»„æˆ–pytorchå¼ é‡ï¼Œåˆ™åº”åŒ…å«ä¸€ä¸ªé¢œè‰²é€šé“ï¼ˆLï¼‰è€Œä¸æ˜¯3ï¼Œå› æ­¤pytorchå¼ é‡çš„é¢„æœŸå½¢çŠ¶å°†æ˜¯`(B, 1, H, W)`ã€`(B, H, W)`ã€`(1, H, W)`ã€`(H, W)`ã€‚å¯¹äºnumpyæ•°ç»„ï¼Œé¢„æœŸå½¢çŠ¶å°†æ˜¯`(B, H, W, 1)`ã€`(B, H, W)`ã€`(H, W, 1)`æˆ–`(H, W)`ã€‚

+   `height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `padding_mask_crop` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`None`) â€” è¦åº”ç”¨äºå›¾åƒå’Œé®ç½©çš„è£å‰ªè¾¹è·å¤§å°ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¸å¯¹å›¾åƒå’Œmask_imageåº”ç”¨è£å‰ªã€‚å¦‚æœ`padding_mask_crop`ä¸æ˜¯`None`ï¼Œå®ƒå°†é¦–å…ˆæ‰¾åˆ°ä¸€ä¸ªå…·æœ‰ä¸å›¾åƒç›¸åŒçºµæ¨ªæ¯”ä¸”åŒ…å«æ‰€æœ‰é®ç½©åŒºåŸŸçš„çŸ©å½¢åŒºåŸŸï¼Œç„¶åæ ¹æ®`padding_mask_crop`æ‰©å±•è¯¥åŒºåŸŸã€‚ç„¶åå°†æ ¹æ®æ‰©å±•åŒºåŸŸå¯¹å›¾åƒå’Œmask_imageè¿›è¡Œè£å‰ªï¼Œç„¶åå°†å…¶è°ƒæ•´ä¸ºä¿®å¤çš„åŸå§‹å›¾åƒå¤§å°ã€‚å½“é®ç½©åŒºåŸŸè¾ƒå°è€Œå›¾åƒè¾ƒå¤§ä¸”åŒ…å«ä¸ä¿®å¤æ— å…³çš„ä¿¡æ¯ï¼ˆå¦‚èƒŒæ™¯ï¼‰æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚

+   `strength` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1.0) â€” è¡¨ç¤ºè½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»åœ¨0å’Œ1ä¹‹é—´ã€‚`image`ç”¨ä½œèµ·ç‚¹ï¼Œ`strength`è¶Šé«˜ï¼Œæ·»åŠ çš„å™ªéŸ³è¶Šå¤šã€‚é™å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“`strength`ä¸º1æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³æœ€å¤§ï¼Œé™å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`çš„å®Œæ•´è¿­ä»£æ¬¡æ•°ã€‚å€¼ä¸º1åŸºæœ¬ä¸Šå¿½ç•¥`image`ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º50) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æ­¤å‚æ•°ç”±`strength`è°ƒèŠ‚ã€‚

+   `timesteps` (`List[int]`, *optional*) â€” ç”¨äºå…·æœ‰æ”¯æŒ `set_timesteps` æ–¹æ³•ä¸­çš„ `timesteps` å‚æ•°çš„è°ƒåº¦å™¨çš„å»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å°†ä½¿ç”¨ä¼ é€’ `num_inference_steps` æ—¶çš„é»˜è®¤è¡Œä¸ºã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º 7.5) â€” æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` or `List[str]`, *optional*) â€” æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’ `negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶ï¼ˆ`guidance_scale < 1`ï¼‰å°†è¢«å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *optional*, é»˜è®¤ä¸º 0.0) â€” å¯¹åº”äº [DDIM](https://arxiv.org/abs/2010.02502) è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œåœ¨å…¶ä»–è°ƒåº¦å™¨ä¸­å°†è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `latents` (`torch.FloatTensor`, *optional*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆå˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆ `negative_prompt_embeds`ã€‚ip_adapter_image â€” (`PipelineImageInput`, *optional*): å¯é€‰çš„å›¾åƒè¾“å…¥ï¼Œç”¨äºä¸ IP é€‚é…å™¨ä¸€èµ·ä½¿ç”¨ã€‚

+   `output_type` (`str`, *optional*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ– `np.array` ä¹‹é—´çš„ä¸€ä¸ªã€‚

+   `return_dict` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `cross_attention_kwargs` (`dict`, *optional*) â€” å¦‚æœæŒ‡å®šäº†çš„ kwargs å­—å…¸å°†ä¼ é€’ç»™ [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) ä¸­å®šä¹‰çš„ `AttentionProcessor`ã€‚

+   `clip_skip` (`int`, *optional*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä» CLIP è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

+   `callback_on_step_end` (`Callable`, *optional*) â€” æ¨ç†è¿‡ç¨‹ä¸­åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs` å°†åŒ…æ‹¬ç”± `callback_on_step_end_tensor_inputs` æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `callback_on_step_end` å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º `callback_kwargs` å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„ `._callback_tensor_inputs` å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è¿”å›

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) æˆ– `tuple`

å¦‚æœ `return_dict` ä¸º `True`ï¼Œåˆ™è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª `tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«ç›¸åº”ç”Ÿæˆå›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹çš„ `bool` åˆ—è¡¨ã€‚

ç”Ÿæˆç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import PIL
>>> import requests
>>> import torch
>>> from io import BytesIO

>>> from diffusers import StableDiffusionInpaintPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
>>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

>>> init_image = download_image(img_url).resize((512, 512))
>>> mask_image = download_image(mask_url).resize((512, 512))

>>> pipe = StableDiffusionInpaintPipeline.from_pretrained(
...     "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
>>> image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]
```

#### `enable_attention_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

å‚æ•°

+   `slice_size` (`str` æˆ– `int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"auto"`) â€” å½“ä¸º `"auto"` æ—¶ï¼Œå°†è¾“å…¥åˆ’åˆ†ä¸ºæ³¨æ„åŠ›å¤´çš„ä¸€åŠï¼Œå› æ­¤æ³¨æ„åŠ›å°†åˆ†ä¸¤æ­¥è®¡ç®—ã€‚å¦‚æœä¸º `"max"`ï¼Œå°†é€šè¿‡ä¸€æ¬¡åªè¿è¡Œä¸€ä¸ªåˆ‡ç‰‡æ¥èŠ‚çœæœ€å¤§å†…å­˜é‡ã€‚å¦‚æœæä¾›ä¸€ä¸ªæ•°å­—ï¼Œåˆ™ä½¿ç”¨ `attention_head_dim // slice_size` ä½œä¸ºåˆ‡ç‰‡æ•°é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`attention_head_dim` å¿…é¡»æ˜¯ `slice_size` çš„å€æ•°ã€‚

å¯ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—å°†è¾“å…¥å¼ é‡åˆ†å‰²ä¸ºå¤šä¸ªåˆ‡ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚å¯¹äºå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å°†æŒ‰é¡ºåºåœ¨æ¯ä¸ªå¤´ä¸Šæ‰§è¡Œã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜ä»¥æ¢å–ä¸€ç‚¹é€Ÿåº¦é™ä½å¾ˆæœ‰ç”¨ã€‚

âš ï¸ å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨PyTorch 2.0æˆ–xFormersçš„`scaled_dot_product_attention`ï¼ˆSDPAï¼‰ï¼Œè¯·ä¸è¦å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ã€‚è¿™äº›æ³¨æ„åŠ›è®¡ç®—å·²ç»éå¸¸é«˜æ•ˆï¼Œå› æ­¤æ‚¨ä¸éœ€è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚å¦‚æœæ‚¨åœ¨SDPAæˆ–xFormersä¸­å¯ç”¨äº†æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡å‡é€Ÿï¼

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¦‚æœä¹‹å‰è°ƒç”¨äº† `enable_attention_slicing`ï¼Œåˆ™æ³¨æ„åŠ›å°†åœ¨ä¸€æ­¥ä¸­è®¡ç®—ã€‚

#### `enable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op` (`Callable`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨ä½œ `xFormers` çš„[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)å‡½æ•°çš„ `op` å‚æ•°çš„é»˜è®¤ `None` æ“ä½œç¬¦çš„è¦†ç›–ã€‚

å¯ç”¨æ¥è‡ª[xFormers](https://facebookresearch.github.io/xformers/)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ‚¨åº”è¯¥è§‚å¯Ÿåˆ°è¾ƒä½çš„GPUå†…å­˜ä½¿ç”¨é‡ï¼Œå¹¶åœ¨æ¨æ–­æœŸé—´å¯èƒ½åŠ é€Ÿã€‚è®­ç»ƒæœŸé—´çš„åŠ é€Ÿä¸è¢«ä¿è¯ã€‚

âš ï¸ å½“å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å’Œåˆ‡ç‰‡æ³¨æ„åŠ›éƒ½å¯ç”¨æ—¶ï¼Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä¼˜å…ˆã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

ç¦ç”¨æ¥è‡ª[xFormers](https://facebookresearch.github.io/xformers/)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚

#### `load_textual_inversion`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path` (`str` æˆ– `os.PathLike` æˆ– `List[str or os.PathLike]` æˆ– `Dict` æˆ– `List[Dict]`) â€” å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å®ƒä»¬çš„åˆ—è¡¨ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹Hubä¸Šæ‰˜ç®¡çš„*æ¨¡å‹ID*ï¼ˆä¾‹å¦‚ `sd-concepts-library/low-poly-hd-logos-icons`ï¼‰ã€‚

    +   åŒ…å«æ–‡æœ¬åæ¼”æƒé‡çš„*ç›®å½•*è·¯å¾„ï¼ˆä¾‹å¦‚ `./my_text_inversion_directory/`ï¼‰ã€‚

    +   åŒ…å«æ–‡æœ¬åæ¼”æƒé‡çš„*æ–‡ä»¶*è·¯å¾„ï¼ˆä¾‹å¦‚ `./my_text_inversions.pt`ï¼‰ã€‚

    +   ä¸€ä¸ª [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚

+   `token` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” è¦†ç›–ç”¨äºæ–‡æœ¬åè½¬æƒé‡çš„ä»¤ç‰Œã€‚å¦‚æœ `pretrained_model_name_or_path` æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ™ `token` ä¹Ÿå¿…é¡»æ˜¯ç›¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel), *å¯é€‰*) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨ self.tokenizerã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer), *å¯é€‰*) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨ self.tokenizerã€‚

+   `weight_name` (`str`, *å¯é€‰*) â€” è‡ªå®šä¹‰æƒé‡æ–‡ä»¶çš„åç§°ã€‚åœ¨ä»¥ä¸‹æƒ…å†µä¸‹åº”ä½¿ç”¨æ­¤é€‰é¡¹ï¼š

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶é‡‡ç”¨ ğŸ¤— Diffusers æ ¼å¼ï¼Œä½†ä¿å­˜åœ¨ç‰¹å®šæƒé‡åç§°ä¸‹ï¼Œä¾‹å¦‚ `text_inv.bin`ã€‚

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶é‡‡ç”¨ Automatic1111 æ ¼å¼ã€‚

+   `cache_dir` (`Union[str, os.PathLike]`, *å¯é€‰*) â€” å¦‚æœæœªä½¿ç”¨æ ‡å‡†ç¼“å­˜ï¼Œåˆ™ç¼“å­˜å·²ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„ç›®å½•è·¯å¾„ã€‚

+   `force_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ç»§ç»­ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º `False`ï¼Œåˆ™ä¼šåˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚

+   `proxies` (`Dict[str, str]`, *å¯é€‰*) â€” ä¸€ä¸ªæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚ä»£ç†å°†åœ¨æ¯ä¸ªè¯·æ±‚ä¸­ä½¿ç”¨ã€‚

+   `local_files_only` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™ä¸ä¼šä» Hub ä¸‹è½½æ¨¡å‹ã€‚

+   `token` (`str` æˆ– *bool*, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶ HTTP ä»¤ç‰Œçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œåˆ™ä½¿ç”¨ä» `diffusers-cli login` ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚

+   `revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤ ID æˆ– Git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `subfolder` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `""`) â€” åœ¨ Hub æˆ–æœ¬åœ°è¾ƒå¤§æ¨¡å‹å­˜å‚¨åº“ä¸­çš„æ¨¡å‹æ–‡ä»¶çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚

+   `mirror` (`str`, *å¯é€‰*) â€” å¦‚æœæ‚¨åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°å¯è®¿é—®æ€§é—®é¢˜ï¼Œè¯·å°†æºé•œåƒä»¥è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

å°†æ–‡æœ¬åè½¬åµŒå…¥åŠ è½½åˆ° [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline) çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼ˆæ”¯æŒ ğŸ¤— Diffusers å’Œ Automatic1111 æ ¼å¼ï¼‰ã€‚

ç¤ºä¾‹ï¼š

è¦åŠ è½½ ğŸ¤— Diffusers æ ¼å¼çš„æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

è¦åŠ è½½ Automatic1111 æ ¼å¼çš„æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼Œè¯·ç¡®ä¿é¦–å…ˆä¸‹è½½å‘é‡ï¼ˆä¾‹å¦‚ä» [civitAI](https://civitai.com/models/3036?modelVersionId=9857)ï¼‰ç„¶ååŠ è½½å‘é‡

æœ¬åœ°ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `load_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path_or_dict` (`str` æˆ– `os.PathLike` æˆ– `dict`) â€” è¯·å‚é˜… [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚

+   `kwargs` (`dict`, *å¯é€‰*) â€” è¯·å‚é˜… [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚

+   `adapter_name` (`str`, *å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­iæ˜¯è¦åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚

å°†åœ¨`pretrained_model_name_or_path_or_dict`ä¸­æŒ‡å®šçš„LoRAæƒé‡åŠ è½½åˆ°`self.unet`å’Œ`self.text_encoder`ä¸­ã€‚

æ‰€æœ‰kwargséƒ½å°†è½¬å‘åˆ°`self.lora_state_dict`ã€‚

æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

æŸ¥çœ‹[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)ä»¥è·å–æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.unet`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

æŸ¥çœ‹[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸åˆ°`self.text_encoder`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

#### `save_lora_weights`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

å‚æ•°

+   `save_directory` (`str`æˆ–`os.PathLike`) â€” ä¿å­˜LoRAå‚æ•°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†ä¼šåˆ›å»ºã€‚

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]`æˆ–`Dict[str, torch.Tensor]`) â€” ä¸`unet`å¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸ã€‚

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]`æˆ–`Dict[str, torch.Tensor]`) â€” ä¸`text_encoder`å¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜¾å¼ä¼ é€’æ–‡æœ¬ç¼–ç å™¨LoRAçŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ªğŸ¤— Transformersã€‚

+   `is_main_process` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¾ˆæœ‰ç”¨ï¼Œå½“æ‚¨éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½®`is_main_process=True`ä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚

+   `save_function` (`Callable`) â€” ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´å¾ˆæœ‰ç”¨ï¼Œå½“æ‚¨éœ€è¦ç”¨å¦ä¸€ç§æ–¹æ³•æ›¿æ¢`torch.save`æ—¶ã€‚å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`DIFFUSERS_SAVE_MODE`è¿›è¡Œé…ç½®ã€‚

+   `safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä½¿ç”¨`safetensors`ä¿å­˜æ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨ä¼ ç»Ÿçš„PyTorchæ–¹å¼ä¸`pickle`ä¿å­˜ã€‚

ä¿å­˜å¯¹åº”äºUNetå’Œæ–‡æœ¬ç¼–ç å™¨çš„LoRAå‚æ•°ã€‚

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L889)

```py
( )
```

å¦‚æœå¯ç”¨ï¼Œåˆ™ç¦ç”¨FreeUæœºåˆ¶ã€‚

#### `enable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L866)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1` (`float`) â€” ç”¨äºè°ƒèŠ‚é˜¶æ®µ1çš„ç¼©æ”¾å› å­ï¼Œä»¥å‡å¼±è·³è·ƒç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `s2` (`float`) â€” ç”¨äºé˜¶æ®µ2çš„ç¼©æ”¾å› å­ï¼Œä»¥å‡å¼±è·³è·ƒç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `b1` (`float`) â€” ç”¨äºé˜¶æ®µ1çš„ç¼©æ”¾å› å­ï¼Œä»¥æ”¾å¤§éª¨å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚

+   `b2` (`float`) â€” ç”¨äºé˜¶æ®µ2çš„ç¼©æ”¾å› å­ï¼Œä»¥æ”¾å¤§éª¨å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚

å¯ç”¨FreeUæœºåˆ¶ï¼Œå¦‚[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)ä¸­æ‰€è¿°ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºå®ƒä»¬è¢«åº”ç”¨çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[official repository](https://github.com/ChenyangSi/FreeU)ä»¥è·å–å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚Stable Diffusion v1ã€v2å’ŒStable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `encode_prompt`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L397)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” è¦ç¼–ç çš„æç¤º device â€” (`torch.device`): torch è®¾å¤‡

+   `num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸è¦å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’ `negative_prompt_embeds`ã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³ï¼Œå¦‚æœ `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰æ—¶å¿½ç•¥ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆ `negative_prompt_embeds`ã€‚

+   `lora_scale` (`float`, *å¯é€‰*) â€” å¦‚æœåŠ è½½äº†LoRAå±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰LoRAå±‚çš„LoRAæ¯”ä¾‹ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä»CLIPè·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º1è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

#### `fuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L894)

```py
( unet: bool = True vae: bool = True )
```

å‚æ•°

+   `unet` (`bool`, é»˜è®¤ä¸º `True`) â€” å¯¹UNetåº”ç”¨èåˆã€‚

+   `vae` (`bool`, é»˜è®¤ä¸º `True`) â€” å¯¹VAEåº”ç”¨èåˆã€‚

å¯ç”¨èåˆçš„QKVæŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

æ­¤APIä¸ºğŸ§ªå®éªŒæ€§è´¨ã€‚

#### `get_guidance_scale_embedding`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L955)

```py
( w embedding_dim = 512 dtype = torch.float32 ) â†’ export const metadata = 'undefined';torch.FloatTensor
```

å‚æ•°

+   `timesteps` (`torch.Tensor`) â€” åœ¨è¿™äº›æ—¶é—´æ­¥ç”ŸæˆåµŒå…¥å‘é‡

+   `embedding_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” è¦ç”Ÿæˆçš„åµŒå…¥çš„ç»´åº¦ dtype â€” ç”Ÿæˆçš„åµŒå…¥çš„æ•°æ®ç±»å‹

è¿”å›

`torch.FloatTensor`

å½¢çŠ¶ä¸º `(len(timesteps), embedding_dim)` çš„åµŒå…¥å‘é‡

å‚è§ [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L926)

```py
( unet: bool = True vae: bool = True )
```

å‚æ•°

+   `unet` (`bool`, é»˜è®¤ä¸º `True`) â€” å¯¹UNetåº”ç”¨èåˆã€‚

+   `vae` (`bool`, é»˜è®¤ä¸º `True`) â€” å¯¹VAEåº”ç”¨èåˆã€‚

å¦‚æœå¯ç”¨ï¼Œè¯·ç¦ç”¨QKVæŠ•å½±èåˆã€‚

æ­¤APIä¸ºğŸ§ªå®éªŒæ€§è´¨ã€‚

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

å‚æ•°

+   `images` (`List[PIL.Image.Image]` æˆ– `np.ndarray`) â€” é•¿åº¦ä¸º `batch_size` çš„å»å™ªPILå›¾åƒåˆ—è¡¨æˆ–å½¢çŠ¶ä¸º `(batch_size, height, width, num_channels)` çš„NumPyæ•°ç»„ã€‚

+   `nsfw_content_detected` (`List[bool]`) â€” åˆ—è¡¨æŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸å®‰å…¨å†…å®¹â€ï¼ˆnsfwï¼‰ï¼Œå¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º `None`ã€‚

ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚

## FlaxStableDiffusionInpaintPipeline

### `class diffusers.FlaxStableDiffusionInpaintPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py#L102)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor dtype: dtype = <class 'jax.numpy.float32'> )
```

å‚æ•°

+   `vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL)) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚

+   `unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel)) â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„ `FlaxUNet2DConditionModel`ã€‚

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) â€” ä¸ `unet` ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡ã€‚å¯ä»¥æ˜¯ `FlaxDDIMScheduler`ã€`FlaxLMSDiscreteScheduler`ã€`FlaxPNDMScheduler` æˆ– `FlaxDPMSolverMultistepScheduler` ä¸­çš„ä¸€ä¸ªã€‚

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) â€” ç”¨äºä¼°è®¡ç”Ÿæˆå›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„åˆ†ç±»æ¨¡å—ã€‚è¯·å‚è€ƒ[æ¨¡å‹å¡ç‰‡](https://huggingface.co/runwayml/stable-diffusion-v1-5)ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) â€” ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„ `CLIPImageProcessor`ï¼›ä½œä¸º `safety_checker` çš„è¾“å…¥ã€‚

åŸºäº Flax çš„ç®¡é“ï¼Œç”¨äºä½¿ç”¨ Stable Diffusion è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤ã€‚

ğŸ§ª è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py#L394)

```py
( prompt_ids: Array mask: Array masked_image: Array params: Union prng_seed: Array num_inference_steps: int = 50 height: Optional = None width: Optional = None guidance_scale: Union = 7.5 latents: Array = None neg_prompt_ids: Array = None return_dict: bool = True jit: bool = False ) â†’ export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `height` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 50) â€” é™¤å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚æ­¤å‚æ•°å— `strength` è°ƒèŠ‚ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 7.5) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `latents` (`jnp.ndarray`, *å¯é€‰*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·æ¥ç”Ÿæˆæ½œå˜é‡æ•°ç»„ã€‚

+   `jit` (`bool`ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿è¡Œç”Ÿæˆå’Œå®‰å…¨è¯„åˆ†å‡½æ•°çš„ `pmap` ç‰ˆæœ¬ã€‚

    æ­¤å‚æ•°å­˜åœ¨æ˜¯å› ä¸º `__call__` å°šä¸æ˜¯ç«¯åˆ°ç«¯çš„ pmap å¯ç”¨ã€‚å®ƒå°†åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­è¢«ç§»é™¤ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å›[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)æˆ– `tuple`

å¦‚æœ `return_dict` ä¸º `True`ï¼Œåˆ™è¿”å›[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª `tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç”Ÿæˆçš„å›¾åƒçš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒæŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸å®‰å…¨å†…å®¹â€ï¼ˆnsfwï¼‰ã€‚

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import jax
>>> import numpy as np
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard
>>> import PIL
>>> import requests
>>> from io import BytesIO
>>> from diffusers import FlaxStableDiffusionInpaintPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
>>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

>>> init_image = download_image(img_url).resize((512, 512))
>>> mask_image = download_image(mask_url).resize((512, 512))

>>> pipeline, params = FlaxStableDiffusionInpaintPipeline.from_pretrained(
...     "xvjiarui/stable-diffusion-2-inpainting"
... )

>>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
>>> prng_seed = jax.random.PRNGKey(0)
>>> num_inference_steps = 50

>>> num_samples = jax.device_count()
>>> prompt = num_samples * [prompt]
>>> init_image = num_samples * [init_image]
>>> mask_image = num_samples * [mask_image]
>>> prompt_ids, processed_masked_images, processed_masks = pipeline.prepare_inputs(
...     prompt, init_image, mask_image
... )
# shard inputs and rng

>>> params = replicate(params)
>>> prng_seed = jax.random.split(prng_seed, jax.device_count())
>>> prompt_ids = shard(prompt_ids)
>>> processed_masked_images = shard(processed_masked_images)
>>> processed_masks = shard(processed_masks)

>>> images = pipeline(
...     prompt_ids, processed_masks, processed_masked_images, params, prng_seed, num_inference_steps, jit=True
... ).images
>>> images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
```

## FlaxStableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

å‚æ•°

+   `images` (`np.ndarray`) â€” å½¢çŠ¶ä¸º `(batch_size, height, width, num_channels)` çš„å»å™ªå›¾åƒæ•°ç»„ã€‚

+   `nsfw_content_detected` (`List[bool]`) â€” åˆ—è¡¨æŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸å®‰å…¨å†…å®¹â€ï¼ˆnsfwï¼‰æˆ–å¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º `None`ã€‚

åŸºäºFlaxçš„ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚

#### `replace`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
