# 量化

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization)

量化技术通过使用低精度数据类型（如8位整数（int8））表示权重和激活来减少内存和计算成本。这使得您可以加载通常无法放入内存的更大模型，并加快推理速度。Transformers支持AWQ和GPTQ量化算法，支持8位和4位量化与bitsandbytes。

了解如何在[量化](../quantization)指南中量化模型。

## AwqConfig

### `class transformers.AwqConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L542)

```py
( bits: int = 4 group_size: int = 128 zero_point: bool = True version: AWQLinearVersion = <AWQLinearVersion.GEMM: 'gemm'> backend: AwqBackendPackingMethod = <AwqBackendPackingMethod.AUTOAWQ: 'autoawq'> do_fuse: Optional = None fuse_max_seq_len: Optional = None modules_to_fuse: Optional = None modules_to_not_convert: Optional = None **kwargs )
```

参数

+   `bits` (`int`，*可选*，默认为4) — 要量化为的位数。

+   `group_size` (`int`，*可选*，默认为128) — 用于量化的组大小。推荐值为128，-1使用每列量化。

+   `zero_point` (`bool`，*可选*，默认为`True`) — 是否使用零点量化。

+   `version` (`AWQLinearVersion`，*可选*，默认为`AWQLinearVersion.GEMM`) — 要使用的量化算法版本。对于大批量大小（例如>= 8），GEMM更好，否则，GEMV更好（例如< 8）

+   `backend` (`AwqBackendPackingMethod`，*可选*，默认为`AwqBackendPackingMethod.AUTOAWQ`) — 量化后端。一些模型可能使用`llm-awq`后端进行量化。这对于使用`llm-awq`库量化自己的模型的用户很有用。

+   `do_fuse` (`bool`，*可选*，默认为`False`) — 是否将注意力和mlp层融合在一起以加快推理速度

+   `fuse_max_seq_len` (`int`，*可选*) — 使用融合时生成的最大序列长度。

+   `modules_to_fuse` (`dict`，*可选*，默认为`None`) — 用用户指定的融合方案覆盖原生支持的融合方案。

+   `modules_to_not_convert` (`list`，*可选*，默认为`None`) — 不量化的模块列表，对于需要明确保留一些模块在原始精度中的模型进行量化很有用（例如Whisper编码器，Llava编码器，Mixtral门层）。请注意，您不能直接使用transformers进行量化，请参考`AutoAWQ`文档以量化HF模型。

这是一个关于所有可能属性和特性的包装类，您可以使用`auto-awq`库加载的模型进行玩耍，该库依赖于auto_awq后端的awq量化。

#### `post_init`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L605)

```py
( )
```

安全检查器检查参数是否正确

## GPTQConfig

### `class transformers.GPTQConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L328)

```py
( bits: int tokenizer: Any = None dataset: Union = None group_size: int = 128 damp_percent: float = 0.1 desc_act: bool = False sym: bool = True true_sequential: bool = True use_cuda_fp16: bool = False model_seqlen: Optional = None block_name_to_quantize: Optional = None module_name_preceding_first_block: Optional = None batch_size: int = 1 pad_token_id: Optional = None use_exllama: Optional = None max_input_length: Optional = None exllama_config: Optional = None cache_block_outputs: bool = True modules_in_block_to_quantize: Optional = None **kwargs )
```

参数

+   `bits` (`int`) — 要量化为的位数，支持的数字为（2、3、4、8）。

+   `tokenizer` (`str`或`PreTrainedTokenizerBase`，*可选*) — 用于处理数据集的分词器。您可以传递以下内容：

    +   自定义分词器对象。

    +   一个字符串，托管在huggingface.co模型存储库中的预定义分词器的*模型id*。有效的模型id可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   包含分词器所需词汇文件的*目录*路径，例如使用[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)方法保存的目录，例如`./my_model_directory/`。

+   `dataset` (`Union[List[str]]`，*可选*) — 用于量化的数据集。您可以提供自己的数据集列表，或者只使用GPTQ论文中使用的原始数据集[‘wikitext2’，‘c4’，‘c4-new’，‘ptb’，‘ptb-new’]

+   `group_size` (`int`, *optional*, defaults to 128) — 用于量化的组大小。推荐值为128，-1使用每列量化。

+   `damp_percent` (`float`, *optional*, defaults to 0.1) — 用于阻尼的平均Hessian对角线的百分比。推荐值为0.1。

+   `desc_act` (`bool`, *optional*, defaults to `False`) — 是否按照激活大小递减的顺序量化列。将其设置为False可以显著加快推理速度，但困惑度可能会略有下降。也称为act-order。

+   `sym` (`bool`, *optional*, defaults to `True`) — 是否使用对称量化。

+   `true_sequential` (`bool`, *optional*, defaults to `True`) — 是否在单个Transformer块内执行顺序量化。我们执行逐层量化，而不是一次性量化整个块。因此，每个层都经历了使用已通过先前量化的层的输入进行量化的过程。

+   `use_cuda_fp16` (`bool`, *optional*, defaults to `False`) — 是否使用优化的cuda内核进行fp16模型。需要将模型设置为fp16。

+   `model_seqlen` (`int`, *optional*) — 模型可以接受的最大序列长度。

+   `block_name_to_quantize` (`str`, *optional*) — 要量化的transformers块名称。如果为None，我们将使用常见模式（例如model.layers）推断块名称。

+   `module_name_preceding_first_block` (`List[str]`, *optional*) — 在第一个Transformer块之前的层。

+   `batch_size` (`int`, *optional*, defaults to 1) — 处理数据集时使用的批量大小

+   `pad_token_id` (`int`, *optional*) — 填充标记id。当`batch_size` > 1时需要准备数据集。

+   `use_exllama` (`bool`, *optional*) — 是否使用exllama后端。如果未设置，默认为`True`。仅在`bits` = 4时有效。

+   `max_input_length` (`int`, *optional*) — 最大输入长度。这是为了初始化一个依赖于最大预期输入长度的缓冲区而需要的。它是特定于具有act-order的exllama后端。

+   `exllama_config` (`Dict[str, Any]`, *optional*) — exllama配置。您可以通过`version`键指定exllama内核的版本。如果未设置，默认为`{"version": 1}`。

+   `cache_block_outputs` (`bool`, *optional*, defaults to `True`) — 是否缓存块输出以便作为后续块的输入重复使用。

+   `modules_in_block_to_quantize` (`List[List[str]]`, *optional*) — 要在指定块中量化的模块名称列表的列表。此参数可用于排除某些线性模块的量化。可以通过设置`block_name_to_quantize`来指定要量化的块。我们将依次量化每个列表。如果未设置，我们将量化所有线性层。示例：`modules_in_block_to_quantize =[["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"], ["self_attn.o_proj"]]`。在此示例中，我们将首先同时量化q、k、v层，因为它们是独立的。然后，我们将在量化q、k、v层的基础上量化`self_attn.o_proj`层。这样，我们将获得更好的结果，因为它反映了模型在量化时`self_attn.o_proj`将获得的真实输入。

这是一个关于使用`optimum` api加载的模型的所有可能属性和特性的包装类，用于依赖于auto_gptq后端的gptq量化。

#### `from_dict_optimum`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L527)

```py
( config_dict )
```

获取与最佳gptq配置字典兼容的类

#### `post_init`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L444)

```py
( )
```

确保参数正确的安全检查器

#### `to_dict_optimum`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L518)

```py
( )
```

获取最佳gptq配置的兼容字典

## BitsAndBytesConfig

### `class transformers.BitsAndBytesConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L150)

```py
( load_in_8bit = False load_in_4bit = False llm_int8_threshold = 6.0 llm_int8_skip_modules = None llm_int8_enable_fp32_cpu_offload = False llm_int8_has_fp16_weight = False bnb_4bit_compute_dtype = None bnb_4bit_quant_type = 'fp4' bnb_4bit_use_double_quant = False **kwargs )
```

参数

+   `load_in_8bit` (`bool`, *optional*, 默认为 `False`) — 此标志用于启用LLM.int8()的8位量化。

+   `load_in_4bit` (`bool`, *optional*, 默认为 `False`) — 此标志用于通过使用`bitsandbytes`中的FP4/NF4层替换线性层来启用4位量化。

+   `llm_int8_threshold` (`float`, *optional*, 默认为 6.0) — 这对应于异常值检测中的异常值阈值，如`LLM.int8()：规模化变压器的8位矩阵乘法`论文中所述：[https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339) 超过此阈值的任何隐藏状态值将被视为异常值，并且对这些值的操作将在fp16中进行。值通常服从正态分布，即，大多数值在范围[-3.5, 3.5]内，但对于大型模型，有一些异常系统性异常值，其分布方式非常不同。这些异常值通常在区间[-60, -6]或[6, 60]内。对于幅度约为5的值，int8量化效果很好，但超过这个范围，性能会受到显著影响。一个很好的默认阈值是6，但对于更不稳定的模型（小模型，微调），可能需要更低的阈值。

+   `llm_int8_skip_modules` (`List[str]`, *optional*) — 我们不希望将其转换为8位的模块的显式列表。这对于具有不同位置的几个头部且不一定在最后位置的模型非常有用，例如Jukebox。例如，对于`CausalLM`模型，最后的`lm_head`保留在其原始`dtype`中。

+   `llm_int8_enable_fp32_cpu_offload` (`bool`, *optional*, 默认为 `False`) — 此标志用于高级用例和了解此功能的用户。如果要将模型分成不同部分，并在GPU上的某些部分中使用int8，在CPU上的某些部分中使用fp32，则可以使用此标志。这对于卸载大型模型（例如`google/flan-t5-xxl`）非常有用。请注意，int8操作将不会在CPU上运行。

+   `llm_int8_has_fp16_weight` (`bool`, *optional*, 默认为 `False`) — 此标志使用16位主权重运行LLM.int8()。这对于微调很有用，因为权重不必在反向传播中来回转换。

+   `bnb_4bit_compute_dtype` (`torch.dtype`或str, *optional*, 默认为 `torch.float32`) — 这将设置可能与输入时间不同的计算类型。例如，输入可能是fp32，但计算可以设置为bf16以加快速度。

+   `bnb_4bit_quant_type` (`str`, *optional*, 默认为 `"fp4"`) — 这将在bnb.nn.Linear4Bit层中设置量化数据类型。选项是FP4和NF4数据类型，由`fp4`或`nf4`指定。

+   `bnb_4bit_use_double_quant` (`bool`, *optional*, 默认为 `False`) — 此标志用于嵌套量化，其中第一次量化的量化常数再次量化。

+   `kwargs` (`Dict[str, Any]`, *optional*) — 从中初始化配置对象的其他参数。

这是一个关于使用`bitsandbytes`加载的模型可以玩耍的所有可能属性和功能的包装类。

因此，这取代了`load_in_8bit`或`load_in_4bit`，因此这两个选项是互斥的。

目前仅支持`LLM.int8()`，`FP4`和`NF4`量化。如果向`bitsandbytes`添加更多方法，则将向此类添加更多参数。

#### `is_quantizable`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L266)

```py
( )
```

如果模型可以量化，则返回`True`，否则返回`False`。

#### `post_init`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L235)

```py
( )
```

安全检查器，检查参数是否正确 - 还将一些NoneType参数替换为它们的默认值。

#### `quantization_method`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L272)

```py
( )
```

此方法返回模型使用的量化方法。如果模型不可量化，则返回`None`。

#### `to_diff_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L300)

```py
( ) → export const metadata = 'undefined';Dict[str, Any]
```

返回

`Dict[str, Any]`

构成此配置实例的所有属性的字典，

从配置中删除所有与默认配置属性相对应的属性，以提高可读性并序列化为Python字典。
