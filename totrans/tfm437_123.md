# 量化

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization)

量化技术通过使用低精度数据类型（如 8 位整数（int8））表示权重和激活来减少内存和计算成本。这使得您可以加载通常无法放入内存的更大模型，并加快推理速度。Transformers 支持 AWQ 和 GPTQ 量化算法，支持 8 位和 4 位量化与 bitsandbytes。

了解如何在量化指南中量化模型。

## AwqConfig

### `class transformers.AwqConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L542)

```py
( bits: int = 4 group_size: int = 128 zero_point: bool = True version: AWQLinearVersion = <AWQLinearVersion.GEMM: 'gemm'> backend: AwqBackendPackingMethod = <AwqBackendPackingMethod.AUTOAWQ: 'autoawq'> do_fuse: Optional = None fuse_max_seq_len: Optional = None modules_to_fuse: Optional = None modules_to_not_convert: Optional = None **kwargs )
```

参数

+   `bits` (`int`，*可选*，默认为 4) — 要量化为的位数。

+   `group_size` (`int`，*可选*，默认为 128) — 用于量化的组大小。推荐值为 128，-1 使用每列量化。

+   `zero_point` (`bool`，*可选*，默认为`True`) — 是否使用零点量化。

+   `version` (`AWQLinearVersion`，*可选*，默认为`AWQLinearVersion.GEMM`) — 要使用的量化算法版本。对于大批量大小（例如>= 8），GEMM 更好，否则，GEMV 更好（例如< 8）

+   `backend` (`AwqBackendPackingMethod`，*可选*，默认为`AwqBackendPackingMethod.AUTOAWQ`) — 量化后端。一些模型可能使用`llm-awq`后端进行量化。这对于使用`llm-awq`库量化自己的模型的用户很有用。

+   `do_fuse` (`bool`，*可选*，默认为`False`) — 是否将注意力和 mlp 层融合在一起以加快推理速度

+   `fuse_max_seq_len` (`int`，*可选*) — 使用融合时生成的最大序列长度。

+   `modules_to_fuse` (`dict`，*可选*，默认为`None`) — 用用户指定的融合方案覆盖原生支持的融合方案。

+   `modules_to_not_convert` (`list`，*可选*，默认为`None`) — 不量化的模块列表，对于需要明确保留一些模块在原始精度中的模型进行量化很有用（例如 Whisper 编码器，Llava 编码器，Mixtral 门层）。请注意，您不能直接使用 transformers 进行量化，请参考`AutoAWQ`文档以量化 HF 模型。

这是一个关于所有可能属性和特性的包装类，您可以使用`auto-awq`库加载的模型进行玩耍，该库依赖于 auto_awq 后端的 awq 量化。

#### `post_init`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L605)

```py
( )
```

安全检查器检查参数是否正确

## GPTQConfig

### `class transformers.GPTQConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L328)

```py
( bits: int tokenizer: Any = None dataset: Union = None group_size: int = 128 damp_percent: float = 0.1 desc_act: bool = False sym: bool = True true_sequential: bool = True use_cuda_fp16: bool = False model_seqlen: Optional = None block_name_to_quantize: Optional = None module_name_preceding_first_block: Optional = None batch_size: int = 1 pad_token_id: Optional = None use_exllama: Optional = None max_input_length: Optional = None exllama_config: Optional = None cache_block_outputs: bool = True modules_in_block_to_quantize: Optional = None **kwargs )
```

参数

+   `bits` (`int`) — 要量化为的位数，支持的数字为（2、3、4、8）。

+   `tokenizer` (`str`或`PreTrainedTokenizerBase`，*可选*) — 用于处理数据集的分词器。您可以传递以下内容：

    +   自定义分词器对象。

    +   一个字符串，托管在 huggingface.co 模型存储库中的预定义分词器的*模型 id*。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   包含分词器所需词汇文件的*目录*路径，例如使用 save_pretrained()方法保存的目录，例如`./my_model_directory/`。

+   `dataset` (`Union[List[str]]`，*可选*) — 用于量化的数据集。您可以提供自己的数据集列表，或者只使用 GPTQ 论文中使用的原始数据集[‘wikitext2’，‘c4’，‘c4-new’，‘ptb’，‘ptb-new’]

+   `group_size` (`int`, *optional*, defaults to 128) — 用于量化的组大小。推荐值为 128，-1 使用每列量化。

+   `damp_percent` (`float`, *optional*, defaults to 0.1) — 用于阻尼的平均 Hessian 对角线的百分比。推荐值为 0.1。

+   `desc_act` (`bool`, *optional*, defaults to `False`) — 是否按照激活大小递减的顺序量化列。将其设置为 False 可以显著加快推理速度，但困惑度可能会略有下降。也称为 act-order。

+   `sym` (`bool`, *optional*, defaults to `True`) — 是否使用对称量化。

+   `true_sequential` (`bool`, *optional*, defaults to `True`) — 是否在单个 Transformer 块内执行顺序量化。我们执行逐层量化，而不是一次性量化整个块。因此，每个层都经历了使用已通过先前量化的层的输入进行量化的过程。

+   `use_cuda_fp16` (`bool`, *optional*, defaults to `False`) — 是否使用优化的 cuda 内核进行 fp16 模型。需要将模型设置为 fp16。

+   `model_seqlen` (`int`, *optional*) — 模型可以接受的最大序列长度。

+   `block_name_to_quantize` (`str`, *optional*) — 要量化的 transformers 块名称。如果为 None，我们将使用常见模式（例如 model.layers）推断块名称。

+   `module_name_preceding_first_block` (`List[str]`, *optional*) — 在第一个 Transformer 块之前的层。

+   `batch_size` (`int`, *optional*, defaults to 1) — 处理数据集时使用的批量大小

+   `pad_token_id` (`int`, *optional*) — 填充标记 id。当`batch_size` > 1 时需要准备数据集。

+   `use_exllama` (`bool`, *optional*) — 是否使用 exllama 后端。如果未设置，默认为`True`。仅在`bits` = 4 时有效。

+   `max_input_length` (`int`, *optional*) — 最大输入长度。这是为了初始化一个依赖于最大预期输入长度的缓冲区而需要的。它是特定于具有 act-order 的 exllama 后端。

+   `exllama_config` (`Dict[str, Any]`, *optional*) — exllama 配置。您可以通过`version`键指定 exllama 内核的版本。如果未设置，默认为`{"version": 1}`。

+   `cache_block_outputs` (`bool`, *optional*, defaults to `True`) — 是否缓存块输出以便作为后续块的输入重复使用。

+   `modules_in_block_to_quantize` (`List[List[str]]`, *optional*) — 要在指定块中量化的模块名称列表的列表。此参数可用于排除某些线性模块的量化。可以通过设置`block_name_to_quantize`来指定要量化的块。我们将依次量化每个列表。如果未设置，我们将量化所有线性层。示例：`modules_in_block_to_quantize =[["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"], ["self_attn.o_proj"]]`。在此示例中，我们将首先同时量化 q、k、v 层，因为它们是独立的。然后，我们将在量化 q、k、v 层的基础上量化`self_attn.o_proj`层。这样，我们将获得更好的结果，因为它反映了模型在量化时`self_attn.o_proj`将获得的真实输入。

这是一个关于使用`optimum` api 加载的模型的所有可能属性和特性的包装类，用于依赖于 auto_gptq 后端的 gptq 量化。

#### `from_dict_optimum`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L527)

```py
( config_dict )
```

获取与最佳 gptq 配置字典兼容的类

#### `post_init`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L444)

```py
( )
```

确保参数正确的安全检查器

#### `to_dict_optimum`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L518)

```py
( )
```

获取最佳 gptq 配置的兼容字典

## BitsAndBytesConfig

### `class transformers.BitsAndBytesConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L150)

```py
( load_in_8bit = False load_in_4bit = False llm_int8_threshold = 6.0 llm_int8_skip_modules = None llm_int8_enable_fp32_cpu_offload = False llm_int8_has_fp16_weight = False bnb_4bit_compute_dtype = None bnb_4bit_quant_type = 'fp4' bnb_4bit_use_double_quant = False **kwargs )
```

参数

+   `load_in_8bit` (`bool`, *optional*, 默认为 `False`) — 此标志用于启用 LLM.int8()的 8 位量化。

+   `load_in_4bit` (`bool`, *optional*, 默认为 `False`) — 此标志用于通过使用`bitsandbytes`中的 FP4/NF4 层替换线性层来启用 4 位量化。

+   `llm_int8_threshold` (`float`, *optional*, 默认为 6.0) — 这对应于异常值检测中的异常值阈值，如`LLM.int8()：规模化变压器的 8 位矩阵乘法`论文中所述：[`arxiv.org/abs/2208.07339`](https://arxiv.org/abs/2208.07339) 超过此阈值的任何隐藏状态值将被视为异常值，并且对这些值的操作将在 fp16 中进行。值通常服从正态分布，即，大多数值在范围[-3.5, 3.5]内，但对于大型模型，有一些异常系统性异常值，其分布方式非常不同。这些异常值通常在区间[-60, -6]或[6, 60]内。对于幅度约为 5 的值，int8 量化效果很好，但超过这个范围，性能会受到显著影响。一个很好的默认阈值是 6，但对于更不稳定的模型（小模型，微调），可能需要更低的阈值。

+   `llm_int8_skip_modules` (`List[str]`, *optional*) — 我们不希望将其转换为 8 位的模块的显式列表。这对于具有不同位置的几个头部且不一定在最后位置的模型非常有用，例如 Jukebox。例如，对于`CausalLM`模型，最后的`lm_head`保留在其原始`dtype`中。

+   `llm_int8_enable_fp32_cpu_offload` (`bool`, *optional*, 默认为 `False`) — 此标志用于高级用例和了解此功能的用户。如果要将模型分成不同部分，并在 GPU 上的某些部分中使用 int8，在 CPU 上的某些部分中使用 fp32，则可以使用此标志。这对于卸载大型模型（例如`google/flan-t5-xxl`）非常有用。请注意，int8 操作将不会在 CPU 上运行。

+   `llm_int8_has_fp16_weight` (`bool`, *optional*, 默认为 `False`) — 此标志使用 16 位主权重运行 LLM.int8()。这对于微调很有用，因为权重不必在反向传播中来回转换。

+   `bnb_4bit_compute_dtype` (`torch.dtype`或 str, *optional*, 默认为 `torch.float32`) — 这将设置可能与输入时间不同的计算类型。例如，输入可能是 fp32，但计算可以设置为 bf16 以加快速度。

+   `bnb_4bit_quant_type` (`str`, *optional*, 默认为 `"fp4"`) — 这将在 bnb.nn.Linear4Bit 层中设置量化数据类型。选项是 FP4 和 NF4 数据类型，由`fp4`或`nf4`指定。

+   `bnb_4bit_use_double_quant` (`bool`, *optional*, 默认为 `False`) — 此标志用于嵌套量化，其中第一次量化的量化常数再次量化。

+   `kwargs` (`Dict[str, Any]`, *optional*) — 从中初始化配置对象的其他参数。

这是一个关于使用`bitsandbytes`加载的模型可以玩耍的所有可能属性和功能的包装类。

因此，这取代了`load_in_8bit`或`load_in_4bit`，因此这两个选项是互斥的。

目前仅支持`LLM.int8()`，`FP4`和`NF4`量化。如果向`bitsandbytes`添加更多方法，则将向此类添加更多参数。

#### `is_quantizable`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L266)

```py
( )
```

如果模型可以量化，则返回`True`，否则返回`False`。

#### `post_init`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L235)

```py
( )
```

安全检查器，检查参数是否正确 - 还将一些 NoneType 参数替换为它们的默认值。

#### `quantization_method`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L272)

```py
( )
```

此方法返回模型使用的量化方法。如果模型不可量化，则返回`None`。

#### `to_diff_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L300)

```py
( ) → export const metadata = 'undefined';Dict[str, Any]
```

返回

`Dict[str, Any]`

构成此配置实例的所有属性的字典，

从配置中删除所有与默认配置属性相对应的属性，以提高可读性并序列化为 Python 字典。
