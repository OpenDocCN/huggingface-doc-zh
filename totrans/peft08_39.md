# LoKr

> 原始文本: [`huggingface.co/docs/peft/package_reference/lokr`](https://huggingface.co/docs/peft/package_reference/lokr)

低秩 Kronecker 乘积（[LoKr](https://hf.co/papers/2309.14859)）是一种 LoRA 变体方法，它用两个低秩矩阵近似大权重矩阵，并将它们与 Kronecker 乘积结合。LoKr 还提供一个可选的第三个低秩矩阵，以在微调期间提供更好的控制。

## LoKrConfig

### `class peft.LoKrConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lokr/config.py#L22)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False rank_pattern: Optional[dict] = <factory> alpha_pattern: Optional[dict] = <factory> r: int = 8 alpha: int = 8 rank_dropout: float = 0.0 module_dropout: float = 0.0 use_effective_conv2d: bool = False decompose_both: bool = False decompose_factor: int = -1 target_modules: Union = None init_weights: bool = True layers_to_transform: Union = None layers_pattern: Optional = None modules_to_save: Optional = None )
```

参数

+   `r` (`int`) — LoKr 等级。

+   `alpha` (`int`) — LoKr 缩放的 alpha 参数。

+   `rank_dropout` (`float`) — 训练期间等级维度的丢失概率。

+   `module_dropout` (`float`) — 训练期间禁用 LoKr 模块的丢失概率。

+   `use_effective_conv2d` (`bool`) — 对于 ksize > 1 的 Conv2d 使用参数有效分解（来自 FedPara 论文的“命题 3”）。

+   `decompose_both` (`bool`) — 对左 Kronecker 乘积矩阵执行等级分解。

+   `decompose_factor` (`int`) — Kronecker 乘积分解因子。

+   `target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了此项，只有指定名称的模块将被替换。当传递一个字符串时，将执行正则表达式匹配。当传递一个字符串列表时，将执行精确匹配或检查模块名称是否以任何传递的字符串结尾。如果指定为 ‘all-linear’，则选择所有线性/Conv1D 模块，不包括输出层。如果未指定，将根据模型架构选择模块。如果不知道架构，将引发错误 — 在这种情况下，应手动指定目标模块。

+   `init_weights` (`bool`) — 是否执行适配器权重的初始化。默认为 `True`，不建议传递 `False`。

+   `layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递了一个整数列表，它将应用于在此列表中指定的层索引的适配器。如果传递了一个单个整数，它将在此索引处的层上应用变换。

+   `layers_pattern` (`str`) — 层模式名称，仅在 `layers_to_transform` 与 `None` 不同的情况下使用。

+   `rank_pattern` (`dict`) — 从层名称或正则表达式到与 `r` 指定的默认等级不同的等级的映射。

+   `alpha_pattern` (`dict`) — 从层名称或正则表达式到与 `alpha` 指定的默认 alpha 不同的 alpha 的映射。

+   `modules_to_save` (`Optional[List[str]]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。

LoKrModel 的配置类。

## LoKrModel

### `class peft.LoKrModel`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lokr/model.py#L27)

```py
( model config adapter_name ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` (`torch.nn.Module`) — 适配器调谐器层将附加到的模型。

+   `config` (LoKrConfig) — LoKr 模型的配置。

+   `adapter_name` (`str`) — 适配器的名称，默认为 `"default"`。

返回

`torch.nn.Module`

LoKr 模型。

从预训练模型创建低秩 Kronecker 乘积模型。原始方法部分描述在 [`arxiv.org/abs/2108.06098`](https://arxiv.org/abs/2108.06098) 和 [`arxiv.org/abs/2309.14859`](https://arxiv.org/abs/2309.14859) 当前实现大量借鉴自 [`github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/lokr.py`](https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/lokr.py)

示例:

```py
>>> from diffusers import StableDiffusionPipeline
>>> from peft import LoKrModel, LoKrConfig

>>> config_te = LoKrConfig(
...     r=8,
...     lora_alpha=32,
...     target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"],
...     rank_dropout=0.0,
...     module_dropout=0.0,
...     init_weights=True,
... )
>>> config_unet = LoKrConfig(
...     r=8,
...     lora_alpha=32,
...     target_modules=[
...         "proj_in",
...         "proj_out",
...         "to_k",
...         "to_q",
...         "to_v",
...         "to_out.0",
...         "ff.net.0.proj",
...         "ff.net.2",
...     ],
...     rank_dropout=0.0,
...     module_dropout=0.0,
...     init_weights=True,
...     use_effective_conv2d=True,
... )

>>> model = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> model.text_encoder = LoKrModel(model.text_encoder, config_te, "default")
>>> model.unet = LoKrModel(model.unet, config_unet, "default")
```

**属性**:

+   `model` (`~torch.nn.Module`) — 要适应的模型。

+   `peft_config` (LoKrConfig): LoKr 模型的配置。
