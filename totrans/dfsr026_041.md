# ControlNet

> 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/controlnet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)

ControlNet 是一种通过在模型中加入额外输入图像来控制图像扩散模型的模型类型。有许多类型的条件输入（灵巧边缘、用户素描、人体姿势、深度等），您可以使用这些输入来控制扩散模型。这非常有用，因为它使您能够更好地控制图像生成，更容易生成特定图像，而无需尝试不同的文本提示或去噪值。

查看 [ControlNet](https://huggingface.co/papers/2302.05543) 论文 v1 的第 3.5 节，了解各种条件输入上的 ControlNet 实现列表。您可以在 [lllyasviel](https://huggingface.co/lllyasviel) 的 Hub 个人资料上找到官方 Stable Diffusion ControlNet 条件模型，以及在 Hub 上更多的 [community-trained](https://huggingface.co/models?other=stable-diffusion&other=controlnet) 模型。

对于 Stable Diffusion XL（SDXL）ControlNet 模型，您可以在 🤗 [Diffusers](https://huggingface.co/diffusers) Hub 组织中找到它们，或者您可以在 Hub 上浏览 [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet) 的模型。

ControlNet 模型有两组权重（或块），通过一个零卷积层连接：

+   *锁定的副本* 保留了大型预训练扩散模型学到的一切

+   *可训练的副本* 是在额外的条件输入上训练的

由于锁定的副本保留了预训练模型，因此在新的条件输入上训练和实现ControlNet与微调任何其他模型一样快，因为您不是从头开始训练模型。

本指南将向您展示如何使用 ControlNet 进行文本到图像、图像到图像、修复等操作！有许多类型的 ControlNet 条件输入可供选择，但在本指南中，我们只关注其中几种。请随意尝试其他条件输入！

在开始之前，请确保已安装以下库：

```py
# uncomment to install the necessary libraries in Colab
#!pip install -q diffusers transformers accelerate opencv-python
```

## 文本到图像

对于文本到图像，通常会向模型传递文本提示。但是使用 ControlNet，您可以指定额外的条件输入。让我们使用灵巧图像对模型进行条件，灵巧图像是在黑色背景上的图像的白色轮廓。这样，ControlNet 可以使用灵巧图像作为控制来指导模型生成具有相同轮廓的图像。

加载一张图像并使用 [opencv-python](https://github.com/opencv/opencv-python) 库提取灵巧图像：

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np

original_image = load_image(
    "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
```

![](../Images/f3d8c311d87a9fa7e106fb97353058b0.png)

原始图像

![](../Images/58be7817d7057b4931067ef80d28b944.png)

灵巧图像

接下来，加载一个在灵巧边缘检测上进行条件的 ControlNet 模型，并将其传递给 [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)。使用更快的 [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler) 并启用模型卸载以加快推理速度并减少内存使用。

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

现在将您的提示和灵巧的图像传递给管道：

```py
output = pipe(
    "the mona lisa", image=canny_image
).images[0]
make_image_grid([original_image, canny_image, output], rows=1, cols=3)
```

![](../Images/0ae01d64d6dd0f507a15d4c469ccb3ea.png)

## 图像到图像

对于图像到图像，通常会将初始图像和提示传递给管道以生成新图像。使用 ControlNet，您可以传递额外的条件输入来指导模型。让我们使用深度图来对模型进行条件，深度图包含空间信息。这样，ControlNet 可以使用深度图作为控制来指导模型生成保留空间信息的图像。

您将使用[StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)来完成此任务，这与[StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)不同，因为它允许您传递一个初始图片作为图像生成过程的起点。

加载一张图片，并使用🤗 Transformers中的`depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)来提取图片的深度图：

```py
import torch
import numpy as np

from transformers import pipeline
from diffusers.utils import load_image, make_image_grid

image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"
)

def get_depth_map(image, depth_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    depth_map = detected_map.permute(2, 0, 1)
    return depth_map

depth_estimator = pipeline("depth-estimation")
depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to("cuda")
```

接下来，加载一个基于深度图条件的ControlNet模型，并将其传递给[StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)。使用更快的[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)，并启用模型卸载以加快推理速度并减少内存使用。

```py
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

现在将您的提示、初始图片和深度图传递给管道：

```py
output = pipe(
    "lego batman and robin", image=image, control_image=depth_map,
).images[0]
make_image_grid([image, output], rows=1, cols=2)
```

![](../Images/1677a0fe907026354ef7fe3200082c57.png)

原始图片

![](../Images/5cb27d107bcf2a908b7a2c6086129663.png)

生成的图片

## 修补

对于修补，您需要一个初始图片、一个蒙版图片和一个描述用什么替换蒙版的提示。ControlNet模型允许您添加另一个控制图片来对模型进行条件化。让我们使用一个修补蒙版来对模型进行条件化。这样，ControlNet可以使用修补蒙版作为控制，引导模型在蒙版区域生成一张图片。

加载一个初始图片和一个蒙版图片：

```py
from diffusers.utils import load_image, make_image_grid

init_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"
)
init_image = init_image.resize((512, 512))

mask_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"
)
mask_image = mask_image.resize((512, 512))
make_image_grid([init_image, mask_image], rows=1, cols=2)
```

创建一个函数，从初始图片和蒙版图片准备控制图片。这将创建一个张量，如果`mask_image`中对应的像素超过一定阈值，则将`init_image`中的像素标记为掩码。

```py
import numpy as np
import torch

def make_inpaint_condition(image, image_mask):
    image = np.array(image.convert("RGB")).astype(np.float32) / 255.0
    image_mask = np.array(image_mask.convert("L")).astype(np.float32) / 255.0

    assert image.shape[0:1] == image_mask.shape[0:1]
    image[image_mask > 0.5] = -1.0  # set as masked pixel
    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return image

control_image = make_inpaint_condition(init_image, mask_image)
```

![](../Images/0fb83d127798531122cbaf23bb76b7c8.png)

原始图片

![](../Images/f1431ffd86896d9e2aed5e38fead866b.png)

蒙版图片

加载一个基于修补的ControlNet模型，并将其传递给[StableDiffusionControlNetInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetInpaintPipeline)。使用更快的[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)，并启用模型卸载以加快推理速度并减少内存使用。

```py
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

现在将您的提示、初始图片、蒙版图片和控制图片传递给管道：

```py
output = pipe(
    "corgi face with large ears, detailed, pixar, animated, disney",
    num_inference_steps=20,
    eta=1.0,
    image=init_image,
    mask_image=mask_image,
    control_image=control_image,
).images[0]
make_image_grid([init_image, mask_image, output], rows=1, cols=3)
```

![](../Images/da98e94db90a9c1f480cba2d1ce9fd5a.png)

## 猜测模式

[猜测模式](https://github.com/lllyasviel/ControlNet/discussions/188)根本不需要向ControlNet提供提示！这迫使ControlNet编码器尽力“猜测”输入控制图（深度图、姿势估计、canny边缘等）的内容。

猜测模式通过固定比例调整ControlNet的输出残差的规模，具体取决于块深度。最浅的`DownBlock`对应于0.1，随着块变得更深，规模呈指数增长，使得`MidBlock`输出的规模变为1.0。

猜测模式对提示条件没有任何影响，如果需要仍然可以提供提示。

在管道中设置`guess_mode=True`，并建议将`guidance_scale`值设置在3.0到5.0之间。

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
from PIL import Image
import cv2

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, use_safetensors=True).to("cuda")

original_image = load_image("https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png")

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe("", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

![](../Images/a552b1e5d87c2575305045e9d6e5c665.png)

带提示的常规模式

![](../Images/0e47994b983e6afdf574ca4730cc4d14.png)

无提示的猜测模式

## 带有稳定扩散XL的ControlNet

目前与Stable Diffusion XL（SDXL）兼容的ControlNet模型并不多，但我们已经为SDXL训练了两个全尺寸的ControlNet模型，这些模型是基于边缘检测和深度图的条件。我们还在尝试创建这些SDXL兼容的ControlNet模型的较小版本，以便更容易在资源受限的硬件上运行。您可以在[🤗 Diffusers Hub组织](https://huggingface.co/diffusers)上找到这些检查点！

让我们使用一个基于边缘检测的SDXL ControlNet来生成一幅图像。首先加载一幅图像并准备边缘检测图像：

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np
import torch

original_image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

![](../Images/dfd7f04394323063a78501b85525d2b3.png)

原始图像

![](../Images/a7144be809b8fc762e76238607a485d2.png)

边缘检测图像

加载一个基于边缘检测的SDXL ControlNet模型，并将其传递给[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)。您还可以启用模型卸载以减少内存使用。

```py
controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    controlnet=controlnet,
    vae=vae,
    torch_dtype=torch.float16,
    use_safetensors=True
)
pipe.enable_model_cpu_offload()
```

现在将您的提示（如果您使用负提示，则可选）和边缘检测图像传递给管道：

[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)参数确定分配给条件输入的权重。建议将值设为0.5以获得良好的泛化效果，但请随意尝试这个数字！

```py
prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = 'low quality, bad quality, sketches'

image = pipe(
    prompt,
    negative_prompt=negative_prompt,
    image=canny_image,
    controlnet_conditioning_scale=0.5,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

![](../Images/033f4d98632798d409eabe8ebfffc8db.png)

您还可以通过将参数设置为`True`，在猜测模式下使用[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)：

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
import cv2
from PIL import Image

prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = "low quality, bad quality, sketches"

original_image = load_image(
    "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.enable_model_cpu_offload()

image = np.array(original_image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe(
    prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

## MultiControlNet

将SDXL模型替换为像[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)这样的模型，以使用多个条件输入与Stable Diffusion模型。

您可以从不同的图像输入中组合多个ControlNet条件，以创建*MultiControlNet*。为了获得更好的结果，通常有帮助的是：

1.  对条件进行遮罩处理，使其不重叠（例如，遮罩姿势条件所在的边缘检测图像区域）

1.  尝试[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)参数，以确定分配给每个条件输入的权重

在这个示例中，您将结合边缘检测图像和人体姿势估计图像来生成一幅新图像。

准备边缘检测图像条件：

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import numpy as np
import cv2

original_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
)
image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)

# zero out middle columns of image where pose will be overlaid
zero_start = image.shape[1] // 4
zero_end = zero_start + image.shape[1] // 2
image[:, zero_start:zero_end] = 0

image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

![](../Images/35bd0f11e1c7ca277ac60a85a75be06a.png)

原始图像

![](../Images/5a37576bad87523fdb8a31aa840b4fdc.png)

边缘检测图像

对于人体姿势估计，安装[controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)：

```py
# uncomment to install the necessary library in Colab
#!pip install -q controlnet-aux
```

准备人体姿势估计条件：

```py
from controlnet_aux import OpenposeDetector

openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
original_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"
)
openpose_image = openpose(original_image)
make_image_grid([original_image, openpose_image], rows=1, cols=2)
```

![](../Images/77e2344bf3a8143f50f20ca5d095bf81.png)

原始图像

![](../Images/5a8432160ffe7c6268d59c8c8d536c20.png)

人体姿势图像

加载与每个条件对应的ControlNet模型列表，并将它们传递给[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)。使用更快的[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)并启用模型卸载以减少内存使用。

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler
import torch

controlnets = [
    ControlNetModel.from_pretrained(
        "thibaud/controlnet-openpose-sdxl-1.0", torch_dtype=torch.float16
    ),
    ControlNetModel.from_pretrained(
        "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
    ),
]

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

现在您可以将您的提示（如果您使用负提示，则可选）、边缘检测图像和姿势图像传递给管道：

```py
prompt = "a giant standing in a fantasy landscape, best quality"
negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

generator = torch.manual_seed(1)

images = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]

images = pipe(
    prompt,
    image=images,
    num_inference_steps=25,
    generator=generator,
    negative_prompt=negative_prompt,
    num_images_per_prompt=3,
    controlnet_conditioning_scale=[1.0, 0.8],
).images
make_image_grid([original_image, canny_image, openpose_image,
                images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)
```

![](../Images/780255e1bc5d49726b432882894f0b81.png)
