# ControlNet

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/controlnet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)

ControlNet æ˜¯ä¸€ç§é€šè¿‡åœ¨æ¨¡å‹ä¸­åŠ å…¥é¢å¤–è¾“å…¥å›¾åƒæ¥æ§åˆ¶å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ç±»å‹ã€‚æœ‰è®¸å¤šç±»å‹çš„æ¡ä»¶è¾“å…¥ï¼ˆçµå·§è¾¹ç¼˜ã€ç”¨æˆ·ç´ æã€äººä½“å§¿åŠ¿ã€æ·±åº¦ç­‰ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›è¾“å…¥æ¥æ§åˆ¶æ‰©æ•£æ¨¡å‹ã€‚è¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä½¿æ‚¨èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶å›¾åƒç”Ÿæˆï¼Œæ›´å®¹æ˜“ç”Ÿæˆç‰¹å®šå›¾åƒï¼Œè€Œæ— éœ€å°è¯•ä¸åŒçš„æ–‡æœ¬æç¤ºæˆ–å»å™ªå€¼ã€‚

æŸ¥çœ‹ [ControlNet](https://huggingface.co/papers/2302.05543) è®ºæ–‡ v1 çš„ç¬¬ 3.5 èŠ‚ï¼Œäº†è§£å„ç§æ¡ä»¶è¾“å…¥ä¸Šçš„ ControlNet å®ç°åˆ—è¡¨ã€‚æ‚¨å¯ä»¥åœ¨ [lllyasviel](https://huggingface.co/lllyasviel) çš„ Hub ä¸ªäººèµ„æ–™ä¸Šæ‰¾åˆ°å®˜æ–¹ Stable Diffusion ControlNet æ¡ä»¶æ¨¡å‹ï¼Œä»¥åŠåœ¨ Hub ä¸Šæ›´å¤šçš„ [community-trained](https://huggingface.co/models?other=stable-diffusion&other=controlnet) æ¨¡å‹ã€‚

å¯¹äº Stable Diffusion XLï¼ˆSDXLï¼‰ControlNet æ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨ ğŸ¤— [Diffusers](https://huggingface.co/diffusers) Hub ç»„ç»‡ä¸­æ‰¾åˆ°å®ƒä»¬ï¼Œæˆ–è€…æ‚¨å¯ä»¥åœ¨ Hub ä¸Šæµè§ˆ [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet) çš„æ¨¡å‹ã€‚

ControlNet æ¨¡å‹æœ‰ä¸¤ç»„æƒé‡ï¼ˆæˆ–å—ï¼‰ï¼Œé€šè¿‡ä¸€ä¸ªé›¶å·ç§¯å±‚è¿æ¥ï¼š

+   *é”å®šçš„å‰¯æœ¬* ä¿ç•™äº†å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å­¦åˆ°çš„ä¸€åˆ‡

+   *å¯è®­ç»ƒçš„å‰¯æœ¬* æ˜¯åœ¨é¢å¤–çš„æ¡ä»¶è¾“å…¥ä¸Šè®­ç»ƒçš„

ç”±äºé”å®šçš„å‰¯æœ¬ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤åœ¨æ–°çš„æ¡ä»¶è¾“å…¥ä¸Šè®­ç»ƒå’Œå®ç°ControlNetä¸å¾®è°ƒä»»ä½•å…¶ä»–æ¨¡å‹ä¸€æ ·å¿«ï¼Œå› ä¸ºæ‚¨ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ControlNet è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€ä¿®å¤ç­‰æ“ä½œï¼æœ‰è®¸å¤šç±»å‹çš„ ControlNet æ¡ä»¶è¾“å…¥å¯ä¾›é€‰æ‹©ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬åªå…³æ³¨å…¶ä¸­å‡ ç§ã€‚è¯·éšæ„å°è¯•å…¶ä»–æ¡ä»¶è¾“å…¥ï¼

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š

```py
# uncomment to install the necessary libraries in Colab
#!pip install -q diffusers transformers accelerate opencv-python
```

## æ–‡æœ¬åˆ°å›¾åƒ

å¯¹äºæ–‡æœ¬åˆ°å›¾åƒï¼Œé€šå¸¸ä¼šå‘æ¨¡å‹ä¼ é€’æ–‡æœ¬æç¤ºã€‚ä½†æ˜¯ä½¿ç”¨ ControlNetï¼Œæ‚¨å¯ä»¥æŒ‡å®šé¢å¤–çš„æ¡ä»¶è¾“å…¥ã€‚è®©æˆ‘ä»¬ä½¿ç”¨çµå·§å›¾åƒå¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶ï¼Œçµå·§å›¾åƒæ˜¯åœ¨é»‘è‰²èƒŒæ™¯ä¸Šçš„å›¾åƒçš„ç™½è‰²è½®å»“ã€‚è¿™æ ·ï¼ŒControlNet å¯ä»¥ä½¿ç”¨çµå·§å›¾åƒä½œä¸ºæ§åˆ¶æ¥æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆå…·æœ‰ç›¸åŒè½®å»“çš„å›¾åƒã€‚

åŠ è½½ä¸€å¼ å›¾åƒå¹¶ä½¿ç”¨ [opencv-python](https://github.com/opencv/opencv-python) åº“æå–çµå·§å›¾åƒï¼š

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np

original_image = load_image(
    "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
```

![](../Images/f3d8c311d87a9fa7e106fb97353058b0.png)

åŸå§‹å›¾åƒ

![](../Images/58be7817d7057b4931067ef80d28b944.png)

çµå·§å›¾åƒ

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªåœ¨çµå·§è¾¹ç¼˜æ£€æµ‹ä¸Šè¿›è¡Œæ¡ä»¶çš„ ControlNet æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)ã€‚ä½¿ç”¨æ›´å¿«çš„ [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler) å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ç°åœ¨å°†æ‚¨çš„æç¤ºå’Œçµå·§çš„å›¾åƒä¼ é€’ç»™ç®¡é“ï¼š

```py
output = pipe(
    "the mona lisa", image=canny_image
).images[0]
make_image_grid([original_image, canny_image, output], rows=1, cols=3)
```

![](../Images/0ae01d64d6dd0f507a15d4c469ccb3ea.png)

## å›¾åƒåˆ°å›¾åƒ

å¯¹äºå›¾åƒåˆ°å›¾åƒï¼Œé€šå¸¸ä¼šå°†åˆå§‹å›¾åƒå’Œæç¤ºä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆæ–°å›¾åƒã€‚ä½¿ç”¨ ControlNetï¼Œæ‚¨å¯ä»¥ä¼ é€’é¢å¤–çš„æ¡ä»¶è¾“å…¥æ¥æŒ‡å¯¼æ¨¡å‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨æ·±åº¦å›¾æ¥å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶ï¼Œæ·±åº¦å›¾åŒ…å«ç©ºé—´ä¿¡æ¯ã€‚è¿™æ ·ï¼ŒControlNet å¯ä»¥ä½¿ç”¨æ·±åº¦å›¾ä½œä¸ºæ§åˆ¶æ¥æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆä¿ç•™ç©ºé—´ä¿¡æ¯çš„å›¾åƒã€‚

æ‚¨å°†ä½¿ç”¨[StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)æ¥å®Œæˆæ­¤ä»»åŠ¡ï¼Œè¿™ä¸[StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)ä¸åŒï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨ä¼ é€’ä¸€ä¸ªåˆå§‹å›¾ç‰‡ä½œä¸ºå›¾åƒç”Ÿæˆè¿‡ç¨‹çš„èµ·ç‚¹ã€‚

åŠ è½½ä¸€å¼ å›¾ç‰‡ï¼Œå¹¶ä½¿ç”¨ğŸ¤— Transformersä¸­çš„`depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)æ¥æå–å›¾ç‰‡çš„æ·±åº¦å›¾ï¼š

```py
import torch
import numpy as np

from transformers import pipeline
from diffusers.utils import load_image, make_image_grid

image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"
)

def get_depth_map(image, depth_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    depth_map = detected_map.permute(2, 0, 1)
    return depth_map

depth_estimator = pipeline("depth-estimation")
depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to("cuda")
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªåŸºäºæ·±åº¦å›¾æ¡ä»¶çš„ControlNetæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™[StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)ã€‚ä½¿ç”¨æ›´å¿«çš„[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)ï¼Œå¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

```py
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ç°åœ¨å°†æ‚¨çš„æç¤ºã€åˆå§‹å›¾ç‰‡å’Œæ·±åº¦å›¾ä¼ é€’ç»™ç®¡é“ï¼š

```py
output = pipe(
    "lego batman and robin", image=image, control_image=depth_map,
).images[0]
make_image_grid([image, output], rows=1, cols=2)
```

![](../Images/1677a0fe907026354ef7fe3200082c57.png)

åŸå§‹å›¾ç‰‡

![](../Images/5cb27d107bcf2a908b7a2c6086129663.png)

ç”Ÿæˆçš„å›¾ç‰‡

## ä¿®è¡¥

å¯¹äºä¿®è¡¥ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªåˆå§‹å›¾ç‰‡ã€ä¸€ä¸ªè’™ç‰ˆå›¾ç‰‡å’Œä¸€ä¸ªæè¿°ç”¨ä»€ä¹ˆæ›¿æ¢è’™ç‰ˆçš„æç¤ºã€‚ControlNetæ¨¡å‹å…è®¸æ‚¨æ·»åŠ å¦ä¸€ä¸ªæ§åˆ¶å›¾ç‰‡æ¥å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¿®è¡¥è’™ç‰ˆæ¥å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ã€‚è¿™æ ·ï¼ŒControlNetå¯ä»¥ä½¿ç”¨ä¿®è¡¥è’™ç‰ˆä½œä¸ºæ§åˆ¶ï¼Œå¼•å¯¼æ¨¡å‹åœ¨è’™ç‰ˆåŒºåŸŸç”Ÿæˆä¸€å¼ å›¾ç‰‡ã€‚

åŠ è½½ä¸€ä¸ªåˆå§‹å›¾ç‰‡å’Œä¸€ä¸ªè’™ç‰ˆå›¾ç‰‡ï¼š

```py
from diffusers.utils import load_image, make_image_grid

init_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"
)
init_image = init_image.resize((512, 512))

mask_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"
)
mask_image = mask_image.resize((512, 512))
make_image_grid([init_image, mask_image], rows=1, cols=2)
```

åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œä»åˆå§‹å›¾ç‰‡å’Œè’™ç‰ˆå›¾ç‰‡å‡†å¤‡æ§åˆ¶å›¾ç‰‡ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªå¼ é‡ï¼Œå¦‚æœ`mask_image`ä¸­å¯¹åº”çš„åƒç´ è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼Œåˆ™å°†`init_image`ä¸­çš„åƒç´ æ ‡è®°ä¸ºæ©ç ã€‚

```py
import numpy as np
import torch

def make_inpaint_condition(image, image_mask):
    image = np.array(image.convert("RGB")).astype(np.float32) / 255.0
    image_mask = np.array(image_mask.convert("L")).astype(np.float32) / 255.0

    assert image.shape[0:1] == image_mask.shape[0:1]
    image[image_mask > 0.5] = -1.0  # set as masked pixel
    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return image

control_image = make_inpaint_condition(init_image, mask_image)
```

![](../Images/0fb83d127798531122cbaf23bb76b7c8.png)

åŸå§‹å›¾ç‰‡

![](../Images/f1431ffd86896d9e2aed5e38fead866b.png)

è’™ç‰ˆå›¾ç‰‡

åŠ è½½ä¸€ä¸ªåŸºäºä¿®è¡¥çš„ControlNetæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™[StableDiffusionControlNetInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetInpaintPipeline)ã€‚ä½¿ç”¨æ›´å¿«çš„[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)ï¼Œå¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

```py
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ç°åœ¨å°†æ‚¨çš„æç¤ºã€åˆå§‹å›¾ç‰‡ã€è’™ç‰ˆå›¾ç‰‡å’Œæ§åˆ¶å›¾ç‰‡ä¼ é€’ç»™ç®¡é“ï¼š

```py
output = pipe(
    "corgi face with large ears, detailed, pixar, animated, disney",
    num_inference_steps=20,
    eta=1.0,
    image=init_image,
    mask_image=mask_image,
    control_image=control_image,
).images[0]
make_image_grid([init_image, mask_image, output], rows=1, cols=3)
```

![](../Images/da98e94db90a9c1f480cba2d1ce9fd5a.png)

## çŒœæµ‹æ¨¡å¼

[çŒœæµ‹æ¨¡å¼](https://github.com/lllyasviel/ControlNet/discussions/188)æ ¹æœ¬ä¸éœ€è¦å‘ControlNetæä¾›æç¤ºï¼è¿™è¿«ä½¿ControlNetç¼–ç å™¨å°½åŠ›â€œçŒœæµ‹â€è¾“å…¥æ§åˆ¶å›¾ï¼ˆæ·±åº¦å›¾ã€å§¿åŠ¿ä¼°è®¡ã€cannyè¾¹ç¼˜ç­‰ï¼‰çš„å†…å®¹ã€‚

çŒœæµ‹æ¨¡å¼é€šè¿‡å›ºå®šæ¯”ä¾‹è°ƒæ•´ControlNetçš„è¾“å‡ºæ®‹å·®çš„è§„æ¨¡ï¼Œå…·ä½“å–å†³äºå—æ·±åº¦ã€‚æœ€æµ…çš„`DownBlock`å¯¹åº”äº0.1ï¼Œéšç€å—å˜å¾—æ›´æ·±ï¼Œè§„æ¨¡å‘ˆæŒ‡æ•°å¢é•¿ï¼Œä½¿å¾—`MidBlock`è¾“å‡ºçš„è§„æ¨¡å˜ä¸º1.0ã€‚

çŒœæµ‹æ¨¡å¼å¯¹æç¤ºæ¡ä»¶æ²¡æœ‰ä»»ä½•å½±å“ï¼Œå¦‚æœéœ€è¦ä»ç„¶å¯ä»¥æä¾›æç¤ºã€‚

åœ¨ç®¡é“ä¸­è®¾ç½®`guess_mode=True`ï¼Œå¹¶å»ºè®®å°†`guidance_scale`å€¼è®¾ç½®åœ¨3.0åˆ°5.0ä¹‹é—´ã€‚

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
from PIL import Image
import cv2

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, use_safetensors=True).to("cuda")

original_image = load_image("https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png")

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe("", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

![](../Images/a552b1e5d87c2575305045e9d6e5c665.png)

å¸¦æç¤ºçš„å¸¸è§„æ¨¡å¼

![](../Images/0e47994b983e6afdf574ca4730cc4d14.png)

æ— æç¤ºçš„çŒœæµ‹æ¨¡å¼

## å¸¦æœ‰ç¨³å®šæ‰©æ•£XLçš„ControlNet

ç›®å‰ä¸Stable Diffusion XLï¼ˆSDXLï¼‰å…¼å®¹çš„ControlNetæ¨¡å‹å¹¶ä¸å¤šï¼Œä½†æˆ‘ä»¬å·²ç»ä¸ºSDXLè®­ç»ƒäº†ä¸¤ä¸ªå…¨å°ºå¯¸çš„ControlNetæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ˜¯åŸºäºè¾¹ç¼˜æ£€æµ‹å’Œæ·±åº¦å›¾çš„æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜åœ¨å°è¯•åˆ›å»ºè¿™äº›SDXLå…¼å®¹çš„ControlNetæ¨¡å‹çš„è¾ƒå°ç‰ˆæœ¬ï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šè¿è¡Œã€‚æ‚¨å¯ä»¥åœ¨[ğŸ¤— Diffusers Hubç»„ç»‡](https://huggingface.co/diffusers)ä¸Šæ‰¾åˆ°è¿™äº›æ£€æŸ¥ç‚¹ï¼

è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŸºäºè¾¹ç¼˜æ£€æµ‹çš„SDXL ControlNetæ¥ç”Ÿæˆä¸€å¹…å›¾åƒã€‚é¦–å…ˆåŠ è½½ä¸€å¹…å›¾åƒå¹¶å‡†å¤‡è¾¹ç¼˜æ£€æµ‹å›¾åƒï¼š

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np
import torch

original_image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

![](../Images/dfd7f04394323063a78501b85525d2b3.png)

åŸå§‹å›¾åƒ

![](../Images/a7144be809b8fc762e76238607a485d2.png)

è¾¹ç¼˜æ£€æµ‹å›¾åƒ

åŠ è½½ä¸€ä¸ªåŸºäºè¾¹ç¼˜æ£€æµ‹çš„SDXL ControlNetæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ã€‚æ‚¨è¿˜å¯ä»¥å¯ç”¨æ¨¡å‹å¸è½½ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

```py
controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    controlnet=controlnet,
    vae=vae,
    torch_dtype=torch.float16,
    use_safetensors=True
)
pipe.enable_model_cpu_offload()
```

ç°åœ¨å°†æ‚¨çš„æç¤ºï¼ˆå¦‚æœæ‚¨ä½¿ç”¨è´Ÿæç¤ºï¼Œåˆ™å¯é€‰ï¼‰å’Œè¾¹ç¼˜æ£€æµ‹å›¾åƒä¼ é€’ç»™ç®¡é“ï¼š

[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)å‚æ•°ç¡®å®šåˆ†é…ç»™æ¡ä»¶è¾“å…¥çš„æƒé‡ã€‚å»ºè®®å°†å€¼è®¾ä¸º0.5ä»¥è·å¾—è‰¯å¥½çš„æ³›åŒ–æ•ˆæœï¼Œä½†è¯·éšæ„å°è¯•è¿™ä¸ªæ•°å­—ï¼

```py
prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = 'low quality, bad quality, sketches'

image = pipe(
    prompt,
    negative_prompt=negative_prompt,
    image=canny_image,
    controlnet_conditioning_scale=0.5,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

![](../Images/033f4d98632798d409eabe8ebfffc8db.png)

æ‚¨è¿˜å¯ä»¥é€šè¿‡å°†å‚æ•°è®¾ç½®ä¸º`True`ï¼Œåœ¨çŒœæµ‹æ¨¡å¼ä¸‹ä½¿ç”¨[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ï¼š

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
import cv2
from PIL import Image

prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = "low quality, bad quality, sketches"

original_image = load_image(
    "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.enable_model_cpu_offload()

image = np.array(original_image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe(
    prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

## MultiControlNet

å°†SDXLæ¨¡å‹æ›¿æ¢ä¸ºåƒ[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)è¿™æ ·çš„æ¨¡å‹ï¼Œä»¥ä½¿ç”¨å¤šä¸ªæ¡ä»¶è¾“å…¥ä¸Stable Diffusionæ¨¡å‹ã€‚

æ‚¨å¯ä»¥ä»ä¸åŒçš„å›¾åƒè¾“å…¥ä¸­ç»„åˆå¤šä¸ªControlNetæ¡ä»¶ï¼Œä»¥åˆ›å»º*MultiControlNet*ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œé€šå¸¸æœ‰å¸®åŠ©çš„æ˜¯ï¼š

1.  å¯¹æ¡ä»¶è¿›è¡Œé®ç½©å¤„ç†ï¼Œä½¿å…¶ä¸é‡å ï¼ˆä¾‹å¦‚ï¼Œé®ç½©å§¿åŠ¿æ¡ä»¶æ‰€åœ¨çš„è¾¹ç¼˜æ£€æµ‹å›¾åƒåŒºåŸŸï¼‰

1.  å°è¯•[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)å‚æ•°ï¼Œä»¥ç¡®å®šåˆ†é…ç»™æ¯ä¸ªæ¡ä»¶è¾“å…¥çš„æƒé‡

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†ç»“åˆè¾¹ç¼˜æ£€æµ‹å›¾åƒå’Œäººä½“å§¿åŠ¿ä¼°è®¡å›¾åƒæ¥ç”Ÿæˆä¸€å¹…æ–°å›¾åƒã€‚

å‡†å¤‡è¾¹ç¼˜æ£€æµ‹å›¾åƒæ¡ä»¶ï¼š

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import numpy as np
import cv2

original_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
)
image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)

# zero out middle columns of image where pose will be overlaid
zero_start = image.shape[1] // 4
zero_end = zero_start + image.shape[1] // 2
image[:, zero_start:zero_end] = 0

image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

![](../Images/35bd0f11e1c7ca277ac60a85a75be06a.png)

åŸå§‹å›¾åƒ

![](../Images/5a37576bad87523fdb8a31aa840b4fdc.png)

è¾¹ç¼˜æ£€æµ‹å›¾åƒ

å¯¹äºäººä½“å§¿åŠ¿ä¼°è®¡ï¼Œå®‰è£…[controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)ï¼š

```py
# uncomment to install the necessary library in Colab
#!pip install -q controlnet-aux
```

å‡†å¤‡äººä½“å§¿åŠ¿ä¼°è®¡æ¡ä»¶ï¼š

```py
from controlnet_aux import OpenposeDetector

openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
original_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"
)
openpose_image = openpose(original_image)
make_image_grid([original_image, openpose_image], rows=1, cols=2)
```

![](../Images/77e2344bf3a8143f50f20ca5d095bf81.png)

åŸå§‹å›¾åƒ

![](../Images/5a8432160ffe7c6268d59c8c8d536c20.png)

äººä½“å§¿åŠ¿å›¾åƒ

åŠ è½½ä¸æ¯ä¸ªæ¡ä»¶å¯¹åº”çš„ControlNetæ¨¡å‹åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬ä¼ é€’ç»™[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ã€‚ä½¿ç”¨æ›´å¿«çš„[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler
import torch

controlnets = [
    ControlNetModel.from_pretrained(
        "thibaud/controlnet-openpose-sdxl-1.0", torch_dtype=torch.float16
    ),
    ControlNetModel.from_pretrained(
        "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
    ),
]

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ç°åœ¨æ‚¨å¯ä»¥å°†æ‚¨çš„æç¤ºï¼ˆå¦‚æœæ‚¨ä½¿ç”¨è´Ÿæç¤ºï¼Œåˆ™å¯é€‰ï¼‰ã€è¾¹ç¼˜æ£€æµ‹å›¾åƒå’Œå§¿åŠ¿å›¾åƒä¼ é€’ç»™ç®¡é“ï¼š

```py
prompt = "a giant standing in a fantasy landscape, best quality"
negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

generator = torch.manual_seed(1)

images = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]

images = pipe(
    prompt,
    image=images,
    num_inference_steps=25,
    generator=generator,
    negative_prompt=negative_prompt,
    num_images_per_prompt=3,
    controlnet_conditioning_scale=[1.0, 0.8],
).images
make_image_grid([original_image, canny_image, openpose_image,
                images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)
```

![](../Images/780255e1bc5d49726b432882894f0b81.png)
