# T2I-Adapter

> 原始文本：[https://huggingface.co/docs/diffusers/training/t2i_adapters](https://huggingface.co/docs/diffusers/training/t2i_adapters)

[T2I-Adapter](https://hf.co/papers/2302.08453) 是一个轻量级的适配器模型，提供额外的条件输入图像（线条艺术、canny、素描、深度、姿势）以更好地控制图像生成。它类似于 ControlNet，但更小（~77M 参数和 ~300MB 文件大小），因为它只向 UNet 插入权重，而不是复制和训练它。

T2I-Adapter 仅适用于使用 Stable Diffusion XL (SDXL) 模型进行训练。

本指南将探讨 [train_t2i_adapter_sdxl.py](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py) 训练脚本，帮助您熟悉它，以及如何为自己的用例进行适应。

在运行脚本之前，请确保从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

然后导航到包含训练脚本的示例文件夹，并安装您正在使用的脚本所需的依赖项：

```py
cd examples/t2i_adapter
pip install -r requirements.txt
```

🤗 Accelerate 是一个帮助您在多个 GPU/TPU 或混合精度下训练的库。它会根据您的硬件和环境自动配置您的训练设置。查看 🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour) 以了解更多。

初始化一个 🤗 Accelerate 环境：

```py
accelerate config
```

要设置一个默认的 🤗 Accelerate 环境而不选择任何配置：

```py
accelerate config default
```

或者如果您的环境不支持交互式 shell，比如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看 [创建用于训练的数据集](create_dataset) 指南，了解如何创建适用于训练脚本的数据集。

以下部分突出显示了训练脚本中重要的部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读 [脚本](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py)，并告诉我们您是否有任何问题或疑虑。

## 脚本参数

训练脚本提供了许多参数，帮助您自定义训练运行。所有参数及其描述都可以在 [`parse_args()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L233) 函数中找到。它为每个参数提供了默认值，如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。

例如，要激活梯度累积，将 `--gradient_accumulation_steps` 参数添加到训练命令中：

```py
accelerate launch train_t2i_adapter_sdxl.py \
  ----gradient_accumulation_steps=4
```

许多基本和重要的参数在 [Text-to-image](text2image#script-parameters) 训练指南中有描述，因此本指南只关注相关的 T2I-Adapter 参数：

+   `--pretrained_vae_model_name_or_path`：预训练 VAE 的路径；SDXL VAE 已知存在数值不稳定性问题，因此此参数允许您指定更好的 [VAE](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)

+   `--crops_coords_top_left_h` 和 `--crops_coords_top_left_w`：要包含在 SDXL 的裁剪坐标嵌入中的高度和宽度坐标

+   `--conditioning_image_column`：数据集中条件图像的列

+   `--proportion_empty_prompts`：要替换为空字符串的图像提示的比例

## 训练脚本

与脚本参数一样，[Text-to-image](text2image#training-script) 训练指南中提供了训练脚本的详细说明。相反，本指南将查看脚本中与 T2I-Adapter 相关的部分。

训练脚本从准备数据集开始。这包括[tokenizing](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L674)提示和[应用变换](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L714)到图像和调节图像。

```py
conditioning_image_transforms = transforms.Compose(
    [
        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.CenterCrop(args.resolution),
        transforms.ToTensor(),
    ]
)
```

在[`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L770)函数中，T2I-Adapter要么从预训练的适配器加载，要么随机初始化：

```py
if args.adapter_model_name_or_path:
    logger.info("Loading existing adapter weights.")
    t2iadapter = T2IAdapter.from_pretrained(args.adapter_model_name_or_path)
else:
    logger.info("Initializing t2iadapter weights.")
    t2iadapter = T2IAdapter(
        in_channels=3,
        channels=(320, 640, 1280, 1280),
        num_res_blocks=2,
        downscale_factor=16,
        adapter_type="full_adapter_xl",
    )
```

[optimizer](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L952)已初始化为T2I-Adapter参数：

```py
params_to_optimize = t2iadapter.parameters()
optimizer = optimizer_class(
    params_to_optimize,
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

最后，在[training loop](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L1086)中，适配器调节图像和文本嵌入被传递给UNet以预测噪声残差：

```py
t2iadapter_image = batch["conditioning_pixel_values"].to(dtype=weight_dtype)
down_block_additional_residuals = t2iadapter(t2iadapter_image)
down_block_additional_residuals = [
    sample.to(dtype=weight_dtype) for sample in down_block_additional_residuals
]

model_pred = unet(
    inp_noisy_latents,
    timesteps,
    encoder_hidden_states=batch["prompt_ids"],
    added_cond_kwargs=batch["unet_added_conditions"],
    down_block_additional_residuals=down_block_additional_residuals,
).sample
```

如果您想了解训练循环的工作原理，请查看[Understanding pipelines, models and schedulers](../using-diffusers/write_own_pipeline)教程，该教程解析了去噪过程的基本模式。

## 启动脚本

现在您已经准备好启动训练脚本了！🚀

对于这个示例训练，您将使用[fusing/fill50k](https://huggingface.co/datasets/fusing/fill50k)数据集。如果您愿意，您也可以创建并使用自己的数据集（请参阅[Create a dataset for training](https://moon-ci-docs.huggingface.co/docs/diffusers/pr_5512/en/training/create_dataset)指南）。

将环境变量`MODEL_DIR`设置为Hub上的模型ID或本地模型的路径，将`OUTPUT_DIR`设置为您想要保存模型的位置。

下载以下图像以调节您的训练：

```py
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png
```

使用Weights & Biases监控训练进度，将`--report_to=wandb`参数添加到训练命令中。您还需要将`--validation_image`，`--validation_prompt`和`--validation_steps`添加到训练命令中以跟踪结果。这对于调试模型和查看中间结果非常有用。

```py
export MODEL_DIR="stabilityai/stable-diffusion-xl-base-1.0"
export OUTPUT_DIR="path to save model"

accelerate launch train_t2i_adapter_sdxl.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --mixed_precision="fp16" \
 --resolution=1024 \
 --learning_rate=1e-5 \
 --max_train_steps=15000 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --validation_steps=100 \
 --train_batch_size=1 \
 --gradient_accumulation_steps=4 \
 --report_to="wandb" \
 --seed=42 \
 --push_to_hub
```

训练完成后，您可以使用您的T2I-Adapter进行推断。

```py
from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest
from diffusers.utils import load_image
import torch

adapter = T2IAdapter.from_pretrained("path/to/adapter", torch_dtype=torch.float16)
pipeline = StableDiffusionXLAdapterPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", adapter=adapter, torch_dtype=torch.float16
)

pipeline.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)
pipeline.enable_xformers_memory_efficient_attention()
pipeline.enable_model_cpu_offload()

control_image = load_image("./conditioning_image_1.png")
prompt = "pale golden rod circle with old lace background"

generator = torch.manual_seed(0)
image = pipeline(
    prompt, image=control_image, generator=generator
).images[0]
image.save("./output.png")
```

## 下一步

恭喜您训练了一个T2I-Adapter模型！🎉 要了解更多：

+   阅读[Efficient Controllable Generation for SDXL with T2I-Adapters](https://huggingface.co/blog/t2i-sdxl-adapters)博文，了解T2I-Adapter团队的实验结果的更多细节。
