- en: Data Collator
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集器
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Data collators are objects that will form a batch by using a list of dataset
    elements as input. These elements are of the same type as the elements of `train_dataset`
    or `eval_dataset`.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集器是通过使用数据集元素列表作为输入来形成批次的对象。这些元素与 `train_dataset` 或 `eval_dataset` 的元素类型相同。
- en: To be able to build batches, data collators may apply some processing (like
    padding). Some of them (like [DataCollatorForLanguageModeling](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling))
    also apply some random data augmentation (like random masking) on the formed batch.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够构建批次，数据收集器可能会应用一些处理（如填充）。其中一些（如 [DataCollatorForLanguageModeling](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)）还会对形成的批次应用一些随机数据增强（如随机掩码）。
- en: Examples of use can be found in the [example scripts](../examples) or [example
    notebooks](../notebooks).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用示例可在[示例脚本](../examples)或[示例笔记本](../notebooks)中找到。
- en: Default data collator
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 默认数据收集器
- en: '#### `transformers.default_data_collator`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.default_data_collator`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L74)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L74)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Very simple data collator that simply collates batches of dict-like objects
    and performs special handling for potential keys named:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单的数据收集器，只是将类似字典的对象的批次汇集在一起，并对名为潜在键执行特殊处理：
- en: '`label`: handles a single value (int or float) per object'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`: 处理每个对象的单个值（int 或 float）'
- en: '`label_ids`: handles a list of values per object'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids`: 处理每个对象的值列表'
- en: 'Does not do any additional preprocessing: property names of the input object
    will be used as corresponding inputs to the model. See glue and ner for example
    of how it’s useful.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不执行任何额外的预处理：输入对象的属性名称将用作模型的相应输入。查看 glue 和 ner 的示例，了解它的用途。
- en: DefaultDataCollator
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DefaultDataCollator
- en: '### `class transformers.DefaultDataCollator`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DefaultDataCollator`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L99)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L99)'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`，*可选*，默认为 `"pt"`）— 要返回的张量类型。允许的值为 “np”、“pt” 和 “tf”。'
- en: 'Very simple data collator that simply collates batches of dict-like objects
    and performs special handling for potential keys named:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单的数据收集器，只是将类似字典的对象的批次汇集在一起，并对名为潜在键执行特殊处理：
- en: '`label`: handles a single value (int or float) per object'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`: 处理每个对象的单个值（int 或 float）'
- en: '`label_ids`: handles a list of values per object'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids`: 处理每个对象的值列表'
- en: 'Does not do any additional preprocessing: property names of the input object
    will be used as corresponding inputs to the model. See glue and ner for example
    of how it’s useful.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不执行任何额外的预处理：输入对象的属性名称将用作模型的相应输入。查看 glue 和 ner 的示例，了解它的用途。
- en: This is an object (like other data collators) rather than a pure function like
    default_data_collator. This can be helpful if you need to set a return_tensors
    value at initialization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个对象（像其他数据收集器一样），而不是像 default_data_collator 那样的纯函数。如果需要在初始化时设置 return_tensors
    值，这可能会有所帮助。
- en: DataCollatorWithPadding
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataCollatorWithPadding
- en: '### `class transformers.DataCollatorWithPadding`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DataCollatorWithPadding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L236)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L236)'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    或 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）—
    用于对数据进行编码的分词器。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为
    `True`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：'
- en: '`True` or `''longest''` (default): Pad to the longest sequence in the batch
    (or no padding if only a single sequence is provided).'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`（默认）：填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度（使用参数 `max_length`）或填充到模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''`: No padding (i.e., can output a batch with sequences
    of different lengths).'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`：不填充（即，可以输出具有不同长度序列的批次）。'
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 返回列表的最大长度，可选填充长度（见上文）。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability >= 7.5 (Volta).
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用具有计算能力 >= 7.5（Volta）的 NVIDIA 硬件上的 Tensor Cores 的使用特别有用。
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`，*可选*，默认为 `"pt"`）— 要返回的张量类型。允许的值为 “np”、“pt” 和 “tf”。'
- en: Data collator that will dynamically pad the inputs received.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集器，将动态填充接收到的输入。
- en: DataCollatorForTokenClassification
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataCollatorForTokenClassification
- en: '### `class transformers.DataCollatorForTokenClassification`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DataCollatorForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L288)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L288)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）—
    用于对数据进行编码的分词器。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`）—
    选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：'
- en: '`True` or `''longest''` (default): Pad to the longest sequence in the batch
    (or no padding if only a single sequence is provided).'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest''`（默认）：填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''`: No padding (i.e., can output a batch with sequences
    of different lengths).'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_pad''`：无填充（即，可以输出具有不同长度序列的批次）。'
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability >= 7.5 (Volta).
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用具有计算能力>= 7.5（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。
- en: '`label_pad_token_id` (`int`, *optional*, defaults to -100) — The id to use
    when padding the labels (-100 will be automatically ignore by PyTorch loss functions).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_pad_token_id`（`int`，*可选*，默认为-100）— 填充标签时要使用的 id（-100 将被 PyTorch 损失函数自动忽略）。'
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`，*可选*，默认为`"pt"`）— 要返回的 Tensor 类型。允许的值为“np”，“pt”和“tf”。'
- en: Data collator that will dynamically pad the inputs received, as well as the
    labels.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集器，将动态填充接收到的输入以及标签。
- en: DataCollatorForSeq2Seq
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataCollatorForSeq2Seq
- en: '### `class transformers.DataCollatorForSeq2Seq`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DataCollatorForSeq2Seq`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L542)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L542)'
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）—
    用于对数据进行编码的分词器。'
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    *optional*) — The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*,
    use it to prepare the *decoder_input_ids*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)，*可选*）—
    正在训练的模型。如果设置并且具有*prepare_decoder_input_ids_from_labels*，则使用它来准备*decoder_input_ids*'
- en: This is useful when using *label_smoothing* to avoid calculating loss twice.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这在使用*label_smoothing*时很有用，以避免计算损失两次。
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`）—
    选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：'
- en: '`True` or `''longest''` (default): Pad to the longest sequence in the batch
    (or no padding if only a single sequence is provided).'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest''`（默认）：填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''`: No padding (i.e., can output a batch with sequences
    of different lengths).'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_pad''`：无填充（即，可以输出具有不同长度序列的批次）。'
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability >= 7.5 (Volta).
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用具有计算能力>= 7.5（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。
- en: '`label_pad_token_id` (`int`, *optional*, defaults to -100) — The id to use
    when padding the labels (-100 will be automatically ignored by PyTorch loss functions).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_pad_token_id`（`int`，*可选*，默认为-100）— 填充标签时要使用的 id（-100 将被 PyTorch 损失函数自动忽略）。'
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`, *optional*, 默认为`"pt"`) — 要返回的张量类型。允许的值为“np”，“pt”和“tf”。'
- en: Data collator that will dynamically pad the inputs received, as well as the
    labels.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集器将动态填充接收到的输入和标签。
- en: DataCollatorForLanguageModeling
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataCollatorForLanguageModeling
- en: '### `class transformers.DataCollatorForLanguageModeling`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DataCollatorForLanguageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L633)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L633)'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）
    — 用于对数据进行编码的分词器。'
- en: '`mlm` (`bool`, *optional*, defaults to `True`) — Whether or not to use masked
    language modeling. If set to `False`, the labels are the same as the inputs with
    the padding tokens ignored (by setting them to -100). Otherwise, the labels are
    -100 for non-masked tokens and the value to predict for the masked token.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlm` (`bool`, *optional*, 默认为 `True`) — 是否使用遮罩语言建模。如果设置为 `False`，则标签与输入相同，忽略填充标记（通过将它们设置为-100）。否则，对于未遮罩的标记，标签为-100，对于遮罩的标记，值为要预测的值。'
- en: '`mlm_probability` (`float`, *optional*, defaults to 0.15) — The probability
    with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlm_probability` (`float`, *optional*, 默认为0.15) — 当`mlm`设置为`True`时，在输入中（随机）遮罩标记的概率。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) — 如果设置，将序列填充到提供的值的倍数。'
- en: '`return_tensors` (`str`) — The type of Tensor to return. Allowable values are
    “np”, “pt” and “tf”.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`) — 要返回的张量类型。允许的值为“np”，“pt”和“tf”。'
- en: Data collator used for language modeling. Inputs are dynamically padded to the
    maximum length of a batch if they are not all of the same length.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 用于语言建模的数据收集器。如果它们的长度不相同，则输入将动态填充到批次的最大长度。
- en: For best performance, this data collator should be used with a dataset having
    items that are dictionaries or BatchEncoding, with the `"special_tokens_mask"`
    key, as returned by a [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or a [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    with the argument `return_special_tokens_mask=True`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，应该将此数据收集器与具有字典或BatchEncoding项目的数据集一起使用，其中包含`"special_tokens_mask"`键，如由[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)返回的，参数`return_special_tokens_mask=True`。
- en: '#### `numpy_mask_tokens`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `numpy_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L839)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L839)'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。
- en: '#### `tf_mask_tokens`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tf_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L686)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L686)'
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。
- en: '#### `torch_mask_tokens`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `torch_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L782)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L782)'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。
- en: DataCollatorForWholeWordMask
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataCollatorForWholeWordMask
- en: '### `class transformers.DataCollatorForWholeWordMask`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DataCollatorForWholeWordMask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L877)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L877)'
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Data collator used for language modeling that masks entire words.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 用于遮罩整个单词的语言建模数据收集器。
- en: collates batches of tensors, honoring their tokenizer’s pad_token
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整理张量批次，尊重它们的分词器的pad_token
- en: preprocesses batches for masked language modeling
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为遮罩语言建模预处理批次
- en: This collator relies on details of the implementation of subword tokenization
    by [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer),
    specifically that subword tokens are prefixed with *##*. For tokenizers that do
    not adhere to this scheme, this collator will produce an output that is roughly
    equivalent to `.DataCollatorForLanguageModeling`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据收集器依赖于[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)对子词分词的实现细节，特别是子词标记以*##*为前缀。对于不遵循此方案的分词器，此数据收集器将产生一个大致等同于`.DataCollatorForLanguageModeling`的输出。
- en: '#### `numpy_mask_tokens`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `numpy_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1108)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1108)'
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original. Set ‘mask_labels’ means we use whole word mask (wwm),
    we directly mask idxs according to it’s ref.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。设置'mask_labels'表示我们使用整词遮罩（wwm），我们根据其引用直接遮罩idxs。
- en: '#### `tf_mask_tokens`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tf_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1066)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1066)'
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original. Set ‘mask_labels’ means we use whole word mask (wwm),
    we directly mask idxs according to it’s ref.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为掩码语言建模准备掩码标记输入/标签：80%掩码，10%随机，10%原始。设置'mask_labels'表示我们使用整词掩码（wwm），我们根据其引用直接掩码idxs。
- en: '#### `torch_mask_tokens`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `torch_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1026)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1026)'
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original. Set ‘mask_labels’ means we use whole word mask (wwm),
    we directly mask idxs according to it’s ref.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为掩码语言建模准备掩码标记输入/标签：80%掩码，10%随机，10%原始。设置'mask_labels'表示我们使用整词掩码（wwm），我们根据其引用直接掩码idxs。
- en: DataCollatorForPermutationLanguageModeling
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataCollatorForPermutationLanguageModeling
- en: '### `class transformers.DataCollatorForPermutationLanguageModeling`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DataCollatorForPermutationLanguageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1232)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1232)'
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Data collator used for permutation language modeling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 用于排列语言建模的数据收集器。
- en: collates batches of tensors, honoring their tokenizer’s pad_token
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整理张量批次，尊重其分词器的pad_token
- en: preprocesses batches for permutation language modeling with procedures specific
    to XLNet
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定于XLNet的程序对排列语言建模进行预处理
- en: '#### `numpy_mask_tokens`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `numpy_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1473)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1473)'
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The masked tokens to be predicted for a particular sequence are determined
    by the following algorithm:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 特定序列要预测的掩码标记由以下算法确定：
- en: Start from the beginning of the sequence by setting `cur_len = 0` (number of
    tokens processed so far).
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从序列开头开始，通过设置`cur_len = 0`（到目前为止已处理的标记数）。
- en: Sample a `span_length` from the interval `[1, max_span_length]` (length of span
    of tokens to be masked)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从区间`[1, max_span_length]`中采样`span_length`（要掩码的标记跨度的长度）
- en: Reserve a context of length `context_length = span_length / plm_probability`
    to surround span to be masked
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留长度为`context_length = span_length / plm_probability`的上下文以包围要掩码的跨度
- en: Sample a starting point `start_index` from the interval `[cur_len, cur_len +
    context_length - span_length]` and mask tokens `start_index:start_index + span_length`
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从区间`[cur_len, cur_len + context_length - span_length]`中采样起始点`start_index`并掩码标记`start_index:start_index
    + span_length`
- en: Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there
    are tokens remaining in the sequence to be processed), repeat from Step 1.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`cur_len = cur_len + context_length`。如果`cur_len < max_len`（即序列中仍有剩余标记要处理），则从步骤1重复。
- en: '#### `tf_mask_tokens`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tf_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1366)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1366)'
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The masked tokens to be predicted for a particular sequence are determined
    by the following algorithm:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 特定序列要预测的掩码标记由以下算法确定：
- en: Start from the beginning of the sequence by setting `cur_len = 0` (number of
    tokens processed so far).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从序列开头开始，通过设置`cur_len = 0`（到目前为止已处理的标记数）。
- en: Sample a `span_length` from the interval `[1, max_span_length]` (length of span
    of tokens to be masked)
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从区间`[1, max_span_length]`中采样`span_length`（要掩码的标记跨度的长度）
- en: Reserve a context of length `context_length = span_length / plm_probability`
    to surround span to be masked
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留长度为`context_length = span_length / plm_probability`的上下文以包围要掩码的跨度
- en: Sample a starting point `start_index` from the interval `[cur_len, cur_len +
    context_length - span_length]` and mask tokens `start_index:start_index + span_length`
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从区间`[cur_len, cur_len + context_length - span_length]`中采样起始点`start_index`并掩码标记`start_index:start_index
    + span_length`
- en: Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there
    are tokens remaining in the sequence to be processed), repeat from Step 1.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`cur_len = cur_len + context_length`。如果`cur_len < max_len`（即序列中仍有剩余标记要处理），则从步骤1重复。
- en: '#### `torch_mask_tokens`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `torch_mask_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1267)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1267)'
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The masked tokens to be predicted for a particular sequence are determined
    by the following algorithm:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 特定序列要预测的掩码标记由以下算法确定：
- en: Start from the beginning of the sequence by setting `cur_len = 0` (number of
    tokens processed so far).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从序列开头开始，通过设置`cur_len = 0`（到目前为止已处理的标记数）。
- en: Sample a `span_length` from the interval `[1, max_span_length]` (length of span
    of tokens to be masked)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从区间`[1, max_span_length]`中采样`span_length`（要掩码的标记跨度的长度）
- en: Reserve a context of length `context_length = span_length / plm_probability`
    to surround span to be masked
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留长度为`context_length = span_length / plm_probability`的上下文以包围要掩码的跨度
- en: Sample a starting point `start_index` from the interval `[cur_len, cur_len +
    context_length - span_length]` and mask tokens `start_index:start_index + span_length`
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从区间`[cur_len, cur_len + context_length - span_length]`中采样起始点`start_index`并掩码标记`start_index:start_index
    + span_length`
- en: Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there
    are tokens remaining in the sequence to be processed), repeat from Step 1.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`cur_len = cur_len + context_length`。如果`cur_len < max_len`（即序列中仍有剩余标记要处理），则从步骤1重复。
