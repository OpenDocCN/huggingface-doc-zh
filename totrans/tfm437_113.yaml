- en: Data Collator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Data collators are objects that will form a batch by using a list of dataset
    elements as input. These elements are of the same type as the elements of `train_dataset`
    or `eval_dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to build batches, data collators may apply some processing (like
    padding). Some of them (like [DataCollatorForLanguageModeling](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling))
    also apply some random data augmentation (like random masking) on the formed batch.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of use can be found in the [example scripts](../examples) or [example
    notebooks](../notebooks).
  prefs: []
  type: TYPE_NORMAL
- en: Default data collator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `transformers.default_data_collator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L74)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Very simple data collator that simply collates batches of dict-like objects
    and performs special handling for potential keys named:'
  prefs: []
  type: TYPE_NORMAL
- en: '`label`: handles a single value (int or float) per object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_ids`: handles a list of values per object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Does not do any additional preprocessing: property names of the input object
    will be used as corresponding inputs to the model. See glue and ner for example
    of how it’s useful.'
  prefs: []
  type: TYPE_NORMAL
- en: DefaultDataCollator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DefaultDataCollator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L99)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Very simple data collator that simply collates batches of dict-like objects
    and performs special handling for potential keys named:'
  prefs: []
  type: TYPE_NORMAL
- en: '`label`: handles a single value (int or float) per object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_ids`: handles a list of values per object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Does not do any additional preprocessing: property names of the input object
    will be used as corresponding inputs to the model. See glue and ner for example
    of how it’s useful.'
  prefs: []
  type: TYPE_NORMAL
- en: This is an object (like other data collators) rather than a pure function like
    default_data_collator. This can be helpful if you need to set a return_tensors
    value at initialization.
  prefs: []
  type: TYPE_NORMAL
- en: DataCollatorWithPadding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DataCollatorWithPadding`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L236)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''` (default): Pad to the longest sequence in the batch
    (or no padding if only a single sequence is provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''`: No padding (i.e., can output a batch with sequences
    of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability >= 7.5 (Volta).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collator that will dynamically pad the inputs received.
  prefs: []
  type: TYPE_NORMAL
- en: DataCollatorForTokenClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DataCollatorForTokenClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L288)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''` (default): Pad to the longest sequence in the batch
    (or no padding if only a single sequence is provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''`: No padding (i.e., can output a batch with sequences
    of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability >= 7.5 (Volta).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`label_pad_token_id` (`int`, *optional*, defaults to -100) — The id to use
    when padding the labels (-100 will be automatically ignore by PyTorch loss functions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collator that will dynamically pad the inputs received, as well as the
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: DataCollatorForSeq2Seq
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DataCollatorForSeq2Seq`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L542)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    *optional*) — The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*,
    use it to prepare the *decoder_input_ids*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful when using *label_smoothing* to avoid calculating loss twice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''` (default): Pad to the longest sequence in the batch
    (or no padding if only a single sequence is provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''`: No padding (i.e., can output a batch with sequences
    of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability >= 7.5 (Volta).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`label_pad_token_id` (`int`, *optional*, defaults to -100) — The id to use
    when padding the labels (-100 will be automatically ignored by PyTorch loss functions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — The type of Tensor
    to return. Allowable values are “np”, “pt” and “tf”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collator that will dynamically pad the inputs received, as well as the
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: DataCollatorForLanguageModeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DataCollatorForLanguageModeling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L633)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast))
    — The tokenizer used for encoding the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlm` (`bool`, *optional*, defaults to `True`) — Whether or not to use masked
    language modeling. If set to `False`, the labels are the same as the inputs with
    the padding tokens ignored (by setting them to -100). Otherwise, the labels are
    -100 for non-masked tokens and the value to predict for the masked token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlm_probability` (`float`, *optional*, defaults to 0.15) — The probability
    with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`) — The type of Tensor to return. Allowable values are
    “np”, “pt” and “tf”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collator used for language modeling. Inputs are dynamically padded to the
    maximum length of a batch if they are not all of the same length.
  prefs: []
  type: TYPE_NORMAL
- en: For best performance, this data collator should be used with a dataset having
    items that are dictionaries or BatchEncoding, with the `"special_tokens_mask"`
    key, as returned by a [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    or a [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    with the argument `return_special_tokens_mask=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `numpy_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L839)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `tf_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L686)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `torch_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L782)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original.'
  prefs: []
  type: TYPE_NORMAL
- en: DataCollatorForWholeWordMask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DataCollatorForWholeWordMask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L877)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data collator used for language modeling that masks entire words.
  prefs: []
  type: TYPE_NORMAL
- en: collates batches of tensors, honoring their tokenizer’s pad_token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: preprocesses batches for masked language modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This collator relies on details of the implementation of subword tokenization
    by [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer),
    specifically that subword tokens are prefixed with *##*. For tokenizers that do
    not adhere to this scheme, this collator will produce an output that is roughly
    equivalent to `.DataCollatorForLanguageModeling`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `numpy_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original. Set ‘mask_labels’ means we use whole word mask (wwm),
    we directly mask idxs according to it’s ref.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `tf_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1066)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original. Set ‘mask_labels’ means we use whole word mask (wwm),
    we directly mask idxs according to it’s ref.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `torch_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1026)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare masked tokens inputs/labels for masked language modeling: 80% MASK,
    10% random, 10% original. Set ‘mask_labels’ means we use whole word mask (wwm),
    we directly mask idxs according to it’s ref.'
  prefs: []
  type: TYPE_NORMAL
- en: DataCollatorForPermutationLanguageModeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DataCollatorForPermutationLanguageModeling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1232)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Data collator used for permutation language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: collates batches of tensors, honoring their tokenizer’s pad_token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: preprocesses batches for permutation language modeling with procedures specific
    to XLNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `numpy_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1473)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The masked tokens to be predicted for a particular sequence are determined
    by the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Start from the beginning of the sequence by setting `cur_len = 0` (number of
    tokens processed so far).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a `span_length` from the interval `[1, max_span_length]` (length of span
    of tokens to be masked)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reserve a context of length `context_length = span_length / plm_probability`
    to surround span to be masked
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a starting point `start_index` from the interval `[cur_len, cur_len +
    context_length - span_length]` and mask tokens `start_index:start_index + span_length`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there
    are tokens remaining in the sequence to be processed), repeat from Step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### `tf_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1366)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The masked tokens to be predicted for a particular sequence are determined
    by the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Start from the beginning of the sequence by setting `cur_len = 0` (number of
    tokens processed so far).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a `span_length` from the interval `[1, max_span_length]` (length of span
    of tokens to be masked)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reserve a context of length `context_length = span_length / plm_probability`
    to surround span to be masked
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a starting point `start_index` from the interval `[cur_len, cur_len +
    context_length - span_length]` and mask tokens `start_index:start_index + span_length`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there
    are tokens remaining in the sequence to be processed), repeat from Step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### `torch_mask_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1267)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The masked tokens to be predicted for a particular sequence are determined
    by the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Start from the beginning of the sequence by setting `cur_len = 0` (number of
    tokens processed so far).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a `span_length` from the interval `[1, max_span_length]` (length of span
    of tokens to be masked)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reserve a context of length `context_length = span_length / plm_probability`
    to surround span to be masked
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a starting point `start_index` from the interval `[cur_len, cur_len +
    context_length - span_length]` and mask tokens `start_index:start_index + span_length`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there
    are tokens remaining in the sequence to be processed), repeat from Step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
