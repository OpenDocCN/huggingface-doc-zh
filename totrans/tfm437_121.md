# 管道

> 原文链接: [`huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines)

管道是使用模型进行推断的一种很好且简单的方式。这些管道是抽象出库中大部分复杂代码的对象，提供了专门用于多个任务的简单 API，包括命名实体识别、掩码语言建模、情感分析、特征提取和问答。查看任务摘要以获取使用示例。

有两种要注意的管道抽象类别：

+   pipeline() 是封装所有其他管道的最强大对象。

+   针对音频、计算机视觉、自然语言处理和多模态任务提供了特定任务的管道。

## 管道抽象

*pipeline* 抽象是围绕所有其他可用管道的包装器。它像任何其他管道一样实例化，但可以提供额外的生活质量。

简单调用一个项目：

```py
>>> pipe = pipeline("text-classification")
>>> pipe("This restaurant is awesome")
[{'label': 'POSITIVE', 'score': 0.9998743534088135}]
```

如果要使用来自[hub](https://huggingface.co)的特定模型，可以忽略任务，如果 hub 上的模型已经定义了它：

```py
>>> pipe = pipeline(model="roberta-large-mnli")
>>> pipe("This restaurant is awesome")
[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]
```

要在多个项目上调用管道，可以使用*列表*调用它。

```py
>>> pipe = pipeline("text-classification")
>>> pipe(["This restaurant is awesome", "This restaurant is awful"])
[{'label': 'POSITIVE', 'score': 0.9998743534088135},
 {'label': 'NEGATIVE', 'score': 0.9996669292449951}]
```

要遍历完整数据集，建议直接使用`dataset`。这意味着您不需要一次性分配整个数据集，也不需要自己进行批处理。这应该与 GPU 上的自定义循环一样快。如果不是，请不要犹豫创建一个问题。

```py
import datasets
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

pipe = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
dataset = datasets.load_dataset("superb", name="asr", split="test")

# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item
# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset
for out in tqdm(pipe(KeyDataset(dataset, "file"))):
    print(out)
    # {"text": "NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND"}
    # {"text": ....}
    # ....
```

为了方便使用，也可以使用生成器：

```py
from transformers import pipeline

pipe = pipeline("text-classification")

def data():
    while True:
        # This could come from a dataset, a database, a queue or HTTP request
        # in a server
        # Caveat: because this is iterative, you cannot use `num_workers > 1` variable
        # to use multiple threads to preprocess data. You can still have 1 thread that
        # does the preprocessing while the main runs the big inference
        yield "This is a test"

for out in pipe(data()):
    print(out)
    # {"text": "NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND"}
    # {"text": ....}
    # ....
```

#### `transformers.pipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/__init__.py#L531)

```py
( task: str = None model: Union = None config: Union = None tokenizer: Union = None feature_extractor: Union = None image_processor: Union = None framework: Optional = None revision: Optional = None use_fast: bool = True token: Union = None device: Union = None device_map = None torch_dtype = None trust_remote_code: Optional = None model_kwargs: Dict = None pipeline_class: Optional = None **kwargs ) → export const metadata = 'undefined';Pipeline
```

参数

+   `task` (`str`) — 定义将返回哪个管道的任务。当前接受的任务有：

    +   `"audio-classification"`: 将返回一个 AudioClassificationPipeline。

    +   `"automatic-speech-recognition"`: 将返回一个 AutomaticSpeechRecognitionPipeline。

    +   `"conversational"`: 将返回一个 ConversationalPipeline。

    +   `"depth-estimation"`: 将返回一个 DepthEstimationPipeline。

    +   `"document-question-answering"`: 将返回一个 DocumentQuestionAnsweringPipeline。

    +   `"feature-extraction"`: 将返回一个 FeatureExtractionPipeline。

    +   `"fill-mask"`: 将返回一个 FillMaskPipeline。

    +   `"image-classification"`: 将返回一个 ImageClassificationPipeline。

    +   `"image-segmentation"`: 将返回一个 ImageSegmentationPipeline。

    +   `"image-to-image"`: 将返回一个 ImageToImagePipeline。

    +   `"image-to-text"`: 将返回一个 ImageToTextPipeline。

    +   `"mask-generation"`: 将返回一个 MaskGenerationPipeline。

    +   `"object-detection"`：将返回一个 ObjectDetectionPipeline。

    +   `"question-answering"`：将返回一个 QuestionAnsweringPipeline。

    +   `"summarization"`：将返回一个 SummarizationPipeline。

    +   `"table-question-answering"`：将返回一个 TableQuestionAnsweringPipeline。

    +   `"text2text-generation"`：将返回一个 Text2TextGenerationPipeline。

    +   `"text-classification"`（别名`"sentiment-analysis"`可用）：将返回一个 TextClassificationPipeline。

    +   `"text-generation"`：将返回一个 TextGenerationPipeline。

    +   `"text-to-audio"`（别名`"text-to-speech"`可用）：将返回一个 TextToAudioPipeline。

    +   `"token-classification"`（别名`"ner"`可用）：将返回一个 TokenClassificationPipeline。

    +   `"translation"`：将返回一个 TranslationPipeline。

    +   `"translation_xx_to_yy"`：将返回一个 TranslationPipeline。

    +   `"video-classification"`：将返回一个 VideoClassificationPipeline。

    +   `"visual-question-answering"`：将返回一个 VisualQuestionAnsweringPipeline。

    +   `"zero-shot-classification"`：将返回一个 ZeroShotClassificationPipeline。

    +   `"zero-shot-image-classification"`：将返回一个 ZeroShotImageClassificationPipeline。

    +   `"zero-shot-audio-classification"`：将返回一个 ZeroShotAudioClassificationPipeline。

    +   `"zero-shot-object-detection"`：将返回一个 ZeroShotObjectDetectionPipeline。

+   `model`（`str`或 PreTrainedModel 或 TFPreTrainedModel，*可选*）— 该模型将被管道用于进行预测。这可以是一个模型标识符或一个实际的继承自 PreTrainedModel（对于 PyTorch）或 TFPreTrainedModel（对于 TensorFlow）的预训练模型实例。

    如果未提供，`task`的默认值将被加载。

+   `config`（`str`或 PretrainedConfig，*可选*）— 该配置将被管道用于实例化模型。这可以是模型标识符或实际的预训练模型配置，继承自 PretrainedConfig。

    如果未提供，则将使用请求的模型的默认配置文件。这意味着如果提供了`model`，将使用其默认配置。但是，如果未提供`model`，则将使用此`task`的默认模型配置。

+   `tokenizer`（`str`或 PreTrainedTokenizer，*可选*）— 该分词器将被管道用于对模型的数据进行编码。这可以是模型标识符或实际的预训练分词器，继承自 PreTrainedTokenizer。

    如果未提供，则将加载给定`model`的默认分词器（如果是字符串）。如果未指定`model`或不是字符串，则将加载`config`的默认分词器（如果是字符串）。但是，如果也未提供`config`或不是字符串，则将加载给定`task`的默认分词器。

+   `feature_extractor`（`str`或`PreTrainedFeatureExtractor`，*可选*）— 该特征提取器将被管道用于对模型的数据进行编码。这可以是模型标识符或实际的预训练特征提取器，继承自`PreTrainedFeatureExtractor`。

    特征提取器用于非 NLP 模型，例如语音或视觉模型以及多模态模型。多模态模型还需要传递一个分词器。

    如果未提供，则将加载给定`model`的默认特征提取器（如果是字符串）。如果未指定`model`或不是字符串，则将加载`config`的默认特征提取器（如果是字符串）。但是，如果也未提供`config`或不是字符串，则将加载给定`task`的默认特征提取器。

+   `framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示 PyTorch，也可以是`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `revision`（`str`，*可选*，默认为`"main"`）— 当传递任务名称或字符串模型标识符时：要使用的特定模型版本。它可以是分支名称、标签名称或提交 ID，因为我们在 huggingface.co 上使用基于 git 的系统存储模型和其他工件，所以`revision`可以是 git 允许的任何标识符。

+   `use_fast`（`bool`，*可选*，默认为`True`）— 是否尽可能使用快速分词器（PreTrainedTokenizerFast）。

+   `use_auth_token`（`str`或*bool*，*可选*）— 用作远程文件的 HTTP bearer 授权的令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。

+   `device`（`int`或`str`或`torch.device`）— 定义此管道将分配到的设备（例如，`"cpu"`，`"cuda:1"`，`"mps"`，或类似`1`的 GPU 序数等）。

+   `device_map`（`str`或`Dict[str, Union[int, str, torch.device]`，*可选*）— 直接作为`model_kwargs`发送（只是一个更简单的快捷方式）。当存在`accelerate`库时，设置`device_map="auto"`以自动计算最优化的`device_map`（有关更多信息，请参见[这里](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)）。

    不要同时使用`device_map`和`device`，因为它们会发生冲突

+   `torch_dtype`（`str`或`torch.dtype`，*可选*）- 直接发送为`model_kwargs`（只是一个更简单的快捷方式）以使用此模型的可用精度（`torch.float16`，`torch.bfloat16`，...或`"auto"`）。

+   `trust_remote_code`（`bool`，*可选*，默认为`False`）- 是否允许在 Hub 上定义的自定义代码在其自己的建模、配置、标记化甚至管道文件中执行。此选项应仅对您信任的存储库设置为`True`，并且您已经阅读了代码，因为它将在本地机器上执行 Hub 上存在的代码。

+   `model_kwargs`（`Dict[str, Any]`，*可选*）- 传递给模型的`from_pretrained(..., **model_kwargs)`函数的其他关键字参数字典。

+   `kwargs`（`Dict[str, Any]`，*可选*）- 传递给特定管道初始化的其他关键字参数（请参阅相应管道类的文档以获取可能的值）。

返回

Pipeline

适合任务的管道。

构建 Pipeline 的实用工厂方法。

管道由以下组成：

+   负责将原始文本输入映射到标记的分词器。

+   从输入中进行预测的模型。

+   一些（可选的）后处理以增强模型的输出。

示例：

```py
>>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer

>>> # Sentiment analysis pipeline
>>> analyzer = pipeline("sentiment-analysis")

>>> # Question answering pipeline, specifying the checkpoint identifier
>>> oracle = pipeline(
...     "question-answering", model="distilbert-base-cased-distilled-squad", tokenizer="bert-base-cased"
... )

>>> # Named entity recognition pipeline, passing in a specific model and tokenizer
>>> model = AutoModelForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> recognizer = pipeline("ner", model=model, tokenizer=tokenizer)
```

## 管道批处理

所有管道都可以使用批处理。每当管道使用其流式处理能力时（因此当传递列表或`Dataset`或`generator`时），它将起作用。

```py
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

dataset = datasets.load_dataset("imdb", name="plain_text", split="unsupervised")
pipe = pipeline("text-classification", device=0)
for out in pipe(KeyDataset(dataset, "text"), batch_size=8, truncation="only_first"):
    print(out)
    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]
    # Exactly the same output as before, but the content are passed
    # as batches to the model
```

然而，这并不自动意味着性能提升。它可能是 10 倍的加速或 5 倍的减速，取决于硬件、数据和实际使用的模型。

主要是加速的示例：

```py
from transformers import pipeline
from torch.utils.data import Dataset
from tqdm.auto import tqdm

pipe = pipeline("text-classification", device=0)

class MyDataset(Dataset):
    def __len__(self):
        return 5000

    def __getitem__(self, i):
        return "This is a test"

dataset = MyDataset()

for batch_size in [1, 8, 64, 256]:
    print("-" * 30)
    print(f"Streaming batch_size={batch_size}")
    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):
        pass
```

```py
# On GTX 970
------------------------------
Streaming no batching
100%|██████████████████████████████████████████████████████████████████████| 5000/5000 [00:26<00:00, 187.52it/s]
------------------------------
Streaming batch_size=8
100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:04<00:00, 1205.95it/s]
------------------------------
Streaming batch_size=64
100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2478.24it/s]
------------------------------
Streaming batch_size=256
100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 2554.43it/s]
(diminishing returns, saturated the GPU)
```

主要是减速的示例：

```py
class MyDataset(Dataset):
    def __len__(self):
        return 5000

    def __getitem__(self, i):
        if i % 64 == 0:
            n = 100
        else:
            n = 1
        return "This is a test" * n
```

与其他句子相比，这是一个偶尔非常长的句子。在这种情况下，整个批次将需要 400 个标记长，因此整个批次将是[64, 400]而不是[64, 4]，导致严重减速。更糟糕的是，在更大的批次上，程序会直接崩溃。

```py
------------------------------
Streaming no batching
100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 183.69it/s]
------------------------------
Streaming batch_size=8
100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 265.74it/s]
------------------------------
Streaming batch_size=64
100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [00:26<00:00, 37.80it/s]
------------------------------
Streaming batch_size=256
  0%|                                                                                 | 0/1000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/nicolas/src/transformers/test.py", line 42, in <module>
    for out in tqdm(pipe(dataset, batch_size=256), total=len(dataset)):
....
    q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)
RuntimeError: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 3.95 GiB total capacity; 1.72 GiB already allocated; 354.88 MiB free; 2.46 GiB reserved in total by PyTorch)
```

对于这个问题没有好的（通用）解决方案，您的使用情况可能会有所不同。经验法则：

对于用户，一个经验法则是：

+   **在您的负载上测量性能，使用您的硬件。测量，测量，继续测量。真实数字是唯一的方法。**

+   如果您受到延迟约束（进行推断的实时产品），则不要批处理。

+   如果您正在使用 CPU，则不要批处理。

+   如果您正在使用吞吐量（希望在一堆静态数据上运行模型），在 GPU 上，则：

    +   如果您对序列长度的大小一无所知（“自然”数据），默认情况下不要批处理，测量并尝试试探性地添加它，添加 OOM 检查以在失败时恢复（如果您不控制序列长度，它将在某个时候失败）。

    +   如果您的序列长度非常规则，则批处理更有可能非常有趣，测量并推动它直到出现 OOM。

    +   GPU 越大，批处理就越有可能更有趣

+   一旦启用批处理，请确保您可以很好地处理 OOM。

## 管道块批处理

`zero-shot-classification`和`question-answering`在某种意义上略有特殊，因为单个输入可能会导致模型的多次前向传递。在正常情况下，这将导致`batch_size`参数出现问题。

为了规避这个问题，这两个管道都有点特殊，它们是`ChunkPipeline`而不是常规的`Pipeline`。简而言之：

```py
preprocessed = pipe.preprocess(inputs)
model_outputs = pipe.forward(preprocessed)
outputs = pipe.postprocess(model_outputs)
```

现在变成了：

```py
all_model_outputs = []
for preprocessed in pipe.preprocess(inputs):
    model_outputs = pipe.forward(preprocessed)
    all_model_outputs.append(model_outputs)
outputs = pipe.postprocess(all_model_outputs)
```

这对您的代码应该非常透明，因为管道的使用方式相同。

这是一个简化的视图，因为管道可以自动处理批处理！这意味着您无需关心实际将触发多少前向传递，您可以独立于输入优化`batch_size`。前一节中的注意事项仍然适用。

## 管道自定义代码

如果要覆盖特定管道。

不要犹豫为您手头的任务创建一个问题，管道的目标是易于使用并支持大多数情况，因此`transformers`可能支持您的用例。

如果您只想简单尝试，可以：

+   子类化您选择的管道

```py
class MyPipeline(TextClassificationPipeline):
    def postprocess():
        # Your code goes here
        scores = scores * 100
        # And here

my_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)
# or if you use *pipeline* function, then:
my_pipeline = pipeline(model="xxxx", pipeline_class=MyPipeline)
```

这应该使您能够执行所有您想要的自定义代码。

## 实现管道

实现新管道

## 音频

音频任务可用的管道包括以下内容。

### 音频分类管道

### `class transformers.AudioClassificationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L66)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该模型将由管道用于进行预测。这需要是继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer` (PreTrainedTokenizer) — 该 tokenizer 将被管道用于为模型编码数据。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为 -1) — CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用任何`AutoModelForAudioClassification`的音频分类管道。该管道预测原始波形或音频文件的类别。在音频文件的情况下，应安装 ffmpeg 以支持多种音频格式。

示例：

```py
>>> from transformers import pipeline

>>> classifier = pipeline(model="superb/wav2vec2-base-superb-ks")
>>> classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac")
[{'score': 0.997, 'label': '_unknown_'}, {'score': 0.002, 'label': 'left'}, {'score': 0.0, 'label': 'yes'}, {'score': 0.0, 'label': 'down'}, {'score': 0.0, 'label': 'stop'}]
```

了解有关在 pipeline 教程中使用管道的基础知识

目前可以使用以下任务标识符从 pipeline()加载此管道："audio-classification"。

在[huggingface.co/models](https://huggingface.co/models?filter=audio-classification)上查看可用模型的列表。

`__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L103)

```py
( inputs: Union **kwargs ) → export const metadata = 'undefined';A list of dict with the following keys
```

参数

+   `inputs` (`np.ndarray` 或 `bytes` 或 `str` 或 `dict`) — 输入可以是：

    +   `str` — 音频文件的文件名，文件将以正确的采样率读取以获取波形，使用*ffmpeg*。这需要在系统上安装*ffmpeg*。

    +   `bytes` 应该是音频文件的内容，并由*ffmpeg*以相同方式解释。

    +   (`np.ndarray`，形状为(n, )，类型为`np.float32`或`np.float64`) — 在正确的采样率下的原始音频（不会进行进一步检查）

    +   `dict` — 可以用于传递以任意`sampling_rate`采样的原始音频，并让此管道进行重新采样。字典必须是以下格式之一：`{"sampling_rate": int, "raw": np.array}`或`{"sampling_rate": int, "array": np.array}`，其中键`"raw"`或`"array"`用于表示原始音频波形。

+   `top_k` (`int`，*可选*，默认为 None) — 管道将返回的顶部标签数。如果提供的数字为`None`或高于模型配置中可用标签的数量，则默认为标签数。

返回

一个带有以下键的`dict`列表

+   `label` (`str`) — 预测的标签。

+   `score` (`float`) — 相应的概率。

对给定的输入序列进行分类。有关更多信息，请参阅 AutomaticSpeechRecognitionPipeline 文档。

### AutomaticSpeechRecognitionPipeline

### `class transformers.AutomaticSpeechRecognitionPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L134)

```py
( model: PreTrainedModel feature_extractor: Union = None tokenizer: Optional = None decoder: Union = None device: Union = None torch_dtype: Union = None **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该模型将被管道用于进行预测。这需要是一个继承自 PreTrainedModel（对于 PyTorch）或 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `feature_extractor` (SequenceFeatureExtractor) — 该特征提取器将被管道用于为模型编码波形。

+   `tokenizer` (PreTrainedTokenizer) — 该分词器将被管道用于为模型编码数据。该对象继承自 PreTrainedTokenizer。

+   `decoder` (`pyctcdecode.BeamSearchDecoderCTC`, *可选*) — 可以传递[PyCTCDecode 的 BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)以进行语言模型增强解码。有关更多信息，请参阅 Wav2Vec2ProcessorWithLM。

+   `chunk_length_s` (`float`，*可选*，默认为 0) — 每个块中的输入长度。如果`chunk_length_s = 0`，则禁用分块（默认）。

    有关如何有效使用 `chunk_length_s` 的更多信息，请查看[ASR 分块博文](https://huggingface.co/blog/asr-chunking)。

+   `stride_length_s` (`float`, *可选*, 默认为 `chunk_length_s / 6`) — 每个块左右两侧的步幅长度。仅在 `chunk_length_s > 0` 时使用。这使得模型能够*看到*更多的上下文，并比没有上下文更好地推断字母，但管道会丢弃末尾的步幅位，以使最终重构尽可能完美。

    有关如何有效使用 `stride_length_s` 的更多信息，请查看[ASR 分块博文](https://huggingface.co/blog/asr-chunking)。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `device` (Union[`int`, `torch.device`], *可选*) — CPU/GPU 支持的设备序数。将其设置为 `None` 将使用 CPU，将其设置为正数将在关联的 CUDA 设备上运行模型。

+   `torch_dtype` (Union[`int`, `torch.dtype`], *可选*) — 计算的数据类型（dtype）。将其设置为 `None` 将使用 float32 精度。设置为 `torch.float16` 或 `torch.bfloat16` 将使用相应 dtype 的半精度。

旨在从某些音频中提取包含的口语文本的管道。

输入可以是原始波形或音频文件。在音频文件的情况下，需要安装 ffmpeg 以支持多种音频格式

示例：

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(model="openai/whisper-base")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac")
{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'}
```

了解有关在 pipeline 教程中使用管道的基础知识

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L229)

```py
( inputs: Union **kwargs ) → export const metadata = 'undefined';Dict
```

参数

+   `inputs` (`np.ndarray` 或 `bytes` 或 `str` 或 `dict`) — 输入可以是：

    +   `str`，可以是本地音频文件的文件名，也可以是下载音频文件的公共 URL 地址。文件将以正确的采样率读取，以使用*ffmpeg*获取波形。这需要系统上安装*ffmpeg*。

    +   `bytes` 应该是音频文件内容，并由*ffmpeg*以相同方式解释。

    +   (`np.ndarray` 的形状为 (n, )，类型为 `np.float32` 或 `np.float64`) — 以正确采样率的原始音频（不会进行进一步检查）

    +   可以使用`dict`形式传递以任意`sampling_rate`采样的原始音频，并让此管道进行重新采样。字典必须采用格式 `{"sampling_rate": int, "raw": np.array}`，可选地包含一个 `"stride": (left: int, right: int)`，可以要求管道在解码时忽略前 `left` 个样本和最后 `right` 个样本（但在推理中使用以向模型提供更多上下文）。仅在 CTC 模型中使用 `stride`。

+   `return_timestamps` (*可选*, `str` 或 `bool`) — 仅适用于纯 CTC 模型（Wav2Vec2、HuBERT 等）和 Whisper 模型。不适用于其他序列到序列模型。

    对于 CTC 模型，时间戳可以采用以下两种格式之一：

    +   `"char"`: 管道将为文本中的每个字符返回时间戳。例如，如果您得到 `[{"text": "h", "timestamp": (0.5, 0.6)}, {"text": "i", "timestamp": (0.7, 0.9)}]`，则表示模型预测字母“h”在 `0.5` 秒后和 `0.6` 秒前被发音。

    +   `"word"`: 管道将为文本中的每个单词返回时间戳。例如，如果您得到 `[{"text": "hi ", "timestamp": (0.5, 0.9)}, {"text": "there", "timestamp": (1.0, 1.5)}]`，则表示模型预测单词“hi”在 `0.5` 秒后和 `0.9` 秒前被发音。

    对于 Whisper 模型，时间戳可以采用以下两种格式之一：

    +   `"word"`: 与单词级 CTC 时间戳相同。单词级时间戳通过*动态时间规整（DTW）*算法预测，通过检查交叉注意力权重来近似单词级时间戳。

    +   `True`：管道将在文本中为*单词段*返回时间戳。例如，如果您获得`[{"text": " Hi there!", "timestamp": (0.5, 1.5)}]`，则表示模型预测段“Hi there!”在`0.5`秒后和`1.5`秒前被说出。请注意，文本段指的是一个或多个单词的序列，而不是单词级时间戳。

+   `generate_kwargs` (`dict`, *可选*) — 用于生成调用的`generate_config`的自定义参数字典。有关 generate 的完整概述，请查看[以下指南](https://huggingface.co/docs/transformers/en/main_classes/text_generation)。

+   `max_new_tokens` (`int`, *可选*) — 要生成的最大标记数，忽略提示中的标记数。

返回

`Dict`

具有以下键的字典：

+   `text` (`str`): 识别的文本。

+   `chunks` (*可选(, `List[Dict]`) 当使用`return_timestamps`时，`chunks`将变成一个包含模型识别的各种文本块的列表，例如`[{"text": "hi ", "timestamp": (0.5, 0.9)}, {"text": "there", "timestamp": (1.0, 1.5)}]`。原始完整文本可以通过`"".join(chunk["text"] for chunk in output["chunks"])`来粗略恢复。

将给定的音频序列转录为文本。有关更多信息，请参阅 AutomaticSpeechRecognitionPipeline 文档。

### 文本到音频管道

### `class transformers.TextToAudioPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L27)

```py
( *args vocoder = None sampling_rate = None **kwargs )
```

使用任何`AutoModelForTextToWaveform`或`AutoModelForTextToSpectrogram`的文本到音频生成管道。此管道从输入文本和可选的其他条件输入生成音频文件。

示例：

```py
>>> from transformers import pipeline

>>> pipe = pipeline(model="suno/bark-small")
>>> output = pipe("Hey it's HuggingFace on the phone!")

>>> audio = output["audio"]
>>> sampling_rate = output["sampling_rate"]
```

了解有关在 pipeline tutorial 中使用管道的基础知识

您可以通过使用`TextToAudioPipeline.__call__.forward_params`或`TextToAudioPipeline.__call__.generate_kwargs`来指定传递给模型的参数。

示例：

```py
>>> from transformers import pipeline

>>> music_generator = pipeline(task="text-to-audio", model="facebook/musicgen-small", framework="pt")

>>> # diversify the music generation by adding randomness with a high temperature and set a maximum music length
>>> generate_kwargs = {
...     "do_sample": True,
...     "temperature": 0.7,
...     "max_new_tokens": 35,
... }

>>> outputs = music_generator("Techno music with high melodic riffs", generate_kwargs=generate_kwargs)
```

此管道目前可以使用以下任务标识符从 pipeline()加载：`"text-to-speech"`或`"text-to-audio"`。

在[huggingface.co/models](https://huggingface.co/models?filter=text-to-speech)上查看可用模型列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L160)

```py
( text_inputs: Union **forward_params ) → export const metadata = 'undefined';A dict or a list of dict
```

参数

+   `text_inputs` (`str`或`List[str]`) — 要生成的文本。

+   `forward_params` (`dict`, *可选*) — 传递给模型生成/前向方法的参数。`forward_params`始终传递给底层模型。

+   `generate_kwargs` (`dict`, *可选*) — 用于生成调用的`generate_config`的自定义参数字典。有关 generate 的完整概述，请查看[以下指南](https://huggingface.co/docs/transformers/en/main_classes/text_generation)。`generate_kwargs`仅在底层模型是生成模型时才传递给底层模型。

返回

一个`dict`或`dict`的列表

字典有两个键：

+   `audio` (`np.ndarray`，形状为`(nb_channels, audio_length)`) — 生成的音频波形。

+   `sampling_rate` (`int`) — 生成的音频波形的采样率。

从输入生成语音/音频。有关更多信息，请参阅 TextToAudioPipeline 文档。

### ZeroShotAudioClassificationPipeline

### `class transformers.ZeroShotAudioClassificationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L32)

```py
( **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，用于 PyTorch，以及继承自 TFPreTrainedModel 的模型，用于 TensorFlow。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str`或`ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 用于管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断来说，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为-1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用`ClapModel`进行零射击音频分类管道。此管道在提供音频和一组`candidate_labels`时预测音频的类。

示例：

```py
>>> from transformers import pipeline
>>> from datasets import load_dataset

>>> dataset = load_dataset("ashraq/esc50")
>>> audio = next(iter(dataset["train"]["audio"]))["array"]
>>> classifier = pipeline(task="zero-shot-audio-classification", model="laion/clap-htsat-unfused")
>>> classifier(audio, candidate_labels=["Sound of a dog", "Sound of vaccum cleaner"])
[{'score': 0.9996, 'label': 'Sound of a dog'}, {'score': 0.0004, 'label': 'Sound of vaccum cleaner'}]
```

在 pipeline 教程中了解如何使用管道的基础知识。此音频分类管道目前可以通过以下任务标识符从 pipeline()加载："zero-shot-audio-classification"。在[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-audio-classification)上查看可用模型的列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L64)

```py
( audios: Union **kwargs )
```

参数

+   `audios` (`str`, `List[str]`, `np.array`或`List[np.array]`) — 管道处理三种类型的输入：

    +   包含指向音频的 http 链接的字符串

    +   包含音频本地路径的字符串

    +   加载在 numpy 中的音频

+   `candidate_labels` (`List[str]`) — 此音频的候选标签

+   `hypothesis_template`（`str`，*可选*，默认为`"This is a sound of {}"`）- 与*candidate_labels*一起使用的句子，通过将占位符替换为 candidate_labels 尝试音频分类。然后通过使用 logits_per_audio 来估计可能性。

为传入的音频分配标签。

## 计算机视觉

计算机视觉任务可用的管道包括以下内容。

### DepthEstimationPipeline

### `class transformers.DepthEstimationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L22)

```py
( *args **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）- 该模型将被管道用于进行预测。这需要是一个继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer`（PreTrainedTokenizer）- 该 tokenizer 将被管道用于对数据进行编码以供模型使用。该对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）- 为此管道的模型分配的模型卡。

+   `framework`（`str`，*可选*）- 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task`（`str`，默认为`""`）- 管道的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）- 当管道将使用*DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上）时，要使用的工作人员数量。

+   `batch_size`（`int`，*可选*，默认为 1）- 当管道将使用*DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上）时，要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*）- 负责解析提供的管道参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）- CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）- 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用任何`AutoModelForDepthEstimation`的深度估计管道。该管道预测图像的深度。

示例：

```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation", model="Intel/dpt-large")
>>> output = depth_estimator("http://images.cocodataset.org/val2017/000000039769.jpg")
>>> # This is a tensor with the values being the depth expressed in meters for each pixel
>>> output["predicted_depth"].shape
torch.Size([1, 384, 384])
```

了解有关在 pipeline 教程中使用管道的基础知识。

此深度估计管道目前可以使用以下任务标识符从 pipeline()中加载："depth-estimation"。

在[huggingface.co/models](https://huggingface.co/models?filter=depth-estimation)上查看可用模型的列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L53)

```py
( images: Union **kwargs )
```

参数

+   `images` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：

    +   包含指向图像的 http 链接的字符串

    +   包含指向图像的本地路径的字符串

    +   直接加载的 PIL 图像

    该管道接受单个图像或一批图像，然后必须将它们作为字符串传递。批处理中的图像必须全部采用相同的格式：全部作为 http 链接，全部作为本地路径，或全部作为 PIL 图像。

+   `top_k` (`int`, *可选*, 默认为 5) — 管道将返回的前几个标签的数量。如果提供的数字高于模型配置中可用的标签数量，则将默认为标签数量。

+   `timeout` (`float`, *可选*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能会永远阻塞。

为传递的图像分配标签。

### ImageClassificationPipeline

### `class transformers.ImageClassificationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L50)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 PyTorch 和继承自 TFPreTrainedModel 的模型。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示 PyTorch，也可以是`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch 框架。

+   `task` (`str`, 默认为`""`) — 管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为-1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

+   `function_to_apply` (`str`, *可选*, 默认为`"default"`) — 用于从模型输出中提取分数的函数。接受四个不同的值：

    +   `"default"`: 如果模型只有一个标签，将在输出上应用 sigmoid 函数。如果模型有多个标签，将在输出上应用 softmax 函数。

    +   `sigmoid`: 在输出上应用 sigmoid 函数。

    +   `"softmax"`: 在输出上应用 softmax 函数。

    +   `"none"`: 不在输出上应用任何函数。

使用任何 `AutoModelForImageClassification` 的图像分类管道。此管道预测图像的类别。

示例：

```py
>>> from transformers import pipeline

>>> classifier = pipeline(model="microsoft/beit-base-patch16-224-pt22k-ft22k")
>>> classifier("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'score': 0.442, 'label': 'macaw'}, {'score': 0.088, 'label': 'popinjay'}, {'score': 0.075, 'label': 'parrot'}, {'score': 0.073, 'label': 'parodist, lampooner'}, {'score': 0.046, 'label': 'poll, poll_parrot'}]
```

了解有关在 pipeline 教程 中使用管道的基础知识。

这个图像分类管道目前可以通过 pipeline() 使用以下任务标识符进行加载：`"image-classification"`。

查看 [huggingface.co/models](https://huggingface.co/models?filter=image-classification) 上可用模型的列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L111)

```py
( images: Union **kwargs )
```

参数

+   `images` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：

    +   包含指向图像的 http 链接的字符串

    +   包含指向图像的本地路径的字符串

    +   直接加载的 PIL 图像

    管道接受单个图像或一批图像，然后必须将它们作为字符串传递。批处理中的图像必须全部采用相同的格式：全部作为 http 链接，全部作为本地路径，或全部作为 PIL 图像。

+   `function_to_apply` (`str`, *可选*, 默认为`"default"`) — 应用于模型输出以检索分数的函数。接受四个不同的值：

    如果未指定此参数，则将根据标签数应用以下函数：

    +   如果模型只有一个标签，将在输出上应用 sigmoid 函数。

    +   如果模型有多个标签，将在输出上应用 softmax 函数。

    可能的值有：

    +   `sigmoid`: 在输出上应用 sigmoid 函数。

    +   `"softmax"`: 在输出上应用 softmax 函数。

    +   `"none"`: 不在输出上应用任何函数。

+   `top_k` (`int`, *可选*, 默认为 5) — 管道将返回的前 k 个标签数。如果提供的数字高于模型配置中可用的标签数，则默认为标签数。

+   `timeout` (`float`, *可选*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能永远阻塞。

为传入的图像分配标签。

### ImageSegmentationPipeline

### `class transformers.ImageSegmentationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L30)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel 是 TensorFlow。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch，或者 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, defaults to `""`) — 管道的任务标识符。

+   `num_workers` (`int`, *optional*, defaults to 8) — 当管道将使用*DataLoader*（在传递数据集时，对于 Pytorch 模型在 GPU 上），要使用的工作程序数量。

+   `batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用*DataLoader*（在传递数据集时，对于 Pytorch 模型在 GPU 上），要使用的批处理大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *optional*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, defaults to -1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，将其设置为正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output` (`bool`, *optional*, defaults to `False`) — 指示管道输出应以二进制格式（即 pickle）还是原始文本格式发生的标志。

使用任何`AutoModelForXXXSegmentation`的图像分割管道。该管道预测对象及其类别的掩模。

示例：

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(model="facebook/detr-resnet-50-panoptic")
>>> segments = segmenter("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
>>> len(segments)
2

>>> segments[0]["label"]
'bird'

>>> segments[1]["label"]
'bird'

>>> type(segments[0]["mask"])  # This is a black and white mask showing where is the bird on the original image.
<class 'PIL.Image.Image'>

>>> segments[0]["mask"].size
(768, 512)
```

此图像分割管道目前可以使用以下任务标识符从 pipeline()加载："image-segmentation"。

在[huggingface.co/models](https://huggingface.co/models?filter=image-segmentation)上查看可用模型的列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L97)

```py
( images **kwargs )
```

参数

+   `images` (`str`, `List[str]`, `PIL.Image`或`List[PIL.Image]`) — 该管道处理三种类型的图像：

    +   包含指向图像的 HTTP(S)链接的字符串

    +   包含图像本地路径的字符串

    +   直接在 PIL 中加载的图像

    该管道接受单个图像或一批图像。批处理中的图像必须全部采用相同的格式：全部作为 HTTP(S)链接，全部作为本地路径，或全部作为 PIL 图像。

+   `subtask` (`str`, *optional*) — 要执行的分割任务，根据模型的能力选择[`semantic`、`instance`和`panoptic`]。如果未设置，管道将尝试按以下顺序解析：`panoptic`、`instance`、`semantic`。

+   `threshold` (`float`, *optional*, defaults to 0.9) — 用于过滤预测掩模的概率阈值。

+   `mask_threshold` (`float`, *optional*, defaults to 0.5) — 在将预测掩模转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.5) — 用于消除小的、不连续段的掩模重叠阈值。

+   `timeout` (`float`, *optional*, defaults to None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，并且调用可能会永远阻塞。

在作为输入传递的图像中执行分割（检测掩模和类别）。

### ImageToImagePipeline

### `class transformers.ImageToImagePipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L39)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，用于 PyTorch，以及继承自 TFPreTrainedModel 的模型，用于 TensorFlow。

+   `tokenizer` (PreTrainedTokenizer) — 该 tokenizer 将被管道用于为模型编码数据。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str`或`ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示 PyTorch，也可以是`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架，并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task` (`str`, 默认为`""`) — 用于管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上），要使用的批处理大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为-1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用任何`AutoModelForImageToImage`的图像到图像管道。该管道基于先前的图像输入生成图像。

示例：

```py
>>> from PIL import Image
>>> import requests

>>> from transformers import pipeline

>>> upscaler = pipeline("image-to-image", model="caidas/swin2SR-classical-sr-x2-64")
>>> img = Image.open(requests.get("http://images.cocodataset.org/val2017/000000039769.jpg", stream=True).raw)
>>> img = img.resize((64, 64))
>>> upscaled_img = upscaler(img)
>>> img.size
(64, 64)

>>> upscaled_img.size
(144, 144)
```

此图像到图像管道目前可以从 pipeline()中使用以下任务标识符加载：`"image-to-image"`。

查看[huggingface.co/models](https://huggingface.co/models?filter=image-to-image)上可用模型的列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L87)

```py
( images: Union **kwargs )
```

参数

+   `images` (`str`, `List[str]`, `PIL.Image`或`List[PIL.Image]`) — 该管道处理三种类型的图像：

    +   包含指向图像的 http 链接的字符串

    +   包含本地图像路径的字符串

    +   直接加载的 PIL 图像

    管道接受单个图像或一批图像，然后必须将它们作为字符串传递。批处理中的图像必须全部采用相同的格式：全部作为 http 链接，全部作为本地路径，或全部作为 PIL 图像。

+   `timeout` (`float`, *可选*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不使用超时，调用可能会永远阻塞。

转换传递的图像。

### ObjectDetectionPipeline

### `class transformers.ObjectDetectionPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L26)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，用于 PyTorch，以及继承自 TFPreTrainedModel 的模型，用于 TensorFlow。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *optional*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch，也可以是 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 管道的任务标识符。

+   `num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *optional*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, 默认为 -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递原生的 `torch.device` 或一个 `str`。

+   `binary_output` (`bool`, *optional*, 默认为 `False`) — 指示管道输出应以二进制格式（即 pickle）还是原始文本格式发生的标志。

使用任何 `AutoModelForObjectDetection` 的对象检测管道。此管道预测对象的边界框和它们的类别。

示例：

```py
>>> from transformers import pipeline

>>> detector = pipeline(model="facebook/detr-resnet-50")
>>> detector("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'score': 0.997, 'label': 'bird', 'box': {'xmin': 69, 'ymin': 171, 'xmax': 396, 'ymax': 507}}, {'score': 0.999, 'label': 'bird', 'box': {'xmin': 398, 'ymin': 105, 'xmax': 767, 'ymax': 507}}]

>>> # x, y  are expressed relative to the top left hand corner.
```

在 pipeline 教程 中了解有关使用管道的基础知识

此对象检测管道目前可以通过以下任务标识符从 pipeline() 加载："object-detection"。

查看 [huggingface.co/models](https://huggingface.co/models?filter=object-detection) 上可用模型的列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L72)

```py
( *args **kwargs )
```

参数

+   `images` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：

    +   包含指向图像的 HTTP(S) 链接的字符串

    +   包含本地图像路径的字符串

    +   直接加载的 PIL 图像

    管道接受单个图像或一批图像。批处理中的图像必须全部采用相同的格式：全部作为 HTTP(S) 链接，全部作为本地路径，或全部作为 PIL 图像。

+   `threshold` (`float`, *optional*, 默认为 0.9) — 进行预测所需的概率。

+   `timeout`（`float`，*可选*，默认为 None）— 从网络获取图像的最大等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能永远阻塞。

检测作为输入传递的图像中的对象（边界框和类）。

### 视频分类管道

### `class transformers.VideoClassificationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L21)

```py
( *args **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）— 该模型将被管道用于进行预测。这需要是继承自 PreTrainedModel 的模型，对于 TensorFlow 是继承自 TFPreTrainedModel。

+   `tokenizer`（PreTrainedTokenizer）— 该管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str` 或 `ModelCard`，*可选*）— 为此管道的模型指定的模型卡。

+   `framework`（`str`，*可选*）— 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task`（`str`，默认为 `""`）— 管道的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）— 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size`（`int`，*可选*，默认为 1）— 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) 。

+   `args_parser`（ArgumentHandler，*可选*）— 负责解析提供的管道参数的对象的引用。

+   `device`（`int`，*可选*，默认为 -1）— 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递本机 `torch.device` 或 `str`。

+   `binary_output`（`bool`，*可选*，默认为 `False`）— 指示管道输出是否以二进制格式（即 pickle）或原始文本格式发生的标志。

使用任何 `AutoModelForVideoClassification` 的视频分类管道。此管道预测视频的类别。

当前可以使用以下任务标识符从 pipeline() 加载此视频分类管道：`"video-classification"`。

查看 [huggingface.co/models](https://huggingface.co/models?filter=video-classification) 上可用模型的列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L51)

```py
( videos: Union **kwargs )
```

参数

+   `videos`（`str`，`List[str]`）— 管道处理三种类型的视频：

    +   包含指向视频的 http 链接的字符串

    +   包含视频本地路径的字符串

    该管道接受单个视频或一批视频，然后必须将其作为字符串传递。批处理中的视频必须全部采用相同的格式：全部作为 http 链接或全部作为本地路径。

+   `top_k` (`int`, *optional*, defaults to 5) — 管道将返回的前 k 个标签的数量。如果提供的数字高于模型配置中可用的标签数量，则默认为标签数量。

+   `num_frames` (`int`, *optional*, defaults to `self.model.config.num_frames`) — 从视频中采样的帧数，用于进行分类。如果未提供，则将默认为模型配置中指定的帧数。

+   `frame_sampling_rate` (`int`, *optional*, defaults to 1) — 用于从视频中选择帧的采样率。如果未提供，则将默认为 1，即将使用每一帧。

为传入的视频分配标签。

### ZeroShotImageClassificationPipeline

### `class transformers.ZeroShotImageClassificationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L32)

```py
( **kwargs )
```

参数

+   `model` (PreTrainedModel or TFPreTrainedModel) — 该管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的 PyTorch 模型和 TFPreTrainedModel 的 TensorFlow 模型。

+   `tokenizer` (PreTrainedTokenizer) — 该 tokenizer 将被管道用于对数据进行编码以供模型使用。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` or `ModelCard`, *optional*) — 为该管道的模型指定的模型卡。

+   `framework` (`str`, *optional*) — 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，则将默认为当前安装的框架。如果未指定框架且两个框架都已安装，则将默认为`model`的框架，或者如果未提供模型，则将默认为 PyTorch。

+   `task` (`str`, defaults to `""`) — 用于管道的任务标识符。

+   `num_workers` (`int`, *optional*, defaults to 8) — 当管道将使用*DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用*DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批处理大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *optional*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, defaults to -1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output` (`bool`, *optional*, defaults to `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本形式发生的标志。

使用`CLIPModel`进行零样本图像分类管道。该管道在提供图像和一组`candidate_labels`时预测图像的类别。

示例：

```py
>>> from transformers import pipeline

>>> classifier = pipeline(model="openai/clip-vit-large-patch14")
>>> classifier(
...     "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png",
...     candidate_labels=["animals", "humans", "landscape"],
... )
[{'score': 0.965, 'label': 'animals'}, {'score': 0.03, 'label': 'humans'}, {'score': 0.005, 'label': 'landscape'}]

>>> classifier(
...     "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png",
...     candidate_labels=["black and white", "photorealist", "painting"],
... )
[{'score': 0.996, 'label': 'black and white'}, {'score': 0.003, 'label': 'photorealist'}, {'score': 0.0, 'label': 'painting'}]
```

在 pipeline tutorial 中了解有关使用管道的基础知识

当前可以使用 pipeline()从中加载此图像分类管道的任务标识符为："zero-shot-image-classification"。

在[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-image-classification)上查看可用模型列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L76)

```py
( images: Union **kwargs )
```

参数

+   `images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — 管道处理三种类型的图像：

    +   包含指向图像的 http 链接的字符串

    +   包含指向图像的本地路径的字符串

    +   直接在 PIL 中加载的图像

+   `candidate_labels` (`List[str]`) — 该图像的候选标签

+   `hypothesis_template` (`str`, *可选*, 默认为`"This is a photo of {}"`) — 与*candidate_labels*一起使用的句子，通过替换占位符与候选标签尝试图像分类。然后通过使用 logits_per_image 来估计可能性

+   `timeout` (`float`, *可选*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能会永远阻塞。

为传入的图像分配标签。

### ZeroShotObjectDetectionPipeline

### `class transformers.ZeroShotObjectDetectionPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L22)

```py
( **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 TensorFlow，需要是继承自 TFPreTrainedModel 的模型。

+   `tokenizer` (PreTrainedTokenizer) — 该 tokenizer 将被管道用于对数据进行编码以供模型使用。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` or `ModelCard`, *可选*) — 为该管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（当传递数据集时，对于 PyTorch 模型在 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（当传递数据集时，对于 PyTorch 模型在 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为-1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递本机的`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用`OwlViTForObjectDetection`进行零击目标检测管道。当您提供一张图像和一组`candidate_labels`时，此管道会预测对象的边界框。

示例：

```py
>>> from transformers import pipeline

>>> detector = pipeline(model="google/owlvit-base-patch32", task="zero-shot-object-detection")
>>> detector(
...     "http://images.cocodataset.org/val2017/000000039769.jpg",
...     candidate_labels=["cat", "couch"],
... )
[{'score': 0.287, 'label': 'cat', 'box': {'xmin': 324, 'ymin': 20, 'xmax': 640, 'ymax': 373}}, {'score': 0.254, 'label': 'cat', 'box': {'xmin': 1, 'ymin': 55, 'xmax': 315, 'ymax': 472}}, {'score': 0.121, 'label': 'couch', 'box': {'xmin': 4, 'ymin': 0, 'xmax': 642, 'ymax': 476}}]

>>> detector(
...     "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png",
...     candidate_labels=["head", "bird"],
... )
[{'score': 0.119, 'label': 'bird', 'box': {'xmin': 71, 'ymin': 170, 'xmax': 410, 'ymax': 508}}]
```

了解有关在 pipeline 教程中使用管道的基础知识

这个目标检测管道目前可以从 pipeline()中加载，使用以下任务标识符："zero-shot-object-detection"。

在[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-object-detection)上查看可用模型列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L65)

```py
( image: Union candidate_labels: Union = None **kwargs )
```

参数

+   `image` (`str`, `PIL.Image`或`List[Dict[str, Any]]`) — 该管道处理三种类型的图像：

    +   包含指向图像的 http url 的字符串

    +   包含指向图像的本地路径的字符串

    +   直接加载的 PIL 图像

    您可以使用此参数直接发送图像列表，数据集或生成器，如下所示：

检测传入的图像中的对象（边界框和类）。

## 自然语言处理

用于自然语言处理任务的管道包括以下内容。

### 对话管道

### `class transformers.Conversation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L18)

```py
( messages: Union = None conversation_id: UUID = None **deprecated_kwargs )
```

参数

+   `messages` (Union[str, List[Dict[str, str]]], *可选*) — 开始对话的初始消息，可以是一个字符串，也可以是包含“role”和“content”键的字典列表。如果传递了一个字符串，它将被解释为具有“user”角色的单个消息。

+   `conversation_id` (`uuid.UUID`, *可选*) — 对话的唯一标识符。如果未提供，将为对话分配一个随机的 UUID4 id。

包含对话及其历史记录的实用类。此类旨在用作 ConversationalPipeline 的输入。对话包含几个实用函数，用于管理新用户输入和生成模型响应的添加。

用法：

```py
conversation = Conversation("Going to the movies tonight - any suggestions?")
conversation.add_message({"role": "assistant", "content": "The Big lebowski."})
conversation.add_message({"role": "user", "content": "Is it good?"})
```

#### `add_user_input`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L89)

```py
( text: str overwrite: bool = False )
```

为下一轮对话添加用户输入。这是一个传统方法，假设输入必须交替用户/助手/用户/助手，因此不会连续添加多个用户消息。我们建议只使用带有“user”角色的`add_message`。

#### `append_response`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L110)

```py
( response: str )
```

这是一个传统方法。我们建议只使用带有适当角色的`add_message`。

#### `mark_processed`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L116)

```py
( )
```

这是一个传统方法，因为 Conversation 不再区分已处理和未处理的用户输入。我们在这里设置一个计数器，以保持行为大体向后兼容，但通常在编写新代码时，您应该直接阅读消息。

### `class transformers.ConversationalPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L194)

```py
( *args **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel） — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 TensorFlow，需要是继承自 TFPreTrainedModel 的模型。

+   `tokenizer`（PreTrainedTokenizer） — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str`或`ModelCard`，*可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示 PyTorch，也可以是`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`，默认为`""`) — 用于管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（当传递数据集时，在 PyTorch 模型的 GPU 上），要使用的工作程序数。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（当传递数据集时，在 PyTorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*） — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为-1) — 用于 CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

+   `min_length_for_response` (`int`, *可选*, 默认为 32) — 响应的最小长度（标记数）。

+   `minimum_tokens` (`int`, *可选*, 默认为 10) — 留给响应的最小标记长度。

多轮对话管道。

示例：

```py
>>> from transformers import pipeline, Conversation
# Any model with a chat template can be used in a ConversationalPipeline.

>>> chatbot = pipeline(model="facebook/blenderbot-400M-distill")
>>> # Conversation objects initialized with a string will treat it as a user message
>>> conversation = Conversation("I'm looking for a movie - what's your favourite one?")
>>> conversation = chatbot(conversation)
>>> conversation.messages[-1]["content"]
"I don't really have a favorite movie, but I do like action movies. What about you?"

>>> conversation.add_message({"role": "user", "content": "That's interesting, why do you like action movies?"})
>>> conversation = chatbot(conversation)
>>> conversation.messages[-1]["content"]
" I think it's just because they're so fast-paced and action-fantastic."
```

学习有关在 pipeline 教程中使用管道的基础知识

当前可以使用以下任务标识符从 pipeline()加载此对话管道：`"conversational"`。

此管道可与具有[聊天模板](https://huggingface.co/docs/transformers/chat_templating)设置的任何模型一起使用。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L262)

```py
( conversations: Union num_workers = 0 **kwargs ) → export const metadata = 'undefined';Conversation or a list of Conversation
```

参数

+   `conversations`（一个 Conversation 或 Conversation 列表） — 用于生成响应的对话。输入也可以作为带有`role`和`content`键的字典列表传递 - 在这种情况下，它们将自动转换为`Conversation`对象。可以作为列表传递任何格式的多个对话。

+   `clean_up_tokenization_spaces`（`bool`，*可选*，默认为`False`）— 是否清除文本输出中可能存在的额外空格。generate_kwargs — 要传递给模型的 generate 方法的其他关键字参数（请参阅与您的框架对应的 generate 方法此处）。

返回

对话或对话列表

包含新用户输入的对话的更新生成响应的对话。

为输入的对话生成响应。

### FillMaskPipeline

### `class transformers.FillMaskPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L22)

```py
( model: Union tokenizer: Optional = None feature_extractor: Optional = None image_processor: Optional = None modelcard: Optional = None framework: Optional = None task: str = '' args_parser: ArgumentHandler = None device: Union = None torch_dtype: Union = None binary_output: bool = False **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）— 流水线将用于进行预测的模型。这需要是继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer`（PreTrainedTokenizer）— 流水线将用于为模型编码数据的分词器。该对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）— 为该流水线的模型指定的模型卡。

+   `framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task`（`str`，默认为`""`）— 用于流水线的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）— 当流水线将使用*DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size`（`int`，*可选*，默认为 1）— 当流水线将使用*DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用流水线进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*）— 负责解析提供的流水线参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）— CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）— 指示流水线输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

+   `top_k`（`int`，默认为 5）— 要返回的预测数量。

+   `targets`（`str`或`List[str]`，*可选*）— 当传递时，模型将限制分数到传递的目标，而不是在整个词汇表中查找。如果提供的目标不在模型词汇表中，它们将被标记化，第一个生成的标记将被使用（带有警告，可能会更慢）。

使用任何`ModelWithLMHead`的掩码语言建模预测管道。有关更多信息，请参阅掩码语言建模示例。

示例：

```py
>>> from transformers import pipeline

>>> fill_masker = pipeline(model="bert-base-uncased")
>>> fill_masker("This is a simple [MASK].")
[{'score': 0.042, 'token': 3291, 'token_str': 'problem', 'sequence': 'this is a simple problem.'}, {'score': 0.031, 'token': 3160, 'token_str': 'question', 'sequence': 'this is a simple question.'}, {'score': 0.03, 'token': 8522, 'token_str': 'equation', 'sequence': 'this is a simple equation.'}, {'score': 0.027, 'token': 2028, 'token_str': 'one', 'sequence': 'this is a simple one.'}, {'score': 0.024, 'token': 3627, 'token_str': 'rule', 'sequence': 'this is a simple rule.'}]
```

了解有关在 pipeline 教程中使用管道的基础知识

此掩码填充管道目前可以使用 pipeline()从以下任务标识符加载："fill-mask"。

此管道可以使用已经使用掩码语言建模目标进行训练的模型，其中包括库中的双向模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=fill-mask)上可用模型的最新列表。

此管道仅适用于具有一个掩码标记的输入。实验性：我们添加了对多个掩码的支持。返回的值是原始模型输出，并对应于不相交的概率，其中一个可能期望联合概率（请参见[讨论](https://github.com/huggingface/transformers/pull/10222)）。

此管道现在支持 tokenizer_kwargs。例如尝试：

```py
>>> from transformers import pipeline

>>> fill_masker = pipeline(model="bert-base-uncased")
>>> tokenizer_kwargs = {"truncation": True}
>>> fill_masker(
...     "This is a simple [MASK]. " + "...with a large amount of repeated text appended. " * 100,
...     tokenizer_kwargs=tokenizer_kwargs,
... )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L248)

```py
( inputs *args **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `args`（`str`或`List[str]`）- 一个或多个文本（或一个提示列表）带有掩码标记。

+   `targets`（`str`或`List[str]`，*可选*）- 当传递时，模型将限制分数到传递的目标，而不是在整个词汇表中查找。如果提供的目标不在模型词汇表中，它们将被标记化，并且将使用第一个生成的标记（带有警告，并且可能会更慢）。

+   `top_k`（`int`，*可选*）- 当传递时，将覆盖要返回的预测数量。

返回

一个字典列表或字典列表

每个结果都以字典列表的形式呈现，具有以下键：

+   `sequence`（`str`）- 具有掩码标记预测的相应输入。

+   `score`（`float`）- 相应的概率。

+   `token`（`int`）- 预测的标记 id（用于替换掩码标记）。

+   `token_str`（`str`）- 预测的标记（用于替换掩码标记）。

填充作为输入的文本中的掩码标记。

### QuestionAnsweringPipeline

### `class transformers.QuestionAnsweringPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L224)

```py
( model: Union tokenizer: PreTrainedTokenizer modelcard: Optional = None framework: Optional = None task: str = '' **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）- 该模型将被管道用于进行预测。这需要是继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer`（PreTrainedTokenizer）- 该 tokenizer 将被管道用于为模型编码数据。此对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）- 为此管道的模型指定的模型卡。

+   `framework`（`str`，*可选*）- 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，则将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task`（`str`，默认为`""`）- 用于管道的任务标识符。

+   `num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *optional*, 默认为 1) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *optional*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, 默认为-1) — 用于 CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递原生的 `torch.device` 或一个 `str`。

+   `binary_output` (`bool`, *optional*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用任何 `ModelForQuestionAnswering` 的问答管道。有关更多信息，请参阅问答示例。

示例：

```py
>>> from transformers import pipeline

>>> oracle = pipeline(model="deepset/roberta-base-squad2")
>>> oracle(question="Where do I live?", context="My name is Wolfgang and I live in Berlin")
{'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}
```

了解如何在 pipeline tutorial 中使用管道的基础知识。

此问答管道目前可以使用以下任务标识符从 pipeline()中加载："question-answering"。

此管道可以使用已在问答任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=question-answering)上可用模型的最新列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L343)

```py
( *args **kwargs ) → export const metadata = 'undefined';A dict or a list of dict
```

参数

+   `args` (`SquadExample` 或 `SquadExample` 列表) — 包含问题和上下文的一个或多个 `SquadExample`。

+   `X` (`SquadExample` 或 `SquadExample` 列表, *optional*) — 包含问题和上下文的一个或多个 `SquadExample`（将被视为第一个位置参数传递时的相同方式）。

+   `data` (`SquadExample` 或 `SquadExample` 列表, *optional*) — 包含问题和上下文的一个或多个 `SquadExample`（将被视为第一个位置参数传递时的相同方式）。

+   `question` (`str` 或 `List[str]`) — 一个或多个问题（必须与 `context` 参数一起使用）。

+   `context` (`str` 或 `List[str]`) — 与问题相关联的一个或多个上下文（必须与 `question` 参数一起使用）。

+   `topk` (`int`, *optional*, 默认为 1) — 要返回的答案数量（将按可能性顺序选择）。请注意，如果在上下文中没有足够的选项可用，我们将返回少于 topk 个答案。

+   `doc_stride` (`int`, *optional*, 默认为 128) — 如果上下文对于模型来说太长而无法与问题匹配，它将被分成几个具有一些重叠的块。此参数控制该重叠的大小。

+   `max_answer_len` (`int`, *optional*, 默认为 15) — 预测答案的最大长度（例如，只考虑长度较短的答案）。

+   `max_seq_len` (`int`, *optional*, 默认为 384) — 每个传递给模型的块中的总句子长度（上下文 + 问题）的最大长度。如果需要，上下文将被分成几个块（使用 `doc_stride` 作为重叠）。

+   `max_question_len` (`int`, *optional*, 默认为 64) — 在标记化后问题的最大长度。如果需要，它将被截断。

+   `handle_impossible_answer` (`bool`, *optional*, 默认为 `False`) — 是否接受不可能作为答案。

+   `align_to_words` (`bool`, *可选*, 默认为 `True`) — 尝试将答案与实际单词对齐。提高了空格分隔语言的质量。可能会对非空格分隔的语言（如日语或中文）造成伤害

返回

一个 `dict` 或一个 `dict` 的列表

每个结果都作为一个带有以下键的字典：

+   `score` (`float`) — 与答案相关联的概率。

+   `start` (`int`) — 答案的字符起始索引（在输入的标记化版本中）。

+   `end` (`int`) — 答案的字符结束索引（在输入的标记化版本中）。

+   `answer` (`str`) — 问题的答案。

通过使用输入的上下文回答提出的问题。

#### `create_sample`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L278)

```py
( question: Union context: Union ) → export const metadata = 'undefined';One or a list of SquadExample
```

参数

+   `question` (`str` 或 `List[str]`) — 提出的问题。

+   `context` (`str` 或 `List[str]`) — 我们将在其中寻找答案的上下文。

返回

一个或多个 `SquadExample`

相应的 `SquadExample` 分组问题和上下文。

QuestionAnsweringPipeline 在内部利用 `SquadExample`。这个辅助方法封装了将问题和上下文转换为 `SquadExample` 的所有逻辑。

我们目前支持抽取式问答。

#### `span_to_answer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L630)

```py
( text: str start: int end: int ) → export const metadata = 'undefined';Dictionary like `{‘answer’
```

参数

+   `text` (`str`) — 从中提取答案的实际上下文。

+   `start` (`int`) — 答案起始标记索引。

+   `end` (`int`) — 答案结束标记索引。

返回

类似于 `{‘answer’

str, ‘start’: int, ‘end’: int}`

从标记概率解码时，此方法将标记索引映射到初始上下文中的实际单词。

### SummarizationPipeline

### `class transformers.SummarizationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L216)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 流水线将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel。

+   `tokenizer` (PreTrainedTokenizer) — 流水线将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此流水线的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 用于流水线的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当流水线将使用 *DataLoader*（当传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当流水线将使用 *DataLoader*（当传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批量大小，对于推断，这并不总是有益的，请阅读 [使用流水线进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) 。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为 -1) — CPU/GPU 支持的设备序号。将其设置为-1 将利用 CPU，设置为正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的 `torch.device` 或 `str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式进行的标志。

总结新闻文章和其他文档。

当前可以使用以下任务标识符从 pipeline() 加载此总结管道: `"summarization"`.

此管道可以使用已在摘要任务上进行了微调的模型，目前有 ’*bart-large-cnn*’, ’*t5-small*’, ’*t5-base*’, ’*t5-large*’, ’*t5-3b*’, ’*t5-11b*’。查看[huggingface.co/models](https://huggingface.co/models?filter=summarization)上可用模型的最新列表。有关可用参数的列表，请参阅[以下文档](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)

用法:

```py
# use bart in pytorch
summarizer = pipeline("summarization")
summarizer("An apple a day, keeps the doctor away", min_length=5, max_length=20)

# use t5 in tf
summarizer = pipeline("summarization", model="t5-base", tokenizer="t5-base", framework="tf")
summarizer("An apple a day, keeps the doctor away", min_length=5, max_length=20)
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L245)

```py
( *args **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `documents` (*str* 或 `List[str]`) — 要总结的一个或多个文章（或一组文章）。

+   `return_text` (`bool`, *可选*, 默认为 `True`) — 是否在输出中包含解码后的文本

+   `return_tensors` (`bool`, *可选*, 默认为 `False`) — 是否在输出中包含预测的张量（作为标记索引）。

+   `clean_up_tokenization_spaces` (`bool`, *可选*, 默认为 `False`) — 是否清除文本输出中可能存在的额外空格。generate_kwargs — 传递给模型的 generate 方法的额外关键字参数（请参阅您框架对应的 generate 方法此处）。

返回

一个 `dict` 列表或 `dict` 列表的列表

每个结果都以以下键的字典形式呈现:

+   `summary_text` (`str`, 当 `return_text=True` 时存在) — 相应输入的摘要。

+   `summary_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True` 时存在) — 摘要的标记 ID。

将输入的文本进行总结。

### TableQuestionAnsweringPipeline

### `class transformers.TableQuestionAnsweringPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L87)

```py
( args_parser = <transformers.pipelines.table_question_answering.TableQuestionAnsweringArgumentHandler object at 0x7f3b448d5f30> *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 TensorFlow 是继承自 TFPreTrainedModel。

+   `tokenizer` (PreTrainedTokenizer) — 该管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, defaults to `""`) — 用于管道的任务标识符。

+   `num_workers` (`int`, *optional*, defaults to 8) — 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) 。

+   `args_parser`（ArgumentHandler, *optional*） — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, defaults to -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递原生 `torch.device` 或 `str`。

+   `binary_output` (`bool`, *optional*, defaults to `False`) — 标志，指示管道输出应以二进制格式（即 pickle）或原始文本形式发生。

使用 `ModelForTableQuestionAnswering` 的表格问答管道。此管道仅在 PyTorch 中可用。

示例：

```py
>>> from transformers import pipeline

>>> oracle = pipeline(model="google/tapas-base-finetuned-wtq")
>>> table = {
...     "Repository": ["Transformers", "Datasets", "Tokenizers"],
...     "Stars": ["36542", "4512", "3934"],
...     "Contributors": ["651", "77", "34"],
...     "Programming language": ["Python", "Python", "Rust, Python and NodeJS"],
... }
>>> oracle(query="How many stars does the transformers repository have?", table=table)
{'answer': 'AVERAGE > 36542', 'coordinates': [(0, 1)], 'cells': ['36542'], 'aggregator': 'AVERAGE'}
```

了解有关在 pipeline 教程 中使用管道的基础知识

此表格问答管道目前可以从 pipeline() 中加载，使用以下任务标识符：`"table-question-answering"`。

此管道可以使用已在表格问答任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?filter=table-question-answering) 上提供的可用模型的最新列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L270)

```py
( *args **kwargs ) → export const metadata = 'undefined';A dictionary or a list of dictionaries containing results
```

参数

+   `table` (`pd.DataFrame` 或 `Dict`) — 将转换为包含所有表格值的 DataFrame 的 Pandas DataFrame 或字典。请参阅上面的字典示例。

+   `query` (`str` 或 `List[str]`) — 将发送到模型的查询或查询列表，以及表格一起。

+   `sequential` (`bool`, *optional*, defaults to `False`) — 是否按顺序进行推理还是批处理。批处理更快，但像 SQA 这样的模型需要按顺序进行推理，以提取序列中的关系，考虑到它们的对话性质。

+   `padding` (`bool`, `str` 或 PaddingStrategy, *optional*, defaults to `False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 填充到使用参数 `max_length` 指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）: 不填充（即可以输出具有不同长度序列的批次）。

+   `truncation` (`bool`, `str` 或 `TapasTruncationStrategy`, *optional*, defaults to `False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'drop_rows_to_fit'`: 截断到使用参数 `max_length` 指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。这将逐行截断，从表中删除行。

    +   `False`或`'do_not_truncate'`（默认）：不截断（即，可以输出序列长度大于模型最大可接受输入大小的批次）。

返回

包含结果的字典或字典列表

每个结果是一个带有以下键的字典：

+   `answer`（`str`）— 给定表格的查询答案。如果有聚合器，答案将以`AGGREGATOR >`开头。

+   `coordinates`（`List[Tuple[int, int]]`）— 答案单元格的坐标。

+   `cells`（`List[str]`）— 由答案单元格值组成的字符串列表。

+   `aggregator`（`str`）— 如果模型有聚合器，则返回聚合器。

根据表格回答查询。管道接受以下几种类型的输入，详细信息如下：

+   `pipeline(table, query)`

+   `pipeline(table, [query])`

+   `pipeline(table=table, query=query)`

+   `pipeline(table=table, query=[query])`

+   `pipeline({"table": table, "query": query})`

+   `pipeline({"table": table, "query": [query]})`

+   `pipeline([{"table": table, "query": query}, {"table": table, "query": query}])`

`table`参数应该是一个从该字典构建的字典或 DataFrame，包含整个表格：

示例：

```py
data = {
    "actors": ["brad pitt", "leonardo di caprio", "george clooney"],
    "age": ["56", "45", "59"],
    "number of movies": ["87", "53", "69"],
    "date of birth": ["7 february 1967", "10 june 1996", "28 november 1967"],
}
```

可以将此字典作为参数传递，或者可以将其转换为 pandas DataFrame：

示例：

```py
import pandas as pd

table = pd.DataFrame.from_dict(data)
```

### TextClassificationPipeline

### `class transformers.TextClassificationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L34)

```py
( **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）— 该模型将被管道用于进行预测。这需要是继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel。

+   `tokenizer`（PreTrainedTokenizer）— 该 tokenizer 将被管道用于为模型编码数据。该对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）— 为该管道的模型指定的模型卡。

+   `framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task`（`str`，默认为`""`）— 用于管道的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）— 当管道将使用*DataLoader*（在 PyTorch 模型的 GPU 上传递数据集时），要使用的工作程序数量。

+   `batch_size`（`int`，*可选*，默认为 1）— 当管道将使用*DataLoader*（传递数据集时，在 PyTorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*）— 负责解析提供的管道参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）— CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）— 指示管道输出应以二进制格式（即 pickle）还是原始文本格式发生的标志。

+   `return_all_scores`（`bool`，*可选*，默认为`False`）— 是否返回所有预测分数还是仅返回预测类的分数。

+   `function_to_apply`（`str`，*可选*，默认为`"default"`）— 用于应用于模型输出以检索分数的函数。接受四个不同的值：

    +   `"default"`：如果模型有一个标签，则将在输出上应用 sigmoid 函数。如果模型有多个标签，则将在输出上应用 softmax 函数。

    +   `"sigmoid"`：在输出上应用 sigmoid 函数。

    +   `"softmax"`：在输出上应用 softmax 函数。

    +   `"none"`：不在输出上应用任何函数。

使用任何`ModelForSequenceClassification`的文本分类管道。有关更多信息，请参阅序列分类示例。

例：

```py
>>> from transformers import pipeline

>>> classifier = pipeline(model="distilbert-base-uncased-finetuned-sst-2-english")
>>> classifier("This movie is disgustingly good !")
[{'label': 'POSITIVE', 'score': 1.0}]

>>> classifier("Director tried too much.")
[{'label': 'NEGATIVE', 'score': 0.996}]
```

了解有关在 pipeline 教程中使用管道的基础知识

此文本分类管道目前可以从 pipeline()中使用以下任务标识符加载：`"sentiment-analysis"`（用于根据积极或消极情绪对序列进行分类）。

如果有多个分类标签可用（`model.config.num_labels >= 2`），则管道将对结果运行 softmax。如果只有一个标签，则管道将对结果运行 sigmoid。

此管道可以使用已在序列分类任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=text-classification)上可用模型的最新列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L122)

```py
( *args **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `args`（`str`或`List[str]`或`Dict[str]`或`List[Dict[str]]`）— 要分类的一个或多个文本。为了在分类中使用文本对，可以发送包含`{"text"，"text_pair"}`键的字典，或者这些字典的列表。

+   `top_k`（`int`，*可选*，默认为`1`）— 要返回多少结果。

+   `function_to_apply`（`str`，*可选*，默认为`"default"`）— 用于应用于模型输出以检索分数的函数。接受四个不同的值：

    如果未指定此参数，则将根据标签数应用以下函数：

    +   如果模型有一个标签，将在输出上应用 sigmoid 函数。

    +   如果模型有多个标签，将在输出上应用 softmax 函数。

    可能的值为：

    +   `"sigmoid"`：在输出上应用 sigmoid 函数。

    +   `"softmax"`：在输出上应用 softmax 函数。

    +   `"none"`：不在输出上应用任何函数。

返回

一个列表或`dict`的列表

每个结果都作为带有以下键的字典列表：

+   `label`（`str`）— 预测的标签。

+   `score`（`float`）— 相应的概率。

如果使用`top_k`，则每个标签返回一个这样的字典。

对给定的文本进行分类。

### TextGenerationPipeline

### `class transformers.TextGenerationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L23)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该 pipeline 将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 PyTorch 和 TFPreTrainedModel 的 TensorFlow。

+   `tokenizer` (PreTrainedTokenizer) — 该 pipeline 将用于为模型编码数据的分词器。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此 pipeline 的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 用于该 pipeline 的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当 pipeline 将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当 pipeline 将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断来说，这并不总是有益的，请阅读[使用 pipeline 进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的 pipeline 参数的对象的引用。

+   `device` (`int`, *可选*, 默认为 -1) — CPU/GPU 支持的设备序数。将其设置为 -1 将使用 CPU，设置为正数将在关联的 CUDA 设备上运行模型。您也可以传递原生的 `torch.device` 或一个 `str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 指示 pipeline 输出是否应以二进制格式（即 pickle）或原始文本形式发生的标志。

使用任何 `ModelWithLMHead` 的语言生成 pipeline。该 pipeline 预测将跟随指定文本提示的单词。

示例:

```py
>>> from transformers import pipeline

>>> generator = pipeline(model="gpt2")
>>> generator("I can't believe you did such a ", do_sample=False)
[{'generated_text': "I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I"}]

>>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.
>>> outputs = generator("My tart needs some", num_return_sequences=4, return_full_text=False)
```

了解有关在 pipeline 教程中使用 pipeline 的基础知识。您可以将文本生成参数传递给此 pipeline 以控制停止标准、解码策略等。在文本生成策略和文本生成中了解更多关于文本生成参数的信息。

目前可以使用以下任务标识符从 pipeline() 加载此语言生成 pipeline: `"text-generation"`.

该 pipeline 可以使用已经训练过自回归语言建模目标的模型，包括库中的单向模型（例如 gpt2）。查看[huggingface.co/models](https://huggingface.co/models?filter=text-generation)上可用模型的列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L178)

```py
( text_inputs **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `args` (`str` 或 `List[str]`) — 一个或多个提示（或一个提示列表）以完成。

+   `return_tensors` (`bool`, *可选*, 默认为 `False`) — 是否在输出中返回预测的张量（作为标记索引）。如果设置为 `True`，则不返回解码后的文本。

+   `return_text` (`bool`, *可选*, 默认为 `True`) — 是否返回输出中解码后的文本。

+   `return_full_text` (`bool`, *可选*, 默认为 `True`) — 如果设置为 `False`，则只返回添加的文本，否则返回完整的文本。仅在 *return_text* 设置为 True 时有意义。

+   `clean_up_tokenization_spaces` (`bool`, *可选*, 默认为 `False`) — 是否清除文本输出中的潜在额外空格。

+   `prefix` (`str`, *可选*) — 添加到提示的前缀。

+   `handle_long_generation` (`str`, *可选*) — 默认情况下，此管道不处理长生成（超过模型最大长度的生成）。没有完美的解决方法（更多信息：[`github.com/huggingface/transformers/issues/14033#issuecomment-948385227`](https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227)）。这提供了根据您的用例解决该问题的常见策略。

    +   `None`：默认策略，没有特别的操作

    +   `"hole"`：截断输入的左侧，并留下足够宽的间隙以进行生成（可能会截断大部分提示，当生成超出模型容量时不适用）

    generate_kwargs — 传递给模型的 generate 方法的额外关键字参数（查看您框架对应的 generate 方法这里）。

返回

一个 `dict` 或 `dict` 的列表

返回以下字典之一（不能返回 `generated_text` 和 `generated_token_ids` 的组合）：

+   `generated_text` (`str`, 当 `return_text=True` 时存在) — 生成的文本。

+   `generated_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True` 时存在) — 生成文本的标记 id。

完成给定的提示作为输入。

### 文本到文本生成管道

### `class transformers.Text2TextGenerationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L25)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 TensorFlow 则是继承自 TFPreTrainedModel。

+   `tokenizer` (PreTrainedTokenizer) — 该管道将用于为模型编码数据的分词器。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为该管道的模型指定的模型卡片。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 用于该管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上）时，要使用的工作人员数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断来说，这并不总是有益的，请阅读 [使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) 。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为 -1) — CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递原生的 `torch.device` 或 `str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用 seq2seq 模型进行文本到文本生成的管道。

示例：

```py
>>> from transformers import pipeline

>>> generator = pipeline(model="mrm8488/t5-base-finetuned-question-generation-ap")
>>> generator(
...     "answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google"
... )
[{'generated_text': 'question: Who created the RuPERTa-base?'}]
```

了解如何在 管道教程 中使用管道的基础知识。您可以将文本生成参数传递给此管道，以控制停止条件、解码策略等。在 文本生成策略 和 文本生成 中了解更多关于文本生成参数的信息。

此 Text2TextGenerationPipeline 管道目前可以通过以下任务标识符从 pipeline() 加载："text2text-generation"。

此管道可以使用已在翻译任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?filter=text2text-generation) 上可用模型的最新列表。有关可用参数的列表，请参阅 [以下文档](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)

用法：

```py
text2text_generator = pipeline("text2text-generation")
text2text_generator("question: What is 42 ? context: 42 is the answer to life, the universe and everything")
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L138)

```py
( *args **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `args` (`str` 或 `List[str]`) — 编码器的输入文本。

+   `return_tensors` (`bool`, *可选*, 默认为 `False`) — 是否在输出中包含预测的张量（作为标记索引）。

+   `return_text` (`bool`, *可选*, 默认为 `True`) — 是否在输出中包含解码后的文本。

+   `clean_up_tokenization_spaces` (`bool`, *可选*, 默认为 `False`) — 是否清除文本输出中可能存在的额外空格。

+   `truncation` (`TruncationStrategy`, *可选*, 默认为 `TruncationStrategy.DO_NOT_TRUNCATE`) — 管道内部标记化的截断策略。`TruncationStrategy.DO_NOT_TRUNCATE`（默认）永远不会截断，但有时希望将输入截断以适应模型的 `max_length` 而不是在后续出错。generate_kwargs — 传递给模型的 generate 方法的其他关键字参数（请参阅您的框架对应的 generate 方法 此处）。

返回

一个字典列表或字典列表

每个结果都作为一个带有以下键的字典。

+   `generated_text` (`str`, 当 `return_text=True` 时存在) — 生成的文本。

+   `generated_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True` 时存在) — 生成文本的标记 id。

使用给定的文本作为输入生成输出文本。

#### `check_inputs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L111)

```py
( input_length: int min_length: int max_length: int )
```

检查给定输入与模型相关是否存在问题。

### TokenClassificationPipeline

### `class transformers.TokenClassificationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L61)

```py
( args_parser = <transformers.pipelines.token_classification.TokenClassificationArgumentHandler object at 0x7f3b448d6350> *args **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）— 流水线将用于进行预测的模型。这需要是继承自 PreTrainedModel 的 PyTorch 模型和继承自 TFPreTrainedModel 的 TensorFlow 模型。

+   `tokenizer`（PreTrainedTokenizer）— 流水线将用于对数据进行编码的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）— 为此流水线的模型指定的模型卡。

+   `framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`代表 PyTorch 或`"tf"`代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task`（`str`，默认为`""`）— 用于流水线的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）— 当流水线将使用*DataLoader*（传递数据集时，在 PyTorch 模型的 GPU 上）时，要使用的工作人员数量。

+   `batch_size`（`int`，*可选*，默认为 1）— 当流水线将使用*DataLoader*（传递数据集时，在 PyTorch 模型的 GPU 上）时，要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用流水线进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*）— 负责解析提供的流水线参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）— 用于 CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）— 指示流水线输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

+   `ignore_labels`（`List[str]`，默认为`["O"]`）— 要忽略的标签列表。

+   `grouped_entities`（`bool`，*可选*，默认为`False`）— 已弃用，请改用`aggregation_strategy`。是否将对应于相同实体的标记分组在预测中一起还是不分组。

+   `stride`（`int`，*可选*）— 如果提供了步幅，流水线将应用于所有文本。文本将被分割成大小为 model_max_length 的块。仅适用于快速分词器和`aggregation_strategy`不同于`NONE`的情况。此参数的值定义了块之间重叠标记的数量。换句话说，模型将在每一步中向前移动`tokenizer.model_max_length - stride`个标记。

+   `aggregation_strategy`（`str`，*可选*，默认为`"none"`）— 基于模型预测融合（或不融合）标记的策略。

    +   “none”：将简单地不执行任何聚合，并直接从模型返回原始结果

    +   “simple”：将尝试按照默认模式对实体进行分组。（A，B-TAG），（B，I-TAG），（C，I-TAG），（D，B-TAG2）（E，B-TAG2）最终将成为[{“word”：ABC，“entity”：“TAG”}，{“word”：“D”，“entity”：“TAG2”}，{“word”：“E”，“entity”：“TAG2”}]请注意，两个连续的 B 标签将成为不同的实体。在基于单词的语言中，我们可能会不希望将单词拆分：想象一下将 Microsoft 标记为[{“word”：“Micro”，“entity”：“企业”}，{“word”：“soft”，“entity”：“名称”}]。查找 FIRST、MAX、AVERAGE 以了解减轻这种情况并消除单词歧义的方法（在支持该含义的语言上，基本上是由空格分隔的标记）。这些减轻措施仅适用于真实单词，“New york”可能仍然会被标记为两个不同的实体。

    +   “first”：（仅适用于基于单词的模型）将使用`SIMPLE`策略，除非单词不能以不同的标签结束。当存在歧义时，单词将简单地使用单词的第一个标记的标签。

    +   “average”：（仅适用于基于单词的模型）将使用`SIMPLE`策略，除非单词不能以不同的标签结束。分数将首先在标记之间平均，然后应用最大标签。

    +   “max”：（仅适用于基于单词的模型）将使用`SIMPLE`策略，除非单词不能以不同的标签结束。单词实体将简单地是具有最高分数的标记。

使用任何`ModelForTokenClassification`来进行命名实体识别管道。有关更多信息，请参阅命名实体识别示例。

示例：

```py
>>> from transformers import pipeline

>>> token_classifier = pipeline(model="Jean-Baptiste/camembert-ner", aggregation_strategy="simple")
>>> sentence = "Je m'appelle jean-baptiste et je vis à montréal"
>>> tokens = token_classifier(sentence)
>>> tokens
[{'entity_group': 'PER', 'score': 0.9931, 'word': 'jean-baptiste', 'start': 12, 'end': 26}, {'entity_group': 'LOC', 'score': 0.998, 'word': 'montréal', 'start': 38, 'end': 47}]

>>> token = tokens[0]
>>> # Start and end provide an easy way to highlight words in the original text.
>>> sentence[token["start"] : token["end"]]
' jean-baptiste'

>>> # Some models use the same idea to do part of speech.
>>> syntaxer = pipeline(model="vblagoje/bert-english-uncased-finetuned-pos", aggregation_strategy="simple")
>>> syntaxer("My name is Sarah and I live in London")
[{'entity_group': 'PRON', 'score': 0.999, 'word': 'my', 'start': 0, 'end': 2}, {'entity_group': 'NOUN', 'score': 0.997, 'word': 'name', 'start': 3, 'end': 7}, {'entity_group': 'AUX', 'score': 0.994, 'word': 'is', 'start': 8, 'end': 10}, {'entity_group': 'PROPN', 'score': 0.999, 'word': 'sarah', 'start': 11, 'end': 16}, {'entity_group': 'CCONJ', 'score': 0.999, 'word': 'and', 'start': 17, 'end': 20}, {'entity_group': 'PRON', 'score': 0.999, 'word': 'i', 'start': 21, 'end': 22}, {'entity_group': 'VERB', 'score': 0.998, 'word': 'live', 'start': 23, 'end': 27}, {'entity_group': 'ADP', 'score': 0.999, 'word': 'in', 'start': 28, 'end': 30}, {'entity_group': 'PROPN', 'score': 0.999, 'word': 'london', 'start': 31, 'end': 37}]
```

了解有关在 pipeline 教程中使用管道的基础知识

此标记识别管道目前可以从 pipeline()中加载，使用以下任务标识符："ner"（用于预测序列中的标记类别：人物、组织、地点或其他）。

此管道可以使用已在标记分类任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=token-classification)上可用模型的最新列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L219)

```py
( inputs: Union **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `inputs`（`str`或`List[str]`）—一个或多个文本（或一个文本列表）用于标记分类。

返回

一个字典列表或字典列表的列表

每个结果都作为字典列表（对应于相应输入中的每个标记，或者如果使用聚合策略实例化此管道，则对应于每个实体）返回，具有以下键：

+   `word`（`str`）—分类的标记/单词。这是通过解码所选标记获得的。如果要获得原始句子中的确切字符串，请使用`start`和`end`。

+   `score`（`float`）—`entity`的相应概率。

+   `entity`（`str`）—为该标记/单词预测的实体（当*aggregation_strategy*不是`"none"`时，它被命名为*entity_group*）。

+   `index`（`int`，仅在`aggregation_strategy="none"`时存在）—句子中对应标记的索引。

+   `start`（`int`，*可选*）—句子中对应实体的起始索引。仅当分词器中存在偏移量时才存在

+   `end`（`int`，*可选*）—句子中对应实体的结束索引。仅当分词器中存在偏移量时才存在

对给定的文本的每个标记进行分类。

#### `aggregate_words`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L470)

```py
( entities: List aggregation_strategy: AggregationStrategy )
```

覆盖给定单词中不同意的标记，以强制在单词边界上达成一致。

示例：micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT 将使用第一种策略重写为 microsoft| company| B-ENT I-ENT

#### `gather_pre_entities`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L356)

```py
( sentence: str input_ids: ndarray scores: ndarray offset_mapping: Optional special_tokens_mask: ndarray aggregation_strategy: AggregationStrategy )
```

将各种 numpy 数组融合成包含所有聚合所需信息的字典

#### `group_entities`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L533)

```py
( entities: List )
```

参数

+   `entities`（`dict`）— 管道预测的实体。

查找并将预测相同实体的相邻标记组合在一起。

#### `group_sub_entities`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L498)

```py
( entities: List )
```

参数

+   `entities`（`dict`）— 管道预测的实体。

将预测相同实体的相邻标记组合在一起。

### TranslationPipeline

### `class transformers.TranslationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L286)

```py
( *args **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）— 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，用于 PyTorch，以及继承自 TFPreTrainedModel 的模型，用于 TensorFlow。

+   `tokenizer`（PreTrainedTokenizer）— 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）— 为此管道的模型指定的模型卡。

+   `framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示 PyTorch，也可以是`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task`（`str`，默认为`""`）— 管道的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）— 当管道将使用*DataLoader*（当传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size`（`int`，*可选*，默认为 1）— 当管道将使用*DataLoader*（当传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批量大小，对于推断，这并不总是有益，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*）— 负责解析提供的管道参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）— CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）— 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

从一种语言翻译成另一种语言。

目前可以使用以下任务标识符从 pipeline()加载此翻译管道：`"translation_xx_to_yy"`。

该 pipeline 可以使用的模型是在翻译任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?filter=translation) 上的可用模型的最新列表。有关可用参数的列表，请参阅[以下文档](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)

用法：

```py
en_fr_translator = pipeline("translation_en_to_fr")
en_fr_translator("How old are you?")
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L341)

```py
( *args **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `args` (`str` 或 `List[str]`) — 要翻译的文本。

+   `return_tensors` (`bool`, *optional*, 默认为 `False`) — 是否在输出中包含预测的张量（作为 token 索引）。

+   `return_text` (`bool`, *optional*, 默认为 `True`) — 是否在输出中包含解码后的文本。

+   `clean_up_tokenization_spaces` (`bool`, *optional*, 默认为 `False`) — 是否清理文本输出中的潜在额外空格。

+   `src_lang` (`str`, *optional*) — 输入文本的语言。对于多语言模型可能是必需的。对于单对翻译模型没有任何效果

+   `tgt_lang` (`str`, *optional*) — 期望输出的语言。对于多语言模型可能是必需的。对于单对翻译模型没有任何效果 generate_kwargs — 传递给模型的 generate 方法的其他关键字参数（请参阅您框架对应的 generate 方法此处）。

返回值

一个 `dict` 列表或 `dict` 列表的列表

每个结果都作为一个带有以下键的字典：

+   `translation_text` (`str`, 当 `return_text=True` 时存在) — 翻译结果。

+   `translation_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True` 时存在) — 翻译的 token ids。

翻译给定的文本。

### ZeroShotClassificationPipeline

### `class transformers.ZeroShotClassificationPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L46)

```py
( args_parser = <transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler object at 0x7f3b4482b130> *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 该 pipeline 将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 TensorFlow 则是 TFPreTrainedModel。

+   `tokenizer` (PreTrainedTokenizer) — 该 pipeline 将用于为模型编码数据的分词器。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *optional*) — 为该 pipeline 的模型指定的模型卡片。

+   `framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 用于标识 pipeline 的任务。

+   `num_workers` (`int`, *optional*, 默认为 8) — 当 pipeline 将使用 *DataLoader*（在传递数据集时，在 PyTorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用 *DataLoader*（当传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批量大小，对于推理来说，这并不总是有益的，请阅读 [使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为 -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递原生 `torch.device` 或 `str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 标志，指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生。

基于 NLI 的零样本分类管道使用在 NLI（自然语言推理）任务上训练的 `ModelForSequenceClassification`。相当于 `text-classification` 管道，但这些模型不需要预先确定的潜在类别数量，可以在运行时选择。这通常意味着速度较慢，但**更**灵活。

可以传递任意序列和标签的组合，每个组合将被视为前提/假设对，并传递给预训练模型。然后，*蕴涵* 的逻辑被视为候选标签有效的逻辑。可以使用任何 NLI 模型，但 *蕴涵* 标签的 id 必须包含在模型配置的 :attr:*~transformers.PretrainedConfig.label2id* 中。

示例：

```py
>>> from transformers import pipeline

>>> oracle = pipeline(model="facebook/bart-large-mnli")
>>> oracle(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}

>>> oracle(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["english", "german"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['english', 'german'], 'scores': [0.814, 0.186]}
```

在 pipeline 教程 中了解有关使用管道的基础知识

此 NLI 管道目前可以从 pipeline() 中使用以下任务标识符加载：`"zero-shot-classification"`。

此管道可以使用已在 NLI 任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?search=nli) 上可用模型的最新列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L163)

```py
( sequences: Union *args **kwargs ) → export const metadata = 'undefined';A dict or a list of dict
```

参数

+   `sequences` (`str` or `List[str]`) — 要分类的序列，如果模型输入过大，将被截断。

+   `candidate_labels` (`str` or `List[str]`) — 用于将每个序列分类的可能类标签集。可以是单个标签、逗号分隔的标签字符串或标签列表。

+   `hypothesis_template` (`str`, *可选*, 默认为 `"This example is {}."`) — 用于将每个标签转换为 NLI 风格假设的模板。此模板必须包含一个 {} 或类似的语法，以便将候选标签插入模板中。例如，默认模板是 `"This example is {}."`，使用候选标签 `"sports"`，这将被馈送到模型中 `"<cls> sequence to classify <sep> This example is sports . <sep>"`。默认模板在许多情况下效果很好，但根据任务设置的不同，尝试不同模板可能是值得的。

+   `multi_label` (`bool`, *可选*, 默认为 `False`) — 是否可以存在多个候选标签为真。如果为 `False`，则对每个序列的标签可能性进行归一化，使得每个序列的标签可能性之和为 1。如果为 `True`，则将标签视为独立的，并通过对蕴涵分数与矛盾分数进行 softmax，对每个候选进行概率归一化。

返回

一个 `dict` 或 `dict` 列表

每个结果都作为一个带有以下键的字典：

+   `sequence` (`str`) — 这是输出的序列。

+   `labels` (`List[str]`) — 按可能性顺序排序的标签。

+   `scores` (`List[float]`) — 每个标签的概率。

对给定的序列进行分类。有关更多信息，请参阅 ZeroShotClassificationPipeline 文档。

## 多模态

可用于多模态任务的管道包括以下内容。

### 文档问答管道

### `class transformers.DocumentQuestionAnsweringPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L101)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *optional*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`，默认为 `""`) — 管道的任务标识符。

+   `num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，对于 Pytorch 模型在 GPU 上），要使用的工作人员数量。

+   `batch_size` (`int`, *optional*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，对于 Pytorch 模型在 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *optional*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, 默认为 -1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output` (`bool`, *optional*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用任何`AutoModelForDocumentQuestionAnswering`的文档问答管道。输入/输出与(抽取式)问答管道类似；但是，该管道将图像（和可选的 OCR 单词/框）作为输入，而不是文本上下文。

示例：

```py
>>> from transformers import pipeline

>>> document_qa = pipeline(model="impira/layoutlm-document-qa")
>>> document_qa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

了解有关在 pipeline 教程中使用管道的基础知识

此文档问答管道目前可以使用以下任务标识符从 pipeline()加载：`"document-question-answering"`。

此管道可以使用已在文档问答任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=document-question-answering)上可用模型的最新列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L194)

```py
( image: Union question: Optional = None word_boxes: Tuple = None **kwargs ) → export const metadata = 'undefined';A dict or a list of dict
```

参数

+   `image` (`str`或`PIL.Image`) — 管道处理三种类型的图像：

    +   包含指向图像的 http 链接的字符串

    +   包含图像本地路径的字符串

    +   直接在 PIL 中加载的图像

    管道接受单个图像或一批图像。如果给定单个图像，则可以广播到多个问题。

+   `question` (`str`) — 要问的问题。

+   `word_boxes` (`List[str, Tuple[float, float, float, float]]`, *可选*) — 一组单词和边界框（标准化为 0->1000）。如果提供此可选输入，则管道将使用这些单词和框，而不是在图像上运行 OCR 来为需要它们的模型（例如 LayoutLM）派生它们。这允许您在管道的许多调用之间重用 OCR 的结果，而无需每次重新运行它。

+   `top_k` (`int`, *可选*, 默认为 1) — 要返回的答案数量（将按可能性顺序选择）。请注意，如果在上下文中没有足够的选项可用，我们将返回少于 top_k 个答案。

+   `doc_stride` (`int`, *可选*, 默认为 128) — 如果文档中的单词太长，无法与模型的问题匹配，它将被分成几个具有一些重叠的块。此参数控制该重叠的大小。

+   `max_answer_len` (`int`, *可选*, 默认为 15) — 预测答案的最大长度（例如，只考虑长度较短的答案）。

+   `max_seq_len` (`int`, *可选*, 默认为 384) — 每个传递给模型的块中的总句子长度（上下文+问题）的最大长度（以标记为单位）。如果需要，上下文将被分成几个块（使用`doc_stride`作为重叠）。

+   `max_question_len` (`int`, *可选*, 默认为 64) — 问题在标记化后的最大长度。如果需要，将被截断。

+   `handle_impossible_answer` (`bool`, *可选*, 默认为`False`) — 是否接受不可能作为答案。

+   `lang` (`str`, *可选*) — 运行 OCR 时要使用的语言。默认为英语。

+   `tesseract_config` (`str`, *可选*) — 在运行 OCR 时传递给 tesseract 的附加标志。

+   `timeout` (`float`, *可选*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能会永远阻塞。

返回

一个`dict`或一个`dict`列表

每个结果都作为一个带有以下键的字典：

+   `score` (`float`) — 与答案相关联的概率。

+   `start` (`int`) — 答案的开始单词索引（在输入的 OCR 版本或提供的`word_boxes`中）。

+   `end` (`int`) — 答案的结束单词索引（在输入的 OCR 版本或提供的`word_boxes`中）。

+   `answer` (`str`) — 问题的答案。

+   `words` (`list[int]`) — 答案中每个单词/框对的索引

通过使用文档回答输入的问题。文档被定义为一幅图像和一个可选的（单词，框）元组列表，表示文档中的文本。如果未提供`word_boxes`，它将使用 Tesseract OCR 引擎（如果可用）自动提取单词和框，以供需要它们作为输入的 LayoutLM 类似模型使用。对于 Donut，不运行 OCR。

您可以以多种方式调用管道：

+   `pipeline(image=image, question=question)`

+   `pipeline(image=image, question=question, word_boxes=word_boxes)`

+   `pipeline([{"image": image, "question": question}])`

+   `pipeline([{"image": image, "question": question, "word_boxes": word_boxes}])`

### FeatureExtractionPipeline

### `class transformers.FeatureExtractionPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L7)

```py
( model: Union tokenizer: Optional = None feature_extractor: Optional = None image_processor: Optional = None modelcard: Optional = None framework: Optional = None task: str = '' args_parser: ArgumentHandler = None device: Union = None torch_dtype: Union = None binary_output: bool = False **kwargs )
```

参数

+   `模型` (PreTrainedModel 或 TFPreTrainedModel) — 流水线将使用的模型来进行预测。这需要是一个继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel。

+   `分词器` (PreTrainedTokenizer) — 流水线将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `模型卡` (`str` 或 `ModelCard`，*可选*) — 为此流水线的模型指定的模型卡。

+   `框架` (`str`，*可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用模型的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `return_tensors` (`bool`，*可选*) — 如果为 `True`，则根据指定的框架返回一个张量，否则返回一个列表。

+   `任务` (`str`，默认为 `""`) — 用于流水线的任务标识符。

+   `args_parser` (ArgumentHandler，*可选*) — 负责解析提供的流水线参数的对象的引用。

+   `设备` (`int`，*可选*，默认为 -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。

+   `tokenize_kwargs` (`dict`，*可选*) — 传递给分词器的额外关键字参数的字典。

使用没有模型头的特征提取流水线。此流水线从基础变换器中提取隐藏状态，可以用作下游任务中的特征。

示例：

```py
>>> from transformers import pipeline

>>> extractor = pipeline(model="bert-base-uncased", task="feature-extraction")
>>> result = extractor("This is a simple test.", return_tensors=True)
>>> result.shape  # This is a tensor of shape [1, sequence_lenth, hidden_dimension] representing the input string.
torch.Size([1, 8, 768])
```

了解有关在 流水线教程 中使用流水线的基础知识

当前可以使用任务标识符 `"feature-extraction"` 从 pipeline() 加载此特征提取流水线。

所有模型都可以用于此流水线。查看包括社区贡献模型在内的所有模型列表，请访问 [huggingface.co/models](https://huggingface.co/models)。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L96)

```py
( *args **kwargs ) → export const metadata = 'undefined';A nested list of float
```

参数

+   `args` (`str` 或 `List[str]`) — 一个或多个文本（或一个文本列表）以获取特征。

返回

一个嵌套的 `float` 列表

模型计算的特征。

提取输入的特征。

### ImageToTextPipeline

### `class transformers.ImageToTextPipeline`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L30)

```py
( *args **kwargs )
```

参数

+   `模型` (PreTrainedModel 或 TFPreTrainedModel) — 流水线将使用的模型来进行预测。这需要是一个继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel。

+   `tokenizer`（PreTrainedTokenizer） — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str`或`ModelCard`，*optional*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`，*optional*) — 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`，默认为`""`) — 管道的任务标识符。

+   `num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的工作程序数量。

+   `batch_size` (`int`, *optional*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，在 Pytorch 模型的 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*optional*） — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *optional*, 默认为-1) — 用于 CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机的`torch.device`或`str`。

+   `binary_output` (`bool`, *optional*, 默认为`False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。

使用`AutoModelForVision2Seq`的图像到文本管道。此管道为给定图像预测标题。

示例：

```py
>>> from transformers import pipeline

>>> captioner = pipeline(model="ydshieh/vit-gpt2-coco-en")
>>> captioner("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'generated_text': 'two birds are standing next to each other '}]
```

了解有关在 pipeline 教程中使用管道的基础知识

目前可以使用以下任务标识符从 pipeline()加载此图像到文本管道：“image-to-text”。

在[huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-to-text)上查看可用模型列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L83)

```py
( images: Union **kwargs ) → export const metadata = 'undefined';A list or a list of list of dict
```

参数

+   `images` (`str`，`List[str]`，`PIL.Image`或`List[PIL.Image]`) — 管道处理三种类型的图像：

    +   包含指向图像的 HTTP(s)链接的字符串

    +   包含指向图像的本地路径的字符串

    +   直接加载的 PIL 图像

    该管道接受单个图像或一批图像。

+   `max_new_tokens` (`int`, *optional*) — 要生成的最大标记数量。默认情况下，它将使用`generate`的默认值。

+   `generate_kwargs` (`Dict`, *optional*) — 将其传递给`generate`，以便直接将所有这些参数发送到`generate`，从而完全控制此函数。

+   `timeout` (`float`, *optional*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，并且调用可能会永远阻塞。

返回

一个字典列表或字典列表

每个结果都以包含以下键的字典形式呈现：

+   `generated_text` (`str`) — 生成的文本。

为传入的图像分配标签。

### MaskGenerationPipeline

### `class transformers.MaskGenerationPipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L22)

```py
( **kwargs )
```

参数

+   `model`（PreTrainedModel 或 TFPreTrainedModel）- 该模型将被管道用于进行预测。这需要是一个继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer`（PreTrainedTokenizer）- 该分词器将被管道用于为模型编码数据。该对象继承自 PreTrainedTokenizer。

+   `feature_extractor`（SequenceFeatureExtractor）- 该特征提取器将被管道用于对输入进行编码。

+   `points_per_batch`（*可选*，int，默认为 64）- 设置模型同时运行的点数。较高的数字可能更快，但会使用更多的 GPU 内存。

+   `output_bboxes_mask`（`bool`，*可选*，默认为`False`）- 是否输出边界框预测。

+   `output_rle_masks`（`bool`，*可选*，默认为`False`）- 是否输出`RLE`格式的掩码

+   `model`（PreTrainedModel 或 TFPreTrainedModel）- 该模型将被管道用于进行预测。这需要是一个继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer`（PreTrainedTokenizer）- 该分词器将被管道用于为模型编码数据。该对象继承自 PreTrainedTokenizer。

+   `modelcard`（`str`或`ModelCard`，*可选*）- 为该管道的模型指定的模型卡。

+   `framework`（`str`，*可选*）- 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task`（`str`，默认为`""`）- 用于该管道的任务标识符。

+   `num_workers`（`int`，*可选*，默认为 8）- 当管道将使用*DataLoader*（在传递数据集时，对于 PyTorch 模型在 GPU 上），要使用的工作程序数量。

+   `batch_size`（`int`，*可选*，默认为 1）- 当管道将使用*DataLoader*（在传递数据集时，对于 PyTorch 模型在 GPU 上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser`（ArgumentHandler，*可选*）- 负责解析提供的管道参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）- CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或一个`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）—指示流水线输出是否以二进制格式（即 pickle）或原始文本格式发生的标志。

使用`SamForMaskGeneration`为图像生成自动蒙版。该流水线预测图像的二进制蒙版，给定一个图像。这是一个`ChunkPipeline`，因为您可以将小批量中的点分开，以避免 OOM 问题。使用`points_per_batch`参数来控制同时处理的点数。默认值为`64`。

该流水线分为 3 个步骤：

1.  `preprocess`：生成 1024 个均匀分隔的点网格，以及边界框和点标签。有关如何创建点和边界框的详细信息，请检查`_generate_crop_boxes`函数。还使用`image_processor`对图像进行预处理。此函数`yield`一个`points_per_batch`的小批量。

1.  `forward`：将`preprocess`的输出馈送到模型。图像嵌入仅计算一次。调用`self.model.get_image_embeddings`并确保不计算梯度，张量和模型在同一设备上。

1.  `postprocess`：自动蒙版生成的最重要部分发生在这里。引入了三个步骤：

    +   image_processor.postprocess_masks（在每个小批量循环中运行）：接受原始输出蒙版，根据图像大小调整其大小，并将其转换为二进制蒙版。

    +   image_processor.filter_masks（在每个小批量循环中）：同时使用`pred_iou_thresh`和`stability_scores`。还应用基于非最大抑制的各种过滤器，以消除不良蒙版。

    +   image_processor.postprocess_masks_for_amg 将 NSM 应用于蒙版，仅保留相关的蒙版。

示例：

```py
>>> from transformers import pipeline

>>> generator = pipeline(model="facebook/sam-vit-base", task="mask-generation")
>>> outputs = generator(
...     "http://images.cocodataset.org/val2017/000000039769.jpg",
... )

>>> outputs = generator(
...     "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png", points_per_batch=128
... )
```

了解有关在 pipeline 教程中使用流水线的基础知识

当前可以使用以下任务标识符从 pipeline()加载此分割流水线：`"mask-generation"`。

在[huggingface.co/models](https://huggingface.co/models?filter=mask-generation)上查看可用模型的列表。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L135)

```py
( image *args num_workers = None batch_size = None **kwargs ) → export const metadata = 'undefined';Dict
```

参数

+   `inputs`（`np.ndarray`或`bytes`或`str`或`dict`）—图像或图像列表。

+   `mask_threshold`（`float`，*可选*，默认为 0.0）—将预测的蒙版转换为二进制值时使用的阈值。

+   `pred_iou_thresh`（`float`，*可选*，默认为 0.88）—在`[0,1]`上应用于模型预测的蒙版质量的过滤阈值。

+   `stability_score_thresh`（`float`，*可选*，默认为 0.95）—在`[0,1]`中的过滤阈值，使用蒙版在截止值变化下的稳定性来对模型的蒙版预测进行二值化。

+   `stability_score_offset`（`int`，*可选*，默认为 1）—在计算稳定性分数时，偏移截止值的量。

+   `crops_nms_thresh`（`float`，*可选*，默认为 0.7）—非极大值抑制使用的框 IoU 截止值，用于过滤重复蒙版。

+   `crops_n_layers`（`int`，*可选*，默认为 0）—如果`crops_n_layers>0`，将再次在图像的裁剪上运行蒙版预测。设置要运行的层数，其中每一层具有 2**i_layer 数量的图像裁剪。

+   `crop_overlap_ratio`（`float`，*可选*，默认为`512 / 1500`）—设置裁剪重叠的程度。在第一层裁剪中，裁剪将以图像长度的这一部分重叠。具有更多裁剪的后续层会缩小此重叠。

+   `crop_n_points_downscale_factor`（`int`，*可选*，默认为`1`）—在第 n 层中采样的每边点数按`crop_n_points_downscale_factor**n`缩小。

+   `timeout`（`float`，*可选*，默认为 None）—从网络获取图像的最长时间（以秒为单位）。如果为 None，则不设置超时，调用可能永远阻塞。

返回

`Dict`

一个具有以下键的字典：

+   `mask` (`PIL.Image`) — 检测到的对象的二进制掩模，作为原始图像的形状为`(width, height)`的 PIL 图像。如果未找到对象，则返回填充有零的掩模。

+   `score` (*可选* `float`) — 可选地，当模型能够估计由标签和掩模描述的“对象”的置信度时。

生成二进制分割掩模

### VisualQuestionAnsweringPipeline

### `class transformers.VisualQuestionAnsweringPipeline`

[`源代码`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L18)

```py
( *args **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是继承自 PreTrainedModel（对于 PyTorch）和 TFPreTrainedModel（对于 TensorFlow）的模型。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。此对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。

+   `framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示 PyTorch 或`"tf"`表示 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 管道的任务标识符。

+   `num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（在 GPU 上为 Pytorch 模型传递数据集时）时，要使用的工作程序数量。

+   `batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（在 GPU 上为 Pytorch 模型传递数据集时）时，要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。

+   `args_parser` (ArgumentHandler, *可选*) — 负责解析提供的管道参数的对象的引用。

+   `device` (`int`, *可选*, 默认为 -1) — CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递原生的`torch.device`或`str`。

+   `binary_output` (`bool`, *可选*, 默认为 `False`) — 标志指示管道输出是否以二进制格式（即 pickle）或原始文本形式发生。

使用`AutoModelForVisualQuestionAnswering`的视觉问答管道。此管道目前仅在 PyTorch 中可用。

示例：

```py
>>> from transformers import pipeline

>>> oracle = pipeline(model="dandelin/vilt-b32-finetuned-vqa")
>>> image_url = "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png"
>>> oracle(question="What is she wearing ?", image=image_url)
[{'score': 0.948, 'answer': 'hat'}, {'score': 0.009, 'answer': 'fedora'}, {'score': 0.003, 'answer': 'clothes'}, {'score': 0.003, 'answer': 'sun hat'}, {'score': 0.002, 'answer': 'nothing'}]

>>> oracle(question="What is she wearing ?", image=image_url, top_k=1)
[{'score': 0.948, 'answer': 'hat'}]

>>> oracle(question="Is this a person ?", image=image_url, top_k=1)
[{'score': 0.993, 'answer': 'yes'}]

>>> oracle(question="Is this a man ?", image=image_url, top_k=1)
[{'score': 0.996, 'answer': 'no'}]
```

在 pipeline 教程中了解如何使用管道的基础知识

此视觉问答管道目前可以从 pipeline()中加载，使用以下任务标识符："visual-question-answering", "vqa"。

此管道可以使用已在视觉问答任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=visual-question-answering)上可用模型的最新列表。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L70)

```py
( image: Union question: str = None **kwargs ) → export const metadata = 'undefined';A dictionary or a list of dictionaries containing the result. The dictionaries contain the following keys
```

参数

+   `image` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：

    +   包含指向图像的 http 链接的字符串

    +   包含本地图像路径的字符串

    +   直接加载的 PIL 图像

    该管道接受单个图像或一批图像。如果给定单个图像，则可以广播到多个问题。

+   `question` (`str`, `List[str]`) — 提出的问题。如果给定单个问题，则可以广播到多个图像。

+   `top_k` (`int`, *optional*, 默认为 5) — 管道将返回的前 k 个标签的数量。如果提供的数字高于模型配置中可用的标签数量，则默认为标签数量。

+   `timeout` (`float`, *optional*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能会永远阻塞。

返回

包含结果的字典或字典列表。字典包含以下键

+   `label` (`str`) — 模型识别的标签。

+   `score` (`int`) — 模型为该标签分配的分数。

回答关于图像的开放性问题。该管道接受下面详细说明的几种类型的输入：

+   `pipeline(image=image, question=question)`

+   `pipeline({"image": image, "question": question})`

+   `pipeline([{"image": image, "question": question}])`

+   `pipeline([{"image": image, "question": question}, {"image": image, "question": question}])`

## 父类：Pipeline

### `class transformers.Pipeline`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L749)

```py
( model: Union tokenizer: Optional = None feature_extractor: Optional = None image_processor: Optional = None modelcard: Optional = None framework: Optional = None task: str = '' args_parser: ArgumentHandler = None device: Union = None torch_dtype: Union = None binary_output: bool = False **kwargs )
```

参数

+   `model` (PreTrainedModel 或 TFPreTrainedModel) — 管道将用于进行预测的模型。这需要是一个继承自 PreTrainedModel 的模型，对于 PyTorch 是 TFPreTrainedModel。

+   `tokenizer` (PreTrainedTokenizer) — 管道将用于为模型编码数据的分词器。该对象继承自 PreTrainedTokenizer。

+   `modelcard` (`str` 或 `ModelCard`, *optional*) — 为该管道的模型分配的模型卡。

+   `framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。

    如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，将默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。

+   `task` (`str`, 默认为 `""`) — 用于管道的任务标识符。

+   `num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch 模型上使用 GPU），要使用的工作进程数。

+   `batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch 模型上使用 GPU），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) 。

+   `args_parser`（ArgumentHandler，*可选*）-负责解析提供的流水线参数的对象的引用。

+   `device`（`int`，*可选*，默认为-1）-CPU/GPU 支持的设备序数。将其设置为-1 将利用 CPU，正数将在关联的 CUDA 设备 ID 上运行模型。您也可以传递本机`torch.device`或`str`。

+   `binary_output`（`bool`，*可选*，默认为`False`）-指示流水线输出应以二进制格式（即 pickle）或原始文本发生的标志。

Pipeline 类是所有流水线继承的类。请参考此类以获取不同流水线共享的方法。

实现流水线操作的基类。流水线工作流定义为以下操作序列：

输入->标记化->模型推断->后处理（任务相关）->输出

Pipeline 支持通过设备参数在 CPU 或 GPU 上运行（见下文）。

某些流水线，例如 FeatureExtractionPipeline（`'feature-extraction'`）将大张量对象输出为嵌套列表。为了避免将这样大的结构转储为文本数据，我们提供了`binary_output`构造参数。如果设置为`True`，输出将以 pickle 格式存储。

#### `check_model_type`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L984)

```py
( supported_models: Union )
```

参数

+   `supported_models`（`List[str]`或`dict`）-流水线支持的模型列表，或具有模型类值的字典。

检查模型类是否受流水线支持。

#### `device_placement`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L923)

```py
( )
```

上下文管理器，以框架不可知的方式在用户指定的设备上分配张量。

示例：

```py
# Explicitly ask for tensor allocation on CUDA device :0
pipe = pipeline(..., device=0)
with pipe.device_placement():
    # Every framework specific tensor allocation will be done on the request device
    output = pipe(...)
```

#### `ensure_tensor_on_device`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L950)

```py
( **inputs ) → export const metadata = 'undefined';Dict[str, torch.Tensor]
```

参数

+   `inputs`（应为`torch.Tensor`的关键字参数，其余部分将被忽略）-要放置在`self.device`上的张量。

+   仅对列表进行**递归**。

返回

`Dict[str, torch.Tensor]`

与`inputs`相同，但在适当的设备上。

确保 PyTorch 张量位于指定设备上。

#### `postprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1047)

```py
( model_outputs: ModelOutput **postprocess_parameters: Dict )
```

后处理将接收`_forward`方法的原始输出，通常是张量，并将其重新格式化为更友好的形式。通常它将输出一个包含字符串和数字的列表或结果字典。

#### `predict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L917)

```py
( X )
```

Scikit / Keras 接口到 transformers 的流水线。此方法将转发到**call**()。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1026)

```py
( input_: Any **preprocess_parameters: Dict )
```

预处理将获取特定流水线的`input_`并返回一个包含一切必要内容以使`_forward`正确运行的字典。它应至少包含一个张量，但可能有任意其他项目。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L861)

```py
( save_directory: str safe_serialization: bool = True )
```

参数

+   `save_directory`（`str`）-要保存的目录的路径。如果不存在，将创建它。

+   `safe_serialization`（`str`）-是否使用`safetensors`保存模型，还是使用 PyTorch 或 Tensorflow 的传统方式。

保存流水线的模型和分词器。

#### `transform`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L911)

```py
( X )
```

Scikit / Keras 接口到 transformers 的管道。这种方法将转发到**call**()。
