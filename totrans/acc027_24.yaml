- en: DeepSpeed
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/deepspeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described
    in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Some of the salient optimizations
    are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer state partitioning (ZeRO stage 1)
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient partitioning (ZeRO stage 2)
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameter partitioning (ZeRO stage 3)
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom mixed precision training handling
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A range of fast CUDA-extension-based optimizers
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO-Offload to CPU and Disk/NVMe
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical partitioning of model parameters (ZeRO++)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale
    Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described
    in the paper [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
    Learning](https://arxiv.org/abs/2104.07857).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-2 is primarily used only for training, as its features are of
    no use to inference.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-3 can be used for inference as well since it allows huge models
    to be loaded on multiple GPUs, which won’t be possible on a single GPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '🤗 Accelerate integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    via 2 options:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Integration of the DeepSpeed features via `deepspeed config file` specification
    in `accelerate config` . You just supply your custom config file or use our template.
    Most of this document is focused on this feature. This supports all the core features
    of DeepSpeed and gives user a lot of flexibility. User may have to change a few
    lines of code depending on the config.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integration via `deepspeed_plugin`.This supports subset of the DeepSpeed features
    and uses default options for the rest of the configurations. User need not change
    any code and is good for those who are fine with most of the default settings
    of DeepSpeed.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is integrated?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Accelerate integrates all features of DeepSpeed ZeRO. This includes all the
    ZeRO stages 1, 2 and 3 as well as ZeRO-Offload, ZeRO-Infinity (which can offload
    to disk/NVMe) and ZeRO++. Below is a short description of Data Parallelism using
    ZeRO - Zero Redundancy Optimizer along with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
    ![ZeRO Data Parallelism](../Images/61da03a3a1a0e2c5e704642f87f2f216.png)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'a. **Stage 1** : Shards optimizer states across data parallel workers/GPUs'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'b. **Stage 2** : Shards optimizer states + gradients across data parallel workers/GPUs'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'c. **Stage 3**: Shards optimizer states + gradients + model parameters across
    data parallel workers/GPUs'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'd. **Optimizer Offload**: Offloads the gradients + optimizer states to CPU/Disk
    building on top of ZERO Stage 2'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'e. **Param Offload**: Offloads the model parameters to CPU/Disk building on
    top of ZERO Stage 3'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'f. **Hierarchical Partitioning**: Enables efficient multi-node training with
    data-parallel training across nodes and ZeRO-3 sharding within a node, built on
    top of ZeRO Stage 3.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: With respect to Disk Offload, the disk should be an NVME for decent speed
    but it technically works on any Disk'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesn’t use an optimizer and a lr scheduler
    and only stage 3 is relevant. For more details see: [deepspeed-zero-inference](#deepspeed-zero-inference).'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pre-Requisites**: Install DeepSpeed version >=0.6.5\. Please refer to the
    [DeepSpeed Installation details](https://github.com/microsoft/DeepSpeed#installation)
    for more information.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: We will first look at easy to use integration via `accelerate config`. Followed
    by more flexible and feature rich `deepspeed config file` integration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看一下通过`accelerate config`进行易于使用的集成。然后是更灵活和功能丰富的`deepspeed配置文件`集成。
- en: Accelerate DeepSpeed Plugin
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加速DeepSpeed插件
- en: 'On your machine(s) just run:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的机器上运行：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: and answer the questions asked. It will ask whether you want to use a config
    file for DeepSpeed to which you should answer no. Then answer the following questions
    to generate a basic DeepSpeed config. This will generate a config file that will
    be used automatically to properly set the default options when doing
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 并回答提出的问题。它会询问您是否要使用一个DeepSpeed配置文件，您应该回答否。然后回答以下问题以生成基本的DeepSpeed配置。这将生成一个配置文件，将自动用于在执行时正确设置默认选项
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For instance, here is how you would run the NLP example `examples/nlp_example.py`
    (from the root of the repo) with DeepSpeed Plugin:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是如何在NLP示例`examples/nlp_example.py`（从存储库的根目录）中使用DeepSpeed插件运行的：
- en: '**ZeRO Stage-2 DeepSpeed Plugin Example**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeRO Stage-2 DeepSpeed插件示例**'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**ZeRO Stage-3 with CPU Offload DeepSpeed Plugin Example**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**带CPU卸载的ZeRO Stage-3 DeepSpeed插件示例**'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Currently, `Accelerate` supports following config through the CLI:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`Accelerate`通过CLI支持以下配置：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To be able to tweak more options, you will need to use a DeepSpeed config file.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够调整更多选项，您需要使用DeepSpeed配置文件。
- en: DeepSpeed Config File
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepSpeed配置文件
- en: 'On your machine(s) just run:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的机器上运行：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: and answer the questions asked. It will ask whether you want to use a config
    file for deepspeed to which you answer yes and provide the path to the deepspeed
    config file. This will generate a config file that will be used automatically
    to properly set the default options when doing
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并回答提出的问题。它会询问您是否要使用一个DeepSpeed配置文件，您需要回答是，并提供DeepSpeed配置文件的路径。这将生成一个配置文件，将自动用于在执行时正确设置默认选项
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For instance, here is how you would run the NLP example `examples/by_feature/deepspeed_with_config_support.py`
    (from the root of the repo) with DeepSpeed Config File:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是如何在NLP示例`examples/by_feature/deepspeed_with_config_support.py`（从存储库的根目录）中使用DeepSpeed配置文件运行的：
- en: '**ZeRO Stage-2 DeepSpeed Config File Example**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeRO Stage-2 DeepSpeed配置文件示例**'
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'with the contents of `zero_stage2_config.json` being:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`zero_stage2_config.json`的内容为：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**ZeRO Stage-3 with CPU offload DeepSpeed Config File Example**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**带CPU卸载的ZeRO Stage-3 DeepSpeed配置文件示例**'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'with the contents of `zero_stage3_offload_config.json` being:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`zero_stage3_offload_config.json`的内容为：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**ZeRO++ Config Example** You can use the the features of ZeRO++ by using the
    appropriate config parameters. Note that ZeRO++ is an extension for ZeRO Stage
    3\. Here is how the config file can be modified, from [DeepSpeed’s ZeRO++ tutorial](https://www.deepspeed.ai/tutorials/zeropp/):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeRO++配置示例** 您可以通过使用适当的配置参数来使用ZeRO++的功能。请注意，ZeRO++是ZeRO Stage 3的扩展。以下是如何修改配置文件，来自[DeepSpeed的ZeRO++教程](https://www.deepspeed.ai/tutorials/zeropp/)：'
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For hierarchical partitioning, the partition size `zero_hpz_partition_size`
    should ideally be set to the number of GPUs per node. (For example, the above
    config file assumes 8 GPUs per node)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分层分区，分区大小`zero_hpz_partition_size`理想情况下应设置为每个节点的GPU数量。（例如，上述配置文件假定每个节点有8个GPU）
- en: '**Important code changes when using DeepSpeed Config File**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DeepSpeed配置文件时的重要代码更改
- en: DeepSpeed Optimizers and Schedulers. For more information on these, see the
    [DeepSpeed Optimizers](https://deepspeed.readthedocs.io/en/latest/optimizers.html)
    and [DeepSpeed Schedulers](https://deepspeed.readthedocs.io/en/latest/schedulers.html)
    documentation. We will look at the changes needed in the code when using these.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepSpeed优化器和调度器。有关更多信息，请参阅[DeepSpeed优化器](https://deepspeed.readthedocs.io/en/latest/optimizers.html)和[DeepSpeed调度器](https://deepspeed.readthedocs.io/en/latest/schedulers.html)文档。我们将看到在使用这些时代码需要的更改。
- en: 'a. DS Optim + DS Scheduler: The case when both `optimizer` and `scheduler`
    keys are present in the DeepSpeed config file. In this situation, those will be
    used and the user has to use `accelerate.utils.DummyOptim` and `accelerate.utils.DummyScheduler`
    to replace the PyTorch/Custom optimizers and schedulers in their code. Below is
    the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing
    this:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. DS Optim + DS Scheduler：当DeepSpeed配置文件中同时存在`optimizer`和`scheduler`键时。在这种情况下，这些将被使用，用户必须使用`accelerate.utils.DummyOptim`和`accelerate.utils.DummyScheduler`来替换他们代码中的PyTorch/自定义优化器和调度器。以下是`examples/by_feature/deepspeed_with_config_support.py`中显示的片段：
- en: '[PRE16]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'b. Custom Optim + Custom Scheduler: The case when both `optimizer` and `scheduler`
    keys are absent in the DeepSpeed config file. In this situation, no code changes
    are needed from the user and this is the case when using integration via DeepSpeed
    Plugin. In the above example we can see that the code remains unchanged if the
    `optimizer` and `scheduler` keys are absent in the DeepSpeed config file.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. Custom Optim + Custom Scheduler：当DeepSpeed配置文件中`optimizer`和`scheduler`键都不存在时。在这种情况下，用户不需要进行任何代码更改，这是使用DeepSpeed插件进行集成时的情况。在上面的示例中，我们可以看到如果DeepSpeed配置文件中不存在`optimizer`和`scheduler`键，则代码保持不变。
- en: 'c. Custom Optim + DS Scheduler: The case when only `scheduler` key is present
    in the DeepSpeed config file. In this situation, the user has to use `accelerate.utils.DummyScheduler`
    to replace the PyTorch/Custom scheduler in their code.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. Custom Optim + DS Scheduler：当DeepSpeed配置文件中只有`scheduler`键存在时。在这种情况下，用户必须使用`accelerate.utils.DummyScheduler`来替换他们代码中的PyTorch/自定义调度器。
- en: 'd. DS Optim + Custom Scheduler: The case when only `optimizer` key is present
    in the DeepSpeed config file. This will result in an error because you can only
    use DS Scheduler when using DS Optim.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. DS Optim + Custom Scheduler：当DeepSpeed配置文件中只有`optimizer`键存在时。这将导致错误，因为只有在使用DS
    Optim时才能使用DS Scheduler。
- en: Notice the `auto` values in the above example DeepSpeed config files. These
    are automatically handled by `prepare` method based on model, dataloaders, dummy
    optimizer and dummy schedulers provided to `prepare` method. Only the `auto` fields
    specified in above examples are handled by `prepare` method and the rest have
    to be explicitly specified by the user.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意上述示例DeepSpeed配置文件中的`auto`值。这些值由`prepare`方法根据提供给`prepare`方法的模型、数据加载器、虚拟优化器和虚拟调度器自动处理。上述示例中指定的`auto`字段由`prepare`方法处理，其余字段必须由用户显式指定。
- en: 'The `auto` values are calculated as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`auto`值的计算如下：'
- en: '`reduce_bucket_size`: `hidden_size * hidden_size`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_bucket_size`: `hidden_size * hidden_size`'
- en: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
- en: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
- en: For the `auto` feature to work for these 3 config entries - Accelerate will
    use `model.config.hidden_size` or `max(model.config.hidden_sizes)` as `hidden_size`.
    If neither of these is available, the launching will fail and you will have to
    set these 3 config entries manually. Remember the first 2 config entries are the
    communication buffers - the larger they are the more efficient the comms will
    be, and the larger they are the more GPU memory they will consume, so it’s a tunable
    performance trade-off.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这3个配置条目的`auto`功能正常工作 - Accelerate将使用`model.config.hidden_size`或`max(model.config.hidden_sizes)`作为`hidden_size`。如果这两者都不可用，启动将失败，您将不得不手动设置这3个配置条目。请记住，前两个配置条目是通信缓冲区
    - 它们越大，通信效率就越高，它们越大，GPU内存消耗就越大，因此这是一个可调整的性能权衡。
- en: '**Things to note when using DeepSpeed Config File**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用DeepSpeed配置文件时需要注意的事项**'
- en: Below is a sample script using `deepspeed_config_file` in different scenarios.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在不同场景中使用`deepspeed_config_file`的示例脚本。
- en: 'Code `test.py`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 代码`test.py`：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Scenario 1**: Manually tampered accelerate config file having `deepspeed_config_file`
    along with other entries.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景1**：手动篡改的带有`deepspeed_config_file`的accelerate配置文件以及其他条目。'
- en: 'Content of the `accelerate` config:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`accelerate`配置的内容：'
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`ds_config.json`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ds_config.json`：'
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output of `accelerate launch test.py`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`accelerate launch test.py`的输出：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Scenario 2**: Use the solution of the error to create new accelerate config
    and check that no ambiguity error is now thrown.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景2**：使用错误的解决方案创建新的accelerate配置，并检查是否现在不会抛出任何歧义错误。'
- en: 'Run `accelerate config`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`accelerate config`：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Content of the `accelerate` config:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`accelerate`配置的内容：'
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output of `accelerate launch test.py`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`accelerate launch test.py`的输出：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Scenario 3**: Setting the `accelerate launch` command arguments related to
    DeepSpeed as `"auto"` in the DeepSpeed` configuration file and check that things
    work as expected.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景3**：在DeepSpeed配置文件中将与DeepSpeed相关的`accelerate launch`命令参数设置为`"auto"`，并检查事情是否按预期工作。'
- en: 'New `ds_config.json` with `"auto"` for the `accelerate launch` DeepSpeed command
    arguments:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`"auto"`为`accelerate launch` DeepSpeed命令参数创建新的`ds_config.json`：
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Output of `accelerate launch --mixed_precision="fp16" --zero_stage=3 --gradient_accumulation_steps=5
    --gradient_clipping=1.0 --offload_param_device="cpu" --offload_optimizer_device="nvme"
    --zero3_save_16bit_model="true" test.py`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`accelerate launch --mixed_precision="fp16" --zero_stage=3 --gradient_accumulation_steps=5
    --gradient_clipping=1.0 --offload_param_device="cpu" --offload_optimizer_device="nvme"
    --zero3_save_16bit_model="true" test.py`的输出：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Note**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：'
- en: Remaining `"auto"` values are handled in `accelerator.prepare()` call as explained
    in point 2 of `Important code changes when using DeepSpeed Config File`.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的`"auto"`值在`accelerator.prepare()`调用中处理，如在使用DeepSpeed配置文件时的重要代码更改的第2点中所解释的。
- en: Only when `gradient_accumulation_steps` is `auto`, the value passed while creating
    `Accelerator` object via `Accelerator(gradient_accumulation_steps=k)` will be
    used. When using DeepSpeed Plugin, the value from it will be used and it will
    overwrite the value passed while creating Accelerator object.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有当`gradient_accumulation_steps`为`auto`时，创建`Accelerator`对象时通过`Accelerator(gradient_accumulation_steps=k)`传递的值才会被使用。使用DeepSpeed插件时，将使用其中的值，并且它将覆盖创建Accelerator对象时传递的值。
- en: Saving and loading
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载
- en: Saving and loading of models is unchanged for ZeRO Stage-1 and Stage-2.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于ZeRO Stage-1和Stage-2，模型的保存和加载保持不变。
- en: 'under ZeRO Stage-3, `state_dict` contains just the placeholders since the model
    weights are partitioned across multiple GPUs. ZeRO Stage-3 has 2 options:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在ZeRO Stage-3下，`state_dict`仅包含占位符，因为模型权重被分区到多个GPU上。ZeRO Stage-3有2个选项：
- en: 'a. Saving the entire 16bit model weights to directly load later on using `model.load_state_dict(torch.load(pytorch_model.bin))`.
    For this, either set `zero_optimization.stage3_gather_16bit_weights_on_model_save`
    to True in DeepSpeed Config file or set `zero3_save_16bit_model` to True in DeepSpeed
    Plugin. **Note that this option requires consolidation of the weights on one GPU
    it can be slow and memory demanding, so only use this feature when needed.** Below
    is the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing
    this:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 将整个16位模型权重保存以便以后直接加载，使用`model.load_state_dict(torch.load(pytorch_model.bin))`。为此，在DeepSpeed配置文件中将`zero_optimization.stage3_gather_16bit_weights_on_model_save`设置为True，或者在DeepSpeed插件中将`zero3_save_16bit_model`设置为True。**请注意，此选项要求在一个GPU上合并权重，可能会很慢且占用内存，因此只在需要时使用此功能。**以下是`examples/by_feature/deepspeed_with_config_support.py`中显示此功能的片段：
- en: '[PRE26]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'b. To get 32bit weights, first save the model using `model.save_checkpoint()`.
    Below is the snippet from `examples/by_feature/deepspeed_with_config_support.py`
    showing this:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 要获取32位权重，首先使用`model.save_checkpoint()`保存模型。以下是`examples/by_feature/deepspeed_with_config_support.py`中显示此功能的片段：
- en: '[PRE27]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will create ZeRO model and optimizer partitions along with `zero_to_fp32.py`
    script in checkpoint directory. You can use this script to do offline consolidation.
    It requires no configuration files or GPUs. Here is an example of its usage:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在检查点目录中创建ZeRO模型和优化器分区，以及`zero_to_fp32.py`脚本。您可以使用此脚本进行离线合并。它不需要配置文件或GPU。以下是其用法示例：
- en: '[PRE28]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To get 32bit model for saving/inference, you can perform:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要获得用于保存/推理的32位模型，您可以执行：
- en: '[PRE29]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you are only interested in the `state_dict`, you can do the following:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您只对`state_dict`感兴趣，可以执行以下操作：
- en: '[PRE30]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that all these functions require ~2x memory (general RAM) of the size of
    the final checkpoint.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，所有这些功能需要最终检查点大小的大约2倍内存（通用RAM）。
- en: ZeRO Inference
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZeRO 推理
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesn’t use an optimizer and a lr scheduler
    and only stage 3 is relevant. With accelerate integration, you just need to prepare
    the model and dataloader as shown below:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO 推理支持使用ZeRO-Infinity的ZeRO阶段3。它使用与训练相同的ZeRO协议，但不使用优化器和学习率调度程序，只有阶段3相关。通过加速集成，您只需要准备如下所示的模型和数据加载器：
- en: '[PRE31]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Few caveats to be aware of
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要注意的一些注意事项
- en: Current integration doesn’t support Pipeline Parallelism of DeepSpeed.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前集成不支持DeepSpeed的管道并行性。
- en: Current integration doesn’t support `mpu`, limiting the tensor parallelism which
    is supported in Megatron-LM.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前集成不支持`mpu`，限制了在Megatron-LM中支持的张量并行性。
- en: Current integration doesn’t support multiple models.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前集成不支持多个模型。
- en: DeepSpeed Resources
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepSpeed 资源
- en: The documentation for the internals related to deepspeed can be found [here](../package_reference/deepspeed).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有关与deepspeed相关的内部文档可以在[这里](../package_reference/deepspeed)找到。
- en: '[Project’s github](https://github.com/microsoft/deepspeed)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[项目的github](https://github.com/microsoft/deepspeed)'
- en: '[Usage docs](https://www.deepspeed.ai/getting-started/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用文档](https://www.deepspeed.ai/getting-started/)'
- en: '[API docs](https://deepspeed.readthedocs.io/en/latest/index.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[API文档](https://deepspeed.readthedocs.io/en/latest/index.html)'
- en: '[Blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[博客文章](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
- en: 'Papers:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '论文:'
- en: '[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO：朝着训练万亿参数模型的内存优化](https://arxiv.org/abs/1910.02054)'
- en: '[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Offload：民主化亿级规模模型训练](https://arxiv.org/abs/2101.06840)'
- en: '[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Infinity：打破极端规模深度学习的GPU内存壁](https://arxiv.org/abs/2104.07857)'
- en: '[ZeRO++: Extremely Efficient Collective Communication for Giant Model Training](https://arxiv.org/abs/2306.10209)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO++：用于巨型模型训练的极其高效的集体通信](https://arxiv.org/abs/2306.10209)'
- en: Finally, please, remember that 🤗 `Accelerate` only integrates DeepSpeed, therefore
    if you have any problems or questions with regards to DeepSpeed usage, please,
    file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住，🤗 `Accelerate` 只集成了DeepSpeed，因此如果您在使用DeepSpeed时遇到任何问题或疑问，请在[DeepSpeed
    GitHub](https://github.com/microsoft/DeepSpeed/issues)上提交问题。
