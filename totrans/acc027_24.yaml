- en: DeepSpeed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/usage_guides/deepspeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/33.3218ace7.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described
    in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Some of the salient optimizations
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer state partitioning (ZeRO stage 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient partitioning (ZeRO stage 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameter partitioning (ZeRO stage 3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom mixed precision training handling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A range of fast CUDA-extension-based optimizers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO-Offload to CPU and Disk/NVMe
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical partitioning of model parameters (ZeRO++)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale
    Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described
    in the paper [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
    Learning](https://arxiv.org/abs/2104.07857).'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-2 is primarily used only for training, as its features are of
    no use to inference.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-3 can be used for inference as well since it allows huge models
    to be loaded on multiple GPUs, which wonâ€™t be possible on a single GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ¤— Accelerate integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    via 2 options:'
  prefs: []
  type: TYPE_NORMAL
- en: Integration of the DeepSpeed features via `deepspeed config file` specification
    in `accelerate config` . You just supply your custom config file or use our template.
    Most of this document is focused on this feature. This supports all the core features
    of DeepSpeed and gives user a lot of flexibility. User may have to change a few
    lines of code depending on the config.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integration via `deepspeed_plugin`.This supports subset of the DeepSpeed features
    and uses default options for the rest of the configurations. User need not change
    any code and is good for those who are fine with most of the default settings
    of DeepSpeed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is integrated?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training:'
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Accelerate integrates all features of DeepSpeed ZeRO. This includes all the
    ZeRO stages 1, 2 and 3 as well as ZeRO-Offload, ZeRO-Infinity (which can offload
    to disk/NVMe) and ZeRO++. Below is a short description of Data Parallelism using
    ZeRO - Zero Redundancy Optimizer along with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
    ![ZeRO Data Parallelism](../Images/61da03a3a1a0e2c5e704642f87f2f216.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'a. **Stage 1** : Shards optimizer states across data parallel workers/GPUs'
  prefs: []
  type: TYPE_NORMAL
- en: 'b. **Stage 2** : Shards optimizer states + gradients across data parallel workers/GPUs'
  prefs: []
  type: TYPE_NORMAL
- en: 'c. **Stage 3**: Shards optimizer states + gradients + model parameters across
    data parallel workers/GPUs'
  prefs: []
  type: TYPE_NORMAL
- en: 'd. **Optimizer Offload**: Offloads the gradients + optimizer states to CPU/Disk
    building on top of ZERO Stage 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'e. **Param Offload**: Offloads the model parameters to CPU/Disk building on
    top of ZERO Stage 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'f. **Hierarchical Partitioning**: Enables efficient multi-node training with
    data-parallel training across nodes and ZeRO-3 sharding within a node, built on
    top of ZeRO Stage 3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: With respect to Disk Offload, the disk should be an NVME for decent speed
    but it technically works on any Disk'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference:'
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesnâ€™t use an optimizer and a lr scheduler
    and only stage 3 is relevant. For more details see: [deepspeed-zero-inference](#deepspeed-zero-inference).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pre-Requisites**: Install DeepSpeed version >=0.6.5\. Please refer to the
    [DeepSpeed Installation details](https://github.com/microsoft/DeepSpeed#installation)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: We will first look at easy to use integration via `accelerate config`. Followed
    by more flexible and feature rich `deepspeed config file` integration.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate DeepSpeed Plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On your machine(s) just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: and answer the questions asked. It will ask whether you want to use a config
    file for DeepSpeed to which you should answer no. Then answer the following questions
    to generate a basic DeepSpeed config. This will generate a config file that will
    be used automatically to properly set the default options when doing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, here is how you would run the NLP example `examples/nlp_example.py`
    (from the root of the repo) with DeepSpeed Plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ZeRO Stage-2 DeepSpeed Plugin Example**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**ZeRO Stage-3 with CPU Offload DeepSpeed Plugin Example**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, `Accelerate` supports following config through the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To be able to tweak more options, you will need to use a DeepSpeed config file.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed Config File
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On your machine(s) just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: and answer the questions asked. It will ask whether you want to use a config
    file for deepspeed to which you answer yes and provide the path to the deepspeed
    config file. This will generate a config file that will be used automatically
    to properly set the default options when doing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, here is how you would run the NLP example `examples/by_feature/deepspeed_with_config_support.py`
    (from the root of the repo) with DeepSpeed Config File:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ZeRO Stage-2 DeepSpeed Config File Example**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'with the contents of `zero_stage2_config.json` being:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**ZeRO Stage-3 with CPU offload DeepSpeed Config File Example**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'with the contents of `zero_stage3_offload_config.json` being:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**ZeRO++ Config Example** You can use the the features of ZeRO++ by using the
    appropriate config parameters. Note that ZeRO++ is an extension for ZeRO Stage
    3\. Here is how the config file can be modified, from [DeepSpeedâ€™s ZeRO++ tutorial](https://www.deepspeed.ai/tutorials/zeropp/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For hierarchical partitioning, the partition size `zero_hpz_partition_size`
    should ideally be set to the number of GPUs per node. (For example, the above
    config file assumes 8 GPUs per node)
  prefs: []
  type: TYPE_NORMAL
- en: '**Important code changes when using DeepSpeed Config File**'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed Optimizers and Schedulers. For more information on these, see the
    [DeepSpeed Optimizers](https://deepspeed.readthedocs.io/en/latest/optimizers.html)
    and [DeepSpeed Schedulers](https://deepspeed.readthedocs.io/en/latest/schedulers.html)
    documentation. We will look at the changes needed in the code when using these.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a. DS Optim + DS Scheduler: The case when both `optimizer` and `scheduler`
    keys are present in the DeepSpeed config file. In this situation, those will be
    used and the user has to use `accelerate.utils.DummyOptim` and `accelerate.utils.DummyScheduler`
    to replace the PyTorch/Custom optimizers and schedulers in their code. Below is
    the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing
    this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b. Custom Optim + Custom Scheduler: The case when both `optimizer` and `scheduler`
    keys are absent in the DeepSpeed config file. In this situation, no code changes
    are needed from the user and this is the case when using integration via DeepSpeed
    Plugin. In the above example we can see that the code remains unchanged if the
    `optimizer` and `scheduler` keys are absent in the DeepSpeed config file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c. Custom Optim + DS Scheduler: The case when only `scheduler` key is present
    in the DeepSpeed config file. In this situation, the user has to use `accelerate.utils.DummyScheduler`
    to replace the PyTorch/Custom scheduler in their code.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd. DS Optim + Custom Scheduler: The case when only `optimizer` key is present
    in the DeepSpeed config file. This will result in an error because you can only
    use DS Scheduler when using DS Optim.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice the `auto` values in the above example DeepSpeed config files. These
    are automatically handled by `prepare` method based on model, dataloaders, dummy
    optimizer and dummy schedulers provided to `prepare` method. Only the `auto` fields
    specified in above examples are handled by `prepare` method and the rest have
    to be explicitly specified by the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `auto` values are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reduce_bucket_size`: `hidden_size * hidden_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the `auto` feature to work for these 3 config entries - Accelerate will
    use `model.config.hidden_size` or `max(model.config.hidden_sizes)` as `hidden_size`.
    If neither of these is available, the launching will fail and you will have to
    set these 3 config entries manually. Remember the first 2 config entries are the
    communication buffers - the larger they are the more efficient the comms will
    be, and the larger they are the more GPU memory they will consume, so itâ€™s a tunable
    performance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: '**Things to note when using DeepSpeed Config File**'
  prefs: []
  type: TYPE_NORMAL
- en: Below is a sample script using `deepspeed_config_file` in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code `test.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Scenario 1**: Manually tampered accelerate config file having `deepspeed_config_file`
    along with other entries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Content of the `accelerate` config:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`ds_config.json`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Output of `accelerate launch test.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Scenario 2**: Use the solution of the error to create new accelerate config
    and check that no ambiguity error is now thrown.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run `accelerate config`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Content of the `accelerate` config:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Output of `accelerate launch test.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Scenario 3**: Setting the `accelerate launch` command arguments related to
    DeepSpeed as `"auto"` in the DeepSpeed` configuration file and check that things
    work as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'New `ds_config.json` with `"auto"` for the `accelerate launch` DeepSpeed command
    arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Output of `accelerate launch --mixed_precision="fp16" --zero_stage=3 --gradient_accumulation_steps=5
    --gradient_clipping=1.0 --offload_param_device="cpu" --offload_optimizer_device="nvme"
    --zero3_save_16bit_model="true" test.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**:'
  prefs: []
  type: TYPE_NORMAL
- en: Remaining `"auto"` values are handled in `accelerator.prepare()` call as explained
    in point 2 of `Important code changes when using DeepSpeed Config File`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only when `gradient_accumulation_steps` is `auto`, the value passed while creating
    `Accelerator` object via `Accelerator(gradient_accumulation_steps=k)` will be
    used. When using DeepSpeed Plugin, the value from it will be used and it will
    overwrite the value passed while creating Accelerator object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving and loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saving and loading of models is unchanged for ZeRO Stage-1 and Stage-2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'under ZeRO Stage-3, `state_dict` contains just the placeholders since the model
    weights are partitioned across multiple GPUs. ZeRO Stage-3 has 2 options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a. Saving the entire 16bit model weights to directly load later on using `model.load_state_dict(torch.load(pytorch_model.bin))`.
    For this, either set `zero_optimization.stage3_gather_16bit_weights_on_model_save`
    to True in DeepSpeed Config file or set `zero3_save_16bit_model` to True in DeepSpeed
    Plugin. **Note that this option requires consolidation of the weights on one GPU
    it can be slow and memory demanding, so only use this feature when needed.** Below
    is the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing
    this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b. To get 32bit weights, first save the model using `model.save_checkpoint()`.
    Below is the snippet from `examples/by_feature/deepspeed_with_config_support.py`
    showing this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create ZeRO model and optimizer partitions along with `zero_to_fp32.py`
    script in checkpoint directory. You can use this script to do offline consolidation.
    It requires no configuration files or GPUs. Here is an example of its usage:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get 32bit model for saving/inference, you can perform:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you are only interested in the `state_dict`, you can do the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that all these functions require ~2x memory (general RAM) of the size of
    the final checkpoint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ZeRO Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesnâ€™t use an optimizer and a lr scheduler
    and only stage 3 is relevant. With accelerate integration, you just need to prepare
    the model and dataloader as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Few caveats to be aware of
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Current integration doesnâ€™t support Pipeline Parallelism of DeepSpeed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Current integration doesnâ€™t support `mpu`, limiting the tensor parallelism which
    is supported in Megatron-LM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Current integration doesnâ€™t support multiple models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepSpeed Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The documentation for the internals related to deepspeed can be found [here](../package_reference/deepspeed).
  prefs: []
  type: TYPE_NORMAL
- en: '[Projectâ€™s github](https://github.com/microsoft/deepspeed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Usage docs](https://www.deepspeed.ai/getting-started/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[API docs](https://deepspeed.readthedocs.io/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ZeRO++: Extremely Efficient Collective Communication for Giant Model Training](https://arxiv.org/abs/2306.10209)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, please, remember that ðŸ¤— `Accelerate` only integrates DeepSpeed, therefore
    if you have any problems or questions with regards to DeepSpeed usage, please,
    file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).
  prefs: []
  type: TYPE_NORMAL
