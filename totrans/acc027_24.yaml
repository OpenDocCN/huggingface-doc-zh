- en: DeepSpeed
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/deepspeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described
    in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Some of the salient optimizations
    are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer state partitioning (ZeRO stage 1)
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient partitioning (ZeRO stage 2)
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameter partitioning (ZeRO stage 3)
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom mixed precision training handling
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A range of fast CUDA-extension-based optimizers
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO-Offload to CPU and Disk/NVMe
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical partitioning of model parameters (ZeRO++)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale
    Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described
    in the paper [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
    Learning](https://arxiv.org/abs/2104.07857).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-2 is primarily used only for training, as its features are of
    no use to inference.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed ZeRO-3 can be used for inference as well since it allows huge models
    to be loaded on multiple GPUs, which wonâ€™t be possible on a single GPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ¤— Accelerate integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    via 2 options:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Integration of the DeepSpeed features via `deepspeed config file` specification
    in `accelerate config` . You just supply your custom config file or use our template.
    Most of this document is focused on this feature. This supports all the core features
    of DeepSpeed and gives user a lot of flexibility. User may have to change a few
    lines of code depending on the config.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integration via `deepspeed_plugin`.This supports subset of the DeepSpeed features
    and uses default options for the rest of the configurations. User need not change
    any code and is good for those who are fine with most of the default settings
    of DeepSpeed.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is integrated?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Accelerate integrates all features of DeepSpeed ZeRO. This includes all the
    ZeRO stages 1, 2 and 3 as well as ZeRO-Offload, ZeRO-Infinity (which can offload
    to disk/NVMe) and ZeRO++. Below is a short description of Data Parallelism using
    ZeRO - Zero Redundancy Optimizer along with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
    ![ZeRO Data Parallelism](../Images/61da03a3a1a0e2c5e704642f87f2f216.png)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'a. **Stage 1** : Shards optimizer states across data parallel workers/GPUs'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'b. **Stage 2** : Shards optimizer states + gradients across data parallel workers/GPUs'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'c. **Stage 3**: Shards optimizer states + gradients + model parameters across
    data parallel workers/GPUs'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'd. **Optimizer Offload**: Offloads the gradients + optimizer states to CPU/Disk
    building on top of ZERO Stage 2'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'e. **Param Offload**: Offloads the model parameters to CPU/Disk building on
    top of ZERO Stage 3'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'f. **Hierarchical Partitioning**: Enables efficient multi-node training with
    data-parallel training across nodes and ZeRO-3 sharding within a node, built on
    top of ZeRO Stage 3.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: With respect to Disk Offload, the disk should be an NVME for decent speed
    but it technically works on any Disk'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesnâ€™t use an optimizer and a lr scheduler
    and only stage 3 is relevant. For more details see: [deepspeed-zero-inference](#deepspeed-zero-inference).'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pre-Requisites**: Install DeepSpeed version >=0.6.5\. Please refer to the
    [DeepSpeed Installation details](https://github.com/microsoft/DeepSpeed#installation)
    for more information.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: We will first look at easy to use integration via `accelerate config`. Followed
    by more flexible and feature rich `deepspeed config file` integration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆçœ‹ä¸€ä¸‹é€šè¿‡`accelerate config`è¿›è¡Œæ˜“äºä½¿ç”¨çš„é›†æˆã€‚ç„¶åæ˜¯æ›´çµæ´»å’ŒåŠŸèƒ½ä¸°å¯Œçš„`deepspeedé…ç½®æ–‡ä»¶`é›†æˆã€‚
- en: Accelerate DeepSpeed Plugin
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ é€ŸDeepSpeedæ’ä»¶
- en: 'On your machine(s) just run:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œï¼š
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: and answer the questions asked. It will ask whether you want to use a config
    file for DeepSpeed to which you should answer no. Then answer the following questions
    to generate a basic DeepSpeed config. This will generate a config file that will
    be used automatically to properly set the default options when doing
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å›ç­”æå‡ºçš„é—®é¢˜ã€‚å®ƒä¼šè¯¢é—®æ‚¨æ˜¯å¦è¦ä½¿ç”¨ä¸€ä¸ªDeepSpeedé…ç½®æ–‡ä»¶ï¼Œæ‚¨åº”è¯¥å›ç­”å¦ã€‚ç„¶åå›ç­”ä»¥ä¸‹é—®é¢˜ä»¥ç”ŸæˆåŸºæœ¬çš„DeepSpeedé…ç½®ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨ç”¨äºåœ¨æ‰§è¡Œæ—¶æ­£ç¡®è®¾ç½®é»˜è®¤é€‰é¡¹
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For instance, here is how you would run the NLP example `examples/nlp_example.py`
    (from the root of the repo) with DeepSpeed Plugin:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•åœ¨NLPç¤ºä¾‹`examples/nlp_example.py`ï¼ˆä»å­˜å‚¨åº“çš„æ ¹ç›®å½•ï¼‰ä¸­ä½¿ç”¨DeepSpeedæ’ä»¶è¿è¡Œçš„ï¼š
- en: '**ZeRO Stage-2 DeepSpeed Plugin Example**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeRO Stage-2 DeepSpeedæ’ä»¶ç¤ºä¾‹**'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**ZeRO Stage-3 with CPU Offload DeepSpeed Plugin Example**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¸¦CPUå¸è½½çš„ZeRO Stage-3 DeepSpeedæ’ä»¶ç¤ºä¾‹**'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Currently, `Accelerate` supports following config through the CLI:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œ`Accelerate`é€šè¿‡CLIæ”¯æŒä»¥ä¸‹é…ç½®ï¼š
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To be able to tweak more options, you will need to use a DeepSpeed config file.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†èƒ½å¤Ÿè°ƒæ•´æ›´å¤šé€‰é¡¹ï¼Œæ‚¨éœ€è¦ä½¿ç”¨DeepSpeedé…ç½®æ–‡ä»¶ã€‚
- en: DeepSpeed Config File
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepSpeedé…ç½®æ–‡ä»¶
- en: 'On your machine(s) just run:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œï¼š
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: and answer the questions asked. It will ask whether you want to use a config
    file for deepspeed to which you answer yes and provide the path to the deepspeed
    config file. This will generate a config file that will be used automatically
    to properly set the default options when doing
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å›ç­”æå‡ºçš„é—®é¢˜ã€‚å®ƒä¼šè¯¢é—®æ‚¨æ˜¯å¦è¦ä½¿ç”¨ä¸€ä¸ªDeepSpeedé…ç½®æ–‡ä»¶ï¼Œæ‚¨éœ€è¦å›ç­”æ˜¯ï¼Œå¹¶æä¾›DeepSpeedé…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨ç”¨äºåœ¨æ‰§è¡Œæ—¶æ­£ç¡®è®¾ç½®é»˜è®¤é€‰é¡¹
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For instance, here is how you would run the NLP example `examples/by_feature/deepspeed_with_config_support.py`
    (from the root of the repo) with DeepSpeed Config File:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•åœ¨NLPç¤ºä¾‹`examples/by_feature/deepspeed_with_config_support.py`ï¼ˆä»å­˜å‚¨åº“çš„æ ¹ç›®å½•ï¼‰ä¸­ä½¿ç”¨DeepSpeedé…ç½®æ–‡ä»¶è¿è¡Œçš„ï¼š
- en: '**ZeRO Stage-2 DeepSpeed Config File Example**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeRO Stage-2 DeepSpeedé…ç½®æ–‡ä»¶ç¤ºä¾‹**'
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'with the contents of `zero_stage2_config.json` being:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`zero_stage2_config.json`çš„å†…å®¹ä¸ºï¼š
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**ZeRO Stage-3 with CPU offload DeepSpeed Config File Example**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¸¦CPUå¸è½½çš„ZeRO Stage-3 DeepSpeedé…ç½®æ–‡ä»¶ç¤ºä¾‹**'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'with the contents of `zero_stage3_offload_config.json` being:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`zero_stage3_offload_config.json`çš„å†…å®¹ä¸ºï¼š
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**ZeRO++ Config Example** You can use the the features of ZeRO++ by using the
    appropriate config parameters. Note that ZeRO++ is an extension for ZeRO Stage
    3\. Here is how the config file can be modified, from [DeepSpeedâ€™s ZeRO++ tutorial](https://www.deepspeed.ai/tutorials/zeropp/):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeRO++é…ç½®ç¤ºä¾‹** æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨é€‚å½“çš„é…ç½®å‚æ•°æ¥ä½¿ç”¨ZeRO++çš„åŠŸèƒ½ã€‚è¯·æ³¨æ„ï¼ŒZeRO++æ˜¯ZeRO Stage 3çš„æ‰©å±•ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œæ¥è‡ª[DeepSpeedçš„ZeRO++æ•™ç¨‹](https://www.deepspeed.ai/tutorials/zeropp/)ï¼š'
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For hierarchical partitioning, the partition size `zero_hpz_partition_size`
    should ideally be set to the number of GPUs per node. (For example, the above
    config file assumes 8 GPUs per node)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†å±‚åˆ†åŒºï¼Œåˆ†åŒºå¤§å°`zero_hpz_partition_size`ç†æƒ³æƒ…å†µä¸‹åº”è®¾ç½®ä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„GPUæ•°é‡ã€‚ï¼ˆä¾‹å¦‚ï¼Œä¸Šè¿°é…ç½®æ–‡ä»¶å‡å®šæ¯ä¸ªèŠ‚ç‚¹æœ‰8ä¸ªGPUï¼‰
- en: '**Important code changes when using DeepSpeed Config File**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DeepSpeedé…ç½®æ–‡ä»¶æ—¶çš„é‡è¦ä»£ç æ›´æ”¹
- en: DeepSpeed Optimizers and Schedulers. For more information on these, see the
    [DeepSpeed Optimizers](https://deepspeed.readthedocs.io/en/latest/optimizers.html)
    and [DeepSpeed Schedulers](https://deepspeed.readthedocs.io/en/latest/schedulers.html)
    documentation. We will look at the changes needed in the code when using these.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepSpeedä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[DeepSpeedä¼˜åŒ–å™¨](https://deepspeed.readthedocs.io/en/latest/optimizers.html)å’Œ[DeepSpeedè°ƒåº¦å™¨](https://deepspeed.readthedocs.io/en/latest/schedulers.html)æ–‡æ¡£ã€‚æˆ‘ä»¬å°†çœ‹åˆ°åœ¨ä½¿ç”¨è¿™äº›æ—¶ä»£ç éœ€è¦çš„æ›´æ”¹ã€‚
- en: 'a. DS Optim + DS Scheduler: The case when both `optimizer` and `scheduler`
    keys are present in the DeepSpeed config file. In this situation, those will be
    used and the user has to use `accelerate.utils.DummyOptim` and `accelerate.utils.DummyScheduler`
    to replace the PyTorch/Custom optimizers and schedulers in their code. Below is
    the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing
    this:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. DS Optim + DS Schedulerï¼šå½“DeepSpeedé…ç½®æ–‡ä»¶ä¸­åŒæ—¶å­˜åœ¨`optimizer`å’Œ`scheduler`é”®æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™äº›å°†è¢«ä½¿ç”¨ï¼Œç”¨æˆ·å¿…é¡»ä½¿ç”¨`accelerate.utils.DummyOptim`å’Œ`accelerate.utils.DummyScheduler`æ¥æ›¿æ¢ä»–ä»¬ä»£ç ä¸­çš„PyTorch/è‡ªå®šä¹‰ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚ä»¥ä¸‹æ˜¯`examples/by_feature/deepspeed_with_config_support.py`ä¸­æ˜¾ç¤ºçš„ç‰‡æ®µï¼š
- en: '[PRE16]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'b. Custom Optim + Custom Scheduler: The case when both `optimizer` and `scheduler`
    keys are absent in the DeepSpeed config file. In this situation, no code changes
    are needed from the user and this is the case when using integration via DeepSpeed
    Plugin. In the above example we can see that the code remains unchanged if the
    `optimizer` and `scheduler` keys are absent in the DeepSpeed config file.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. Custom Optim + Custom Schedulerï¼šå½“DeepSpeedé…ç½®æ–‡ä»¶ä¸­`optimizer`å’Œ`scheduler`é”®éƒ½ä¸å­˜åœ¨æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·ä¸éœ€è¦è¿›è¡Œä»»ä½•ä»£ç æ›´æ”¹ï¼Œè¿™æ˜¯ä½¿ç”¨DeepSpeedæ’ä»¶è¿›è¡Œé›†æˆæ—¶çš„æƒ…å†µã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¦‚æœDeepSpeedé…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨`optimizer`å’Œ`scheduler`é”®ï¼Œåˆ™ä»£ç ä¿æŒä¸å˜ã€‚
- en: 'c. Custom Optim + DS Scheduler: The case when only `scheduler` key is present
    in the DeepSpeed config file. In this situation, the user has to use `accelerate.utils.DummyScheduler`
    to replace the PyTorch/Custom scheduler in their code.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. Custom Optim + DS Schedulerï¼šå½“DeepSpeedé…ç½®æ–‡ä»¶ä¸­åªæœ‰`scheduler`é”®å­˜åœ¨æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·å¿…é¡»ä½¿ç”¨`accelerate.utils.DummyScheduler`æ¥æ›¿æ¢ä»–ä»¬ä»£ç ä¸­çš„PyTorch/è‡ªå®šä¹‰è°ƒåº¦å™¨ã€‚
- en: 'd. DS Optim + Custom Scheduler: The case when only `optimizer` key is present
    in the DeepSpeed config file. This will result in an error because you can only
    use DS Scheduler when using DS Optim.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. DS Optim + Custom Schedulerï¼šå½“DeepSpeedé…ç½®æ–‡ä»¶ä¸­åªæœ‰`optimizer`é”®å­˜åœ¨æ—¶ã€‚è¿™å°†å¯¼è‡´é”™è¯¯ï¼Œå› ä¸ºåªæœ‰åœ¨ä½¿ç”¨DS
    Optimæ—¶æ‰èƒ½ä½¿ç”¨DS Schedulerã€‚
- en: Notice the `auto` values in the above example DeepSpeed config files. These
    are automatically handled by `prepare` method based on model, dataloaders, dummy
    optimizer and dummy schedulers provided to `prepare` method. Only the `auto` fields
    specified in above examples are handled by `prepare` method and the rest have
    to be explicitly specified by the user.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ä¸Šè¿°ç¤ºä¾‹DeepSpeedé…ç½®æ–‡ä»¶ä¸­çš„`auto`å€¼ã€‚è¿™äº›å€¼ç”±`prepare`æ–¹æ³•æ ¹æ®æä¾›ç»™`prepare`æ–¹æ³•çš„æ¨¡å‹ã€æ•°æ®åŠ è½½å™¨ã€è™šæ‹Ÿä¼˜åŒ–å™¨å’Œè™šæ‹Ÿè°ƒåº¦å™¨è‡ªåŠ¨å¤„ç†ã€‚ä¸Šè¿°ç¤ºä¾‹ä¸­æŒ‡å®šçš„`auto`å­—æ®µç”±`prepare`æ–¹æ³•å¤„ç†ï¼Œå…¶ä½™å­—æ®µå¿…é¡»ç”±ç”¨æˆ·æ˜¾å¼æŒ‡å®šã€‚
- en: 'The `auto` values are calculated as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`auto`å€¼çš„è®¡ç®—å¦‚ä¸‹ï¼š'
- en: '`reduce_bucket_size`: `hidden_size * hidden_size`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_bucket_size`: `hidden_size * hidden_size`'
- en: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
- en: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
- en: For the `auto` feature to work for these 3 config entries - Accelerate will
    use `model.config.hidden_size` or `max(model.config.hidden_sizes)` as `hidden_size`.
    If neither of these is available, the launching will fail and you will have to
    set these 3 config entries manually. Remember the first 2 config entries are the
    communication buffers - the larger they are the more efficient the comms will
    be, and the larger they are the more GPU memory they will consume, so itâ€™s a tunable
    performance trade-off.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è¿™3ä¸ªé…ç½®æ¡ç›®çš„`auto`åŠŸèƒ½æ­£å¸¸å·¥ä½œ - Accelerateå°†ä½¿ç”¨`model.config.hidden_size`æˆ–`max(model.config.hidden_sizes)`ä½œä¸º`hidden_size`ã€‚å¦‚æœè¿™ä¸¤è€…éƒ½ä¸å¯ç”¨ï¼Œå¯åŠ¨å°†å¤±è´¥ï¼Œæ‚¨å°†ä¸å¾—ä¸æ‰‹åŠ¨è®¾ç½®è¿™3ä¸ªé…ç½®æ¡ç›®ã€‚è¯·è®°ä½ï¼Œå‰ä¸¤ä¸ªé…ç½®æ¡ç›®æ˜¯é€šä¿¡ç¼“å†²åŒº
    - å®ƒä»¬è¶Šå¤§ï¼Œé€šä¿¡æ•ˆç‡å°±è¶Šé«˜ï¼Œå®ƒä»¬è¶Šå¤§ï¼ŒGPUå†…å­˜æ¶ˆè€—å°±è¶Šå¤§ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªå¯è°ƒæ•´çš„æ€§èƒ½æƒè¡¡ã€‚
- en: '**Things to note when using DeepSpeed Config File**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨DeepSpeedé…ç½®æ–‡ä»¶æ—¶éœ€è¦æ³¨æ„çš„äº‹é¡¹**'
- en: Below is a sample script using `deepspeed_config_file` in different scenarios.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨ä¸åŒåœºæ™¯ä¸­ä½¿ç”¨`deepspeed_config_file`çš„ç¤ºä¾‹è„šæœ¬ã€‚
- en: 'Code `test.py`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç `test.py`ï¼š
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Scenario 1**: Manually tampered accelerate config file having `deepspeed_config_file`
    along with other entries.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœºæ™¯1**ï¼šæ‰‹åŠ¨ç¯¡æ”¹çš„å¸¦æœ‰`deepspeed_config_file`çš„accelerateé…ç½®æ–‡ä»¶ä»¥åŠå…¶ä»–æ¡ç›®ã€‚'
- en: 'Content of the `accelerate` config:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`accelerate`é…ç½®çš„å†…å®¹ï¼š'
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`ds_config.json`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ds_config.json`ï¼š'
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output of `accelerate launch test.py`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œ`accelerate launch test.py`çš„è¾“å‡ºï¼š
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Scenario 2**: Use the solution of the error to create new accelerate config
    and check that no ambiguity error is now thrown.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœºæ™¯2**ï¼šä½¿ç”¨é”™è¯¯çš„è§£å†³æ–¹æ¡ˆåˆ›å»ºæ–°çš„accelerateé…ç½®ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦ç°åœ¨ä¸ä¼šæŠ›å‡ºä»»ä½•æ­§ä¹‰é”™è¯¯ã€‚'
- en: 'Run `accelerate config`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œ`accelerate config`ï¼š
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Content of the `accelerate` config:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`accelerate`é…ç½®çš„å†…å®¹ï¼š'
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output of `accelerate launch test.py`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œ`accelerate launch test.py`çš„è¾“å‡ºï¼š
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Scenario 3**: Setting the `accelerate launch` command arguments related to
    DeepSpeed as `"auto"` in the DeepSpeed` configuration file and check that things
    work as expected.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœºæ™¯3**ï¼šåœ¨DeepSpeedé…ç½®æ–‡ä»¶ä¸­å°†ä¸DeepSpeedç›¸å…³çš„`accelerate launch`å‘½ä»¤å‚æ•°è®¾ç½®ä¸º`"auto"`ï¼Œå¹¶æ£€æŸ¥äº‹æƒ…æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œã€‚'
- en: 'New `ds_config.json` with `"auto"` for the `accelerate launch` DeepSpeed command
    arguments:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`"auto"`ä¸º`accelerate launch` DeepSpeedå‘½ä»¤å‚æ•°åˆ›å»ºæ–°çš„`ds_config.json`ï¼š
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Output of `accelerate launch --mixed_precision="fp16" --zero_stage=3 --gradient_accumulation_steps=5
    --gradient_clipping=1.0 --offload_param_device="cpu" --offload_optimizer_device="nvme"
    --zero3_save_16bit_model="true" test.py`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œ`accelerate launch --mixed_precision="fp16" --zero_stage=3 --gradient_accumulation_steps=5
    --gradient_clipping=1.0 --offload_param_device="cpu" --offload_optimizer_device="nvme"
    --zero3_save_16bit_model="true" test.py`çš„è¾“å‡ºï¼š
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Note**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼š'
- en: Remaining `"auto"` values are handled in `accelerator.prepare()` call as explained
    in point 2 of `Important code changes when using DeepSpeed Config File`.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‰©ä½™çš„`"auto"`å€¼åœ¨`accelerator.prepare()`è°ƒç”¨ä¸­å¤„ç†ï¼Œå¦‚åœ¨ä½¿ç”¨DeepSpeedé…ç½®æ–‡ä»¶æ—¶çš„é‡è¦ä»£ç æ›´æ”¹çš„ç¬¬2ç‚¹ä¸­æ‰€è§£é‡Šçš„ã€‚
- en: Only when `gradient_accumulation_steps` is `auto`, the value passed while creating
    `Accelerator` object via `Accelerator(gradient_accumulation_steps=k)` will be
    used. When using DeepSpeed Plugin, the value from it will be used and it will
    overwrite the value passed while creating Accelerator object.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åªæœ‰å½“`gradient_accumulation_steps`ä¸º`auto`æ—¶ï¼Œåˆ›å»º`Accelerator`å¯¹è±¡æ—¶é€šè¿‡`Accelerator(gradient_accumulation_steps=k)`ä¼ é€’çš„å€¼æ‰ä¼šè¢«ä½¿ç”¨ã€‚ä½¿ç”¨DeepSpeedæ’ä»¶æ—¶ï¼Œå°†ä½¿ç”¨å…¶ä¸­çš„å€¼ï¼Œå¹¶ä¸”å®ƒå°†è¦†ç›–åˆ›å»ºAcceleratorå¯¹è±¡æ—¶ä¼ é€’çš„å€¼ã€‚
- en: Saving and loading
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿å­˜å’ŒåŠ è½½
- en: Saving and loading of models is unchanged for ZeRO Stage-1 and Stage-2.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºZeRO Stage-1å’ŒStage-2ï¼Œæ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½ä¿æŒä¸å˜ã€‚
- en: 'under ZeRO Stage-3, `state_dict` contains just the placeholders since the model
    weights are partitioned across multiple GPUs. ZeRO Stage-3 has 2 options:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ZeRO Stage-3ä¸‹ï¼Œ`state_dict`ä»…åŒ…å«å ä½ç¬¦ï¼Œå› ä¸ºæ¨¡å‹æƒé‡è¢«åˆ†åŒºåˆ°å¤šä¸ªGPUä¸Šã€‚ZeRO Stage-3æœ‰2ä¸ªé€‰é¡¹ï¼š
- en: 'a. Saving the entire 16bit model weights to directly load later on using `model.load_state_dict(torch.load(pytorch_model.bin))`.
    For this, either set `zero_optimization.stage3_gather_16bit_weights_on_model_save`
    to True in DeepSpeed Config file or set `zero3_save_16bit_model` to True in DeepSpeed
    Plugin. **Note that this option requires consolidation of the weights on one GPU
    it can be slow and memory demanding, so only use this feature when needed.** Below
    is the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing
    this:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. å°†æ•´ä¸ª16ä½æ¨¡å‹æƒé‡ä¿å­˜ä»¥ä¾¿ä»¥åç›´æ¥åŠ è½½ï¼Œä½¿ç”¨`model.load_state_dict(torch.load(pytorch_model.bin))`ã€‚ä¸ºæ­¤ï¼Œåœ¨DeepSpeedé…ç½®æ–‡ä»¶ä¸­å°†`zero_optimization.stage3_gather_16bit_weights_on_model_save`è®¾ç½®ä¸ºTrueï¼Œæˆ–è€…åœ¨DeepSpeedæ’ä»¶ä¸­å°†`zero3_save_16bit_model`è®¾ç½®ä¸ºTrueã€‚**è¯·æ³¨æ„ï¼Œæ­¤é€‰é¡¹è¦æ±‚åœ¨ä¸€ä¸ªGPUä¸Šåˆå¹¶æƒé‡ï¼Œå¯èƒ½ä¼šå¾ˆæ…¢ä¸”å ç”¨å†…å­˜ï¼Œå› æ­¤åªåœ¨éœ€è¦æ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚**ä»¥ä¸‹æ˜¯`examples/by_feature/deepspeed_with_config_support.py`ä¸­æ˜¾ç¤ºæ­¤åŠŸèƒ½çš„ç‰‡æ®µï¼š
- en: '[PRE26]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'b. To get 32bit weights, first save the model using `model.save_checkpoint()`.
    Below is the snippet from `examples/by_feature/deepspeed_with_config_support.py`
    showing this:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. è¦è·å–32ä½æƒé‡ï¼Œé¦–å…ˆä½¿ç”¨`model.save_checkpoint()`ä¿å­˜æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯`examples/by_feature/deepspeed_with_config_support.py`ä¸­æ˜¾ç¤ºæ­¤åŠŸèƒ½çš„ç‰‡æ®µï¼š
- en: '[PRE27]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will create ZeRO model and optimizer partitions along with `zero_to_fp32.py`
    script in checkpoint directory. You can use this script to do offline consolidation.
    It requires no configuration files or GPUs. Here is an example of its usage:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å°†åœ¨æ£€æŸ¥ç‚¹ç›®å½•ä¸­åˆ›å»ºZeROæ¨¡å‹å’Œä¼˜åŒ–å™¨åˆ†åŒºï¼Œä»¥åŠ`zero_to_fp32.py`è„šæœ¬ã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ­¤è„šæœ¬è¿›è¡Œç¦»çº¿åˆå¹¶ã€‚å®ƒä¸éœ€è¦é…ç½®æ–‡ä»¶æˆ–GPUã€‚ä»¥ä¸‹æ˜¯å…¶ç”¨æ³•ç¤ºä¾‹ï¼š
- en: '[PRE28]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To get 32bit model for saving/inference, you can perform:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¦è·å¾—ç”¨äºä¿å­˜/æ¨ç†çš„32ä½æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œï¼š
- en: '[PRE29]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you are only interested in the `state_dict`, you can do the following:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åªå¯¹`state_dict`æ„Ÿå…´è¶£ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE30]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that all these functions require ~2x memory (general RAM) of the size of
    the final checkpoint.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éœ€è¦æœ€ç»ˆæ£€æŸ¥ç‚¹å¤§å°çš„å¤§çº¦2å€å†…å­˜ï¼ˆé€šç”¨RAMï¼‰ã€‚
- en: ZeRO Inference
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZeRO æ¨ç†
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesnâ€™t use an optimizer and a lr scheduler
    and only stage 3 is relevant. With accelerate integration, you just need to prepare
    the model and dataloader as shown below:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO æ¨ç†æ”¯æŒä½¿ç”¨ZeRO-Infinityçš„ZeROé˜¶æ®µ3ã€‚å®ƒä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„ZeROåè®®ï¼Œä½†ä¸ä½¿ç”¨ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ç¨‹åºï¼Œåªæœ‰é˜¶æ®µ3ç›¸å…³ã€‚é€šè¿‡åŠ é€Ÿé›†æˆï¼Œæ‚¨åªéœ€è¦å‡†å¤‡å¦‚ä¸‹æ‰€ç¤ºçš„æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨ï¼š
- en: '[PRE31]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Few caveats to be aware of
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„ä¸€äº›æ³¨æ„äº‹é¡¹
- en: Current integration doesnâ€™t support Pipeline Parallelism of DeepSpeed.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“å‰é›†æˆä¸æ”¯æŒDeepSpeedçš„ç®¡é“å¹¶è¡Œæ€§ã€‚
- en: Current integration doesnâ€™t support `mpu`, limiting the tensor parallelism which
    is supported in Megatron-LM.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“å‰é›†æˆä¸æ”¯æŒ`mpu`ï¼Œé™åˆ¶äº†åœ¨Megatron-LMä¸­æ”¯æŒçš„å¼ é‡å¹¶è¡Œæ€§ã€‚
- en: Current integration doesnâ€™t support multiple models.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“å‰é›†æˆä¸æ”¯æŒå¤šä¸ªæ¨¡å‹ã€‚
- en: DeepSpeed Resources
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepSpeed èµ„æº
- en: The documentation for the internals related to deepspeed can be found [here](../package_reference/deepspeed).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ä¸deepspeedç›¸å…³çš„å†…éƒ¨æ–‡æ¡£å¯ä»¥åœ¨[è¿™é‡Œ](../package_reference/deepspeed)æ‰¾åˆ°ã€‚
- en: '[Projectâ€™s github](https://github.com/microsoft/deepspeed)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é¡¹ç›®çš„github](https://github.com/microsoft/deepspeed)'
- en: '[Usage docs](https://www.deepspeed.ai/getting-started/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨æ–‡æ¡£](https://www.deepspeed.ai/getting-started/)'
- en: '[API docs](https://deepspeed.readthedocs.io/en/latest/index.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[APIæ–‡æ¡£](https://deepspeed.readthedocs.io/en/latest/index.html)'
- en: '[Blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åšå®¢æ–‡ç« ](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
- en: 'Papers:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®ºæ–‡:'
- en: '[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeROï¼šæœç€è®­ç»ƒä¸‡äº¿å‚æ•°æ¨¡å‹çš„å†…å­˜ä¼˜åŒ–](https://arxiv.org/abs/1910.02054)'
- en: '[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Offloadï¼šæ°‘ä¸»åŒ–äº¿çº§è§„æ¨¡æ¨¡å‹è®­ç»ƒ](https://arxiv.org/abs/2101.06840)'
- en: '[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Infinityï¼šæ‰“ç ´æç«¯è§„æ¨¡æ·±åº¦å­¦ä¹ çš„GPUå†…å­˜å£](https://arxiv.org/abs/2104.07857)'
- en: '[ZeRO++: Extremely Efficient Collective Communication for Giant Model Training](https://arxiv.org/abs/2306.10209)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO++ï¼šç”¨äºå·¨å‹æ¨¡å‹è®­ç»ƒçš„æå…¶é«˜æ•ˆçš„é›†ä½“é€šä¿¡](https://arxiv.org/abs/2306.10209)'
- en: Finally, please, remember that ğŸ¤— `Accelerate` only integrates DeepSpeed, therefore
    if you have any problems or questions with regards to DeepSpeed usage, please,
    file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè¯·è®°ä½ï¼ŒğŸ¤— `Accelerate` åªé›†æˆäº†DeepSpeedï¼Œå› æ­¤å¦‚æœæ‚¨åœ¨ä½¿ç”¨DeepSpeedæ—¶é‡åˆ°ä»»ä½•é—®é¢˜æˆ–ç–‘é—®ï¼Œè¯·åœ¨[DeepSpeed
    GitHub](https://github.com/microsoft/DeepSpeed/issues)ä¸Šæäº¤é—®é¢˜ã€‚
