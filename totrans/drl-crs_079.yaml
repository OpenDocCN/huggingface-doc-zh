- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/68.91c286b0.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail](../Images/9f49f2880784ef300d40b68768960852.png)'
  prefs: []
  type: TYPE_IMG
- en: In unit 4, we learned about our first Policy-Based algorithm called **Reinforce**.
  prefs: []
  type: TYPE_NORMAL
- en: In Policy-Based methods, **we aim to optimize the policy directly without using
    a value function**. More precisely, Reinforce is part of a subclass of *Policy-Based
    Methods* called *Policy-Gradient methods*. This subclass optimizes the policy
    directly by **estimating the weights of the optimal policy using Gradient Ascent**.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that Reinforce worked well. However, because we use Monte-Carlo sampling
    to estimate return (we use an entire episode to calculate the return), **we have
    significant variance in policy gradient estimation**.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the policy gradient estimation is **the direction of the steepest
    increase in return**. In other words, how to update our policy weights so that
    actions that lead to good returns have a higher probability of being taken. The
    Monte Carlo variance, which we will further study in this unit, **leads to slower
    training since we need a lot of samples to mitigate it**.
  prefs: []
  type: TYPE_NORMAL
- en: 'So today weâ€™ll study **Actor-Critic methods**, a hybrid architecture combining
    value-based and Policy-Based methods that helps to stabilize the training by reducing
    the variance using:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An Actor* that controls **how our agent behaves** (Policy-Based method)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Critic* that measures **how good the taken action is** (Value-Based method)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weâ€™ll study one of these hybrid methods, Advantage Actor Critic (A2C), **and
    train our agent using Stable-Baselines3 in robotic environments**. Weâ€™ll train:'
  prefs: []
  type: TYPE_NORMAL
- en: A robotic arm ðŸ¦¾ to move to the correct position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sound exciting? Letâ€™s get started!
  prefs: []
  type: TYPE_NORMAL
