- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail](../Images/9f49f2880784ef300d40b68768960852.png)'
  prefs: []
  type: TYPE_IMG
- en: In unit 4, we learned about our first Policy-Based algorithm called **Reinforce**.
  prefs: []
  type: TYPE_NORMAL
- en: In Policy-Based methods, **we aim to optimize the policy directly without using
    a value function**. More precisely, Reinforce is part of a subclass of *Policy-Based
    Methods* called *Policy-Gradient methods*. This subclass optimizes the policy
    directly by **estimating the weights of the optimal policy using Gradient Ascent**.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that Reinforce worked well. However, because we use Monte-Carlo sampling
    to estimate return (we use an entire episode to calculate the return), **we have
    significant variance in policy gradient estimation**.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the policy gradient estimation is **the direction of the steepest
    increase in return**. In other words, how to update our policy weights so that
    actions that lead to good returns have a higher probability of being taken. The
    Monte Carlo variance, which we will further study in this unit, **leads to slower
    training since we need a lot of samples to mitigate it**.
  prefs: []
  type: TYPE_NORMAL
- en: 'So today weâ€™ll study **Actor-Critic methods**, a hybrid architecture combining
    value-based and Policy-Based methods that helps to stabilize the training by reducing
    the variance using:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An Actor* that controls **how our agent behaves** (Policy-Based method)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Critic* that measures **how good the taken action is** (Value-Based method)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weâ€™ll study one of these hybrid methods, Advantage Actor Critic (A2C), **and
    train our agent using Stable-Baselines3 in robotic environments**. Weâ€™ll train:'
  prefs: []
  type: TYPE_NORMAL
- en: A robotic arm ðŸ¦¾ to move to the correct position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sound exciting? Letâ€™s get started!
  prefs: []
  type: TYPE_NORMAL
