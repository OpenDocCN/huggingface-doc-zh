- en: Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail](../Images/9f49f2880784ef300d40b68768960852.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![ç¼©ç•¥å›¾](../Images/9f49f2880784ef300d40b68768960852.png)'
- en: In unit 4, we learned about our first Policy-Based algorithm called **Reinforce**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬4å•å…ƒï¼Œæˆ‘ä»¬å­¦ä¹ äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªåŸºäºç­–ç•¥çš„ç®—æ³•å«åš **Reinforce**ã€‚
- en: In Policy-Based methods, **we aim to optimize the policy directly without using
    a value function**. More precisely, Reinforce is part of a subclass of *Policy-Based
    Methods* called *Policy-Gradient methods*. This subclass optimizes the policy
    directly by **estimating the weights of the optimal policy using Gradient Ascent**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºç­–ç•¥çš„æ–¹æ³•ä¸­ï¼Œ**æˆ‘ä»¬æ—¨åœ¨ç›´æ¥ä¼˜åŒ–ç­–ç•¥è€Œä¸ä½¿ç”¨ä»·å€¼å‡½æ•°**ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼ŒReinforceæ˜¯*åŸºäºç­–ç•¥æ–¹æ³•* ä¸­çš„ä¸€ä¸ªå­ç±»ï¼Œç§°ä¸º*ç­–ç•¥æ¢¯åº¦æ–¹æ³•*ã€‚è¿™ä¸ªå­ç±»é€šè¿‡**ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ¥ä¼°è®¡æœ€ä¼˜ç­–ç•¥çš„æƒé‡**
    ç›´æ¥ä¼˜åŒ–ç­–ç•¥ã€‚
- en: We saw that Reinforce worked well. However, because we use Monte-Carlo sampling
    to estimate return (we use an entire episode to calculate the return), **we have
    significant variance in policy gradient estimation**.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°Reinforceæ•ˆæœå¾ˆå¥½ã€‚ç„¶è€Œï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·æ¥ä¼°è®¡å›æŠ¥ï¼ˆæˆ‘ä»¬ä½¿ç”¨æ•´ä¸ªæƒ…èŠ‚æ¥è®¡ç®—å›æŠ¥ï¼‰ï¼Œ**æˆ‘ä»¬åœ¨ç­–ç•¥æ¢¯åº¦ä¼°è®¡ä¸­æœ‰æ˜¾è‘—çš„æ–¹å·®**ã€‚
- en: Remember that the policy gradient estimation is **the direction of the steepest
    increase in return**. In other words, how to update our policy weights so that
    actions that lead to good returns have a higher probability of being taken. The
    Monte Carlo variance, which we will further study in this unit, **leads to slower
    training since we need a lot of samples to mitigate it**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œç­–ç•¥æ¢¯åº¦ä¼°è®¡æ˜¯ **å›æŠ¥å¢åŠ æœ€é™¡å³­çš„æ–¹å‘**ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚ä½•æ›´æ–°æˆ‘ä»¬çš„ç­–ç•¥æƒé‡ï¼Œä½¿å¯¼è‡´è‰¯å¥½å›æŠ¥çš„è¡ŒåŠ¨æœ‰æ›´é«˜çš„è¢«é‡‡å–çš„æ¦‚ç‡ã€‚è’™ç‰¹å¡æ´›æ–¹å·®ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬å•å…ƒè¿›ä¸€æ­¥ç ”ç©¶ï¼Œ**å¯¼è‡´è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤§é‡æ ·æœ¬æ¥å‡è½»å®ƒ**ã€‚
- en: 'So today weâ€™ll study **Actor-Critic methods**, a hybrid architecture combining
    value-based and Policy-Based methods that helps to stabilize the training by reducing
    the variance using:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»Šå¤©æˆ‘ä»¬å°†å­¦ä¹  **æ¼”å‘˜è¯„è®ºå®¶æ–¹æ³•**ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œç»“åˆäº†åŸºäºä»·å€¼å’ŒåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•æ¥å‡å°‘æ–¹å·®ï¼Œä»è€Œç¨³å®šè®­ç»ƒï¼š
- en: '*An Actor* that controls **how our agent behaves** (Policy-Based method)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä¸€ä¸ªæ¼”å‘˜* æ§åˆ¶ **æˆ‘ä»¬çš„ä»£ç†å¦‚ä½•è¡Œä¸º**ï¼ˆåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼‰'
- en: '*A Critic* that measures **how good the taken action is** (Value-Based method)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä¸€ä¸ªè¯„è®ºå®¶* è¯„ä¼° **é‡‡å–çš„è¡ŒåŠ¨æœ‰å¤šå¥½**ï¼ˆåŸºäºä»·å€¼çš„æ–¹æ³•ï¼‰'
- en: 'Weâ€™ll study one of these hybrid methods, Advantage Actor Critic (A2C), **and
    train our agent using Stable-Baselines3 in robotic environments**. Weâ€™ll train:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç ”ç©¶å…¶ä¸­ä¸€ç§æ··åˆæ–¹æ³•ï¼Œä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå®¶ï¼ˆA2Cï¼‰ï¼Œ**å¹¶åœ¨æœºå™¨äººç¯å¢ƒä¸­ä½¿ç”¨Stable-Baselines3è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†**ã€‚æˆ‘ä»¬å°†è®­ç»ƒï¼š
- en: A robotic arm ğŸ¦¾ to move to the correct position.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœºæ¢°è‡‚ ğŸ¦¾ ç§»åŠ¨åˆ°æ­£ç¡®çš„ä½ç½®ã€‚
- en: Sound exciting? Letâ€™s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥å¾ˆæ¿€åŠ¨äººå¿ƒå—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ï¼
