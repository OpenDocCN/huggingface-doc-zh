- en: UPerNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/275.21bd9ac9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)
    by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a
    general framework to effectively segment a wide range of concepts from images,
    leveraging any vision backbone like [ConvNeXt](convnext) or [Swin](swin).
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Humans recognize the visual world at multiple levels: we effortlessly categorize
    scenes and detect objects inside, while also identifying the textures and surfaces
    of the objects along with their different compositional parts. In this paper,
    we study a new task called Unified Perceptual Parsing, which requires the machine
    vision systems to recognize as many visual concepts as possible from a given image.
    A multi-task framework called UPerNet and a training strategy are developed to
    learn from heterogeneous image annotations. We benchmark our framework on Unified
    Perceptual Parsing and show that it is able to effectively segment a wide range
    of concepts from images. The trained networks are further applied to discover
    visual knowledge in natural scenes.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/a144fe8a7a6a0326f562f0adcb5e8255.png) UPerNet framework.
    Taken from the [original paper](https://arxiv.org/abs/1807.10221).'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code is based on OpenMMLabâ€™s mmsegmentation [here](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py).
  prefs: []
  type: TYPE_NORMAL
- en: Usage examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'UPerNet is a general framework for semantic segmentation. It can be used with
    any vision backbone, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To use another vision backbone, like [ConvNeXt](convnext), simply instantiate
    the model with the appropriate backbone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that this will randomly initialize all the weights of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to
    help you get started with UPerNet.
  prefs: []
  type: TYPE_NORMAL
- en: Demo notebooks for UPerNet can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Semantic segmentation task guide](../tasks/semantic_segmentation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: UperNetConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.UperNetConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/configuration_upernet.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: ( backbone_config = None hidden_size = 512 initializer_range = 0.02 pool_scales
    = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_in_channels
    = 384 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input
    = False loss_ignore_index = 255 **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**backbone_config** (`PretrainedConfig` or `dict`, *optional*, defaults to
    `ResNetConfig()`) â€” The configuration of the backbone model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 512) â€” The number of hidden
    units in the convolutional layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pool_scales** (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) â€” Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_auxiliary_head** (`bool`, *optional*, defaults to `True`) â€” Whether to
    use an auxiliary head during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**auxiliary_loss_weight** (`float`, *optional*, defaults to 0.4) â€” Weight of
    the cross-entropy loss of the auxiliary head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**auxiliary_channels** (`int`, *optional*, defaults to 256) â€” Number of channels
    to use in the auxiliary head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**auxiliary_num_convs** (`int`, *optional*, defaults to 1) â€” Number of convolutional
    layers to use in the auxiliary head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**auxiliary_concat_input** (`bool`, *optional*, defaults to `False`) â€” Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**loss_ignore_index** (`int`, *optional*, defaults to 255) â€” The index that
    is ignored by the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of an [UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation).
    It is used to instantiate an UperNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the UperNet [openmmlab/upernet-convnext-tiny](https://huggingface.co/openmmlab/upernet-convnext-tiny)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: UperNetForSemanticSegmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.UperNetForSemanticSegmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L343)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**This** model is a PyTorch [torch.nn.Module](https â€”//pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**it** as a regular PyTorch Module and refer to the PyTorch documentation for
    all matter related to general usage and â€” behavior. â€” config ([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig)):
    Model configuration class with all the parameters of the model. Initializing with
    a config file does not load the weights associated with the model, only the configuration.
    Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UperNet framework leveraging any vision backbone e.g. for ADE20k, CityScapes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L360)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states:
    Optional = None labels: Optional = None return_dict: Optional = None ) â†’ [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) â€” Pixel values. Padding will be ignored by default should you
    provide it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [SegformerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers in case the backbone has them. See `attentions`
    under returned tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers of the backbone. See `hidden_states` under returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” Classification scores for each pixel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
