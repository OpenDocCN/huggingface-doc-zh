- en: UPerNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UPerNet
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)
    by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a
    general framework to effectively segment a wide range of concepts from images,
    leveraging any vision backbone like [ConvNeXt](convnext) or [Swin](swin).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: UPerNet模型是由肖特、刘英成、周博磊、姜玉宁、孙坚在[统一感知解析用于场景理解](https://arxiv.org/abs/1807.10221)中提出的。UPerNet是一个通用框架，可以有效地从图像中分割各种概念，利用任何视觉骨干，如[ConvNeXt](convnext)或[Swin](swin)。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Humans recognize the visual world at multiple levels: we effortlessly categorize
    scenes and detect objects inside, while also identifying the textures and surfaces
    of the objects along with their different compositional parts. In this paper,
    we study a new task called Unified Perceptual Parsing, which requires the machine
    vision systems to recognize as many visual concepts as possible from a given image.
    A multi-task framework called UPerNet and a training strategy are developed to
    learn from heterogeneous image annotations. We benchmark our framework on Unified
    Perceptual Parsing and show that it is able to effectively segment a wide range
    of concepts from images. The trained networks are further applied to discover
    visual knowledge in natural scenes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*人类在多个层次上识别视觉世界：我们轻松地对场景进行分类，并检测其中的对象，同时还识别对象的纹理和表面以及它们不同的组成部分。在本文中，我们研究了一个称为统一感知解析的新任务，该任务要求机器视觉系统从给定图像中尽可能识别尽可能多的视觉概念。开发了一个名为UPerNet的多任务框架和训练策略，以从异构图像注释中学习。我们在统一感知解析上对我们的框架进行基准测试，并展示它能够有效地从图像中分割各种概念。训练的网络进一步应用于发现自然场景中的视觉知识。*'
- en: '![drawing](../Images/a144fe8a7a6a0326f562f0adcb5e8255.png) UPerNet framework.
    Taken from the [original paper](https://arxiv.org/abs/1807.10221).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示](../Images/a144fe8a7a6a0326f562f0adcb5e8255.png) UPerNet 框架。取自[原始论文](https://arxiv.org/abs/1807.10221)。'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code is based on OpenMMLab’s mmsegmentation [here](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码基于OpenMMLab的mmsegmentation
    [这里](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py)。
- en: Usage examples
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: 'UPerNet is a general framework for semantic segmentation. It can be used with
    any vision backbone, like so:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: UPerNet是语义分割的通用框架。可以与任何视觉骨干一起使用，如：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To use another vision backbone, like [ConvNeXt](convnext), simply instantiate
    the model with the appropriate backbone:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用另一个视觉骨干，如[ConvNeXt](convnext)，只需使用适当的骨干实例化模型：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that this will randomly initialize all the weights of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这将随机初始化模型的所有权重。
- en: Resources
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with UPerNet.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 官方Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用UPerNet。
- en: Demo notebooks for UPerNet can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UPerNet的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet)找到。
- en: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)支持。'
- en: 'See also: [Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[语义分割任务指南](../tasks/semantic_segmentation)
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新内容，而不是重复现有资源。
- en: UperNetConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UperNetConfig
- en: '### `class transformers.UperNetConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UperNetConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/configuration_upernet.py#L26)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/configuration_upernet.py#L26)'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`backbone_config` (`PretrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`)
    — The configuration of the backbone model.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` (`PretrainedConfig`或`dict`, *可选*, 默认为`ResNetConfig()`) —
    骨干模型的配置。'
- en: '`hidden_size` (`int`, *optional*, defaults to 512) — The number of hidden units
    in the convolutional layers.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为512) — 卷积层中隐藏单元的数量。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) — Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool_scales` (`Tuple[int]`, *可选*, 默认为`[1, 2, 3, 6]`) — 应用于最后特征图的Pooling Pyramid
    Module中使用的池化尺度。'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) — Whether to
    use an auxiliary head during training.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *可选*, 默认为`True`) — 训练期间是否使用辅助头。'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) — Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *可选*, 默认为0.4) — 辅助头的交叉熵损失的权重。'
- en: '`auxiliary_channels` (`int`, *optional*, defaults to 256) — Number of channels
    to use in the auxiliary head.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_channels` (`int`，*可选*，默认为256) — 辅助头中要使用的通道数。'
- en: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) — Number of convolutional
    layers to use in the auxiliary head.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_num_convs` (`int`，*可选*，默认为1) — 辅助头中要使用的卷积层数。'
- en: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) — Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_concat_input` (`bool`，*可选*，默认为`False`) — 是否在分类层之前将辅助头的输出与输入连接。'
- en: '`loss_ignore_index` (`int`, *optional*, defaults to 255) — The index that is
    ignored by the loss function.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_ignore_index` (`int`，*可选*，默认为255) — 损失函数忽略的索引。'
- en: This is the configuration class to store the configuration of an [UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation).
    It is used to instantiate an UperNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the UperNet [openmmlab/upernet-convnext-tiny](https://huggingface.co/openmmlab/upernet-convnext-tiny)
    architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)的配置。它用于根据指定的参数实例化一个UperNet模型，定义模型架构。使用默认值实例化配置将产生类似于UperNet
    [openmmlab/upernet-convnext-tiny](https://huggingface.co/openmmlab/upernet-convnext-tiny)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: UperNetForSemanticSegmentation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UperNetForSemanticSegmentation
- en: '### `class transformers.UperNetForSemanticSegmentation`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UperNetForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L343)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L343)'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`This` model is a PyTorch [torch.nn.Module](https —//pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`This`模型是PyTorch [torch.nn.Module](https —//pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。使用'
- en: '`it` as a regular PyTorch Module and refer to the PyTorch documentation for
    all matter related to general usage and — behavior. — config ([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig)):
    Model configuration class with all the parameters of the model. Initializing with
    a config file does not load the weights associated with the model, only the configuration.
    Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`it`作为常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。配置([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig))：模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: UperNet framework leveraging any vision backbone e.g. for ADE20k, CityScapes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: UperNet框架利用任何视觉骨干，例如ADE20k，CityScapes。
- en: '#### `forward`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L360)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L360)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [SegformerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__)
    for details.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。默认情况下会忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[SegformerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__)。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers in case the backbone has them. See `attentions`
    under returned tensors for more detail.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回骨干具有注意力张量的所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers of the backbone. See `hidden_states` under returned
    tensors for more detail.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回骨干的所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, height, width)`，*可选*) — 用于计算损失的地面真实语义分割地图。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig))
    and inputs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig)）和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`, *可选的*, 当提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) — Classification scores for each pixel.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels, logits_height,
    logits_width)`) — 每个像素的分类分数。'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">返回的logits不一定与作为输入传递的`pixel_values`具有相同的大小。这是为了避免进行两次插值并在用户需要将logits调整为原始图像大小时丢失一些质量。您应始终检查logits的形状并根据需要调整大小。</tip>
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, patch_size, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
