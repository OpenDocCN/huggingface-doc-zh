["```py\nimport { AutoModel, AutoTokenizer } from '@xenova/transformers';\n\nlet tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\nlet model = await AutoModel.from_pretrained('Xenova/bert-base-uncased');\n\nlet inputs = await tokenizer('I love transformers!');\nlet { logits } = await model(inputs);\n// Tensor {\n//     data: Float32Array(183132) [-7.117443084716797, -7.107812881469727, -7.092104911804199, ...]\n//     dims: (3) [1, 6, 30522],\n//     type: \"float32\",\n//     size: 183132,\n// }\n```", "```py\nimport { AutoModelForSeq2SeqLM, AutoTokenizer } from '@xenova/transformers';\n\nlet tokenizer = await AutoTokenizer.from_pretrained('Xenova/t5-small');\nlet model = await AutoModelForSeq2SeqLM.from_pretrained('Xenova/t5-small');\n\nlet { input_ids } = await tokenizer('translate English to German: I love transformers!');\nlet outputs = await model.generate(input_ids);\nlet decoded = tokenizer.decode(outputs[0], { skip_special_tokens: true });\n// 'Ich liebe Transformatoren!'\n```", "```py\nimport { AutoTokenizer, AutoProcessor, CLIPModel, RawImage } from '@xenova/transformers';\n\n// Load tokenizer, processor, and model\nlet tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\nlet processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\nlet model = await CLIPModel.from_pretrained('Xenova/clip-vit-base-patch16');\n\n// Run tokenization\nlet texts = ['a photo of a car', 'a photo of a football match']\nlet text_inputs = tokenizer(texts, { padding: true, truncation: true });\n\n// Read image and run processor\nlet image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\nlet image_inputs = await processor(image);\n\n// Run model with both text and pixel inputs\nlet output = await model({ ...text_inputs, ...image_inputs });\n// {\n//   logits_per_image: Tensor {\n//     dims: [ 1, 2 ],\n//     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n//   },\n//   logits_per_text: Tensor {\n//     dims: [ 2, 1 ],\n//     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n//   },\n//   text_embeds: Tensor {\n//     dims: [ 2, 512 ],\n//     data: Float32Array(1024) [ ... ],\n//   },\n//   image_embeds: Tensor {\n//     dims: [ 1, 512 ],\n//     data: Float32Array(512) [ ... ],\n//   }\n// }\n```", "```py\nimport { AutoTokenizer, CLIPTextModelWithProjection } from '@xenova/transformers';\n\n// Load tokenizer and text model\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\nconst text_model = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n\n// Run tokenization\nlet texts = ['a photo of a car', 'a photo of a football match'];\nlet text_inputs = tokenizer(texts, { padding: true, truncation: true });\n\n// Compute embeddings\nconst { text_embeds } = await text_model(text_inputs);\n// Tensor {\n//   dims: [ 2, 512 ],\n//   type: 'float32',\n//   data: Float32Array(1024) [ ... ],\n//   size: 1024\n// }\n```", "```py\nimport { AutoProcessor, CLIPVisionModelWithProjection, RawImage} from '@xenova/transformers';\n\n// Load processor and vision model\nconst processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\nconst vision_model = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n\n// Read image and run processor\nlet image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\nlet image_inputs = await processor(image);\n\n// Compute embeddings\nconst { image_embeds } = await vision_model(image_inputs);\n// Tensor {\n//   dims: [ 1, 512 ],\n//   type: 'float32',\n//   data: Float32Array(512) [ ... ],\n//   size: 512\n// }\n```", "```py\nimport { AutoTokenizer, AutoProcessor, SiglipModel, RawImage } from '@xenova/transformers';\n\n// Load tokenizer, processor, and model\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');\nconst processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');\nconst model = await SiglipModel.from_pretrained('Xenova/siglip-base-patch16-224');\n\n// Run tokenization\nconst texts = ['a photo of 2 cats', 'a photo of 2 dogs'];\nconst text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });\n\n// Read image and run processor\nconst image = await RawImage.read('http://images.cocodataset.org/val2017/000000039769.jpg');\nconst image_inputs = await processor(image);\n\n// Run model with both text and pixel inputs\nconst output = await model({ ...text_inputs, ...image_inputs });\n// {\n//   logits_per_image: Tensor {\n//     dims: [ 1, 2 ],\n//     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],\n//   },\n//   logits_per_text: Tensor {\n//     dims: [ 2, 1 ],\n//     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],\n//   },\n//   text_embeds: Tensor {\n//     dims: [ 2, 768 ],\n//     data: Float32Array(1536) [ ... ],\n//   },\n//   image_embeds: Tensor {\n//     dims: [ 1, 768 ],\n//     data: Float32Array(768) [ ... ],\n//   }\n// }\n```", "```py\nimport { AutoTokenizer, SiglipTextModel } from '@xenova/transformers';\n\n// Load tokenizer and text model\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');\nconst text_model = await SiglipTextModel.from_pretrained('Xenova/siglip-base-patch16-224');\n\n// Run tokenization\nconst texts = ['a photo of 2 cats', 'a photo of 2 dogs'];\nconst text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });\n\n// Compute embeddings\nconst { pooler_output } = await text_model(text_inputs);\n// Tensor {\n//   dims: [ 2, 768 ],\n//   type: 'float32',\n//   data: Float32Array(1536) [ ... ],\n//   size: 1536\n// }\n```", "```py\nimport { AutoProcessor, SiglipVisionModel, RawImage} from '@xenova/transformers';\n\n// Load processor and vision model\nconst processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');\nconst vision_model = await SiglipVisionModel.from_pretrained('Xenova/siglip-base-patch16-224');\n\n// Read image and run processor\nconst image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\nconst image_inputs = await processor(image);\n\n// Compute embeddings\nconst { pooler_output } = await vision_model(image_inputs);\n// Tensor {\n//   dims: [ 1, 768 ],\n//   type: 'float32',\n//   data: Float32Array(768) [ ... ],\n//   size: 768\n// }\n```", "```py\nimport { AutoTokenizer, AutoProcessor, CLIPSegForImageSegmentation, RawImage } from '@xenova/transformers';\n\n// Load tokenizer, processor, and model\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/clipseg-rd64-refined');\nconst processor = await AutoProcessor.from_pretrained('Xenova/clipseg-rd64-refined');\nconst model = await CLIPSegForImageSegmentation.from_pretrained('Xenova/clipseg-rd64-refined');\n\n// Run tokenization\nconst texts = ['a glass', 'something to fill', 'wood', 'a jar'];\nconst text_inputs = tokenizer(texts, { padding: true, truncation: true });\n\n// Read image and run processor\nconst image = await RawImage.read('https://github.com/timojl/clipseg/blob/master/example_image.jpg?raw=true');\nconst image_inputs = await processor(image);\n\n// Run model with both text and pixel inputs\nconst { logits } = await model({ ...text_inputs, ...image_inputs });\n// logits: Tensor {\n//   dims: [4, 352, 352],\n//   type: 'float32',\n//   data: Float32Array(495616) [ ... ],\n//   size: 495616\n// }\n```", "```py\nconst preds = logits\n  .unsqueeze_(1)\n  .sigmoid_()\n  .mul_(255)\n  .round_()\n  .to('uint8');\n\nfor (let i = 0; i < preds.dims[0]; ++i) {\n  const img = RawImage.fromTensor(preds[i]);\n  img.save(`prediction_${i}.png`);\n}\n```", "```py\nimport { AutoProcessor, VitMatteForImageMatting, RawImage } from '@xenova/transformers';\n\n// Load processor and model\nconst processor = await AutoProcessor.from_pretrained('Xenova/vitmatte-small-distinctions-646');\nconst model = await VitMatteForImageMatting.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n\n// Load image and trimap\nconst image = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_image.png');\nconst trimap = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_trimap.png');\n\n// Prepare image + trimap for the model\nconst inputs = await processor(image, trimap);\n\n// Predict alpha matte\nconst { alphas } = await model(inputs);\n// Tensor {\n//   dims: [ 1, 1, 640, 960 ],\n//   type: 'float32',\n//   size: 614400,\n//   data: Float32Array(614400) [ 0.9894027709960938, 0.9970508813858032, ... ]\n// }\n```", "```py\nimport { Tensor, cat } from '@xenova/transformers';\n\n// Visualize predicted alpha matte\nconst imageTensor = new Tensor(\n  'uint8',\n  new Uint8Array(image.data),\n  [image.height, image.width, image.channels]\n).transpose(2, 0, 1);\n\n// Convert float (0-1) alpha matte to uint8 (0-255)\nconst alphaChannel = alphas\n  .squeeze(0)\n  .mul_(255)\n  .clamp_(0, 255)\n  .round_()\n  .to('uint8');\n\n// Concatenate original image with predicted alpha\nconst imageData = cat([imageTensor, alphaChannel], 0);\n\n// Save output image\nconst outputImage = RawImage.fromTensor(imageData);\noutputImage.save('output.png');\n```", "```py\nimport { AutoProcessor, Swin2SRForImageSuperResolution, RawImage } from '@xenova/transformers';\n\n// Load processor and model\nconst model_id = 'Xenova/swin2SR-classical-sr-x2-64';\nconst processor = await AutoProcessor.from_pretrained(model_id);\nconst model = await Swin2SRForImageSuperResolution.from_pretrained(model_id);\n\n// Prepare model inputs\nconst url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/butterfly.jpg';\nconst image = await RawImage.fromURL(url);\nconst inputs = await processor(image);\n\n// Run model\nconst outputs = await model(inputs);\n\n// Convert Tensor to RawImage\nconst output = outputs.reconstruction.squeeze().clamp_(0, 1).mul_(255).round_().to('uint8');\nconst outputImage = RawImage.fromTensor(output);\n// RawImage {\n//   data: Uint8Array(786432) [ 41, 31, 24, ... ],\n//   width: 512,\n//   height: 512,\n//   channels: 3\n// }\n```", "```py\nimport { DPTForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n\n// Load model and processor\nconst model_id = 'Xenova/dpt-hybrid-midas';\nconst model = await DPTForDepthEstimation.from_pretrained(model_id);\nconst processor = await AutoProcessor.from_pretrained(model_id);\n\n// Load image from URL\nconst url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\nconst image = await RawImage.fromURL(url);\n\n// Prepare image for the model\nconst inputs = await processor(image);\n\n// Run model\nconst { predicted_depth } = await model(inputs);\n\n// Interpolate to original size\nconst prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n\n// Visualize the prediction\nconst formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\nconst depth = RawImage.fromTensor(formatted);\n// RawImage {\n//   data: Uint8Array(307200) [ 85, 85, 84, ... ],\n//   width: 640,\n//   height: 480,\n//   channels: 1\n// }\n```", "```py\nimport { GLPNForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n\n// Load model and processor\nconst model_id = 'Xenova/glpn-kitti';\nconst model = await GLPNForDepthEstimation.from_pretrained(model_id);\nconst processor = await AutoProcessor.from_pretrained(model_id);\n\n// Load image from URL\nconst url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\nconst image = await RawImage.fromURL(url);\n\n// Prepare image for the model\nconst inputs = await processor(image);\n\n// Run model\nconst { predicted_depth } = await model(inputs);\n\n// Interpolate to original size\nconst prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n\n// Visualize the prediction\nconst formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\nconst depth = RawImage.fromTensor(formatted);\n// RawImage {\n//   data: Uint8Array(307200) [ 207, 169, 154, ... ],\n//   width: 640,\n//   height: 480,\n//   channels: 1\n// }\n```", "```py\nimport { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n\n// Choose model to use\nconst model_id = 'Xenova/donut-base-finetuned-cord-v2';\n\n// Prepare image inputs\nconst processor = await AutoProcessor.from_pretrained(model_id);\nconst url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/receipt.png';\nconst image = await RawImage.read(url);\nconst image_inputs = await processor(image);\n\n// Prepare decoder inputs\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst task_prompt = '<s_cord-v2>';\nconst decoder_input_ids = tokenizer(task_prompt, {\n  add_special_tokens: false,\n}).input_ids;\n\n// Create the model\nconst model = await AutoModelForVision2Seq.from_pretrained(model_id);\n\n// Run inference\nconst output = await model.generate(image_inputs.pixel_values, {\n  decoder_input_ids,\n  max_length: model.config.decoder.max_position_embeddings,\n});\n\n// Decode output\nconst decoded = tokenizer.batch_decode(output)[0];\n// <s_cord-v2><s_menu><s_nm> CINNAMON SUGAR</s_nm><s_unitprice> 17,000</s_unitprice><s_cnt> 1 x</s_cnt><s_price> 17,000</s_price></s_menu><s_sub_total><s_subtotal_price> 17,000</s_subtotal_price></s_sub_total><s_total><s_total_price> 17,000</s_total_price><s_cashprice> 20,000</s_cashprice><s_changeprice> 3,000</s_changeprice></s_total></s>\n```", "```py\nimport { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n\n// Choose model to use\nconst model_id = 'Xenova/donut-base-finetuned-docvqa';\n\n// Prepare image inputs\nconst processor = await AutoProcessor.from_pretrained(model_id);\nconst url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/invoice.png';\nconst image = await RawImage.read(url);\nconst image_inputs = await processor(image);\n\n// Prepare decoder inputs\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst question = 'What is the invoice number?';\nconst task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;\nconst decoder_input_ids = tokenizer(task_prompt, {\n  add_special_tokens: false,\n}).input_ids;\n\n// Create the model\nconst model = await AutoModelForVision2Seq.from_pretrained(model_id);\n\n// Run inference\nconst output = await model.generate(image_inputs.pixel_values, {\n  decoder_input_ids,\n  max_length: model.config.decoder.max_position_embeddings,\n});\n\n// Decode output\nconst decoded = tokenizer.batch_decode(output)[0];\n// <s_docvqa><s_question> What is the invoice number?</s_question><s_answer> us-001</s_answer></s>\n```", "```py\nimport { SamModel, AutoProcessor, RawImage } from '@xenova/transformers';\n\nconst model = await SamModel.from_pretrained('Xenova/sam-vit-base');\nconst processor = await AutoProcessor.from_pretrained('Xenova/sam-vit-base');\n\nconst img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png';\nconst raw_image = await RawImage.read(img_url);\nconst input_points = [[[450, 600]]] // 2D localization of a window\n\nconst inputs = await processor(raw_image, input_points);\nconst outputs = await model(inputs);\n\nconst masks = await processor.post_process_masks(outputs.pred_masks, inputs.original_sizes, inputs.reshaped_input_sizes);\n// [\n//   Tensor {\n//     dims: [ 1, 3, 1764, 2646 ],\n//     type: 'bool',\n//     data: Uint8Array(14002632) [ ... ],\n//     size: 14002632\n//   }\n// ]\nconst scores = outputs.iou_scores;\n// Tensor {\n//   dims: [ 1, 1, 3 ],\n//   type: 'float32',\n//   data: Float32Array(3) [\n//     0.8892380595207214,\n//     0.9311248064041138,\n//     0.983696699142456\n//   ],\n//   size: 3\n// }\n```", "```py\nimport { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n\n// Read and preprocess audio\nconst processor = await AutoProcessor.from_pretrained('Xenova/mms-300m');\nconst audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\nconst inputs = await processor(audio);\n\n// Run model with inputs\nconst model = await AutoModel.from_pretrained('Xenova/mms-300m');\nconst output = await model(inputs);\n// {\n//   last_hidden_state: Tensor {\n//     dims: [ 1, 1144, 1024 ],\n//     type: 'float32',\n//     data: Float32Array(1171456) [ ... ],\n//     size: 1171456\n//   }\n// }\n```", "```py\nimport { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n\n// Read and preprocess audio\nconst processor = await AutoProcessor.from_pretrained('Xenova/hubert-base-ls960');\nconst audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\nconst inputs = await processor(audio);\n\n// Load and run model with inputs\nconst model = await AutoModel.from_pretrained('Xenova/hubert-base-ls960');\nconst output = await model(inputs);\n// {\n//   last_hidden_state: Tensor {\n//     dims: [ 1, 549, 768 ],\n//     type: 'float32',\n//     data: Float32Array(421632) [0.0682469978928566, 0.08104046434164047, -0.4975186586380005, ...],\n//     size: 421632\n//   }\n// }\n```", "```py\nimport { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n\n// Read and preprocess audio\nconst processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base');\nconst audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\nconst inputs = await processor(audio);\n\n// Run model with inputs\nconst model = await AutoModel.from_pretrained('Xenova/wavlm-base');\nconst output = await model(inputs);\n// {\n//   last_hidden_state: Tensor {\n//     dims: [ 1, 549, 768 ],\n//     type: 'float32',\n//     data: Float32Array(421632) [-0.349443256855011, -0.39341306686401367,  0.022836603224277496, ...],\n//     size: 421632\n//   }\n// }\n```", "```py\nimport { AutoTokenizer, AutoProcessor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, Tensor } from '@xenova/transformers';\n\n// Load the tokenizer and processor\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/speecht5_tts');\nconst processor = await AutoProcessor.from_pretrained('Xenova/speecht5_tts');\n\n// Load the models\n// NOTE: We use the unquantized versions as they are more accurate\nconst model = await SpeechT5ForTextToSpeech.from_pretrained('Xenova/speecht5_tts', { quantized: false });\nconst vocoder = await SpeechT5HifiGan.from_pretrained('Xenova/speecht5_hifigan', { quantized: false });\n\n// Load speaker embeddings from URL\nconst speaker_embeddings_data = new Float32Array(\n    await (await fetch('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin')).arrayBuffer()\n);\nconst speaker_embeddings = new Tensor(\n    'float32',\n    speaker_embeddings_data,\n    [1, speaker_embeddings_data.length]\n)\n\n// Run tokenization\nconst { input_ids } = tokenizer('Hello, my dog is cute');\n\n// Generate waveform\nconst { waveform } = await model.generate_speech(input_ids, speaker_embeddings, { vocoder });\nconsole.log(waveform)\n// Tensor {\n//   dims: [ 26112 ],\n//   type: 'float32',\n//   size: 26112,\n//   data: Float32Array(26112) [ -0.00043630177970044315, -0.00018082228780258447, ... ],\n// }\n```", "```py\nimport { AutoTokenizer, ClapTextModelWithProjection } from '@xenova/transformers';\n\n// Load tokenizer and text model\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/clap-htsat-unfused');\nconst text_model = await ClapTextModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n\n// Run tokenization\nconst texts = ['a sound of a cat', 'a sound of a dog'];\nconst text_inputs = tokenizer(texts, { padding: true, truncation: true });\n\n// Compute embeddings\nconst { text_embeds } = await text_model(text_inputs);\n// Tensor {\n//   dims: [ 2, 512 ],\n//   type: 'float32',\n//   data: Float32Array(1024) [ ... ],\n//   size: 1024\n// }\n```", "```py\nimport { AutoProcessor, ClapAudioModelWithProjection, read_audio } from '@xenova/transformers';\n\n// Load processor and audio model\nconst processor = await AutoProcessor.from_pretrained('Xenova/clap-htsat-unfused');\nconst audio_model = await ClapAudioModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n\n// Read audio and run processor\nconst audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cat_meow.wav');\nconst audio_inputs = await processor(audio);\n\n// Compute embeddings\nconst { audio_embeds } = await audio_model(audio_inputs);\n// Tensor {\n//   dims: [ 1, 512 ],\n//   type: 'float32',\n//   data: Float32Array(512) [ ... ],\n//   size: 512\n// }\n```", "```py\nimport { AutoTokenizer, VitsModel } from '@xenova/transformers';\n\n// Load the tokenizer and model\nconst tokenizer = await AutoTokenizer.from_pretrained('Xenova/mms-tts-eng');\nconst model = await VitsModel.from_pretrained('Xenova/mms-tts-eng');\n\n// Run tokenization\nconst inputs = tokenizer('I love transformers');\n\n// Generate waveform\nconst { waveform } = await model(inputs);\n// Tensor {\n//   dims: [ 1, 35328 ],\n//   type: 'float32',\n//   data: Float32Array(35328) [ ... ],\n//   size: 35328,\n// }\n```"]