- en: MatCha
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MatCha
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining
    with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662),
    from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,
    Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'MatCha在论文[MatCha: Enhancing Visual Language Pretraining with Math Reasoning
    and Chart Derendering](https://arxiv.org/abs/2212.09662)中提出，作者是Fangyu Liu、Francesco
    Piccinno、Syrine Krichene、Chenxi Pang、Kenton Lee、Mandar Joshi、Yasemin Altun、Nigel
    Collier、Julian Martin Eisenschlos。'
- en: 'The abstract of the paper states the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Visual language data such as plots, charts, and infographics are ubiquitous
    in the human world. However, state-of-the-art vision-language models do not perform
    well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining)
    to enhance visual language models’ capabilities in jointly modeling charts/plots
    and language data. Specifically, we propose several pretraining tasks that cover
    plot deconstruction and numerical reasoning which are the key capabilities in
    visual language modeling. We perform the MatCha pretraining starting from Pix2Struct,
    a recently proposed image-to-text visual language model. On standard benchmarks
    such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods
    by as much as nearly 20%. We also examine how well MatCha pretraining transfers
    to domains such as screenshots, textbook diagrams, and document figures and observe
    overall improvement, verifying the usefulness of MatCha pretraining on broader
    visual language tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*视觉语言数据，如图表和信息图表，在人类世界中随处可见。然而，最先进的视觉语言模型在这些数据上表现不佳。我们提出MatCha（数学推理和图表解渲染预训练）来增强视觉语言模型在联合建模图表/图表和语言数据方面的能力。具体来说，我们提出了几个预训练任务，涵盖了图表解构和数值推理，这是视觉语言建模中的关键能力。我们从Pix2Struct开始执行MatCha预训练，Pix2Struct是最近提出的图像到文本视觉语言模型。在PlotQA和ChartQA等标准基准测试中，MatCha模型的表现优于最先进的方法近20％。我们还研究了MatCha预训练在诸如屏幕截图、教科书图表和文档图表等领域的转移情况，并观察到整体改进，验证了MatCha预训练在更广泛的视觉语言任务上的有用性。*'
- en: Model description
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型描述
- en: MatCha is a model that is trained using `Pix2Struct` architecture. You can find
    more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).
    MatCha is a Visual Question Answering subset of `Pix2Struct` architecture. It
    renders the input question on the image and predicts the answer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MatCha是使用`Pix2Struct`架构训练的模型。您可以在[Pix2Struct文档](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct)中找到有关`Pix2Struct`的更多信息。MatCha是`Pix2Struct`架构的视觉问答子集。它在图像上呈现输入问题并预测答案。
- en: Usage
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法
- en: 'Currently 6 checkpoints are available for MatCha:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有6个MatCha的检查点可用：
- en: '`google/matcha`: the base MatCha model, used to fine-tune MatCha on downstream
    tasks'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/matcha`：基础MatCha模型，用于在下游任务上微调MatCha'
- en: '`google/matcha-chartqa`: MatCha model fine-tuned on ChartQA dataset. It can
    be used to answer questions about charts.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/matcha-chartqa`：在ChartQA数据集上微调的MatCha模型。可用于回答有关图表的问题。'
- en: '`google/matcha-plotqa-v1`: MatCha model fine-tuned on PlotQA dataset. It can
    be used to answer questions about plots.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/matcha-plotqa-v1`：在PlotQA数据集上微调的MatCha模型。可用于回答有关图表的问题。'
- en: '`google/matcha-plotqa-v2`: MatCha model fine-tuned on PlotQA dataset. It can
    be used to answer questions about plots.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/matcha-plotqa-v2`：在PlotQA数据集上微调的MatCha模型。可用于回答有关图表的问题。'
- en: '`google/matcha-chart2text-statista`: MatCha model fine-tuned on Statista dataset.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/matcha-chart2text-statista`：在Statista数据集上微调的MatCha模型。'
- en: '`google/matcha-chart2text-pew`: MatCha model fine-tuned on Pew dataset.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/matcha-chart2text-pew`：在Pew数据集上微调的MatCha模型。'
- en: The models finetuned on `chart2text-pew` and `chart2text-statista` are more
    suited for summarization, whereas the models finetuned on `plotqa` and `chartqa`
    are more suited for question answering.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在`chart2text-pew`和`chart2text-statista`上微调的模型更适合摘要，而在`plotqa`和`chartqa`上微调的模型更适合回答问题。
- en: 'You can use these models as follows (example on a ChatQA dataset):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按以下方式使用这些模型（在ChatQA数据集上的示例）：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Fine-tuning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: 'To fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb).
    For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor
    and cosine learning rate scheduler leads to faste convergence:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要微调MatCha，请参考pix2struct [微调笔记本](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)。对于`Pix2Struct`模型，我们发现使用Adafactor和余弦学习率调度器微调模型会导致更快的收敛：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: MatCha is a model that is trained using `Pix2Struct` architecture. You can find
    more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MatCha是使用`Pix2Struct`架构训练的模型。您可以在[Pix2Struct文档](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct)中找到有关`Pix2Struct`的更多信息。
