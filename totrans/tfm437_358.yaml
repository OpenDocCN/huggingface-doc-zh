- en: MatCha
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining
    with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662),
    from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,
    Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract of the paper states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Visual language data such as plots, charts, and infographics are ubiquitous
    in the human world. However, state-of-the-art vision-language models do not perform
    well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining)
    to enhance visual language modelsâ€™ capabilities in jointly modeling charts/plots
    and language data. Specifically, we propose several pretraining tasks that cover
    plot deconstruction and numerical reasoning which are the key capabilities in
    visual language modeling. We perform the MatCha pretraining starting from Pix2Struct,
    a recently proposed image-to-text visual language model. On standard benchmarks
    such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods
    by as much as nearly 20%. We also examine how well MatCha pretraining transfers
    to domains such as screenshots, textbook diagrams, and document figures and observe
    overall improvement, verifying the usefulness of MatCha pretraining on broader
    visual language tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: Model description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MatCha is a model that is trained using `Pix2Struct` architecture. You can find
    more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).
    MatCha is a Visual Question Answering subset of `Pix2Struct` architecture. It
    renders the input question on the image and predicts the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Currently 6 checkpoints are available for MatCha:'
  prefs: []
  type: TYPE_NORMAL
- en: '`google/matcha`: the base MatCha model, used to fine-tune MatCha on downstream
    tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`google/matcha-chartqa`: MatCha model fine-tuned on ChartQA dataset. It can
    be used to answer questions about charts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`google/matcha-plotqa-v1`: MatCha model fine-tuned on PlotQA dataset. It can
    be used to answer questions about plots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`google/matcha-plotqa-v2`: MatCha model fine-tuned on PlotQA dataset. It can
    be used to answer questions about plots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`google/matcha-chart2text-statista`: MatCha model fine-tuned on Statista dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`google/matcha-chart2text-pew`: MatCha model fine-tuned on Pew dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The models finetuned on `chart2text-pew` and `chart2text-statista` are more
    suited for summarization, whereas the models finetuned on `plotqa` and `chartqa`
    are more suited for question answering.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use these models as follows (example on a ChatQA dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb).
    For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor
    and cosine learning rate scheduler leads to faste convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: MatCha is a model that is trained using `Pix2Struct` architecture. You can find
    more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).
  prefs: []
  type: TYPE_NORMAL
