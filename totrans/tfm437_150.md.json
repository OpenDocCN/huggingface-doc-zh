["```py\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```", "```py\n>>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n\n>>> tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n>>> model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n>>> PROMPT = '''def remove_non_ascii(s: str) -> str:\n    \"\"\" <FILL_ME>\n    return result\n'''\n>>> input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n>>> generated_ids = model.generate(input_ids, max_new_tokens=128)\n\n>>> filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n>>> print(PROMPT.replace(\"<FILL_ME>\", filling))\ndef remove_non_ascii(s: str) -> str:\n    \"\"\" Remove non-ASCII characters from a string.\n\n    Args:\n        s: The string to remove non-ASCII characters from.\n\n    Returns:\n        The string with non-ASCII characters removed.\n    \"\"\"\n    result = \"\"\n    for c in s:\n        if ord(c) < 128:\n            result += c\n    return result\n```", "```py\n>>> from transformers import pipeline\n>>> import torch\n\n>>> generator = pipeline(\"text-generation\",model=\"codellama/CodeLlama-7b-hf\",torch_dtype=torch.float16, device_map=\"auto\")\n>>> generator('def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return result', max_new_tokens = 128, return_type = 1)\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n>>> from transformers import CodeLlamaTokenizerFast\n\n>>> tokenizer = CodeLlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n>>> tokenizer.encode(\"Hello this is a test\")\n[1, 15043, 445, 338, 263, 1243]\n```"]