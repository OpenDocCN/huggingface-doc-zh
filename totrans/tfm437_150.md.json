["```py\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```", "```py\n>>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n\n>>> tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n>>> model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n>>> PROMPT = '''def remove_non_ascii(s: str) -> str:\n    \"\"\" <FILL_ME>\n    return result\n'''\n>>> input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n>>> generated_ids = model.generate(input_ids, max_new_tokens=128)\n\n>>> filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n>>> print(PROMPT.replace(\"<FILL_ME>\", filling))\ndef remove_non_ascii(s: str) -> str:\n    \"\"\" Remove non-ASCII characters from a string.\n\n    Args:\n        s: The string to remove non-ASCII characters from.\n\n    Returns:\n        The string with non-ASCII characters removed.\n    \"\"\"\n    result = \"\"\n    for c in s:\n        if ord(c) < 128:\n            result += c\n    return result\n```", "```py\n>>> from transformers import pipeline\n>>> import torch\n\n>>> generator = pipeline(\"text-generation\",model=\"codellama/CodeLlama-7b-hf\",torch_dtype=torch.float16, device_map=\"auto\")\n>>> generator('def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return result', max_new_tokens = 128, return_type = 1)\n```", "```py\n( vocab_file unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' prefix_token = '\u2581<PRE>' middle_token = '\u2581<MID>' suffix_token = '\u2581<SUF>' eot_token = '\u2581<EOT>' fill_token = '<FILL_ME>' suffix_first = False sp_model_kwargs: Optional = None add_bos_token = True add_eos_token = False clean_up_tokenization_spaces = False additional_special_tokens = None use_default_system_prompt = False **kwargs )\n```", "```py\n( token_ids_0 token_ids_1 = None )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( save_directory filename_prefix: Optional = None ) \u2192 export const metadata = 'undefined';Tuple(str)\n```", "```py\n( vocab_file = None tokenizer_file = None clean_up_tokenization_spaces = False unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' prefix_token = '\u2581<PRE>' middle_token = '\u2581<MID>' suffix_token = '\u2581<SUF>' eot_token = '\u2581<EOT>' fill_token = '<FILL_ME>' additional_special_tokens = None add_bos_token = True add_eos_token = False use_default_system_prompt = False **kwargs )\n```", "```py\n>>> from transformers import CodeLlamaTokenizerFast\n\n>>> tokenizer = CodeLlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n>>> tokenizer.encode(\"Hello this is a test\")\n[1, 15043, 445, 338, 263, 1243]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';A list of integers in the range [0, 1]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```"]