- en: Prompt tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/prompt_tuning](https://huggingface.co/docs/peft/package_reference/prompt_tuning)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[Prompt tuning](https://hf.co/papers/2104.08691) adds task-specific prompts
    to the input, and these prompt parameters are updated independently of the pretrained
    model parameters which are frozen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this work, we explore “prompt tuning”, a simple yet effective mechanism
    for learning “soft prompts” to condition frozen language models to perform specific
    downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts
    are learned through backpropagation and can be tuned to incorporate signal from
    any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s
    “few-shot” learning by a large margin. More remarkably, through ablations on model
    size using T5, we show that prompt tuning becomes more competitive with scale:
    as models exceed billions of parameters, our method “closes the gap” and matches
    the strong performance of model tuning (where all model weights are tuned). This
    finding is especially relevant in that large models are costly to share and serve,
    and the ability to reuse one frozen model for multiple downstream tasks can ease
    this burden. Our method can be seen as a simplification of the recently proposed
    “prefix tuning” of Li and Liang (2021), and we provide a comparison to this and
    other similar approaches. Finally, we show that conditioning a frozen model with
    soft prompts confers benefits in robustness to domain transfer, as compared to
    full model tuning*.'
  prefs: []
  type: TYPE_NORMAL
- en: PromptTuningConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PromptTuningConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/config.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt_tuning_init` (Union[`PromptTuningInit`, `str`]) — The initialization
    of the prompt embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_tuning_init_text` (`str`, *optional*) — The text to initialize the
    prompt embedding. Only used if `prompt_tuning_init` is `TEXT`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer_name_or_path` (`str`, *optional*) — The name or path of the tokenizer.
    Only used if `prompt_tuning_init` is `TEXT`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer_kwargs` (`dict`, *optional*) — The keyword arguments to pass to
    `AutoTokenizer.from_pretrained`. Only used if `prompt_tuning_init` is `TEXT`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [PromptEmbedding](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptEmbedding).
  prefs: []
  type: TYPE_NORMAL
- en: PromptEmbedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PromptEmbedding`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/model.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PromptTuningConfig](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptTuningConfig))
    — The configuration of the prompt embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_embeddings` (`torch.nn.Module`) — The word embeddings of the base transformer
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model to encode virtual tokens into prompt embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding` (`torch.nn.Embedding`) — The embedding layer of the prompt embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Input Shape: (`batch_size`, `total_virtual_tokens`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output Shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)'
  prefs: []
  type: TYPE_NORMAL
