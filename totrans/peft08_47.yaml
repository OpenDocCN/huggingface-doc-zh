- en: Prompt tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示调整
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/prompt_tuning](https://huggingface.co/docs/peft/package_reference/prompt_tuning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/package_reference/prompt_tuning](https://huggingface.co/docs/peft/package_reference/prompt_tuning)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Prompt tuning](https://hf.co/papers/2104.08691) adds task-specific prompts
    to the input, and these prompt parameters are updated independently of the pretrained
    model parameters which are frozen.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prompt tuning](https://hf.co/papers/2104.08691) 将任务特定的提示添加到输入中，这些提示参数独立于冻结的预训练模型参数进行更新。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*In this work, we explore “prompt tuning”, a simple yet effective mechanism
    for learning “soft prompts” to condition frozen language models to perform specific
    downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts
    are learned through backpropagation and can be tuned to incorporate signal from
    any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s
    “few-shot” learning by a large margin. More remarkably, through ablations on model
    size using T5, we show that prompt tuning becomes more competitive with scale:
    as models exceed billions of parameters, our method “closes the gap” and matches
    the strong performance of model tuning (where all model weights are tuned). This
    finding is especially relevant in that large models are costly to share and serve,
    and the ability to reuse one frozen model for multiple downstream tasks can ease
    this burden. Our method can be seen as a simplification of the recently proposed
    “prefix tuning” of Li and Liang (2021), and we provide a comparison to this and
    other similar approaches. Finally, we show that conditioning a frozen model with
    soft prompts confers benefits in robustness to domain transfer, as compared to
    full model tuning*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这项工作中，我们探索了“提示调整”，这是一种简单而有效的机制，用于学习“软提示”，以使冻结的语言模型执行特定的下游任务。与GPT-3使用的离散文本提示不同，软提示是通过反向传播学习的，可以调整以包含任意数量的标记示例的信号。我们的端到端学习方法在“少样本”学习方面远远优于GPT-3。更为显著的是，通过使用T5对模型规模进行消融，我们发现随着模型超过数十亿个参数，提示调整变得更具竞争力：当所有模型权重都被调整时，我们的方法“缩小了差距”并与模型调整的强大性能相匹配。这一发现尤其重要，因为大型模型的共享和服务成本高昂，而重复使用一个冻结模型来执行多个下游任务可以减轻这一负担。我们的方法可以看作是李和梁（2021年）最近提出的“前缀调整”的简化版本，并且我们将其与此及其他类似方法进行比较。最后，我们展示了使用软提示对冻结模型进行调整在对领域转移的鲁棒性方面具有益处，相比之下，与完全模型调整相比*。'
- en: PromptTuningConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PromptTuningConfig
- en: '### `class peft.PromptTuningConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PromptTuningConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/config.py#L28)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/config.py#L28)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt_tuning_init` (Union[`PromptTuningInit`, `str`]) — The initialization
    of the prompt embedding.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_tuning_init` (Union[`PromptTuningInit`, `str`]) — 提示嵌入的初始化。'
- en: '`prompt_tuning_init_text` (`str`, *optional*) — The text to initialize the
    prompt embedding. Only used if `prompt_tuning_init` is `TEXT`.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_tuning_init_text` (`str`, *可选*) — 用于初始化提示嵌入的文本。仅在`prompt_tuning_init`为`TEXT`时使用。'
- en: '`tokenizer_name_or_path` (`str`, *optional*) — The name or path of the tokenizer.
    Only used if `prompt_tuning_init` is `TEXT`.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_name_or_path` (`str`, *可选*) — 分词器的名称或路径。仅在`prompt_tuning_init`为`TEXT`时使用。'
- en: '`tokenizer_kwargs` (`dict`, *optional*) — The keyword arguments to pass to
    `AutoTokenizer.from_pretrained`. Only used if `prompt_tuning_init` is `TEXT`.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_kwargs` (`dict`, *可选*) — 传递给`AutoTokenizer.from_pretrained`的关键字参数。仅在`prompt_tuning_init`为`TEXT`时使用。'
- en: This is the configuration class to store the configuration of a [PromptEmbedding](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptEmbedding).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[PromptEmbedding](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptEmbedding)的配置。
- en: PromptEmbedding
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PromptEmbedding
- en: '### `class peft.PromptEmbedding`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PromptEmbedding`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/model.py#L22)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/model.py#L22)'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PromptTuningConfig](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptTuningConfig))
    — The configuration of the prompt embedding.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PromptTuningConfig](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptTuningConfig))
    — 提示嵌入的配置。'
- en: '`word_embeddings` (`torch.nn.Module`) — The word embeddings of the base transformer
    model.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_embeddings` (`torch.nn.Module`) — 基础变换器模型的词嵌入。'
- en: The model to encode virtual tokens into prompt embeddings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将虚拟标记编码为提示嵌入的模型。
- en: '**Attributes**:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**：'
- en: '`embedding` (`torch.nn.Embedding`) — The embedding layer of the prompt embedding.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding` (`torch.nn.Embedding`) — 提示嵌入的嵌入层。'
- en: 'Example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Input Shape: (`batch_size`, `total_virtual_tokens`)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输入形状：(`batch_size`, `total_virtual_tokens`)
- en: 'Output Shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状：(`batch_size`, `total_virtual_tokens`, `token_dim`)
