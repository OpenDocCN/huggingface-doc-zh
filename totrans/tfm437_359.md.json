["```py\n>>> from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\n>>> model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\n\n>>> # load image from the IIIT-5k dataset\n>>> url = \"https://i.postimg.cc/ZKwLg2Gw/367-14.png\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(pixel_values)\n\n>>> generated_text = processor.batch_decode(outputs.logits)['generated_text']\n```", "```py\n( image_size = [32, 128] patch_size = 4 num_channels = 3 max_token_length = 27 num_character_labels = 38 num_bpe_labels = 50257 num_wordpiece_labels = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 mlp_ratio = 4.0 qkv_bias = True distilled = False layer_norm_eps = 1e-05 drop_rate = 0.0 attn_drop_rate = 0.0 drop_path_rate = 0.0 output_a3_attentions = False initializer_range = 0.02 **kwargs )\n```", "```py\n>>> from transformers import MgpstrConfig, MgpstrForSceneTextRecognition\n\n>>> # Initializing a Mgpstr mgp-str-base style configuration\n>>> configuration = MgpstrConfig()\n\n>>> # Initializing a model (with random weights) from the mgp-str-base style configuration\n>>> model = MgpstrForSceneTextRecognition(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file unk_token = '[GO]' bos_token = '[GO]' eos_token = '[s]' pad_token = '[GO]' **kwargs )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( text = None images = None return_tensors = None **kwargs )\n```", "```py\n( sequences ) \u2192 export const metadata = 'undefined';Dict[str, any]\n```", "```py\n( config: MgpstrConfig )\n```", "```py\n( pixel_values: FloatTensor output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: MgpstrConfig )\n```", "```py\n( pixel_values: FloatTensor output_attentions: Optional = None output_a3_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.mgp_str.modeling_mgp_str.MgpstrModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import (\n...     MgpstrProcessor,\n...     MgpstrForSceneTextRecognition,\n... )\n>>> import requests\n>>> from PIL import Image\n\n>>> # load image from the IIIT-5k dataset\n>>> url = \"https://i.postimg.cc/ZKwLg2Gw/367-14.png\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> processor = MgpstrProcessor.from_pretrained(\"alibaba-damo/mgp-str-base\")\n>>> pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n>>> model = MgpstrForSceneTextRecognition.from_pretrained(\"alibaba-damo/mgp-str-base\")\n\n>>> # inference\n>>> outputs = model(pixel_values)\n>>> out_strs = processor.batch_decode(outputs.logits)\n>>> out_strs[\"generated_text\"]\n'[\"ticket\"]'\n```"]