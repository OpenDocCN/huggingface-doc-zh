- en: PPO Trainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/trl/ppo_trainer](https://huggingface.co/docs/trl/ppo_trainer)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: TRL supports the [PPO](https://arxiv.org/abs/1707.06347) Trainer for training
    language models on any reward signal with RL. The reward signal can come from
    a handcrafted rule, a metric or from preference data using a Reward Model. For
    a full example have a look at [`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb).
    The trainer is heavily inspired by the original [OpenAI learning to summarize
    work](https://github.com/openai/summarize-from-feedback).
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to train your SFT model (see the [SFTTrainer](sft_trainer)),
    to ensure the data we train on is in-distribution for the PPO algorithm. In addition
    we need to train a Reward model (see [RewardTrainer](reward_trainer)) which will
    be used to optimize the SFT model using the PPO algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Expected dataset format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `PPOTrainer` expects to align a generated response with a query given the
    rewards obtained from the Reward model. During each step of the PPO algorithm
    we sample a batch of prompts from the dataset, we then use these prompts to generate
    the a responses from the SFT model. Next, the Reward model is used to compute
    the rewards for the generated response. Finally, these rewards are used to optimize
    the SFT model using the PPO algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore the dataset should contain a text column which we can rename to `query`.
    Each of the other data-points required to optimize the SFT model are obtained
    during the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example with the [HuggingFaceH4/cherry_picked_prompts](https://huggingface.co/datasets/HuggingFaceH4/cherry_picked_prompts)
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Resulting in the following subset of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the PPOTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed example have a look at the [`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)
    notebook. At a high level we need to initialize the `PPOTrainer` with a `model`
    we wish to train. Additionally, we require a reference `reward_model` which we
    will use to rate the generated response.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the PPOTrainer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `PPOConfig` dataclass controls all the hyperparameters and settings for
    the PPO algorithm and trainer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can initialize our model. Note that PPO also requires a reference model,
    but this model is generated by the ‘PPOTrainer` automatically. The model can be
    initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned above, the reward can be generated using any function that returns
    a single value for a string, be it a simple rule (e.g. length of string), a metric
    (e.g. BLEU), or a reward model based on human preferences. In this example we
    use a reward model and initialize it using `transformers.pipeline` for ease of
    use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we pretokenize our dataset using the `tokenizer` to ensure we can efficiently
    generate responses during the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to initialize the `PPOTrainer` using the defined config, datasets,
    and model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Starting the training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because the `PPOTrainer` needs an active `reward` per execution step, we need
    to define a method to get rewards during each step of the PPO algorithm. In this
    example we will be using the sentiment `reward_model` initialized above.
  prefs: []
  type: TYPE_NORMAL
- en: To guide the generation process we use the `generation_kwargs` which are passed
    to the `model.generate` method for the SFT-model during each step. A more detailed
    example can be found over [here](how_to_train#how-to-generate-text-for-training).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can then loop over all examples in the dataset and generate a response for
    each query. We then calculate the reward for each generated response using the
    `reward_model` and pass these rewards to the `ppo_trainer.step` method. The `ppo_trainer.step`
    method will then optimize the SFT model using the PPO algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While training and evaluating we log the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stats`: The statistics of the PPO algorithm, including the loss, entropy,
    etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch`: The batch of data used to train the SFT model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards`: The rewards obtained from the Reward model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PPOTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.PPOTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*`*config**` (`PPOConfig`) — Configuration object for PPOTrainer. Check the
    documentation of `PPOConfig` for more — details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*model**` (`PreTrainedModelWrapper`) — Model to be optimized, Hugging Face
    transformer model with a value head. — Check the documentation of `PreTrainedModelWrapper`
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*ref_model**` (`PreTrainedModelWrapper`, *optional*) — Reference model to
    be used for KL penalty, Hugging Face — transformer model with a casual language
    modelling head. Check the documentation of `PreTrainedModelWrapper` for more details.
    If no reference model is provided, the trainer will create a reference model with
    the same architecture as the model to be optimized with shared layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*tokenizer**` (`PreTrainedTokenizerBase`) — Tokenizer to be used for encoding
    the — data. Check the documentation of `transformers.PreTrainedTokenizer` and
    `transformers.PreTrainedTokenizerFast` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*dataset**` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*)
    — PyTorch dataset or Hugging — Face dataset. This is used to create a PyTorch
    dataloader. If no dataset is provided, the dataloader must be created outside
    the trainer users needs to design their own dataloader and make sure the batch
    size that is used is the same as the one specified in the configuration object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*optimizer**` (`torch.optim.Optimizer`, *optional*) — Optimizer to be used
    for training. If no optimizer is — provided, the trainer will create an Adam optimizer
    with the learning rate specified in the configuration object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*data_collator**` (DataCollatorForLanguageModeling, *optional*) — Data collator
    to be used for training and — passed along the dataloader'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*num_shared_layers**` (int, *optional*) — Number of layers to be shared between
    the model and the reference — model, if no reference model is passed. If no number
    is provided, all the layers will be shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*lr_scheduler**` (`torch.optim.lr_scheduler`, *optional*) — Learning rate
    scheduler to be used for training. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize
    work here: [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batched_forward_pass`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L941)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`queries` (`torch.LongTensor`) — List of tensors containing the encoded queries,
    shape (`batch_size`, `query_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`responses` (`torch.LongTensor`) — List of tensors containing the encoded responses,
    shape (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_logits` (`bool`, *optional*, defaults to `False`) — Whether to return
    all_logits. Set to `False` if logits are not needed to reduce memory consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: (tuple)
  prefs: []
  type: TYPE_NORMAL
- en: 'all_logprobs (`torch.FloatTensor`): Log probabilities of the responses, shape
    (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
    shape (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate model outputs in multiple batches.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_rewards`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1078)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor`) — Scores from the reward model, shape (`batch_size`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ref_logprobs` (`torch.FloatTensor`) — Log probabilities of the reference model,
    shape (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Per token rewards, shape (`batch_size`, `response_length`) `torch.FloatTensor`:
    Non score rewards, shape (`batch_size`, `response_length`) `torch.FloatTensor`:
    KL penalty, shape (`batch_size`, `response_length`)'
  prefs: []
  type: TYPE_NORMAL
- en: Compute per token rewards from scores and KL-penalty.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_model_card`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1386)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — The path to save the model card to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name` (`str`, *optional*) — The name of the model, defaults to `TRL
    Model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and saves a model card for a TRL model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `gather_stats`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L897)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`stats` (dict[str, Any]) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a` dictionary of stats to be gathered. The stats should contain torch tensors.
    —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary of stats with the tensors gathered.
  prefs: []
  type: TYPE_NORMAL
- en: Gather stats from all processes. Useful in the context of distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L431)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`query_tensor` (`torch.LongTensor`) — A tensor of shape (`seq_len`) containing
    query tokens or a list of tensors of shape (`seq_len`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_sampler` (`Callable`, *optional*) — Callable that returns the number
    of newly generated tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional) — Batch size used for generation, defaults
    to `4`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_prompt` (`bool`, *optional*) — If set to `False` the prompt is not
    returned but only the newly generated tokens, defaults to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_ref_response` (`bool`, *optional*) — If set to `True` the reference
    response is also generated, defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_kwargs` (dict[str, Any]) — Keyword arguments for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Generate response with the model given the query tensor. call the `generate`
    method of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `log_stats`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1313)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`stats` (dict[str, Any]) — A dictionary of training stats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch` (dict[str, Any]) — A dictionary of batch data, this contains the queries
    and responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`List[torch.FloatTensor]`) — A tensor of rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that logs all the training stats. Call it at the end of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `loss`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1160)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`old_logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape
    (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values` (`torch.FloatTensor`) — Values of the value head, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`torch.FloatTensor`) — Rewards from the reward model, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor`) — Logits of the model, shape (`batch_size`,
    `response_length`, `vocab_size`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v_pred` (`torch.FloatTensor`) — Values of the value head, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate policy and value losses.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_dataloader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L376)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]) — PyTorch
    dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
    will be preprocessed by removing the columns that are not used by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_collator` (Optional[function]) — Data collator function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.utils.data.DataLoader`'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch dataloader
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the dataloader for training.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `record_step_stats`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1249)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`kl_coef` (`float`) — KL coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`dict`) — Dictionary of training step data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: stats (`dict`)
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of training step statistics
  prefs: []
  type: TYPE_NORMAL
- en: Record training step statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L617)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`queries` (List`torch.LongTensor`) — List of tensors containing the encoded
    queries of shape (`query_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`responses` (List`torch.LongTensor`) — List of tensors containing the encoded
    responses of shape (`response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (List`torch.FloatTensor`) — List of tensors containing the scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response_masks` (List`torch.FloatTensor`, *optional*)) — List of tensors containing
    masks of the response tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: A summary of the training statistics
  prefs: []
  type: TYPE_NORMAL
- en: Run a PPO optimisation step given a list of queries, model responses, and rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `train_minibatch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1032)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape [mini_batch_size,
    response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values` (`torch.FloatTensor`) — Values of the value head, shape [mini_batch_size,
    response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`torch.LongTensor`) — Encoded queries, shape [mini_batch_size, query_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response` (`torch.LongTensor`) — Encoded responses, shape [mini_batch_size,
    response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_input` (`torch.LongTensor`) — Concatenated queries and responses, shape
    [mini_batch_size, query_length+response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: train_stats (dict[str, `torch.Tensor`])
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of training statistics
  prefs: []
  type: TYPE_NORMAL
- en: Train one PPO minibatch
  prefs: []
  type: TYPE_NORMAL
- en: '### `class trl.PPOConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_config.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Configuration class for PPOTrainer
  prefs: []
  type: TYPE_NORMAL
