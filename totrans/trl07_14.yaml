- en: PPO Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO训练器
- en: 'Original text: [https://huggingface.co/docs/trl/ppo_trainer](https://huggingface.co/docs/trl/ppo_trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/ppo_trainer](https://huggingface.co/docs/trl/ppo_trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TRL supports the [PPO](https://arxiv.org/abs/1707.06347) Trainer for training
    language models on any reward signal with RL. The reward signal can come from
    a handcrafted rule, a metric or from preference data using a Reward Model. For
    a full example have a look at [`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb).
    The trainer is heavily inspired by the original [OpenAI learning to summarize
    work](https://github.com/openai/summarize-from-feedback).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TRL支持[PPO](https://arxiv.org/abs/1707.06347)训练器，用于在RL中使用任何奖励信号训练语言模型。奖励信号可以来自手工制作的规则、度量标准或使用奖励模型的偏好数据。有关完整示例，请查看[`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)。该训练器受到原始[OpenAI学习摘要工作](https://github.com/openai/summarize-from-feedback)的启发。
- en: The first step is to train your SFT model (see the [SFTTrainer](sft_trainer)),
    to ensure the data we train on is in-distribution for the PPO algorithm. In addition
    we need to train a Reward model (see [RewardTrainer](reward_trainer)) which will
    be used to optimize the SFT model using the PPO algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是训练您的SFT模型（参见[SFTTrainer](sft_trainer)），以确保我们训练的数据对PPO算法是符合分布的。此外，我们需要训练一个奖励模型（参见[RewardTrainer](reward_trainer)），该模型将用于使用PPO算法优化SFT模型。
- en: Expected dataset format
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预期的数据集格式
- en: The `PPOTrainer` expects to align a generated response with a query given the
    rewards obtained from the Reward model. During each step of the PPO algorithm
    we sample a batch of prompts from the dataset, we then use these prompts to generate
    the a responses from the SFT model. Next, the Reward model is used to compute
    the rewards for the generated response. Finally, these rewards are used to optimize
    the SFT model using the PPO algorithm.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`PPOTrainer`期望将生成的响应与查询对齐，给定从奖励模型获得的奖励。在PPO算法的每个步骤中，我们从数据集中抽取一批提示，然后使用这些提示从SFT模型生成响应。接下来，奖励模型用于计算生成响应的奖励。最后，这些奖励用于使用PPO算法优化SFT模型。'
- en: Therefore the dataset should contain a text column which we can rename to `query`.
    Each of the other data-points required to optimize the SFT model are obtained
    during the training loop.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集应包含一个文本列，我们可以将其重命名为`query`。在训练循环中获取用于优化SFT模型的每个其他数据点。
- en: 'Here is an example with the [HuggingFaceH4/cherry_picked_prompts](https://huggingface.co/datasets/HuggingFaceH4/cherry_picked_prompts)
    dataset:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例，使用[HuggingFaceH4/cherry_picked_prompts](https://huggingface.co/datasets/HuggingFaceH4/cherry_picked_prompts)数据集：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Resulting in the following subset of the dataset:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 导致数据集的以下子集：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the PPOTrainer
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PPOTrainer
- en: For a detailed example have a look at the [`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)
    notebook. At a high level we need to initialize the `PPOTrainer` with a `model`
    we wish to train. Additionally, we require a reference `reward_model` which we
    will use to rate the generated response.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细示例，请查看[`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)笔记本。在高层次上，我们需要使用我们希望训练的`model`初始化`PPOTrainer`。此外，我们需要一个参考`reward_model`，用于评价生成的响应。
- en: Initializing the PPOTrainer
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化PPOTrainer
- en: The `PPOConfig` dataclass controls all the hyperparameters and settings for
    the PPO algorithm and trainer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`PPOConfig`数据类控制PPO算法和训练器的所有超参数和设置。'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we can initialize our model. Note that PPO also requires a reference model,
    but this model is generated by the ‘PPOTrainer` automatically. The model can be
    initialized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化我们的模型。请注意，PPO还需要一个参考模型，但这个模型是由‘PPOTrainer`自动生成的。模型可以按以下方式初始化：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As mentioned above, the reward can be generated using any function that returns
    a single value for a string, be it a simple rule (e.g. length of string), a metric
    (e.g. BLEU), or a reward model based on human preferences. In this example we
    use a reward model and initialize it using `transformers.pipeline` for ease of
    use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，奖励可以使用任何返回字符串的单个值的函数生成，无论是简单规则（例如字符串长度）、度量标准（例如BLEU）还是基于人类偏好的奖励模型。在这个示例中，我们使用奖励模型，并使用`transformers.pipeline`进行初始化，以便使用。
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Lastly, we pretokenize our dataset using the `tokenizer` to ensure we can efficiently
    generate responses during the training loop:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`tokenizer`对数据集进行pretokenize，以确保我们可以在训练循环期间高效生成响应：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we are ready to initialize the `PPOTrainer` using the defined config, datasets,
    and model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用定义的配置、数据集和模型初始化`PPOTrainer`。
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Starting the training loop
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始训练循环
- en: Because the `PPOTrainer` needs an active `reward` per execution step, we need
    to define a method to get rewards during each step of the PPO algorithm. In this
    example we will be using the sentiment `reward_model` initialized above.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`PPOTrainer`需要每个执行步骤的活动`reward`，所以我们需要定义一个方法，在PPO算法的每个步骤中获取奖励。在这个示例中，我们将使用上面初始化的情感`reward_model`。
- en: To guide the generation process we use the `generation_kwargs` which are passed
    to the `model.generate` method for the SFT-model during each step. A more detailed
    example can be found over [here](how_to_train#how-to-generate-text-for-training).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导生成过程，我们使用传递给SFT模型的`model.generate`方法的`generation_kwargs`。更详细的示例可以在[这里](how_to_train#how-to-generate-text-for-training)找到。
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can then loop over all examples in the dataset and generate a response for
    each query. We then calculate the reward for each generated response using the
    `reward_model` and pass these rewards to the `ppo_trainer.step` method. The `ppo_trainer.step`
    method will then optimize the SFT model using the PPO algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以循环遍历数据集中的所有示例，并为每个查询生成一个响应。然后，我们使用`reward_model`计算每个生成的响应的奖励，并将这些奖励传递给`ppo_trainer.step`方法。`ppo_trainer.step`方法将使用PPO算法优化SFT模型。
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Logging
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'While training and evaluating we log the following metrics:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和评估过程中，我们记录以下指标：
- en: '`stats`: The statistics of the PPO algorithm, including the loss, entropy,
    etc.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats`: PPO算法的统计信息，包括损失、熵等。'
- en: '`batch`: The batch of data used to train the SFT model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch`: 用于训练SFT模型的数据批次。'
- en: '`rewards`: The rewards obtained from the Reward model.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards`: 从奖励模型获得的奖励。'
- en: PPOTrainer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPOTrainer
- en: '### `class trl.PPOTrainer`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.PPOTrainer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L109)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L109)'
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '*`*config**` (`PPOConfig`) — Configuration object for PPOTrainer. Check the
    documentation of `PPOConfig` for more — details.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*config**` (`PPOConfig`) — PPOTrainer的配置对象。查看`PPOConfig`的文档以获取更多详细信息。'
- en: '*`*model**` (`PreTrainedModelWrapper`) — Model to be optimized, Hugging Face
    transformer model with a value head. — Check the documentation of `PreTrainedModelWrapper`
    for more details.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*model**` (`PreTrainedModelWrapper`) — 要优化的模型，带有值头的Hugging Face transformer模型。
    — 查看`PreTrainedModelWrapper`的文档以获取更多详细信息。'
- en: '*`*ref_model**` (`PreTrainedModelWrapper`, *optional*) — Reference model to
    be used for KL penalty, Hugging Face — transformer model with a casual language
    modelling head. Check the documentation of `PreTrainedModelWrapper` for more details.
    If no reference model is provided, the trainer will create a reference model with
    the same architecture as the model to be optimized with shared layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*ref_model**` (`PreTrainedModelWrapper`, *可选*) — 用于KL惩罚的参考模型，带有一个休闲语言建模头的Hugging
    Face transformer模型。查看`PreTrainedModelWrapper`的文档以获取更多详细信息。如果没有提供参考模型，训练器将使用与要优化的模型相同架构的参考模型，并共享层。'
- en: '*`*tokenizer**` (`PreTrainedTokenizerBase`) — Tokenizer to be used for encoding
    the — data. Check the documentation of `transformers.PreTrainedTokenizer` and
    `transformers.PreTrainedTokenizerFast` for more details.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*tokenizer**` (`PreTrainedTokenizerBase`) — 用于编码数据的分词器。查看`transformers.PreTrainedTokenizer`和`transformers.PreTrainedTokenizerFast`的文档以获取更多详细信息。'
- en: '*`*dataset**` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*)
    — PyTorch dataset or Hugging — Face dataset. This is used to create a PyTorch
    dataloader. If no dataset is provided, the dataloader must be created outside
    the trainer users needs to design their own dataloader and make sure the batch
    size that is used is the same as the one specified in the configuration object.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*dataset**` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *可选*)
    — PyTorch数据集或Hugging Face数据集。这用于创建一个PyTorch数据加载器。如果没有提供数据集，数据加载器必须在训练器之外创建，用户需要设计自己的数据加载器，并确保使用的批量大小与配置对象中指定的批量大小相同。'
- en: '*`*optimizer**` (`torch.optim.Optimizer`, *optional*) — Optimizer to be used
    for training. If no optimizer is — provided, the trainer will create an Adam optimizer
    with the learning rate specified in the configuration object.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*optimizer**` (`torch.optim.Optimizer`, *可选*) — 用于训练的优化器。如果没有提供优化器，训练器将使用配置对象中指定的学习率创建一个Adam优化器。'
- en: '*`*data_collator**` (DataCollatorForLanguageModeling, *optional*) — Data collator
    to be used for training and — passed along the dataloader'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*data_collator**` (DataCollatorForLanguageModeling, *可选*) — 用于训练的数据收集器，并传递给数据加载器'
- en: '*`*num_shared_layers**` (int, *optional*) — Number of layers to be shared between
    the model and the reference — model, if no reference model is passed. If no number
    is provided, all the layers will be shared.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*num_shared_layers**` (int, *可选*) — 模型和参考模型之间共享的层数，如果没有传递参考模型。如果没有提供数字，所有层将被共享。'
- en: '*`*lr_scheduler**` (`torch.optim.lr_scheduler`, *optional*) — Learning rate
    scheduler to be used for training. —'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*lr_scheduler**` (`torch.optim.lr_scheduler`, *可选*) — 用于训练的学习率调度器。 —'
- en: 'The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize
    work here: [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PPOTrainer使用Proximal Policy Optimization来优化语言模型。请注意，这个训练器受到原始OpenAI学习摘要工作的启发，链接在这里：[https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)
- en: '#### `batched_forward_pass`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batched_forward_pass`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L941)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L941)'
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`queries` (`torch.LongTensor`) — List of tensors containing the encoded queries,
    shape (`batch_size`, `query_length`)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`queries` (`torch.LongTensor`) — 包含编码查询的张量列表，形状为 (`batch_size`, `query_length`)'
- en: '`responses` (`torch.LongTensor`) — List of tensors containing the encoded responses,
    shape (`batch_size`, `response_length`)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`responses` (`torch.LongTensor`) — 包含编码响应的张量列表，形状为 (`batch_size`, `response_length`)'
- en: '`return_logits` (`bool`, *optional*, defaults to `False`) — Whether to return
    all_logits. Set to `False` if logits are not needed to reduce memory consumption.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_logits` (`bool`, *可选*, 默认为 `False`) — 是否返回所有的logits。如果不需要logits以减少内存消耗，则设置为
    `False`。'
- en: Returns
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: (tuple)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (元组)
- en: 'all_logprobs (`torch.FloatTensor`): Log probabilities of the responses, shape
    (`batch_size`, `response_length`)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'all_logprobs (`torch.FloatTensor`): 响应的对数概率，形状为 (`batch_size`, `response_length`)'
- en: 'all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
    shape (`batch_size`, `response_length`)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'all_ref_logprobs (`torch.FloatTensor`): 响应的对数概率，形状为 (`batch_size`, `response_length`)'
- en: 'all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`,
    `response_length`)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'all_values (`torch.FloatTensor`): 响应的值，形状为 (`batch_size`, `response_length`)'
- en: Calculate model outputs in multiple batches.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个批次中计算模型输出。
- en: '#### `compute_rewards`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_rewards`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1078)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1078)'
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`scores` (`torch.FloatTensor`) — Scores from the reward model, shape (`batch_size`)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`) — 来自奖励模型的分数，形状为 (`batch_size`)'
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape (`batch_size`,
    `response_length`)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为 (`batch_size`, `response_length`)'
- en: '`ref_logprobs` (`torch.FloatTensor`) — Log probabilities of the reference model,
    shape (`batch_size`, `response_length`)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ref_logprobs` (`torch.FloatTensor`) — 参考模型的对数概率，形状为 (`batch_size`, `response_length`)'
- en: Returns
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Per token rewards, shape (`batch_size`, `response_length`) `torch.FloatTensor`:
    Non score rewards, shape (`batch_size`, `response_length`) `torch.FloatTensor`:
    KL penalty, shape (`batch_size`, `response_length`)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Compute per token rewards from scores and KL-penalty.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_model_card`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1386)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — The path to save the model card to.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name` (`str`, *optional*) — The name of the model, defaults to `TRL
    Model`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and saves a model card for a TRL model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '#### `gather_stats`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L897)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '`stats` (dict[str, Any]) —'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a` dictionary of stats to be gathered. The stats should contain torch tensors.
    —'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary of stats with the tensors gathered.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Gather stats from all processes. Useful in the context of distributed training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L431)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '`query_tensor` (`torch.LongTensor`) — A tensor of shape (`seq_len`) containing
    query tokens or a list of tensors of shape (`seq_len`).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_sampler` (`Callable`, *optional*) — Callable that returns the number
    of newly generated tokens.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional) — Batch size used for generation, defaults
    to `4`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_prompt` (`bool`, *optional*) — If set to `False` the prompt is not
    returned but only the newly generated tokens, defaults to `True`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_ref_response` (`bool`, *optional*) — If set to `True` the reference
    response is also generated, defaults to `False`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_kwargs` (dict[str, Any]) — Keyword arguments for generation.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.LongTensor`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Generate response with the model given the query tensor. call the `generate`
    method of the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '#### `log_stats`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1313)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '`stats` (dict[str, Any]) — A dictionary of training stats.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch` (dict[str, Any]) — A dictionary of batch data, this contains the queries
    and responses.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`List[torch.FloatTensor]`) — A tensor of rewards.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that logs all the training stats. Call it at the end of each epoch.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '#### `loss`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1160)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '`old_logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape
    (`batch_size`, `response_length`)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values` (`torch.FloatTensor`) — Values of the value head, shape (`batch_size`,
    `response_length`)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`torch.FloatTensor`) — Rewards from the reward model, shape (`batch_size`,
    `response_length`)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor`) — Logits of the model, shape (`batch_size`,
    `response_length`, `vocab_size`)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v_pred` (`torch.FloatTensor`) — Values of the value head, shape (`batch_size`,
    `response_length`)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape (`batch_size`,
    `response_length`)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate policy and value losses.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_dataloader`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L376)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]) — PyTorch
    dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
    will be preprocessed by removing the columns that are not used by the model.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_collator` (Optional[function]) — Data collator function.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.utils.data.DataLoader`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch dataloader
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the dataloader for training.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '#### `record_step_stats`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1249)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`kl_coef` (`float`) — KL coefficient'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`dict`) — Dictionary of training step data'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: stats (`dict`)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of training step statistics
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Record training step statistics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 记录训练步骤统计信息。
- en: '#### `step`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L617)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L617)'
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`queries` (List`torch.LongTensor`) — List of tensors containing the encoded
    queries of shape (`query_length`)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`queries`（List`torch.LongTensor`）— 包含形状为（query_length）的编码查询的张量列表'
- en: '`responses` (List`torch.LongTensor`) — List of tensors containing the encoded
    responses of shape (`response_length`)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`responses`（List`torch.LongTensor`）— 包含编码响应的张量列表，形状为（response_length）'
- en: '`scores` (List`torch.FloatTensor`) — List of tensors containing the scores.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（List`torch.FloatTensor`）— 包含分数的张量列表。'
- en: '`response_masks` (List`torch.FloatTensor`, *optional*)) — List of tensors containing
    masks of the response tokens.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response_masks`（List`torch.FloatTensor`，*可选*）— 包含响应标记掩码的张量列表。'
- en: Returns
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`dict[str, Any]`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`dict[str, Any]`'
- en: A summary of the training statistics
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练统计摘要
- en: Run a PPO optimisation step given a list of queries, model responses, and rewards.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 给定查询列表、模型响应和奖励，运行PPO优化步骤。
- en: '#### `train_minibatch`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `train_minibatch`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1032)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1032)'
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape [mini_batch_size,
    response_length]'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logprobs`（`torch.FloatTensor`）— 模型的对数概率，形状[mini_batch_size，response_length]'
- en: '`values` (`torch.FloatTensor`) — Values of the value head, shape [mini_batch_size,
    response_length]'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`values`（`torch.FloatTensor`）— 值头的值，形状[mini_batch_size，response_length]'
- en: '`query` (`torch.LongTensor`) — Encoded queries, shape [mini_batch_size, query_length]'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query`（`torch.LongTensor`）— 编码的查询，形状[mini_batch_size，query_length]'
- en: '`response` (`torch.LongTensor`) — Encoded responses, shape [mini_batch_size,
    response_length]'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response`（`torch.LongTensor`）— 编码的响应，形状[mini_batch_size，response_length]'
- en: '`model_input` (`torch.LongTensor`) — Concatenated queries and responses, shape
    [mini_batch_size, query_length+response_length]'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input`（`torch.LongTensor`）— 连接的查询和响应，形状[mini_batch_size，query_length+response_length]'
- en: Returns
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: train_stats (dict[str, `torch.Tensor`])
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: train_stats（字典[str，`torch.Tensor`]）
- en: Dictionary of training statistics
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 训练统计字典
- en: Train one PPO minibatch
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个PPO小批量
- en: '### `class trl.PPOConfig`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.PPOConfig`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_config.py#L34)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_config.py#L34)'
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Configuration class for PPOTrainer
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PPOTrainer的配置类
