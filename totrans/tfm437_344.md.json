["```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)\n>>> # load document image\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[1][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_rvlcdip>\"\n>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(\n...     pixel_values.to(device),\n...     decoder_input_ids=decoder_input_ids.to(device),\n...     max_length=model.decoder.config.max_position_embeddings,\n...     pad_token_id=processor.tokenizer.pad_token_id,\n...     eos_token_id=processor.tokenizer.eos_token_id,\n...     use_cache=True,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n...     return_dict_in_generate=True,\n... )\n\n>>> sequence = processor.batch_decode(outputs.sequences)[0]\n>>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n>>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n>>> print(processor.token2json(sequence))\n{'class': 'advertisement'}\n```", "```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)\n>>> # load document image\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[2][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_cord-v2>\"\n>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(\n...     pixel_values.to(device),\n...     decoder_input_ids=decoder_input_ids.to(device),\n...     max_length=model.decoder.config.max_position_embeddings,\n...     pad_token_id=processor.tokenizer.pad_token_id,\n...     eos_token_id=processor.tokenizer.eos_token_id,\n...     use_cache=True,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n...     return_dict_in_generate=True,\n... )\n\n>>> sequence = processor.batch_decode(outputs.sequences)[0]\n>>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n>>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n>>> print(processor.token2json(sequence))\n{'menu': {'nm': 'CINNAMON SUGAR', 'unitprice': '17,000', 'cnt': '1 x', 'price': '17,000'}, 'sub_total': {'subtotal_price': '17,000'}, 'total': {'total_price': '17,000', 'cashprice': '20,000', 'changeprice': '3,000'}}\n```", "```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)\n>>> # load document image from the DocVQA dataset\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[0][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n>>> question = \"When is the coffee break?\"\n>>> prompt = task_prompt.replace(\"{user_input}\", question)\n>>> decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(\n...     pixel_values.to(device),\n...     decoder_input_ids=decoder_input_ids.to(device),\n...     max_length=model.decoder.config.max_position_embeddings,\n...     pad_token_id=processor.tokenizer.pad_token_id,\n...     eos_token_id=processor.tokenizer.eos_token_id,\n...     use_cache=True,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n...     return_dict_in_generate=True,\n... )\n\n>>> sequence = processor.batch_decode(outputs.sequences)[0]\n>>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n>>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n>>> print(processor.token2json(sequence))\n{'question': 'When is the coffee break?', 'answer': '11-14 to 11:39 a.m.'}\n```", "```py\n( image_size = 224 patch_size = 4 num_channels = 3 embed_dim = 96 depths = [2, 2, 6, 2] num_heads = [3, 6, 12, 24] window_size = 7 mlp_ratio = 4.0 qkv_bias = True hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 drop_path_rate = 0.1 hidden_act = 'gelu' use_absolute_embeddings = False initializer_range = 0.02 layer_norm_eps = 1e-05 **kwargs )\n```", "```py\n>>> from transformers import DonutSwinConfig, DonutSwinModel\n\n>>> # Initializing a Donut naver-clova-ix/donut-base style configuration\n>>> configuration = DonutSwinConfig()\n\n>>> # Randomly initializing a model from the naver-clova-ix/donut-base style configuration\n>>> model = DonutSwinModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_thumbnail: bool = True do_align_long_axis: bool = False do_pad: bool = True do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )\n```", "```py\n( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_thumbnail: bool = None do_align_long_axis: bool = None do_pad: bool = None random_padding: bool = False do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Optional = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n( save_directory push_to_hub: bool = False **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config add_pooling_layer = True use_mask_token = False )\n```", "```py\n( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.donut.modeling_donut_swin.DonutSwinModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, DonutSwinModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"https://huggingface.co/naver-clova-ix/donut-base\")\n>>> model = DonutSwinModel.from_pretrained(\"https://huggingface.co/naver-clova-ix/donut-base\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 49, 768]\n```"]