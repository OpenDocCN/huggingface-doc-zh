["```py\n( bits: int = 4 group_size: int = 128 zero_point: bool = True version: AWQLinearVersion = <AWQLinearVersion.GEMM: 'gemm'> backend: AwqBackendPackingMethod = <AwqBackendPackingMethod.AUTOAWQ: 'autoawq'> do_fuse: Optional = None fuse_max_seq_len: Optional = None modules_to_fuse: Optional = None modules_to_not_convert: Optional = None **kwargs )\n```", "```py\n( )\n```", "```py\n( bits: int tokenizer: Any = None dataset: Union = None group_size: int = 128 damp_percent: float = 0.1 desc_act: bool = False sym: bool = True true_sequential: bool = True use_cuda_fp16: bool = False model_seqlen: Optional = None block_name_to_quantize: Optional = None module_name_preceding_first_block: Optional = None batch_size: int = 1 pad_token_id: Optional = None use_exllama: Optional = None max_input_length: Optional = None exllama_config: Optional = None cache_block_outputs: bool = True modules_in_block_to_quantize: Optional = None **kwargs )\n```", "```py\n( config_dict )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( load_in_8bit = False load_in_4bit = False llm_int8_threshold = 6.0 llm_int8_skip_modules = None llm_int8_enable_fp32_cpu_offload = False llm_int8_has_fp16_weight = False bnb_4bit_compute_dtype = None bnb_4bit_quant_type = 'fp4' bnb_4bit_use_double_quant = False **kwargs )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, Any]\n```"]