# 对预训练模型进行微调

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/training](https://huggingface.co/docs/transformers/v4.37.2/en/training)

使用预训练模型有很多好处。它可以减少计算成本、减少碳足迹，并且可以让您使用最先进的模型，而无需从头开始训练。🤗 Transformers 提供了数千个预训练模型，适用于各种任务。当您使用预训练模型时，您需要在特定于您任务的数据集上对其进行训练。这被称为微调，是一种非常强大的训练技术。在本教程中，您将使用您选择的深度学习框架对预训练模型进行微调：

+   使用 🤗 Transformers [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 对预训练模型进行微调。

+   使用 Keras 在 TensorFlow 中对预训练模型进行微调。

+   在原生 PyTorch 中对预训练模型进行微调。

## 准备数据集

[https://www.youtube-nocookie.com/embed/_BZearw7f0w](https://www.youtube-nocookie.com/embed/_BZearw7f0w)

在对预训练模型进行微调之前，下载一个数据集并为训练做好准备。之前的教程向您展示了如何处理训练数据，现在您有机会将这些技能付诸实践！

首先加载 [Yelp 评论](https://huggingface.co/datasets/yelp_review_full) 数据集：

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("yelp_review_full")
>>> dataset["train"][100]
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

现在您知道，您需要一个分词器来处理文本，并包含填充和截断策略以处理任何可变序列长度。为了一次处理您的数据集，使用 🤗 Datasets [`map`](https://huggingface.co/docs/datasets/process#map) 方法在整个数据集上应用预处理函数：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)

>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

如果您愿意，可以创建一个较小的数据集子集进行微调，以减少所需的时间：

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## 训练

在这一点上，您应该按照您想要使用的框架对应的部分进行操作。您可以使用右侧边栏中的链接跳转到您想要的部分 - 如果您想隐藏给定框架的所有内容，只需使用该框架块右上角的按钮！

PytorchHide Pytorch 内容

[https://www.youtube-nocookie.com/embed/nvBXf7s7vTI](https://www.youtube-nocookie.com/embed/nvBXf7s7vTI)

## 使用 PyTorch Trainer 进行训练

🤗 Transformers 提供了一个专为训练 🤗 Transformers 模型优化的 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 类，使得开始训练变得更加容易，而无需手动编写自己的训练循环。[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) API 支持各种训练选项和功能，如日志记录、梯度累积和混合精度。

首先加载您的模型并指定预期标签的数量。从 Yelp 评论 [数据集卡片](https://huggingface.co/datasets/yelp_review_full#data-fields) 中，您知道有五个标签：

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

您将看到一个警告，指出一些预训练权重未被使用，一些权重被随机初始化。不用担心，这是完全正常的！BERT 模型的预训练头被丢弃，并用随机初始化的分类头替换。您将对这个新模型头进行微调，将预训练模型的知识转移到它上面进行序列分类任务。

### 训练超参数

接下来，创建一个包含所有可以调整的超参数以及激活不同训练选项的标志的 [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments) 类。对于本教程，您可以从默认的训练 [超参数](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) 开始，但请随时尝试这些参数以找到您的最佳设置。

指定保存训练检查点的位置：

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(output_dir="test_trainer")
```

### 评估

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)在训练期间不会自动评估模型性能。您需要传递[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)一个函数来计算和报告指标。[🤗 Evaluate](https://huggingface.co/docs/evaluate/index)库提供了一个简单的[`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy)函数，您可以使用`evaluate.load`加载（有关更多信息，请参阅此[快速导览](https://huggingface.co/docs/evaluate/a_quick_tour)）：

```py
>>> import numpy as np
>>> import evaluate

>>> metric = evaluate.load("accuracy")
```

在`metric`上调用`compute`以计算您预测的准确性。在将预测传递给`compute`之前，您需要将logits转换为预测（请记住，所有🤗 Transformers模型都返回logits）：

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     predictions = np.argmax(logits, axis=-1)
...     return metric.compute(predictions=predictions, references=labels)
```

如果您想在微调期间监视评估指标，请在训练参数中指定`evaluation_strategy`参数，以在每个时期结束时报告评估指标：

```py
>>> from transformers import TrainingArguments, Trainer

>>> training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")
```

### Trainer

使用您的模型、训练参数、训练和测试数据集以及评估函数创建一个[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对象：

```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

然后通过调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)微调您的模型：

```py
>>> trainer.train()
```

TensorFlow隐藏TensorFlow内容

[https://www.youtube-nocookie.com/embed/rnTGBy2ax1c](https://www.youtube-nocookie.com/embed/rnTGBy2ax1c)

## 使用Keras训练TensorFlow模型

您还可以使用Keras API在TensorFlow中训练🤗 Transformers模型！

### 为Keras加载数据

当您想要使用Keras API训练🤗 Transformers模型时，您需要将数据集转换为Keras理解的格式。如果您的数据集很小，您可以将整个数据集转换为NumPy数组并将其传递给Keras。在我们做更复杂的事情之前，让我们先尝试这个。

首先，加载一个数据集。我们将使用来自[GLUE基准](https://huggingface.co/datasets/glue)的CoLA数据集，因为它是一个简单的二进制文本分类任务，现在只取训练拆分。

```py
from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]  # Just take the training split for now
```

接下来，加载一个分词器并将数据标记为NumPy数组。请注意，标签已经是0和1的列表，因此我们可以直接将其转换为NumPy数组而无需进行标记化！

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])  # Label is already an array of 0 and 1
```

最后，加载，[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)，和[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)模型。请注意，Transformers模型都有一个默认的与任务相关的损失函数，因此除非您想要，否则不需要指定一个：

```py
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# Load and compile our model
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
# Lower learning rates are often better for fine-tuning transformers
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tokenized_data, labels)
```

当您`compile()`模型时，您不必向模型传递损失参数！如果将此参数留空，Hugging Face模型会自动选择适合其任务和模型架构的损失。如果您想要，您始终可以通过指定自己的损失来覆盖这一点！

这种方法对于较小的数据集效果很好，但对于较大的数据集，您可能会发现它开始成为一个问题。为什么？因为标记化的数组和标签必须完全加载到内存中，而且因为NumPy不处理“不规则”数组，所以每个标记化的样本都必须填充到整个数据集中最长样本的长度。这将使您的数组变得更大，所有这些填充标记也会减慢训练速度！

### 将数据加载为tf.data.Dataset

如果您想避免减慢训练速度，可以将数据加载为`tf.data.Dataset`。虽然如果您愿意，您可以编写自己的`tf.data`流水线，但我们有两种方便的方法来做到这一点：

+   [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)：这是我们在大多数情况下推荐的方法。因为它是在您的模型上的一个方法，它可以检查模型以自动找出哪些列可用作模型输入，并丢弃其他列以使数据集更简单、更高效。

+   [to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)：此方法更低级，当您想要精确控制数据集创建方式时很有用，通过指定要包含的确切`columns`和`label_cols`。

在您可以使用[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)之前，您需要将分词器的输出添加到数据集中作为列，如下面的代码示例所示：

```py
def tokenize_dataset(data):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(data["text"])

dataset = dataset.map(tokenize_dataset)
```

请记住，Hugging Face数据集默认存储在磁盘上，因此这不会增加您的内存使用！一旦添加了列，您可以从数据集中流式传输批次并对每个批次进行填充，这将大大减少与填充整个数据集相比的填充标记数量。

```py
>>> tf_dataset = model.prepare_tf_dataset(dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer)
```

请注意，在上面的代码示例中，您需要将分词器传递给`prepare_tf_dataset`，以便它可以正确地填充批次。如果数据集中的所有样本长度相同且不需要填充，则可以跳过此参数。如果您需要执行比填充样本更复杂的操作（例如，为了进行掩码语言建模而破坏标记），则可以使用`collate_fn`参数，而不是传递一个函数，该函数将被调用以将样本列表转换为批次并应用任何您想要的预处理。查看我们的[示例](https://github.com/huggingface/transformers/tree/main/examples)或[notebooks](https://huggingface.co/docs/transformers/notebooks)以查看此方法的实际操作。

一旦创建了`tf.data.Dataset`，您可以像以前一样编译和拟合模型：

```py
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tf_dataset)
```

## 在本机PyTorch中训练

PytorchHide Pytorch content

[https://www.youtube-nocookie.com/embed/Dh9CL8fyG80](https://www.youtube-nocookie.com/embed/Dh9CL8fyG80)

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)负责训练循环，并允许您在一行代码中微调模型。对于喜欢编写自己训练循环的用户，您也可以在本机PyTorch中微调🤗 Transformers模型。

此时，您可能需要重新启动笔记本或执行以下代码以释放一些内存：

```py
del model
del trainer
torch.cuda.empty_cache()
```

接下来，手动后处理`tokenized_dataset`以准备训练。

1.  删除`text`列，因为模型不接受原始文本作为输入：

    ```py
    >>> tokenized_datasets = tokenized_datasets.remove_columns(["text"])
    ```

1.  将`label`列重命名为`labels`，因为模型期望参数命名为`labels`：

    ```py
    >>> tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
    ```

1.  将数据集的格式设置为返回PyTorch张量而不是列表：

    ```py
    >>> tokenized_datasets.set_format("torch")
    ```

然后创建数据集的较小子集，以加快微调速度：

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

### DataLoader

为您的训练和测试数据集创建一个`DataLoader`，这样您就可以迭代处理数据批次：

```py
>>> from torch.utils.data import DataLoader

>>> train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
>>> eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

加载您的模型并指定预期标签的数量：

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

### 优化器和学习率调度程序

创建一个优化器和学习率调度程序来微调模型。让我们使用PyTorch中的[`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)优化器：

```py
>>> from torch.optim import AdamW

>>> optimizer = AdamW(model.parameters(), lr=5e-5)
```

从[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)创建默认学习率调度程序：

```py
>>> from transformers import get_scheduler

>>> num_epochs = 3
>>> num_training_steps = num_epochs * len(train_dataloader)
>>> lr_scheduler = get_scheduler(
...     name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
... )
```

最后，如果您可以使用GPU，请指定`device`来使用GPU。否则，使用CPU进行训练可能需要几个小时，而不是几分钟。

```py
>>> import torch

>>> device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
>>> model.to(device)
```

如果您没有云GPU，可以通过像[Colaboratory](https://colab.research.google.com/)或[SageMaker StudioLab](https://studiolab.sagemaker.aws/)这样的托管笔记本获得免费访问。

很好，现在您已经准备好开始训练了！🥳

### 训练循环

为了跟踪您的训练进度，使用[tqdm](https://tqdm.github.io/)库在训练步骤数量上添加进度条：

```py
>>> from tqdm.auto import tqdm

>>> progress_bar = tqdm(range(num_training_steps))

>>> model.train()
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs.loss
...         loss.backward()

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

### 评估

就像您在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中添加了一个评估函数一样，当您编写自己的训练循环时，您需要做同样的事情。但是，这次您将累积所有批次并在最后计算指标，而不是在每个epoch结束时计算和报告指标。

```py
>>> import evaluate

>>> metric = evaluate.load("accuracy")
>>> model.eval()
>>> for batch in eval_dataloader:
...     batch = {k: v.to(device) for k, v in batch.items()}
...     with torch.no_grad():
...         outputs = model(**batch)

...     logits = outputs.logits
...     predictions = torch.argmax(logits, dim=-1)
...     metric.add_batch(predictions=predictions, references=batch["labels"])

>>> metric.compute()
```

## 额外资源

有关更多微调示例，请参考:

+   [🤗 Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) 包括了用于在PyTorch和TensorFlow中训练常见NLP任务的脚本。

+   [🤗 Transformers Notebooks](notebooks) 包含了关于如何在PyTorch和TensorFlow中为特定任务微调模型的各种笔记本。
