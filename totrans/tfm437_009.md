# å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/training](https://huggingface.co/docs/transformers/v4.37.2/en/training)

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ‰å¾ˆå¤šå¥½å¤„ã€‚å®ƒå¯ä»¥å‡å°‘è®¡ç®—æˆæœ¬ã€å‡å°‘ç¢³è¶³è¿¹ï¼Œå¹¶ä¸”å¯ä»¥è®©æ‚¨ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚ğŸ¤— Transformers æä¾›äº†æ•°åƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ã€‚å½“æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨éœ€è¦åœ¨ç‰¹å®šäºæ‚¨ä»»åŠ¡çš„æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚è¿™è¢«ç§°ä¸ºå¾®è°ƒï¼Œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„è®­ç»ƒæŠ€æœ¯ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨æ‚¨é€‰æ‹©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

+   ä½¿ç”¨ ğŸ¤— Transformers [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

+   ä½¿ç”¨ Keras åœ¨ TensorFlow ä¸­å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

+   åœ¨åŸç”Ÿ PyTorch ä¸­å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

## å‡†å¤‡æ•°æ®é›†

[https://www.youtube-nocookie.com/embed/_BZearw7f0w](https://www.youtube-nocookie.com/embed/_BZearw7f0w)

åœ¨å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œä¸‹è½½ä¸€ä¸ªæ•°æ®é›†å¹¶ä¸ºè®­ç»ƒåšå¥½å‡†å¤‡ã€‚ä¹‹å‰çš„æ•™ç¨‹å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•å¤„ç†è®­ç»ƒæ•°æ®ï¼Œç°åœ¨æ‚¨æœ‰æœºä¼šå°†è¿™äº›æŠ€èƒ½ä»˜è¯¸å®è·µï¼

é¦–å…ˆåŠ è½½ [Yelp è¯„è®º](https://huggingface.co/datasets/yelp_review_full) æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("yelp_review_full")
>>> dataset["train"][100]
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

ç°åœ¨æ‚¨çŸ¥é“ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªåˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬ï¼Œå¹¶åŒ…å«å¡«å……å’Œæˆªæ–­ç­–ç•¥ä»¥å¤„ç†ä»»ä½•å¯å˜åºåˆ—é•¿åº¦ã€‚ä¸ºäº†ä¸€æ¬¡å¤„ç†æ‚¨çš„æ•°æ®é›†ï¼Œä½¿ç”¨ ğŸ¤— Datasets [`map`](https://huggingface.co/docs/datasets/process#map) æ–¹æ³•åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)

>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†å­é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘æ‰€éœ€çš„æ—¶é—´ï¼š

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## è®­ç»ƒ

åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæ‚¨åº”è¯¥æŒ‰ç…§æ‚¨æƒ³è¦ä½¿ç”¨çš„æ¡†æ¶å¯¹åº”çš„éƒ¨åˆ†è¿›è¡Œæ“ä½œã€‚æ‚¨å¯ä»¥ä½¿ç”¨å³ä¾§è¾¹æ ä¸­çš„é“¾æ¥è·³è½¬åˆ°æ‚¨æƒ³è¦çš„éƒ¨åˆ† - å¦‚æœæ‚¨æƒ³éšè—ç»™å®šæ¡†æ¶çš„æ‰€æœ‰å†…å®¹ï¼Œåªéœ€ä½¿ç”¨è¯¥æ¡†æ¶å—å³ä¸Šè§’çš„æŒ‰é’®ï¼

PytorchHide Pytorch å†…å®¹

[https://www.youtube-nocookie.com/embed/nvBXf7s7vTI](https://www.youtube-nocookie.com/embed/nvBXf7s7vTI)

## ä½¿ç”¨ PyTorch Trainer è¿›è¡Œè®­ç»ƒ

ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªä¸“ä¸ºè®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹ä¼˜åŒ–çš„ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) ç±»ï¼Œä½¿å¾—å¼€å§‹è®­ç»ƒå˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè€Œæ— éœ€æ‰‹åŠ¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯ã€‚[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) API æ”¯æŒå„ç§è®­ç»ƒé€‰é¡¹å’ŒåŠŸèƒ½ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ã€‚

é¦–å…ˆåŠ è½½æ‚¨çš„æ¨¡å‹å¹¶æŒ‡å®šé¢„æœŸæ ‡ç­¾çš„æ•°é‡ã€‚ä» Yelp è¯„è®º [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/yelp_review_full#data-fields) ä¸­ï¼Œæ‚¨çŸ¥é“æœ‰äº”ä¸ªæ ‡ç­¾ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

æ‚¨å°†çœ‹åˆ°ä¸€ä¸ªè­¦å‘Šï¼ŒæŒ‡å‡ºä¸€äº›é¢„è®­ç»ƒæƒé‡æœªè¢«ä½¿ç”¨ï¼Œä¸€äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ã€‚ä¸ç”¨æ‹…å¿ƒï¼Œè¿™æ˜¯å®Œå…¨æ­£å¸¸çš„ï¼BERT æ¨¡å‹çš„é¢„è®­ç»ƒå¤´è¢«ä¸¢å¼ƒï¼Œå¹¶ç”¨éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´æ›¿æ¢ã€‚æ‚¨å°†å¯¹è¿™ä¸ªæ–°æ¨¡å‹å¤´è¿›è¡Œå¾®è°ƒï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å®ƒä¸Šé¢è¿›è¡Œåºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚

### è®­ç»ƒè¶…å‚æ•°

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ä»¥åŠæ¿€æ´»ä¸åŒè®­ç»ƒé€‰é¡¹çš„æ ‡å¿—çš„ [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments) ç±»ã€‚å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å¯ä»¥ä»é»˜è®¤çš„è®­ç»ƒ [è¶…å‚æ•°](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) å¼€å§‹ï¼Œä½†è¯·éšæ—¶å°è¯•è¿™äº›å‚æ•°ä»¥æ‰¾åˆ°æ‚¨çš„æœ€ä½³è®¾ç½®ã€‚

æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„ä½ç½®ï¼š

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(output_dir="test_trainer")
```

### è¯„ä¼°

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åœ¨è®­ç»ƒæœŸé—´ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ‚¨éœ€è¦ä¼ é€’[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ã€‚[ğŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index)åº“æä¾›äº†ä¸€ä¸ªç®€å•çš„[`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy)å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`evaluate.load`åŠ è½½ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤[å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼‰ï¼š

```py
>>> import numpy as np
>>> import evaluate

>>> metric = evaluate.load("accuracy")
```

åœ¨`metric`ä¸Šè°ƒç”¨`compute`ä»¥è®¡ç®—æ‚¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨å°†é¢„æµ‹ä¼ é€’ç»™`compute`ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†logitsè½¬æ¢ä¸ºé¢„æµ‹ï¼ˆè¯·è®°ä½ï¼Œæ‰€æœ‰ğŸ¤— Transformersæ¨¡å‹éƒ½è¿”å›logitsï¼‰ï¼š

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     predictions = np.argmax(logits, axis=-1)
...     return metric.compute(predictions=predictions, references=labels)
```

å¦‚æœæ‚¨æƒ³åœ¨å¾®è°ƒæœŸé—´ç›‘è§†è¯„ä¼°æŒ‡æ ‡ï¼Œè¯·åœ¨è®­ç»ƒå‚æ•°ä¸­æŒ‡å®š`evaluation_strategy`å‚æ•°ï¼Œä»¥åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶æŠ¥å‘Šè¯„ä¼°æŒ‡æ ‡ï¼š

```py
>>> from transformers import TrainingArguments, Trainer

>>> training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")
```

### Trainer

ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°åˆ›å»ºä¸€ä¸ª[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¯¹è±¡ï¼š

```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

ç„¶åé€šè¿‡è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)å¾®è°ƒæ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.train()
```

TensorFlowéšè—TensorFlowå†…å®¹

[https://www.youtube-nocookie.com/embed/rnTGBy2ax1c](https://www.youtube-nocookie.com/embed/rnTGBy2ax1c)

## ä½¿ç”¨Kerasè®­ç»ƒTensorFlowæ¨¡å‹

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨Keras APIåœ¨TensorFlowä¸­è®­ç»ƒğŸ¤— Transformersæ¨¡å‹ï¼

### ä¸ºKerasåŠ è½½æ•°æ®

å½“æ‚¨æƒ³è¦ä½¿ç”¨Keras APIè®­ç»ƒğŸ¤— Transformersæ¨¡å‹æ—¶ï¼Œæ‚¨éœ€è¦å°†æ•°æ®é›†è½¬æ¢ä¸ºKerasç†è§£çš„æ ¼å¼ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†å¾ˆå°ï¼Œæ‚¨å¯ä»¥å°†æ•´ä¸ªæ•°æ®é›†è½¬æ¢ä¸ºNumPyæ•°ç»„å¹¶å°†å…¶ä¼ é€’ç»™Kerasã€‚åœ¨æˆ‘ä»¬åšæ›´å¤æ‚çš„äº‹æƒ…ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå°è¯•è¿™ä¸ªã€‚

é¦–å…ˆï¼ŒåŠ è½½ä¸€ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª[GLUEåŸºå‡†](https://huggingface.co/datasets/glue)çš„CoLAæ•°æ®é›†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç®€å•çš„äºŒè¿›åˆ¶æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œç°åœ¨åªå–è®­ç»ƒæ‹†åˆ†ã€‚

```py
from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]  # Just take the training split for now
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªåˆ†è¯å™¨å¹¶å°†æ•°æ®æ ‡è®°ä¸ºNumPyæ•°ç»„ã€‚è¯·æ³¨æ„ï¼Œæ ‡ç­¾å·²ç»æ˜¯0å’Œ1çš„åˆ—è¡¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶è½¬æ¢ä¸ºNumPyæ•°ç»„è€Œæ— éœ€è¿›è¡Œæ ‡è®°åŒ–ï¼

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])  # Label is already an array of 0 and 1
```

æœ€åï¼ŒåŠ è½½ï¼Œ[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ï¼Œå’Œ[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦ï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šä¸€ä¸ªï¼š

```py
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# Load and compile our model
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
# Lower learning rates are often better for fine-tuning transformers
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tokenized_data, labels)
```

å½“æ‚¨`compile()`æ¨¡å‹æ—¶ï¼Œæ‚¨ä¸å¿…å‘æ¨¡å‹ä¼ é€’æŸå¤±å‚æ•°ï¼å¦‚æœå°†æ­¤å‚æ•°ç•™ç©ºï¼ŒHugging Faceæ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©é€‚åˆå…¶ä»»åŠ¡å’Œæ¨¡å‹æ¶æ„çš„æŸå¤±ã€‚å¦‚æœæ‚¨æƒ³è¦ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥é€šè¿‡æŒ‡å®šè‡ªå·±çš„æŸå¤±æ¥è¦†ç›–è¿™ä¸€ç‚¹ï¼

è¿™ç§æ–¹æ³•å¯¹äºè¾ƒå°çš„æ•°æ®é›†æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºè¾ƒå¤§çš„æ•°æ®é›†ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°å®ƒå¼€å§‹æˆä¸ºä¸€ä¸ªé—®é¢˜ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæ ‡è®°åŒ–çš„æ•°ç»„å’Œæ ‡ç­¾å¿…é¡»å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè€Œä¸”å› ä¸ºNumPyä¸å¤„ç†â€œä¸è§„åˆ™â€æ•°ç»„ï¼Œæ‰€ä»¥æ¯ä¸ªæ ‡è®°åŒ–çš„æ ·æœ¬éƒ½å¿…é¡»å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦ã€‚è¿™å°†ä½¿æ‚¨çš„æ•°ç»„å˜å¾—æ›´å¤§ï¼Œæ‰€æœ‰è¿™äº›å¡«å……æ ‡è®°ä¹Ÿä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼

### å°†æ•°æ®åŠ è½½ä¸ºtf.data.Dataset

å¦‚æœæ‚¨æƒ³é¿å…å‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼Œå¯ä»¥å°†æ•°æ®åŠ è½½ä¸º`tf.data.Dataset`ã€‚è™½ç„¶å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨å¯ä»¥ç¼–å†™è‡ªå·±çš„`tf.data`æµæ°´çº¿ï¼Œä½†æˆ‘ä»¬æœ‰ä¸¤ç§æ–¹ä¾¿çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼š

+   [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)ï¼šè¿™æ˜¯æˆ‘ä»¬åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ¨èçš„æ–¹æ³•ã€‚å› ä¸ºå®ƒæ˜¯åœ¨æ‚¨çš„æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œå®ƒå¯ä»¥æ£€æŸ¥æ¨¡å‹ä»¥è‡ªåŠ¨æ‰¾å‡ºå“ªäº›åˆ—å¯ç”¨ä½œæ¨¡å‹è¾“å…¥ï¼Œå¹¶ä¸¢å¼ƒå…¶ä»–åˆ—ä»¥ä½¿æ•°æ®é›†æ›´ç®€å•ã€æ›´é«˜æ•ˆã€‚

+   [to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)ï¼šæ­¤æ–¹æ³•æ›´ä½çº§ï¼Œå½“æ‚¨æƒ³è¦ç²¾ç¡®æ§åˆ¶æ•°æ®é›†åˆ›å»ºæ–¹å¼æ—¶å¾ˆæœ‰ç”¨ï¼Œé€šè¿‡æŒ‡å®šè¦åŒ…å«çš„ç¡®åˆ‡`columns`å’Œ`label_cols`ã€‚

åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†åˆ†è¯å™¨çš„è¾“å‡ºæ·»åŠ åˆ°æ•°æ®é›†ä¸­ä½œä¸ºåˆ—ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç¤ºä¾‹æ‰€ç¤ºï¼š

```py
def tokenize_dataset(data):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(data["text"])

dataset = dataset.map(tokenize_dataset)
```

è¯·è®°ä½ï¼ŒHugging Faceæ•°æ®é›†é»˜è®¤å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼Œå› æ­¤è¿™ä¸ä¼šå¢åŠ æ‚¨çš„å†…å­˜ä½¿ç”¨ï¼ä¸€æ—¦æ·»åŠ äº†åˆ—ï¼Œæ‚¨å¯ä»¥ä»æ•°æ®é›†ä¸­æµå¼ä¼ è¾“æ‰¹æ¬¡å¹¶å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå¡«å……ï¼Œè¿™å°†å¤§å¤§å‡å°‘ä¸å¡«å……æ•´ä¸ªæ•°æ®é›†ç›¸æ¯”çš„å¡«å……æ ‡è®°æ•°é‡ã€‚

```py
>>> tf_dataset = model.prepare_tf_dataset(dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer)
```

è¯·æ³¨æ„ï¼Œåœ¨ä¸Šé¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæ‚¨éœ€è¦å°†åˆ†è¯å™¨ä¼ é€’ç»™`prepare_tf_dataset`ï¼Œä»¥ä¾¿å®ƒå¯ä»¥æ­£ç¡®åœ°å¡«å……æ‰¹æ¬¡ã€‚å¦‚æœæ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬é•¿åº¦ç›¸åŒä¸”ä¸éœ€è¦å¡«å……ï¼Œåˆ™å¯ä»¥è·³è¿‡æ­¤å‚æ•°ã€‚å¦‚æœæ‚¨éœ€è¦æ‰§è¡Œæ¯”å¡«å……æ ·æœ¬æ›´å¤æ‚çš„æ“ä½œï¼ˆä¾‹å¦‚ï¼Œä¸ºäº†è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡è€Œç ´åæ ‡è®°ï¼‰ï¼Œåˆ™å¯ä»¥ä½¿ç”¨`collate_fn`å‚æ•°ï¼Œè€Œä¸æ˜¯ä¼ é€’ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†è¢«è°ƒç”¨ä»¥å°†æ ·æœ¬åˆ—è¡¨è½¬æ¢ä¸ºæ‰¹æ¬¡å¹¶åº”ç”¨ä»»ä½•æ‚¨æƒ³è¦çš„é¢„å¤„ç†ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„[ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples)æˆ–[notebooks](https://huggingface.co/docs/transformers/notebooks)ä»¥æŸ¥çœ‹æ­¤æ–¹æ³•çš„å®é™…æ“ä½œã€‚

ä¸€æ—¦åˆ›å»ºäº†`tf.data.Dataset`ï¼Œæ‚¨å¯ä»¥åƒä»¥å‰ä¸€æ ·ç¼–è¯‘å’Œæ‹Ÿåˆæ¨¡å‹ï¼š

```py
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tf_dataset)
```

## åœ¨æœ¬æœºPyTorchä¸­è®­ç»ƒ

PytorchHide Pytorch content

[https://www.youtube-nocookie.com/embed/Dh9CL8fyG80](https://www.youtube-nocookie.com/embed/Dh9CL8fyG80)

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)è´Ÿè´£è®­ç»ƒå¾ªç¯ï¼Œå¹¶å…è®¸æ‚¨åœ¨ä¸€è¡Œä»£ç ä¸­å¾®è°ƒæ¨¡å‹ã€‚å¯¹äºå–œæ¬¢ç¼–å†™è‡ªå·±è®­ç»ƒå¾ªç¯çš„ç”¨æˆ·ï¼Œæ‚¨ä¹Ÿå¯ä»¥åœ¨æœ¬æœºPyTorchä¸­å¾®è°ƒğŸ¤— Transformersæ¨¡å‹ã€‚

æ­¤æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨ç¬”è®°æœ¬æˆ–æ‰§è¡Œä»¥ä¸‹ä»£ç ä»¥é‡Šæ”¾ä¸€äº›å†…å­˜ï¼š

```py
del model
del trainer
torch.cuda.empty_cache()
```

æ¥ä¸‹æ¥ï¼Œæ‰‹åŠ¨åå¤„ç†`tokenized_dataset`ä»¥å‡†å¤‡è®­ç»ƒã€‚

1.  åˆ é™¤`text`åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸æ¥å—åŸå§‹æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼š

    ```py
    >>> tokenized_datasets = tokenized_datasets.remove_columns(["text"])
    ```

1.  å°†`label`åˆ—é‡å‘½åä¸º`labels`ï¼Œå› ä¸ºæ¨¡å‹æœŸæœ›å‚æ•°å‘½åä¸º`labels`ï¼š

    ```py
    >>> tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
    ```

1.  å°†æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸ºè¿”å›PyTorchå¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ï¼š

    ```py
    >>> tokenized_datasets.set_format("torch")
    ```

ç„¶ååˆ›å»ºæ•°æ®é›†çš„è¾ƒå°å­é›†ï¼Œä»¥åŠ å¿«å¾®è°ƒé€Ÿåº¦ï¼š

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

### DataLoader

ä¸ºæ‚¨çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†åˆ›å»ºä¸€ä¸ª`DataLoader`ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥è¿­ä»£å¤„ç†æ•°æ®æ‰¹æ¬¡ï¼š

```py
>>> from torch.utils.data import DataLoader

>>> train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
>>> eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

åŠ è½½æ‚¨çš„æ¨¡å‹å¹¶æŒ‡å®šé¢„æœŸæ ‡ç­¾çš„æ•°é‡ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

### ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ç¨‹åº

åˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ç¨‹åºæ¥å¾®è°ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨PyTorchä¸­çš„[`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)ä¼˜åŒ–å™¨ï¼š

```py
>>> from torch.optim import AdamW

>>> optimizer = AdamW(model.parameters(), lr=5e-5)
```

ä»[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åˆ›å»ºé»˜è®¤å­¦ä¹ ç‡è°ƒåº¦ç¨‹åºï¼š

```py
>>> from transformers import get_scheduler

>>> num_epochs = 3
>>> num_training_steps = num_epochs * len(train_dataloader)
>>> lr_scheduler = get_scheduler(
...     name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
... )
```

æœ€åï¼Œå¦‚æœæ‚¨å¯ä»¥ä½¿ç”¨GPUï¼Œè¯·æŒ‡å®š`device`æ¥ä½¿ç”¨GPUã€‚å¦åˆ™ï¼Œä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ï¼Œè€Œä¸æ˜¯å‡ åˆ†é’Ÿã€‚

```py
>>> import torch

>>> device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
>>> model.to(device)
```

å¦‚æœæ‚¨æ²¡æœ‰äº‘GPUï¼Œå¯ä»¥é€šè¿‡åƒ[Colaboratory](https://colab.research.google.com/)æˆ–[SageMaker StudioLab](https://studiolab.sagemaker.aws/)è¿™æ ·çš„æ‰˜ç®¡ç¬”è®°æœ¬è·å¾—å…è´¹è®¿é—®ã€‚

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒäº†ï¼ğŸ¥³

### è®­ç»ƒå¾ªç¯

ä¸ºäº†è·Ÿè¸ªæ‚¨çš„è®­ç»ƒè¿›åº¦ï¼Œä½¿ç”¨[tqdm](https://tqdm.github.io/)åº“åœ¨è®­ç»ƒæ­¥éª¤æ•°é‡ä¸Šæ·»åŠ è¿›åº¦æ¡ï¼š

```py
>>> from tqdm.auto import tqdm

>>> progress_bar = tqdm(range(num_training_steps))

>>> model.train()
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs.loss
...         loss.backward()

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

### è¯„ä¼°

å°±åƒæ‚¨åœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä¸­æ·»åŠ äº†ä¸€ä¸ªè¯„ä¼°å‡½æ•°ä¸€æ ·ï¼Œå½“æ‚¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ—¶ï¼Œæ‚¨éœ€è¦åšåŒæ ·çš„äº‹æƒ…ã€‚ä½†æ˜¯ï¼Œè¿™æ¬¡æ‚¨å°†ç´¯ç§¯æ‰€æœ‰æ‰¹æ¬¡å¹¶åœ¨æœ€åè®¡ç®—æŒ‡æ ‡ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ã€‚

```py
>>> import evaluate

>>> metric = evaluate.load("accuracy")
>>> model.eval()
>>> for batch in eval_dataloader:
...     batch = {k: v.to(device) for k, v in batch.items()}
...     with torch.no_grad():
...         outputs = model(**batch)

...     logits = outputs.logits
...     predictions = torch.argmax(logits, dim=-1)
...     metric.add_batch(predictions=predictions, references=batch["labels"])

>>> metric.compute()
```

## é¢å¤–èµ„æº

æœ‰å…³æ›´å¤šå¾®è°ƒç¤ºä¾‹ï¼Œè¯·å‚è€ƒ:

+   [ğŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) åŒ…æ‹¬äº†ç”¨äºåœ¨PyTorchå’ŒTensorFlowä¸­è®­ç»ƒå¸¸è§NLPä»»åŠ¡çš„è„šæœ¬ã€‚

+   [ğŸ¤— Transformers Notebooks](notebooks) åŒ…å«äº†å…³äºå¦‚ä½•åœ¨PyTorchå’ŒTensorFlowä¸­ä¸ºç‰¹å®šä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„å„ç§ç¬”è®°æœ¬ã€‚
