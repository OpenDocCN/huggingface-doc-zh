# å®è·µ

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit4/hands-on](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)

[![æé—®](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord) [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb)

ç°åœ¨æˆ‘ä»¬å·²ç»ç ”ç©¶äº†ReinforceèƒŒåçš„ç†è®ºï¼Œ**æ‚¨å·²ç»å‡†å¤‡å¥½ä½¿ç”¨PyTorchç¼–å†™æ‚¨çš„Reinforceä»£ç†**ã€‚å¹¶ä¸”æ‚¨å°†ä½¿ç”¨CartPole-v1å’ŒPixelCopteræµ‹è¯•å…¶ç¨³å¥æ€§ã€‚

ç„¶åï¼Œæ‚¨å°†èƒ½å¤Ÿè¿­ä»£å’Œæ”¹è¿›æ­¤å®ç°ï¼Œä»¥é€‚ç”¨äºæ›´é«˜çº§çš„ç¯å¢ƒã€‚

![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)

ä¸ºäº†éªŒè¯è¿™ä¸ªå®è·µè¿‡ç¨‹ï¼Œæ‚¨éœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hubå¹¶ï¼š

+   è·å¾—`Cartpole-v1`çš„ç»“æœ>= 350

+   è·å¾—`PixelCopter`çš„ç»“æœ>= 5ã€‚

è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·è½¬åˆ°æ’è¡Œæ¦œå¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**ã€‚**å¦‚æœæ‚¨åœ¨æ’è¡Œæ¦œä¸Šæ‰¾ä¸åˆ°æ‚¨çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°æ’è¡Œæ¦œé¡µé¢åº•éƒ¨ï¼Œç„¶åå•å‡»åˆ·æ–°æŒ‰é’®**ã€‚

**å¦‚æœæ‚¨æ‰¾ä¸åˆ°æ‚¨çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°é¡µé¢åº•éƒ¨ï¼Œç„¶åå•å‡»åˆ·æ–°æŒ‰é’®ã€‚**

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)

æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥æ‚¨çš„è¿›åº¦ğŸ‘‰ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)

**è¦å¼€å§‹å®è·µï¼Œè¯·ç‚¹å‡»â€œåœ¨Colabä¸­æ‰“å¼€â€æŒ‰é’®**ğŸ‘‡ï¼š

[![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)

æˆ‘ä»¬å¼ºçƒˆå»ºè®®å­¦ç”Ÿä»¬åœ¨è¿›è¡Œå®è·µç»ƒä¹ æ—¶ä½¿ç”¨Google Colab**ï¼Œè€Œä¸æ˜¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šè¿è¡Œå®ƒä»¬**ã€‚

é€šè¿‡ä½¿ç”¨Google Colabï¼Œ**æ‚¨å¯ä»¥ä¸“æ³¨äºå­¦ä¹ å’Œå®éªŒï¼Œè€Œä¸å¿…æ‹…å¿ƒè®¾ç½®ç¯å¢ƒçš„æŠ€æœ¯ç»†èŠ‚**ã€‚

# ç¬¬4å•å…ƒï¼šä½¿ç”¨PyTorchç¼–å†™æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šReinforceã€‚å¹¶æµ‹è¯•å…¶ç¨³å¥æ€§ğŸ’ª

![ç¼©ç•¥å›¾](../Images/207886028f30a9a8c43010256f915e88.png)

åœ¨è¿™æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæ‚¨å°†ä»å¤´å¼€å§‹ç¼–å†™æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šReinforceï¼ˆä¹Ÿç§°ä¸ºè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰ã€‚

Reinforceæ˜¯ä¸€ç§*åŸºäºç­–ç•¥çš„æ–¹æ³•*ï¼šä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯•å›¾**ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œè€Œä¸ä½¿ç”¨åŠ¨ä½œå€¼å‡½æ•°**ã€‚

æ›´å‡†ç¡®åœ°è¯´ï¼ŒReinforceæ˜¯ä¸€ç§*ç­–ç•¥æ¢¯åº¦æ–¹æ³•*ï¼Œæ˜¯*åŸºäºç­–ç•¥çš„æ–¹æ³•*çš„ä¸€ä¸ªå­ç±»ï¼Œæ—¨åœ¨**é€šè¿‡ä¼°è®¡æœ€ä¼˜ç­–ç•¥çš„æƒé‡æ¥ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿ç”¨æ¢¯åº¦ä¸Šå‡**ã€‚

ä¸ºäº†æµ‹è¯•å…¶ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å°†åœ¨2ä¸ªä¸åŒçš„ç®€å•ç¯å¢ƒä¸­å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼š

+   Cartpole-v1

+   PixelcopterEnv

â¬‡ï¸ è¿™æ˜¯**æ‚¨å°†åœ¨æœ¬ç¬”è®°æœ¬æœ«å°¾å®ç°çš„ç¤ºä¾‹ã€‚** â¬‡ï¸

![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)

### ğŸ® ç¯å¢ƒï¼š

+   [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)

+   [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)

### ğŸ“š RL-Library:

+   Python

+   PyTorch

æˆ‘ä»¬ä¸æ–­åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœæ‚¨åœ¨æœ¬ç¬”è®°æœ¬ä¸­å‘ç°ä¸€äº›é—®é¢˜**ï¼Œè¯·[åœ¨GitHub Repoä¸Šæå‡ºé—®é¢˜](https://github.com/huggingface/deep-rl-class/issues)ã€‚

## æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ğŸ†

åœ¨ç¬”è®°æœ¬çš„æœ«å°¾ï¼Œæ‚¨å°†ï¼š

+   èƒ½å¤Ÿ**ä½¿ç”¨PyTorchä»å¤´å¼€å§‹ç¼–å†™ä¸€ä¸ªReinforceç®—æ³•**ã€‚

+   èƒ½å¤Ÿ**ä½¿ç”¨ç®€å•ç¯å¢ƒæµ‹è¯•æ‚¨çš„ä»£ç†çš„ç¨³å¥æ€§**ã€‚

+   èƒ½å¤Ÿ**å°†è®­ç»ƒå¥½çš„ä»£ç†æ¨é€åˆ°Hub**ï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•°ğŸ”¥ã€‚

## å…ˆå†³æ¡ä»¶ğŸ—ï¸

åœ¨æ·±å…¥ç ”ç©¶ç¬”è®°æœ¬ä¹‹å‰ï¼Œæ‚¨éœ€è¦ï¼š

ğŸ”² ğŸ“š [é€šè¿‡é˜…è¯»ç¬¬4å•å…ƒæ¥å­¦ä¹ ç­–ç•¥æ¢¯åº¦](https://huggingface.co/deep-rl-course/unit4/introduction)

# è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ç¼–å†™Reinforceç®—æ³• ğŸ”¥

## ä¸€äº›å»ºè®® ğŸ’¡

æœ€å¥½åœ¨æ‚¨çš„è°·æ­Œé©±åŠ¨å™¨ä¸Šè¿è¡Œæ­¤colabï¼Œè¿™æ ·**å¦‚æœè¶…æ—¶**ï¼Œæ‚¨ä»ç„¶å¯ä»¥åœ¨æ‚¨çš„è°·æ­Œé©±åŠ¨å™¨ä¸Šä¿å­˜ç¬”è®°æœ¬ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»å¤´å¼€å§‹å¡«å†™æ‰€æœ‰å†…å®¹ã€‚

ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `Ctrl + S` æˆ– `æ–‡ä»¶ > åœ¨è°·æ­Œé©±åŠ¨å™¨ä¸­ä¿å­˜å‰¯æœ¬ã€‚`

## è®¾ç½®GPU ğŸ’ª

+   ä¸ºäº†**åŠ é€Ÿä»£ç†çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨GPU**ã€‚ä¸ºæ­¤ï¼Œè¯·è½¬åˆ° `è¿è¡Œæ—¶ > æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`

![GPU æ­¥éª¤ 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)

+   `ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU`

![GPU æ­¥éª¤ 2](../Images/e0fec252447f98378386ccca8e57a80a.png)

## åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ˜¾ç¤º ğŸ–¥

åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªé‡æ’­è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œåœ¨colabä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚

ä»¥ä¸‹å•å…ƒæ ¼å°†å®‰è£…åº“å¹¶åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªè™šæ‹Ÿå±å¹• ğŸ–¥

```py
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip install pyvirtualdisplay
!pip install pyglet==1.5.1
```

```py
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## å®‰è£…ä¾èµ– ğŸ”½

ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–é¡¹ã€‚æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªï¼š

+   `gym`

+   `gym-games`ï¼šä½¿ç”¨PyGameåˆ¶ä½œçš„é¢å¤–gymç¯å¢ƒã€‚

+   `huggingface_hub`ï¼šHubä½œä¸ºä¸€ä¸ªä¸­å¿ƒåœ°æ–¹ï¼Œä»»ä½•äººéƒ½å¯ä»¥åˆ†äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒå…·æœ‰ç‰ˆæœ¬æ§åˆ¶ã€æŒ‡æ ‡ã€å¯è§†åŒ–å’Œå…¶ä»–åŠŸèƒ½ï¼Œå¯ä»¥è®©æ‚¨è½»æ¾ä¸ä»–äººåˆä½œã€‚

æ‚¨å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬å®‰è£…gymè€Œä¸æ˜¯gymnasiumï¼Œä¸€ä¸ªæ›´æ–°ç‰ˆæœ¬çš„gymï¼Ÿ**å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„gym-gamesè¿˜æ²¡æœ‰ä¸gymnasiumæ›´æ–°ã€‚**

åœ¨è¿™é‡Œä½ ä¼šé‡åˆ°çš„åŒºåˆ«ï¼š

+   åœ¨`gym`ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰`terminated`å’Œ`truncated`ï¼Œåªæœ‰`done`ã€‚

+   åœ¨`gym`ä¸­ï¼Œä½¿ç”¨`env.step()`è¿”å›`state, reward, done, info`

æ‚¨å¯ä»¥åœ¨è¿™é‡Œäº†è§£Gymå’ŒGymnasiumä¹‹é—´çš„åŒºåˆ« ğŸ‘‰ [https://gymnasium.farama.org/content/migration-guide/](https://gymnasium.farama.org/content/migration-guide/)

æ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ‰€æœ‰å¯ç”¨çš„Reinforceæ¨¡å‹ ğŸ‘‰ [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)

æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æ‰€æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ ğŸ‘‰ [https://huggingface.co/models?pipeline_tag=reinforcement-learning](https://huggingface.co/models?pipeline_tag=reinforcement-learning)

```py
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt
```

## å¯¼å…¥åŒ… ğŸ“¦

é™¤äº†å¯¼å…¥å·²å®‰è£…çš„åº“ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¼å…¥ï¼š

+   `imageio`ï¼šä¸€ä¸ªå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç”Ÿæˆé‡æ’­è§†é¢‘çš„åº“

```py
import numpy as np

from collections import deque

import matplotlib.pyplot as plt
%matplotlib inline

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Gym
import gym
import gym_pygame

# Hugging Face Hub
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
import imageio
```

## æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰GPU

+   è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰GPU

+   å¦‚æœæ˜¯çš„è¯ï¼Œä½ åº”è¯¥çœ‹åˆ° `device:cuda0`

```py
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

```py
print(device)
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å®ç°æˆ‘ä»¬çš„Reinforceç®—æ³• ğŸ”¥

# ç¬¬ä¸€ä¸ªä»£ç†ï¼šç©CartPole-v1 ğŸ¤–

## åˆ›å»ºCartPoleç¯å¢ƒå¹¶äº†è§£å…¶å·¥ä½œåŸç†

### ç¯å¢ƒ ğŸ®

### ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ä½¿ç”¨åƒCartPole-v1è¿™æ ·ç®€å•çš„ç¯å¢ƒï¼Ÿ

å¦‚[å¼ºåŒ–å­¦ä¹ æŠ€å·§å’ŒæŠ€å·§](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)ä¸­æ‰€è§£é‡Šçš„ï¼Œå½“æ‚¨ä»å¤´å¼€å§‹å®ç°ä»£ç†æ—¶ï¼Œæ‚¨éœ€è¦**ç¡®ä¿å®ƒèƒ½å¤Ÿæ­£ç¡®è¿è¡Œï¼Œå¹¶åœ¨æ›´ç®€å•çš„ç¯å¢ƒä¸­æ‰¾åˆ°é”™è¯¯**ï¼Œå› ä¸ºåœ¨ç®€å•ç¯å¢ƒä¸­æ‰¾åˆ°é”™è¯¯ä¼šæ›´å®¹æ˜“ã€‚

> å°è¯•åœ¨ç©å…·é—®é¢˜ä¸Šæœ‰ä¸€äº›â€œç”Ÿå‘½è¿¹è±¡â€
> 
> é€šè¿‡åœ¨è¶Šæ¥è¶Šå›°éš¾çš„ç¯å¢ƒä¸­è¿è¡Œæ¥éªŒè¯å®ç°ï¼ˆæ‚¨å¯ä»¥å°†ç»“æœä¸RLåŠ¨ç‰©å›­è¿›è¡Œæ¯”è¾ƒï¼‰ã€‚é€šå¸¸éœ€è¦è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–æ¥è¿›è¡Œè¿™ä¸€æ­¥ã€‚

### CartPole-v1ç¯å¢ƒ

> ä¸€ä¸ªæ†é€šè¿‡ä¸€ä¸ªæœªæ¿€æ´»çš„å…³èŠ‚è¿æ¥åˆ°ä¸€ä¸ªå°è½¦ä¸Šï¼Œè¯¥å°è½¦æ²¿ç€ä¸€ä¸ªæ— æ‘©æ“¦çš„è½¨é“ç§»åŠ¨ã€‚æ‘†æ”¾åœ¨å°è½¦ä¸Šæ–¹ï¼Œå¹¶ä¸”ç›®æ ‡æ˜¯é€šè¿‡åœ¨å°è½¦å·¦å³æ–¹å‘ä¸Šæ–½åŠ åŠ›æ¥å¹³è¡¡æ†ã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬ä»CartPole-v1å¼€å§‹ã€‚ç›®æ ‡æ˜¯å°†å°è½¦å‘å·¦æˆ–å‘å³æ¨åŠ¨ï¼Œ**ä½¿æ†ä¿æŒåœ¨å¹³è¡¡çŠ¶æ€ã€‚**

å¦‚æœï¼š

+   æ†è§’å¤§äºÂ±12Â°

+   å°è½¦ä½ç½®å¤§äºÂ±2.4

+   è¯¥é›†æ•°é•¿åº¦å¤§äº500

æˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥è·å¾— +1 çš„å¥–åŠ± ğŸ’°ï¼Œæ†ä¿æŒåœ¨å¹³è¡¡çŠ¶æ€ã€‚

```py
env_id = "CartPole-v1"
# Create the env
env = gym.make(env_id)

# Create the evaluation env
eval_env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```py
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```py
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

## è®©æˆ‘ä»¬æ„å»ºReinforceæ¶æ„

è¿™ä¸ªå®ç°åŸºäºä¸‰ä¸ªå®ç°ï¼š

+   [PyTorchå®˜æ–¹å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)

+   [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)

+   [ç”±Chris1nexusæ”¹è¿›çš„é›†æˆ](https://github.com/huggingface/deep-rl-class/pull/95)

![Reinforce](../Images/36cf0376b1e1c1f32df0bf4cc6199001.png)

æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›ï¼š

+   ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆfc1å’Œfc2ï¼‰ã€‚

+   ä½¿ç”¨ReLUä½œä¸ºfc1çš„æ¿€æ´»å‡½æ•°

+   ä½¿ç”¨Softmaxè¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ

```py
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Create two fully connected layers

    def forward(self, x):
        # Define the forward pass
        # state goes to fc1 then we apply ReLU activation function

        # fc1 outputs goes to fc2

        # We output the softmax

    def act(self, state):
        """
        Given a state, take action
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

### è§£å†³æ–¹æ¡ˆ

```py
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œä½ èƒ½çŒœåˆ°åœ¨å“ªé‡Œå—ï¼Ÿ

+   ä¸ºäº†æ‰¾å‡ºæ¥ï¼Œè®©æˆ‘ä»¬è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼š

```py
debug_policy = Policy(s_size, a_size, 64).to(device)
debug_policy.act(env.reset())
```

+   åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°é”™è¯¯è¯´`ValueError: The value argument to log_prob must be a Tensor`

+   è¿™æ„å‘³ç€`m.log_prob(action)`ä¸­çš„`action`å¿…é¡»æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œä½†å®ƒä¸æ˜¯ã€‚

+   ä½ çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿæ£€æŸ¥actå‡½æ•°ï¼Œçœ‹çœ‹ä¸ºä»€ä¹ˆå®ƒä¸èµ·ä½œç”¨ã€‚

å»ºè®®ğŸ’¡ï¼šè¿™ä¸ªå®ç°ä¸­æœ‰é—®é¢˜ã€‚è®°ä½å¯¹äºactå‡½æ•°ï¼Œæˆ‘ä»¬æƒ³è¦ä»åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œã€‚

### ï¼ˆçœŸæ­£çš„ï¼‰è§£å†³æ–¹æ¡ˆ

```py
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

é€šè¿‡ä½¿ç”¨CartPoleï¼Œæ›´å®¹æ˜“è°ƒè¯•ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“é”™è¯¯æ¥è‡ªäºæˆ‘ä»¬çš„é›†æˆï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ç®€å•çš„ç¯å¢ƒã€‚

+   å› ä¸ºæˆ‘ä»¬æƒ³è¦ä»åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨`action = np.argmax(m)`ï¼Œå› ä¸ºå®ƒæ€»æ˜¯è¾“å‡ºå…·æœ‰æœ€é«˜æ¦‚ç‡çš„åŠ¨ä½œã€‚

+   æˆ‘ä»¬éœ€è¦ç”¨`action = m.sample()`æ›¿æ¢è¿™ä¸ªï¼Œå®ƒå°†ä»æ¦‚ç‡åˆ†å¸ƒP(.|s)ä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ

### è®©æˆ‘ä»¬æ„å»ºReinforceè®­ç»ƒç®—æ³•

è¿™æ˜¯Reinforceç®—æ³•çš„ä¼ªä»£ç ï¼š

![ç­–ç•¥æ¢¯åº¦ä¼ªä»£ç ](../Images/98fc23971db3e1a9e35baeee827641dc.png)

+   å½“æˆ‘ä»¬è®¡ç®—å›æŠ¥Gtï¼ˆç¬¬6è¡Œï¼‰æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬è®¡ç®—ä»æ—¶é—´æ­¥tå¼€å§‹çš„æŠ˜æ‰£å¥–åŠ±çš„æ€»å’Œã€‚

+   ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæˆ‘ä»¬çš„ç­–ç•¥åº”è¯¥åªæ ¹æ®åæœæ¥å¼ºåŒ–åŠ¨ä½œï¼šå› æ­¤åœ¨é‡‡å–è¡ŒåŠ¨ä¹‹å‰è·å¾—çš„å¥–åŠ±æ˜¯æ— ç”¨çš„ï¼ˆå› ä¸ºå®ƒä»¬ä¸æ˜¯å› ä¸ºè¡ŒåŠ¨ï¼‰ï¼Œåªæœ‰ä¹‹åçš„å¥–åŠ±æ‰é‡è¦ã€‚

+   åœ¨ç¼–å†™è¿™ä¹‹å‰ï¼Œä½ åº”è¯¥é˜…è¯»è¿™ä¸€éƒ¨åˆ†[ä¸è¦è®©è¿‡å»åˆ†æ•£ä½ çš„æ³¨æ„åŠ›](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±é€æ­¥ç­–ç•¥æ¢¯åº¦ã€‚

æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç”±[Chris1nexus](https://github.com/Chris1nexus)ç¼–å†™çš„æœ‰è¶£çš„æŠ€æœ¯æ¥é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥ã€‚è¯„è®ºä¸­è§£é‡Šäº†è¯¥è¿‡ç¨‹ã€‚ä¹Ÿä¸è¦çŠ¹è±«[æŸ¥çœ‹PRè§£é‡Š](https://github.com/huggingface/deep-rl-class/pull/95)ï¼Œä½†æ€»ä½“æ€è·¯æ˜¯é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥ã€‚

ç¬¬äºŒä¸ªé—®é¢˜ä½ å¯èƒ½ä¼šé—®çš„æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±ï¼Ÿä¹‹å‰ä¸æ˜¯è®¨è®ºè¿‡æ¢¯åº¦ä¸Šå‡ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™å—ï¼Ÿ

+   æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–æˆ‘ä»¬çš„æ•ˆç”¨å‡½æ•°$J(\theta)$ï¼Œä½†åœ¨PyTorchå’ŒTensorFlowä¸­ï¼Œæ›´å¥½çš„åšæ³•æ˜¯æœ€å°åŒ–ä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚

    +   å‡è®¾æˆ‘ä»¬æƒ³è¦åœ¨æŸä¸ªæ—¶é—´æ­¥å¼ºåŒ–åŠ¨ä½œ3ã€‚åœ¨è®­ç»ƒè¿™ä¸ªåŠ¨ä½œä¹‹å‰ï¼ŒPä¸º0.25ã€‚

    +   æ‰€ä»¥æˆ‘ä»¬æƒ³è¦ä¿®æ”¹Î¸ thetaï¼Œä½¿å¾—Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)>0.25

    +   å› ä¸ºæ‰€æœ‰På¿…é¡»æ€»å’Œä¸º1ï¼Œmax<math><semantics><mrow><mi>p</mi><msub><mi>i</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">pi_\theta(a_3|s; \theta)</annotation></semantics></math>piÎ¸â€‹(a3â€‹âˆ£s;Î¸)å°†**æœ€å°åŒ–å…¶ä»–åŠ¨ä½œçš„æ¦‚ç‡ã€‚**

    +   æ‰€ä»¥æˆ‘ä»¬åº”è¯¥å‘Šè¯‰PyTorch**æœ€å°åŒ–<math><semantics><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1 - \pi_\theta(a_3|s; \theta)</annotation></semantics></math>1âˆ’Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)ã€‚**

    +   è¿™ä¸ªæŸå¤±å‡½æ•°åœ¨<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s; \theta)</annotation></semantics></math>Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)æ¥è¿‘1æ—¶è¶‹è¿‘äº0ã€‚

    +   å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±æ¢¯åº¦æœ€å¤§åŒ–<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s; \theta)</annotation></semantics></math>Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)

```py
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes+1):
        saved_log_probs = []
        rewards = []
        state = # TODO: reset the environment
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = # TODO get the action
            saved_log_probs.append(log_prob)
            state, reward, done, _ = # TODO: take an env step
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t

        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...

        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = (returns[0] if len(returns)>0 else 0)
            returns.appendleft(    ) # TODO: complete here

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()

        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))

    return scores
```

#### è§£å†³æ–¹æ¡ˆ

```py
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes + 1):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as
        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t
        #
        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...

        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = returns[0] if len(returns) > 0 else 0
            returns.appendleft(gamma * disc_return_t + rewards[t])

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()
        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print("Episode {}\tAverage Score: {:.2f}".format(i_episode, np.mean(scores_deque)))

    return scores
```

## è®­ç»ƒå®ƒ

+   æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ã€‚

+   ä½†é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°çš„å˜é‡ã€‚

+   æ‚¨å¯ä»¥æ›´æ”¹è®­ç»ƒå‚æ•°ï¼ˆå¹¶åº”è¯¥ğŸ˜‰ï¼‰

```py
cartpole_hyperparameters = {
    "h_size": 16,
    "n_training_episodes": 1000,
    "n_evaluation_episodes": 10,
    "max_t": 1000,
    "gamma": 1.0,
    "lr": 1e-2,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

```py
# Create policy and place it to the device
cartpole_policy = Policy(
    cartpole_hyperparameters["state_space"],
    cartpole_hyperparameters["action_space"],
    cartpole_hyperparameters["h_size"],
).to(device)
cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters["lr"])
```

```py
scores = reinforce(
    cartpole_policy,
    cartpole_optimizer,
    cartpole_hyperparameters["n_training_episodes"],
    cartpole_hyperparameters["max_t"],
    cartpole_hyperparameters["gamma"],
    100,
)
```

## å®šä¹‰è¯„ä¼°æ–¹æ³• ğŸ“

+   åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†è¦ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•æ¥æµ‹è¯•æˆ‘ä»¬çš„Reinforceä»£ç†ã€‚

```py
def evaluate_agent(env, max_steps, n_eval_episodes, policy):
    """
    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
    :param env: The evaluation environment
    :param n_eval_episodes: Number of episode to evaluate the agent
    :param policy: The Reinforce agent
    """
    episode_rewards = []
    for episode in range(n_eval_episodes):
        state = env.reset()
        step = 0
        done = False
        total_rewards_ep = 0

        for step in range(max_steps):
            action, _ = policy.act(state)
            new_state, reward, done, info = env.step(action)
            total_rewards_ep += reward

            if done:
                break
            state = new_state
        episode_rewards.append(total_rewards_ep)
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return mean_reward, std_reward
```

## è¯„ä¼°æˆ‘ä»¬çš„ä»£ç† ğŸ“ˆ

```py
evaluate_agent(
    eval_env, cartpole_hyperparameters["max_t"], cartpole_hyperparameters["n_evaluation_episodes"], cartpole_policy
)
```

### åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹ğŸ”¥

ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å‘å¸ƒåˆ°hub ğŸ¤—ã€‚

è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å¡çš„ç¤ºä¾‹ï¼š

![](../Images/afdb3be9b09aa528c9ee40968ca15774.png)

### æ¨é€åˆ°Hub

#### ä¸è¦ä¿®æ”¹è¿™æ®µä»£ç 

```py
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json
import imageio

import tempfile

import os
```

```py
def record_video(env, policy, out_directory, fps=30):
    """
    Generate a replay video of the agent
    :param env
    :param Qtable: Qtable of our agent
    :param out_directory
    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)
    """
    images = []
    done = False
    state = env.reset()
    img = env.render(mode="rgb_array")
    images.append(img)
    while not done:
        # Take the action (index) that have the maximum expected future reward given that state
        action, _ = policy.act(state)
        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic
        img = env.render(mode="rgb_array")
        images.append(img)
    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```py
def push_to_hub(repo_id,
                model,
                hyperparameters,
                eval_env,
                video_fps=30 ):
  """
  Evaluate, Generate a video and Upload a model to Hugging Face Hub.
  This method does the complete pipeline:
  - It evaluates the model
  - It generates the model card
  - It generates a replay video of the agent
  - It pushes everything to the Hub

  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub
  :param model: the pytorch model we want to save
  :param hyperparameters: training hyperparameters
  :param eval_env: evaluation environment
  :param video_fps: how many frame per seconds to record our video replay
  """

  _, repo_name = repo_id.split("/")
  api = HfApi()

  # Step 1: Create the repo
  repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
  )

  with tempfile.TemporaryDirectory() as tmpdirname:
    local_directory = Path(tmpdirname)

    # Step 2: Save the model
    torch.save(model, local_directory / "model.pt")

    # Step 3: Save the hyperparameters to JSON
    with open(local_directory / "hyperparameters.json", "w") as outfile:
      json.dump(hyperparameters, outfile)

    # Step 4: Evaluate the model and build JSON
    mean_reward, std_reward = evaluate_agent(eval_env,
                                            hyperparameters["max_t"],
                                            hyperparameters["n_evaluation_episodes"],
                                            model)
    # Get datetime
    eval_datetime = datetime.datetime.now()
    eval_form_datetime = eval_datetime.isoformat()

    evaluate_data = {
          "env_id": hyperparameters["env_id"],
          "mean_reward": mean_reward,
          "n_evaluation_episodes": hyperparameters["n_evaluation_episodes"],
          "eval_datetime": eval_form_datetime,
    }

    # Write a JSON file
    with open(local_directory / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # Step 5: Create the model card
    env_name = hyperparameters["env_id"]

    metadata = {}
    metadata["tags"] = [
          env_name,
          "reinforce",
          "reinforcement-learning",
          "custom-implementation",
          "deep-rl-class"
      ]

    # Add metrics
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
      )

    # Merges both dictionaries
    metadata = {**metadata, **eval}

    model_card = f"""
  # **Reinforce** Agent playing **{env_id}**
  This is a trained model of a **Reinforce** agent playing **{env_id}** .
  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction
  """

    readme_path = local_directory / "README.md"
    readme = ""
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
          readme = f.read()
    else:
      readme = model_card

    with readme_path.open("w", encoding="utf-8") as f:
      f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

    # Step 6: Record a video
    video_path =  local_directory / "replay.mp4"
    record_video(env, model, video_path, video_fps)

    # Step 7\. Push everything to the Hub
    api.upload_folder(
          repo_id=repo_id,
          folder_path=local_directory,
          path_in_repo=".",
    )

    print(f"Your model is pushed to the Hub. You can view your model here: {repo_url}")
```

é€šè¿‡ä½¿ç”¨`push_to_hub`ï¼Œ**æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•é‡æ’­ã€ç”Ÿæˆä»£ç†çš„æ¨¡å‹å¡ï¼Œå¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚

è¿™æ ·ï¼š

+   æ‚¨å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ**ğŸ”¥

+   æ‚¨å¯ä»¥**å¯è§†åŒ–æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ**ğŸ‘€

+   æ‚¨å¯ä»¥**ä¸ç¤¾åŒºåˆ†äº«ä¸€ä¸ªå…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†**ğŸ’¾

+   æ‚¨å¯ä»¥**è®¿é—®æ’è¡Œæ¦œğŸ†ï¼ŒæŸ¥çœ‹æ‚¨çš„ä»£ç†ä¸åŒå­¦ç›¸æ¯”è¡¨ç°å¦‚ä½•**ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)

è¦ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œéœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ªHFå¸æˆ·â¡ [https://huggingface.co/join](https://huggingface.co/join)

2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦å­˜å‚¨æ¥è‡ªHugging Faceç½‘ç«™çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚

+   åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œï¼ˆ[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)ï¼‰**å…·æœ‰å†™å…¥è§’è‰²**

![åˆ›å»ºHFä»¤ç‰Œ](../Images/d21a97c736edaab9119d2d1c1da9deac.png)

```py
notebook_login()
```

å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`ï¼ˆæˆ–`login`ï¼‰

3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡å°†æˆ‘ä»¬è®­ç»ƒçš„ä»£ç†æ¨é€åˆ°ğŸ¤— Hub ğŸ”¥ï¼Œä½¿ç”¨`package_to_hub()`å‡½æ•°

```py
repo_id = ""  # TODO Define your repo id {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    cartpole_policy,  # The model we want to save
    cartpole_hyperparameters,  # Hyperparameters
    eval_env,  # Evaluation environment
    video_fps=30
)
```

ç°åœ¨æˆ‘ä»¬æµ‹è¯•äº†æˆ‘ä»¬å®ç°çš„é²æ£’æ€§ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ›´å¤æ‚çš„ç¯å¢ƒï¼šPixelCopter ğŸš

## ç¬¬äºŒä¸ªä»£ç†ï¼šPixelCopter ğŸš

### ç ”ç©¶PixelCopterç¯å¢ƒğŸ‘€

+   [ç¯å¢ƒæ–‡æ¡£](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)

```py
env_id = "Pixelcopter-PLE-v0"
env = gym.make(env_id)
eval_env = gym.make(env_id)
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```py
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```py
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

è§‚å¯Ÿç©ºé—´ï¼ˆ7ï¼‰ğŸ‘€ï¼š

+   ç©å®¶yä½ç½®

+   ç©å®¶é€Ÿåº¦

+   ç©å®¶åˆ°åœ°æ¿çš„è·ç¦»

+   ç©å®¶åˆ°å¤©èŠ±æ¿çš„è·ç¦»

+   ä¸‹ä¸€ä¸ªæ–¹å—xè·ç¦»ç©å®¶

+   ä¸‹ä¸€ä¸ªæ–¹å—çš„é¡¶éƒ¨yä½ç½®

+   ä¸‹ä¸€ä¸ªæ–¹å—çš„åº•éƒ¨yä½ç½®

åŠ¨ä½œç©ºé—´ï¼ˆ2ï¼‰ï¼š

+   åŠ é€Ÿï¼ˆæŒ‰ä¸‹åŠ é€Ÿå™¨ï¼‰

+   ä»€ä¹ˆéƒ½ä¸åšï¼ˆä¸æŒ‰ä¸‹åŠ é€Ÿå™¨ï¼‰

å¥–åŠ±å‡½æ•°ï¼š

+   æ¯é€šè¿‡ä¸€ä¸ªå‚ç›´å—ï¼Œå®ƒè·å¾—+1çš„æ­£å¥–åŠ±ã€‚æ¯æ¬¡è¾¾åˆ°ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œå®ƒä¼šè·å¾—-1çš„è´Ÿå¥–åŠ±ã€‚

### å®šä¹‰æ–°ç­–ç•¥

+   ç”±äºç¯å¢ƒæ›´åŠ å¤æ‚ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´æ·±å±‚çš„ç¥ç»ç½‘ç»œ

```py
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Define the three layers here

    def forward(self, x):
        # Define the forward process here
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

#### è§£å†³æ–¹æ¡ˆ

```py
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, h_size * 2)
        self.fc3 = nn.Linear(h_size * 2, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

### å®šä¹‰è¶…å‚æ•°

+   å› ä¸ºè¿™ä¸ªç¯å¢ƒæ›´åŠ å¤æ‚ã€‚

+   ç‰¹åˆ«æ˜¯å¯¹äºéšè—å¤§å°ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ç¥ç»å…ƒã€‚

```py
pixelcopter_hyperparameters = {
    "h_size": 64,
    "n_training_episodes": 50000,
    "n_evaluation_episodes": 10,
    "max_t": 10000,
    "gamma": 0.99,
    "lr": 1e-4,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

### è®­ç»ƒå®ƒ

+   æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ã€‚

```py
# Create policy and place it to the device
# torch.manual_seed(50)
pixelcopter_policy = Policy(
    pixelcopter_hyperparameters["state_space"],
    pixelcopter_hyperparameters["action_space"],
    pixelcopter_hyperparameters["h_size"],
).to(device)
pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters["lr"])
```

```py
scores = reinforce(
    pixelcopter_policy,
    pixelcopter_optimizer,
    pixelcopter_hyperparameters["n_training_episodes"],
    pixelcopter_hyperparameters["max_t"],
    pixelcopter_hyperparameters["gamma"],
    1000,
)
```

### åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹

```py
repo_id = ""  # TODO Define your repo id {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    pixelcopter_policy,  # The model we want to save
    pixelcopter_hyperparameters,  # Hyperparameters
    eval_env,  # Evaluation environment
    video_fps=30
)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜

å­¦ä¹ çš„æœ€ä½³æ–¹å¼æ˜¯è‡ªå·±å°è¯•ï¼æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰ä»£ç†è¡¨ç°ä¸ä½³ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œæ‚¨å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚ä½†ä¹Ÿå°è¯•æ‰¾åˆ°æ›´å¥½çš„å‚æ•°ã€‚

åœ¨æ’è¡Œæ¦œä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°æ‚¨çš„ä»£ç†ã€‚æ‚¨èƒ½å¤Ÿè¾¾åˆ°æ¦œé¦–å—ï¼Ÿ

ä»¥ä¸‹æ˜¯ä¸€äº›æå‡æ’è¡Œæ¦œçš„æƒ³æ³•ï¼š

+   è®­ç»ƒæ›´å¤šæ­¥éª¤

+   å°è¯•ä¸åŒçš„è¶…å‚æ•°ï¼ŒæŸ¥çœ‹åŒå­¦ä»¬çš„åšæ³•

+   å°†æ‚¨çš„æ–°è®­ç»ƒæ¨¡å‹å‘å¸ƒåˆ°Hub

+   æ”¹è¿›å®ç°ä»¥é€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œå°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†å¸§ä½œä¸ºè§‚å¯Ÿï¼Ÿï¼‰

* * *

ç¥è´ºæ‚¨å®Œæˆè¿™ä¸ªå•å…ƒï¼è¿™é‡Œæœ‰å¾ˆå¤šä¿¡æ¯ã€‚ç¥è´ºæ‚¨å®Œæˆæ•™ç¨‹ã€‚æ‚¨åˆšåˆšä½¿ç”¨PyTorchä»å¤´å¼€å§‹ç¼–å†™äº†æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œå¹¶åœ¨Hubä¸Šåˆ†äº«äº†å®ƒã€‚

ä¸è¦çŠ¹è±«ï¼Œé€šè¿‡æ”¹è¿›å®ç°ä»¥é€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œå°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†å¸§ä½œä¸ºè§‚å¯Ÿï¼‰æ¥è¿­ä»£è¿™ä¸ªå•å…ƒã€‚

åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æ›´å¤šå…³äºUnity MLAgentsï¼Œé€šè¿‡åœ¨Unityç¯å¢ƒä¸­è®­ç»ƒä»£ç†ã€‚è¿™æ ·ï¼Œæ‚¨å°†å‡†å¤‡å¥½å‚åŠ AIå¯¹AIæŒ‘æˆ˜ï¼Œæ‚¨å°†è®­ç»ƒæ‚¨çš„ä»£ç†æ¥ä¸å…¶ä»–ä»£ç†åœ¨é›ªä»—å’Œè¶³çƒæ¯”èµ›ä¸­ç«äº‰ã€‚

å¬èµ·æ¥å¾ˆæœ‰è¶£å—ï¼Ÿä¸‹æ¬¡è§ï¼

æœ€åï¼Œæˆ‘ä»¬å¾ˆæƒ³çŸ¥é“æ‚¨å¯¹è¯¾ç¨‹çš„çœ‹æ³•ä»¥åŠæˆ‘ä»¬å¦‚ä½•æ”¹è¿›å®ƒã€‚å¦‚æœæ‚¨æœ‰åé¦ˆæ„è§ï¼Œè¯·å¡«å†™æ­¤è¡¨æ ¼ã€‚

ä¸‹æ¬¡è§åœ¨ç¬¬5å•å…ƒï¼

### ç»§ç»­å­¦ä¹ ï¼Œä¿æŒç²¾å½©
