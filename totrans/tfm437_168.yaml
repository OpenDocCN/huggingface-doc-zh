- en: FLAN-UL2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/129.ff484769.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses
    the same configuration as the [UL2](ul2) model released earlier last year. It
    was fine tuned using the “Flan” prompt tuning and dataset collection. Similar
    to `Flan-T5`, one can directly use FLAN-UL2 weights without finetuning the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the original blog here are the notable improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: The original UL2 model was only trained with receptive field of 512, which made
    it non-ideal for N-shot prompting where N is large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable
    for few-shot in-context learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original UL2 model also had mode switch tokens that was rather mandatory
    to get good performance. However, they were a little cumbersome as this requires
    often some changes during inference or finetuning. In this update/change, we continue
    training UL2 20B for an additional 100k steps (with small batch) to forget “mode
    tokens” before applying Flan instruction tuning. This Flan-UL2 checkpoint does
    not require mode tokens anymore. Google has released the following variants:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints).
  prefs: []
  type: TYPE_NORMAL
- en: Running on low resource devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model is pretty heavy (~40GB in half precision) so if you just want to run
    the model, make sure you load your model in 8bit, and use `device_map="auto"`
    to make sure you don’t have any OOM issue!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Refer to [T5’s documentation page](t5) for API reference, tips, code examples
    and notebooks.
  prefs: []
  type: TYPE_NORMAL
