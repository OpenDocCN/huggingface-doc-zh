- en: FLAN-UL2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses
    the same configuration as the [UL2](ul2) model released earlier last year. It
    was fine tuned using the “Flan” prompt tuning and dataset collection. Similar
    to `Flan-T5`, one can directly use FLAN-UL2 weights without finetuning the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the original blog here are the notable improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: The original UL2 model was only trained with receptive field of 512, which made
    it non-ideal for N-shot prompting where N is large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable
    for few-shot in-context learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original UL2 model also had mode switch tokens that was rather mandatory
    to get good performance. However, they were a little cumbersome as this requires
    often some changes during inference or finetuning. In this update/change, we continue
    training UL2 20B for an additional 100k steps (with small batch) to forget “mode
    tokens” before applying Flan instruction tuning. This Flan-UL2 checkpoint does
    not require mode tokens anymore. Google has released the following variants:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints).
  prefs: []
  type: TYPE_NORMAL
- en: Running on low resource devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model is pretty heavy (~40GB in half precision) so if you just want to run
    the model, make sure you load your model in 8bit, and use `device_map="auto"`
    to make sure you don’t have any OOM issue!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Refer to [T5’s documentation page](t5) for API reference, tips, code examples
    and notebooks.
  prefs: []
  type: TYPE_NORMAL
