# CodeLlama

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/code_llama](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/code_llama)

## 概述

Code Llama模型是由Baptiste Rozière，Jonas Gehring，Fabian Gloeckle，Sten Sootla，Itai Gat，Xiaoqing Ellen Tan，Yossi Adi，Jingyu Liu，Tal Remez，Jérémy Rapin，Artyom Kozhevnikov，Ivan Evtimov，Joanna Bitton，Manish Bhatt，Cristian Canton Ferrer，Aaron Grattafiori，Wenhan Xiong，Alexandre Défossez，Jade Copet，Faisal Azhar，Hugo Touvron，Louis Martin，Nicolas Usunier，Thomas Scialom，Gabriel Synnaeve提出的，详见[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama/)。

论文摘要如下：

*我们发布了Code Llama，这是基于Llama 2的一系列大型代码语言模型，提供了在开放模型中最先进的性能，填充能力，支持大型输入上下文以及编程任务的零-shot指令跟随能力。我们提供多种风格以涵盖广泛的应用：基础模型（Code Llama），Python专业化模型（Code Llama - Python）和指令跟随模型（Code Llama - Instruct），分别具有7B，13B和34B参数。所有模型都是在16k标记序列上训练的，并在最多100k标记的输入上显示出改进。7B和13B的Code Llama和Code Llama - Instruct变体支持基于周围内容的填充。Code Llama在几个代码基准测试中达到了开放模型的最先进性能，分别在HumanEval和MBPP上达到了53%和55%的分数。值得注意的是，Code Llama - Python 7B在HumanEval和MBPP上优于Llama 2 70B，而我们所有的模型都优于MultiPL-E上的其他任何公开可用模型。我们以一种允许研究和商业使用的宽松许可证发布了Code Llama。*

查看所有Code Llama模型检查点[这里](https://huggingface.co/models?search=code_llama)，以及在[codellama org](https://huggingface.co/codellama)上正式发布的模型。

此模型由[ArthurZucker](https://huggingface.co/ArthurZ)贡献。作者的原始代码可以在[这里](https://github.com/facebookresearch/llama)找到。

## 用法提示和示例

Code Llama基于的`Llama2`系列模型是使用`bfloat16`训练的，但原始推断使用`float16`。让我们看看不同的精度：

+   `float32`：PyTorch在模型初始化时的惯例是以`float32`加载模型，无论模型权重存储时使用的是哪种`dtype`。`transformers`也遵循这种惯例，以保持与PyTorch的一致性。这将被默认选择。如果您希望`AutoModel` API将加载检查点时的存储权重类型转换为`dtype`，则必须指定`torch_dtype="auto"`，例如`model = AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`。

+   `bfloat16`：Code Llama是使用这种精度训练的，因此我们建议在进一步训练或微调时使用它。

+   `float16`：我们建议使用这种精度来运行推断，因为通常比`bfloat16`更快，并且评估指标显示与`bfloat16`相比没有明显的降级。您也可以使用`bfloat16`来运行推断，我们建议您在微调后使用`float16`和`bfloat16`检查推断结果。

如上所述，存储权重的`dtype`大多数情况下并不重要，除非您在初始化模型时使用`torch_dtype="auto"`。原因是模型将首先被下载（使用在线检查点的`dtype`），然后将被转换为`torch`的默认`dtype`（变为`torch.float32`）。如果有指定的`torch_dtype`，则将使用该指定值。

提示：

+   填充任务是开箱即用的。您应该在想要填充输入的地方使用`tokenizer.fill_token`。

+   模型转换脚本与`Llama2`系列相同：

以下是一个示例用法：

```py
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

请注意，执行脚本需要足够的CPU RAM来承载整个模型的float16精度（即使最大版本分为几个检查点，每个检查点都包含模型的每个权重的一部分，因此我们需要将它们全部加载到RAM中）。

转换后，可以通过以下方式加载模型和分词器：

```py
>>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer

>>> tokenizer = CodeLlamaTokenizer.from_pretrained("codellama/CodeLlama-7b-hf")
>>> model = LlamaForCausalLM.from_pretrained("codellama/CodeLlama-7b-hf")
>>> PROMPT = '''def remove_non_ascii(s: str) -> str:
    """ <FILL_ME>
    return result
'''
>>> input_ids = tokenizer(PROMPT, return_tensors="pt")["input_ids"]
>>> generated_ids = model.generate(input_ids, max_new_tokens=128)

>>> filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
>>> print(PROMPT.replace("<FILL_ME>", filling))
def remove_non_ascii(s: str) -> str:
    """ Remove non-ASCII characters from a string.

    Args:
        s: The string to remove non-ASCII characters from.

    Returns:
        The string with non-ASCII characters removed.
    """
    result = ""
    for c in s:
        if ord(c) < 128:
            result += c
    return result
```

如果只想要填充部分：

```py
>>> from transformers import pipeline
>>> import torch

>>> generator = pipeline("text-generation",model="codellama/CodeLlama-7b-hf",torch_dtype=torch.float16, device_map="auto")
>>> generator('def remove_non_ascii(s: str) -> str:\n    """ <FILL_ME>\n    return result', max_new_tokens = 128, return_type = 1)
```

在底层，分词器[自动通过`<FILL_ME>`进行拆分](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer.fill_token)以创建一个格式化的输入字符串，遵循[原始训练模式](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402)。这比自己准备模式更加健壮：它避免了很难调试的诸如标记粘合等问题。要查看此模型或其他模型所需的CPU和GPU内存量，请尝试使用[此计算器](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)，该计算器可以帮助确定该值。

LLaMA分词器是基于[sentencepiece](https://github.com/google/sentencepiece)的BPE模型。SentencePiece的一个特点是，在解码序列时，如果第一个标记是单词的开头（例如“Banana”），分词器不会在字符串前面添加前缀空格。

Code Llama与`Llama2`模型具有相同的架构，请参考[Llama2的文档页面](llama2)获取API参考。以下是Code Llama分词器的参考。

## CodeLlamaTokenizer

### `class transformers.CodeLlamaTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L59)

```py
( vocab_file unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' prefix_token = '▁<PRE>' middle_token = '▁<MID>' suffix_token = '▁<SUF>' eot_token = '▁<EOT>' fill_token = '<FILL_ME>' suffix_first = False sp_model_kwargs: Optional = None add_bos_token = True add_eos_token = False clean_up_tokenization_spaces = False additional_special_tokens = None use_default_system_prompt = False **kwargs )
```

参数

+   `vocab_file`（`str`）— 词汇表文件的路径。

+   `unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可用作序列分类器标记。

+   `eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结束标记。

    在构建使用特殊标记的序列时，这不是用于表示序列结束的标记。使用的标记是`sep_token`。

+   `prefix_token`（`str`，*可选*，默认为`"▁<PRE>"`）— 用于填充的前缀标记。

+   `middle_token`（`str`，*可选*，默认为`"▁<MID>"`）— 用于填充的中间标记。

+   `suffix_token`（`str`，*可选*，默认为`"▁<SUF>"`）— 用于填充的后缀标记。

+   `eot_token`（`str`，*可选*，默认为`"▁<EOT>"`）— 用于填充的文本结束标记。

+   `fill_token`（`str`，*可选*，默认为`"<FILL_ME>"`）— 用于在前缀和后缀之间拆分输入的标记。

+   `suffix_first`（`bool`，*可选*，默认为`False`）— 输入提示和后缀是否应该首先格式化后缀。

+   `sp_model_kwargs`（`dict`，*可选*）— 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`：启用子词规范化。

    +   `nbest_size`：unigram的抽样参数。对于BPE-Dropout无效。

        +   `nbest_size = {0,1}`：不执行抽样。

        +   `nbest_size > 1`：从nbest_size结果中进行抽样。

        +   `nbest_size < 0`：假设nbest_size是无限的，并使用前向过滤和后向抽样算法从所有假设（格子）中进行抽样。

    +   `alpha`：unigram抽样的平滑参数，以及BPE-dropout合并操作的丢弃概率。

+   `add_bos_token`（`bool`，*可选*，默认为`True`）— 是否在序列开头添加一个序列开始标记。

+   `add_eos_token`（`bool`，*可选*，默认为`False`）— 是否在序列末尾添加一个序列结束标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*，默认为`False`）— 是否清除分词空格。

+   `additional_special_tokens`（`List[str]`，*可选*）— 分词器使用的额外特殊标记。

+   `use_default_system_prompt`（`bool`，*可选*，默认为`False`）— 是否应使用Llama的默认系统提示。

构建一个CodeLlama分词器。基于字节级字节对编码。默认填充标记未设置，因为原始模型中没有填充标记。

默认配置与[sentencepiece/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)匹配，支持提示填充。

`build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L369)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L381)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经使用特殊标记格式化为模型。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列ID。在使用分词器的`prepare_for_model`方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L419)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。

返回

`List[int]`

根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。一个ALBERT

序列对掩码具有以下格式：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果`token_ids_1`为`None`，则仅返回掩码的第一部分（0s）。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L341)

```py
( save_directory filename_prefix: Optional = None ) → export const metadata = 'undefined';Tuple(str)
```

参数

+   `save_directory`（`str`）— 保存词汇表的目录。

返回

`Tuple(str)`

保存的文件路径。

将词汇表和特殊标记文件保存到目录中。

## CodeLlamaTokenizerFast

### `class transformers.CodeLlamaTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L52)

```py
( vocab_file = None tokenizer_file = None clean_up_tokenization_spaces = False unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' prefix_token = '▁<PRE>' middle_token = '▁<MID>' suffix_token = '▁<SUF>' eot_token = '▁<EOT>' fill_token = '<FILL_ME>' additional_special_tokens = None add_bos_token = True add_eos_token = False use_default_system_prompt = False **kwargs )
```

参数

+   `vocab_file`（`str`，*可选*）— [SentencePiece](https://github.com/google/sentencepiece)文件（通常具有`.model`扩展名），其中包含实例化分词器所需的词汇表。

+   `tokenizer_file`（`str`，*可选*）— [tokenizers](https://github.com/huggingface/tokenizers)文件（通常具有`.json`扩展名），其中包含加载分词器所需的所有内容。

+   `clean_up_tokenization_spaces`（`str`，*可选*，默认为`False`）— 解码后是否清除空格，清除包括删除额外的空格等潜在工件。

+   `unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。

+   `bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。

+   `prefix_token` (`str`, *optional*, defaults to `"▁<PRE>"`) — 用于填充的前缀标记。

+   `middle_token` (`str`, *optional*, defaults to `"▁<MID>"`) — 用于填充的中间标记。

+   `suffix_token` (`str`, *optional*, defaults to `"▁<SUF>"`) — 用于填充的后缀标记。

+   `eot_token` (`str`, *optional*, defaults to `"▁<EOT>"`) — 用于填充的文本结束标记。

+   `fill_token` (`str`, *optional*, defaults to `"<FILL_ME>"`) — 用于在前缀和后缀之间分割输入的标记。

+   `additional_special_tokens` (`List[str]`, *optional*) — 分词器使用的额外特殊标记。

+   `add_bos_token` (`bool`, *optional*, defaults to `True`) — 是否在序列开头添加一个起始标记。

+   `add_eos_token` (`bool`, *optional*, defaults to `False`) — 是否在序列末尾添加一个序列结束标记。

+   `use_default_system_prompt` (`bool`, *optional*, defaults to `False`) — 是否使用 Llama 的默认系统提示。

构建一个 Llama 分词器。基于字节级字节对编码。

这里特别使用了 ByteFallback 和无规范化。

```py
>>> from transformers import CodeLlamaTokenizerFast

>>> tokenizer = CodeLlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
>>> tokenizer.encode("Hello this is a test")
[1, 15043, 445, 338, 263, 1243]
```

如果要更改 `bos_token` 或 `eos_token`，请确保在初始化模型时指定它们，或调用 `tokenizer.update_post_processor()` 确保后处理正确完成（否则编码序列的第一个标记和最后一个标记的值将不正确）。有关更多详细信息，请查看 [post-processors] ([https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors)) 文档。

这个分词器继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。默认配置与 [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json) 匹配，支持提示填充。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L413)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 第二个序列的可选 ID 列表。

返回

`List[int]`

带有适当特殊标记的 [输入 ID](../glossary#input-ids) 列表。

通过连接和添加特殊标记，为序列分类任务构建模型输入的序列或序列对。特殊标记取决于调用 set_lang。

NLLB 序列具有以下格式，其中 `X` 表示序列：

+   `input_ids`（用于编码器）`X [eos, src_lang_code]`

+   `decoder_input_ids`: (用于解码器) `X [eos, tgt_lang_code]`

BOS 从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';A list of integers in the range [0, 1]
```

参数

+   `token_ids_0` (`List[int]`) — 第一个序列的 ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 第二个序列的 ID 列表。

+   `already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — 标记列表是否已经使用模型的特殊标记格式化。

返回

一个在范围[0, 1]内的整数列表

1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。当使用分词器的 `prepare_for_model` 或 `encode_plus` 方法添加特殊标记时，将调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 第一个标记化序列。

+   `token_ids_1` (`List[int]`, *optional*) — 第二个标记化序列。

返回

`List[int]`

标记类型ID。

创建与传递的序列相对应的标记类型ID。[什么是标记类型ID？](../glossary#token-type-ids)

如果模型有一种特殊的构建方式，则应该在子类中重写这个方法。

#### `update_post_processor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L179)

```py
( )
```

使用当前的`bos_token`和`eos_token`更新底层后处理器。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L333)

```py
( save_directory: str filename_prefix: Optional = None )
```
