- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/introduction](https://huggingface.co/learn/deep-rl-course/unit8/introduction)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In Unit 6, we learned about Advantage Actor Critic (A2C), a hybrid architecture
    combining value-based and policy-based methods that helps to stabilize the training
    by reducing the variance with:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An Actor* that controls **how our agent behaves** (policy-based method).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Critic* that measures **how good the action taken is** (value-based method).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Today weâ€™ll learn about Proximal Policy Optimization (PPO), an architecture
    that **improves our agentâ€™s training stability by avoiding policy updates that
    are too large**. To do that, we use a ratio that indicates the difference between
    our current and old policy and clip this ratio to a specific range<math><semantics><mrow><mo
    stretchy="false">[</mo><mn>1</mn><mo>âˆ’</mo><mi>Ïµ</mi><mo separator="true">,</mo><mn>1</mn><mo>+</mo><mi>Ïµ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 - \epsilon,
    1 + \epsilon]</annotation></semantics></math> [1âˆ’Ïµ,1+Ïµ] .
  prefs: []
  type: TYPE_NORMAL
- en: Doing this will ensure **that our policy update will not be too large and that
    the training is more stable.**
  prefs: []
  type: TYPE_NORMAL
- en: 'This Unit is in two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: In this first part, youâ€™ll learn the theory behind PPO and code your PPO agent
    from scratch using the [CleanRL](https://github.com/vwxyzjn/cleanrl) implementation.
    To test its robustness youâ€™ll use LunarLander-v2\. LunarLander-v2 **is the first
    environment you used when you started this course**. At that time, you didnâ€™t
    know how PPO worked, and now, **you can code it from scratch and train it. How
    incredible is that ðŸ¤©**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second part, weâ€™ll get deeper into PPO optimization by using [Sample-Factory](https://samplefactory.dev/)
    and train an agent playing vizdoom (an open source version of Doom).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Environment](../Images/3244d92e568ba653445e92e451579990.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These are the environments you''re going to use to train your agents: VizDoom
    environments'
  prefs: []
  type: TYPE_NORMAL
- en: Sound exciting? Letâ€™s get started! ðŸš€
  prefs: []
  type: TYPE_NORMAL
