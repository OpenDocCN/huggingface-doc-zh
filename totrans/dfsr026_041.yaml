- en: ControlNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/controlnet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/controlnet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet is a type of model for controlling image diffusion models by conditioning
    the model with an additional input image. There are many types of conditioning
    inputs (canny edge, user sketching, human pose, depth, and more) you can use to
    control a diffusion model. This is hugely useful because it affords you greater
    control over image generation, making it easier to generate specific images without
    experimenting with different text prompts or denoising values as much.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet æ˜¯ä¸€ç§é€šè¿‡åœ¨æ¨¡å‹ä¸­åŠ å…¥é¢å¤–è¾“å…¥å›¾åƒæ¥æ§åˆ¶å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ç±»å‹ã€‚æœ‰è®¸å¤šç±»å‹çš„æ¡ä»¶è¾“å…¥ï¼ˆçµå·§è¾¹ç¼˜ã€ç”¨æˆ·ç´ æã€äººä½“å§¿åŠ¿ã€æ·±åº¦ç­‰ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›è¾“å…¥æ¥æ§åˆ¶æ‰©æ•£æ¨¡å‹ã€‚è¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä½¿æ‚¨èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶å›¾åƒç”Ÿæˆï¼Œæ›´å®¹æ˜“ç”Ÿæˆç‰¹å®šå›¾åƒï¼Œè€Œæ— éœ€å°è¯•ä¸åŒçš„æ–‡æœ¬æç¤ºæˆ–å»å™ªå€¼ã€‚
- en: Check out Section 3.5 of the [ControlNet](https://huggingface.co/papers/2302.05543)
    paper v1 for a list of ControlNet implementations on various conditioning inputs.
    You can find the official Stable Diffusion ControlNet conditioned models on [lllyasviel](https://huggingface.co/lllyasviel)â€™s
    Hub profile, and more [community-trained](https://huggingface.co/models?other=stable-diffusion&other=controlnet)
    ones on the Hub.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ [ControlNet](https://huggingface.co/papers/2302.05543) è®ºæ–‡ v1 çš„ç¬¬ 3.5 èŠ‚ï¼Œäº†è§£å„ç§æ¡ä»¶è¾“å…¥ä¸Šçš„
    ControlNet å®ç°åˆ—è¡¨ã€‚æ‚¨å¯ä»¥åœ¨ [lllyasviel](https://huggingface.co/lllyasviel) çš„ Hub ä¸ªäººèµ„æ–™ä¸Šæ‰¾åˆ°å®˜æ–¹
    Stable Diffusion ControlNet æ¡ä»¶æ¨¡å‹ï¼Œä»¥åŠåœ¨ Hub ä¸Šæ›´å¤šçš„ [community-trained](https://huggingface.co/models?other=stable-diffusion&other=controlnet)
    æ¨¡å‹ã€‚
- en: For Stable Diffusion XL (SDXL) ControlNet models, you can find them on the ğŸ¤—
    [Diffusers](https://huggingface.co/diffusers) Hub organization, or you can browse
    [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
    ones on the Hub.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº Stable Diffusion XLï¼ˆSDXLï¼‰ControlNet æ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨ ğŸ¤— [Diffusers](https://huggingface.co/diffusers)
    Hub ç»„ç»‡ä¸­æ‰¾åˆ°å®ƒä»¬ï¼Œæˆ–è€…æ‚¨å¯ä»¥åœ¨ Hub ä¸Šæµè§ˆ [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
    çš„æ¨¡å‹ã€‚
- en: 'A ControlNet model has two sets of weights (or blocks) connected by a zero-convolution
    layer:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet æ¨¡å‹æœ‰ä¸¤ç»„æƒé‡ï¼ˆæˆ–å—ï¼‰ï¼Œé€šè¿‡ä¸€ä¸ªé›¶å·ç§¯å±‚è¿æ¥ï¼š
- en: a *locked copy* keeps everything a large pretrained diffusion model has learned
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*é”å®šçš„å‰¯æœ¬* ä¿ç•™äº†å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å­¦åˆ°çš„ä¸€åˆ‡'
- en: a *trainable copy* is trained on the additional conditioning input
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¯è®­ç»ƒçš„å‰¯æœ¬* æ˜¯åœ¨é¢å¤–çš„æ¡ä»¶è¾“å…¥ä¸Šè®­ç»ƒçš„'
- en: Since the locked copy preserves the pretrained model, training and implementing
    a ControlNet on a new conditioning input is as fast as finetuning any other model
    because you arenâ€™t training the model from scratch.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºé”å®šçš„å‰¯æœ¬ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤åœ¨æ–°çš„æ¡ä»¶è¾“å…¥ä¸Šè®­ç»ƒå’Œå®ç°ControlNetä¸å¾®è°ƒä»»ä½•å…¶ä»–æ¨¡å‹ä¸€æ ·å¿«ï¼Œå› ä¸ºæ‚¨ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚
- en: This guide will show you how to use ControlNet for text-to-image, image-to-image,
    inpainting, and more! There are many types of ControlNet conditioning inputs to
    choose from, but in this guide weâ€™ll only focus on several of them. Feel free
    to experiment with other conditioning inputs!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ControlNet è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€ä¿®å¤ç­‰æ“ä½œï¼æœ‰è®¸å¤šç±»å‹çš„ ControlNet æ¡ä»¶è¾“å…¥å¯ä¾›é€‰æ‹©ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬åªå…³æ³¨å…¶ä¸­å‡ ç§ã€‚è¯·éšæ„å°è¯•å…¶ä»–æ¡ä»¶è¾“å…¥ï¼
- en: 'Before you begin, make sure you have the following libraries installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Text-to-image
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒ
- en: For text-to-image, you normally pass a text prompt to the model. But with ControlNet,
    you can specify an additional conditioning input. Letâ€™s condition the model with
    a canny image, a white outline of an image on a black background. This way, the
    ControlNet can use the canny image as a control to guide the model to generate
    an image with the same outline.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ–‡æœ¬åˆ°å›¾åƒï¼Œé€šå¸¸ä¼šå‘æ¨¡å‹ä¼ é€’æ–‡æœ¬æç¤ºã€‚ä½†æ˜¯ä½¿ç”¨ ControlNetï¼Œæ‚¨å¯ä»¥æŒ‡å®šé¢å¤–çš„æ¡ä»¶è¾“å…¥ã€‚è®©æˆ‘ä»¬ä½¿ç”¨çµå·§å›¾åƒå¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶ï¼Œçµå·§å›¾åƒæ˜¯åœ¨é»‘è‰²èƒŒæ™¯ä¸Šçš„å›¾åƒçš„ç™½è‰²è½®å»“ã€‚è¿™æ ·ï¼ŒControlNet
    å¯ä»¥ä½¿ç”¨çµå·§å›¾åƒä½œä¸ºæ§åˆ¶æ¥æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆå…·æœ‰ç›¸åŒè½®å»“çš„å›¾åƒã€‚
- en: 'Load an image and use the [opencv-python](https://github.com/opencv/opencv-python)
    library to extract the canny image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€å¼ å›¾åƒå¹¶ä½¿ç”¨ [opencv-python](https://github.com/opencv/opencv-python) åº“æå–çµå·§å›¾åƒï¼š
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/f3d8c311d87a9fa7e106fb97353058b0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d8c311d87a9fa7e106fb97353058b0.png)'
- en: original image
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å›¾åƒ
- en: '![](../Images/58be7817d7057b4931067ef80d28b944.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58be7817d7057b4931067ef80d28b944.png)'
- en: canny image
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: çµå·§å›¾åƒ
- en: Next, load a ControlNet model conditioned on canny edge detection and pass it
    to the [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to speed up inference and reduce memory usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªåœ¨çµå·§è¾¹ç¼˜æ£€æµ‹ä¸Šè¿›è¡Œæ¡ä»¶çš„ ControlNet æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)ã€‚ä½¿ç”¨æ›´å¿«çš„
    [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now pass your prompt and canny image to the pipeline:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†æ‚¨çš„æç¤ºå’Œçµå·§çš„å›¾åƒä¼ é€’ç»™ç®¡é“ï¼š
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/0ae01d64d6dd0f507a15d4c469ccb3ea.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ae01d64d6dd0f507a15d4c469ccb3ea.png)'
- en: Image-to-image
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒ
- en: For image-to-image, youâ€™d typically pass an initial image and a prompt to the
    pipeline to generate a new image. With ControlNet, you can pass an additional
    conditioning input to guide the model. Letâ€™s condition the model with a depth
    map, an image which contains spatial information. This way, the ControlNet can
    use the depth map as a control to guide the model to generate an image that preserves
    spatial information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾åƒåˆ°å›¾åƒï¼Œé€šå¸¸ä¼šå°†åˆå§‹å›¾åƒå’Œæç¤ºä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆæ–°å›¾åƒã€‚ä½¿ç”¨ ControlNetï¼Œæ‚¨å¯ä»¥ä¼ é€’é¢å¤–çš„æ¡ä»¶è¾“å…¥æ¥æŒ‡å¯¼æ¨¡å‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨æ·±åº¦å›¾æ¥å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶ï¼Œæ·±åº¦å›¾åŒ…å«ç©ºé—´ä¿¡æ¯ã€‚è¿™æ ·ï¼ŒControlNet
    å¯ä»¥ä½¿ç”¨æ·±åº¦å›¾ä½œä¸ºæ§åˆ¶æ¥æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆä¿ç•™ç©ºé—´ä¿¡æ¯çš„å›¾åƒã€‚
- en: Youâ€™ll use the [StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)
    for this task, which is different from the [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)
    because it allows you to pass an initial image as the starting point for the image
    generation process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Load an image and use the `depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)
    from ğŸ¤— Transformers to extract the depth map of an image:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, load a ControlNet model conditioned on depth maps and pass it to the [StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to speed up inference and reduce memory usage.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now pass your prompt, initial image, and depth map to the pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/1677a0fe907026354ef7fe3200082c57.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: original image
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cb27d107bcf2a908b7a2c6086129663.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: generated image
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Inpainting
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For inpainting, you need an initial image, a mask image, and a prompt describing
    what to replace the mask with. ControlNet models allow you to add another control
    image to condition a model with. Letâ€™s condition the model with an inpainting
    mask. This way, the ControlNet can use the inpainting mask as a control to guide
    the model to generate an image within the mask area.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Load an initial image and a mask image:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create a function to prepare the control image from the initial and mask images.
    Thisâ€™ll create a tensor to mark the pixels in `init_image` as masked if the corresponding
    pixel in `mask_image` is over a certain threshold.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/0fb83d127798531122cbaf23bb76b7c8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: original image
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1431ffd86896d9e2aed5e38fead866b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: mask image
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Load a ControlNet model conditioned on inpainting and pass it to the [StableDiffusionControlNetInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetInpaintPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to speed up inference and reduce memory usage.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now pass your prompt, initial image, mask image, and control image to the pipeline:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/da98e94db90a9c1f480cba2d1ce9fd5a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Guess mode
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Guess mode](https://github.com/lllyasviel/ControlNet/discussions/188) does
    not require supplying a prompt to a ControlNet at all! This forces the ControlNet
    encoder to do itâ€™s best to â€œguessâ€ the contents of the input control map (depth
    map, pose estimation, canny edge, etc.).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Guess mode adjusts the scale of the output residuals from a ControlNet by a
    fixed ratio depending on the block depth. The shallowest `DownBlock` corresponds
    to 0.1, and as the blocks get deeper, the scale increases exponentially such that
    the scale of the `MidBlock` output becomes 1.0.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Guess mode does not have any impact on prompt conditioning and you can still
    provide a prompt if you want.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Set `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode)
    to set the `guidance_scale` value between 3.0 and 5.0.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a552b1e5d87c2575305045e9d6e5c665.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: regular mode with prompt
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e47994b983e6afdf574ca4730cc4d14.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: guess mode without prompt
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet with Stable Diffusion XL
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There arenâ€™t too many ControlNet models compatible with Stable Diffusion XL
    (SDXL) at the moment, but weâ€™ve trained two full-sized ControlNet models for SDXL
    conditioned on canny edge detection and depth maps. Weâ€™re also experimenting with
    creating smaller versions of these SDXL-compatible ControlNet models so it is
    easier to run on resource-constrained hardware. You can find these checkpoints
    on the [ğŸ¤— Diffusers Hub organization](https://huggingface.co/diffusers)!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ä¸Stable Diffusion XLï¼ˆSDXLï¼‰å…¼å®¹çš„ControlNetæ¨¡å‹å¹¶ä¸å¤šï¼Œä½†æˆ‘ä»¬å·²ç»ä¸ºSDXLè®­ç»ƒäº†ä¸¤ä¸ªå…¨å°ºå¯¸çš„ControlNetæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ˜¯åŸºäºè¾¹ç¼˜æ£€æµ‹å’Œæ·±åº¦å›¾çš„æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜åœ¨å°è¯•åˆ›å»ºè¿™äº›SDXLå…¼å®¹çš„ControlNetæ¨¡å‹çš„è¾ƒå°ç‰ˆæœ¬ï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šè¿è¡Œã€‚æ‚¨å¯ä»¥åœ¨[ğŸ¤—
    Diffusers Hubç»„ç»‡](https://huggingface.co/diffusers)ä¸Šæ‰¾åˆ°è¿™äº›æ£€æŸ¥ç‚¹ï¼
- en: 'Letâ€™s use a SDXL ControlNet conditioned on canny images to generate an image.
    Start by loading an image and prepare the canny image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŸºäºè¾¹ç¼˜æ£€æµ‹çš„SDXL ControlNetæ¥ç”Ÿæˆä¸€å¹…å›¾åƒã€‚é¦–å…ˆåŠ è½½ä¸€å¹…å›¾åƒå¹¶å‡†å¤‡è¾¹ç¼˜æ£€æµ‹å›¾åƒï¼š
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/dfd7f04394323063a78501b85525d2b3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfd7f04394323063a78501b85525d2b3.png)'
- en: original image
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å›¾åƒ
- en: '![](../Images/a7144be809b8fc762e76238607a485d2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7144be809b8fc762e76238607a485d2.png)'
- en: canny image
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¹ç¼˜æ£€æµ‹å›¾åƒ
- en: Load a SDXL ControlNet model conditioned on canny edge detection and pass it
    to the [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline).
    You can also enable model offloading to reduce memory usage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€ä¸ªåŸºäºè¾¹ç¼˜æ£€æµ‹çš„SDXL ControlNetæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ã€‚æ‚¨è¿˜å¯ä»¥å¯ç”¨æ¨¡å‹å¸è½½ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now pass your prompt (and optionally a negative prompt if youâ€™re using one)
    and canny image to the pipeline:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†æ‚¨çš„æç¤ºï¼ˆå¦‚æœæ‚¨ä½¿ç”¨è´Ÿæç¤ºï¼Œåˆ™å¯é€‰ï¼‰å’Œè¾¹ç¼˜æ£€æµ‹å›¾åƒä¼ é€’ç»™ç®¡é“ï¼š
- en: The [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)
    parameter determines how much weight to assign to the conditioning inputs. A value
    of 0.5 is recommended for good generalization, but feel free to experiment with
    this number!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)å‚æ•°ç¡®å®šåˆ†é…ç»™æ¡ä»¶è¾“å…¥çš„æƒé‡ã€‚å»ºè®®å°†å€¼è®¾ä¸º0.5ä»¥è·å¾—è‰¯å¥½çš„æ³›åŒ–æ•ˆæœï¼Œä½†è¯·éšæ„å°è¯•è¿™ä¸ªæ•°å­—ï¼'
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/033f4d98632798d409eabe8ebfffc8db.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/033f4d98632798d409eabe8ebfffc8db.png)'
- en: 'You can use [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)
    in guess mode as well by setting the parameter to `True`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡å°†å‚æ•°è®¾ç½®ä¸º`True`ï¼Œåœ¨çŒœæµ‹æ¨¡å¼ä¸‹ä½¿ç”¨[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ï¼š
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: MultiControlNet
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MultiControlNet
- en: Replace the SDXL model with a model like [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    to use multiple conditioning inputs with Stable Diffusion models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å°†SDXLæ¨¡å‹æ›¿æ¢ä¸ºåƒ[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)è¿™æ ·çš„æ¨¡å‹ï¼Œä»¥ä½¿ç”¨å¤šä¸ªæ¡ä»¶è¾“å…¥ä¸Stable
    Diffusionæ¨¡å‹ã€‚
- en: 'You can compose multiple ControlNet conditionings from different image inputs
    to create a *MultiControlNet*. To get better results, it is often helpful to:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä»ä¸åŒçš„å›¾åƒè¾“å…¥ä¸­ç»„åˆå¤šä¸ªControlNetæ¡ä»¶ï¼Œä»¥åˆ›å»º*MultiControlNet*ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œé€šå¸¸æœ‰å¸®åŠ©çš„æ˜¯ï¼š
- en: mask conditionings such that they donâ€™t overlap (for example, mask the area
    of a canny image where the pose conditioning is located)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æ¡ä»¶è¿›è¡Œé®ç½©å¤„ç†ï¼Œä½¿å…¶ä¸é‡å ï¼ˆä¾‹å¦‚ï¼Œé®ç½©å§¿åŠ¿æ¡ä»¶æ‰€åœ¨çš„è¾¹ç¼˜æ£€æµ‹å›¾åƒåŒºåŸŸï¼‰
- en: experiment with the [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)
    parameter to determine how much weight to assign to each conditioning input
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°è¯•[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)å‚æ•°ï¼Œä»¥ç¡®å®šåˆ†é…ç»™æ¯ä¸ªæ¡ä»¶è¾“å…¥çš„æƒé‡
- en: In this example, youâ€™ll combine a canny image and a human pose estimation image
    to generate a new image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†ç»“åˆè¾¹ç¼˜æ£€æµ‹å›¾åƒå’Œäººä½“å§¿åŠ¿ä¼°è®¡å›¾åƒæ¥ç”Ÿæˆä¸€å¹…æ–°å›¾åƒã€‚
- en: 'Prepare the canny image conditioning:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡è¾¹ç¼˜æ£€æµ‹å›¾åƒæ¡ä»¶ï¼š
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/35bd0f11e1c7ca277ac60a85a75be06a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35bd0f11e1c7ca277ac60a85a75be06a.png)'
- en: original image
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å›¾åƒ
- en: '![](../Images/5a37576bad87523fdb8a31aa840b4fdc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a37576bad87523fdb8a31aa840b4fdc.png)'
- en: canny image
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¹ç¼˜æ£€æµ‹å›¾åƒ
- en: 'For human pose estimation, install [controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºäººä½“å§¿åŠ¿ä¼°è®¡ï¼Œå®‰è£…[controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)ï¼š
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Prepare the human pose estimation conditioning:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡äººä½“å§¿åŠ¿ä¼°è®¡æ¡ä»¶ï¼š
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/77e2344bf3a8143f50f20ca5d095bf81.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77e2344bf3a8143f50f20ca5d095bf81.png)'
- en: original image
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å›¾åƒ
- en: '![](../Images/5a8432160ffe7c6268d59c8c8d536c20.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a8432160ffe7c6268d59c8c8d536c20.png)'
- en: human pose image
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: äººä½“å§¿åŠ¿å›¾åƒ
- en: Load a list of ControlNet models that correspond to each conditioning, and pass
    them to the [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to reduce memory usage.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸æ¯ä¸ªæ¡ä»¶å¯¹åº”çš„ControlNetæ¨¡å‹åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬ä¼ é€’ç»™[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)ã€‚ä½¿ç”¨æ›´å¿«çš„[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now you can pass your prompt (an optional negative prompt if youâ€™re using one),
    canny image, and pose image to the pipeline:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†æ‚¨çš„æç¤ºï¼ˆå¦‚æœæ‚¨ä½¿ç”¨è´Ÿæç¤ºï¼Œåˆ™å¯é€‰ï¼‰ã€è¾¹ç¼˜æ£€æµ‹å›¾åƒå’Œå§¿åŠ¿å›¾åƒä¼ é€’ç»™ç®¡é“ï¼š
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/780255e1bc5d49726b432882894f0b81.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/780255e1bc5d49726b432882894f0b81.png)'
