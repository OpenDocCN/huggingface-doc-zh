- en: ControlNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/controlnet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/controlnet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet is a type of model for controlling image diffusion models by conditioning
    the model with an additional input image. There are many types of conditioning
    inputs (canny edge, user sketching, human pose, depth, and more) you can use to
    control a diffusion model. This is hugely useful because it affords you greater
    control over image generation, making it easier to generate specific images without
    experimenting with different text prompts or denoising values as much.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet 是一种通过在模型中加入额外输入图像来控制图像扩散模型的模型类型。有许多类型的条件输入（灵巧边缘、用户素描、人体姿势、深度等），您可以使用这些输入来控制扩散模型。这非常有用，因为它使您能够更好地控制图像生成，更容易生成特定图像，而无需尝试不同的文本提示或去噪值。
- en: Check out Section 3.5 of the [ControlNet](https://huggingface.co/papers/2302.05543)
    paper v1 for a list of ControlNet implementations on various conditioning inputs.
    You can find the official Stable Diffusion ControlNet conditioned models on [lllyasviel](https://huggingface.co/lllyasviel)’s
    Hub profile, and more [community-trained](https://huggingface.co/models?other=stable-diffusion&other=controlnet)
    ones on the Hub.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [ControlNet](https://huggingface.co/papers/2302.05543) 论文 v1 的第 3.5 节，了解各种条件输入上的
    ControlNet 实现列表。您可以在 [lllyasviel](https://huggingface.co/lllyasviel) 的 Hub 个人资料上找到官方
    Stable Diffusion ControlNet 条件模型，以及在 Hub 上更多的 [community-trained](https://huggingface.co/models?other=stable-diffusion&other=controlnet)
    模型。
- en: For Stable Diffusion XL (SDXL) ControlNet models, you can find them on the 🤗
    [Diffusers](https://huggingface.co/diffusers) Hub organization, or you can browse
    [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
    ones on the Hub.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Stable Diffusion XL（SDXL）ControlNet 模型，您可以在 🤗 [Diffusers](https://huggingface.co/diffusers)
    Hub 组织中找到它们，或者您可以在 Hub 上浏览 [community-trained](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
    的模型。
- en: 'A ControlNet model has two sets of weights (or blocks) connected by a zero-convolution
    layer:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet 模型有两组权重（或块），通过一个零卷积层连接：
- en: a *locked copy* keeps everything a large pretrained diffusion model has learned
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*锁定的副本* 保留了大型预训练扩散模型学到的一切'
- en: a *trainable copy* is trained on the additional conditioning input
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可训练的副本* 是在额外的条件输入上训练的'
- en: Since the locked copy preserves the pretrained model, training and implementing
    a ControlNet on a new conditioning input is as fast as finetuning any other model
    because you aren’t training the model from scratch.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于锁定的副本保留了预训练模型，因此在新的条件输入上训练和实现ControlNet与微调任何其他模型一样快，因为您不是从头开始训练模型。
- en: This guide will show you how to use ControlNet for text-to-image, image-to-image,
    inpainting, and more! There are many types of ControlNet conditioning inputs to
    choose from, but in this guide we’ll only focus on several of them. Feel free
    to experiment with other conditioning inputs!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何使用 ControlNet 进行文本到图像、图像到图像、修复等操作！有许多类型的 ControlNet 条件输入可供选择，但在本指南中，我们只关注其中几种。请随意尝试其他条件输入！
- en: 'Before you begin, make sure you have the following libraries installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保已安装以下库：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Text-to-image
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像
- en: For text-to-image, you normally pass a text prompt to the model. But with ControlNet,
    you can specify an additional conditioning input. Let’s condition the model with
    a canny image, a white outline of an image on a black background. This way, the
    ControlNet can use the canny image as a control to guide the model to generate
    an image with the same outline.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本到图像，通常会向模型传递文本提示。但是使用 ControlNet，您可以指定额外的条件输入。让我们使用灵巧图像对模型进行条件，灵巧图像是在黑色背景上的图像的白色轮廓。这样，ControlNet
    可以使用灵巧图像作为控制来指导模型生成具有相同轮廓的图像。
- en: 'Load an image and use the [opencv-python](https://github.com/opencv/opencv-python)
    library to extract the canny image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一张图像并使用 [opencv-python](https://github.com/opencv/opencv-python) 库提取灵巧图像：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/f3d8c311d87a9fa7e106fb97353058b0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d8c311d87a9fa7e106fb97353058b0.png)'
- en: original image
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像
- en: '![](../Images/58be7817d7057b4931067ef80d28b944.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58be7817d7057b4931067ef80d28b944.png)'
- en: canny image
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 灵巧图像
- en: Next, load a ControlNet model conditioned on canny edge detection and pass it
    to the [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to speed up inference and reduce memory usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载一个在灵巧边缘检测上进行条件的 ControlNet 模型，并将其传递给 [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)。使用更快的
    [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    并启用模型卸载以加快推理速度并减少内存使用。
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now pass your prompt and canny image to the pipeline:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将您的提示和灵巧的图像传递给管道：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/0ae01d64d6dd0f507a15d4c469ccb3ea.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ae01d64d6dd0f507a15d4c469ccb3ea.png)'
- en: Image-to-image
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像到图像
- en: For image-to-image, you’d typically pass an initial image and a prompt to the
    pipeline to generate a new image. With ControlNet, you can pass an additional
    conditioning input to guide the model. Let’s condition the model with a depth
    map, an image which contains spatial information. This way, the ControlNet can
    use the depth map as a control to guide the model to generate an image that preserves
    spatial information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像到图像，通常会将初始图像和提示传递给管道以生成新图像。使用 ControlNet，您可以传递额外的条件输入来指导模型。让我们使用深度图来对模型进行条件，深度图包含空间信息。这样，ControlNet
    可以使用深度图作为控制来指导模型生成保留空间信息的图像。
- en: You’ll use the [StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)
    for this task, which is different from the [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)
    because it allows you to pass an initial image as the starting point for the image
    generation process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Load an image and use the `depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)
    from 🤗 Transformers to extract the depth map of an image:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, load a ControlNet model conditioned on depth maps and pass it to the [StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to speed up inference and reduce memory usage.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now pass your prompt, initial image, and depth map to the pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/1677a0fe907026354ef7fe3200082c57.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: original image
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cb27d107bcf2a908b7a2c6086129663.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: generated image
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Inpainting
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For inpainting, you need an initial image, a mask image, and a prompt describing
    what to replace the mask with. ControlNet models allow you to add another control
    image to condition a model with. Let’s condition the model with an inpainting
    mask. This way, the ControlNet can use the inpainting mask as a control to guide
    the model to generate an image within the mask area.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Load an initial image and a mask image:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create a function to prepare the control image from the initial and mask images.
    This’ll create a tensor to mark the pixels in `init_image` as masked if the corresponding
    pixel in `mask_image` is over a certain threshold.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/0fb83d127798531122cbaf23bb76b7c8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: original image
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1431ffd86896d9e2aed5e38fead866b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: mask image
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Load a ControlNet model conditioned on inpainting and pass it to the [StableDiffusionControlNetInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetInpaintPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to speed up inference and reduce memory usage.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now pass your prompt, initial image, mask image, and control image to the pipeline:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/da98e94db90a9c1f480cba2d1ce9fd5a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Guess mode
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Guess mode](https://github.com/lllyasviel/ControlNet/discussions/188) does
    not require supplying a prompt to a ControlNet at all! This forces the ControlNet
    encoder to do it’s best to “guess” the contents of the input control map (depth
    map, pose estimation, canny edge, etc.).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Guess mode adjusts the scale of the output residuals from a ControlNet by a
    fixed ratio depending on the block depth. The shallowest `DownBlock` corresponds
    to 0.1, and as the blocks get deeper, the scale increases exponentially such that
    the scale of the `MidBlock` output becomes 1.0.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Guess mode does not have any impact on prompt conditioning and you can still
    provide a prompt if you want.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Set `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode)
    to set the `guidance_scale` value between 3.0 and 5.0.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a552b1e5d87c2575305045e9d6e5c665.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: regular mode with prompt
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e47994b983e6afdf574ca4730cc4d14.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: guess mode without prompt
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet with Stable Diffusion XL
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There aren’t too many ControlNet models compatible with Stable Diffusion XL
    (SDXL) at the moment, but we’ve trained two full-sized ControlNet models for SDXL
    conditioned on canny edge detection and depth maps. We’re also experimenting with
    creating smaller versions of these SDXL-compatible ControlNet models so it is
    easier to run on resource-constrained hardware. You can find these checkpoints
    on the [🤗 Diffusers Hub organization](https://huggingface.co/diffusers)!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目前与Stable Diffusion XL（SDXL）兼容的ControlNet模型并不多，但我们已经为SDXL训练了两个全尺寸的ControlNet模型，这些模型是基于边缘检测和深度图的条件。我们还在尝试创建这些SDXL兼容的ControlNet模型的较小版本，以便更容易在资源受限的硬件上运行。您可以在[🤗
    Diffusers Hub组织](https://huggingface.co/diffusers)上找到这些检查点！
- en: 'Let’s use a SDXL ControlNet conditioned on canny images to generate an image.
    Start by loading an image and prepare the canny image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个基于边缘检测的SDXL ControlNet来生成一幅图像。首先加载一幅图像并准备边缘检测图像：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/dfd7f04394323063a78501b85525d2b3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfd7f04394323063a78501b85525d2b3.png)'
- en: original image
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像
- en: '![](../Images/a7144be809b8fc762e76238607a485d2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7144be809b8fc762e76238607a485d2.png)'
- en: canny image
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘检测图像
- en: Load a SDXL ControlNet model conditioned on canny edge detection and pass it
    to the [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline).
    You can also enable model offloading to reduce memory usage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个基于边缘检测的SDXL ControlNet模型，并将其传递给[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)。您还可以启用模型卸载以减少内存使用。
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now pass your prompt (and optionally a negative prompt if you’re using one)
    and canny image to the pipeline:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将您的提示（如果您使用负提示，则可选）和边缘检测图像传递给管道：
- en: The [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)
    parameter determines how much weight to assign to the conditioning inputs. A value
    of 0.5 is recommended for good generalization, but feel free to experiment with
    this number!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)参数确定分配给条件输入的权重。建议将值设为0.5以获得良好的泛化效果，但请随意尝试这个数字！'
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/033f4d98632798d409eabe8ebfffc8db.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/033f4d98632798d409eabe8ebfffc8db.png)'
- en: 'You can use [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)
    in guess mode as well by setting the parameter to `True`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过将参数设置为`True`，在猜测模式下使用[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: MultiControlNet
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MultiControlNet
- en: Replace the SDXL model with a model like [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    to use multiple conditioning inputs with Stable Diffusion models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将SDXL模型替换为像[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)这样的模型，以使用多个条件输入与Stable
    Diffusion模型。
- en: 'You can compose multiple ControlNet conditionings from different image inputs
    to create a *MultiControlNet*. To get better results, it is often helpful to:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从不同的图像输入中组合多个ControlNet条件，以创建*MultiControlNet*。为了获得更好的结果，通常有帮助的是：
- en: mask conditionings such that they don’t overlap (for example, mask the area
    of a canny image where the pose conditioning is located)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对条件进行遮罩处理，使其不重叠（例如，遮罩姿势条件所在的边缘检测图像区域）
- en: experiment with the [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)
    parameter to determine how much weight to assign to each conditioning input
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试[`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)参数，以确定分配给每个条件输入的权重
- en: In this example, you’ll combine a canny image and a human pose estimation image
    to generate a new image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，您将结合边缘检测图像和人体姿势估计图像来生成一幅新图像。
- en: 'Prepare the canny image conditioning:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 准备边缘检测图像条件：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/35bd0f11e1c7ca277ac60a85a75be06a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35bd0f11e1c7ca277ac60a85a75be06a.png)'
- en: original image
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像
- en: '![](../Images/5a37576bad87523fdb8a31aa840b4fdc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a37576bad87523fdb8a31aa840b4fdc.png)'
- en: canny image
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘检测图像
- en: 'For human pose estimation, install [controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人体姿势估计，安装[controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Prepare the human pose estimation conditioning:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 准备人体姿势估计条件：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/77e2344bf3a8143f50f20ca5d095bf81.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77e2344bf3a8143f50f20ca5d095bf81.png)'
- en: original image
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像
- en: '![](../Images/5a8432160ffe7c6268d59c8c8d536c20.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a8432160ffe7c6268d59c8c8d536c20.png)'
- en: human pose image
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿势图像
- en: Load a list of ControlNet models that correspond to each conditioning, and pass
    them to the [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline).
    Use the faster [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    and enable model offloading to reduce memory usage.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 加载与每个条件对应的ControlNet模型列表，并将它们传递给[StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)。使用更快的[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)并启用模型卸载以减少内存使用。
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now you can pass your prompt (an optional negative prompt if you’re using one),
    canny image, and pose image to the pipeline:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将您的提示（如果您使用负提示，则可选）、边缘检测图像和姿势图像传递给管道：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/780255e1bc5d49726b432882894f0b81.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/780255e1bc5d49726b432882894f0b81.png)'
