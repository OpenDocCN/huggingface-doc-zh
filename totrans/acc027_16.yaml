- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/quantization](https://huggingface.co/docs/accelerate/usage_guides/quantization)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: bitsandbytes Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ü§ó Accelerate brings `bitsandbytes` quantization to your model. You can now load
    any pytorch model in 8-bit or 4-bit with a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use ü§ó Transformers models with `bitsandbytes`, you should follow
    this [documentation](https://huggingface.co/docs/transformers/main_classes/quantization).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about how the `bitsandbytes` quantization works, check out the
    blog posts on [8-bit quantization](https://huggingface.co/blog/hf-bitsandbytes-integration)
    and [4-bit quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Requisites
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You will need to install the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Install `bitsandbytes` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Install latest `accelerate` from source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Install `minGPT` and `huggingface_hub` to run examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we need to initialize our model. To save memory, we can initialize an
    empty model using the context manager [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights).
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs take the GPT2 model from minGPT library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to get the path to the weights of your model. The path can be
    the state_dict file (e.g. ‚Äúpytorch_model.bin‚Äù) or a folder containing the sharded
    checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you need to set your quantization configuration with [BnbQuantizationConfig](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.BnbQuantizationConfig).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example for 8-bit quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs an example for 4-bit quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To quantize your empty model with the selected configuration, you need to use
    [load_and_quantize_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.load_and_quantize_model).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Saving and loading 8-bit model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can save your 8-bit model with accelerate using [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that 4-bit model serialization is currently not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Offload modules to cpu and disk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can offload some modules to cpu/disk if you don‚Äôt have enough space on the
    GPU to store the entire model on your GPUs. This uses big model inference under
    the hood. Check this [documentation](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: For 8-bit quantization, the selected modules will be converted to 8-bit precision.
  prefs: []
  type: TYPE_NORMAL
- en: For 4-bit quantization, the selected modules will be kept in `torch_dtype` that
    the user passed in `BnbQuantizationConfig`. We will add support to convert these
    offloaded modules in 4-bit when 4-bit serialization will be possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'You just need to pass a custom `device_map` in order to offload modules on
    cpu/disk. The offload modules will be dispatched on the GPU when needed. Here‚Äôs
    an example :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tune a quantized model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is not possible to perform pure 8bit or 4bit training on these models. However,
    you can train these models by leveraging parameter efficient fine tuning methods
    (PEFT) and train for example adapters on top of them. Please have a look at [peft](https://github.com/huggingface/peft)
    library for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, you can‚Äôt add adapters on top of any quantized model. However, with
    the official support of adapters with ü§ó Transformers models, you can fine-tune
    quantized models. If you want to finetune a ü§ó Transformers model , follow this
    [documentation](https://huggingface.co/docs/transformers/main_classes/quantization)
    instead. Check out this [demo](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)
    on how to fine-tune a 4-bit ü§ó Transformers model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you don‚Äôt need to pass `device_map` when loading the model for training.
    It will automatically load your model on your GPU. Please note that `device_map=auto`
    should be used for inference only.
  prefs: []
  type: TYPE_NORMAL
- en: Example demo - running GPT2 1.5b on a Google Colab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Check out the Google Colab [demo](https://colab.research.google.com/drive/1T1pOgewAWVpR9gKpaEWw4orOrzPFb3yM?usp=sharing)
    for running quantized models on a GTP2 model. The GPT2-1.5B model checkpoint is
    in FP32 which uses 6GB of memory. After quantization, it uses 1.6GB with 8-bit
    modules and 1.2GB with 4-bit modules.
  prefs: []
  type: TYPE_NORMAL
