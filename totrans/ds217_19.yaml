- en: Using Datasets with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的数据集
- en: 'Original text: [https://huggingface.co/docs/datasets/use_with_tensorflow](https://huggingface.co/docs/datasets/use_with_tensorflow)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/datasets/use_with_tensorflow](https://huggingface.co/docs/datasets/use_with_tensorflow)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This document is a quick introduction to using `datasets` with TensorFlow, with
    a particular focus on how to get `tf.Tensor` objects out of our datasets, and
    how to stream data from Hugging Face `Dataset` objects to Keras methods like `model.fit()`.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于如何在 TensorFlow 中使用 `datasets` 的快速介绍，特别关注如何从我们的数据集中获取 `tf.Tensor` 对象，以及如何将数据从
    Hugging Face 的 `Dataset` 对象流式传输到 Keras 方法如 `model.fit()`。
- en: Dataset format
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集格式
- en: 'By default, datasets return regular Python objects: integers, floats, strings,
    lists, etc.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，数据集返回常规的 Python 对象：整数、浮点数、字符串、列表等。
- en: 'To get TensorFlow tensors instead, you can set the format of the dataset to
    `tf`:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 TensorFlow 张量，您可以将数据集的格式设置为 `tf`：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object is a wrapper of an Arrow table, which allows fast reads from arrays in
    the dataset to TensorFlow tensors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    对象是 Arrow 表的包装器，允许从数据集中的数组快速读取到 TensorFlow 张量中。'
- en: 'This can be useful for converting your dataset to a dict of `Tensor` objects,
    or for writing a generator to load TF samples from it. If you wish to convert
    the entire dataset to `Tensor`, simply query the full dataset:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于将数据集转换为 `Tensor` 对象的字典，或者编写一个生成器从中加载 TF 样本非常有用。如果您希望将整个数据集转换为 `Tensor`，只需查询完整数据集即可：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: N-dimensional arrays
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N 维数组
- en: 'If your dataset consists of N-dimensional arrays, you will see that by default
    they are considered as nested lists. In particular, a TensorFlow formatted dataset
    outputs a `RaggedTensor` instead of a single tensor:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集由 N 维数组组成，您会发现默认情况下它们被视为嵌套列表。特别是，一个经过 TensorFlow 格式化的数据集会输出一个 `RaggedTensor`
    而不是单个张量：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To get a single tensor, you must explicitly use the `Array` feature type and
    specify the shape of your tensors:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得单个张量，您必须显式使用 `Array` 特征类型并指定张量的形状：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Other feature types
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他特征类型
- en: '[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    data are properly converted to tensors:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    数据会被正确转换为张量：'
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Strings and binary objects are also supported:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串和二进制对象也受支持：
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can also explicitly format certain columns and leave the other columns
    unformatted:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以显式格式化某些列并保留其他列的格式：
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: String and binary objects are unchanged, since PyTorch only supports numbers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串和二进制对象保持不变，因为 PyTorch 仅支持数字。
- en: The [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    and [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature types are also supported.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    和 [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    特征类型也受支持。'
- en: To use the [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    feature type, you’ll need to install the `vision` extra as `pip install datasets[vision]`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    特征类型，您需要安装 `vision` 额外模块，命令为 `pip install datasets[vision]`。
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To use the [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature type, you’ll need to install the `audio` extra as `pip install datasets[audio]`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    特征类型，您需要安装 `audio` 额外模块，命令为 `pip install datasets[audio]`。
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Data loading
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载
- en: Although you can load individual samples and batches just by indexing into your
    dataset, this won’t work if you want to use Keras methods like `fit()` and `predict()`.
    You could write a generator function that shuffles and loads batches from your
    dataset and `fit()` on that, but that sounds like a lot of unnecessary work. Instead,
    if you want to stream data from your dataset on-the-fly, we recommend converting
    your dataset to a `tf.data.Dataset` using the `to_tf_dataset()` method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以通过索引进入数据集来加载单个样本和批次，但如果您想使用 Keras 的方法如 `fit()` 和 `predict()`，这种方法就行不通了。您可以编写一个生成器函数，从数据集中洗牌并加载批次，然后在其上执行
    `fit()`，但这听起来像是很多不必要的工作。相反，如果您想要从数据集中实时流式传输数据，我们建议使用 `to_tf_dataset()` 方法将数据集转换为
    `tf.data.Dataset`。
- en: The `tf.data.Dataset` class covers a wide range of use-cases - it is often created
    from Tensors in memory, or using a load function to read files on disc or external
    storage. The dataset can be transformed arbitrarily with the `map()` method, or
    methods like `batch()` and `shuffle()` can be used to create a dataset that’s
    ready for training. These methods do not modify the stored data in any way - instead,
    the methods build a data pipeline graph that will be executed when the dataset
    is iterated over, usually during model training or inference. This is different
    from the `map()` method of Hugging Face `Dataset` objects, which runs the map
    function immediately and saves the new or changed columns.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset` 类涵盖了各种用例 - 它通常是从内存中的张量创建的，或者使用加载函数从磁盘或外部存储器中读取文件。数据集可以通过
    `map()` 方法任意转换，或者可以使用 `batch()` 和 `shuffle()` 等方法创建一个准备好用于训练的数据集。这些方法不会以任何方式修改存储的数据
    - 相反，这些方法构建了一个数据管道图，当数据集被迭代时将执行该图，通常在模型训练或推断期间。这与 Hugging Face 的 `Dataset` 对象的
    `map()` 方法不同，后者会立即运行 map 函数并保存新的或更改的列。'
- en: Since the entire data preprocessing pipeline can be compiled in a `tf.data.Dataset`,
    this approach allows for massively parallel, asynchronous data loading and training.
    However, the requirement for graph compilation can be a limitation, particularly
    for Hugging Face tokenizers, which are usually not (yet!) compilable as part of
    a TF graph. As a result, we usually advise pre-processing the dataset as a Hugging
    Face dataset, where arbitrary Python functions can be used, and then converting
    to `tf.data.Dataset` afterwards using `to_tf_dataset()` to get a batched dataset
    ready for training. To see examples of this approach, please see the [examples](https://github.com/huggingface/transformers/tree/main/examples)
    or [notebooks](https://huggingface.co/docs/transformers/notebooks) for `transformers`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整个数据预处理流程可以在`tf.data.Dataset`中编译，这种方法允许进行大规模并行、异步数据加载和训练。然而，图形编译的要求可能是一个限制，特别是对于Hugging
    Face的分词器，通常不是（尚未！）可编译为TF图的一部分。因此，我们通常建议将数据集预处理为Hugging Face数据集，其中可以使用任意Python函数，然后使用`to_tf_dataset()`将其转换为`tf.data.Dataset`，以获得一个准备好进行训练的批处理数据集。要查看这种方法的示例，请参阅`transformers`的[examples](https://github.com/huggingface/transformers/tree/main/examples)或[notebooks](https://huggingface.co/docs/transformers/notebooks)。
- en: Using to_tf_dataset()
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用to_tf_dataset()
- en: 'Using `to_tf_dataset()` is straightforward. Once your dataset is preprocessed
    and ready, simply call it like so:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`to_tf_dataset()`很简单。一旦您的数据集经过预处理并准备好，只需这样调用：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The returned `tf_ds` object here is now fully ready to train on, and can be
    passed directly to `model.fit()`. Note that you set the batch size when creating
    the dataset, and so you don’t need to specify it when calling `fit()`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里返回的`tf_ds`对象现在已经完全准备好进行训练，并且可以直接传递给`model.fit()`。请注意，在创建数据集时设置批次大小，因此在调用`fit()`时不需要指定它：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For a full description of the arguments, please see the [to_tf_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    documentation. In many cases, you will also need to add a `collate_fn` to your
    call. This is a function that takes multiple elements of the dataset and combines
    them into a single batch. When all elements have the same length, the built-in
    default collator will suffice, but for more complex tasks a custom collator may
    be necessary. In particular, many tasks have samples with varying sequence lengths
    which will require a [data collator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator)
    that can pad batches correctly. You can see examples of this in the `transformers`
    NLP [examples](https://github.com/huggingface/transformers/tree/main/examples)
    and [notebooks](https://huggingface.co/docs/transformers/notebooks), where variable
    sequence lengths are very common.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有关参数的完整描述，请参阅[to_tf_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)文档。在许多情况下，您还需要在调用中添加`collate_fn`。这是一个函数，它接受数据集的多个元素并将它们组合成一个批次。当所有元素具有相同长度时，内置的默认整理器就足够了，但对于更复杂的任务，可能需要一个自定义整理器。特别是，许多任务具有具有不同序列长度的样本，这将需要一个能够正确填充批次的[数据整理器](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator)。您可以在`transformers`
    NLP [examples](https://github.com/huggingface/transformers/tree/main/examples)和[notebooks](https://huggingface.co/docs/transformers/notebooks)中看到这种情况的示例，其中变长序列是非常常见的。
- en: If you find that loading with `to_tf_dataset` is slow, you can also use the
    `num_workers` argument. This spins up multiple subprocesses to load data in parallel.
    This feature is recent and still somewhat experimental - please file an issue
    if you encounter any bugs while using it!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发现使用`to_tf_dataset`加载速度慢，您也可以使用`num_workers`参数。这会启动多个子进程并行加载数据。这个功能是最近添加的，仍然有些实验性
    - 如果在使用过程中遇到任何错误，请提交问题！
- en: When to use to_tf_dataset
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用to_tf_dataset
- en: The astute reader may have noticed at this point that we have offered two approaches
    to achieve the same goal - if you want to pass your dataset to a TensorFlow model,
    you can either convert the dataset to a `Tensor` or `dict` of `Tensors` using
    `.with_format('tf')`, or you can convert the dataset to a `tf.data.Dataset` with
    `to_tf_dataset()`. Either of these can be passed to `model.fit()`, so which should
    you choose?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 敏锐的读者可能已经注意到，我们在这一点上提供了两种方法来实现相同的目标 - 如果您想将数据集传递给TensorFlow模型，您可以将数据集转换为`Tensor`或`dict`
    of `Tensors`，使用`.with_format('tf')`，或者您可以使用`to_tf_dataset()`将数据集转换为`tf.data.Dataset`。这两者都可以传递给`model.fit()`，那么您应该选择哪一个呢？
- en: 'The key thing to recognize is that when you convert the whole dataset to `Tensor`s,
    it is static and fully loaded into RAM. This is simple and convenient, but if
    any of the following apply, you should probably use `to_tf_dataset()` instead:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 识别的关键是，当您将整个数据集转换为`Tensor`时，它是静态的并完全加载到RAM中。这很简单和方便，但如果以下任何情况适用，您应该使用`to_tf_dataset()`：
- en: Your dataset is too large to fit in RAM. `to_tf_dataset()` streams only one
    batch at a time, so even very large datasets can be handled with this method.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的数据集太大，无法放入RAM中。`to_tf_dataset()`一次只流式传输一个批次，因此即使是非常大的数据集也可以使用这种方法处理。
- en: You want to apply random transformations using `dataset.with_transform()` or
    the `collate_fn`. This is common in several modalities, such as image augmentations
    when training vision models, or random masking when training masked language models.
    Using `to_tf_dataset()` will apply those transformations at the moment when a
    batch is loaded, which means the same samples will get different augmentations
    each time they are loaded. This is usually what you want.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您想使用`dataset.with_transform()`或`collate_fn`应用随机转换。这在几种模态中很常见，例如在训练视觉模型时进行图像增强，或者在训练掩蔽语言模型时进行随机掩蔽。使用`to_tf_dataset()`将在加载批次时应用这些转换，这意味着每次加载时相同的样本将获得不同的增强。这通常是您想要的。
- en: Your data has a variable dimension, such as input texts in NLP that consist
    of varying numbers of tokens. When you create a batch with samples with a variable
    dimension, the standard solution is to pad the shorter samples to the length of
    the longest one. When you stream samples from a dataset with `to_tf_dataset`,
    you can apply this padding to each batch via your `collate_fn`. However, if you
    want to convert such a dataset to dense `Tensor`s, then you will have to pad samples
    to the length of the longest sample in *the entire dataset!* This can result in
    huge amounts of padding, which wastes memory and reduces your model’s speed.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的数据具有可变维度，例如自然语言处理中的输入文本，其中包含不同数量的标记。当您创建一个包含可变维度样本的批次时，标准解决方案是将较短的样本填充到最长样本的长度。当您从数据集中流式传输样本时，可以通过您的`collate_fn`将此填充应用于每个批次。但是，如果您想将这样的数据集转换为稠密的`Tensor`，那么您将不得不将样本填充到*整个数据集中最长样本的长度*！这可能导致大量的填充，浪费内存并降低模型的速度。
- en: Caveats and limitations
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项和限制
- en: Right now, `to_tf_dataset()` always returns a batched dataset - we will add
    support for unbatched datasets soon!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`to_tf_dataset()`始终返回一个批处理的数据集 - 我们将很快添加对未批处理数据集的支持！
