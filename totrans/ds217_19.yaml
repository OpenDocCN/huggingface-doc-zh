- en: Using Datasets with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/datasets/use_with_tensorflow](https://huggingface.co/docs/datasets/use_with_tensorflow)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/datasets/v2.17.0/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/start.146395b0.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/scheduler.bdbef820.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/singletons.98dc5b8b.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.8a885b74.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/paths.a483fec8.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/app.e612c4fb.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.c0aea24a.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/0.5e8dbda6.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/55.8aac320c.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Tip.31005f7d.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/CodeBlock.6ccca92e.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Heading.2eb892cb.js">
  prefs: []
  type: TYPE_NORMAL
- en: This document is a quick introduction to using `datasets` with TensorFlow, with
    a particular focus on how to get `tf.Tensor` objects out of our datasets, and
    how to stream data from Hugging Face `Dataset` objects to Keras methods like `model.fit()`.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, datasets return regular Python objects: integers, floats, strings,
    lists, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get TensorFlow tensors instead, you can set the format of the dataset to
    `tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object is a wrapper of an Arrow table, which allows fast reads from arrays in
    the dataset to TensorFlow tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be useful for converting your dataset to a dict of `Tensor` objects,
    or for writing a generator to load TF samples from it. If you wish to convert
    the entire dataset to `Tensor`, simply query the full dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: N-dimensional arrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If your dataset consists of N-dimensional arrays, you will see that by default
    they are considered as nested lists. In particular, a TensorFlow formatted dataset
    outputs a `RaggedTensor` instead of a single tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a single tensor, you must explicitly use the `Array` feature type and
    specify the shape of your tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Other feature types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    data are properly converted to tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Strings and binary objects are also supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also explicitly format certain columns and leave the other columns
    unformatted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: String and binary objects are unchanged, since PyTorch only supports numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    and [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature types are also supported.
  prefs: []
  type: TYPE_NORMAL
- en: To use the [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    feature type, you’ll need to install the `vision` extra as `pip install datasets[vision]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To use the [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature type, you’ll need to install the `audio` extra as `pip install datasets[audio]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Data loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although you can load individual samples and batches just by indexing into your
    dataset, this won’t work if you want to use Keras methods like `fit()` and `predict()`.
    You could write a generator function that shuffles and loads batches from your
    dataset and `fit()` on that, but that sounds like a lot of unnecessary work. Instead,
    if you want to stream data from your dataset on-the-fly, we recommend converting
    your dataset to a `tf.data.Dataset` using the `to_tf_dataset()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.data.Dataset` class covers a wide range of use-cases - it is often created
    from Tensors in memory, or using a load function to read files on disc or external
    storage. The dataset can be transformed arbitrarily with the `map()` method, or
    methods like `batch()` and `shuffle()` can be used to create a dataset that’s
    ready for training. These methods do not modify the stored data in any way - instead,
    the methods build a data pipeline graph that will be executed when the dataset
    is iterated over, usually during model training or inference. This is different
    from the `map()` method of Hugging Face `Dataset` objects, which runs the map
    function immediately and saves the new or changed columns.
  prefs: []
  type: TYPE_NORMAL
- en: Since the entire data preprocessing pipeline can be compiled in a `tf.data.Dataset`,
    this approach allows for massively parallel, asynchronous data loading and training.
    However, the requirement for graph compilation can be a limitation, particularly
    for Hugging Face tokenizers, which are usually not (yet!) compilable as part of
    a TF graph. As a result, we usually advise pre-processing the dataset as a Hugging
    Face dataset, where arbitrary Python functions can be used, and then converting
    to `tf.data.Dataset` afterwards using `to_tf_dataset()` to get a batched dataset
    ready for training. To see examples of this approach, please see the [examples](https://github.com/huggingface/transformers/tree/main/examples)
    or [notebooks](https://huggingface.co/docs/transformers/notebooks) for `transformers`.
  prefs: []
  type: TYPE_NORMAL
- en: Using to_tf_dataset()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using `to_tf_dataset()` is straightforward. Once your dataset is preprocessed
    and ready, simply call it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned `tf_ds` object here is now fully ready to train on, and can be
    passed directly to `model.fit()`. Note that you set the batch size when creating
    the dataset, and so you don’t need to specify it when calling `fit()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For a full description of the arguments, please see the [to_tf_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    documentation. In many cases, you will also need to add a `collate_fn` to your
    call. This is a function that takes multiple elements of the dataset and combines
    them into a single batch. When all elements have the same length, the built-in
    default collator will suffice, but for more complex tasks a custom collator may
    be necessary. In particular, many tasks have samples with varying sequence lengths
    which will require a [data collator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator)
    that can pad batches correctly. You can see examples of this in the `transformers`
    NLP [examples](https://github.com/huggingface/transformers/tree/main/examples)
    and [notebooks](https://huggingface.co/docs/transformers/notebooks), where variable
    sequence lengths are very common.
  prefs: []
  type: TYPE_NORMAL
- en: If you find that loading with `to_tf_dataset` is slow, you can also use the
    `num_workers` argument. This spins up multiple subprocesses to load data in parallel.
    This feature is recent and still somewhat experimental - please file an issue
    if you encounter any bugs while using it!
  prefs: []
  type: TYPE_NORMAL
- en: When to use to_tf_dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The astute reader may have noticed at this point that we have offered two approaches
    to achieve the same goal - if you want to pass your dataset to a TensorFlow model,
    you can either convert the dataset to a `Tensor` or `dict` of `Tensors` using
    `.with_format('tf')`, or you can convert the dataset to a `tf.data.Dataset` with
    `to_tf_dataset()`. Either of these can be passed to `model.fit()`, so which should
    you choose?
  prefs: []
  type: TYPE_NORMAL
- en: 'The key thing to recognize is that when you convert the whole dataset to `Tensor`s,
    it is static and fully loaded into RAM. This is simple and convenient, but if
    any of the following apply, you should probably use `to_tf_dataset()` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: Your dataset is too large to fit in RAM. `to_tf_dataset()` streams only one
    batch at a time, so even very large datasets can be handled with this method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to apply random transformations using `dataset.with_transform()` or
    the `collate_fn`. This is common in several modalities, such as image augmentations
    when training vision models, or random masking when training masked language models.
    Using `to_tf_dataset()` will apply those transformations at the moment when a
    batch is loaded, which means the same samples will get different augmentations
    each time they are loaded. This is usually what you want.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your data has a variable dimension, such as input texts in NLP that consist
    of varying numbers of tokens. When you create a batch with samples with a variable
    dimension, the standard solution is to pad the shorter samples to the length of
    the longest one. When you stream samples from a dataset with `to_tf_dataset`,
    you can apply this padding to each batch via your `collate_fn`. However, if you
    want to convert such a dataset to dense `Tensor`s, then you will have to pad samples
    to the length of the longest sample in *the entire dataset!* This can result in
    huge amounts of padding, which wastes memory and reduces your model’s speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caveats and limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Right now, `to_tf_dataset()` always returns a batched dataset - we will add
    support for unbatched datasets soon!
  prefs: []
  type: TYPE_NORMAL
