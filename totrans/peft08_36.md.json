["```py\n( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False target_modules: Union = None feedforward_modules: Union = None fan_in_fan_out: bool = False modules_to_save: Optional = None init_ia3_weights: bool = True )\n```", "```py\n( model config adapter_name ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, ia3Config\n>>> from peft import IA3Model, IA3Config\n\n>>> config = IA3Config(\n...     peft_type=\"IA3\",\n...     task_type=\"SEQ_2_SEQ_LM\",\n...     target_modules=[\"k\", \"v\", \"w0\"],\n...     feedforward_modules=[\"w0\"],\n... )\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> ia3_model = IA3Model(config, model)\n```", "```py\n( adapter_name: str )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( safe_merge: bool = False adapter_names: Optional[list[str]] = None )\n```", "```py\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModel\n\n>>> base_model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\")\n>>> peft_model_id = \"smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample\"\n>>> model = PeftModel.from_pretrained(base_model, peft_model_id)\n>>> merged_model = model.merge_and_unload()\n```", "```py\n( adapter_name: str | list[str] )\n```", "```py\n>>> for name, param in model_peft.named_parameters():\n...     if ...:  # some check on name (ex. if 'lora' in name)\n...         param.requires_grad = False\n```", "```py\n( )\n```"]